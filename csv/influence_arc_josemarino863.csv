2004.iwslt-evaluation.14,N04-1033,0,0.0910244,"This has growing importance when dealing with very different pairs of languages, in terms of word ordering, as with a ChineseEnglish task. In section 4.3 the impact of this technique is evaluated in practice. 1. It induces a monotonous segmentation of the pair of sentences. 2. Words are consecutive along both source and target sides of the tuple. 3. No word on either side of the tuple is aligned to a word out of the tuple. 4. Each tuple cannot be decomposed into smaller phrases without violating the previous constraint. 3.4. Xgram estimation Note that this set is unique under these conditions [8]. The only ambiguity appears when a target word is aligned to NULL, in which case we append it to the next tuple (if exists, else to the previous). An example of the tuple extraction process is drawn in figure 3. When extracting tuples with more than one word in each language (as the third tuple in figure 3), a certain local reordering of the target is necessarily encoded. While helping the system to avoid local reordering mistakes, this strategy can suffer from an information loss, as the source words appearing in this tuple may not have any Finally, given the parallel corpus described in a s"
2004.iwslt-evaluation.14,C96-2141,0,0.578405,"on the Chinese-English supplied task of the Int. Workshop on Spoken Language Translation (IWSLT’04) Evaluation Campaign are shown and discussed. Figure 1: A translation FST from Spanish to English the same structure and search method, acoustic models can be omitted to perform text translation tasks only. This translation FST is learned automatically from a parallel corpus in three main steps (and an optional preprocessing). First, an automatic word alignment is produced. Currently this is done by the freely-available GIZA++ software [2], implementing well-known IBM and HMM translation models [3, 4]. From this alignment, a tuple extraction algorithm generates the set tuples that induces a sequential segmentation of both source and target sentences. These tuples must respect word order in both languages, as this is necessary for the transducer to produce a correct-order translated output. Finally, Xgrams are learned using standard language modeling techniques. Previous publications on this system include [5] and [6]. The organization of the paper is as follows. Section 2 offers an overview of the system architecture, whereas sections 2 and 3 deepen into details on translation generation a"
2004.iwslt-papers.3,W03-0301,0,0.062964,"ocessing, such as bilingual dictionaries extraction or transfer rules learning. However, it is in the context of statistical machine translation where it becomes particularly crucial. As an essential block in the learning process of current statistical translation models (single-word or phrase-based, conditionalor joined-probability based), its correct production has a sound correlation with translation quality [1]. This relevance has been corresponded by many previous works on the matter, including a shared task in the frame of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts [2]. Several competing systems were presented and evaluated against a manual reference using AER, the most widely used evaluation measure. Among the wide range of approaches presented, two basic trends should be highlighted. On the other side of the spectrum, we find the approach based on word cooccurrences and link probabilities, presented in [10]. Its relative simplicity, its flexibility to introduce more knowledge sources, its symmetry and its promising results [2] make it appealing despite its dependence on empirical data and tuning strategies. However, the most important disadvantage of this"
2004.iwslt-papers.3,C96-2141,0,0.435176,"Missing"
2004.iwslt-papers.3,W99-0604,0,0.110404,"Missing"
2004.iwslt-papers.3,N03-1017,0,0.0373062,"Missing"
2004.iwslt-papers.3,W02-1012,0,0.0356565,"target sentence to positions in source sentence, it is strictly asymmetric, generating one-to-many word alignments that do not account for many translation phenomena. This effect has been tackled by several kinds of symmetrization heuristics (all of them linguistically blind), in search of a strategy to provide posterior phrase-based translation systems with the most accurate possible source. Moreover, the complexity of IBM models and their overload of parameters to estimate turn it very hard to introduce linguistic information into this setting in a reasonable way (some efforts being done in [9]). This paper introduces a phrase alignment strategy that seeks phrase and word links in two stages using cooccurrence measures and linguistic information. On a first stage, the algorithm finds high-precision links involving a linguistically-derived set of phrases, leaving word alignment to be performed in a second phase. Experiments have been carried out for an English-Spanish parallel corpus, and we show how phrase cooccurrence measures convey a complementary information to word cooccurrences, and a stronger evidence of a good alignment. Alignment Error Rate (AER) results are presented, bein"
2004.iwslt-papers.3,P03-1012,0,0.0281238,"correct production has a sound correlation with translation quality [1]. This relevance has been corresponded by many previous works on the matter, including a shared task in the frame of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts [2]. Several competing systems were presented and evaluated against a manual reference using AER, the most widely used evaluation measure. Among the wide range of approaches presented, two basic trends should be highlighted. On the other side of the spectrum, we find the approach based on word cooccurrences and link probabilities, presented in [10]. Its relative simplicity, its flexibility to introduce more knowledge sources, its symmetry and its promising results [2] make it appealing despite its dependence on empirical data and tuning strategies. However, the most important disadvantage of this approach is the one-to-one constrain, producing high precision alignments with low recall, what can represent a severe limitation to its use in practical translation systems. We present in this paper an alignment strategy that is also based on bilingual cooccurrences, but aims at finding phrase-to-phrase alignment by using linguistic knowledge"
2004.iwslt-papers.3,W02-1018,0,0.0440718,"rases. Typically, initial Giza-based alignments are generated and a symmetrization strategy is followed to obtain the core alignment from which bilingual phrases are built. These in turn are fed to the statistical translation model for estimation. However, current symmetrization strategies lack linguistic knowledge to decide ambiguities, and the translation model faces the task of learning translation probabilities from a noisy source. To face this problem, we present an alignment strategy that generates directly a phrase alignment from corpus cooccurrence counts. In contrast to previous work [11], we do so by generating very high-confidence links between phrases before proceeding onto word alignment. This search for phrase links is limited to a small adequate set of possible phrases. In the following subsections our proposal is described in detail. Table 1: Examples of φ2 between words and phrases. por favor Association or cooccurrence measures extracted from parallel corpora give strong evidence of so-called translation equivalence [12], or simply alignment adequacy, between a pair of phrases or words. Among these measures we find Dice-score, φ2 score and some others, offering a simi"
2004.iwslt-papers.3,J03-1002,0,0.0116221,"subwords or phrases) in a given parallel corpus, detecting which words from each language are connected together in a given situation. This has many applications in natural language processing, such as bilingual dictionaries extraction or transfer rules learning. However, it is in the context of statistical machine translation where it becomes particularly crucial. As an essential block in the learning process of current statistical translation models (single-word or phrase-based, conditionalor joined-probability based), its correct production has a sound correlation with translation quality [1]. This relevance has been corresponded by many previous works on the matter, including a shared task in the frame of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts [2]. Several competing systems were presented and evaluated against a manual reference using AER, the most widely used evaluation measure. Among the wide range of approaches presented, two basic trends should be highlighted. On the other side of the spectrum, we find the approach based on word cooccurrences and link probabilities, presented in [10]. Its relative simplicity, its flexibility to introduce more knowled"
2004.iwslt-papers.3,N03-2017,0,0.0297579,"d include regular expressions such as numbers, dates or times of the day (that could also be classified) or even collocations and phrasal verbs. As this selection is language-dependent, every language will 109 define its own adequate rules. sented in [10]. Basically, an initial alignment is generated using word cooccurrence measures, from which link probabilities are estimated. Then, a best first search is performed, following an heuristic function based on the global aligned sentence link probabilities. The search is further improved with a syntactic constrain (also called cohesion constrain [15]) and can introduce features on the links, such as a dependence on adjacent links. Our implementation allows certain positions to be prohibited, so that previous phrase alignment is fixed, although its links also compute in link probability estimation at each iteration. Given the enormous space of possible word alignments to choose from, the heuristic function becomes the key to efficiency, so long as it is correctly defined. Basic parameters are: If no linguistic knowledge is available, statistical procedures can also be used to obtain a set of possible phrases. For example, we can select the"
2004.iwslt-papers.3,A00-1031,0,0.0489206,"ment strategy proposed against state-of-the-art alignments. Lmean 7.6 7.3 3.2. Preprocessing All preprocessing that has been carried out is described as follows: 4.1. Verb groups • Normalization of contracted forms for English (ie. wouldn’t = would not, we’ve = we have) and Spanish (del = de el) Verb groups detection rules include 14 rules for English language and just 6 for Spanish, which usually employs declined verb forms omitting thus personal pronouns and using thus a single word. Verb groups rules have detected a total of: • English data has been tagged using freely-available TnT tagger [16], and base forms have been obtained using wnmorph, included in the WordNet package [17]. • 1156 verb groups in English, classified into 238 different verb lemmas • Spanish data has been tagged using maco+ and relax package already mentioned. This software also generates a lemma or base form for each input word. • 658 verb groups in Spanish, classified into 188 different verb lemmas The classification of these phrases produces an efficient reduction of the cooccurrence table from 0.35M to 0.33M, we do not compute cooccurrence counts for all words internal to the phrase, but just for the lemma o"
2004.iwslt-papers.3,H92-1116,0,0.0518371,"ssing All preprocessing that has been carried out is described as follows: 4.1. Verb groups • Normalization of contracted forms for English (ie. wouldn’t = would not, we’ve = we have) and Spanish (del = de el) Verb groups detection rules include 14 rules for English language and just 6 for Spanish, which usually employs declined verb forms omitting thus personal pronouns and using thus a single word. Verb groups rules have detected a total of: • English data has been tagged using freely-available TnT tagger [16], and base forms have been obtained using wnmorph, included in the WordNet package [17]. • 1156 verb groups in English, classified into 238 different verb lemmas • Spanish data has been tagged using maco+ and relax package already mentioned. This software also generates a lemma or base form for each input word. • 658 verb groups in Spanish, classified into 188 different verb lemmas The classification of these phrases produces an efficient reduction of the cooccurrence table from 0.35M to 0.33M, we do not compute cooccurrence counts for all words internal to the phrase, but just for the lemma of its head verb. The results of the phrase alignment with these phrases are shown in th"
2004.iwslt-papers.3,P00-1056,0,0.146299,"Missing"
2004.iwslt-papers.3,H91-1026,0,\N,Missing
2005.iwslt-1.23,2002.tmi-tutorials.2,0,0.0759388,"ared under monotone conditions. We finally report comparative results in terms of translation accuracy, computation time and memory size. Results show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1. Introduction From the initial word-based translation models [1], research on statistical machine translation has been strongly boosted. At the end of the last decade the use of context in the translation model (phrase-based approach) lead to a clear improvement in translation quality ( [2], [3], [4]). Nowadays the introduction of some reordering abilities is of crucial importance for some language pairs and is an important focus of research in the area of SMT. In parallel to the phrase-based approach, the ngrambased approach [5] also introduces the word context in the translation model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phras"
2005.iwslt-1.23,P01-1067,0,0.0561396,"under monotone conditions. We finally report comparative results in terms of translation accuracy, computation time and memory size. Results show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1. Introduction From the initial word-based translation models [1], research on statistical machine translation has been strongly boosted. At the end of the last decade the use of context in the translation model (phrase-based approach) lead to a clear improvement in translation quality ( [2], [3], [4]). Nowadays the introduction of some reordering abilities is of crucial importance for some language pairs and is an important focus of research in the area of SMT. In parallel to the phrase-based approach, the ngrambased approach [5] also introduces the word context in the translation model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, w"
2005.iwslt-1.23,W02-1018,0,0.0292349,"monotone conditions. We finally report comparative results in terms of translation accuracy, computation time and memory size. Results show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1. Introduction From the initial word-based translation models [1], research on statistical machine translation has been strongly boosted. At the end of the last decade the use of context in the translation model (phrase-based approach) lead to a clear improvement in translation quality ( [2], [3], [4]). Nowadays the introduction of some reordering abilities is of crucial importance for some language pairs and is an important focus of research in the area of SMT. In parallel to the phrase-based approach, the ngrambased approach [5] also introduces the word context in the translation model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, where"
2005.iwslt-1.23,N04-1033,0,0.415123,"tuples methods. As can be seen,to produce the source sentence, the extracted unfolded tuples must be reordered. It is not the case of the target sentence, as it can be produced in order using both sequence of units. Figure 1 shows the bilingual units extracted using the extract-tuples and extract-unfold-tuples methods, for a given word-to-word aligned sentence pair. 2.2. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any gi"
2005.iwslt-1.23,J04-4002,0,0.0562728,"t is not the case of the target sentence, as it can be produced in order using both sequence of units. Figure 1 shows the bilingual units extracted using the extract-tuples and extract-unfold-tuples methods, for a given word-to-word aligned sentence pair. 2.2. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provid"
2005.iwslt-1.23,W05-0827,1,0.833763,"t can be produced in order using both sequence of units. Figure 1 shows the bilingual units extracted using the extract-tuples and extract-unfold-tuples methods, for a given word-to-word aligned sentence pair. 2.2. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [16] as the"
2005.iwslt-1.23,P05-1032,0,0.0181694,"he target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [16] as the probability of reappearence of larger phrases decreases. In our system we considered two length limits. We first extract all the phrases of length X or less (usually X equal to 3 or 4). Then, we also add phrases up to length Y (Y greater than X) if they cannot be generated by smaller phrases. Basically, we select additional phrases with source words that otherwise would be missed because"
2005.iwslt-1.23,N03-1017,0,0.0564353,"gth in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [16] as the probability of reappearence of larger phrases decreases. In our system we considered two length limits. We first extract all the phrases of length X or less (usually X equal to 3 or 4). Then, we also add phrases up to length Y (Y greater than X) if they cannot be generated by smaller phrases. Basically, we select additional phrases with source words that otherwise would be missed because of cross or long alignments [14]. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frecuency. N (f, e) P (f |e) = N (e) (3) where N(f,e) means t"
2005.iwslt-1.23,N04-1021,0,0.198833,"s is approximated by the product of word 3-gram probabilities: p(Tk ) ≈ k Y p(wn |wn−2 , wn−1 ) (4) n=1 where Tk refers to the partial translation hypothesis and wn to the nth word in it. As default language model feature, we use a standard word-based trigram language model generated with smoothing Kneser-Ney and interpolation of higher and lower order ngrams (by using SRILM [17]). • The following two feature functions correspond to a forward and backwards lexicon models. These models provides lexicon translation probabilities for each tuple based on the word-to-word IBM model 1 probabilities [18]. These lexicon models are computed according to the following equation: p((t, s)n ) = J X I Y 1 pIBM 1 (tin |sjn ) (I + 1)J j=1 i=0 (5) where sjn and tin are the j th and ith words in the source and target sides of tuple (t, s)n , being J and I the corresponding total number words in each side of it. For computing the forward lexicon model, IBM model 1 probabilities from GIZA++ [19] source-to-target alignments are used. In the case of the backwards lexicon model, GIZA++ target-to-source alignments are used instead. • The last feature in common we consider corresponds to a word penalty model."
2005.iwslt-1.23,2005.mtsummit-papers.37,1,0.848127,"model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, where the sequential composition of the phrases source words corresponds to the source sentence reordered. This procedure poses additional difficulties when applied to the ngram-based approach, because the characteristics of the ngram-based translation model. Despite of this, recent works ( [7], [8]) have shown how applying a reordering schema in the training process the ngram-based approach can also take advantage of the distortion capabilities. In this paper we study the differences and similarities of both approaches (ngram-based and phrase-based), focusing on the translation model, where the translation context is differently taken into account. We also investigate the differences in the translation (bilingual) units (tuples and phrases) and show efficiency results in terms of computation time and memory size for both systems. We have extended the comparison in [6] to a Chinese"
2005.iwslt-1.23,W05-0831,0,0.0547079,"l, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, where the sequential composition of the phrases source words corresponds to the source sentence reordered. This procedure poses additional difficulties when applied to the ngram-based approach, because the characteristics of the ngram-based translation model. Despite of this, recent works ( [7], [8]) have shown how applying a reordering schema in the training process the ngram-based approach can also take advantage of the distortion capabilities. In this paper we study the differences and similarities of both approaches (ngram-based and phrase-based), focusing on the translation model, where the translation context is differently taken into account. We also investigate the differences in the translation (bilingual) units (tuples and phrases) and show efficiency results in terms of computation time and memory size for both systems. We have extended the comparison in [6] to a Chinese to En"
2005.iwslt-1.23,P02-1038,0,0.234488,"els taken into account in the log-linear combination of features (see equation 1), and the bilingual units extraction methods (namely tuples and phrases). In section 3 is discussed the decoder used in both systems (MARIE) [9], giving details of pruning and reordering techniques. The comparison framework, experiments and results are shown in section 4, while conclusions are detailed in section 5. 2. Modeling Alternatively to the classical source channel approach, statistical machine translation models directly the posterior probability p(eI1 |f1J ) as a log-linear combination of feature models [10], based on the maximum entropy framework, as shown in [11]. This simplifies the introduction of several additional models explaining the translation process, as the search becomes: arg max{exp( eI1 X λi hi (e, f ))} (1) i where the feature functions hi are the system models (translation model, language model, reordering model, ...), and the λi weights are typically optimized to maximize a scoring function on a development set. The Translation Model is based on bilingual units (here called tuples and phrases). A bilingual unit consists of two monolingual fragments, where each one is supposed to"
2005.iwslt-1.23,J96-1002,0,0.0712527,"atures (see equation 1), and the bilingual units extraction methods (namely tuples and phrases). In section 3 is discussed the decoder used in both systems (MARIE) [9], giving details of pruning and reordering techniques. The comparison framework, experiments and results are shown in section 4, while conclusions are detailed in section 5. 2. Modeling Alternatively to the classical source channel approach, statistical machine translation models directly the posterior probability p(eI1 |f1J ) as a log-linear combination of feature models [10], based on the maximum entropy framework, as shown in [11]. This simplifies the introduction of several additional models explaining the translation process, as the search becomes: arg max{exp( eI1 X λi hi (e, f ))} (1) i where the feature functions hi are the system models (translation model, language model, reordering model, ...), and the λi weights are typically optimized to maximize a scoring function on a development set. The Translation Model is based on bilingual units (here called tuples and phrases). A bilingual unit consists of two monolingual fragments, where each one is supposed to be the translation of its counterpart. During training, t"
2005.iwslt-1.23,takezawa-etal-2002-toward,0,0.0468191,"sary trade-off between quality and efficiency. 4. Comparison 4.1. Evaluation Framework Figure 2: Search graph corresponding to a source sentence with four words. Details of constraints are given in following sections. The search loops expanding available hypotheses. The expansion proceeds incrementally starting in the group of lists covering 1 source word, ending with the group of lists covering J − 1 source words (J is the size in words of the source sentence). See [9] for further details. Experiments have been carried out using two databases: the EPPS database (Spanish-English) and the BTEC [20] database (Chinese-English). The BTEC is a small corpus translation task, used in the IWSLT’04 spoken language campaign1. Table 1 shows the main statistics of the used data, namely number of sentences, words, vocabulary, and mean sentence lengths for each language. The EPPS data set corresponds to the parliamentary session transcriptions of the European Parliament and is currently available at the Parliament’s website (http://www.euro parl.eu.int/). In the case of the results presented here, we have used the version of the EPPS data that was made available by RWTH Aachen University through the"
2005.iwslt-1.23,2005.iwslt-1.24,1,0.680304,"ated to the corpus size). Similar accuracy results in all tasks are reached for the baseline configurations. When upgrading the systems with additional features, slight differences appear. Although improvements added by each feature depends on the task and system, similar performances are reached in the best system’s configurations. Under reordering conditions, the ngram-based system seems to take advantage of the unfolding method applied in training, outperforming the phrase-based system. However, last results obtained for the IWSLT’05 show an opposite behaviour of both systems, see [22] and [23]. We can conclude that both approaches have a similar performance in terms of translation quality. The slight differences seen in the experiments are related to how the systems take advantage of each feature model and to the current system’s implementation. In terms of the memory size and computation time, the ngram-based system has obtained consistently better results. This indicates how even though using a smaller vocabulary of bilingual units, it has been more efficiently built and managed. The last characteristic becomes of great importance when working with large databases. 6. Acknowledgm"
2005.iwslt-1.25,2005.mtsummit-papers.36,1,0.794738,"m (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, the translation system described in this paper implements a log-linear combination of one translation model and four additional feature models. In contrast with standard phrase-based approaches, our translation model is expressed in tuples as bilingual units. Given a word alignment, tuples define a unique and monotonic segmentation of each bilingual sentence, building up a much smaller set of units than with phrases and allowing N-gram estimation to account for the history of the translation process [5, 6]. This approach has its origins in SMT by using finite state transducers [7, 8, 9]. The organization of the paper is as follows. Section 2 describes in detail the tuple n-gram translation model, while section 3 introduces the additional features used in the system. Section 4 provides a brief overview of the decoding tool and search strategy used. Next, sections 5 and 6 report and discuss results on IWSLT’05 Chineseto-English and Arabic-to-English tracks, respectively. Finally, Section 7 concludes and outlines future research lines. 2. The Tuple N-gram translation model The tuple N-gram transla"
2005.iwslt-1.25,2004.iwslt-evaluation.14,1,0.888051,"this approach, the translation system described in this paper implements a log-linear combination of one translation model and four additional feature models. In contrast with standard phrase-based approaches, our translation model is expressed in tuples as bilingual units. Given a word alignment, tuples define a unique and monotonic segmentation of each bilingual sentence, building up a much smaller set of units than with phrases and allowing N-gram estimation to account for the history of the translation process [5, 6]. This approach has its origins in SMT by using finite state transducers [7, 8, 9]. The organization of the paper is as follows. Section 2 describes in detail the tuple n-gram translation model, while section 3 introduces the additional features used in the system. Section 4 provides a brief overview of the decoding tool and search strategy used. Next, sections 5 and 6 report and discuss results on IWSLT’05 Chineseto-English and Arabic-to-English tracks, respectively. Finally, Section 7 concludes and outlines future research lines. 2. The Tuple N-gram translation model The tuple N-gram translation model is a language model of a particular language composed by bilingual unit"
2005.iwslt-1.25,N04-1033,0,0.0439184,"referred to as tuples. This model approximates the joint probability between source and target languages by using N-grams as described by the following equation: p(sJ1 , tI1 ) = · · · = K Y i=1 p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (2) (3) where (s, t)i refers to the ith tuple of a given bilingual sentence pair, which is segmented into K tuples. It is important to notice that, since both languages are linked up in tuples, the context information provided by this translation model is bilingual. Tuples are extracted from a word-to-word aligned corpus according to the following constraints [10]: • a monotonic segmentation of each bilingual sentence pair is produced • no word inside the tuple is aligned to words outside the tuple • no smaller tuples can be extracted without violating the previous constraints As a consequence of these constraints, only one segmentation is possible for a given parallel sentence pair and a word alignment. Usually, automatic word-to-word alignments are generated in both source-to-target and target-to-source directions by using GIZA++ [11], and tuples are usually extracted from the union set of alignments. However, in section 5 results are also reported w"
2005.iwslt-1.25,P00-1056,0,0.157829,"on model is bilingual. Tuples are extracted from a word-to-word aligned corpus according to the following constraints [10]: • a monotonic segmentation of each bilingual sentence pair is produced • no word inside the tuple is aligned to words outside the tuple • no smaller tuples can be extracted without violating the previous constraints As a consequence of these constraints, only one segmentation is possible for a given parallel sentence pair and a word alignment. Usually, automatic word-to-word alignments are generated in both source-to-target and target-to-source directions by using GIZA++ [11], and tuples are usually extracted from the union set of alignments. However, in section 5 results are also reported when extracting tuples with the alignment from sourceto-target direction. Figure 1 presents a simple example illustrating the tuple extraction process. I would like NULL NULL quisiera ir t1 t2 t3 to have a comer un helado t4 a t5 huge ice−cream gigante t6 Figure 1: Example of tuple extraction from an aligned bilingual sentence pair. Once tuples have been extracted, the tuple vocabulary can be pruned by using histogram counts, thus keeping the N most frequent tuples sharing the s"
2005.iwslt-1.25,2005.mtsummit-papers.37,1,0.781011,"uentiality contraint may lead to an unpractical tuple length and excessive amount of embedded words. In this case, it is more reasonable to allow for a certain reordering in the training data. This means that the tuples are broken into smaller tuples, and these are sequenced in the order of the target words. In order not to lose the information on the correct order, the decoder performs then a reordered search, which is guided by the N-gram model of the unfolded tuples and the additional feature models. On the other hand, the tuple unfolding process highly reduces the effect of embedded words [12]. Figure 2 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual N-gram language model with reordered source words. Usually, this feature function is accompanied by a word penalty model. This model introduces a sentence length penalty in order to compensate the system’s preference for short target sentences, caused by the presence of the previous target language model. This penalization depends on the total number of words contained in the partial translation hypothesis, and it is computed as follows: pW P (tk ) = exp(n"
2005.iwslt-1.25,2005.iwslt-1.23,1,0.865304,"nments are used. In the case of the backward lexicon model, GIZA++ target-to-source alignments are used instead. 4. N-gram based Decoding • a target-to-source lexicon model 3.1. Target language model The first of these feature functions is a standard target language model, estimated as an N-gram over the target words, as expressed by this equation: k Y J X I Y 1 p(tin |sjn ) (I + 1)J j=1 i=0 For decoding given the combination of models presented above, we used MARIE, a decoder implemeting a beam search strategy with distortion (or reordering) capabilities developed at the TALP Research Center [13]. For efficient pruning of the search space, several pruning techniques are used, such as: (5) • Threshold pruning: Hypotheses with lower scores than a certain threshold are eliminated. where tk refers to the partial translation hypothesis and wn to the nth word in it. Although this model could be trained from a larger monolingual data set, this has not been done for IWSLT’05 experiments, which use as target text the same amount of data used as parallel text. As with the tuple translation model, the SRI Language Modeling toolkit was used. • Histogram pruning: Only the K-best ranked hypotheses"
2005.iwslt-1.25,2002.tmi-tutorials.2,0,0.0510661,"the target language is a possible translation of a given sentence s in the source language, and the main difference between two translation hypotheses is a probability assigned to each, which is to be learned from a bilingual corpus. The first SMT systems were based on the noisy channel approach on a word-based basis, modeling the translation of a target language sentence t given a source language sentence t as a translation model probability p(s|t) times a target language model probability p(t) [1]. Recently, word-based translation models have been replaced by phrase-based translation models [2, 3], which are estimated from aligned bilingual corpora by using relative frequencies. On the other hand, according to the maximum entropy framework [4], we can define the translation hypothesis t given a source sentence s, as the target sentence maximizing a log-linear combination of feature functions, as described in the following equation: tˆI1 = arg max tI1 ( M X λm hm (sJ1 , tI1 ) m=1 ) (1) where λm correspond to the weighting coefficients of the log-linear combination, and the feature functions hm (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, t"
2005.iwslt-1.25,N03-1017,0,0.0257197,"the target language is a possible translation of a given sentence s in the source language, and the main difference between two translation hypotheses is a probability assigned to each, which is to be learned from a bilingual corpus. The first SMT systems were based on the noisy channel approach on a word-based basis, modeling the translation of a target language sentence t given a source language sentence t as a translation model probability p(s|t) times a target language model probability p(t) [1]. Recently, word-based translation models have been replaced by phrase-based translation models [2, 3], which are estimated from aligned bilingual corpora by using relative frequencies. On the other hand, according to the maximum entropy framework [4], we can define the translation hypothesis t given a source sentence s, as the target sentence maximizing a log-linear combination of feature functions, as described in the following equation: tˆI1 = arg max tI1 ( M X λm hm (sJ1 , tI1 ) m=1 ) (1) where λm correspond to the weighting coefficients of the log-linear combination, and the feature functions hm (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, t"
2005.iwslt-1.25,P96-1041,0,0.0830768,"), in order to separate articles from words. Note that this process is neither guided by tagging information (it does not use any tagging software) nor complete (several other Arabic particles are usually attached to words). However, it already produces a significative vocabulary reduction, leading to improved performance. During word alignment, IBM model 1 tables are used directly to compute the lexicon feature. Finally, in order to learn the target and the tuples language models we used SRILM [14]. All models were learnt using interpolation of higher and lower order n-grams with Knesser-Ney [15] smoothing. 5.3. Development work Several configurations were tested on the development set optimizing BLEU, namely baseline and three alternatives. Results are shown in table 3. The baseline configuration system is built using: • The union alignment [11] to extract unfolded tuples and the intersection to solve embedded words. • All source-nulled tuples are linked the the target word of the next tuple. • The order of the target and the translation Ngram language models is set to 4 and 3, respectively. • The reordering parameters of the decoder are fixed to m = 5 and j = 3 for the Chinese-to-En"
2005.iwslt-1.25,J96-1002,0,0.0198229,"s a probability assigned to each, which is to be learned from a bilingual corpus. The first SMT systems were based on the noisy channel approach on a word-based basis, modeling the translation of a target language sentence t given a source language sentence t as a translation model probability p(s|t) times a target language model probability p(t) [1]. Recently, word-based translation models have been replaced by phrase-based translation models [2, 3], which are estimated from aligned bilingual corpora by using relative frequencies. On the other hand, according to the maximum entropy framework [4], we can define the translation hypothesis t given a source sentence s, as the target sentence maximizing a log-linear combination of feature functions, as described in the following equation: tˆI1 = arg max tI1 ( M X λm hm (sJ1 , tI1 ) m=1 ) (1) where λm correspond to the weighting coefficients of the log-linear combination, and the feature functions hm (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, the translation system described in this paper implements a log-linear combination of one translation model and four additional feature models. In con"
2005.iwslt-1.25,W05-0823,1,0.827145,"m (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, the translation system described in this paper implements a log-linear combination of one translation model and four additional feature models. In contrast with standard phrase-based approaches, our translation model is expressed in tuples as bilingual units. Given a word alignment, tuples define a unique and monotonic segmentation of each bilingual sentence, building up a much smaller set of units than with phrases and allowing N-gram estimation to account for the history of the translation process [5, 6]. This approach has its origins in SMT by using finite state transducers [7, 8, 9]. The organization of the paper is as follows. Section 2 describes in detail the tuple n-gram translation model, while section 3 introduces the additional features used in the system. Section 4 provides a brief overview of the decoding tool and search strategy used. Next, sections 5 and 6 report and discuss results on IWSLT’05 Chineseto-English and Arabic-to-English tracks, respectively. Finally, Section 7 concludes and outlines future research lines. 2. The Tuple N-gram translation model The tuple N-gram transla"
2005.iwslt-1.25,2005.iwslt-1.24,0,0.0195904,"s are correlated for both dev and test sets in the zh2en task, both improving in the primary run. However, they are incorrelated for both dev and test sets in the ar2en task. 6. Discussion When studying the test results, we can note that the Chinese test set seems to be ’easier’ to translate than the development (obtaining higher scores), whereas the effect is opposite in the case of Arabic. This behaviour could easily be explained by the nature of the data. However, when comparing the two TALP systems which competed in the same tracks and under the same conditions (TALP-Ngram and TALP-Phrase [17]), a surprisingly different behaviour between development and test can be found. Regarding development results, the TALPNgram system improves the performance of the TALPPhrase system (table 6) in the Chinese-to-English task (0.384 &gt; 0.373), while it achieves the same score in the Arabic-to-English task (0.573 ≈ 0.572), both measured in BLEU. However, regarding the test set, the TALPNgram system is clearly beaten by the TALP-Phrase system in both tasks (0.444 &lt; 0.452 in Chinese-to-English, and 0.533 &lt; 0.573 in Arabic-to-English). Experiments have been conducted in order to find out the reason e"
2005.mtsummit-papers.36,N01-1018,0,0.0681459,"ation of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper implements a translation model based on the ﬁnite-state perspective, (de Gispert and Mari˜ no, 2002) and (de Gispert et al., 2004), which is used along with a log-linear combination of four additional feature functions (Crego et al., 2005). The implemented translation model, which is referred to as tuple n-gram model, diﬀers from the well known phrase-model approach (Koehn et al., 2003) in two basic issues."
2005.mtsummit-papers.36,J96-1002,0,0.115652,"entioned in the introduction, the translation system presented here implements a log-linear combination of feature functions 1 In the present version of the system, target words aligned to NULL are always attached to the following word. Further work in this area is proposed in the last section. along with the tuple n-gram model. This section describes the log-linear model and each of the four speciﬁc feature functions that are used. Finally, a brief description of the customized decoding tool that is used is presented. 3.1 Log-Linear Model Framework According to the maximum entropy framework (Berger et al., 1996), the corresponding translation hypothesis T , for a given source sentence S, is deﬁned by the target sentence that maximizes a log-linear combination of feature functions hi (S, T ), as described in the following equation:  argmax λi hi (S, T ) (2) T i where the λi ’s constitute the weighting coeﬃcients of the log-linear combination and the feature function hi (S, T ) corresponds to a logarithmic scaling of the ith -model probabilities. These weights are computed via an optimization procedure which maximizes the translation BLEU (Papineni et al., 2002) over a given development set. This opti"
2005.mtsummit-papers.36,J90-2002,0,0.523079,"Missing"
2005.mtsummit-papers.36,J93-2003,0,0.0465928,"ementation of translation algorithms based on statistical methods (Brown et al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is l"
2005.mtsummit-papers.36,N04-1033,0,0.157603,"Missing"
2005.mtsummit-papers.36,2005.iwslt-1.23,1,0.857305,"Missing"
2005.mtsummit-papers.36,2004.iwslt-evaluation.14,1,0.687498,"Missing"
2005.mtsummit-papers.36,P05-2012,1,0.78658,"Missing"
2005.mtsummit-papers.36,N03-1017,0,0.067546,"The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper implements a translation"
2005.mtsummit-papers.36,P02-1038,0,0.221853,"otivated by the development of computer resources needed to allow the implementation of translation algorithms based on statistical methods (Brown et al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of"
2005.mtsummit-papers.36,N04-1021,0,0.0290838,"length penalization in order to compensate the system preference for short target sentences caused by the presence of the previous target language model. This penalization depends on the total number of words contained in the partial translation hypothesis, and it is computed as follows: wp(Tk ) = exp(number of words in Tk ) (4) where, again, Tk refers to the partial translation hypothesis. The fourth and ﬁfth feature functions correspond to a forward and backward lexicon models. These models provide IBM 1 translation probabilities for each tuple based on the IBM 1 lexical parameters p(t|s) (Och et al., 2004). These lexicon models are computed according to the following equation: pIBM 1 ((t, s)n ) = I J   1 p(tin |sjn ) (5) (I + 1)J j=1 i=0 (3) where sjn and tin are the j th and ith words in the source and target sides of tuple (t, s)n , being J and I the corresponding total number words in each side of it. For computing the forward lexicon model, IBM model 1 lexical parameters from GIZA++ source-to-target alignments are used. In the case of the backward lexicon model, GIZA++ target-to-source alignments are used instead. where Tk refers to the partial translation hypothesis and wn to the nth wor"
2005.mtsummit-papers.36,P02-1040,0,0.115781,"cording to the maximum entropy framework (Berger et al., 1996), the corresponding translation hypothesis T , for a given source sentence S, is deﬁned by the target sentence that maximizes a log-linear combination of feature functions hi (S, T ), as described in the following equation:  argmax λi hi (S, T ) (2) T i where the λi ’s constitute the weighting coeﬃcients of the log-linear combination and the feature function hi (S, T ) corresponds to a logarithmic scaling of the ith -model probabilities. These weights are computed via an optimization procedure which maximizes the translation BLEU (Papineni et al., 2002) over a given development set. This optimization is performed by using an in-house developed optimization algorithm, which is based on a simplex method (Press et al., 2002). 3.2 Implemented Feature Functions The proposed translation system implements a total of ﬁve feature functions: • tuple 3-gram model, • target language model, • word penalty model, • source-to-target lexicon model, and • target-to-source lexicon model. The ﬁrst of these functions is the tuple 3-gram model, which was already described in the previous section. The second feature function implemented is a target language model"
2005.mtsummit-papers.36,2002.tmi-tutorials.2,0,0.0433398,"al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper"
2005.mtsummit-papers.36,P00-1056,0,\N,Missing
2005.mtsummit-papers.37,J96-1002,0,0.0120462,"rce sentence f1J is transformed into (or generates) a target sentence eI1 by means of a stochastic process. The translation of a source sentence f1J can be formulated as the search of the target sentence eI1 that maximizes the conditional probability p(eI1 |f1J ), which can be rewritten using the Bayes rule as:  arg max eI1 p(f1J |eI1 ) · p(eI1 )  (1) Alternatively to this classical source channel approach, the posterior probability p(eI1 |f1J ) can be modeled directly as a log-linear combination of feature models (Och and Ney, H., 2002), based on the maximum entropy framework, as shown in (Berger et al., 1996). This simpliﬁes the introduction of several additional models explaining the translation process, as the search becomes:  arg max{exp( eI1 λi hi (e, f ))} (2) i where the feature functions hi are the system models (translation model, language model, reordering model, ...), and the λi weights are typically optimized to maximize a scoring function on a development set. In this work a combination of 4 feature models is used, which include: • a translation Ngram tuple-based model Figure 1: Tuples extraction from a pair of word aligned source sentences. 2.1 Translation model As for the Translatio"
2005.mtsummit-papers.37,N04-1033,0,0.160951,"Missing"
2005.mtsummit-papers.37,2005.iwslt-1.23,1,0.8677,"Missing"
2005.mtsummit-papers.37,2004.iwslt-evaluation.14,1,0.855497,"Missing"
2005.mtsummit-papers.37,koen-2004-pharaoh,0,0.0499024,"means that the target sentence is generated by translating parts of the source sentence in a non-sequential fashion. However, this search can cause a combinatory explosion of the search graph if arbitrary reorderings are allowed, being an NP-hard problem (Knight, 1999). This issue has been tackled in previous work for a phrase-based SMT approach, typically reducing the combinatory explosion of the search space by using a distortion model that penalizes the longest reorderings, which are only allowed if well supported by the other feature models involved in the search (Och and Ney, H., 2004), (Koehn, 2004). In this paper we deal with the question of reordering for an Ngram-based SMT approach with two complementary strategies, namely reordered search and tuple unfolding. These strategies interact to improve translation quality in a Chinese to English task. On the one hand, we introduce reordering capabilities into an Ngram-based decoder. The decoder then performs a reordered search over the source sentence, and combines a translation tuples Ngram model, a target language model, a word penalty and a word distance model. Interestingly, even though the translation units are learnt sequentially, its"
2005.mtsummit-papers.37,P00-1056,0,0.333674,"Missing"
2005.mtsummit-papers.37,P02-1038,0,0.132368,"Missing"
2005.mtsummit-papers.37,J04-4002,0,0.160354,"Missing"
2005.mtsummit-papers.37,2001.mtsummit-papers.68,0,0.0211217,"of the whole training corpus, and reﬁned the links by the union of both alignment directions (Och and Ney, H., 2004). Afterwards we segmented the bilingual sentence pairs of the training set, extracting translation units (tuples) using the extract-tuples method described in (Crego et al., 2004). To train the Ngram models, we used the SRILM toolkit (Stolcke, 2002). The type of discounting algorithm used was the modiﬁed 1 Kneser-Ney combining higher and lower order estimates via interpolation. The weights λi for the log-linear combination of models were set in order to minimize the BLEU score (Papineni et al., 2001) on the development set, using the simplex (Nelder and Mead, 1965) algorithm. All the experiments were performed on a Pentium IV (Xeon 3.06GHz), with 4Gb of RAM memory. www.slt.atr.jp/IWSLT2004 287 5.3 Results Conﬁg. baseline tpl.mon tpl.reo utpl.mon utpl.reo BLEU 28.85 33.14 36.33 33.16 37.82 mWER 53.42 51.5 49.68 50.16 47.31 mPER 42.89 41.53 41.41 41.25 39.9 time 11 333 12 354 Table 3: Translation quality (using BLEU, mWER and mPER) and eﬃciency (measured in decoding time) results. The ﬁrst row shows the results of the baseline TALP system. Table 3 shows the results in BLEU, mWER and mPER, a"
2006.amta-papers.4,J96-1002,0,0.152165,"problem is classified NP-complete (Knight, 1999), while polynomial time search algorithms can be obtained under monotone conditions. The first SMT systems introducing reordering capabilities were founded on the brute force of computers, aiming at finding the best hypothesis through traversing a fully reordered graph (the whole permutations of source-side words are allowed in the search). This approach is computationally very expensive, even for very short input sentences. Therefore, different distance-based reordering constraints must be used to make the search feasible: ITG (Wu, 1996), IBM (Berger et al., 1996), Local (Kanthak et al., 2005), MaxJumps (Crego et al., 2005), etc. The use of these constraints implies a necessary balance between translation accuracy and efficiency. Typically, a distance-based reordering model is used in the search to penalize longer reorderings, only allowed when well supported by the rest of models. Obviously, this model does not follow any property of language. Lexicalized reordering models, which use distance of words seen in train to score reorderings in search, (Koehn et al., 2005) , (Kumar and Byrne, 2005) have also been introduced. A main criticism to this brute f"
2006.amta-papers.4,A00-1031,0,0.033872,"in both sides of an instance (dif < 4). Table 1: TC-Star English-Spanish Parallel corpus statistics. pute the alignments, five iterations for models IBM1 and HMM, and three iterations for models IBM3 and IBM4, were performed. Then, a tuple set for each translation direction was extracted from the union set of alignments. The resulting tuple vocabularies were pruned considering the N best translations for each tuple source-side (N = 30 for the English-to-Spanish and N = 20 for the Spanish-to-English). The English side of the training corpus was POS tagged using the freely available TNT tagger (Brants, 2000), for the Spanish side we used the freely available Freeling (Carreras et al., 2004). Only the first two characters of each tag were used for the Spanish side, aiming at achieving a higher level of generalization. We used the SRI Language modelling toolkit (Stolcke, 2002) to compute the three Ngram language models, using respectively 4, 5 and 5 as ngram orders for the translation, target and tagged target models. Once the models were computed, sets of optimal log-linear coefficients were estimated for each translation direction and system configuration using an in-house implementation of the w"
2006.amta-papers.4,carreras-etal-2004-freeling,0,0.0240478,"Missing"
2006.amta-papers.4,P05-1033,0,0.0121688,"main criticism to this brute force approach is that it does not make use of any linguistic information, while in linguistic theory, reorderings between linguistic phrases in different language pairs are well described. Lately, some SMT systems have introduced linguistic information in order to tackle the reordering 29 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 29-36, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas problem: • Reordering encoded within translation units in form of hierarchical units (Chiang, 2005), or phrases with gaps (Simard et al., 2005). • Word order monotonization (in train and/or test). Consisting of learning reorderings into the source side to achieve a similar word order to that of the target side (Collins et al., 2005), (Xia and McCord, 2004). We found specially interesting the work in (Xia and McCord, 2004), where reorderings are applied following a set of patterns which are automatically learned using lexical, syntactical and morphological information (words, parse trees, and POS tags). In test, a monotone search is applied after reordering the source words using the learnt"
2006.amta-papers.4,P05-1066,0,0.0703591,"some SMT systems have introduced linguistic information in order to tackle the reordering 29 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 29-36, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas problem: • Reordering encoded within translation units in form of hierarchical units (Chiang, 2005), or phrases with gaps (Simard et al., 2005). • Word order monotonization (in train and/or test). Consisting of learning reorderings into the source side to achieve a similar word order to that of the target side (Collins et al., 2005), (Xia and McCord, 2004). We found specially interesting the work in (Xia and McCord, 2004), where reorderings are applied following a set of patterns which are automatically learned using lexical, syntactical and morphological information (words, parse trees, and POS tags). In test, a monotone search is applied after reordering the source words using the learnt patterns. In this work we follow a similar strategy to learn reordering patterns but aiming at reducing the search graph. Our goal is double. On the one hand we add some linguistic information to the problem of guessing which reorderin"
2006.amta-papers.4,W05-0831,0,0.0449427,"plete (Knight, 1999), while polynomial time search algorithms can be obtained under monotone conditions. The first SMT systems introducing reordering capabilities were founded on the brute force of computers, aiming at finding the best hypothesis through traversing a fully reordered graph (the whole permutations of source-side words are allowed in the search). This approach is computationally very expensive, even for very short input sentences. Therefore, different distance-based reordering constraints must be used to make the search feasible: ITG (Wu, 1996), IBM (Berger et al., 1996), Local (Kanthak et al., 2005), MaxJumps (Crego et al., 2005), etc. The use of these constraints implies a necessary balance between translation accuracy and efficiency. Typically, a distance-based reordering model is used in the search to penalize longer reorderings, only allowed when well supported by the rest of models. Obviously, this model does not follow any property of language. Lexicalized reordering models, which use distance of words seen in train to score reorderings in search, (Koehn et al., 2005) , (Kumar and Byrne, 2005) have also been introduced. A main criticism to this brute force approach is that it does"
2006.amta-papers.4,J99-4005,0,0.0800766,"-toSpanish). Results are presented regarding translation accuracy (using human and automatic evaluations) and computational efficiency, showing significant improvements in translation quality for both translation directions at a very low computational cost. 1 Introduction In statistical machine translation, the use of reordering strategies allows for an important improvement in translation accuracy, specially when translating between language pairs with high disparity in word order. On the other hand, when arbitrary word reorderings are permitted, the search problem is classified NP-complete (Knight, 1999), while polynomial time search algorithms can be obtained under monotone conditions. The first SMT systems introducing reordering capabilities were founded on the brute force of computers, aiming at finding the best hypothesis through traversing a fully reordered graph (the whole permutations of source-side words are allowed in the search). This approach is computationally very expensive, even for very short input sentences. Therefore, different distance-based reordering constraints must be used to make the search feasible: ITG (Wu, 1996), IBM (Berger et al., 1996), Local (Kanthak et al., 2005"
2006.amta-papers.4,2005.iwslt-1.8,0,0.0223291,"eordering constraints must be used to make the search feasible: ITG (Wu, 1996), IBM (Berger et al., 1996), Local (Kanthak et al., 2005), MaxJumps (Crego et al., 2005), etc. The use of these constraints implies a necessary balance between translation accuracy and efficiency. Typically, a distance-based reordering model is used in the search to penalize longer reorderings, only allowed when well supported by the rest of models. Obviously, this model does not follow any property of language. Lexicalized reordering models, which use distance of words seen in train to score reorderings in search, (Koehn et al., 2005) , (Kumar and Byrne, 2005) have also been introduced. A main criticism to this brute force approach is that it does not make use of any linguistic information, while in linguistic theory, reorderings between linguistic phrases in different language pairs are well described. Lately, some SMT systems have introduced linguistic information in order to tackle the reordering 29 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 29-36, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas problem: • Reordering encoded w"
2006.amta-papers.4,H05-1021,0,0.0187456,"ust be used to make the search feasible: ITG (Wu, 1996), IBM (Berger et al., 1996), Local (Kanthak et al., 2005), MaxJumps (Crego et al., 2005), etc. The use of these constraints implies a necessary balance between translation accuracy and efficiency. Typically, a distance-based reordering model is used in the search to penalize longer reorderings, only allowed when well supported by the rest of models. Obviously, this model does not follow any property of language. Lexicalized reordering models, which use distance of words seen in train to score reorderings in search, (Koehn et al., 2005) , (Kumar and Byrne, 2005) have also been introduced. A main criticism to this brute force approach is that it does not make use of any linguistic information, while in linguistic theory, reorderings between linguistic phrases in different language pairs are well described. Lately, some SMT systems have introduced linguistic information in order to tackle the reordering 29 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 29-36, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas problem: • Reordering encoded within translation units in"
2006.amta-papers.4,2005.mtsummit-papers.36,1,0.899477,"Missing"
2006.amta-papers.4,2005.eamt-1.25,0,0.0445366,". In test, a monotone search is applied after reordering the source words using the learnt patterns. In this work we follow a similar strategy to learn reordering patterns but aiming at reducing the search graph. Our goal is double. On the one hand we add some linguistic information to the problem of guessing which reorderings must be applied (achieving generalization power through using POS tags instead of words). On the other hand, the final decision about reordering is taken in decoding time, when all the information is available (not just reordering patterns but the whole SMT models). In (Matusov et al., 2005) a similar work can be found, where search graphs are restricted without linguistic motivation but using monotonic sequences seen in training. The paper is organized as follows. In section 2 we review the translation system used in this work. Section 3 introduces the reordering framework proposed, giving details of the method used to extract reordering patterns, and how reorderings are supplied to the decoder in form of a reordering graph. Section 4 presents the experiments conducted to test the efectiveness of using the new reordering framework. Finally, Conclusion and further work are outlin"
2006.amta-papers.4,H05-1095,0,0.0184552,"pproach is that it does not make use of any linguistic information, while in linguistic theory, reorderings between linguistic phrases in different language pairs are well described. Lately, some SMT systems have introduced linguistic information in order to tackle the reordering 29 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 29-36, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas problem: • Reordering encoded within translation units in form of hierarchical units (Chiang, 2005), or phrases with gaps (Simard et al., 2005). • Word order monotonization (in train and/or test). Consisting of learning reorderings into the source side to achieve a similar word order to that of the target side (Collins et al., 2005), (Xia and McCord, 2004). We found specially interesting the work in (Xia and McCord, 2004), where reorderings are applied following a set of patterns which are automatically learned using lexical, syntactical and morphological information (words, parse trees, and POS tags). In test, a monotone search is applied after reordering the source words using the learnt patterns. In this work we follow a similar s"
2006.amta-papers.4,P96-1021,0,0.0901638,"tted, the search problem is classified NP-complete (Knight, 1999), while polynomial time search algorithms can be obtained under monotone conditions. The first SMT systems introducing reordering capabilities were founded on the brute force of computers, aiming at finding the best hypothesis through traversing a fully reordered graph (the whole permutations of source-side words are allowed in the search). This approach is computationally very expensive, even for very short input sentences. Therefore, different distance-based reordering constraints must be used to make the search feasible: ITG (Wu, 1996), IBM (Berger et al., 1996), Local (Kanthak et al., 2005), MaxJumps (Crego et al., 2005), etc. The use of these constraints implies a necessary balance between translation accuracy and efficiency. Typically, a distance-based reordering model is used in the search to penalize longer reorderings, only allowed when well supported by the rest of models. Obviously, this model does not follow any property of language. Lexicalized reordering models, which use distance of words seen in train to score reorderings in search, (Koehn et al., 2005) , (Kumar and Byrne, 2005) have also been introduced. A mai"
2006.amta-papers.4,C04-1073,0,0.109977,"ntroduced linguistic information in order to tackle the reordering 29 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 29-36, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas problem: • Reordering encoded within translation units in form of hierarchical units (Chiang, 2005), or phrases with gaps (Simard et al., 2005). • Word order monotonization (in train and/or test). Consisting of learning reorderings into the source side to achieve a similar word order to that of the target side (Collins et al., 2005), (Xia and McCord, 2004). We found specially interesting the work in (Xia and McCord, 2004), where reorderings are applied following a set of patterns which are automatically learned using lexical, syntactical and morphological information (words, parse trees, and POS tags). In test, a monotone search is applied after reordering the source words using the learnt patterns. In this work we follow a similar strategy to learn reordering patterns but aiming at reducing the search graph. Our goal is double. On the one hand we add some linguistic information to the problem of guessing which reorderings must be applied (achi"
2006.iwslt-evaluation.17,W05-0820,0,0.0116848,", namely from Arabic, Chinese, Italian and Japanese into English for the open data track, thoroughly explaining all language-related preprocessing and optimization schemes. 1. Introduction Rooted in the Finite-State Transducers approach to SMT [1, 2] and estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternative to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3]. This is specially true when dealing with pairs of languages with a relatively similar word order [4, 5]. Given the language pairs involved in this year’s evaluation, efforts have been focused on improving the word reordering strategies for Ngram-based SMT. Specifically, a novel reordering strategy based on extending the search graph with automatically-extracted reordering patterns is explored. Results are very promising while keeping computational expenses at a similar level of monotone search. Additionally, a novel tuple segmentation strategy based on the entropy of Part-Of-Speech distributions was used with slight improvements in model estimation. This paper is organized as follows. Section 2"
2006.iwslt-evaluation.17,W06-3114,0,0.0159274,", namely from Arabic, Chinese, Italian and Japanese into English for the open data track, thoroughly explaining all language-related preprocessing and optimization schemes. 1. Introduction Rooted in the Finite-State Transducers approach to SMT [1, 2] and estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternative to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3]. This is specially true when dealing with pairs of languages with a relatively similar word order [4, 5]. Given the language pairs involved in this year’s evaluation, efforts have been focused on improving the word reordering strategies for Ngram-based SMT. Specifically, a novel reordering strategy based on extending the search graph with automatically-extracted reordering patterns is explored. Results are very promising while keeping computational expenses at a similar level of monotone search. Additionally, a novel tuple segmentation strategy based on the entropy of Part-Of-Speech distributions was used with slight improvements in model estimation. This paper is organized as follows. Section 2"
2006.iwslt-evaluation.17,2005.mtsummit-papers.36,1,0.472143,"g patterns. Section 4 focuses on tuple segmentation strategies, and contrasts the criterion on IBM model 1 probabilities from 2005 with a novel criterion based on Part-Of-Speech entropy distributions. Later on, Section 5 reports on all experiments carried out from Arabic, Chinese, Italian and Japanese into English for IWSLT 2006. Finally, Section 6 sums up the main conclusions from the paper and discusses future research lines. 2. 2005 system review The TALP Ngram-based SMT system performs a log-linear combination of a translation model and additional feature functions (see further details in [6, 7]). In contrast to phrasebased models, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel"
2006.iwslt-evaluation.17,2005.iwslt-1.23,1,0.868702,"l and additional feature functions (see further details in [6, 7]). In contrast to phrasebased models, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints 116 3. Word ordering strategies 2.2. Feature functions As additional feature functions to better guide the translation process, the system incorporates a target language model, a word bonus model and two lexicon models. The target language model is estimated"
2006.iwslt-evaluation.17,N04-1033,0,0.120187,"ated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints 116 3. Word ordering strategies 2.2. Feature functions As additional feature functions to better guide the translation process, the system incorporates a target language model, a word bonus model and two lexicon models. The target language model is estimated as a standard ngram over the target words, as follows: pLM (tk ) ≈ k Y p(wn |wn−N +1 , ..., wn−1 ) (2) n=1 where tk refers to the par"
2006.iwslt-evaluation.17,2005.mtsummit-papers.37,1,0.788658,"n |wn−N +1 , ..., wn−1 ) (2) n=1 where tk refers to the partial hypothesis and wn to the nth word in it. Usually, this feature is accompanied by a word bonus model based on sentence length, compensating the target language model preference for short sentences (in number of target words). This bonus depends on the number of target words in the partial hypothesis, denoted as: pW P (tk ) = exp(number of words in tk ) When dealing with pairs of languages with non-monotonic word order, a certain reordering strategy is required. Apart from that, tuples need to be extracted by an unfolding technique [11]. This means that the tuples are broken into smaller tuples, and these are sequenced in the order of the target words. In order not to lose the information on the correct order, the decoder performs then a reordered search (or a monotone search extended with reordering paths), which is guided by the n-gram model of the unfolded tuples and the additional feature models. Figure 1 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual n-gram language model with reordered source words. (3) where tk refers to the partial hypo"
2006.iwslt-evaluation.17,A00-1031,0,0.0103336,"Language-dependent preprocessing For all language pairs, training sentences were split by using final dots on both sides of the bilingual text (when the number of dots was equal), increasing the number of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. English Table 1: Arabic→English corpus statistics. sent. it en it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronom"
2006.iwslt-evaluation.17,N06-2013,0,0.0357882,"mber of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. English Table 1: Arabic→English corpus statistics. sent. it en it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. The Arabic Treebank tag set used contains 20 different tags. Corpora statistics for all language pairs can be found in 2 Ver"
2006.iwslt-evaluation.17,P05-1071,0,0.0218458,"n it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. The Arabic Treebank tag set used contains 20 different tags. Corpora statistics for all language pairs can be found in 2 Version 119 2.0. Linguistic Data Consortium Catalog: LDC2004L02. 5.2.3. Chinese official Chinese preprocessing included resegmentation and POStagging. These tasks were done by using ICTCLAS [19]. Resultan"
2006.iwslt-evaluation.17,W03-1730,0,0.0274114,"Missing"
2006.iwslt-evaluation.17,2004.iwslt-evaluation.14,1,\N,Missing
2006.iwslt-evaluation.17,E99-1010,0,\N,Missing
2006.iwslt-evaluation.17,J96-1002,0,\N,Missing
2006.iwslt-evaluation.17,W05-0823,1,\N,Missing
2006.iwslt-evaluation.17,N07-2022,1,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.1,0,\N,Missing
2006.iwslt-evaluation.17,J06-4004,1,\N,Missing
2006.iwslt-evaluation.17,N03-1017,0,\N,Missing
2006.iwslt-evaluation.17,J03-1002,0,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-evaluation.1,0,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.24,1,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-papers.2,1,\N,Missing
2006.iwslt-evaluation.17,atserias-etal-2006-freeling,0,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.11,0,\N,Missing
2006.iwslt-evaluation.17,P00-1056,0,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-papers.5,1,\N,Missing
2006.iwslt-evaluation.18,2005.iwslt-1.24,1,0.787344,"TALP phrase-based statistical machine translation system, enriched with the statistical machine reordering technique. We also report the combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report th"
2006.iwslt-evaluation.18,W06-1609,1,0.921033,"e combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to Eng"
2006.iwslt-evaluation.18,W06-3125,1,0.836838,"ional Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to English. 2. Description of the TALP-phrase System 2.1. Phrase-based Model The basic idea of phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and finally compose the target sentence from th"
2006.iwslt-evaluation.18,P02-1038,0,0.0577834,"re consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. 2.2. Feature functions The baseline phrase-based system implements a log-linear combination of four feature functions, which are described as follows. • The translation model is estimated with relative frequencies. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions. (1) The feature functions, hm , and weights, λi , are typically optimized to maximize the scoring function [4]. Two basic issues differentiate the n-gram-based system from the phrase-based system: the bilingual units are extracted from a monotonic segmentation of the training data; the unit probabilities are based on a standard back-off language model rather than directly on relative frequencies. In both systems, the introduction of reordering capabilities is crucial for certain language pairs. This paper is organized as follows. Section 2 describes the TALP-phrase system, with particular emphasis on a new reordering technique: the statistical machine reordering approach. In Section 3, we combine the"
2006.iwslt-evaluation.18,J04-4002,0,0.0302302,"X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to English. 2. Description of the TALP-phrase System 2.1. Phrase-based Model The basic idea of phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and finally compose the target sentence from these phrase translations. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [5]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. 2.2. Feature functions The baseline phrase-based system implements a log-linear combination of four feature functions, which are described as follows. • The translation model is estimated with relative frequencies. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequen"
2006.iwslt-evaluation.18,2005.iwslt-1.6,0,0.0322379,"using the GIZA++ tool [7]. During word alignment, we used 50 classes per language. We aligned both translation directions and combined the two alignments with the union operation. train dev4 dev123 test ASRtest • Word classes (which were used to help the aligner and to perform the SMR process) were determined using “mkcls”, a tool freely-available with GIZA++. • The language model was estimated using the SRILM toolkit [8]. • The decoder was MARIE [9]. • The optimization tool used for computing log-linear weights was based on the simplex method [6]. Following the consensus strategy proposed in [10], the objective function was set to 100 · BLEU + 4 · N IST . 489 500 500 500 voc. 9.7k 9.6k 1,096 909 1,292 1,311 slen. 6.7 7.0 11.2 6.0 11.7 11.6 refs. 1 7 16 7 7 Corpus statistics for all language pairs can be found in Tables 1, 2, 3 and 4, respectively, where number of sentences, running words, vocabulary, sentence length and human references are shown. sent. train dev4 dev123 test ASRtest Experiments were carried out for all tasks of the IWSLT06 evaluation (Zh2En, Jp2En, Ar2En and It2En) using the BTEC Corpus provided for the open data track1 . it en it it it it 24.6k 489 500 500 500 wrds"
2006.iwslt-evaluation.18,A00-1031,0,0.0229282,"while randomly selecting 500 sentences from developments 1, 2 and 3 (around 160 sentences from each) to build an internal test set (dev123). zh en zh zh zh zh 4.4. Language-dependent preprocessing For all language pairs, training sentences were split by using full stops on both sides of the bilingual text (when the number of stops was equal), increasing the number of sentences and reducing their length. Specific preprocessing for each language is detailed in the respective section below. 4.4.1. English English preprocessing includes part-of-speech tagging using the freely-available TnT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly p"
2006.iwslt-evaluation.18,N06-2013,0,0.0411805,"ing their length. Specific preprocessing for each language is detailed in the respective section below. 4.4.1. English English preprocessing includes part-of-speech tagging using the freely-available TnT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. search (with m = 5 and j = 3) for all tasks and for all systems (with or without SMR technique); except for the Italian to E"
2006.iwslt-evaluation.18,P05-1071,0,0.0382995,"nT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. search (with m = 5 and j = 3) for all tasks and for all systems (with or without SMR technique); except for the Italian to English task where a monotonic search was used. The primary system of each task is that which had the best performance in the internal test. In all tasks, the SMR improved the results in the internal te"
2006.iwslt-evaluation.18,W03-1730,0,0.0516051,"Missing"
2006.iwslt-evaluation.18,atserias-etal-2006-freeling,0,0.0138517,"for the internal test set (specially, for the Arabic and Japanese tasks). The higher the number of unknown words, the worse the SMR output and, consequently, the quality of translation. Here, a possible solution would be to predict word classes for unknown words in order to avoid their bad influence in the SMR output. 4.4.3. Chinese Set development test evaluation Chinese preprocessing included re-segmentation and POStagging. These tasks were performed using ICTCLAS [15]. 4.4.4. Italian Italian was POS-tagged and lemmatized using the freelyavailable FreeLing morpho-syntactic analysis package [16]. Additionally, Italian contracted prepositions were separated into preposition + article, for example ’alla’→’a la’, ’degli’→’di gli’ or ’dallo’→’da lo’. 4.4.5. Japanese When dealing with Japanese, one has to come up with new methods for overcoming the absence of delimiters between words. We addressed this issue by word segmentation using the freely available JUMAN tool [17] version 5.1. This tool was also used for POS-tagging of the Japanese text. 4.5. Results In Table 6 we show the results for all the TALP systems that participated in the IWSLT 2006: the TALP-phrase, the TALP-tuple and the"
2006.iwslt-evaluation.18,2004.iwslt-evaluation.8,0,\N,Missing
2007.iwslt-1.26,2006.iwslt-papers.2,1,0.882609,"4]. Efforts have been focused on improving translation according to human evaluation by further developing different stages of the SMT system: alignment and rescoring. As in previous years, we aligned the training corpus using Giza++ software. However, instead of keeping the default parameters, we performed a minimum translation error training procedure to adjust Giza++ smoothing parameters to the task. This procedure had been successful with an alignment system based on discriminative training [5]. For the rescoring we incorporate a neural network language model as previously experienced in [6]. The neural network language model mainly is able to produce a better generalization in the translation system. This paper is organized as follows. Section 2 briefly reviews last year’s system, including tuple definition and extraction, translation model and feature functions, decoding tool and reordering and optimization criterion. Section 3 describes the alignment translation-minimum-error training procedure. Section 4 focuses on rescoring using a neural language model (NNLM). Next, Section 5 reports on all experiments carried out from Arabic and Chinese into English for IWSLT 2007. Finally"
2007.iwslt-1.26,N04-1033,0,0.0623584,"ls, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. In this way, it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(S, T ) = K Y p((˜ s, t˜)k |(˜ s, t˜)k−N +1 , ..., (˜ s, t˜)k−1 ) (1) k=1 where s refers to source, t to target, and (˜ s, t˜)k to the k th tuple of a given bilingual sentence pair segmented in K tuples. 2.2. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints However, when dealing with pairs of languages with nonmonotonic word order, a certain reordering strategy is required to extract more reusable units (less sparse). Hence, we allow the source words to be reordered before extracting translation units from training sentence pairs by following the word-to-word alignments. The unfolding technique is fully described in [10]. Figure 1 shows an ex"
2007.iwslt-1.26,2005.mtsummit-papers.37,1,0.856996,"following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints However, when dealing with pairs of languages with nonmonotonic word order, a certain reordering strategy is required to extract more reusable units (less sparse). Hence, we allow the source words to be reordered before extracting translation units from training sentence pairs by following the word-to-word alignments. The unfolding technique is fully described in [10]. Figure 1 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual n-gram language model with reordered source words. where tn refers to the nth word in the partial translation hypothesis T . Usually, this feature is accompanied by a word bonus model based on sentence length, compensating the target language model preference for short sentences (in number of target words). This bonus depends on the number of target words in the partial hypothesis, denoted as: pW P (T ) = exp(number of words in T ). The third and fourth fe"
2007.iwslt-1.26,2006.iwslt-papers.5,1,0.867108,"ChineseEnglish task, a secondary run was performed with a rescoring module, as described in Sections 4 and 5.3.2. 2.5. Feature Weights Optimization To tune the weight of each feature function in the SMT system, we used the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm [12]. SPSA is a stochastic implementation of the conjugate gradient method which requires only two evaluations of the objective function in each iteration, regardless of the dimension of the optimization problem. It was observed to be more robust than the Downhill Simplex method when tuning SMT coefficients [13]. The SPSA procedure is in the general recursive stochastic approximation form: ˆ k+1 = λ ˆ k − ak g ˆk ) ˆk (λ λ (5) lation tuples (as no word within a tuple can be linked to a word out of it [9]). Starting from the monotonic graph, each sequence of input POS tags fulfilling a source-side rewrite rule implies the addition of a reordering arc (which encodes the reordering detailed in the target-side of the rule). Figure 2 shows how three rewrite rules applied over an input sentence extend the search graph given the reordering patterns that match the source POS tag sequence 1 . ˆ k ) is the esˆ"
2007.iwslt-1.26,N07-2022,1,0.823419,"o phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3, 4]. Efforts have been focused on improving translation according to human evaluation by further developing different stages of the SMT system: alignment and rescoring. As in previous years, we aligned the training corpus using Giza++ software. However, instead of keeping the default parameters, we performed a minimum translation error training procedure to adjust Giza++ smoothing parameters to the task. This procedure had been successful with an alignment system based on discriminative training [5]. For the rescoring we incorporate a neural network language model as previously experienced in [6]. The neural network language model mainly is able to produce a better generalization in the translation system. This paper is organized as follows. Section 2 briefly reviews last year’s system, including tuple definition and extraction, translation model and feature functions, decoding tool and reordering and optimization criterion. Section 3 describes the alignment translation-minimum-error training procedure. Section 4 focuses on rescoring using a neural language model (NNLM). Next, Section 5"
2007.iwslt-1.26,N06-2013,0,0.076004,"fluency and METEOR is well correlated to adequacy [4], we supposed that adding all references was beneficial to monolingual language models but not to the bilingual language model. Table 2: Chinese→English corpus statistics. 5.2. Data Preprocessing For all language pairs, training sentences were split by using final dots on both sides of the bilingual text (when the number of dots was equal), increasing the number of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. Arabic Following a similar approach to that in [16], we used the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available"
2007.iwslt-1.26,W03-1730,0,0.018621,"ts length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. Arabic Following a similar approach to that in [16], we used the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal"
2007.iwslt-1.26,A00-1031,0,0.0277074,"he MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results i"
2007.iwslt-1.26,E99-1010,0,0.0816899,"ion produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results in all cases, and the best combination for the system with pattern-based reordering was the union6 . At the end, the best alignment configuration for our baseline system was obtained with Giza++ software, running respectively 5, 5, 3 and 3 iterations of models 1, HMM, 3 and 4, using English stems and 50"
2007.iwslt-1.26,J03-1002,0,0.00791937,"y available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results in all cases, and the best combination for the system with pattern-based reordering was the union6 . At the end, the best alignment configuration for our baseline system was obtained with Giza++ software, running respectively 5, 5, 3 and 3 iterations of models 1, HMM, 3 and 4, using English stems and 50 classes and taking the union of source-target and target-source alignments. Table 3 show results for the new features of this year’s system. We optimized the foll"
2007.mtsummit-papers.16,J90-2002,0,0.364965,"d on the BTEC corpus (Chinese to English task) Results are presented regarding translation accuracy and computational efficiency, showing significant improvements in translation quality at a reasonable computational cost. 1 Introduction In the statistical machine translation (SMT) community, it is widely accepted the need for structural information to account for mappings between the different language pairs. These mappings offer a greater potential to learn generalizations about relationships between languages than flat-structured models, such as the word-based IBM models of the early 1990s (Brown et al., 1990) or the more recent phrasebased models (Zens et al., 2002; Marcu and Wong, 2002; Koehn et al., 2003), which to date remain widely used. The need for structural information is specially relevant when handling language pairs with very different word order (such as Chinese-English), because the flatstructured models fail to derive generalizations from the training corpus. Several alternatives have been proposed in the recent years to boost the use of syntactic information in SMT systems. They range from those aiming at monotonizing the word order of the considered language pairs by means of a set"
2007.mtsummit-papers.16,P05-1033,0,0.0864727,"ased system), what allows to further improve translation accuracy by avoiding some of the errors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules for a German-English translation task. The second group (syntax-directed) has gained many adepts in the last few years because of the significant improvements made by exploiting the power of synchronous rewriting systems. These systems employ source and/or target dependency (Quirk et al., 2005; Langlais and Gotti, 2006) or constituent trees, which can be formally syntax-based (Chiang, 2005; Watanabe et al., 2006) or linguistically syntaxbased (Yamada and Knight, 2002; Wu, 1997; Marcu et al., 2006). A main criticism to the first group is that it has shown a relatively good performance when tackling language pairs with reduced reordering needs (such as Spanish-English or FrenchEnglish). On the other hand, syntax-directed systems show a main weakness on their poor efficiency results, recently overrided by the apparition of new decoders, which show significant improvements when handling with syntactically divergent language pairs, under large-scale data translation tasks. An exampl"
2007.mtsummit-papers.16,P05-1066,0,0.36496,"Missing"
2007.mtsummit-papers.16,W06-1609,0,0.0535677,"alternatives have been proposed in the recent years to boost the use of syntactic information in SMT systems. They range from those aiming at monotonizing the word order of the considered language pairs by means of a set of linguistically-based reordering patterns (though, reducing the reordering needs in the overall search), to others considering translation as a synchronous parsing process, where reorderings introduced in the overall search are syntactically motivated. Among the first group (word order monotonization) we can find (Xia and McCord, 2004), (Collins et al., 2005), (Costa-juss` a and Fonollosa, 2006) or (Popovic and Ney, 2006). They modify the source language word order before decoding in order to acquire the word order of the target language. Then, the reordered source sentence is sent to a standard phrasebased decoder to be translated under monotonic conditions. In (Crego and Mari˜ no, 2006) the same idea is enhanced by coupling reordering and decoding (using an N -gram-based system), what allows to further improve translation accuracy by avoiding some of the errors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules"
2007.mtsummit-papers.16,P07-2054,1,0.747839,"Missing"
2007.mtsummit-papers.16,2005.iwslt-1.23,1,0.868481,"Missing"
2007.mtsummit-papers.16,2005.mtsummit-papers.37,1,0.897802,"Missing"
2007.mtsummit-papers.16,N03-1017,0,0.0632272,"Missing"
2007.mtsummit-papers.16,W06-3106,0,0.0168258,"6) the same idea is enhanced by coupling reordering and decoding (using an N -gram-based system), what allows to further improve translation accuracy by avoiding some of the errors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules for a German-English translation task. The second group (syntax-directed) has gained many adepts in the last few years because of the significant improvements made by exploiting the power of synchronous rewriting systems. These systems employ source and/or target dependency (Quirk et al., 2005; Langlais and Gotti, 2006) or constituent trees, which can be formally syntax-based (Chiang, 2005; Watanabe et al., 2006) or linguistically syntaxbased (Yamada and Knight, 2002; Wu, 1997; Marcu et al., 2006). A main criticism to the first group is that it has shown a relatively good performance when tackling language pairs with reduced reordering needs (such as Spanish-English or FrenchEnglish). On the other hand, syntax-directed systems show a main weakness on their poor efficiency results, recently overrided by the apparition of new decoders, which show significant improvements when handling with syntactically diverg"
2007.mtsummit-papers.16,W02-1018,0,0.0227728,"translation accuracy and computational efficiency, showing significant improvements in translation quality at a reasonable computational cost. 1 Introduction In the statistical machine translation (SMT) community, it is widely accepted the need for structural information to account for mappings between the different language pairs. These mappings offer a greater potential to learn generalizations about relationships between languages than flat-structured models, such as the word-based IBM models of the early 1990s (Brown et al., 1990) or the more recent phrasebased models (Zens et al., 2002; Marcu and Wong, 2002; Koehn et al., 2003), which to date remain widely used. The need for structural information is specially relevant when handling language pairs with very different word order (such as Chinese-English), because the flatstructured models fail to derive generalizations from the training corpus. Several alternatives have been proposed in the recent years to boost the use of syntactic information in SMT systems. They range from those aiming at monotonizing the word order of the considered language pairs by means of a set of linguistically-based reordering patterns (though, reducing the reordering n"
2007.mtsummit-papers.16,W06-1606,0,0.0452889,"Missing"
2007.mtsummit-papers.16,J06-4004,1,0.904196,"Missing"
2007.mtsummit-papers.16,popovic-ney-2006-pos,0,0.0324834,"posed in the recent years to boost the use of syntactic information in SMT systems. They range from those aiming at monotonizing the word order of the considered language pairs by means of a set of linguistically-based reordering patterns (though, reducing the reordering needs in the overall search), to others considering translation as a synchronous parsing process, where reorderings introduced in the overall search are syntactically motivated. Among the first group (word order monotonization) we can find (Xia and McCord, 2004), (Collins et al., 2005), (Costa-juss` a and Fonollosa, 2006) or (Popovic and Ney, 2006). They modify the source language word order before decoding in order to acquire the word order of the target language. Then, the reordered source sentence is sent to a standard phrasebased decoder to be translated under monotonic conditions. In (Crego and Mari˜ no, 2006) the same idea is enhanced by coupling reordering and decoding (using an N -gram-based system), what allows to further improve translation accuracy by avoiding some of the errors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules for a German-English trans"
2007.mtsummit-papers.16,takezawa-etal-2002-toward,0,0.0183881,"e rule extraction. Each sequence of source words is hypothesized as reordering sequence. Hence, for each sequence, its parse subtree is identified and seeked in the training set of reordering rules (the successively pruned versions of the subtree are also searched). If the subtree (or pruned subtrees) exist in the set of rules, the input graph will be extended with a new reordering path, following the word order indicated in the reordering rule. 4 Experiments In this section we report on the experimental work carried out in this paper. 4.1 Experimental framework We have used the BTEC2 corpus (Takezawa et al., 2002) from Chinese to English. It consists exactly of the corpus used in the IWSLT 2006 evaluation campaign as training and development sets. We have used two disjoint sets of the official Development set to build our Dev and Test sets. Table 1 shows the main statistics of the used data, namely number of sentences, words, vocabulary, average sentence length and number of references for each language. The training data was preprocessed by using standard tools for tokenizing and filtering. Word-to-word alignments were computed using GIZA++3 . The union of both alignment directions was used as startin"
2007.mtsummit-papers.16,P06-1098,0,0.0244065,"what allows to further improve translation accuracy by avoiding some of the errors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules for a German-English translation task. The second group (syntax-directed) has gained many adepts in the last few years because of the significant improvements made by exploiting the power of synchronous rewriting systems. These systems employ source and/or target dependency (Quirk et al., 2005; Langlais and Gotti, 2006) or constituent trees, which can be formally syntax-based (Chiang, 2005; Watanabe et al., 2006) or linguistically syntaxbased (Yamada and Knight, 2002; Wu, 1997; Marcu et al., 2006). A main criticism to the first group is that it has shown a relatively good performance when tackling language pairs with reduced reordering needs (such as Spanish-English or FrenchEnglish). On the other hand, syntax-directed systems show a main weakness on their poor efficiency results, recently overrided by the apparition of new decoders, which show significant improvements when handling with syntactically divergent language pairs, under large-scale data translation tasks. An example of such a system can b"
2007.mtsummit-papers.16,J97-3002,0,0.0794855,"ors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules for a German-English translation task. The second group (syntax-directed) has gained many adepts in the last few years because of the significant improvements made by exploiting the power of synchronous rewriting systems. These systems employ source and/or target dependency (Quirk et al., 2005; Langlais and Gotti, 2006) or constituent trees, which can be formally syntax-based (Chiang, 2005; Watanabe et al., 2006) or linguistically syntaxbased (Yamada and Knight, 2002; Wu, 1997; Marcu et al., 2006). A main criticism to the first group is that it has shown a relatively good performance when tackling language pairs with reduced reordering needs (such as Spanish-English or FrenchEnglish). On the other hand, syntax-directed systems show a main weakness on their poor efficiency results, recently overrided by the apparition of new decoders, which show significant improvements when handling with syntactically divergent language pairs, under large-scale data translation tasks. An example of such a system can be found in (Marcu et al., 2006). Similar to (Crego and Mari˜ no,"
2007.mtsummit-papers.16,C04-1073,0,0.108736,"to derive generalizations from the training corpus. Several alternatives have been proposed in the recent years to boost the use of syntactic information in SMT systems. They range from those aiming at monotonizing the word order of the considered language pairs by means of a set of linguistically-based reordering patterns (though, reducing the reordering needs in the overall search), to others considering translation as a synchronous parsing process, where reorderings introduced in the overall search are syntactically motivated. Among the first group (word order monotonization) we can find (Xia and McCord, 2004), (Collins et al., 2005), (Costa-juss` a and Fonollosa, 2006) or (Popovic and Ney, 2006). They modify the source language word order before decoding in order to acquire the word order of the target language. Then, the reordered source sentence is sent to a standard phrasebased decoder to be translated under monotonic conditions. In (Crego and Mari˜ no, 2006) the same idea is enhanced by coupling reordering and decoding (using an N -gram-based system), what allows to further improve translation accuracy by avoiding some of the errors performed in the monotonization preprocessing step. (Collins"
2007.mtsummit-papers.16,P02-1039,0,0.0267621,"avoiding some of the errors performed in the monotonization preprocessing step. (Collins et al., 2005) proposes a set of hand-crafted reordering rules for a German-English translation task. The second group (syntax-directed) has gained many adepts in the last few years because of the significant improvements made by exploiting the power of synchronous rewriting systems. These systems employ source and/or target dependency (Quirk et al., 2005; Langlais and Gotti, 2006) or constituent trees, which can be formally syntax-based (Chiang, 2005; Watanabe et al., 2006) or linguistically syntaxbased (Yamada and Knight, 2002; Wu, 1997; Marcu et al., 2006). A main criticism to the first group is that it has shown a relatively good performance when tackling language pairs with reduced reordering needs (such as Spanish-English or FrenchEnglish). On the other hand, syntax-directed systems show a main weakness on their poor efficiency results, recently overrided by the apparition of new decoders, which show significant improvements when handling with syntactically divergent language pairs, under large-scale data translation tasks. An example of such a system can be found in (Marcu et al., 2006). Similar to (Crego and"
2007.mtsummit-papers.16,2002.tmi-tutorials.2,0,0.064188,"presented regarding translation accuracy and computational efficiency, showing significant improvements in translation quality at a reasonable computational cost. 1 Introduction In the statistical machine translation (SMT) community, it is widely accepted the need for structural information to account for mappings between the different language pairs. These mappings offer a greater potential to learn generalizations about relationships between languages than flat-structured models, such as the word-based IBM models of the early 1990s (Brown et al., 1990) or the more recent phrasebased models (Zens et al., 2002; Marcu and Wong, 2002; Koehn et al., 2003), which to date remain widely used. The need for structural information is specially relevant when handling language pairs with very different word order (such as Chinese-English), because the flatstructured models fail to derive generalizations from the training corpus. Several alternatives have been proposed in the recent years to boost the use of syntactic information in SMT systems. They range from those aiming at monotonizing the word order of the considered language pairs by means of a set of linguistically-based reordering patterns (though, red"
2008.iwslt-evaluation.17,J03-1002,0,0.00397375,"I1 maximizing a loglinear combination of several feature models [4]: - 116 - ( eˆI1 = arg max eI1 M X ) λm hm (eI1 , f1J ) m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. Proceedings of IWSLT 2008, Hawaii - U.S.A. The N gram-based approach regards translation as a stochastic process maximizing the joint probability p(f, e), leading to a decomposition based on bilingual n-grams, socalled tuples, that are extracted from a word-to-word alignment (performed with GIZA++ tool1 and generated by growdiag-final method [5]). Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [6]: languages capturing bilingual context, as described by the following equation: p(S, T ) = K Y p((˜ s, t˜)k |(˜ s, t˜)k−N +1 , ..., (˜ s, t˜)k−1 ) (1) k=1 where s refers to source, t to target, and (˜ s, t˜)k to the k tuple of a given bilingual sentence pair segmented in K tuples. The bilingual TM actually constitutes an n-gram-based language model (LM) of tuples, which approximates the joint probability between the languages under consideration and can be seen here as a LM, where t"
2008.iwslt-evaluation.17,N04-1033,0,0.0316825,"I1 , f1J ) m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. Proceedings of IWSLT 2008, Hawaii - U.S.A. The N gram-based approach regards translation as a stochastic process maximizing the joint probability p(f, e), leading to a decomposition based on bilingual n-grams, socalled tuples, that are extracted from a word-to-word alignment (performed with GIZA++ tool1 and generated by growdiag-final method [5]). Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [6]: languages capturing bilingual context, as described by the following equation: p(S, T ) = K Y p((˜ s, t˜)k |(˜ s, t˜)k−N +1 , ..., (˜ s, t˜)k−1 ) (1) k=1 where s refers to source, t to target, and (˜ s, t˜)k to the k tuple of a given bilingual sentence pair segmented in K tuples. The bilingual TM actually constitutes an n-gram-based language model (LM) of tuples, which approximates the joint probability between the languages under consideration and can be seen here as a LM, where the language is composed by tuples. th • a monotonic segmentation of each bilingual sentence pair is produced • n"
2008.iwslt-evaluation.17,W06-1609,1,0.710642,"2008, Hawaii - U.S.A. 2.5. Statistical Machine Reordering 3.1. Punctuation restoration The conception of the Statistical Machine Reordering (SMR) stems from the idea of using the powerful techniques developed for SMT and to translate the source language (S) into a reordered source language (S’), which more closely matches the order of the target language. To infer more reorderings, it makes use of word classes. To correctly integrate the SMT and SMR systems, both are concatenated by using a word graph which offers weighted reordering hypotheses to the SMT system. The details are described in [8] and [9]. We decided to embed punctuation restoration in the main translation step. For this purpose we preprocessed the training corpus as follows: 2.6. Translation models interpolation The resulting preprocessed training corpus is used to train a standard SMT system (wi stands for the i-th word). During the post-evaluation period we have implemented a TM interpolation strategy following the ideas proposed in [3], where the authors present a promising technique of target LMs linear interpolation. These findings open the way to involve additional monolingual information into the translation pr"
2008.iwslt-evaluation.17,W07-0721,1,0.843062,"awaii - U.S.A. 2.5. Statistical Machine Reordering 3.1. Punctuation restoration The conception of the Statistical Machine Reordering (SMR) stems from the idea of using the powerful techniques developed for SMT and to translate the source language (S) into a reordered source language (S’), which more closely matches the order of the target language. To infer more reorderings, it makes use of word classes. To correctly integrate the SMT and SMR systems, both are concatenated by using a word graph which offers weighted reordering hypotheses to the SMT system. The details are described in [8] and [9]. We decided to embed punctuation restoration in the main translation step. For this purpose we preprocessed the training corpus as follows: 2.6. Translation models interpolation The resulting preprocessed training corpus is used to train a standard SMT system (wi stands for the i-th word). During the post-evaluation period we have implemented a TM interpolation strategy following the ideas proposed in [3], where the authors present a promising technique of target LMs linear interpolation. These findings open the way to involve additional monolingual information into the translation process, a"
2008.iwslt-evaluation.17,2007.mtsummit-papers.29,0,0.0263997,"iod we have implemented a TM interpolation strategy following the ideas proposed in [3], where the authors present a promising technique of target LMs linear interpolation. These findings open the way to involve additional monolingual information into the translation process, and also gives a motivation to interpolate the translation and reordering tables in a linear way. Due to a small amount of available in-domain data (IWSLT training material), we have used an out-of-domain 130K-line subset from the Arabic News, English Translation of Arabic Treebank and Ummah LDC parallel corpora (VIOLIN) [10] to increase the final translation and reordering tables. Both corpus statistics can be found in table 1. Instead of time-consuming iterative TM reconstruction and using the highest BLEU score as an maximization criteria, we adjust the weights as a function of the lowest perplexity estimated by the corresponding interpolated combination of the target-side LMs and generalize the optimization results on the interpolated translation and reordering models. The word-to-word alignment was obtained from the joint database (IWSLT + VIOLIN). Then, we separately computed the translation and reordering t"
2008.iwslt-evaluation.17,P07-2045,0,0.0159161,"ssion Corpus (BTEC) Arabic to English translation task. The model weights were tuned with the 2006 development corpus (Dev6), containing 489 sentences and 6 reference translations and the 2002 development set (500 sentences and 16 reference translations) was used as an internal test, according to which we take a decision about better or worse system performance. 4.1.1. Arabic data preprocessing In this section we present a phrase-based MT system that was used in the evaluation. This system is based on the well-known MOSES2 toolkit, which is nowadays considered as a state-of-the-art SMT system [11]. The training and weights tuning procedures are explained in details in the above-mentioned publication, as well as, on the MOSES web page: http://www.statmt.org/moses/. 2 www.statmt.org/moses/ We used a similar approach to that shown in [12], namely the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of enclitics: w+, f+, b+, k+, l+, Al+ and pronominal enclitics. The -TAGBIES option produces Bies POS tags on all taggable t"
2008.iwslt-evaluation.17,N06-2013,0,0.0588409,"anslations) was used as an internal test, according to which we take a decision about better or worse system performance. 4.1.1. Arabic data preprocessing In this section we present a phrase-based MT system that was used in the evaluation. This system is based on the well-known MOSES2 toolkit, which is nowadays considered as a state-of-the-art SMT system [11]. The training and weights tuning procedures are explained in details in the above-mentioned publication, as well as, on the MOSES web page: http://www.statmt.org/moses/. 2 www.statmt.org/moses/ We used a similar approach to that shown in [12], namely the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of enclitics: w+, f+, b+, k+, l+, Al+ and pronominal enclitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 3 http://www.slc.atr.jp/IWSLT2008/ - 118 - Proceedings of IWSLT 2008, Hawaii - U.S.A. Sentences Words Average sentence length Vocabulary IWSLT Arabic 24.45 K 170.24 K 6.96 10.89 K English 24.45 K 188.54 K 7.71 6.92 K VIOLIN Arabic 130.5"
2008.iwslt-evaluation.17,2005.iwslt-1.8,0,0.0320169,"n”) outperforms BTEC-only system by 1.8 BLEU points and 1.2 METEOR points for the CRR track and by 2.1 BLEU points and by about 1 METEOR points for the ASR track measured on the official evaluation test set. ”Supplied 2” line stands for the results obtained with the TALPtuples system as described in sub-section 4.1.3. - 119 - • TM(s), direct and inverse phrase/word based TM. • Distortion model, which assigns a cost linear to the reordering distance, while the cost is based on the number of source words which are skipped when translating a new source phrase. • Lexicalized word reordering model [15]. • Word and phrase penalties, which count the number of words and phrases in the target string. • Target-side LM. The TM and reordering model were trained using the standard MOSES tools. Weights of feature functions were tuned by using the optimization tools from the MOSES package. The search operation was accomplished by MOSES decoder. The experiments with the Chinese to English MT were carried out on the BTEC Chinese-English data [16] augmented with HIT-corpus4 , Olympic-corpus5 and PKUcorpus6 from Chinese LDC. 20K BTEC sentence pairs were supplied for the IWSLT 2008 evaluation campaign. HI"
2008.iwslt-evaluation.17,takezawa-etal-2002-toward,0,0.0149266,"ng distance, while the cost is based on the number of source words which are skipped when translating a new source phrase. • Lexicalized word reordering model [15]. • Word and phrase penalties, which count the number of words and phrases in the target string. • Target-side LM. The TM and reordering model were trained using the standard MOSES tools. Weights of feature functions were tuned by using the optimization tools from the MOSES package. The search operation was accomplished by MOSES decoder. The experiments with the Chinese to English MT were carried out on the BTEC Chinese-English data [16] augmented with HIT-corpus4 , Olympic-corpus5 and PKUcorpus6 from Chinese LDC. 20K BTEC sentence pairs were supplied for the IWSLT 2008 evaluation campaign. HIT corpus contains 132K sentence pairs in total, and is known as a multi-source ChineseEnglish parallel corpus; Olympic corpus has 54K bilingual sentences mainly from sport and travelling domains; while PKU-corpus has about 200K parallel phrases and is considered as a domain-balanced corpus. Besides, the English part of the Tanaka corpus7 was used as a complementary training 4 http://mitlab.hit.edu.cn/index.php/resources 5 http://www.chin"
2008.iwslt-evaluation.17,W03-1730,0,0.0212439,"OR)/2 0.6016 0.6055 0.6210 0.5892 0.5320 0.5320 0.5473 0.5296 NIST 8.5253 8.5940 8.8772 8.7421 7.2878 7.2808 7.6113 7.5862 Table 2: Official and post-evaluation results for Arabic-English translation. Sentences Words Vocabulary Chinese 19,972 164K 8,506 IWSLT’08 English Spanish 19,972 19,972 182K 147K 8,301 16,953 All additional data Chinese English 379,065 379,065 4,834K 5,036K 57,055 75,156 Table 3: Corpus used during the Chinese-English training material for the target-side LM. The I2R research group performed word segmentation for the Chinese part using ICTCLAS tools8 developed in the ICT [17]. Table 3 reports the basic statistics of the principal and additional corpora that were used to build the Chinese-toEnglish SMT system. Regarding English-to-Spanish translation, no extra corpora were used. “you ’re”, and negations like “don’t”, “wouldn’t” or “can’t” were split as “do n’t”, “would n’t” and “ca n’t”. The output of this system was performed in accordance with the official evaluation specification, without any postprocessing needed. Table 5 shows the results of the EnglishSpanish system trained with the BTEC corpus. 4.2.1. Chinese-English independent results The union of the BTEC"
2008.iwslt-evaluation.17,N04-1022,0,0.0541414,"and “you’re” were split as “we ’ll” and 8 http://www.nlp.org.cn/project/project.php?proj BLEU NIST METEOR id=6 - 120 - Our primary approach to the pivot task was a system cascade. Using the 50-best list of translation hypotheses generated by the decoder for the Chinese-to-English system, a 4-best list was made for each of the first list instances, totally representing a 200-best of possible Spanish translations for each Chinese sentence. From that 200-best list, which is allowed for repetitions, the single-best translation was computed using a Minimum Bayes Risk (MBR) strategy as described in [18]. We used the MOSES implementation of the MBR algorithm. This strategy of 200-best list rescoring performed better than a single-best list selection for both systems, gaining 2.5 BLEU points in the development set. Proceedings of IWSLT 2008, Hawaii - U.S.A. 4.2.4. Secondary submission As an alternative approach to the system cascade, we followed a different strategy for the secondary submission combining the phrase translation probabilities of the two language pairs (Chinese-English and English-Spanish translations) with the strategy proposed in [19] to obtain the translation probabilities for"
2008.iwslt-evaluation.17,P07-1108,0,0.0348074,"m Bayes Risk (MBR) strategy as described in [18]. We used the MOSES implementation of the MBR algorithm. This strategy of 200-best list rescoring performed better than a single-best list selection for both systems, gaining 2.5 BLEU points in the development set. Proceedings of IWSLT 2008, Hawaii - U.S.A. 4.2.4. Secondary submission As an alternative approach to the system cascade, we followed a different strategy for the secondary submission combining the phrase translation probabilities of the two language pairs (Chinese-English and English-Spanish translations) with the strategy proposed in [19] to obtain the translation probabilities for each Chinese-Spanish phrase. The final phrase probabilities are calculated as followed: φ(fi |ei ) = X φ(fi |pi )φ(pi |ei ) (2) pi where φ(fi |ei ) corresponds to the translation probability of the Chinese phrase fi given the Spanish phrase ei , φ(fi |pi ) stands for the translation probability of the Chinese phrase fi given the English phrase pi and φ(pi |ei ) stands for the translation probability of the English phrase pi given the Spanish phrase ei . It is important to mention that the English and Spanish phrases are lowercased in this system and"
2008.iwslt-evaluation.17,carreras-etal-2004-freeling,0,0.029059,"6 sentences with 16 Spanish references for tuning the system. The basic statistics of this corpus can be seen in table 7. 4.3.1. Data preprocessing The Chinese corpus was not preprocessed before translation: the corpus was tokenized by words and the punctuation marks were separated. Note that the TM, as well as the LM and reordering model, was trained with punctuation marks and the official test set that did not contain this information, therefore it was preprocessed with the hidden-ngram tool to restore it. The Spanish part of the corpus was lowercased and tokenized using the Freeling toolkit[20], an open source tool for language analysis. It splitted the enclitics from the Spanish verbs (d´amelo → da +me +lo) and also generated the POS tags that were lately used to estimate a target-side POS LM and in postprocessing. 4.3.2. Data postprocessing Once the decoding process had finished, the output of the system was still lowercased and splitted with the enclitics and the POS tags were generated. Afterwards, a postprocess including two steps was performed: firstly, the original morphological verbs form was restored using the enclitics and POS tags information; on the next step, the case i"
2008.iwslt-evaluation.17,2007.iwslt-1.1,0,\N,Missing
2008.iwslt-evaluation.17,2006.iwslt-evaluation.1,0,\N,Missing
2009.eamt-1.8,carreras-etal-2004-freeling,0,0.0189299,"Missing"
2009.eamt-1.8,C00-2162,0,0.168791,"anslator with open-source tools as long as a parallel corpus is available. If the languages involved in the translation belong to the same linguistic family, the translation quality can be surprisingly nice. Furthermore, one of the most attractive reasons to build an statistical system instead of an standard rule-based system is the little human effort required. Theoretically, when using SMT, no linguistic knowledge is required. In practice, once the system is built and specially, if the translation quality is high, then the linguistic knowledge becomes necessary to make further improvements (Niessen and Ney, 2000; Popovi´c and Ney, 2004; Popovi´c et al., 2006). In fact, the main question that arose at the beginning of this work was: which are the steps to follow when the intention is to improve a high quality statistical translation? Let’s consider a high quality statistical translation defined as the system which has a BLEU 2 Ngram-based statistical translation system An Ngram-based SMT system regards translation as a stochastic process. In recent systems, such an approach is faced using a general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented"
2009.eamt-1.8,popovic-ney-2004-towards,0,0.0695144,"Missing"
2009.eamt-1.8,W06-3101,1,0.938373,"Missing"
2009.eamt-1.8,popovic-ney-2006-pos,0,\N,Missing
2009.eamt-1.8,J06-4004,1,\N,Missing
2009.eamt-1.8,P03-1021,0,\N,Missing
2010.eamt-1.12,popovic-ney-2006-pos,0,0.0248284,"Missing"
2010.eamt-1.12,W06-3101,1,0.93751,"Missing"
2010.eamt-1.12,2009.eamt-1.8,1,0.756731,"Missing"
2010.eamt-1.12,C00-2162,0,0.0200814,"mpute another probability to the translation units based on the probability of translating word per word of the unit. The probability estimated by lexical models tends to be in some situations less sparse than the probability given directly by the translation model. Many additional feature functions can also be introduced in the SMT framework to improve the translation, like the word or the phrase bonus. Although SMT systems provide, in general, good performance, it has been demonstrated in recent papers that the addition of linguistic information can be highly useful in this kind of systems (Niessen and Ney, 2000; Popovi´c and Ney, 2004; Popovi´c and Ney, 2006; Popovi´c et al., 2006). automatic translation meets their standards. Large amounts of bilingual texts are needed to further develop new systems. N-II3 , developed at the UPC mainly for the Spanish-Catalan pair, is an engine based on an Ngram translation model integrated in an optimized log-linear combination of additional features. Although it is mainly statistical, additional linguistic rules are included in order to solve some errors caused by the statistical translation, such as ambiguity in adjective and possessive pronouns, orthographic er"
2010.eamt-1.12,P03-1021,0,0.0146839,"the overlap in phrases. Thus, given a source string sJ1 = s1 . . . sj . . . sJ to be translated into a target string tI1 = t1 . . . ti . . . tI , the aim is to choose, among all possible target strings, the string with the highest probability: t˜I1 = argmax P (tI1 |sJ1 ) tI1 where I and J are the number of words of the target and source sentence, respectively. The first SMT systems were reformulated using Bayes’ rule. In recent systems, such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och, 2003). This approach leads to maximising a linear combination of feature functions: t˜ = argmax t nP M o m=1 λm hm (t, s) . Given a target sentence and a foreign sentence, the translation model tries to assign a probability that tI1 generates sJ1 . While these probabilities can be estimated by thinking about how each individual word is translated, modern statistical MT is based on the intuition that a better way to compute these probabilities is by considering the behavior of phrases (sequences of words). The intuition of phrase-based statistical MT is to use phrases as well as single words as the"
2010.eamt-1.12,P02-1040,0,0.0928963,"human evaluation based on the expert knowledge about the errors encountered at several linguistic levels: orthographic, morphological, lexical, semantic and syntactic. The results obtained in these experiments show that some linguistic errors could have more influence than other at the time of performing a perceptual evaluation. 1 Introduction One of the aims in the research community is to find accurate evaluation methods that allow analyzing and comparing the performance of these translation systems. The most commonly used evaluation methods are the standard automatic measures such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover and Dorr, 2006) and WER (McCowan, 2004 et al.), as well as the use of human native evaluators that analyze and compare translated sentences according to a general perception of the linguistic quality. In this paper, these evaluation methods are used to evaluate and compare two translation systems based on the statistical approaches in the Catalanto-Spanish language pair: Google Translate and Nc 2010 European Association for Machine Translation. II; this one developed at the Universitat Polit`ecnica de Catalunya (UPC). In addition, a new human evaluation m"
2010.eamt-1.12,popovic-ney-2004-towards,0,0.0153117,"Missing"
2010.eamt-1.12,2006.amta-papers.25,0,0.0482924,"he errors encountered at several linguistic levels: orthographic, morphological, lexical, semantic and syntactic. The results obtained in these experiments show that some linguistic errors could have more influence than other at the time of performing a perceptual evaluation. 1 Introduction One of the aims in the research community is to find accurate evaluation methods that allow analyzing and comparing the performance of these translation systems. The most commonly used evaluation methods are the standard automatic measures such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover and Dorr, 2006) and WER (McCowan, 2004 et al.), as well as the use of human native evaluators that analyze and compare translated sentences according to a general perception of the linguistic quality. In this paper, these evaluation methods are used to evaluate and compare two translation systems based on the statistical approaches in the Catalanto-Spanish language pair: Google Translate and Nc 2010 European Association for Machine Translation. II; this one developed at the Universitat Polit`ecnica de Catalunya (UPC). In addition, a new human evaluation method is applied, based on an expert linguistic evalua"
2010.eamt-1.12,vilar-etal-2006-error,0,0.775854,"Missing"
2010.eamt-1.12,W09-0401,0,\N,Missing
2010.eamt-1.12,J06-4004,1,\N,Missing
2010.iwslt-evaluation.26,2010.eamt-1.17,1,0.915535,"ase is a pair of m source words and n target words. For extraction from a bilingual word aligned training corpus, two additional constraints are considered: similar threshold definition conditions for different associativity measures. Different associativity measures have different scale of values and it is difficult to set threshold manually. Threshold level is kept as low as possible. Higher threshold value makes shorter collocation segments and vice versa. Shorter collocation segments are more confident collocations and we may expect better translation results. Nevertheless, the results of [3] show that longer collocation segments are more preferable. There are many associativity measures that could be used to calculate the associativity values between tokens (a more comprehensive list could be found in [11]). To explore different measures we included the six following metrics: 1. Mutual Information (MI): M I(wi , wi+1 ) = N ∗ f (wi , wi+1 ) f (w1 ) + f (wi+1 ) dice(wi , wi+1 ) = 2 ∗ f (wi , wi+1 ) f (wi ) + f (wi+1 ) 1. the words are consecutive, and, 2. they are consistent with the word alignment matrix. Given the collected phrase pairs, the phrase translation probability distrib"
2010.iwslt-evaluation.26,J04-4002,0,0.0195448,"segmented data into the phrase-based system. As follows, Section 6 shows the experimental details of the system and the experiments performed with the novel technique. Finally, Section 7 presents the conclusions. 2. Related work One of the main problems in the statistical machine translation approach is how to segment the bilingual corpus in order to build the most appropriate translation dictionary. Standard phrase-based SMT systems first align the parallel corpus at the word level by using IBM probabilities and then use standard constraints (see section 3) to extract final translation units [10]. Variations of this type of segmentation can be found in [8, 1, 6]. Other approaches consist in integrating the phrase segmentation and alignment, one example is in [14] where they use the point-wise mutual information between the source and target words to identify aligned phrase pairs. In [9] they use a greedy algorithm to compute recursive alignments from a bilingual parallel corpus. Here, we propose to combine the standard phrase-based segmentation [10] with a complementary bilingual segmentation which is learned from a statistical collocation segmen189 Proceedings of the 7th Internationa"
2010.iwslt-evaluation.26,P06-2084,0,0.0305339,"tivity measures. Different associativity measures have different scale of values and it is difficult to set threshold manually. Threshold level is kept as low as possible. Higher threshold value makes shorter collocation segments and vice versa. Shorter collocation segments are more confident collocations and we may expect better translation results. Nevertheless, the results of [3] show that longer collocation segments are more preferable. There are many associativity measures that could be used to calculate the associativity values between tokens (a more comprehensive list could be found in [11]). To explore different measures we included the six following metrics: 1. Mutual Information (MI): M I(wi , wi+1 ) = N ∗ f (wi , wi+1 ) f (w1 ) + f (wi+1 ) dice(wi , wi+1 ) = 2 ∗ f (wi , wi+1 ) f (wi ) + f (wi+1 ) 1. the words are consecutive, and, 2. they are consistent with the word alignment matrix. Given the collected phrase pairs, the phrase translation probability distribution is commonly estimated by relative frequency in both directions. The translation model is combined together with the following six additional feature models: the target language model, the word and the phrase bonus"
2010.iwslt-evaluation.26,J93-1007,0,0.203913,", four tokens). The boundary of a segment is set between adjacent tokens when the value of associativity between these two adjacent tokens is lower than the average of preceding and following associativity values. Some examples of segmentation of English and French sentences are presented in Table 1. The result of collocation segmentation is a segmented text, no dictionaries are produces and no evaluation of segments is made. The segmented text could be used to create a dictionary of collocations. Such dictionary accepts all collocation segments. The main difference from Choueka [2] and Smadja[12] methods is that collocation segmentation accepts all collocations and no significance tests for collocations are performed. The main advantage of this segmentation is the ability to perform collocation segmentation of both small and large corpora, and no manually segmented corpora or other databases and language processing tools are required. 5. Introducing the collocation segmentation into a phrase-based system In order to build the augmented phrase table with the technique mentioned in section 4, we segmented each language of the bilingual corpus independently and then, using the collocatio"
2010.iwslt-evaluation.26,takezawa-etal-2002-toward,0,0.0385848,"integrate the baseline segmentation with the new phrases (hereinafter, collocation segmentation new phrases) or the baseline segmentation with the existing phrases (hereinafter, collocation segmentation smooth). chi2(wi , wi+1 ) = N = f (wi ) ∗ f (wi+1 ) N ∗ f (wi , wi+1 ) − f (wi ) ∗ f (wi+1 ) ∗ N − f (wi ) + f (wi , wi+1 ) N ∗ f (wi , wi+1 ) − f (wi ) ∗ f (wi+1 ) ∗ N − f (wi+1 ) + f (wi , wi+1 ) 5. Gravity-Counts (GC)[5]: gc(wi , wi+1 ) =   f (wi ) ∗ f (wi , wi+1 ) = log n(wi )   f (wi+1 ) ∗ f (wi , wi+1 ) +log n0 (wi+1 ) 6. Experiments We participated in the French-to-English BTEC task [13] in the correct recognition results. We build our baseline system using MOSES with the standard configuration http://www.statmt.org/moses/. 6. T-score: tscore(wi , wi+1 ) = f (wi , wi+1 ) − f (wi )fN(wi+1 ) p f (wi , wi+1 ) The next step after setting the associativity threshold boundaries is to apply an average minimum law (AML) as described in [3] and [4]. The average minimum law is applied to the three adjacent associativity values (i.e., four tokens). The boundary of a segment is set between adjacent tokens when the value of associativity between these two adjacent tokens is lower than the"
2010.iwslt-evaluation.26,W10-1712,1,0.834596,"systems. 1. Introduction The Universitat Polit`ecnica de Catalunya (UPC), Barcelona Media Innovation Center (BMIC) and Vytautas Magnus University (VMU) participated together in the IWSLT 2010 evaluation campaign. This paper describes the UPC-BMICVMU system, which is basically a statistical phrase-based system enriched with collocation segmentation information. Adding a novel segmentation in an SMT system allows to enrich the translation dictionary and/or to smooth the existing translation probabilities. Basically, we extend the work presented in the WMT 2010 evaluation for Spanish-to-English [7] by experimenting with different statistical scores to segment a monolingual training corpus and by analysing if it is better to add new translation phrases and/or to smooth the existing ones. We participated in the French-to-English BTEC task. Our primary and contrastive systems were two standard phrase-based SMT systems enriched with different novel segmentations. This paper is organized as follows. Section 2 makes a brief description of some related work to the introduction of new segmentations in SMT. Section 3 describes the baseline system. Then, Section 4 reports different statistical cr"
2010.iwslt-evaluation.26,J06-4004,1,0.847561,"on 6 shows the experimental details of the system and the experiments performed with the novel technique. Finally, Section 7 presents the conclusions. 2. Related work One of the main problems in the statistical machine translation approach is how to segment the bilingual corpus in order to build the most appropriate translation dictionary. Standard phrase-based SMT systems first align the parallel corpus at the word level by using IBM probabilities and then use standard constraints (see section 3) to extract final translation units [10]. Variations of this type of segmentation can be found in [8, 1, 6]. Other approaches consist in integrating the phrase segmentation and alignment, one example is in [14] where they use the point-wise mutual information between the source and target words to identify aligned phrase pairs. In [9] they use a greedy algorithm to compute recursive alignments from a bilingual parallel corpus. Here, we propose to combine the standard phrase-based segmentation [10] with a complementary bilingual segmentation which is learned from a statistical collocation segmen189 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3"
2011.eamt-1.18,J07-2003,0,0.0521452,"euven, Belgium, May 2011 get language sentence eˆ (usually referred as “English”). Among all possible target language sentences e we choose the one with the highest score, as show in equation (1): "" M # X eˆ = arg max λm hm (f, e) (1) e m=1 This equation, called the log-linear model, is a variation of the source-channel approach to SMT (Brown et al., 1990). It was proposed by Och and Ney (2002) and allows using more than two models and to weight them independently. Frequently used paradigms of SMT based on the log-linear model are Phrase-based SMT (Koehn et al., 2003), Hierarchical-based SMT (Chiang, 2007) and Ngram-based SMT (Mari˜no et al., 2006). In our experiments we used the Ngrambased approach. The Ngram-based approach relies on the concept of tuple. A tuple is a bilingual unit with consecutive words both on the source and target side that is consistent with the word alignment. They must provide a unique monotonic segmentation of the sentence pair and they cannot be inside other tuple. This unique segmentation allows us to see the translation model as a language model, where the language is composed of tuples instead of words. That way, the context used in the translation model is bilingu"
2011.eamt-1.18,2005.iwslt-1.23,1,0.892438,"Missing"
2011.eamt-1.18,2009.eamt-1.8,1,0.899687,"Missing"
2011.eamt-1.18,2010.amta-papers.21,0,0.147527,"m such adaptation, and they all require a small indomain corpus whether it is bilingual or not, for the system to adapt. Some of them include: concatenate corpora and model interpolation (Koehn and Schroeder, 2007), using mono-lingual and crosslingual information retrieval (Hildebrand et al., 2005; Xu et al., 2007; Snover et al., 2008), language model adaptation for difficult to translate phrases (Mohit et al., 2009), generating a synthetic corpus (Ueffing et al., 2007; Schwenk and Senellart, 2009) and finally post-editing approaches (Isabelle et al., 2007) combined with incremental training (Hardt and Elming, 2010). The strategy proposed here assumes we have an out-of-domain system and a revised corpus that is composed of a small bilingual in-domain corpus and the translation of its source side computed with the system we like to adapt. 3.2 User feedback. Similar to domain adaptation, user feedback is also a valid scenario for the proposed strategy. In this case, a previously trained SMT system is used to translate sentences provided by different users. Then, if the users consider it convenient, they can suggest a better translation than the one the system obtained. If we saved all those suggestions tog"
2011.eamt-1.18,2005.eamt-1.19,0,0.0305893,"a method to adapt to different domains is preferred than building a whole new system for each domain we face. Moreover, it might be the only plausible solution if we have a big out-of-domain parallel corpus but a small indomain corpus which, if used alone, would perform poorly. Different methods have been studied to perform such adaptation, and they all require a small indomain corpus whether it is bilingual or not, for the system to adapt. Some of them include: concatenate corpora and model interpolation (Koehn and Schroeder, 2007), using mono-lingual and crosslingual information retrieval (Hildebrand et al., 2005; Xu et al., 2007; Snover et al., 2008), language model adaptation for difficult to translate phrases (Mohit et al., 2009), generating a synthetic corpus (Ueffing et al., 2007; Schwenk and Senellart, 2009) and finally post-editing approaches (Isabelle et al., 2007) combined with incremental training (Hardt and Elming, 2010). The strategy proposed here assumes we have an out-of-domain system and a revised corpus that is composed of a small bilingual in-domain corpus and the translation of its source side computed with the system we like to adapt. 3.2 User feedback. Similar to domain adaptation,"
2011.eamt-1.18,W07-0733,0,0.0306855,"reviews as expected. Text corpora can be different in vocabulary, style or grammar and a method to adapt to different domains is preferred than building a whole new system for each domain we face. Moreover, it might be the only plausible solution if we have a big out-of-domain parallel corpus but a small indomain corpus which, if used alone, would perform poorly. Different methods have been studied to perform such adaptation, and they all require a small indomain corpus whether it is bilingual or not, for the system to adapt. Some of them include: concatenate corpora and model interpolation (Koehn and Schroeder, 2007), using mono-lingual and crosslingual information retrieval (Hildebrand et al., 2005; Xu et al., 2007; Snover et al., 2008), language model adaptation for difficult to translate phrases (Mohit et al., 2009), generating a synthetic corpus (Ueffing et al., 2007; Schwenk and Senellart, 2009) and finally post-editing approaches (Isabelle et al., 2007) combined with incremental training (Hardt and Elming, 2010). The strategy proposed here assumes we have an out-of-domain system and a revised corpus that is composed of a small bilingual in-domain corpus and the translation of its source side compute"
2011.eamt-1.18,N03-1017,0,0.0188374,"ciation for Machine Translation, p. 121128 Leuven, Belgium, May 2011 get language sentence eˆ (usually referred as “English”). Among all possible target language sentences e we choose the one with the highest score, as show in equation (1): "" M # X eˆ = arg max λm hm (f, e) (1) e m=1 This equation, called the log-linear model, is a variation of the source-channel approach to SMT (Brown et al., 1990). It was proposed by Och and Ney (2002) and allows using more than two models and to weight them independently. Frequently used paradigms of SMT based on the log-linear model are Phrase-based SMT (Koehn et al., 2003), Hierarchical-based SMT (Chiang, 2007) and Ngram-based SMT (Mari˜no et al., 2006). In our experiments we used the Ngrambased approach. The Ngram-based approach relies on the concept of tuple. A tuple is a bilingual unit with consecutive words both on the source and target side that is consistent with the word alignment. They must provide a unique monotonic segmentation of the sentence pair and they cannot be inside other tuple. This unique segmentation allows us to see the translation model as a language model, where the language is composed of tuples instead of words. That way, the context u"
2011.eamt-1.18,W04-3250,0,0.0453004,"Missing"
2011.eamt-1.18,2005.mtsummit-papers.11,0,0.0199597,"has actually three parts: The source side of the bilingual corpus, the target output (computed with the trained system) and the target correction (the target side of the bilingual corpus). We present now two different cases that illustrates the scenario described before. Additionally, we can see a graphical description of the general case in Figure 1. 3.1 Domain adaptation. Because SMT systems are tightly coupled to their corpus domain, they are prone to commit errors when they translate sentences that belong to a different domain. For instance, a SMT system trained with the Europarl Corpus (Koehn, 2005) may not translate movie reviews as expected. Text corpora can be different in vocabulary, style or grammar and a method to adapt to different domains is preferred than building a whole new system for each domain we face. Moreover, it might be the only plausible solution if we have a big out-of-domain parallel corpus but a small indomain corpus which, if used alone, would perform poorly. Different methods have been studied to perform such adaptation, and they all require a small indomain corpus whether it is bilingual or not, for the system to adapt. Some of them include: concatenate corpora a"
2011.eamt-1.18,J06-4004,1,0.890768,"Missing"
2011.eamt-1.18,2009.eamt-1.22,0,0.151889,"ght be the only plausible solution if we have a big out-of-domain parallel corpus but a small indomain corpus which, if used alone, would perform poorly. Different methods have been studied to perform such adaptation, and they all require a small indomain corpus whether it is bilingual or not, for the system to adapt. Some of them include: concatenate corpora and model interpolation (Koehn and Schroeder, 2007), using mono-lingual and crosslingual information retrieval (Hildebrand et al., 2005; Xu et al., 2007; Snover et al., 2008), language model adaptation for difficult to translate phrases (Mohit et al., 2009), generating a synthetic corpus (Ueffing et al., 2007; Schwenk and Senellart, 2009) and finally post-editing approaches (Isabelle et al., 2007) combined with incremental training (Hardt and Elming, 2010). The strategy proposed here assumes we have an out-of-domain system and a revised corpus that is composed of a small bilingual in-domain corpus and the translation of its source side computed with the system we like to adapt. 3.2 User feedback. Similar to domain adaptation, user feedback is also a valid scenario for the proposed strategy. In this case, a previously trained SMT system is used t"
2011.eamt-1.18,P02-1038,0,0.0276868,"uage sentence f (usually referred as “French”) into a tarMikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 121128 Leuven, Belgium, May 2011 get language sentence eˆ (usually referred as “English”). Among all possible target language sentences e we choose the one with the highest score, as show in equation (1): "" M # X eˆ = arg max λm hm (f, e) (1) e m=1 This equation, called the log-linear model, is a variation of the source-channel approach to SMT (Brown et al., 1990). It was proposed by Och and Ney (2002) and allows using more than two models and to weight them independently. Frequently used paradigms of SMT based on the log-linear model are Phrase-based SMT (Koehn et al., 2003), Hierarchical-based SMT (Chiang, 2007) and Ngram-based SMT (Mari˜no et al., 2006). In our experiments we used the Ngrambased approach. The Ngram-based approach relies on the concept of tuple. A tuple is a bilingual unit with consecutive words both on the source and target side that is consistent with the word alignment. They must provide a unique monotonic segmentation of the sentence pair and they cannot be inside oth"
2011.eamt-1.18,padro-etal-2010-freeling,0,0.116155,"Missing"
2011.eamt-1.18,2001.mtsummit-papers.68,0,0.010657,"n-gram n, T MBase is the baseline translation model and T MAdd is the new translation model computed in the third step. To determine the value of α we tuned the system considering five different values and kept the one that obtained the highest BLEU score. Table 5 shows the different BLEU scores obtained with the development set and α = 0.85 as the best candidate. 5 Results and discussion. We built two different systems and used α = 0.85 for the interpolation. The results obtained over the correction and test corpora can be seen in Table 6. The second and third column correspond to the BLEU (Papineni et al., 2001) scores obtained by the different systems in the Correction and Test corpora, using only one reference. The fourth column gives the confidence level for test BLEU being higher than the baseline test BLEU. Notice that all revised system performed better in the correction corpus, which is obvious because it is part of the revised corpus. Also, they all improved the baseline test score. What is interesting is that once we added the lexical filter, the correction BLEU decreased and the test BLEU increased. It means that the filter is helping the system generalize its learning. Moreover, even thoug"
2011.eamt-1.18,2009.mtsummit-posters.17,0,0.0157044,"corpus but a small indomain corpus which, if used alone, would perform poorly. Different methods have been studied to perform such adaptation, and they all require a small indomain corpus whether it is bilingual or not, for the system to adapt. Some of them include: concatenate corpora and model interpolation (Koehn and Schroeder, 2007), using mono-lingual and crosslingual information retrieval (Hildebrand et al., 2005; Xu et al., 2007; Snover et al., 2008), language model adaptation for difficult to translate phrases (Mohit et al., 2009), generating a synthetic corpus (Ueffing et al., 2007; Schwenk and Senellart, 2009) and finally post-editing approaches (Isabelle et al., 2007) combined with incremental training (Hardt and Elming, 2010). The strategy proposed here assumes we have an out-of-domain system and a revised corpus that is composed of a small bilingual in-domain corpus and the translation of its source side computed with the system we like to adapt. 3.2 User feedback. Similar to domain adaptation, user feedback is also a valid scenario for the proposed strategy. In this case, a previously trained SMT system is used to translate sentences provided by different users. Then, if the users consider it c"
2011.eamt-1.18,D08-1090,0,0.0552669,"preferred than building a whole new system for each domain we face. Moreover, it might be the only plausible solution if we have a big out-of-domain parallel corpus but a small indomain corpus which, if used alone, would perform poorly. Different methods have been studied to perform such adaptation, and they all require a small indomain corpus whether it is bilingual or not, for the system to adapt. Some of them include: concatenate corpora and model interpolation (Koehn and Schroeder, 2007), using mono-lingual and crosslingual information retrieval (Hildebrand et al., 2005; Xu et al., 2007; Snover et al., 2008), language model adaptation for difficult to translate phrases (Mohit et al., 2009), generating a synthetic corpus (Ueffing et al., 2007; Schwenk and Senellart, 2009) and finally post-editing approaches (Isabelle et al., 2007) combined with incremental training (Hardt and Elming, 2010). The strategy proposed here assumes we have an out-of-domain system and a revised corpus that is composed of a small bilingual in-domain corpus and the translation of its source side computed with the system we like to adapt. 3.2 User feedback. Similar to domain adaptation, user feedback is also a valid scenario"
2011.eamt-1.18,2007.mtsummit-tutorials.1,0,0.0566478,"out-of-domain parallel corpus but a small indomain corpus which, if used alone, would perform poorly. Different methods have been studied to perform such adaptation, and they all require a small indomain corpus whether it is bilingual or not, for the system to adapt. Some of them include: concatenate corpora and model interpolation (Koehn and Schroeder, 2007), using mono-lingual and crosslingual information retrieval (Hildebrand et al., 2005; Xu et al., 2007; Snover et al., 2008), language model adaptation for difficult to translate phrases (Mohit et al., 2009), generating a synthetic corpus (Ueffing et al., 2007; Schwenk and Senellart, 2009) and finally post-editing approaches (Isabelle et al., 2007) combined with incremental training (Hardt and Elming, 2010). The strategy proposed here assumes we have an out-of-domain system and a revised corpus that is composed of a small bilingual in-domain corpus and the translation of its source side computed with the system we like to adapt. 3.2 User feedback. Similar to domain adaptation, user feedback is also a valid scenario for the proposed strategy. In this case, a previously trained SMT system is used to translate sentences provided by different users. Th"
2011.eamt-1.18,2007.mtsummit-papers.68,0,0.0286385,"ferent domains is preferred than building a whole new system for each domain we face. Moreover, it might be the only plausible solution if we have a big out-of-domain parallel corpus but a small indomain corpus which, if used alone, would perform poorly. Different methods have been studied to perform such adaptation, and they all require a small indomain corpus whether it is bilingual or not, for the system to adapt. Some of them include: concatenate corpora and model interpolation (Koehn and Schroeder, 2007), using mono-lingual and crosslingual information retrieval (Hildebrand et al., 2005; Xu et al., 2007; Snover et al., 2008), language model adaptation for difficult to translate phrases (Mohit et al., 2009), generating a synthetic corpus (Ueffing et al., 2007; Schwenk and Senellart, 2009) and finally post-editing approaches (Isabelle et al., 2007) combined with incremental training (Hardt and Elming, 2010). The strategy proposed here assumes we have an out-of-domain system and a revised corpus that is composed of a small bilingual in-domain corpus and the translation of its source side computed with the system we like to adapt. 3.2 User feedback. Similar to domain adaptation, user feedback is"
2011.eamt-1.18,J90-2002,0,\N,Missing
2011.eamt-1.18,P02-1040,0,\N,Missing
2011.eamt-1.20,padro-etal-2010-freeling,0,\N,Missing
2011.eamt-1.20,J90-2002,0,\N,Missing
2011.eamt-1.20,P02-1040,0,\N,Missing
2011.eamt-1.20,2009.eamt-1.8,1,\N,Missing
2011.eamt-1.20,D08-1090,0,\N,Missing
2011.eamt-1.20,2009.eamt-1.22,0,\N,Missing
2011.eamt-1.20,W07-0733,0,\N,Missing
2011.eamt-1.20,J06-4004,1,\N,Missing
2011.eamt-1.20,N03-1017,0,\N,Missing
2011.eamt-1.20,P02-1038,0,\N,Missing
2011.eamt-1.20,2005.mtsummit-papers.11,0,\N,Missing
2011.eamt-1.20,W04-3250,0,\N,Missing
2011.eamt-1.20,2005.eamt-1.19,0,\N,Missing
2011.eamt-1.20,2009.mtsummit-posters.17,0,\N,Missing
2011.eamt-1.20,J07-2003,0,\N,Missing
2012.amta-monomt.1,W12-3102,0,0.0560857,"most reliable decision taken by the classifiers. In this paper we follow the latter criteria: After processing the features by all classifiers simultaneously, the most consistent decision from all binary classifiers is taken in first place, afterwards the second best is considered and so on, until the final class is answered by the binary decisions. The experiments are explained in section 4.4. 4 Moses from 0 to 0,2) using two separate language models for improving the fluency of the output. We did the alignment with stems through mGIZA (Gao and Vogel, 2008). We used the material from WMT12 (Callison-Burch et al., 2012) MT Shared Task for training. We used the Freeling analyzer (Padr´o et al., 2010) to tokenize, lemmatize and POStag both sides of the corpus (English and Spanish). In the same way we use the Freeling libraries in order to conjugate the verbs. We trained the language models (LM) with the SRILM Toolkit (Stolcke, 2002) at 5-gram level for words and 7-gram level for POS-tags. In order to study the impact of the morphology at different training levels we have considered two different scenarios: First, we train a system only with texts from the European Parliament being a limited resource scenario,"
2012.amta-monomt.1,P11-1004,0,0.0150626,"ranslates into Spanish simplified forms and then predicts the final inflected forms through a morphology generation step based on shallow and deep-projected linguistic information available from both the source and targetlanguage sentences. Obtained results highlight the importance of generalization, and therefore generation, for dealing with out-ofdomain data. 1 Introduction The problems raised when translating into richer morphology languages are well known and are being continuously studied (Popovic and Ney, 2004; Koehn and Hoang, 2007; de Gispert and Mari˜no, 2008; Toutanova et al., 2008; Clifton and Sarkar, 2011; Bojar and Tamchyna, 2011). When translating from English into Spanish, inflected words make the lexicon to be very large causing a significant data sparsity problem. In addition, system output is limited only to inflected phrases available in the parallel training corpus (Bojar and Tamchyna, 2011). Hence, phrase-based SMT systems cannot generate proper inflections unless they have learned them from the appropriate phrases. 6 2 System architecture is due to the following reasons: i) text from formal acts converts the second person (t´u) subject into usted formal form, which uses third person"
2012.amta-monomt.1,N09-2019,0,0.0400993,"Missing"
2012.amta-monomt.1,W11-2107,0,0.0317035,"improvement between 0.5 to 0.7 B LEU points for the News sets and an improvement from 1 to 1.3 B LEU points for weblog datasets. For the full trained system we observed a similar improvement for the News sets (between 0.5 and 0.7 B LEU points) but a better improvement, between 2 and 3 B LEU points, for the out-of-domain weblog data. These oracles demonstrate the potential of morphology generalization as a good strategy for dealing with outof-domain data. After analyzing the oracles we studied the overall translation performance of the strategy. We analyzed the results with B LEU and M ETEOR (Denkowski and Lavie, 2011). However, M ETEOR properties of synonymy and paraphrasing did not make it suitable for evaluating the oracles for the simplified references. In addition, table 7 details the results for the full generation strategy. In general terms, we observe better improvements for the weblog (out-ofdomain) data than for the News data. For the constrained system, weblog test sets improve by 0.55 B LEU/0.20 M ETEOR points while News test sets only improve 0.25 B LEU/0.14 M ETEOR points. For the fully trained system, the out-of-domain improvement is 1.49 B LEU/1.27 M ETEOR points in average and the News (in-"
2012.amta-monomt.1,W08-0509,0,0.0168432,"rate, the balance between samples of each class or the most reliable decision taken by the classifiers. In this paper we follow the latter criteria: After processing the features by all classifiers simultaneously, the most consistent decision from all binary classifiers is taken in first place, afterwards the second best is considered and so on, until the final class is answered by the binary decisions. The experiments are explained in section 4.4. 4 Moses from 0 to 0,2) using two separate language models for improving the fluency of the output. We did the alignment with stems through mGIZA (Gao and Vogel, 2008). We used the material from WMT12 (Callison-Burch et al., 2012) MT Shared Task for training. We used the Freeling analyzer (Padr´o et al., 2010) to tokenize, lemmatize and POStag both sides of the corpus (English and Spanish). In the same way we use the Freeling libraries in order to conjugate the verbs. We trained the language models (LM) with the SRILM Toolkit (Stolcke, 2002) at 5-gram level for words and 7-gram level for POS-tags. In order to study the impact of the morphology at different training levels we have considered two different scenarios: First, we train a system only with texts f"
2012.amta-monomt.1,P12-1016,0,0.0156853,"rino Universitat Polit`ecnica de Catalunya (UPC), Barcelona, 08034 Spain {lluis.formiga,adolfo.hernandez,jose.marino,enric.monte}@upc.edu Abstract That would require to have a parallel corpus containing all possible word inflections for all phrases available, which it is an unfeasible task. Different approaches to address the morphology into SMT may be summarized in four, not mutually exclusive, categories: i) factored models (Koehn and Hoang, 2007), enriched input models (Avramidis and Koehn, 2008; Ueffing and Ney, 2003), segmented translation (Virpioja et al., 2007; de Gispert et al., 2009; Green and DeNero, 2012) and morphology generation (Toutanova et al., 2008; de Gispert and Mari˜no, 2008; Bojar and Tamchyna, 2011). Whereas segmented translation is intended for agglutinative languages, translation into Spanish has been classically addressed either by factored models (Koehn and Hoang, 2007), enriched input scheme (Ueffing and Ney, 2003) or target language simplification plus a morphology generation as an independent step (de Gispert and Mari˜no, 2008). This latter approach has also been used to translate to other rich morphology languages such as Czech (Bojar and Tamchyna, 2011). The problem of morp"
2012.amta-monomt.1,W12-3154,0,0.146617,"keeps information of verb type (‘VM’ ! main verb), mode and tense (‘IP’ ! indicative, present), while ‘p’ and ‘n’ represent any person and number once generalized (from 3rd person singular). It is important to highlight that we do not perform just a simple lemmatization as we also keep the information about the type, mode and tense of the verb. After simplifying the corpus we can build the models following the standard procedures explained in section 4.1. Note that the tuning of the system is performed with the simplified reference of the development texts. Some recent domain-adaptation work (Haddow and Koehn, 2012) has dealt implicitly with this problem using the OpenSubtitles1 bilingual corpus that contains plenty of dialogs and therefore second person inflected Spanish forms. However, their study found drawbacks in the use of an additional corpus as training material: the improvement of the quality of the out-of-domain translations worsened the quality of in-domain translations. On the other hand, the use of an additional corpus to train specific inflected-forms language generator has not yet been addressed. This paper presents our findings on tackling the problem to inflect out-of-domain verbs. We bu"
2012.amta-monomt.1,D07-1091,0,0.161964,"task. Different approaches to address the morphology into SMT may be summarized in four, not mutually exclusive, categories: i) factored models (Koehn and Hoang, 2007), enriched input models (Avramidis and Koehn, 2008; Ueffing and Ney, 2003), segmented translation (Virpioja et al., 2007; de Gispert et al., 2009; Green and DeNero, 2012) and morphology generation (Toutanova et al., 2008; de Gispert and Mari˜no, 2008; Bojar and Tamchyna, 2011). Whereas segmented translation is intended for agglutinative languages, translation into Spanish has been classically addressed either by factored models (Koehn and Hoang, 2007), enriched input scheme (Ueffing and Ney, 2003) or target language simplification plus a morphology generation as an independent step (de Gispert and Mari˜no, 2008). This latter approach has also been used to translate to other rich morphology languages such as Czech (Bojar and Tamchyna, 2011). The problem of morphology sparsity becomes crucial when addressing translations out-of-domain. Under that scenario, there is a high presence of previously unseen inflected forms even though their lemma could have been learned with the training material. A typical scenario out-of-domain is based on weblo"
2012.amta-monomt.1,W03-0430,0,0.0115066,"Missing"
2012.amta-monomt.1,padro-etal-2010-freeling,0,0.0393135,"Missing"
2012.amta-monomt.1,pighin-etal-2012-faust,1,0.808709,"Missing"
2012.amta-monomt.1,popovic-ney-2004-towards,0,0.0253806,"uage causes poor quality translation when translating out-ofdomain. In detail, this approach first translates into Spanish simplified forms and then predicts the final inflected forms through a morphology generation step based on shallow and deep-projected linguistic information available from both the source and targetlanguage sentences. Obtained results highlight the importance of generalization, and therefore generation, for dealing with out-ofdomain data. 1 Introduction The problems raised when translating into richer morphology languages are well known and are being continuously studied (Popovic and Ney, 2004; Koehn and Hoang, 2007; de Gispert and Mari˜no, 2008; Toutanova et al., 2008; Clifton and Sarkar, 2011; Bojar and Tamchyna, 2011). When translating from English into Spanish, inflected words make the lexicon to be very large causing a significant data sparsity problem. In addition, system output is limited only to inflected phrases available in the parallel training corpus (Bojar and Tamchyna, 2011). Hence, phrase-based SMT systems cannot generate proper inflections unless they have learned them from the appropriate phrases. 6 2 System architecture is due to the following reasons: i) text fro"
2012.amta-monomt.1,P08-1059,0,0.313201,"arcelona, 08034 Spain {lluis.formiga,adolfo.hernandez,jose.marino,enric.monte}@upc.edu Abstract That would require to have a parallel corpus containing all possible word inflections for all phrases available, which it is an unfeasible task. Different approaches to address the morphology into SMT may be summarized in four, not mutually exclusive, categories: i) factored models (Koehn and Hoang, 2007), enriched input models (Avramidis and Koehn, 2008; Ueffing and Ney, 2003), segmented translation (Virpioja et al., 2007; de Gispert et al., 2009; Green and DeNero, 2012) and morphology generation (Toutanova et al., 2008; de Gispert and Mari˜no, 2008; Bojar and Tamchyna, 2011). Whereas segmented translation is intended for agglutinative languages, translation into Spanish has been classically addressed either by factored models (Koehn and Hoang, 2007), enriched input scheme (Ueffing and Ney, 2003) or target language simplification plus a morphology generation as an independent step (de Gispert and Mari˜no, 2008). This latter approach has also been used to translate to other rich morphology languages such as Czech (Bojar and Tamchyna, 2011). The problem of morphology sparsity becomes crucial when addressing tr"
2012.amta-monomt.1,2007.mtsummit-papers.65,0,0.0602037,"Missing"
2012.amta-monomt.1,P08-1087,0,\N,Missing
costa-jussa-etal-2010-automatic,E06-1032,0,\N,Missing
costa-jussa-etal-2010-automatic,P02-1040,0,\N,Missing
costa-jussa-etal-2010-automatic,J06-4004,1,\N,Missing
costa-jussa-etal-2010-automatic,P03-1021,0,\N,Missing
J06-4004,W05-0823,1,0.838951,"Missing"
J06-4004,W00-0508,0,0.0142428,"y approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state perspective—more specifically, from"
J06-4004,J96-1002,0,0.02988,"Missing"
J06-4004,J90-2002,0,0.81424,"nslation was conceived as the problem of finding a sentence by decoding a given “encrypted” version of it (Weaver 1955). Although the idea seemed very feasible, enthusiasm faded shortly afterward because of the computational limitations of the time (Hutchins 1986). Finally, during the nineties, two factors made it possible for SMT to become an actual and practical technology: first, significant increment in both the computational power and storage capacity of computers, and second, the availability of large volumes of bilingual data. The first SMT systems were developed in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for adequacy of translation contents, times a target language probability p(T), which accounts for fluency of target constructions. For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora (Brown et al. 1993). In the case of target"
J06-4004,J93-2003,0,0.0556776,"d in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for adequacy of translation contents, times a target language probability p(T), which accounts for fluency of target constructions. For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora (Brown et al. 1993). In the case of target language probabilities, these were generally trained from monolingual data by using n-grams. Present SMT systems have evolved from the original ones in such a way that mainly differ from them in two respects: first, word-based translation models have been ∗ Department of Signal Theory and Communications, Campus Nord, Barcelona 08034, Spain. Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for publication: 5 July 2006 © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 replaced by phrase-b"
J06-4004,J04-2004,0,0.856987,"models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state perspective—more specifically, from the work of Casacuberta (2001) and Casacuberta and Vidal (2004). However, whereas in this earlier work the translation model is implemented by using a finite-state transducer, in the system presented here the translation model is implemented by using n-grams. In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models. The translation model presented here actually constitutes a language model of a sort of “bilanguage” composed of bilin˜ 2002). An alternagual units, which will be referred to as tuples (de Gispert and Marino tive approach, which relies on bilingual-unit un"
J06-4004,N04-1033,0,0.0140655,"Missing"
J06-4004,2005.iwslt-1.23,1,0.883592,"Missing"
J06-4004,2005.mtsummit-papers.37,1,0.855856,"Missing"
J06-4004,2006.amta-papers.4,1,0.763242,"Missing"
J06-4004,P05-2012,1,0.804737,"Missing"
J06-4004,C86-1155,0,0.079146,"ament Plenary Sessions (EPPS). 1. Introduction The beginnings of statistical machine translation (SMT) can be traced back to the early fifties, closely related to the ideas from which information theory arose (Shannon and Weaver 1949) and inspired by works on cryptography (Shannon 1949, 1951) during World War II. According to this view, machine translation was conceived as the problem of finding a sentence by decoding a given “encrypted” version of it (Weaver 1955). Although the idea seemed very feasible, enthusiasm faded shortly afterward because of the computational limitations of the time (Hutchins 1986). Finally, during the nineties, two factors made it possible for SMT to become an actual and practical technology: first, significant increment in both the computational power and storage capacity of computers, and second, the availability of large volumes of bilingual data. The first SMT systems were developed in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for"
J06-4004,knight-al-onaizan-1998-translation,0,0.230855,"more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state persp"
J06-4004,N03-1017,0,0.0258625,"Missing"
J06-4004,2005.mtsummit-papers.36,1,0.177804,"Missing"
J06-4004,P00-1056,0,0.0440511,"tence pairs are removed from the training data to allow for a better performance of the alignment tool. Sentence pairs are removed according to the following two criteria: r r Fertility filtering: removes sentence pairs with a word ratio larger than a predefined threshold value. Length filtering: removes sentence pairs with at least one sentence of more than 100 words in length. This helps to maintain bounded alignment computational times. After preprocessing, word-to-word alignments are performed in both directions, source-to-target and target-to-source. In our system implementation, GIZA++ (Och and Ney 2000) is used for computing the alignments. A total of five iterations for models IBM-1 and HMM, and three iterations for models IBM-3 and IBM-4, are performed. Then, the obtained alignment sets are used for computing the intersection and the union of alignments from which tuples and embedded-word tuples are extracted, respectively. 4.2.2 Tuple Extraction and Pruning. A tuple set for each translation direction is extracted from the union set of alignments while avoiding source-nulled tuples by using the procedure described in Section 2.2.2. Then, the resulting tuple vocabularies are pruned accordin"
J06-4004,P02-1038,0,0.884384,"034, Spain. Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for publication: 5 July 2006 © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 replaced by phrase-based translation models (Zens, Och, and Ney 2002; Koehn, Och, and Marcu 2003) which are directly estimated from aligned bilingual corpora by considering relative frequencies, and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitesta"
J06-4004,J03-1002,0,0.00706932,"gram model alone, in the case of Table 2, and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2, in the case of Table 3. Both translation directions, Spanish to English (ES → EN) and English to Spanish (EN → ES), are considered in each table. In the case of Table 2, model size and translation accuracy are evaluated against the type of alignment set used for extracting tuples. Three different alignment sets are considered: source-to-target, the union of source-to-target and target-to-source, and the “refined” alignment method described by Och and Ney (2003). For the results presented in Table 2, a pruning parameter value of N = 20 was used for the Spanish-to-English direction, while a value of N = 30 was used for the English-to-Spanish direction. As can be clearly seen in Table 2, the union alignment set happens to be the most favorable one for extracting tuples in both translation directions since it provides a significantly better translation accuracy, in terms of BLEU score, than the other two alignment sets considered. Notice also in Table 2 that the union set is the one providing the smallest model sizes according to the number of bigrams a"
J06-4004,P02-1040,0,0.105596,"alignment sets. Notice that BLEU measurements in this table correspond to translations computed by using the tuple n-gram model alone. Direction Alignment set Tuple voc. Bigrams Trigrams BLEU ES → EN Source-to-target union refined Source-to-target union refined 1.920 2.040 2.111 1.813 2.023 2.081 6.426 6.009 6.851 6.263 6.092 6.920 2.353 1.798 2.398 2.268 1.747 2.323 0.4424 0.4745 0.4594 0.4152 0.4276 0.4193 EN → ES when tuples are extracted from different alignment sets and when different pruning parameters are used, respectively. Translation accuracy is measured in terms of the BLEU score (Papineni et al. 2002), which is computed here for translations generated by using the tuple n-gram model alone, in the case of Table 2, and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2, in the case of Table 3. Both translation directions, Spanish to English (ES → EN) and English to Spanish (EN → ES), are considered in each table. In the case of Table 2, model size and translation accuracy are evaluated against the type of alignment set used for extracting tuples. Three different alignment sets are considered: source-to-target, the union of source-to-targ"
J06-4004,N03-2036,0,0.00459439,"k the translation model is implemented by using a finite-state transducer, in the system presented here the translation model is implemented by using n-grams. In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models. The translation model presented here actually constitutes a language model of a sort of “bilanguage” composed of bilin˜ 2002). An alternagual units, which will be referred to as tuples (de Gispert and Marino tive approach, which relies on bilingual-unit unigram probabilities, was developed by Tillmann and Xia (2003); in contrast, the approach presented here considers bilingualunit n-gram probabilities. In addition to the tuple n-gram translation model, the translation system presented here implements four specific feature functions that are log-linearly combined along with the translation model for performing the decoding ˜ et al. 2005). (Marino This article is intended to provide a detailed description of the n-gram-based translation system, as well as to demonstrate the system performance in a widedomain, large-vocabulary translation task. The article is structured as follows. First, Section 2 presents"
J06-4004,2002.tmi-tutorials.2,0,0.201063,"Missing"
J06-4004,2004.iwslt-evaluation.14,1,\N,Missing
J06-4004,N04-1021,0,\N,Missing
N07-2035,2006.iwslt-evaluation.18,1,0.890672,"Missing"
N07-2035,2005.iwslt-1.23,1,0.905555,"Missing"
N07-2035,J06-4004,1,0.895476,"Missing"
N07-2035,E06-1005,1,0.799617,"ingual N -gram language model. In the phrase-based model, no monotonicity restriction is imposed on the segmentation and the probabilities are normally estimated simply by relative frequencies. This paper extends the analysis of both systems performed in (Crego et al., 2005a) by additionally performing a manual error analysis of both systems, which were the ones used by UPC and RWTH in the last Tc-Star evaluation. Furthermore, we will propose a way to combine both systems in order to improve the quality of translations. Experiments combining several kinds of MT systems have been presented in (Matusov et al., 2006), based only on the single best output of each system. Recently, a more straightforward approach of both systems has been performed in (Costa-juss` a et al., 2006) which simply selects, for each sentence, one of the provided hypotheses. This paper is organized as follows. In section 2, we briefly describe the phrase and the N -gram-based baseline systems. In the next section we present the evaluation framework. In Section 4 we report a structural comparison performed for both systems and, afterwards, in Section 5, we analyze the errors of both systems. Finally, in the last two sections we resc"
N07-2035,vilar-etal-2006-error,1,0.861806,"Missing"
N07-2035,N04-1033,1,0.812475,"n N -gram-based one. The exhaustive analysis includes a comparison of the translation models in terms of efficiency (number of translation units used in the search and computational time) and an examination of the errors in each system’s output. Additionally, we combine both systems, showing accuracy improvements. 1 Introduction Statistical machine translation (SMT) has evolved from the initial word-based translation models to more advanced models that take the context surrounding the words into account. The so-called phrase-based and N -gram-based models are two examples of these approaches (Zens and Ney, 2004; Mari˜ no et al., 2006). In current state-of-the-art SMT systems, the phrase-based or the N -gram-based models are usually the main features in a log-linear framework, reminiscent of the maximum entropy modeling approach. Two basic issues differentiate the N -gram-based system from the phrase-based one: the training data is sequentially segmented into bilingual units; and the probability of these units is estimated as a bilingual N -gram language model. In the phrase-based model, no monotonicity restriction is imposed on the segmentation and the probabilities are normally estimated simply by"
N07-2035,W06-3120,1,\N,Missing
P07-2054,2006.amta-papers.4,1,0.751245,"Missing"
P07-2054,2005.iwslt-1.23,1,0.868915,"Missing"
P07-2054,koen-2004-pharaoh,0,0.0425744,"and efficient algorithm (O(n), being n the search size) can be used in order to discard them, before rescoring work. Additionally, given that partial model costs are needed in rescoring work, our decoder allows to output the individual model costs computed for each translation unit (token t). Costs are encoded within the token s, as in the next example: (0 (1 &quot;o#or{1.5,0.9,0.6,0.2}&quot; 6)) where the token t is now composed of the translation unit ’o#or’, followed by (four) model costs. Multiple translation hypotheses can only be extracted if hypotheses recombinations are carefully saved. As in (Koehn, 2004), the decoder takes a record of any recombined hypothesis, allowing for a rigorous N -best generation. Model costs are referred to the current unit while the global score s is accumulated. Notice also that translation units (not words) are now used as tokens. 5 Experiments Experiments are carried out for a Spanish-to-English translation task using the EPPS data set, corresponding to session transcriptions of the European Parliament. Eff. base Beam size = 50 w/o cache 1, 820 w/ cache −50 Beam size = 100 w/o cache 2, 900 w/ cache −175 +tpos +reor +spos 2, 170 −110 2, 970 −190 3, 260 −210 4, 350"
P07-2054,J06-4004,1,0.914192,"Missing"
W05-0823,N04-1033,0,0.265878,"Missing"
W05-0823,2004.iwslt-evaluation.14,1,0.848754,"Missing"
W05-0823,N03-1017,0,0.0598703,"Missing"
W05-0823,P00-1056,0,0.0836903,"e pairs with a word ratio larger than 2.4. As a result of this preprocessing, the number of sentences in each training set was slightly reduced. However, no significant reduction was produced. In the case of French, a re-tokenizing procedure was performed in which all apostrophes appearing alone were attached to their corresponding words. For example, pairs of tokens such as l ’ and qu ’ were reduced to single tokens such as l’ and qu’. 134 Once the training data was preprocessed, a wordto-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000). As an approximation to the most probable alignment, the Viterbi alignment was considered. Then, the intersection and union of alignment sets in both directions were computed for each training set. 3.2 Feature Function Computation The considered translation system implements a total of five feature functions. The first of these models is the tuple 3-gram model, which was already described in section 2. Tuples for the translation model were extracted from the union set of alignments as shown in Figure 1. Once tuples had been extracted, the tuple vocabulary was pruned by using histogram pruning"
W05-0823,P02-1038,0,0.0900962,"were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model. 2 Bilingual N-gram Translation Model 1 Introduction During the last decade, statistical machine translation (SMT) systems have evolved from the original word-based approach (Brown et al., 1993) into phrase-based translation systems (Koehn et al., 2003). Similarly, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple models is implemented (Och and Ney, 2002). The SMT approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams. This translation model was developed by de Gispert and Mari˜no (2002), and it differs from the well known phrase-based translation model in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This model is described in section 2. Translation results from the four source languages made available for the shared task ("
W05-0823,P02-1040,0,0.0765659,"ts a beam-search strategy based on dynamic programming and takes into account all the five feature functions described above simultaneously. It also allows for three different pruning methods: threshold pruning, histogram pruning, and hypothesis recombination. For all the results presented in this work the decoder’s monotonic search modality was used. An optimization tool, which is based on a simplex method (Press et al., 2002), was developed and used for computing log-linear weights for each of the feature functions described above. This algorithm adjusts the log-linear weights so that BLEU (Papineni et al., 2002) is maximized over a given development set. One optimization for each language pair was performed by using the 2000-sentence development sets made available for the shared task. 4 Shared Task Results Table 2 presents the BLEU scores obtained for the shared task test data. Each test set consisted of 2000 sentences. The computed BLEU scores were case insensitive and used one translation reference. 135 sufficient and that , in the future , it is necessary to develop the Union better and a different structure... It is evident from these translation outputs that translation quality decreases when m"
W05-0823,J93-2003,0,\N,Missing
W06-3101,P04-1079,0,0.0149782,"sibilities for improvements. 1 Patrik Lambert† Rafael Banchs† 2 Introduction The evaluation of the generated output is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al."
W06-3101,W05-0909,0,0.0366343,"lation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nie"
W06-3101,N04-4015,0,0.00900173,"has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic information in combination with the automatic evaluation measures WER and PER in order to get more details about the tran"
W06-3101,2005.iwslt-1.19,1,0.729357,"s. 1 Patrik Lambert† Rafael Banchs† 2 Introduction The evaluation of the generated output is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed"
W06-3101,C00-2162,1,0.81769,"Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of"
W06-3101,2001.mtsummit-papers.45,1,0.834325,"05). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic infor"
W06-3101,W01-1407,1,0.840883,"05). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic infor"
W06-3101,niessen-etal-2000-evaluation,1,0.397729,"nt issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our kno"
W06-3101,P02-1040,0,0.115841,"t`ecnica de Catalunya (UPC), Barcelona, Spain ⊥ ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy {popovic,ney}@informatik.rwth-aachen.de {gupta,federico}@itc.it {agispert,canton}@gps.tsc.upc.es {lambert,banchs}@gps.tsc.upc.es Abstract A variety of automatic evaluation measures have been proposed and studied over the last years, some of them are shown to be a very useful tool for comparing different systems as well as for evaluating improvements within one system. The most widely used are Word Error Rate (WER), Position Independent Word Error Rate (PER), the BLEU score (Papineni et al., 2002) and the NIST score (Doddington, 2002). However, none of these measures give any details about the nature of translation errors. A relationship between these error measures and the actual errors in the translation outputs is not easy to find. Therefore some analysis of the translation errors is necessary in order to define the main problems and to focus the research efforts. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), but like human evaluation, this is also a time consuming task. The goal of this work is to present a framework for au"
W06-3101,popovic-ney-2004-towards,1,0.340303,"Missing"
W06-3101,popovic-ney-2006-pos,1,0.738548,"Missing"
W06-3101,2005.mtsummit-papers.34,1,0.721067,"173 0.15 0.09 2.7 1.7 840 1094 22774 26917 4081 3958 0.14 0.25 2.8 2.6 Table 1: Corpus statistics for the Spanish-English EPPS task (running words include punctuation marks) corpus). The statistics of the corpora can be seen in Table 1. The statistical machine translation system used in this work is based on a log-linear combination of seven different models. The most important ones are phrase based models in both directions, additionally IBM1 models at the phrase level in both directions as well as phrase and length penalty are used. A more detailed description of the system can be found in (Vilar et al., 2005; Zens et al., 2005). 4.3 Experiments The translation experiments have been done in both translation directions on both sizes of the corpus. In order to examine improvements of the baseline system, a new system with POS-based word reorderings of nouns and adjectives as proposed in (Popovi´c and Ney, 2006) is also analysed. Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round. Therefore, local reorderings of nouns and ad3 WER 34.5 33.5 41.8 38.9 PER 25.5 25.2 30.7 29.5 BLEU 54.7 56.4 43.2 48.5 English→Spanish full base"
W06-3101,vilar-etal-2006-error,1,0.542481,"Missing"
W06-3101,2005.iwslt-1.20,1,0.479674,"7 840 1094 22774 26917 4081 3958 0.14 0.25 2.8 2.6 Table 1: Corpus statistics for the Spanish-English EPPS task (running words include punctuation marks) corpus). The statistics of the corpora can be seen in Table 1. The statistical machine translation system used in this work is based on a log-linear combination of seven different models. The most important ones are phrase based models in both directions, additionally IBM1 models at the phrase level in both directions as well as phrase and length penalty are used. A more detailed description of the system can be found in (Vilar et al., 2005; Zens et al., 2005). 4.3 Experiments The translation experiments have been done in both translation directions on both sizes of the corpus. In order to examine improvements of the baseline system, a new system with POS-based word reorderings of nouns and adjectives as proposed in (Popovi´c and Ney, 2006) is also analysed. Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round. Therefore, local reorderings of nouns and ad3 WER 34.5 33.5 41.8 38.9 PER 25.5 25.2 30.7 29.5 BLEU 54.7 56.4 43.2 48.5 English→Spanish full baseline reorder 13k bas"
W06-3101,H05-1085,0,\N,Missing
W06-3120,A00-1031,0,0.0229195,"ese figures in the optimization function. 3 Shared Task Results 3.1 Data The data provided for this shared task corresponds to a subset of the official transcriptions of the European Parliament Plenary Sessions, and it is available through the shared task website at: http://www.statmt.org/wmt06/shared-task/. The development set used to tune the system consists of a subset (500 first sentences) of the official development set made available for the Shared Task. We carried out a morphological analysis of the data. The English POS-tagging has been carried out using freely available T N T tagger (Brants, 2000). In the Spanish case, we have used the F reeling (Carreras et al., 2004) analysis tool which generates the POS-tagging for each input word. 3.2 Systems configurations The baseline system is the same for all tasks and includes the following features functions: cp, pp, lm, ibm1, ibm1−1 , wb, pb. The POStag target language model has been used in those tasks for which the tagger was available. Table 1 shows the reordering configuration used for each task. The Block Reordering (application 2) has been used when the source language belongs to the Romanic family. The length of the block is limited t"
W06-3120,W05-0827,1,0.879283,"Missing"
W06-3120,2005.iwslt-1.23,1,0.907982,"Missing"
W06-3120,W06-3125,1,0.884485,"Missing"
W06-3120,P05-2012,1,0.889176,"Missing"
W06-3120,W05-0831,0,0.0297232,"Full verb forms The morphology of the verbs usually differs in each language. Therefore, it is interesting to classify the verbs in order to address the rich variety of verbal forms. Each verb is reduced into its base form and reduced POS tag as explained in (de Gispert, 2005). This transformation is only done for the alignment, and its goal is to simplify the work of the word alignment improving its quality. Block reordering (br) The difference in word order between two languages is one of the most significant sources of error in SMT. Related works either deal with reordering in general as (Kanthak et al., 2005) or deal with local reordering as (Tillmann and Ney, 2003). We report a local reordering technique, which is implemented as a preprocessing stage, with two applications: (1) to improve only alignment quality, and (2) to improve alignment quality and to infer reordering in translation. Here, we present a short explanation of the algorithm, for further details see Costa-juss`a and Fonollosa (2006). 142 Proceedings of the Workshop on Statistical Machine Translation, pages 142–145, c New York City, June 2006. 2006 Association for Computational Linguistics of the bilingual phrase, and no word on ei"
W06-3120,W06-3114,0,0.0221223,"ts to observe its efficiency in all the pairs used in this evaluation. The rgraph has been applied in those cases where: we do not use br2 (there is no sense in applying them simultaneously); and we have the tagger for the source language model available. In the case of the pair GeEn, we have not experimented any reordering, we left the application of both reordering approaches as future work. 3.3 Discussion Table 2 presents the BLEU scores evaluated on the test set (using TRUECASE) for each configuration. The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006). For both, Es2En and Fr2En tasks, br helps slightly. The improvement of the approach depends on the quality of the alignment. The better alignments allow to extract higher quality Alignment Blocks (Costa-juss`a and Fonollosa, 2006). The En2Es task is improved when adding both br1 and rgraph. Similarly, the En2Fr task seems to perform fairly well when using the rgraph. In this case, the improvement of the approach depends on the quality of the alignment patterns (Crego et al., 2006). However, it has the advantage of delaying the final decision of reordering to the overall search, where all mod"
W06-3120,N03-1017,0,0.00728769,"o infer reordering in translation. Here, we present a short explanation of the algorithm, for further details see Costa-juss`a and Fonollosa (2006). 142 Proceedings of the Workshop on Statistical Machine Translation, pages 142–145, c New York City, June 2006. 2006 Association for Computational Linguistics of the bilingual phrase, and no word on either side of the phrase is aligned to a word out of the phrase. We limit the maximum size of any given phrase to 7. The huge increase in computational and storage cost of including longer phrases does not provide a significant improvement in quality (Koehn et al., 2003) as the probability of reappearance of larger phrases decreases. 2.3 Figure 1: Example of an Alignment Block, i.e. a pair of consecutive blocks whose target translation is swapped This reordering strategy is intended to infer the most probable reordering for sequences of words, which are referred to as blocks, in order to monotonize current data alignments and generalize reordering for unseen pairs of blocks. Given a word alignment, we identify those pairs of consecutive source blocks whose translation is swapped, i.e. those blocks which, if swapped, generate a correct monotone translation. Fi"
W06-3120,J04-4002,0,0.0268059,"created). Based on this information, the source side of the bilingual corpora are reordered. In case of applying the reordering technique for purpose (1), we modify only the source training corpora to realign and then we recover the original order of the training corpora. In case of using Block Reordering for purpose (2), we modify all the source corpora (both training and test), and we use the new training corpora to realign and build the final translation system. 2.2 Phrase Extraction Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in Och and Ney (2004). A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: words are consecutive along both sides 143 Feature functions Conditional and posterior probability (cp, pp) Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions. The target language model (lm) consists of an n-gram model, in which the probability of a translation hypothesis is approximated by the product of word n-gram probabilities. As default language model feature, we use a standard word-base"
W06-3120,carreras-etal-2004-freeling,0,\N,Missing
W06-3120,J03-1005,0,\N,Missing
W06-3120,N04-1033,0,\N,Missing
W06-3125,W05-0823,1,0.872672,"Missing"
W06-3125,A00-1031,0,0.117706,"d over the development set for each of the six translation directions considered. 163 This baseline system is actually very similar to the system used for last year’s shared task “Exploiting Parallel Texts for Statistical Machine Translation” of ACL’05 Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond (Banchs et al., 2005), whose results are available at: http://www.statmt.org/wpt05/ mt-shared-task/. A more detailed description of the system can be found in (2005). The tools used for POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English. All language models were estimated using the SRI language modeling toolkit. Word-to-word alignments were extracted with GIZA++. Improvements in word-toword alignments were achieved through verb group classification as described in (de Gispert, 2005). 3 Reordering Framework In this section we outline the reordering framework used for the experiments (Crego and Mari˜no, 2006). A highly constrained reordered search is performed by means of a set of reordering patterns (linguistically motivated rewrite patterns) which are used to extend the monotone search graph with additional arcs."
W06-3125,carreras-etal-2004-freeling,0,0.0548444,"Missing"
W06-3125,W06-3120,1,0.883993,"Missing"
W06-3125,N04-1033,0,0.0842803,"Missing"
W06-3125,P05-2012,1,0.901179,"Missing"
W06-3125,N03-1017,0,0.00542142,"luation with a tagged target language model (using Part-Of-Speech tags). For both Spanish-English translation directions and the English-to-French translation task, the baseline system allows for linguistically motivated sourceside reorderings. 2 Baseline N-gram-based SMT System 1 Introduction The statistical machine translation approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams (de Gispert and Mari˜no, 2002). This translation model differs from the well known phrase-based translation approach (Koehn et al., 2003) in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This translation approach is described in detail in (Mari˜no et al., 2005). For those translation tasks with Spanish or English as target language, an additional tagged (usAs already mentioned, the translation model used here is based on bilingual n-grams. It actually constitutes a language model of bilingual units, referred to as tuples, which approximates the joint probability between source and target languages by us"
W06-3125,2005.mtsummit-papers.36,1,0.909254,"Missing"
W06-3125,J93-2003,0,\N,Missing
W07-0720,W06-3125,1,0.849606,"Missing"
W07-0720,W06-1609,1,0.795127,"Missing"
W07-0720,W06-3114,0,0.0213063,"this system participation in the ACL 2007 SECOND WORK SHOP ON STATISTICAL MACHINE TRANSLA TION . Results on three pairs of languages are reported, namely from Spanish, French and German into English (and the other way round) for both the in-domain and out-of-domain tasks. 2 Baseline N-gram-based SMT System 1 Introduction Based on estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternatively to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in (Koehn and Monz, 2005; Koehn and Monz, 2006). Given the challenge of domain adaptation, efforts have been focused on improving strategies for Ngram-based SMT which could generalize better. Specifically, a novel reordering strategy is explored. It is based on extending the search by using precomputed statistical information. Results are promising while keeping computational expenses at a similar level as monotonic search. Additionally, a bonus for tuples from the out-of-domain corpus is The translation model is based on bilingual n-grams. It actually constitutes a language model of bilingual units, referred to as tuples, which approximat"
W07-0720,J06-4004,1,0.847357,"Missing"
W07-0720,E99-1010,0,0.731366,"smaller tuples which reduces the translation vocabulary sparseness. These new tuples are used to build the SMT system. 3 Baseline System Enhanced with a Weighted Reordering Input Graph This section briefly describes the statistical machine reordering (SMR) technique. Further details on the architecture of SMR system can be found on (Costa-juss`a and Fonollosa, 2006). 3.1 Concept The SMR system can be seen as a SMT system which translates from an original source language (S) to a reordered source language (S’), given a target language (T). The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). Figure 1: SMR approach in the (A) training step (B) in the test step (the weight of each arch is in brackets). 3.2 Using SMR technique to improve SMT training The original source corpus S is translated into the reordered source corpus S’ with the SMR system. Figure 1 (A) shows the corresponding block diagram. The reordered training source corpus and the original training target corpus are used to build the SMT system. The main difference here is that the training is computed with the S’2T task instead of"
W07-0720,W05-0820,0,\N,Missing
W08-0315,W08-0315,1,0.0512755,"Missing"
W08-0315,J90-2002,0,0.809551,"Missing"
W08-0315,W07-0718,0,0.152941,"Missing"
W08-0315,carreras-etal-2004-freeling,0,0.138533,"Missing"
W08-0315,W06-3114,0,0.151633,"Missing"
W08-0315,P00-1056,0,0.073606,"Missing"
W08-0315,J04-4002,0,0.0735646,"Missing"
W08-0315,W05-0820,0,\N,Missing
W08-0315,A00-1031,0,\N,Missing
W08-0315,J06-4004,1,\N,Missing
W09-0414,J04-4002,0,0.0266217,"is translated into target language using translation table, (3) the target phrases are reordered to be inherent in the target language. A bilingual phrase (which in the context of SMT do not necessarily coincide with their linguistic analogies) is any pair of m source words and n target words that satisfies two basic constraints: (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. Given a sentence pair and a corresponding wordto-word alignment, phrases are extracted following the criterion in (Och and Ney, 2004). The probability of the phrases is estimated by relative frequencies of their appearance in the training corpus. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85–89, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 85 Classically, a phrase-based translation system implements a log-linear model in which a foreign language sentence f1J = f1 , f2 , ..., fJ is translated into another language eI1 = e1 , e2 , ..., eI by searching for the translation hypothesis eˆI1 maximizing a log-linear combination of several feature model"
W09-0414,J90-2002,0,0.572532,"he probability of the phrases is estimated by relative frequencies of their appearance in the training corpus. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85–89, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 85 Classically, a phrase-based translation system implements a log-linear model in which a foreign language sentence f1J = f1 , f2 , ..., fJ is translated into another language eI1 = e1 , e2 , ..., eI by searching for the translation hypothesis eˆI1 maximizing a log-linear combination of several feature models (Brown et al., 1990): eˆI1 = arg max eI1 ( M X w P (w) = λEuroparl · PEuroparl + λN C · PNwC (1) w and PNwC are probabilities aswhere PEuroparl signed to the word sequence w by the LM estimated on Europarl and NC data, respectively. The scale factor values are automatically optimized to obtain the lowest perplexity ppl(w) produced by the interpolated LM P (w). We used the standard script compute − best − mix from the SRI LM package (Stolcke, 2002) for optimization. On the next step, the optimized coefficients λEuroparl and λN C are generalized on the interpolated translation and reordering models. In other words,"
W09-0414,W07-0717,0,0.0343889,"he corresponding improvement in BLEU score is presented in Section 3.3 and summary of the obtained results (Table 4). ) λm hm (eI1 , f1J ) m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. 2.1 Translation models interpolation We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estève, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. In the framework of the evaluation campaign, there were two Spanish-to-English parallel training corpora available: Europarl v.4 corpus (about 50M tokens) and News Commentary (NC) corpus (about 2M tokens). The test dataset provided by the organizers this year was from the news domain, so we considered the Europarl training corpus as &quot;out-of-domain&quot; data and the News Commentary as &quot;in-domain&quot; training material. Unfortunately, the in-domain corpus is much smaller in size, however the Europar"
W09-0414,W07-0733,0,0.0262365,"l LMs and the 2009 development set (English and Spanish references) can be found in Table 1, while the corresponding improvement in BLEU score is presented in Section 3.3 and summary of the obtained results (Table 4). ) λm hm (eI1 , f1J ) m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. 2.1 Translation models interpolation We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estève, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. In the framework of the evaluation campaign, there were two Spanish-to-English parallel training corpora available: Europarl v.4 corpus (about 50M tokens) and News Commentary (NC) corpus (about 2M tokens). The test dataset provided by the organizers this year was from the news domain, so we considered the Europarl training corpus as &quot;out-of-domain&quot; data and the News Commentary as &quot;in-domain&quot; tra"
W09-0414,P07-2045,0,0.00511337,"investigate the translation models (TMs) interpolation for a state-of-the-art phrase-based translation system. Inspired by the work presented in (Schwenk and Estève, 2008), we attack this challenge using the coefficients obtained for the corresponding monolingual language models (LMs) for TMs interpolation. On the second step, we have performed additional word reordering experiments, comparing the results obtained with a statistiTALP-UPC phrase-based SMT The system developed for this year’s shared task is based on a state-of-the-art SMT system implemented within the open-source MOSES toolkit (Koehn et al., 2007). A phrase-based translation is considered as a three step algorithm: (1) the source sequence of words is segmented in phrases, (2) each phrase is translated into target language using translation table, (3) the target phrases are reordered to be inherent in the target language. A bilingual phrase (which in the context of SMT do not necessarily coincide with their linguistic analogies) is any pair of m source words and n target words that satisfies two basic constraints: (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned t"
W09-0414,N04-1022,0,\N,Missing
W09-0414,2009.eamt-1.27,1,\N,Missing
W10-1712,J93-1007,0,0.542371,"Missing"
W10-1712,2010.eamt-1.17,1,0.900144,"els among others. Introduction 1 in coop2 3 eration with BMIC and VMU participated in the The TALP Research Center of the UPC 3 Collocation segmentation Collocation segmentation is the process of deSpanish-to-English WMT task. Our primary subtecting boundaries between collocation segments mission was a phrase-based SMT system enhanced within a text (Daudaravicius and Marcinkeviciene, with POS tags and our contrastive submission was 2004). A collocation segment is a piece of text bean augmented phrase-based system using collocatween boundaries. The boundaries are established tion segmentation (Costa-jussà et al., 2010), which in two steps using two dierent measures: the Dice mainly is a way of introducing new phrases in the score and a Average Minimum Law (AML). translation table. This paper presents the descripThe Dice score is used to measure the association of both systems together with the results that tion strength between two words. It has been used we obtained in the evaluation task and is organized before in the collocation compiler XTract (Smadja, as follows: rst, Section 2 and 3 present a brief de1993) and in the lexicon extraction system Chamscription of a phrase-based SMT, followed by a genpol"
W10-1712,N04-4026,0,0.143207,"Missing"
W10-1712,P07-2045,0,0.00766655,"Missing"
W10-1712,2005.mtsummit-papers.11,0,0.00867525,"lues. 4 Ocial test sent Internal test Ocial test 137 369 408 213 119 1246 72 188 2, 106 128 168 2662 The language models were built using SRILM (Stolcke, Table 2: 2002). ocial test sets 4.1 Corpus Unknown words found in internal and It is important to notice that neither the United This year, the translation task provided four difNations nor the Gigaword corpus were used for ferent sources to collect corpora for the Spanishbilingual training. Nevertheless, the English part English pair. Bilingual corpora included version 5 from the United Nations and the monolingual of the Europarl Corpus (Koehn, 2005), the News News corpus were used to build the language model Commentary corpus and the United Nations corof our systems. pus. Additional English corpora was available from the News corpus. The organizers also allowed the 4.1.1 Unknown words use of the English Gigaword Third and Fourth EdiWe analyzed the content from the internal and oftion, released by the LDC. As for development cial test and realized that they both contained and internal test, the test sets from 2008 and 2009 many words that were not seen in the training data. translation tasks were available. Table 2 shows the number of un"
W10-1712,J03-1002,0,0.0100814,"peration with BMIC2 and Bilingual phrases are translation VMU . In phrase-based SMT, the phrase units that contain source words and target words, table is the main tool in translation. It is e.g. created extracting phrases from an aligned and have dierent scores associated to them. These parallel corpus and then computing transbilingual phrases are then sorted in order to maxlation model scores with them. Performing imize a linear combination of feature functions. a collocation segmentation over the source Such strategy is known as the log-linear model and target corpus before the alignment (Och and Ney, 2003) and it is formally dened as: < unidad de traducci´ on |translation unit >, causes that dierent and larger phrases "" are extracted from the same original doceˆ = arg max uments. We performed this segmentation e and used the union of this phrase set with 1 M X λm hm (e, f ) (1) m=1 the phrase set extracted from the nonwhere segmented corpus to compute the phrase weights table. We present the congurations conare the translation model (TM) and the target sidered and also report results obtained language model (LM). Additional models include with internal and ocial test sets. POS target langua"
W10-1712,J96-1001,0,\N,Missing
W10-1712,A00-1031,0,\N,Missing
W10-1712,W09-0414,1,\N,Missing
W10-1712,W07-0702,0,\N,Missing
W12-3133,P08-1087,0,0.0134042,"$* Figure 2: Above, flow diagram of the training of simplified morphology translation models. Below, Spanish morphology generation as an independent classification task. Type PLAIN TARGET: TARGET+PoS (Gen. Sur.): TARGET+PoS (Simpl. PoS): Text la Comisi´on puede llegar a paralizar el programa la Comisi´on VMIP3S0[poder] llegar a paralizar el programa la Comisi´on VMIPpn0[poder] llegar a paralizar el programa Table 4: Example of morphology simplification steps taken for Spanish verbs. be summarized in four categories: i) factored models (Koehn and Hoang, 2007), enriched input models (Avramidis and Koehn, 2008; Ueffing and Ney, 2003), segmented translation (Virpioja et al., 2007) and morphology generation (Toutanova et al., 2008; de Gispert and Mari˜no, 2008). Our strategy for dealing with morphology generation is based in the latter approach (de Gispert and Mari˜no, 2008) (Figure 2). We center our strategy in simplifying only verb forms as previous studies indicate that they contribute to the main improvement (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). That strategy makes clear the real impact of morphology simplification by providing an upper bound oracle for the studied scenarios. The"
W12-3133,J90-2002,0,0.616365,"s. For the Spanish to English task we submitted a baseline system that uses all parallel training data and a combination of different target language models (LM) and Part-Of-Speech (POS) language models. A similar configuration was submitted for the Baseline system: Phrase-Based SMT Classically, a phrase-based translation system implements a log-linear model in which a foreign language sentence f1j = f1 , f2 , . . . , fj is translated into another language sentence eI1 = e1 , e2 , . . . , eI by searching for the translation hypothesis that maximizes a log-linear combination of feature models (Brown et al., 1990): eˆI1 = arg max eI1 ( M X I J m hm e 1 , f 1 m=1 ) (1) where the separate feature functions hm refer to the system models and the set of m refers to the weights corresponding to these models. As feature functions we used the standard models available 275 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 275–282, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics the$ NATO$ mission$ oﬃcially$ Corpus ended$ EPPS la$ DAFS$ misión$ NCFS$ de$ SPS$ la$ DAFS$ OTAN$ NP$ terminó$ VMIS3S0$ oﬁcialmente$ RG$ News.Com UN Sent. Eng Spa Eng Spa Eng Sp"
W12-3133,W07-0722,0,0.0230476,"ut-ofdomain corpus (in our case the European Parliament) and we aim to adapt to another domain that has limited data, either only monolingual or hopefully bilingual as well. The WMT Translation Task focuses on adapting the system to a news domain, offering an in-domain parallel corpus to work with. In case of additional target monolingual data, previous works have focused on language model interpolations (Bulyko et al., 2007; Mohit et al., 2009; Wu et al., 2008). When parallel in-domain data is available, the latest researches have focused on mixture model adaptation of the translation model (Civera and Juan, 2007; Foster and Kuhn, 2007; Foster et al., 2010). Our work is closer to the latest approaches. We used the in-domain parallel data to adapt the translation model, but focusing on the decoding errors that the out-of-domain baseline system made while translating the in-domain corpus. The idea is to detect where the system made its mistakes and use the in-domain data to teach it how to correct them. Our approach began with a baseline system built with the Parliament and the United Nations parallel corpora but without the News parallel corpus. The rest of the configuration remained the same for the b"
W12-3133,P11-1004,0,0.0153204,"eration to deal with the problems raised by morphological rich languages such as Spanish. Second we present a domain adaptation strategy that consists in deriving new units into the phrase-table. 3.2 Morphology simplification The first improvement strategy is based on morphology simplification when translating from English to Spanish. The problems raised when translating from a language such as English into richer morphology languages are well known and are a research line of interest nowadays (Popovic and Ney, 2004; Koehn and Hoang, 2007; de Gispert and Mari˜no, 2008; Toutanova et al., 2008; Clifton and Sarkar, 2011). In that direction, inflection causes a very large targetlanguage lexicon with a significant data sparsity problem. In addition, system output is limited only to the inflected phrases available in the parallel training corpus. Hence, SMT systems cannot generate proper inflections unless they have learned them from the appropriate phrases. That would require to have a parallel corpus containing all possible word inflections for all phrases available, which it is an unfeasible task. The morphology related problems in MT have been addressed from different approaches and may"
W12-3133,W11-2107,0,0.213955,"Missing"
W12-3133,W07-0717,0,0.0268143,"our case the European Parliament) and we aim to adapt to another domain that has limited data, either only monolingual or hopefully bilingual as well. The WMT Translation Task focuses on adapting the system to a news domain, offering an in-domain parallel corpus to work with. In case of additional target monolingual data, previous works have focused on language model interpolations (Bulyko et al., 2007; Mohit et al., 2009; Wu et al., 2008). When parallel in-domain data is available, the latest researches have focused on mixture model adaptation of the translation model (Civera and Juan, 2007; Foster and Kuhn, 2007; Foster et al., 2010). Our work is closer to the latest approaches. We used the in-domain parallel data to adapt the translation model, but focusing on the decoding errors that the out-of-domain baseline system made while translating the in-domain corpus. The idea is to detect where the system made its mistakes and use the in-domain data to teach it how to correct them. Our approach began with a baseline system built with the Parliament and the United Nations parallel corpora but without the News parallel corpus. The rest of the configuration remained the same for the baseline. With this alte"
W12-3133,W06-1607,0,0.0526363,"15 M 8.38 M Words 49.40 M 52.66 M 3.73 M 4.33 M 205.68 M 239.40 M Vocab. 124.03 k 154.67 k 62.70 k 73.97 k 575.04 k 598.54 k avg.len. 26.05 27.28 24.20 28.09 24.54 28.56 Figure 1: Factored phrase-based MT based on translation from surface to surface and Part-of-Speech Table 1: English-Spanish corpora statistics for NAACL-WMT 2012 after cleaning process on Moses, i.e., relative frequencies, lexical weights, word and phrase penalty, wbe-msd-bidirectional-fe reordering models and two language models, one for surface and one for POS tags. Phrase scoring was computed using Good-Turing discounting (Foster et al., 2006). The tuning process was done using MERT (Och, 2003) with Minimum Bayes-Risk decoding (MBR) (Kumar and Bryne, 2004) on Moses and focusing on minimizing the BLEU score (Papineni et al., 2002) of the development set. Final translations were also computed using MBR decoding. Additionally to the settings mentioned before, we worked with a factored version of the corpus. Factored corpora augments surface forms with additional information, such as POS tags or lemmas as shown in Figure 1. In that case, factors other than surface (e.g. POS) are usually less sparse, allowing to build factor-specific la"
W12-3133,D10-1044,0,0.0582046,"Missing"
W12-3133,2011.eamt-1.18,1,0.812022,"Missing"
W12-3133,D07-1091,0,0.0297742,"e present an improvement strategy based on morphology simplification plus generation to deal with the problems raised by morphological rich languages such as Spanish. Second we present a domain adaptation strategy that consists in deriving new units into the phrase-table. 3.2 Morphology simplification The first improvement strategy is based on morphology simplification when translating from English to Spanish. The problems raised when translating from a language such as English into richer morphology languages are well known and are a research line of interest nowadays (Popovic and Ney, 2004; Koehn and Hoang, 2007; de Gispert and Mari˜no, 2008; Toutanova et al., 2008; Clifton and Sarkar, 2011). In that direction, inflection causes a very large targetlanguage lexicon with a significant data sparsity problem. In addition, system output is limited only to the inflected phrases available in the parallel training corpus. Hence, SMT systems cannot generate proper inflections unless they have learned them from the appropriate phrases. That would require to have a parallel corpus containing all possible word inflections for all phrases available, which it is an unfeasible task. The morphology related problems"
W12-3133,P07-2045,0,0.00929111,"eLing (Padr´o et al., 2010). Freeling tokenization is able to deal with contractions (“del” ! “de el”) and clitics separation (“c´ompramelo” ! “compra me lo”) in Spanish and English. Stemming was performed using Snowball (Porter, 2001). Surface text was lowercased conditionally based on the POS tagging: proper nouns and adjectives were separated from other POS categories to determine if a string should be fully lowercased (no special property), partially lowercased (proper noun or adjective) or not lowercased at all (acronym). Bilingual corpora were cleaned with cleancorpus-n script of Moses (Koehn et al., 2007) removing all sentence pair with more than 70 words in any language, considering the already tokenized data. That script also ensures a maximum length ratio below of nine (9) words between source and target sentences. Postprocessing in both languages consisted of a recasing step using Moses recaser script. Furthermore we built an additional script in order to check the casing of output names with respect to source sentence names and case them accordingly, with exception of names placed at beginning of the sentence. After recasing, a final detokenization step was performed using standard Moses"
W12-3133,2005.mtsummit-papers.11,0,0.0149384,"odel. Initially, a LM was built for every corpus and then they were combined to produce de final LM. Table 2 presents the statistics of each corpora, again after the cleaning process. 2.1 For internal testing we used the News 2011’s data and concatenated the remaining three years of News data as a single parallel corpus for development. Table 3 shows the statistics for these two sets and includes in the last rows the statistics of the official test set for this year’s translation task. Corpus used The baseline system was trained using all parallel corpora, i.e. the European Parliament (EPPS) (Koehn, 2005), News Commentary and United Nations. Table 1 shows the statistics of the training data after the cleaning process described later on Subsection 2.2. Regarding the monolingual data, there was also more News corpora separated by years for Spanish and English and there was the Gigaword monolingual corpus for English. All data can be found on the Translation Task’s website1 . We used all News corpora (and Gigaword for English) to build the lan1 http://www.statmt.org/wmt12/translation-task.html 276 Corpus EPPS News.Com. UN News.07 News.08 News.09 News.10 News.11 Giga Eng Spa Eng Spa Eng Spa Eng Sp"
W12-3133,N04-1022,0,0.0330698,"k 598.54 k avg.len. 26.05 27.28 24.20 28.09 24.54 28.56 Figure 1: Factored phrase-based MT based on translation from surface to surface and Part-of-Speech Table 1: English-Spanish corpora statistics for NAACL-WMT 2012 after cleaning process on Moses, i.e., relative frequencies, lexical weights, word and phrase penalty, wbe-msd-bidirectional-fe reordering models and two language models, one for surface and one for POS tags. Phrase scoring was computed using Good-Turing discounting (Foster et al., 2006). The tuning process was done using MERT (Och, 2003) with Minimum Bayes-Risk decoding (MBR) (Kumar and Bryne, 2004) on Moses and focusing on minimizing the BLEU score (Papineni et al., 2002) of the development set. Final translations were also computed using MBR decoding. Additionally to the settings mentioned before, we worked with a factored version of the corpus. Factored corpora augments surface forms with additional information, such as POS tags or lemmas as shown in Figure 1. In that case, factors other than surface (e.g. POS) are usually less sparse, allowing to build factor-specific language models with higherorder n-grams. These higher-order language models usually help to obtain more syntacticall"
W12-3133,2009.eamt-1.22,0,0.0247568,"10). 3.3 Domain adaptation Depending on the available resources, different domain adaptation techniques are possible. Usually, the baseline system is built with a large out-ofdomain corpus (in our case the European Parliament) and we aim to adapt to another domain that has limited data, either only monolingual or hopefully bilingual as well. The WMT Translation Task focuses on adapting the system to a news domain, offering an in-domain parallel corpus to work with. In case of additional target monolingual data, previous works have focused on language model interpolations (Bulyko et al., 2007; Mohit et al., 2009; Wu et al., 2008). When parallel in-domain data is available, the latest researches have focused on mixture model adaptation of the translation model (Civera and Juan, 2007; Foster and Kuhn, 2007; Foster et al., 2010). Our work is closer to the latest approaches. We used the in-domain parallel data to adapt the translation model, but focusing on the decoding errors that the out-of-domain baseline system made while translating the in-domain corpus. The idea is to detect where the system made its mistakes and use the in-domain data to teach it how to correct them. Our approach began with a base"
W12-3133,J03-1002,0,0.00485021,"Missing"
W12-3133,P03-1021,0,0.0060492,"0 M Vocab. 124.03 k 154.67 k 62.70 k 73.97 k 575.04 k 598.54 k avg.len. 26.05 27.28 24.20 28.09 24.54 28.56 Figure 1: Factored phrase-based MT based on translation from surface to surface and Part-of-Speech Table 1: English-Spanish corpora statistics for NAACL-WMT 2012 after cleaning process on Moses, i.e., relative frequencies, lexical weights, word and phrase penalty, wbe-msd-bidirectional-fe reordering models and two language models, one for surface and one for POS tags. Phrase scoring was computed using Good-Turing discounting (Foster et al., 2006). The tuning process was done using MERT (Och, 2003) with Minimum Bayes-Risk decoding (MBR) (Kumar and Bryne, 2004) on Moses and focusing on minimizing the BLEU score (Papineni et al., 2002) of the development set. Final translations were also computed using MBR decoding. Additionally to the settings mentioned before, we worked with a factored version of the corpus. Factored corpora augments surface forms with additional information, such as POS tags or lemmas as shown in Figure 1. In that case, factors other than surface (e.g. POS) are usually less sparse, allowing to build factor-specific language models with higherorder n-grams. These higher"
W12-3133,padro-etal-2010-freeling,0,0.0632915,"Missing"
W12-3133,P02-1040,0,0.0865944,"phrase-based MT based on translation from surface to surface and Part-of-Speech Table 1: English-Spanish corpora statistics for NAACL-WMT 2012 after cleaning process on Moses, i.e., relative frequencies, lexical weights, word and phrase penalty, wbe-msd-bidirectional-fe reordering models and two language models, one for surface and one for POS tags. Phrase scoring was computed using Good-Turing discounting (Foster et al., 2006). The tuning process was done using MERT (Och, 2003) with Minimum Bayes-Risk decoding (MBR) (Kumar and Bryne, 2004) on Moses and focusing on minimizing the BLEU score (Papineni et al., 2002) of the development set. Final translations were also computed using MBR decoding. Additionally to the settings mentioned before, we worked with a factored version of the corpus. Factored corpora augments surface forms with additional information, such as POS tags or lemmas as shown in Figure 1. In that case, factors other than surface (e.g. POS) are usually less sparse, allowing to build factor-specific language models with higherorder n-grams. These higher-order language models usually help to obtain more syntactically correct output. Concretely we map input source surfaces to target surface"
W12-3133,popovic-ney-2004-towards,0,0.0328289,"ent strategies. First we present an improvement strategy based on morphology simplification plus generation to deal with the problems raised by morphological rich languages such as Spanish. Second we present a domain adaptation strategy that consists in deriving new units into the phrase-table. 3.2 Morphology simplification The first improvement strategy is based on morphology simplification when translating from English to Spanish. The problems raised when translating from a language such as English into richer morphology languages are well known and are a research line of interest nowadays (Popovic and Ney, 2004; Koehn and Hoang, 2007; de Gispert and Mari˜no, 2008; Toutanova et al., 2008; Clifton and Sarkar, 2011). In that direction, inflection causes a very large targetlanguage lexicon with a significant data sparsity problem. In addition, system output is limited only to the inflected phrases available in the parallel training corpus. Hence, SMT systems cannot generate proper inflections unless they have learned them from the appropriate phrases. That would require to have a parallel corpus containing all possible word inflections for all phrases available, which it is an unfeasible task. The morph"
W12-3133,2006.amta-papers.25,0,0.0252999,"us consists of the source side, the output translation and the target side, also called the target correction. The output translation and its reference are then compare to detect possible mistakes that the system caused during decoding. The translation was used as a pivot to find a wordto-word alignment between the source side and the target correction. The word-to-word alignment between source side and translation was provided by Moses during decoding. The word-to-word alignment between the output translation and target correction was obtained following these steps: 1. Translation Edit Rate (Snover et al., 2006) between each output translation and target correction sentence pair was computed to obtain its edit path and detect which words do not change between sentences. Words that did not change were directly linked 2. Going from left to right, for each unaligned word wout on the output translation sentence and each word wtrg on the target correction sentence, a similarity function was computed between them and wout got aligned with the word wtrg that maximized this similarity. The similarity function was defined as a linear combination of features that considered if the words wout and wtrg were iden"
W12-3133,P08-1059,0,0.116941,"simplification plus generation to deal with the problems raised by morphological rich languages such as Spanish. Second we present a domain adaptation strategy that consists in deriving new units into the phrase-table. 3.2 Morphology simplification The first improvement strategy is based on morphology simplification when translating from English to Spanish. The problems raised when translating from a language such as English into richer morphology languages are well known and are a research line of interest nowadays (Popovic and Ney, 2004; Koehn and Hoang, 2007; de Gispert and Mari˜no, 2008; Toutanova et al., 2008; Clifton and Sarkar, 2011). In that direction, inflection causes a very large targetlanguage lexicon with a significant data sparsity problem. In addition, system output is limited only to the inflected phrases available in the parallel training corpus. Hence, SMT systems cannot generate proper inflections unless they have learned them from the appropriate phrases. That would require to have a parallel corpus containing all possible word inflections for all phrases available, which it is an unfeasible task. The morphology related problems in MT have been addressed from different approaches an"
W12-3133,2007.mtsummit-papers.65,0,0.0443673,"Missing"
W12-3133,C08-1125,0,0.0365424,"Missing"
W12-3133,2011.eamt-1.20,1,\N,Missing
W13-2215,W11-2107,0,0.0192041,"E models we used the data from the WMT13 shared task on quality estimation (System Selection Quality Estimation at Sentence Level task5 ), which contains the test sets from other WMT campaigns with human assessments. We used five groups of features, namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the follow"
W13-2215,W12-3133,1,0.860966,"Missing"
W13-2215,2012.amta-monomt.1,1,0.796291,"Missing"
W13-2215,2013.mtsummit-papers.9,1,0.774503,"Missing"
W13-2215,padro-etal-2010-freeling,0,0.0202206,"Missing"
W13-2215,W06-1607,0,0.0198766,"s with additional information, such as POS tags or lemmas. In that case, factors other than surface (e.g. POS) are usually less sparse, allowing the construction of factor-specific language models with higher-order n-grams. Such language models can help to obtain syntactically more correct outputs. We used the standard models available in Moses as feature functions: relative frequencies, lexical weights, word and phrase penalties, wbe-msdbidirectional-fe reordering models, and two language models (one for surface and one for POS tags). Phrase scoring was computed using GoodTuring discounting (Foster et al., 2006). As aforementioned, we developed five factored Moses-based independent systems with different 134 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 134–140, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the"
W13-2215,P02-1040,0,0.0862502,"on Statistical Machine Translation, pages 134–140, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation"
W13-2215,D08-1090,0,0.0227644,"namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the following parameters: linear kernel, expanding the working set by 9 variables at each iteration, for a maximum of 50,000 iterations and with a cache size of 100 for kernel evaluations. The trade-off parameter was empirically set to 0.001. Table 2 sh"
W13-2215,2011.eamt-1.18,1,0.897081,"Missing"
W13-2215,P10-1063,0,0.0150842,"task5 ), which contains the test sets from other WMT campaigns with human assessments. We used five groups of features, namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the following parameters: linear kernel, expanding the working set by 9 variables at each iteration, for a maximum of 50,000 iterations and"
W13-2215,D11-1125,0,0.0166218,"nal Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation Task’s website1 . We used all the News corpora to busld the language model (LM). Firstly, a LM was built for every cor"
W13-2215,P07-2045,0,0.10399,"such as morphology generation, training sentence filtering, and domain adaptation through unit derivation. The results show a coherent improvement on TER, METEOR, NIST, and BLEU scores when compared to our baseline system. 1 Introduction 2 The TALP-UPC center (Center for Language and Speech Technologies and Applications at Universitat Polit`ecnica de Catalunya) focused on the English to Spanish translation of the WMT13 shared task. Our primary (contrastive) run is an internal system selection comprised of different training approaches (without CommonCrawl, unless stated): (a) Moses Baseline (Koehn et al., 2007b), (b) Moses Baseline + Morphology Generation (Formiga et al., 2012b), (c) Moses Baseline + News Adaptation (Henr´ıquez Q. et al., 2011), (d) Moses Baseline + News Adaptation + Morphology Generation , and (e) Moses Baseline + News Adaptation + Filtered CommonCrawl Adaptation (Barr´on-Cede˜no et al., 2013). Our secondary run includes is the full training strategy marked as (e) in the previous description. The main differences with respect to our last year’s participation (Formiga et al., 2012a) are: i) the inclusion of the CommonCrawl corpus, using Baseline system: Phrase-Based SMT Our contrib"
W13-2215,2005.mtsummit-papers.11,0,0.0437825,"s for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation Task’s website1 . We used all the News corpora to busld the language model (LM). Firstly, a LM was built for every corpus independently. Afterwards, they were combined to produce de final LM. For internal testing we used the News 2011 and News 2012 data and concatenated the remaining three years of News data as a single parallel corpus for development. We processed the corpora as in our participation to WMT12 (Formiga et al"
W13-2215,P03-1021,0,\N,Missing
W13-2215,2011.eamt-1.20,1,\N,Missing
