2005.iwslt-1.27,W99-0604,0,0.0401063,"common and something different. The important common feature is to use bilingual corpus, or translation examples, for the translation of new inputs. Both methods exploit translation knowledge implicitly embedded in translation examples, and make MT system maintenance and improvement much easier compared with Rule-Based Machine Translation. The difference is that SMT supposes bilingual corpus is the only available resource (but not a bilingual lexicon and parsers); EBMT does not consider such a constraint. SMT basically combines words or phrases (relatively small pieces) with high probability [2]; EBMT tries to use larger translation examples. When EBMT tries to use larger examples, it had better handle examples which are discontinuous as a word-string, but continuous structurally. Accordingly, though it is not inevitable, EBMT naturally seeks syntactic information. The difference in the problem setting is important. SMT is a natural approach when linguistic resources such as parsers and a bilingual lexicon are not available. On the other hand, in case of such linguistic resources are available, it is also natural to see how accurate MT can be achieved using all the available resource"
2005.iwslt-1.27,J94-4001,1,0.547121,"Missing"
2005.iwslt-1.27,A00-2018,0,0.0468239,"Missing"
2005.iwslt-1.27,J03-1002,0,0.00380044,"other remaining nodes are merged into correspondences of their parent (or ancestor) nodes. In the case of Figure 1, “あの (that)” is merged into the correspondence “ 車 (car) ↔ the car”, since it is within an NP. Then, “突然 (suddenly)”, “at me” and “from the side” are merged into their parent correspondence, “ 飛び出して来 たのです (rush out) ↔ came”. We call the correspondences constructed so far as basic correspondences. 2.5. Comparison with EM based Alignment Here, let us compare our alignment method with an EM based alignment. We tested an EM based tool, giza++ for the alignment of 20,000 training data [6]. We found many inappropriate word alignments in the giza++ results, and concluded that this size of training data might be too small for EM based alignment. On the other hand, our method using a 0.9M-entry bilingual dictionary and a transliteration module could find correspondences quite accurately. For the given training set, we could conclude that our proposed method is superior to the EM based method. However, the correspondence statistics in the whole training data must be an important information, and it is our future target to use a flat bilingual dictionary and the statistical informat"
2005.iwslt-1.27,2005.mtsummit-papers.29,1,0.795144,"Missing"
2005.iwslt-1.27,P93-1004,0,0.0606899,"that parsing might cause sideeffects and lower translation performance. As we mentioned above, parsing errors are not a principal cause of translation errors, but these are not a few. One of the possible countermeasures is to reconsider the learning process of an English parser. The English parser used here is learned from Penn Treebank, and seems to be vulnerable to conversational sentences in travel domain. Furthermore, it is quite possible to improve parsing accuracies of both languages complementarily by taking advantage of the difference of syntactic ambiguities between the two languages [9]. This approach may not substantially improve the parsing accuracy of the travel domain sentences, because of their short length, but is promising for translating longer general sentences. 7. Conclusion As we stated in Introduction, we not only aim at the development of machine translation through some evaluation measure, but also tackle this task from the comprehensive viewpoint including the development of structural NLP. The examination of translation errors revealed the problems, such as problems in parsing and inflexible matching of a Japanese input and Japanese translation examples. Reso"
2006.iwslt-evaluation.9,P93-1004,0,0.0526775,"parsing might cause sideeffects and lower translation performance. As we mentioned above, parsing errors are not a principal cause of translation errors, but these are not a few. One of the possible countermeasures is to reconsider the learning process of an English parser. The English parser used here is learned from the penn Treebank, and seems to be vulnerable to conversational sentences in travel domain. Furthermore, it is quite possible to improve parsing accuracies of both languages complementarily by taking advantage of the difference of syntactic ambiguities between the two languages [8]. This approach may not substantially improve the parsing accuracy of the travel domain sentences, because of their short length, but is promising for translating longer general sentences. Other main points are as follows: ࡎ࠹࡞1. 0 ߦ &lt;hotel> ߦ 0.99 ৻⇟ (best) 1. 0 &lt;most>0.99 &lt;nearest> ㄭ (near ) 1. 0 0.99 㚞 (station) 1. 0 ߪ ߤߎ (where) 1. 0 ߢߔ ߆ Figure 3: Example of SYNGRAPH. Table 1: JP to EN Evaluation results. Dev 1 Dev 2 Dev 3 Dev 4 Dev 4 ASR Test Test ASR BLEU 0.5087 0.4881 0.4468 0.1921 0.1590 0.1655 0.1418 NIST 9.6803 9.4918 9.1883 5.7880 5.0107 5.4325 4.8804 • Punctuation marks are remove"
2006.iwslt-evaluation.9,E06-1031,0,0.0225558,"des are basic nodes and the other nodes are SYN nodes 2 . Then, if the expression conjoining two or more nodes corresponds to one synonymous group, a SYN node will be added there. For example, in Figure 3,  nearest  is such a SYN node. Furthermore, if one SYN node has a hyper synonymous group in the synonymy database, the SYN node with the hyper SYNID is also added. In this SYNGRAPH data structure, each node has a score, NS (Node Score), which reflects how much the expression of the node is shifted from the original expression. • Automatic evaluation methods are a little advantageous to SMT [9],[10]. • The soundness of dictionaries heavily affects on the accuracy of alignment. • The extension rules of remaining nodes should be revised. • The constraint of selecting translation examples should be more robust. It is currently impossible to use ’almost equal’ exanples to the input sentence, such as those that differ perhaps only with respect to whether or not it contains a negation adverb such as ’not’. 5. Results and Discussion 6. Related Work Our Japanese-English translation system tried two tasks: manual manuscript translation and ASR output translation (for ASR output we just trans"
2006.iwslt-evaluation.9,E06-1032,0,0.0178145,"are basic nodes and the other nodes are SYN nodes 2 . Then, if the expression conjoining two or more nodes corresponds to one synonymous group, a SYN node will be added there. For example, in Figure 3,  nearest  is such a SYN node. Furthermore, if one SYN node has a hyper synonymous group in the synonymy database, the SYN node with the hyper SYNID is also added. In this SYNGRAPH data structure, each node has a score, NS (Node Score), which reflects how much the expression of the node is shifted from the original expression. • Automatic evaluation methods are a little advantageous to SMT [9],[10]. • The soundness of dictionaries heavily affects on the accuracy of alignment. • The extension rules of remaining nodes should be revised. • The constraint of selecting translation examples should be more robust. It is currently impossible to use ’almost equal’ exanples to the input sentence, such as those that differ perhaps only with respect to whether or not it contains a negation adverb such as ’not’. 5. Results and Discussion 6. Related Work Our Japanese-English translation system tried two tasks: manual manuscript translation and ASR output translation (for ASR output we just translated"
2006.iwslt-evaluation.9,W01-1402,0,0.157807,"Missing"
2006.iwslt-evaluation.9,W01-1406,0,0.0234825,"Missing"
2006.iwslt-evaluation.9,W99-0604,0,0.0355217,"tant common feature between SMT and EBMT is to use a bilingual corpus, or translation examples, for the translation of new inputs. Both methods exploit translation knowledge implicitly embedded in translation examples, and make MT system maintenance and improvement much easier compared with Rule-Based Machine Translation. On the other hand, EBMT is different from SMT in that SMT hesitates to exploit rich linguistic resources such as a bilingual lexicon and parsers; EBMT does not consider such a constraint. SMT basically combines words or phrases (relatively small pieces) with high probability [2]; EBMT tries to use larger translation examples. When EBMT tries to use larger examples, it can better handle examples which are discontinuous as a word-string, but continuous structurally. Accordingly, though it is not inevitable, EBMT can quite naturally handle syntactic information. Besides that, the difference in the problem setting between EBMT and SMT is also important. SMT is a natural approach when linguistic resources such as parsers and a bilingual lexicon are not available. On the other hand, in case that such linguistic resources are available, it is also natural to see how accurat"
2006.iwslt-evaluation.9,J94-4001,1,0.562861,"ching, we need to recognize the synonymous relations among this expressive divergence. However, the combination of synonymous expressions will cause combinatorial explosion, which makes both pre-unfolding and dynamic search are infeasible. To handle this problem, we introduce SYNGRAPH data structure which packs all synonymous expressions and can generate all the possible paraphrase sentences. Figure 3 shows an example SYNGRAPH which can generate the above paraphrases. The basis of SYNGRAPH is the dependency structure of the original sentence (so, in this paper we always employ a robust parser [3]). In the dependency structure, each 2. Extracted synonymous expressions are effectively handled by SYNGRAPH data structure, which can pack expressive divergence. A thesaurus is a knowledge source to provide synonym and hypernym-hyponym relations. However, existing thesauri are not appropriate for flexible matching. One reason is that the number of words assigned to one unit is often too large, and it is difficult to distinguish synonyms in a narrow sense from similar words. Such distinction is important for 68 there were variety of problems, such as parsing errors of both languages, excess an"
2006.iwslt-evaluation.9,A00-2018,0,0.061125,"Missing"
2006.iwslt-evaluation.9,P06-1043,0,0.0425398,"Missing"
2006.iwslt-evaluation.9,2005.mtsummit-papers.29,1,0.740268,"t) the intersection” is attached on “家に ”, which means “house” is replaced with “the intersection”. On the other hand, a parent bond tells that the translation 3.4. Handling of Numerals Numerals in Japanese are translated into English in several ways. • cardinal : 124 → one hundred twenty four • ordinal (e.g., day) : 2 日 → second • two-figure (e.g., room number, year) : 124 → one twenty four • one-figure (e.g., flight number, phone number) : 124 便 → one two four • non-numeral (e.g., month) : 8 月 → August 1 We proposed a method of selecting translation examples based on translation probability [6]. Though we used size- and similarity-based criteria for IWSLT06 because of time constraints, we are planning to use probabilitybased criteria from now on. At the time of parallel sentence alignment, it is checked in which type Japanese numerals are translated. 67 Translation examples of non-numeral type are used only if the numerals match exactly (“8 月 → August” cannot be used to translate “7 月”). However, translation examples of the other types can be used by generalizing numerals, and the input numeral is transformed according to the type. For example, “2 日 → second” can be used to translat"
2006.iwslt-evaluation.9,2001.mtsummit-ebmt.4,0,\N,Missing
2006.iwslt-evaluation.9,P06-1085,0,\N,Missing
2009.mtsummit-papers.18,J93-2003,0,0.029315,"e to mine parallel texts from mixedlanguage web pages. We define a mixedlanguage web page as a web page consisting of (at least) two languages. We mined Japanese-English parallel texts from mixedlanguage web pages. We presented the statistics for extracted parallel texts and conducted machine translation experiments. These statistics and experiments showed that mixedlanguage web pages are rich sources of parallel texts. 1 Introduction Parallel corpora are indispensable language resources for multi-lingual natural language processing, such as corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) and cross-lingual information retrieval. However, there are relatively few widely available parallel corpora. These include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and langu"
2009.mtsummit-papers.18,W08-0334,1,0.880155,"Missing"
2009.mtsummit-papers.18,W07-0717,0,0.0203036,"ure 1 and Table 4 show that the performance of the SMT systems trained with our extracted sentence alignments are inferior to that of the SMT system trained with the IWSLT training data. A likely reason is that the extracted alignments are out-ofdomain data with respect to the IWSLT testsets. In the following, we show that the extracted alignments are useful for improving the performance of the SMT system trained with the IWSLT training data, even though these alignments are not best suited to the testsets. 4.3 Interpolation of models We linearly interpolated8 language and translation models (Foster and Kuhn, 2007) to improve the performance of the SMT system. 4.3.1 Interpolation of language models We first interpolated language models (LMs). We interpolated the language model made from 900,000 sentences in Section 4.1 (hereafter LM(900k)) and that made from the IWSLT training data in Section 4.2 (hereafter LM(IWSLT)). The weight of LM(IWSLT) was 0.1, 0.2, ..., 0.9. In addition to these interpolated language models, we used the translation model made from the IWSLT training data in Section 4.2 for all of the weights. The figures in the 0.1, ..., 0.9 rows in Table 5 show the BLEU scores for set2, ..., se"
2009.mtsummit-papers.18,W04-3208,0,0.161396,"005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to our work, previous studie"
2009.mtsummit-papers.18,J93-1004,0,0.340799,"es into noisy parallel text files. That is, given a web page containing Japanese and English texts, we made a Japanese text file and an English text file from the web page.3 We regarded these two text files as a pair of noisy parallel text files and applied Utiyama and Isahara’s method to these. In the following, we briefly describe how we applied Utiyama and Isahara’s method to these parallel texts. See (Utiyama and Isahara, 2007) for details of their method. We first aligned the sentences in each pair of noisy parallel text files by using a standard dynamic programming (DP) matching method (Gale and Church, 1993; Utsuro et al., 1994). That is, let J and E be a Japanese text file and an English text file, respectively, we calculated the maximum similarity sen2 Our mining method will not be much affected by N because our method can extract parallel sentences very accurately as shown in Section 3. 3 We simply extracted Japanese (English) sentences from the web page and put them into a Japanese (English) text file. Pm AVSIM(J, E) = i=1 SIM(Ji , Ei ) R(J, E) = min( m |J ||E| , ) |E ||J| (1) (2) where |J |is the number of sentences in J, and |E |is the number of sentences in E. A high R(J, E) value occurs"
2009.mtsummit-papers.18,kawahara-kurohashi-2006-case,1,0.855263,"Missing"
2009.mtsummit-papers.18,P07-2045,0,0.0122038,"Missing"
2009.mtsummit-papers.18,2005.mtsummit-papers.11,0,0.0791756,"chine translation experiments. These statistics and experiments showed that mixedlanguage web pages are rich sources of parallel texts. 1 Introduction Parallel corpora are indispensable language resources for multi-lingual natural language processing, such as corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) and cross-lingual information retrieval. However, there are relatively few widely available parallel corpora. These include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung"
2009.mtsummit-papers.18,ma-cieri-2006-corpus,0,0.0118588,"extracted parallel texts and conducted machine translation experiments. These statistics and experiments showed that mixedlanguage web pages are rich sources of parallel texts. 1 Introduction Parallel corpora are indispensable language resources for multi-lingual natural language processing, such as corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) and cross-lingual information retrieval. However, there are relatively few widely available parallel corpora. These include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002"
2009.mtsummit-papers.18,1999.mtsummit-1.79,0,0.219583,"llel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to our work, previous studies have mined parallel texts from parallel web pages. A pair of parallel web pages consists of two monolingual web pages in different languages with almost the same meaning. For exa"
2009.mtsummit-papers.18,J05-4003,0,0.437791,"uropean languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to our work, previous studies have mined parallel texts fro"
2009.mtsummit-papers.18,J03-1002,0,0.00533169,"000 sentence alignments were effective for multi-lingual natural language processing. Based on the statistics presented in Sections 3.2 and 3.3, we concluded that we extracted a clean parallel corpus from the original web corpus. 4 Machine translation experiments We verify the usefulness of the extracted sentence alignments for SMT in this section. We used a state-of-the-art phrase-based SMT system (Finch and Sumita, 2008), which is comparable in performance to the MOSES system (Koehn et al., 2007). To train SMT models, we used a training toolkit adapted from the MOSES system. We used GIZA++ (Och and Ney, 2003) for word alignment and SRILM (Stolcke, 2002) for language modeling. We used 5-gram language models trained with modified Kneser–Ney smoothing. Minimum error rate training was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score (Papineni et al., 2002), and tuning was performed using the standard technique developed by Och (Och, 2003). We used the development data for the IWSLT2007 Japanese-English translation task (Fordyce, 2007) to verify the usefulness of the extracted sentence alignments. The development data consisted of five sets, devset1"
2009.mtsummit-papers.18,P03-1021,0,0.0112609,"system (Finch and Sumita, 2008), which is comparable in performance to the MOSES system (Koehn et al., 2007). To train SMT models, we used a training toolkit adapted from the MOSES system. We used GIZA++ (Och and Ney, 2003) for word alignment and SRILM (Stolcke, 2002) for language modeling. We used 5-gram language models trained with modified Kneser–Ney smoothing. Minimum error rate training was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score (Papineni et al., 2002), and tuning was performed using the standard technique developed by Och (Och, 2003). We used the development data for the IWSLT2007 Japanese-English translation task (Fordyce, 2007) to verify the usefulness of the extracted sentence alignments. The development data consisted of five sets, devset1, devset2, devset3, devset4, and devset5. Each of these data sets had about 500 sentences. The numbers of reference translations were 16 for devset1, devset2, and devset3 and 7 for devset4 and devset5. We used devset1 to tune the SMT system and used devset2, devset3, devset4, and devset5 as the testsets to evaluate the performance of the SMT system in terms of BLEU scores. Hereafter,"
2009.mtsummit-papers.18,P02-1040,0,0.079854,"tracted sentence alignments for SMT in this section. We used a state-of-the-art phrase-based SMT system (Finch and Sumita, 2008), which is comparable in performance to the MOSES system (Koehn et al., 2007). To train SMT models, we used a training toolkit adapted from the MOSES system. We used GIZA++ (Och and Ney, 2003) for word alignment and SRILM (Stolcke, 2002) for language modeling. We used 5-gram language models trained with modified Kneser–Ney smoothing. Minimum error rate training was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score (Papineni et al., 2002), and tuning was performed using the standard technique developed by Och (Och, 2003). We used the development data for the IWSLT2007 Japanese-English translation task (Fordyce, 2007) to verify the usefulness of the extracted sentence alignments. The development data consisted of five sets, devset1, devset2, devset3, devset4, and devset5. Each of these data sets had about 500 sentences. The numbers of reference translations were 16 for devset1, devset2, and devset3 and 7 for devset4 and devset5. We used devset1 to tune the SMT system and used devset2, devset3, devset4, and devset5 as the testse"
2009.mtsummit-papers.18,J03-3002,0,0.231052,"ese include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two"
2009.mtsummit-papers.18,P06-1062,0,0.286421,"gh these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to our work, previous studies have mined parallel texts from parallel web pages. A pair of parallel web pages consists of two monolingual web pages in different languages with almost the same meaning. For example, Shi et al. (2006) have aligned parall"
2009.mtsummit-papers.18,steinberger-etal-2006-jrc,0,0.0659403,"Missing"
2009.mtsummit-papers.18,P03-1010,1,0.869044,"he Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to o"
2009.mtsummit-papers.18,2007.mtsummit-papers.63,1,0.758763,"es for multi-lingual natural language processing, such as corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) and cross-lingual information retrieval. However, there are relatively few widely available parallel corpora. These include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith"
2009.mtsummit-papers.18,C94-2175,0,0.099231,"text files. That is, given a web page containing Japanese and English texts, we made a Japanese text file and an English text file from the web page.3 We regarded these two text files as a pair of noisy parallel text files and applied Utiyama and Isahara’s method to these. In the following, we briefly describe how we applied Utiyama and Isahara’s method to these parallel texts. See (Utiyama and Isahara, 2007) for details of their method. We first aligned the sentences in each pair of noisy parallel text files by using a standard dynamic programming (DP) matching method (Gale and Church, 1993; Utsuro et al., 1994). That is, let J and E be a Japanese text file and an English text file, respectively, we calculated the maximum similarity sen2 Our mining method will not be much affected by N because our method can extract parallel sentences very accurately as shown in Section 3. 3 We simply extracted Japanese (English) sentences from the web page and put them into a Japanese (English) text file. Pm AVSIM(J, E) = i=1 SIM(Ji , Ei ) R(J, E) = min( m |J ||E| , ) |E ||J| (1) (2) where |J |is the number of sentences in J, and |E |is the number of sentences in E. A high R(J, E) value occurs when |J |∼ |E|. Conseq"
2012.eamt-1.7,I05-3025,0,0.0307599,"nd “» ‰↔» T(anesthesia)” are not identical, because “ ↔u(create)”, “4↔è(arrive)” and “‰↔T(drunk)” are common Chinese characters, “uË(found)” is converted into “ Ë(found)”, “èŠ(clinical)” is converted into “4Š(clinical)” and “»T(anesthesia)” is converted into “»‰(anesthesia)” in Step 2. In preliminary experiments, we extracted 14,359 lexicons using Strategy 1, and 18,584 lexicons using Strategy 2 from a paper abstract parallel corpus containing 680K sentence pairs. 3.2 Chinese Lexicons Incorporation Several studies showed that using a system dictionary is helpful for Chinese word segmentation (Low et al., 2005; Wang et al., 2011). Therefore, we use a corpus-based Chinese word segmentation and POS tagging tool with a system dictionary. We incorporate the extracted lexicons into the system dictionary. The extracted lexicons are not only effective for the unknown word problem, but also helpful to solve the word segmentation granularity problem. However, setting POS tags for the extracted lexicons is problematic. To solve this problem, we made a POS tags mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Ja"
2012.eamt-1.7,E09-1063,0,0.316822,"Missing"
2012.eamt-1.7,I08-1033,0,0.17868,"ctive for the unknown word problem, but also helpful to solve the word segmentation granularity problem. However, setting POS tags for the extracted lexicons is problematic. To solve this problem, we made a POS tags mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Japanese, we use the POS tagset defined in the morphological analyzer JUMAN (Kurohashi et al., 1994). JUMAN adapts a POS tagset containing sub POS tags. For example, the POS tag “ ^(noun)” contains sub POS 3.3 Short Unit Transformation Bai et al. (2008) showed that adjusting Chinese word segmentation to make tokens 1-to-1 mapping as many as possible between a parallel sentences can improve alignment accuracy which is crucial for corpus-based MT. Wang et al. (2010) proposed a short unit standard for Chinese word segmentation that is more similar to the Japanese word segmentation standard, which can reduce the number of 1-to-n alignments and improve MT performance. Here, we propose a method to transform the annotated training data of Chinese segmenter into Japanese word segmentation standard using the extracted Chinese lexicons, and use the tr"
2012.eamt-1.7,C04-1081,0,0.0241624,"Missing"
2012.eamt-1.7,W08-0336,0,0.0350343,"orter unit standard. Therefore, the segmentation unit in Chinese may be longer than Japanese even for the same concept. This can increase the number of 1-to-n alignments which makes the word alignment task more difficult. Taking “founder” Introduction As there are no explicit word boundary markers in Chinese, word segmentation is considered as an important first step in MT. Studies showed that a MT system with Chinese word segmentation outperforms the one treating each Chinese character as a single word, and the quality of Chinese word segmentation affects the MT performance (Xu et al., 2004; Chang et al., 2008). It has been found that besides segmentation accuracy, segmentation consistency and granularity of Chinese words are also important for MT (Chang et al., 2008). Moreover, optimal Chinese word segmentation for MT is dependent on the other language, therefore, a bilingual approach is necessary (Ma and Way, 2009). c 2012 European Association for Machine Translation. 35 Meaning TC SC Kanji snow ê(U+96EA) ê(U+96EA) ê(U+96EA) love (U+611B) 1(U+7231) (U+611B) begin |(U+767C) Ñ(U+53D1) z(U+767A) used the occurrence of identical common Chinese characters (e.g. “snow” in Table 1) in automatic sentence"
2012.eamt-1.7,2011.mtsummit-papers.53,1,0.899763,"iation for Machine Translation. 35 Meaning TC SC Kanji snow ê(U+96EA) ê(U+96EA) ê(U+96EA) love (U+611B) 1(U+7231) (U+611B) begin |(U+767C) Ñ(U+53D1) z(U+767A) used the occurrence of identical common Chinese characters (e.g. “snow” in Table 1) in automatic sentence alignment task. Goh et al. (2005) detected common Chinese characters where Kanji are identical to Traditional Chinese but different from Simplified Chinese (e.g. “love” in Table 1). They used Chinese encoding converter1 which can convert Traditional Chinese into Simplified Chinese, and built a Japanese-Simplified Chinese dictionary. Chu et al. (2011) made use of the Unihan database2 to detect common Chinese characters which are visual variants of each other (e.g. “begin” in Table 1), and proved the effectiveness of common Chinese characters in Chinese-Japanese phrase alignment. In this paper, we focus on Simplified Chinese-Japanese MT and exploit common Chinese characters in Chinese word segmentation optimization. Table 1: Examples of common Chinese characters (TC denotes Traditional Chinese and SC denotes Simplified Chinese). in Figure 1 as an example, the Chinese segmenter recognizes it as one token, while the Japanese segmenter splits"
2012.eamt-1.7,wang-etal-2010-adapting,0,0.0166521,"s mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Japanese, we use the POS tagset defined in the morphological analyzer JUMAN (Kurohashi et al., 1994). JUMAN adapts a POS tagset containing sub POS tags. For example, the POS tag “ ^(noun)” contains sub POS 3.3 Short Unit Transformation Bai et al. (2008) showed that adjusting Chinese word segmentation to make tokens 1-to-1 mapping as many as possible between a parallel sentences can improve alignment accuracy which is crucial for corpus-based MT. Wang et al. (2010) proposed a short unit standard for Chinese word segmentation that is more similar to the Japanese word segmentation standard, which can reduce the number of 1-to-n alignments and improve MT performance. Here, we propose a method to transform the annotated training data of Chinese segmenter into Japanese word segmentation standard using the extracted Chinese lexicons, and use the transformed data for training the Chinese segmenter. Because the extracted lexicons are derived from Japanese word segmentation results, they follow Japanese 37 从_P/ 有效性_NN /高_VA/的_DEC/ 格要素_NN /… CTB: Lexicon: Lexicon"
2012.eamt-1.7,W02-1001,0,0.0137644,"e same domain as the parallel training corpus. The statistics of the test sets are shown in Table 4. Note that all sentences in the test sets are not included in the parallel training corpus. Settings Parallel Training Corpus 4.2 The parallel training corpus we used is a paper abstract corpus provided by JST4 and NICT5 . This 4 Chinese and Japanese Segmenters For Chinese, we used a corpus-based word segmentation and POS tagging tool with a system dictionary, weights for the lexicons in the system dictionary are automatically learned from the training data using averaged structured perceptron (Collins, 2002). For Japanese, we used JUMAN (Kurohashi et al., 1994). We conducted Chinese-Japanese translation experiments to show the effectiveness of exploiting common Chinese characters in Chinese word segmentation optimization. 4.1.1 Chinese Annotated Corpus We used two types of manually annotated Chinese corpus for training the Chinese segmenter. One is NICT Chinese Treebank, which is from the same domain as the parallel training corpus and contains 9,792 sentences. Note that the annotated sentences in this corpus are not included in the parallel training corpus. The other corpus is CTB 7 (LDC2010T07)"
2012.eamt-1.7,I11-1035,0,0.118655,"esia)” are not identical, because “ ↔u(create)”, “4↔è(arrive)” and “‰↔T(drunk)” are common Chinese characters, “uË(found)” is converted into “ Ë(found)”, “èŠ(clinical)” is converted into “4Š(clinical)” and “»T(anesthesia)” is converted into “»‰(anesthesia)” in Step 2. In preliminary experiments, we extracted 14,359 lexicons using Strategy 1, and 18,584 lexicons using Strategy 2 from a paper abstract parallel corpus containing 680K sentence pairs. 3.2 Chinese Lexicons Incorporation Several studies showed that using a system dictionary is helpful for Chinese word segmentation (Low et al., 2005; Wang et al., 2011). Therefore, we use a corpus-based Chinese word segmentation and POS tagging tool with a system dictionary. We incorporate the extracted lexicons into the system dictionary. The extracted lexicons are not only effective for the unknown word problem, but also helpful to solve the word segmentation granularity problem. However, setting POS tags for the extracted lexicons is problematic. To solve this problem, we made a POS tags mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Japanese, we use the P"
2012.eamt-1.7,I05-1059,0,0.173956,"und that besides segmentation accuracy, segmentation consistency and granularity of Chinese words are also important for MT (Chang et al., 2008). Moreover, optimal Chinese word segmentation for MT is dependent on the other language, therefore, a bilingual approach is necessary (Ma and Way, 2009). c 2012 European Association for Machine Translation. 35 Meaning TC SC Kanji snow ê(U+96EA) ê(U+96EA) ê(U+96EA) love (U+611B) 1(U+7231) (U+611B) begin |(U+767C) Ñ(U+53D1) z(U+767A) used the occurrence of identical common Chinese characters (e.g. “snow” in Table 1) in automatic sentence alignment task. Goh et al. (2005) detected common Chinese characters where Kanji are identical to Traditional Chinese but different from Simplified Chinese (e.g. “love” in Table 1). They used Chinese encoding converter1 which can convert Traditional Chinese into Simplified Chinese, and built a Japanese-Simplified Chinese dictionary. Chu et al. (2011) made use of the Unihan database2 to detect common Chinese characters which are visual variants of each other (e.g. “begin” in Table 1), and proved the effectiveness of common Chinese characters in Chinese-Japanese phrase alignment. In this paper, we focus on Simplified Chinese-Ja"
2012.eamt-1.7,P07-2045,0,0.00948082,"racters, such as “L‚(praise)”, “×L(poem)” etc. Obviously, splitting “L ‚(praise)” into “L(song)” and “‚(eulogy)”, or splitting “× L(poem)” into “×(poem)” and “L(song)” is undesirable. Also, there are few consecutive tokens in the training data that can be combined to one extracted lexicon, we do not consider this pattern. 4 corpus was created by the Japanese project “Development and Research of Chinese-Japanese Natural Language Processing Technology”. The statistics of this corpora are shown in Table 3. 4.1.2 4.1.3 4.1.4 5 SMT Model We used the state-of-the-art phrase-based SMT toolkit Moses (Koehn et al., 2007) with default options, except for the distortion limit (6→20). It was tuned by MERT using another 500 development sentence pairs. 4.1.5 Test Sets We translated 5 test sets of Chinese sentences from the same domain as the parallel training corpus. The statistics of the test sets are shown in Table 4. Note that all sentences in the test sets are not included in the parallel training corpus. Settings Parallel Training Corpus 4.2 The parallel training corpus we used is a paper abstract corpus provided by JST4 and NICT5 . This 4 Chinese and Japanese Segmenters For Chinese, we used a corpus-based wo"
2012.eamt-1.7,xia-etal-2000-developing,0,0.106997,"Missing"
2012.eamt-1.7,W04-1118,0,0.085468,"Missing"
2012.eamt-1.7,W04-3230,0,0.0697192,"Missing"
2014.amta-wptp.15,aziz-etal-2012-pet,0,0.0300338,"shiaki Nakazawa‡ Daisuke Kawahara† Sadao Kurohashi† † Graduate School of Informatics, Kyoto University, Kyoto 606-8501 ‡ Japan Science and Technology Agency, Kawaguchi-shi, Saitama 332-0012 † {kishimoto,dk,kuro}@nlp.ist.i.kyoto-u.ac.jp ‡ nakazawa@pa.jst.jp 1 Purpose and characteristics Translation has become increasingly important by virtue of globalization. To reduce the cost of translation, it is necessary to use machine translation and further to take advantage of post-editing based on the result of a machine translation for accurate information dissemination. Such post-editing (e.g., PET [Aziz et al., 2012]) can be used practically for translation between European languages, which has a high performance in statistical machine translation. However, due to the low accuracy of machine translation between languages with diﬀerent word order, such as Japanese-English and Japanese-Chinese, post-editing has not been used actively. We propose a post-editing system based on syntaxbased machine translation to deal with diﬀerent word order. For language pairs with diﬀerent word order, it is time-consuming for a translator to understand what a machine translation system did. To solve this problem, our syste"
2020.acl-srw.31,D11-1052,0,0.0203729,"annot simply apply a spelling checker, and (2) the way people inputting kanji logographs results in typos with drastically different surface forms from correct ones. We address them by combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction. 1 Introduction For over a decade, user generated content (UGC) has been an important target of NLP technology. It is characterized by phenomena not found in standard texts, such as word lengthening (Brody and Diakopoulos, 2011), dialectal variations (Saito et al., 2017; Blodgett et al., 2016), unknown onomatopoeias (Sasano et al., 2013), grammatical errors (Mizumoto et al., 2011; Lee et al., 2018), and mother tongue interference in non-native writing (Goldin et al., 2018). Typographical errors (typos) also occur often in UGC.1 Typos prevent machines from analyzing texts properly (Belinkov and Bisk, 2018). Typo correction systems are important because applying them before analysis would reduce analysis errors and lead to improved accuracy in various NLP tasks. Neural networks are a promising choice for building a typ"
2020.acl-srw.31,N19-1348,0,0.0398173,"Missing"
2020.acl-srw.31,D18-1395,0,0.0532572,"Missing"
2020.acl-srw.31,2020.lrec-1.835,0,0.0368508,"king it difficult to identify the word affected. Although state-of-the-art word segmenters provide reasonable accuracy for clean texts, word segmentation on texts with typos remains a challenging problem. In addition, languages with complex writing systems such as Japanese and Chinese have typos not found in French and English. These languages use logographs, kanji in Japanese, and they are ∗ Current affiliation is Waseda University 1 In the present study, typos may cover some grammatical errors in addition to spelling errors. 2 Although the publicly available multilingual GitHub Typo Corpus (Hagiwara and Mita, 2020) covers Japanese, it contains only about 1,000 instances and ignores erroneous kanjiconversion, an important class of typos in Japanese. 230 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 230–236 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics typically entered using input methods, with which people enter phonetic symbols, kana in the case of Japanese, and then select a correct logograph from a list of logographs matching the reading. Typos occurring during this process can be drastically differ"
2020.acl-srw.31,P17-4012,0,0.0103195,"because i-omission is considered inappropriate in formal writing, but crowdworkers turned out to be tolerant of colloquialism. “Other” of kanji-conversion was more frequent than those of other categories. This means that the answers of crowdworkers were diverse. We conjecture that judging whether kanji is correct or not needs higher-level knowledge of kanji. Some pairs that should have been classified as “Correct revision” were classified as “Other” or “Bad revision”. These imply that the quality of deletion and kanji-conversion was better than the scores indicate. 6 Settings We used OpenNMT (Klein et al., 2017)12 , a Python toolkit of encoder-decoder-based machine translation, as a typo correction system. We trained the model separately for each category of typos. For training and validation, we used sentence pairs not used in the crowdsourced evaluation. The training set contained 79,714 substitutions, 82,227 deletions, 102,897 insertions, and 230,490 kanjiconversions and the validation set contained 5,000 sentence pairs of each category. The test set contained 1,689 substitutions13 , 1,665 deletion, 2,127 insertion, and 3,061 kanji-conversion sentence pairs classified as “Correct revision” as the"
2020.acl-srw.31,W04-3230,0,0.422318,"Missing"
2020.acl-srw.31,L18-1363,0,0.0386803,"combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction. 1 Introduction For over a decade, user generated content (UGC) has been an important target of NLP technology. It is characterized by phenomena not found in standard texts, such as word lengthening (Brody and Diakopoulos, 2011), dialectal variations (Saito et al., 2017; Blodgett et al., 2016), unknown onomatopoeias (Sasano et al., 2013), grammatical errors (Mizumoto et al., 2011; Lee et al., 2018), and mother tongue interference in non-native writing (Goldin et al., 2018). Typographical errors (typos) also occur often in UGC.1 Typos prevent machines from analyzing texts properly (Belinkov and Bisk, 2018). Typo correction systems are important because applying them before analysis would reduce analysis errors and lead to improved accuracy in various NLP tasks. Neural networks are a promising choice for building a typo correction system because they have demonstrated their success in a closely related task, spelling correction (Sakaguchi et al., 2017). Since neural networks are known to"
2020.acl-srw.31,max-wisniewski-2010-mining,0,0.042826,"ce neural networks are known to be data-hungry, the first step to develop a neural typo correction system is to prepare a large number of typos and their corrections. However, to our best knowledge, no such dataset is available for Japanese.2 This motivated us to build a large Japanese typo dataset. Typos are usually collected using data mining techniques because thorough corpus annotation is inefficient for infrequently occurring typos. Previous studies on building typo datasets have exploited Wikipedia because it is large, and more importantly, keeps track of all changes made to an article (Max and Wisniewski, 2010; Zesch, 2012). In these studies, to collect typo–correction pairs, the first step is to identify words changed in revisions and the second step is to apply a spell checker to them or calculate the edit distance between them. While these methods work well on the target languages of the previous studies, namely French and English, they cannot be applied directly to languages such as Japanese and Chinese, where words are not delimited by white space. This is because a typo may cause a word segmentation error and can be misinterpreted as a multiple-word change, making it difficult to identify the"
2020.acl-srw.31,I11-1017,0,0.0402207,"nes. We address them by combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction. 1 Introduction For over a decade, user generated content (UGC) has been an important target of NLP technology. It is characterized by phenomena not found in standard texts, such as word lengthening (Brody and Diakopoulos, 2011), dialectal variations (Saito et al., 2017; Blodgett et al., 2016), unknown onomatopoeias (Sasano et al., 2013), grammatical errors (Mizumoto et al., 2011; Lee et al., 2018), and mother tongue interference in non-native writing (Goldin et al., 2018). Typographical errors (typos) also occur often in UGC.1 Typos prevent machines from analyzing texts properly (Belinkov and Bisk, 2018). Typo correction systems are important because applying them before analysis would reduce analysis errors and lead to improved accuracy in various NLP tasks. Neural networks are a promising choice for building a typo correction system because they have demonstrated their success in a closely related task, spelling correction (Sakaguchi et al., 2017). Since neural net"
2020.acl-srw.31,I17-2044,0,0.0211667,"ay people inputting kanji logographs results in typos with drastically different surface forms from correct ones. We address them by combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction. 1 Introduction For over a decade, user generated content (UGC) has been an important target of NLP technology. It is characterized by phenomena not found in standard texts, such as word lengthening (Brody and Diakopoulos, 2011), dialectal variations (Saito et al., 2017; Blodgett et al., 2016), unknown onomatopoeias (Sasano et al., 2013), grammatical errors (Mizumoto et al., 2011; Lee et al., 2018), and mother tongue interference in non-native writing (Goldin et al., 2018). Typographical errors (typos) also occur often in UGC.1 Typos prevent machines from analyzing texts properly (Belinkov and Bisk, 2018). Typo correction systems are important because applying them before analysis would reduce analysis errors and lead to improved accuracy in various NLP tasks. Neural networks are a promising choice for building a typo correction system because they have demo"
2020.acl-srw.31,I13-1019,1,0.822011,"lly different surface forms from correct ones. We address them by combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction. 1 Introduction For over a decade, user generated content (UGC) has been an important target of NLP technology. It is characterized by phenomena not found in standard texts, such as word lengthening (Brody and Diakopoulos, 2011), dialectal variations (Saito et al., 2017; Blodgett et al., 2016), unknown onomatopoeias (Sasano et al., 2013), grammatical errors (Mizumoto et al., 2011; Lee et al., 2018), and mother tongue interference in non-native writing (Goldin et al., 2018). Typographical errors (typos) also occur often in UGC.1 Typos prevent machines from analyzing texts properly (Belinkov and Bisk, 2018). Typo correction systems are important because applying them before analysis would reduce analysis errors and lead to improved accuracy in various NLP tasks. Neural networks are a promising choice for building a typo correction system because they have demonstrated their success in a closely related task, spelling correction"
2020.acl-srw.31,D18-2010,1,0.8337,"ble decision because each category has its own characteristics. However, this mining strategy prevents us from obtaining a balanced dataset. We leave it for future work. 3.2 Filtering Sentence pairs obtained according to the above procedure contain pairs that do not seem to be typo corrections. We use the following three methods to remove them. losspost − losspre the number of characters changed in the pairs 10 We use the Python3 regex library (https://pypi. org/project/regex/) to determine character types, hiragana, katakana, kanji, or alphabet. 11 We use the morphological analyzers Juman++ (Tolmachev et al., 2018) (http://nlp.ist.i.kyoto-u. ac.jp/index.php?JUMAN++) and MeCab to get readings of kanji. If at least one of the analyses of reading matches, we regard the pairs as having the same reading. > α, where α is determined heuristically. It is set to −4 for substitution, −5 for deletion, and −6 for insertion. We do not apply this filter to kanji-conversion. We found that a change from high-frequency kanji to low-frequency kanji often yielded a large value even if the change was correct. The second filter focused on the loss of the postrevision sentence. This filters out sentence pairs 232 Typo Substi"
2020.acl-srw.31,Q16-1029,0,0.017311,"stem using JWTD. 234 Model Typo Subst. Deletion Morph Insertion Kanji-conv. Subst Deletion Char Insertion Kanji-conv P 11.9 23.2 16.8 30.8 4.5 6.5 6.2 10.0 R 39.8 69.9 79.7 57.0 37.2 59.4 76.0 43.5 F0.5 Match SARI 13.8 31.6 61.2 26.7 60.9 80.2 19.9 69.3 83.8 33.9 48.7 71.0 5.5 25.2 54.2 8.0 44.7 69.6 7.6 51.8 72.5 11.8 33.7 60.9 7 Table 6: Results of the typo correction experiment. and the hidden size were 500. 6.2 Evaluation metrics Our evaluation metrics were precision, recall and F0.5 score in typo correction, the percentage of exact matches between system outputs and references, and SARI (Xu et al., 2016). We defined precision and recall as follows. For each sentence, we calculate the character-level minimum edits from the input to the gold G, the character-level minimum edits from the input to the system output O, and G ∩ O. Let NG , NO , and NG∩O be the sums of |G|, |O|, and |G ∩ O |in all sentences, respectively. We calculated Precision = NG∩O /NO and Recall = NG∩O /NG . We used the Python3 python-Levenshtein library14 for calculating minimum edits. SARI is a metric for text editing. This calculates the averaged F1 scores of the added, kept, and deleted n-grams. We used character-level 4gra"
2020.acl-srw.31,E12-1054,0,0.0269591,"wn to be data-hungry, the first step to develop a neural typo correction system is to prepare a large number of typos and their corrections. However, to our best knowledge, no such dataset is available for Japanese.2 This motivated us to build a large Japanese typo dataset. Typos are usually collected using data mining techniques because thorough corpus annotation is inefficient for infrequently occurring typos. Previous studies on building typo datasets have exploited Wikipedia because it is large, and more importantly, keeps track of all changes made to an article (Max and Wisniewski, 2010; Zesch, 2012). In these studies, to collect typo–correction pairs, the first step is to identify words changed in revisions and the second step is to apply a spell checker to them or calculate the edit distance between them. While these methods work well on the target languages of the previous studies, namely French and English, they cannot be applied directly to languages such as Japanese and Chinese, where words are not delimited by white space. This is because a typo may cause a word segmentation error and can be misinterpreted as a multiple-word change, making it difficult to identify the word affected"
2020.coling-main.114,Y12-1058,1,0.853219,"Experimental Settings In our experiments, we used CAModel and CorefCAModel. CAModel was trained on various combinations of tasks. We ﬁne-tuned both the models for 4 epochs using cross entropy loss following Devlin et al. (2019). Since CorefCAModel cannot perform CR with sufﬁciently high accuracy in the early stage of training, we mixed gold coreference data with the ﬁrst stage prediction and gradually reduced the gold ratio inspired by Scheduled Sampling (Bengio et al., 2015)． We used two kinds of datasets for our experiment. One is the Kyoto University Web Document Leads Corpus (Web corpus) (Hangyo et al., 2012), and the other is the Kyoto University Text Corpus (News corpus) (Kawahara et al., 2002). Verbal predicate-argument relations, nominal predicate-argument relations, coreference relations, and bridging anaphora relations are manually annotated in both the corpora. Table 1 lists the number of sentences in each corpus. In our experiment, training was performed on a mixture of both corpora2 and the evaluation was done on each corpus. We used the NICT BERT Japanese pre-trained model (with BPE).3 This model was trained after morphological and subword segmentation using the full text of Japanese Wik"
2020.coling-main.114,2020.acl-main.132,0,0.0334042,"Missing"
2020.coling-main.114,kawahara-etal-2002-construction,1,0.276561,"trained on various combinations of tasks. We ﬁne-tuned both the models for 4 epochs using cross entropy loss following Devlin et al. (2019). Since CorefCAModel cannot perform CR with sufﬁciently high accuracy in the early stage of training, we mixed gold coreference data with the ﬁrst stage prediction and gradually reduced the gold ratio inspired by Scheduled Sampling (Bengio et al., 2015)． We used two kinds of datasets for our experiment. One is the Kyoto University Web Document Leads Corpus (Web corpus) (Hangyo et al., 2012), and the other is the Kyoto University Text Corpus (News corpus) (Kawahara et al., 2002). Verbal predicate-argument relations, nominal predicate-argument relations, coreference relations, and bridging anaphora relations are manually annotated in both the corpora. Table 1 lists the number of sentences in each corpus. In our experiment, training was performed on a mixture of both corpora2 and the evaluation was done on each corpus. We used the NICT BERT Japanese pre-trained model (with BPE).3 This model was trained after morphological and subword segmentation using the full text of Japanese Wikipedia for approximately 1 million steps. At the ﬁne-tuning stage, we set the maximum seq"
2020.coling-main.114,P18-1044,1,0.752299,"AR, and CR all together using BERT. This model is called Cohesion Analysis Model (CAModel). In this section, we ﬁrst describe our Base Model for performing only VPA, and then describe the CAModel for multi-task learning of all four tasks. Finally, we describe the Coreference-aware Cohesion Analysis Model (CorefCAModel), which deals with CR specially. 3.1 Base Model Our Base Model using BERT is shown in Figure 2. This ﬁgure shows the analysis of nominative (NOM) of the predicate ti in VPA. The predictions are performed by an argument selection method, following Shibata and Kurohashi (2018) and Kurita et al. (2018). When the predicate ti is the target, the model calculates the probability that a word is the nominative argument of ti for all other words in the 1325 Figure 2: Our Base Model (in the case of analyzing the nominative (NOM) of verbal predicate ti ). documents. The one with the highest probability among them is adopted as the nominative argument of ti . This calculation is done for all the other cases, such as accusative (ACC) and dative (DAT), and for all predicates in the document. 3.1.1 Input Representation The segmentation of a document consists of three steps: sentence, word, and subword"
2020.coling-main.114,J94-4001,1,0.528332,"ng the full text of Japanese Wikipedia for approximately 1 million steps. At the ﬁne-tuning stage, we set the maximum sequence length to 128. The maximum sequence length of the Web corpus was shorter than 128. In the News corpus, there are many documents with sequence lengths exceeding 128, and one document is divided into multiple parts for training. To do this, we divided a document so that it had as many preceding contexts as possible. For VPA, we extracted all predicates in a document, and analyzed them in terms of four cases of NOM, ACC, DAT and NOM2.4 The Japanese dependency parser KNP (Kurohashi and Nagao, 1994) was used for predicate extraction. For NPA, we analyzed nouns that KNP judged to have arguments. For both of VPA and NPA, we used arguments that have case or zero relation for training and evaluation.5 BAR and CR were performed on nouns. Following Shibata and Kurohashi (2018), we consider author, reader, and unspeciﬁed person as targets of exophora, and the evaluation of VPA, NPA, BAR, and CR was relaxed using a gold coreference chain.6 2 In our preliminary experiments, we have veriﬁed that mixing the corpora leads to better performance than using them alone. 3 https://alaginrc.nict.go.jp/nic"
2020.coling-main.114,2020.acl-main.744,0,0.0203185,"Missing"
2020.coling-main.114,I17-2022,0,0.0574946,"Missing"
2020.coling-main.114,C18-1009,0,0.0411379,"Missing"
2020.coling-main.114,N19-1344,0,0.0328333,"Missing"
2020.coling-main.114,P17-1146,0,0.278134,"Missing"
2020.coling-main.114,C04-1174,1,0.560255,"Missing"
2020.coling-main.114,P16-1162,0,0.0279555,"nt of ti for all other words in the 1325 Figure 2: Our Base Model (in the case of analyzing the nominative (NOM) of verbal predicate ti ). documents. The one with the highest probability among them is adopted as the nominative argument of ti . This calculation is done for all the other cases, such as accusative (ACC) and dative (DAT), and for all predicates in the document. 3.1.1 Input Representation The segmentation of a document consists of three steps: sentence, word, and subword segmentation. Sentence and word segmentation is annotated in the corpora. For subword segmentation, we use BPE (Sennrich et al., 2016) following the segmentation method used at the pre-training stage. Following Devlin et al. (2019), we insert [CLS]and [SEP]tokens at the beginning and end of a document, respectively. In addition, we insert ﬁve special tokens at the end of input sequence: [author], [reader], [unspecified person], [NULL], and [NA]. [author], [reader], and [unspecified person]are used in exophora resolution. In anaphora resolution, an anaphor sometimes refers to an entity that does not appear in the document. This phenomenon is referred to as exophora. In this study, author, reader, and unspeciﬁed:person are tak"
2020.coling-main.114,P18-1054,1,0.806091,"his study, we perform VPA, NPA, BAR, and CR all together using BERT. This model is called Cohesion Analysis Model (CAModel). In this section, we ﬁrst describe our Base Model for performing only VPA, and then describe the CAModel for multi-task learning of all four tasks. Finally, we describe the Coreference-aware Cohesion Analysis Model (CorefCAModel), which deals with CR specially. 3.1 Base Model Our Base Model using BERT is shown in Figure 2. This ﬁgure shows the analysis of nominative (NOM) of the predicate ti in VPA. The predictions are performed by an argument selection method, following Shibata and Kurohashi (2018) and Kurita et al. (2018). When the predicate ti is the target, the model calculates the probability that a word is the nominative argument of ti for all other words in the 1325 Figure 2: Our Base Model (in the case of analyzing the nominative (NOM) of verbal predicate ti ). documents. The one with the highest probability among them is adopted as the nominative argument of ti . This calculation is done for all the other cases, such as accusative (ACC) and dative (DAT), and for all predicates in the document. 3.1.1 Input Representation The segmentation of a document consists of three steps: sen"
2020.coling-main.114,2020.acl-main.622,0,0.0334479,"Missing"
2020.coling-main.114,2020.coling-main.315,0,0.0499898,"Missing"
2020.emnlp-main.192,N19-1423,0,0.0456148,"k problems from the whole data of ConceptNet. Introduction Along with the progress of deep learning, there have been many studies that consider task settings and build their datasets for training/evaluating language understanding ability by computers (Wang et al., 2019b,a). Language understanding by computers requires two types of knowledge: knowledge of language (meaning of words, syntax, and so forth) and knowledge of our world and society beyond language. The former problem of acquiring linguistic knowledge has been solved to a large extent by general-purpose language models, such as BERT (Devlin et al., 2019), which are pre-trained using a large corpus. It is now possible to represent the meaning of a word as a vector according to its context. Fine-tuning based on these vectors has made natural language inference, paraphrase recognition, and question answering without requiring deep inference as accurate as humans. On the other hand, there are still many problems with acquiring knowledge beyond language. Actu∗ 1 Current affiliation is Waseda University. http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JCID Another important point is that biases in building datasets must be reduced as much as possible."
2020.emnlp-main.192,P84-1044,0,0.381226,"Missing"
2020.emnlp-main.192,N18-2017,0,0.0679104,"Missing"
2020.emnlp-main.192,E14-1007,1,0.879084,"Missing"
2020.emnlp-main.192,P19-1441,0,0.014713,"porating such knowledge bases into an NLP model have been studied but have not been established yet. Many QA datasets for commonsense inference have been built. They include COPA (Choice of Plausible Alternatives) (Roemmele et al., 2011), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), and CommonsenseQA (Talmor et al., 2019). These datasets can be solved to some extent by machine comprehension models (Devlin et al., 2019) that have been rapidly improved. There have been also some approaches that transfer knowledge in such a dataset to downstream tasks using multi-task learning (Liu et al., 2019). We briefly introduce these datasets below. COPA consists of 1,000 two-choice questions that ask a causal relation between two sentences. Each question provides a premise sentence and requires to choose its cause or ending sentence from two alternatives. This dataset was manually created for the purpose of evaluation and is too small to learn commonsense by computers. SWAG is a commonsense inference dataset consisting of 113k multiple-choice questions that ask the most appropriate verb phrase following a given context. To guarantee generality as commonsense, questions were created from video"
2020.emnlp-main.192,prasad-etal-2008-penn,0,0.0604091,"ied in a straightforward way, but we need to take care of the case that an argument in the latter event is pronominalized or omitted. If the latter event does not have an explicit argument, we recover it with any of the arguments in the former event and examine whether the recovered latter event is composed of a basic event. Table 3: Examples of Japanese basic events. The contingency relation between events should be expressed by an explicit discourse marker and be a causal or conditional relation, corresponding to “CONTINGENCY:Cause” or “CONTINGENCY:Condition” in the Penn Discourse Treebank (Prasad et al., 2008). To select highly reliable parts from analysis results and to extract only general event pairs as commonsense, we keep event pairs satisfying the following conditions. Here, we call the first event that represents a cause or reason former event and the second event latter event. For example, consider the event pair “the glass breaks on impact → I replace it”. In this case, we generate recovered latter events “I replace the glass” and “I replace impact” by substituting an argument in the former event for “it”. Then, we examine whether either of them is composed of a basic event and extract thi"
2020.emnlp-main.192,N19-1421,0,0.0602062,"is comprised of 877k if-then pairs of basic events. They collected these pairs using crowdsourcing based on frequent basic events extracted from several corpora. These fully manual or crowdsourcing approaches are costly and have a problem of scalability. Also, methods for incorporating such knowledge bases into an NLP model have been studied but have not been established yet. Many QA datasets for commonsense inference have been built. They include COPA (Choice of Plausible Alternatives) (Roemmele et al., 2011), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), and CommonsenseQA (Talmor et al., 2019). These datasets can be solved to some extent by machine comprehension models (Devlin et al., 2019) that have been rapidly improved. There have been also some approaches that transfer knowledge in such a dataset to downstream tasks using multi-task learning (Liu et al., 2019). We briefly introduce these datasets below. COPA consists of 1,000 two-choice questions that ask a causal relation between two sentences. Each question provides a premise sentence and requires to choose its cause or ending sentence from two alternatives. This dataset was manually created for the purpose of evaluation and"
2020.emnlp-main.192,D18-1009,0,0.0252109,"se relations is not high. ATOMIC (Sap et al., 2019) is a knowledge base that is comprised of 877k if-then pairs of basic events. They collected these pairs using crowdsourcing based on frequent basic events extracted from several corpora. These fully manual or crowdsourcing approaches are costly and have a problem of scalability. Also, methods for incorporating such knowledge bases into an NLP model have been studied but have not been established yet. Many QA datasets for commonsense inference have been built. They include COPA (Choice of Plausible Alternatives) (Roemmele et al., 2011), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), and CommonsenseQA (Talmor et al., 2019). These datasets can be solved to some extent by machine comprehension models (Devlin et al., 2019) that have been rapidly improved. There have been also some approaches that transfer knowledge in such a dataset to downstream tasks using multi-task learning (Liu et al., 2019). We briefly introduce these datasets below. COPA consists of 1,000 two-choice questions that ask a causal relation between two sentences. Each question provides a premise sentence and requires to choose its cause or ending sentence from two alterna"
2020.emnlp-main.192,P19-1472,0,0.0625795,"Sap et al., 2019) is a knowledge base that is comprised of 877k if-then pairs of basic events. They collected these pairs using crowdsourcing based on frequent basic events extracted from several corpora. These fully manual or crowdsourcing approaches are costly and have a problem of scalability. Also, methods for incorporating such knowledge bases into an NLP model have been studied but have not been established yet. Many QA datasets for commonsense inference have been built. They include COPA (Choice of Plausible Alternatives) (Roemmele et al., 2011), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), and CommonsenseQA (Talmor et al., 2019). These datasets can be solved to some extent by machine comprehension models (Devlin et al., 2019) that have been rapidly improved. There have been also some approaches that transfer knowledge in such a dataset to downstream tasks using multi-task learning (Liu et al., 2019). We briefly introduce these datasets below. COPA consists of 1,000 two-choice questions that ask a causal relation between two sentences. Each question provides a premise sentence and requires to choose its cause or ending sentence from two alternatives. This dataset was manually c"
2020.findings-emnlp.23,Q14-1037,0,0.0296602,"en these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models ("
2020.findings-emnlp.23,D17-1181,0,0.0426636,"Missing"
2020.findings-emnlp.23,P19-1136,0,0.0256531,"Missing"
2020.findings-emnlp.23,C16-1239,0,0.0720788,"Missing"
2020.findings-emnlp.23,P19-1407,0,0.0251793,"th Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) E E [sE 0 , s1 , . . . , sn ] = Encoder([x0 , x1 , . . . , xn ]) (1) Then we pass the output s sequence to Conven : E E o0 = Conven ([sE 0 , s1 , . . . , sn ]) (2) where Conven is the encoder convolutional layer. Conven maps sE to o0 , which is also a sequence and has the identical dimension as the s sequence. The output is denoted as o0 ∈ Rn×h , where h is the hidden size, n is the length of the input sentence. o0 is the auxiliary representation of the sentence, which is used for decoding with scratchpad attention mechanism (Benmalek et al., 2019): on−1 is used to calculate attention score, and on−1 will be updated to on at every decoding step. During decoding, we use different input embeddings and output layers for relation and entity ex238 SOS sentence Encoder Obama Decoder Obama HLR Decoder Encoder Decoder Obama HLR sentence Encoder Obama Decoder graduate Decoder graduate president CU SOS sentence graduate SOS HLS sentence president HLR Decoder Decoder Obama HLR afﬁliation Encoder Decoder Decoder graduate president Obama HLR Decoder graduate president SOS Obama HLR Decoder HLR graduate afﬁliation Decoder CU HLS president Decoder HLR"
2020.findings-emnlp.23,P11-1056,0,0.0387397,"d time step, thus the model is prone to feed entity pairs to the classification layer with an low odds (low recall) but high confidence (high precision). In contrast, for the order h-r-t, given the predicted h, the corresponding r can be easily identified according to the context. Subsequently, the predicted h-r pair gives strong hint to the last time step prediction, hence the model will not collapse from the no-relation. This also applies to any other order with r in the first two time steps. 5 Related Work Previous work uses P IPELINE to extract triplets from text (Nadeau and Sekine, 2007; Chan and Roth, 2011). They first recognize all entities in the input sentence then classify relations for each entity pair exhaustively. Li and Ji (2014) point out that the classification errors may propagate across subtasks. Instead of treating these two subtasks separately, for joint entities and relations extraction (JERE), TABLE methods calculate the similarity score of all token pairs and relations by exhaustive enumeration and the extracted triplets are found by the position of the output in the table (Miwa and Bansal, 2016; Gupta et al., 2016). However, as a triplet may contain entities with different leng"
2020.findings-emnlp.23,K19-1055,0,0.0136371,"es auto-regressive decoding strategy. The decoder predicts the nodes layer by layer, where the prediction results of the previous layer are used as the input of the next time step separately, as shown in Fig. 3b. 3 3.1 Experiments Settings Dataset We evaluate our model on two datasets, NYT and DuIE1 . NYT (Riedel et al., 2010) is a English news dataset that is generated by distant supervision without manual annotation, which is widely used in JERE studies (Zheng et al., 2017; Zeng et al., 2018; Takanobu et al., 2018; Dai et al., 2019; Fu et al., 2019; Nayak and Ng, 2019; Zeng et al., 2019a,b; Chen et al., 2019; Wei et al., 2019). We use the same data split as CopyRE (Zeng et al., 2018). DuIE (Li et al., 2019) is a large-scale Chinese JERE dataset where sentences are from Baidu 1 https://ai.baidu.com/broad/introduction? dataset=dureader Baselines We compare the proposed model, Seq2UMTree, with strong baselines under the same hyperparameters, as follows: 1) CopyMTL (Zeng et al., 2019a) is a Seq2Seq model with copy mechanism, and the entities are found by multi-task learning. 2) WDec (Nayak and Ng, 2019) is a standard Seq2Seq model with dynamic masking, and decode the entity token by token. 3) MHS (Be"
2020.findings-emnlp.23,D14-1179,0,0.0369231,"Missing"
2020.findings-emnlp.23,P82-1020,0,0.803956,"Missing"
2020.findings-emnlp.23,P05-1051,1,0.612724,"iversity), Obama and Columbia University are the head and tail entities appearing in the text, and graduate from is the relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belong"
2020.findings-emnlp.23,H05-1003,1,0.628363,"lumbia University are the head and tail entities appearing in the text, and graduate from is the relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple rel"
2020.findings-emnlp.23,P17-1085,0,0.0853559,"classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can be assigned to one entity, which solves the problem naturally (Zeng et"
2020.findings-emnlp.23,P16-4011,0,0.0283357,"ethods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can"
2020.findings-emnlp.23,P14-1038,1,0.924787,"he relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSe"
2020.findings-emnlp.23,D14-1198,1,0.921782,"Missing"
2020.findings-emnlp.23,2020.acl-main.713,1,0.797784,"the effects of exposure bias. Our method differs from previous solution on exposure bias that we remove the order by structure decoding rather than random sampling (Tsai and Lee, 2019). CASREL (Wei et al., 2020) is a recently proposed two-step tagging method, which first finds all the head entities in the sentence then labels a relation-tail table for each head entity, which can also be seen as a UMTree decoder with a decoding length two. However, they overlook the data bias problem in NYT, which causing model unreliability and possible model bias. Note that our task is different from ONEIE (Lin et al., 2020), which models event extraction, entity span detection, entity type recognition and relation extraction in a Seq2Graph way. In contrast to ONEIE, JERE aims to extract only relation-entity triplets, which can be modeled by our UMTree structure naturally. The simplicity of the tree enables the model to conduct global extraction. 243 6 Conclusions In this paper, we thoroughly analyze the effects of exposure bias of Seq2Seq models on joint entity and relation extraction. Exposure bias causes overfitting that hurts the reliability of the performance scores. To solve the problem of exposure bias, we"
2020.findings-emnlp.23,D19-1241,1,0.820988,") and propose a novel model Seq2UMTree. The Seq2UMTree model is based on an Encoder-Decoder framework, which is composed of a conventional encoder and a UMTree decoder. The UMTree decoder models entities and relations jointly and structurally, using a copy mechanism with unordered multi-label classification as the output layer. This multi-label classification model ensures the nodes in the same layer are unordered and discards the predefined triplet order so that the prediction deviation will not aggregate and affect other triplets. Different from the standard Seq2Tree (Dong and Lapata, 2016; Liu et al., 2019), the decoding length is limited to three (one triplet), which is the shortest feasible length for JERE task. In this way, the exposure bias is minimized under the triplet-level F1 metrics. In conclusion, our contributions are listed as follows: To mitigate the exposure bias problem while keeping the simplicity of Seq2Seq, we recast the one-dimension triplet sequence to two-dimension 237 • We point out the redundancy of the predefined triplet order of the Seq2Seq model, and propose a novel Seq2UMTree model to minimize exposure bias by recasting the ordered triplet sequence to an Unordered-Mult"
2020.findings-emnlp.23,D15-1102,0,0.027607,"xtraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an e"
2020.findings-emnlp.23,D15-1166,0,0.0236156,"sidered as depth 0, (b) relation embedding: wtr ∈ Rh , e2 h (c) entity embedding: wte = oe1 t−1 + ot−1 ∈ R , where e1 and e2 are the beginning position and the end position of the predicted entity respectively. t ∈ {1, 2, 3}, which is the decoding time step. The decoding order can be predefined arbitrarily, such as h-r-t or t-r-h. Given the input embedding wt and the output of the previous time step sD t−1 , a unary LSTM decoder is used to generate decoder hidden state: D sD t = Decoder(wt , st−1 ) (3) D where sD t is the decoder hidden states; s0 is iniE tialized by sn . Attention mechanism (Luong et al., 2015) is used to generate context-aware embedding: at = Attention(ot−1 , sD t ) (5) where Convde maps dimension 2h to h and at is replicated n times before concatenation. The output layer of the relation prediction is a linear transformation followed by a max-pooling over sequence: probr = σ(Max(ot Wr + br )) The output layers of the entity prediction are two binary classification layers over the whole sequence, predicting the positions of the beginning and the end of the entities respectively: probeb = σ(WeTb ot + beb ) probee = σ(WeTe ot + bee ) (7) where We ∈ Rh×1 , be is a scalar and probe ∈ Rn"
2020.findings-emnlp.23,P16-1105,0,0.174523,"ty extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can be assigned to one enti"
2020.findings-emnlp.23,D14-1200,0,0.0636813,"r supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are a"
2020.findings-emnlp.23,W04-2401,0,0.330979,"mple, in the triplet (Obama, graduate from, Columbia University), Obama and Columbia University are the head and tail entities appearing in the text, and graduate from is the relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a sin"
2020.findings-emnlp.23,D18-1249,0,0.0156993,"oint out that the classification errors may propagate across subtasks. Instead of treating these two subtasks separately, for joint entities and relations extraction (JERE), TABLE methods calculate the similarity score of all token pairs and relations by exhaustive enumeration and the extracted triplets are found by the position of the output in the table (Miwa and Bansal, 2016; Gupta et al., 2016). However, as a triplet may contain entities with different lengths, the table methods either suffer from exponential computational burden (Adel and Sch¨utze, 2017) or roll back to pipeline methods (Sun et al., 2018; Bekoulis et al., 2018; Fu et al., 2019). Furthermore, such table enumeration dilutes the positive labels quadratically, thus aggravating the classimbalanced problem. To model the task in a more concise way, Zheng et al. (2017) propose a N OVELTAGGING scheme, which represents relation and entity in one tag, so that the joint extraction can be solved by the well-studied sequence labeling approach. However, this tagging scheme cannot assign multiple tags to one token thus fail on overlapping triplets. The follow-on methods revise the tagging scheme to enable multi-pass sequence labeling (Takano"
2020.findings-emnlp.23,2020.acl-main.136,0,0.0997078,". As the exposure bias problem stems from the ordered left-to-right triplet decoding, we block the decoding of them from each other by removing the order of the triplet generation, thus the possible prediction error cannot propagate from triplet to triplet. Furthermore, because each triplet is generated by an independent decoding process, the decoding length has been extremely shortened, thus minimizes the effects of exposure bias. Our method differs from previous solution on exposure bias that we remove the order by structure decoding rather than random sampling (Tsai and Lee, 2019). CASREL (Wei et al., 2020) is a recently proposed two-step tagging method, which first finds all the head entities in the sentence then labels a relation-tail table for each head entity, which can also be seen as a UMTree decoder with a decoding length two. However, they overlook the data bias problem in NYT, which causing model unreliability and possible model bias. Note that our task is different from ONEIE (Lin et al., 2020), which models event extraction, entity span detection, entity type recognition and relation extraction in a Seq2Graph way. In contrast to ONEIE, JERE aims to extract only relation-entity triplet"
2020.findings-emnlp.23,P17-1113,0,0.504717,"asks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can be assigned to one entity, which solves the problem naturally (Zeng et al., 2018, 2019a,b; Nayak and Ng, 2019). Specifically, all existing Seq2Seq models pre-define a sequential order for the target triplets, e.g. triplet alphab"
2020.findings-emnlp.23,N16-1033,0,0.0501813,"udies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thu"
2020.findings-emnlp.23,P19-1518,0,0.0496315,"Missing"
2020.findings-emnlp.23,C18-1330,0,0.0233643,"lapping triplets problem. Although this paper introduces a problem that multi-token entities cannot be predicted, this problem has been solved by multiple follow-up papers (Zeng et al., 2019a; Nayak and Ng, 2019). However, there still remains a weakness in Seq2Seq models, i.e., the exposure bias, which has been overlooked. Exposure bias originates from the discrepancy between training and testing: Seq2Seq models use data distribution for training and model distribution for testing (Ranzato et al., 2015). Existing work mainly focuses on how to mitigate the information loss of arg max sampling (Yang et al., 2018, 2019; Zhang et al., 2019). Nam et al. (2017) notice that different orders affect the performance of the Seq2Seq models in Multi-Class Classification (MCC), and conduct thoroughly experiments on frequency order and topology order. In JERE, Zeng et al. (2019b) study additional rule-based triplet prediction orders, including alphabetical, shuffle and fix-unsort, and then propose a reinforcement learning framework to generate triplets in adaptive orders dynamically. Tsai and Lee (2019) first point out the unnecessary order causes exposure bias altering the performance in MCC, and they find that"
2020.findings-emnlp.23,C10-2160,0,0.0361426,"are the head and tail entities appearing in the text, and graduate from is the relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the predic"
2020.findings-emnlp.23,D19-1035,1,0.90365,"g phase, the UMTree uses auto-regressive decoding strategy. The decoder predicts the nodes layer by layer, where the prediction results of the previous layer are used as the input of the next time step separately, as shown in Fig. 3b. 3 3.1 Experiments Settings Dataset We evaluate our model on two datasets, NYT and DuIE1 . NYT (Riedel et al., 2010) is a English news dataset that is generated by distant supervision without manual annotation, which is widely used in JERE studies (Zheng et al., 2017; Zeng et al., 2018; Takanobu et al., 2018; Dai et al., 2019; Fu et al., 2019; Nayak and Ng, 2019; Zeng et al., 2019a,b; Chen et al., 2019; Wei et al., 2019). We use the same data split as CopyRE (Zeng et al., 2018). DuIE (Li et al., 2019) is a large-scale Chinese JERE dataset where sentences are from Baidu 1 https://ai.baidu.com/broad/introduction? dataset=dureader Baselines We compare the proposed model, Seq2UMTree, with strong baselines under the same hyperparameters, as follows: 1) CopyMTL (Zeng et al., 2019a) is a Seq2Seq model with copy mechanism, and the entities are found by multi-task learning. 2) WDec (Nayak and Ng, 2019) is a standard Seq2Seq model with dynamic masking, and decode the entity toke"
2020.findings-emnlp.23,P18-1047,1,0.937366,", 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can be assigned to one entity, which solves the problem naturally (Zeng et al., 2018, 2019a,b; Nayak and Ng, 2019). Specifically, all existing Seq2Seq models pre-define a sequential order for the target triplets, e.g. triplet alphabetical order, and then decode the triplet sequence according to the order autoregressively, which means the current triplet prediction relies on the previous output. For exmaple, in Figure 1, the triplet list is flattened to [Obama]-[graduate from][Columbia University]-[Obama]-[graduate from][Harvard Law School]... However, the autoregressive decoding of the Seq2Seq models introduces exposure bias problem which may severely reduce the performance."
2020.lrec-1.281,Y18-1026,1,0.819791,"Missing"
2020.lrec-1.281,W19-6704,1,0.241475,"texts from social media to infer author personality (e.g., Golbeck et al., 2011; Park et al., 2015; Plank and Hovy, 2015; Schwartz et al., 2013). This NLP approach requires texts for personality inference. We, however, need what personality a person is described with a personality descriptor. Accordingly, we developed a personality dictionary where word entries have weights that can be used to infer the five traits from each entry (Iwai et al., 2020). First, we developed a 20-item Big Five questionnaire, Trait Descriptors Personality Inventory (TDPI), based on the responses of 17,591 people (Iwai et al., 2019b). Each item contains a personality word obtained from English personality adjectives, using word embeddings and phrase-based statistical machine translation. We collected 527 personality words from the seeds of 116 personality descriptors obtained in the development process (Iwai et al., 2017; Iwai, Kumada et al., 2018; Iwai, Kawahara et al., 2019b), using word embeddings trained with 200 million Japanese sentences. Furthermore, we conducted a websurvey on 1,938 participants to evaluate their personality based on each personality descriptor in addition to TDPI (Iwai, Kawahara et al., 2019b),"
2020.lrec-1.281,2020.lrec-1.379,1,0.872345,"nfirmed in Japanese (Kashiwagi et al., 2005; Oshio et al., 2014). Meanwhile, NLP researchers have used Big Five to develop language models from a certain volume of texts from social media to infer author personality (e.g., Golbeck et al., 2011; Park et al., 2015; Plank and Hovy, 2015; Schwartz et al., 2013). This NLP approach requires texts for personality inference. We, however, need what personality a person is described with a personality descriptor. Accordingly, we developed a personality dictionary where word entries have weights that can be used to infer the five traits from each entry (Iwai et al., 2020). First, we developed a 20-item Big Five questionnaire, Trait Descriptors Personality Inventory (TDPI), based on the responses of 17,591 people (Iwai et al., 2019b). Each item contains a personality word obtained from English personality adjectives, using word embeddings and phrase-based statistical machine translation. We collected 527 personality words from the seeds of 116 personality descriptors obtained in the development process (Iwai et al., 2017; Iwai, Kumada et al., 2018; Iwai, Kawahara et al., 2019b), using word embeddings trained with 200 million Japanese sentences. Furthermore, we"
2020.lrec-1.281,C08-1111,0,0.0753523,"Missing"
2020.lrec-1.379,W19-6704,1,0.356478,"e users’ personalities by using a certain amount of texts (e.g., Golbeck et al., 2011; Nasukawa et al., 2016; Nasukawa and Kamijo, 2017; Park et al., 2015; Plank and Hovy, 2011; Schwartz et al., 2013). They developed models to infer the self-evaluated personalities. However, 3103 this approach does not provide information on what words represent the personality or the weights for personality inference. Our goal was to acquire personality words that allow computers to infer an individual’s personality from a single personality word. 2.3 An Incorporative Approach Recently, Iwai, Kawahara et al. (2019) introduced a new Big Five questionnaire in Japanese named as TraitDescriptors Personality Inventory (TDPI). To the best of our knowledge, it is the only personality measurement developed by NLP techniques such as word embeddings and phrase-based statistical machine translation. It was constructed based on the responses of more than 40,000 Japanese people (Iwai et al., 2017, 2018; Ueda et al., 2016). In addition, Iwai, Kawahara et al. (2019) demonstrated reliability and the five-factor structure replications among different samples. However, it only included 20 items, and one personality word"
2020.lrec-1.379,2020.lrec-1.281,1,0.720122,"). …車が、…直進車を妨害してる。…なぜ かこういう自分勝手な運転手が増えるん だよなぁ。… a car, …., is obstructing another straight traveling car. Somehow such selfish drivers increase. If computers know that a car obstructing another straight traveling car is selfish and disconscientious, computers can predict that the selfish driver causes similar disconscientious driving behaviors such as sudden turns at the corner of an intersection or lane changes without any signals. By using the personality dictionary and the Driving Behavior and Subjectivity corpus, we acquired social knowledge about personality and driving-related behavior (Iwai et al., 2020). 6. Conclusions In this study, we developed a Japanese personality dictionary that comprises two sub-dictionaries, using psychological methods and statistical analyses. To the best of our knowledge, this is the first Japanese language resource developed based on theories and methodologies of personality psychology and has such weights. Furthermore, it is the only Japanese personality dictionary that is available for NLP researchers. The interests in human-machine interactions such as virtual agents, chat bots, and social robots are growing. Our dictionary and methodology will inspire those st"
2021.naacl-srw.15,C18-1179,0,0.0419632,"Missing"
2021.naacl-srw.15,D19-1445,0,0.0131715,"o generate a response with both rationality and emotion. Through latent concepts obtained from an emotionally aware knowledge graph, predicted responses can be emotional and rational. Actually, the above models require separate units or special architecture for understanding emotion in a dialogue. In contrast, our proposed model achieves that with a single structure, inherited from Transformer (Vaswani et al., 2017) and BART (Lewis et al., 2020). In other words, our model does not need an extra unit. Therefore, the proposed method consequently reduces the redundancy of Transformer parameters (Kovaleva et al., 2019) and realizes more efficient understanding of emotion to generate a response. 3 3.1 Emotion-Aware Response Generation by Multi-Task Learning Overview thus the performance of each task can be improved. However, the purpose of our multi-task learning method is to improve the quality of response generation, not to improve the performance of emotion recognition. This is different from general multitask learning. Our model is based on BART (Lewis et al., 2020). Its architecture is shown in Figure 1. The model has several output layers, or heads, for the tasks to be trained, which include an LM head"
2021.naacl-srw.15,2020.acl-main.703,0,0.353615,"s simultaneously. Furthermore, we weight the losses for the tasks to control the update of parameters. Automatic evaluations and crowdsourced manual evaluations show that the proposed model makes generated responses more emotionally aware. 1 Introduction previous work (Liu et al., 2019), and we do not aim at improving the accuracy of emotion recognition. Instead, we focus on generating emotion-aware responses. Also, concerned that the ratio of emotion recognition in multi-task learning is too large, we explore further quality improvement by weighting each loss. We build a model based on BART (Lewis et al., 2020), a pre-trained Transformer (Vaswani et al., 2017) model, to implement multi-task learning of response generation and emotion recognition. Experiments are performed using a dialogue corpus without context. The effectiveness of the proposed method in generating responses is confirmed by automatic and manual evaluations. Multi-task learning of response generation and emotion recognition makes generated responses more emotionally aware of utterances. The improvement is not only on the emotional aspect but also on the quality of fluency, informativeness, and relevance. We also found that controlli"
2021.naacl-srw.15,N16-1014,0,0.0324635,"likelihood loss. The number of input and output tokens is set to 64, and training is performed for 64 epochs. We use beam search with 5 beams to select words and eliminate cases where there are more than three repeated n-grams. Training and generation are performed on NVIDIA Tesla V100. 4.3 Evaluation Metrics We evaluate the trained models automatically and manually. Automatic Evaluation First, we evaluate how much the output responses are related to the correct response using B LEU (Papineni et al., 2002). Second, we evaluate whether the output responses are lexically diverse using distinct (Li et al., 2016). For distinct, distinct-1 and distinct-2 are calculated, which focus on unigrams and bigrams, respectively. We also compare the average number of words in output responses, which is based on the assumption 2 https://github.com/pytorch/fairseq/ blob/master/examples/bart/README. summarization.md. 4.4 Results Multi-Task Learning The evaluation results are shown in Table 2. The response generation is denoted by R, and the emotion recognition for the Twitter Emotion Corpus, SST-2, and CrowdFlower datasets is denoted by E6, E2, and E12, respectively. In terms of automatic evaluation, R+E6+E2 and R+"
2021.naacl-srw.15,I17-1099,0,0.0518413,"Missing"
2021.naacl-srw.15,D16-1230,0,0.0682607,"Missing"
2021.naacl-srw.15,P19-1441,0,0.163396,"with a human, it needs to be human-like. In this paper, we propose a neural response generation model with multi-task learning of generation and classification, focusing on emotion. Our model based on BART (Lewis et al., 2020), a pre-trained transformer encoderdecoder model, is trained to generate responses and recognize emotions simultaneously. Furthermore, we weight the losses for the tasks to control the update of parameters. Automatic evaluations and crowdsourced manual evaluations show that the proposed model makes generated responses more emotionally aware. 1 Introduction previous work (Liu et al., 2019), and we do not aim at improving the accuracy of emotion recognition. Instead, we focus on generating emotion-aware responses. Also, concerned that the ratio of emotion recognition in multi-task learning is too large, we explore further quality improvement by weighting each loss. We build a model based on BART (Lewis et al., 2020), a pre-trained Transformer (Vaswani et al., 2017) model, to implement multi-task learning of response generation and emotion recognition. Experiments are performed using a dialogue corpus without context. The effectiveness of the proposed method in generating respons"
2021.naacl-srw.15,S12-1033,0,0.115393,"Missing"
2021.naacl-srw.15,P02-1040,0,0.110364,"imized by Adam with weight decay. For response generation, we apply label smoothing of 0.1 to the negative log-likelihood loss. The number of input and output tokens is set to 64, and training is performed for 64 epochs. We use beam search with 5 beams to select words and eliminate cases where there are more than three repeated n-grams. Training and generation are performed on NVIDIA Tesla V100. 4.3 Evaluation Metrics We evaluate the trained models automatically and manually. Automatic Evaluation First, we evaluate how much the output responses are related to the correct response using B LEU (Papineni et al., 2002). Second, we evaluate whether the output responses are lexically diverse using distinct (Li et al., 2016). For distinct, distinct-1 and distinct-2 are calculated, which focus on unigrams and bigrams, respectively. We also compare the average number of words in output responses, which is based on the assumption 2 https://github.com/pytorch/fairseq/ blob/master/examples/bart/README. summarization.md. 4.4 Results Multi-Task Learning The evaluation results are shown in Table 2. The response generation is denoted by R, and the emotion recognition for the Twitter Emotion Corpus, SST-2, and CrowdFlow"
2021.naacl-srw.15,P18-1205,0,0.0234188,"s on emotion-based To interact naturally with a human, the computer response generation is the Emotional Chatting needs to be human-like. Several methods have been proposed to build such dialogue systems. They in- Machine (ECM) (Zhou et al., 2018). ECM is used together with an emotion classifier to clude a system interacting based on knowledge and common sense (Dinan et al., 2019) and that inter- generate a response based on a given emotion. EmpTransfo (Zandie and Mahoor, 2020) is a simacting by considering one’s own and the other’s ilar model to ours. Given an utterance, a model personality (Zhang et al., 2018). In particular, we based on GPT (Radford et al., 2018) learns an emofocus on the viewpoint of emotion as targeted in tion and an action simultaneously in addition to a Rashkin et al. (2019). response, which improves the quality of generated In this paper, we propose a multi-task learning responses. These models focus on the emotion of method for building a dialogue system that takes the speaker’s emotions into account. Also, we fo- a response so that they do not generate a response cus on the hierarchy of emotions (Kumar et al., based on the emotion of an utterance. 2019) and simultaneously t"
2021.naacl-srw.15,P19-1534,0,0.0525461,"ialogue systems. They in- Machine (ECM) (Zhou et al., 2018). ECM is used together with an emotion classifier to clude a system interacting based on knowledge and common sense (Dinan et al., 2019) and that inter- generate a response based on a given emotion. EmpTransfo (Zandie and Mahoor, 2020) is a simacting by considering one’s own and the other’s ilar model to ours. Given an utterance, a model personality (Zhang et al., 2018). In particular, we based on GPT (Radford et al., 2018) learns an emofocus on the viewpoint of emotion as targeted in tion and an action simultaneously in addition to a Rashkin et al. (2019). response, which improves the quality of generated In this paper, we propose a multi-task learning responses. These models focus on the emotion of method for building a dialogue system that takes the speaker’s emotions into account. Also, we fo- a response so that they do not generate a response cus on the hierarchy of emotions (Kumar et al., based on the emotion of an utterance. 2019) and simultaneously train multiple emotion Lubis et al. (2018) incorporate an emotion enrecognition tasks with different granularity. Our coder into a hierarchical seq2seq architecture, enmulti-task learning mod"
2021.naacl-srw.15,D13-1170,0,0.00692655,"Missing"
C00-1063,A97-1052,0,0.0619708,"Missing"
C00-1063,J94-4001,1,0.802931,"s of about 40,000 analyzed sentences of newspaper articles, very basic verbs like tetsudau ‘help’ or uketsukeru ‘accept’ appear only 10 times or 15 times respectively. It is obvious that such small data are insufficient for automatic case frame learning. That is, case frame learning must be done from enormous unanalyzed corpora, in unsupervised way2 . We can collect pairs of verbs and case components from the automatic analyses of large corpora by KNP. 3.1 Good parser NLP research group at Kyoto University has been developing a robust and accurate parsing system, KNP, over the last ten years (Kurohashi and Nagao, 1994; Kurohashi and Nagao, 1998). This parser has the following advantages: The following sections explain how to solve these problems. 3.2.1 Word sense ambiguity If a verb has two or more meanings and their case frame patterns differ, we have to disambiguate the sense of each occurrence of the verb in a corpus first, and collect case components for each sense respectively. However, unsupervised word sense disambiguation of free texts is one of the most difficult problems in NLP. At the very beginning, even the definition of word senses is open to question. To cope with this problem, we made a ver"
C00-1063,P98-2214,0,0.0188322,"such as scrambling, omission of case components, and disappearance of case markers. Therefore, in Japanese sentence analysis, case structure analysis is an important issue, and a case frame dictionary is necessary for the analysis. Some research institutes have constructed Japanese case frame dictionaries manually (Ikehara et al., 1997; Information-Technology Promotion Agency, Japan, 1987). However, it is quite expensive, or almost impossible to construct a wide-coverage case frame dictionary by hand. Others have tried to construct a case frame dictionary automatically from analyzed corpora (Utsuro et al., 1998). However, existing syntactically analyzed corpora are too small to learn a dictionary, since case frame information consists of relations between nouns and verbs, which multiplies to millions of combinations. Based on such a consideration, we took the following unsupervised learning strategy to the Japanese case structure analysis: 1. At first, a robust and accurate parser is developed, which does not utilize a case frame dictionary, 2. a very large corpus is parsed by the parser, 3. reliable noun-verb relations are extracted from the parse results, and a case frame dictionary is constructed"
C00-1063,P93-1032,0,\N,Missing
C02-1122,H01-1043,1,0.878382,"ame dictionary by incorporating newly acquired information. 1 Introduction To understand a text, it is necessary to find out relations between words in the text. What is required to do so is a case frame dictionary. It describes what kinds of cases each verb has and what kinds of nouns can fill a case slot. Since these relations have millions of combinations, it is difficult to construct a case frame dictionary by hand. We proposed a method to construct a Japanese case frame dictionary automatically by arranging large volumes of parse results by coupling a verb and its closest case component (Kawahara and Kurohashi, 2001). This case frame dictionary, however, could not handle complicated expressions: double nominative sentences, non-gapping relation of relative clauses, and case change. This paper proposes a method of fertilizing the case frame dictionary to handle these complicated expressions. We take an iterative method which consists of two stages. This means gradual learning of what is understood by an analyzer in each stage. In the first stage, we parse a large raw corpus and construct a Japanese case frame dictionary automatically from the parse results. This is the method proposed by (Kawahara and Kuro"
C02-1122,kawahara-etal-2002-construction,1,0.780805,"Missing"
C04-1050,W03-2604,0,0.0537192,"Missing"
C04-1050,W03-1024,0,0.161186,"Missing"
C04-1050,C02-1122,1,0.946217,"ined in the NTT thesaurus, respectively. This section describes the NTT thesaurus and the case frames briefly. 2.1 NTT thesaurus NTT Communication Science Laboratories constructed a semantic feature tree, whose 3,000 nodes are semantic features, and a nominal dictionary containing about 300,000 nouns, each of which is given one or more appropriate semantic features. Figure 1 shows the upper levels of the semantic feature tree. The similarity between two words is defined by formula (1) in Appendix A. 2.2 Automatically constructed case frames We employ the automatically constructed case frames (Kawahara and Kurohashi, 2002) as the basic resource for zero pronoun resolution and word sense disambiguation. This section outlines the method of constructing the case frames. The biggest problem in automatic case frame construction is verb sense ambiguity. Verbs which have different meanings should have different case frames, but it is hard to disambiguate verb senses precisely. To deal with this problem, predicate-argument examples which are collected from a large corpus are distinguished by coupling a verb and its closest case component. That is, examples are not distinguished by verbs (e.g. “tsumu” (load/accumulate))"
C04-1050,kawahara-etal-2002-construction,1,0.788048,"method 526/911 (0.577) 526/1087 (0.484) F 0.512 0.527 Table 3: Accuracy (cooking). precision recall 696/1092 (0.637) 696/1482 (0.470) 713/1081 (0.660) 713/1482 (0.481) F 0.541 0.556 baseline our method case frames have <agent> in their “ga” case slots). In our approach, each of the semantic features are matched against the case example “soup”, and only the best matched semantic feature <soup> is given to “osumashi ”. 5 Experimental Results and Discussion We conducted experiments of zero pronoun resolution on two different domain corpora. One is newspaper articles of “Relevance-tagged corpus” (Kawahara et al., 2002), and the other is utterances of cooking TV programs. These cooking utterances were handled by (Shibata et al., 2003). They annotated various relations to closed captions of the cooking utterances based on the specification of the “Relevance-tagged corpus” (Kawahara et al., 2002). For newspaper domain, the antecedent preference and the classifier were trained with 1,841 sentences in the newspaper corpus, and the newspaper case frames were used. The experiment was performed on 633 sentences. For cooking domain, we used 813 sentences (5 TV programs), and conducted 5-fold cross validation using t"
C04-1050,P02-1014,0,0.0998525,"Missing"
C04-1050,P03-1023,0,0.0291639,"Missing"
C04-1050,J94-4001,1,\N,Missing
C04-1174,A97-1052,0,0.0397582,"en a noun and its indispensable entity is parallel to that between a verb and its arguments or obligatory cases. In this paper, we call indispensable entities of nouns obligatory cases. Indirect anaphora resolution needs a comprehensive information or dictionary of obligatory cases of nouns. In case of verbs, syntactic structures such as subject/object/PP in English or case markers such as ga, wo, ni in Japanese can be utilized as a strong clue to distinguish several obligatory cases and adjuncts (and adverbs), which makes it feasible to construct case frames from large corpora automatically (Briscoe and Carroll, 1997; Kawahara and Kurohashi, 2002). (Kawahara and Kurohashi, 2004) then utilized the automatically constructed case frames to Japanese zero pronoun resolution. On the other hand, in case of nouns, obligatory cases of noun Nh appear, in most cases, in the single form of noun phrase “Nh of Nm ” in English, or “Nm no Nh ” in Japanese. This single form can express several obligatory cases, and furthermore optional cases, for example, “rugby no coach” (obligatory case concerning what sport), “club no coach” (obligatory case concerning which institution), and “kyonen ‘last year’ no coach” (optional cas"
C04-1174,C96-1084,0,0.117274,"y no coach” (obligatory case concerning what sport), “club no coach” (obligatory case concerning which institution), and “kyonen ‘last year’ no coach” (optional case). Therefore, the key issue to construct nominal case frames is to analyze “Nh of Nm ” or “Nm no Nh ” phrases to distinguish obligatory case examples and others. Work which addressed indirect anaphora in English texts so far restricts relationships to a small, relatively well-defined set, mainly part-of relation like the above example (2), and utilized hand-crafted heuristic rules or hand-crafted lexical knowledge such as WordNet (Hahn et al., 1996; Vieira and Poesio, 2000; Strube and Hahn, 1999). (Poesio et al., 2002) proposed a method of acquiring lexical knowledge from “Nh of Nm ” phrases, but again concentrated on part-of relation. In case of Japanese text analysis, (Murata et al., 1999) proposed a method of utilizing “Nm no Nh ” phrases for indirect anaphora resolution of diverse relationships. However, they basically used all “Nm no Nh ” phrases from corpora, just excluding some pre-fixed stop words. They confessed that an accurate analysis of “Nm no Nh ” phrases is necessary for the further improvement of indirect anaphora resolu"
C04-1174,C02-1122,1,0.873634,"able entity is parallel to that between a verb and its arguments or obligatory cases. In this paper, we call indispensable entities of nouns obligatory cases. Indirect anaphora resolution needs a comprehensive information or dictionary of obligatory cases of nouns. In case of verbs, syntactic structures such as subject/object/PP in English or case markers such as ga, wo, ni in Japanese can be utilized as a strong clue to distinguish several obligatory cases and adjuncts (and adverbs), which makes it feasible to construct case frames from large corpora automatically (Briscoe and Carroll, 1997; Kawahara and Kurohashi, 2002). (Kawahara and Kurohashi, 2004) then utilized the automatically constructed case frames to Japanese zero pronoun resolution. On the other hand, in case of nouns, obligatory cases of noun Nh appear, in most cases, in the single form of noun phrase “Nh of Nm ” in English, or “Nm no Nh ” in Japanese. This single form can express several obligatory cases, and furthermore optional cases, for example, “rugby no coach” (obligatory case concerning what sport), “club no coach” (obligatory case concerning which institution), and “kyonen ‘last year’ no coach” (optional case). Therefore, the key issue to"
C04-1174,kawahara-etal-2002-construction,1,0.8331,"the highly restricted conditions in the example collection. For instance, maker does not have obligatory case slot for its products. This is because maker is usually used in the form of compound noun phrase, “products maker ”, and there are few occurrences of “products no maker ”. To address this problem, not only “Nm no Nh ” but also “Nm Nh ” (compound noun phrase) and “Nm ni-kansuru ‘in terms of’ Nh ” should be collected. 6.2 Experimental results of indirect anaphora resolution We conducted a preliminary experiment of our indirect anaphora resolution system using “Relevance-tagged corpus” (Kawahara et al., 2002). This corpus consists of Japanese newspaper articles, and has relevance tags, including antecedents of indirect anaphors. We prepared a small test corpus that consists of randomly selected 10 articles. The test corpus contains 217 nouns. Out of them, 106 nouns are indirect anaphors, and have 108 antecedents, which is because two nouns have double antecedents. 49 antecedents directly depend on their anaphors, and 59 do not. For 91 antecedents out of 108, a case frame of its anaphor Table 6: Experimental results of indirect anaphora resolution. precision recall F w dep. 40/46 (0.870) 40/59 (0.6"
C04-1174,J94-4001,1,0.804365,"] relief, margin, · · · (a boxlike container in a desk or a chest.) [desk, chest] desk, chest, dresser, · · · &lt;other&gt; credit, fund, saving, · · · (a person who teaches technique in some sport.) [sport] baseball, swimming, · · · &lt;belonging&gt; team, club, · · · (the total value of a company’s shares.) [company] company, corporation, · · · soccer-no Indirect Anaphora Resolution To examine the practical usefulness of the constructed nominal case frames, we built a preliminary system of indirect anaphora resolution based on the case frames. An input sentence is parsed using the Japanese parser, KNP (Kurohashi and Nagao, 1994). Then, from the beginning of the sentence, each noun x is analyzed. When x has more than one case frame, the process of antecedent estimation (stated in the next paragraph) is performed for each case frame, and the case frame with the highest similarity score (described below) and assignments of antecedents to the case frame are selected as a final result. For each case slot of the target case frame of x, its antecedent is estimated. A possible antecedent y in the target sentence and the previous two sentences is checked. This is done one by one, from the syntactically closer y. If the simila"
C04-1174,P99-1062,1,0.908492,", 2002) proposed a method of acquiring lexical knowledge from “Nh of Nm ” phrases, but again concentrated on part-of relation. In case of Japanese text analysis, (Murata et al., 1999) proposed a method of utilizing “Nm no Nh ” phrases for indirect anaphora resolution of diverse relationships. However, they basically used all “Nm no Nh ” phrases from corpora, just excluding some pre-fixed stop words. They confessed that an accurate analysis of “Nm no Nh ” phrases is necessary for the further improvement of indirect anaphora resolution. As a response to these problems and following the work in (Kurohashi and Sakai, 1999), we propose a method to construct Japanese nominal case frames from large corpora, based on an accurate analysis of “Nm no Nh ” phrases using an ordinary dictionary and a thesaurus. To examine the practical usefulness of the constructed nominal case frames, we also built a system of indirect anaphora resolution based on the case frames. 2 Semantic Feature Dictionary First of all, we briefly introduce NTT Semantic Feature Dictionary employed in this paper. NTT Semantic Feature Dictionary consists of a semantic feature tree, whose 3,000 nodes are semantic features, and a nominal dictionary cont"
C04-1174,W99-0206,0,0.0263306,"f Nm ” or “Nm no Nh ” phrases to distinguish obligatory case examples and others. Work which addressed indirect anaphora in English texts so far restricts relationships to a small, relatively well-defined set, mainly part-of relation like the above example (2), and utilized hand-crafted heuristic rules or hand-crafted lexical knowledge such as WordNet (Hahn et al., 1996; Vieira and Poesio, 2000; Strube and Hahn, 1999). (Poesio et al., 2002) proposed a method of acquiring lexical knowledge from “Nh of Nm ” phrases, but again concentrated on part-of relation. In case of Japanese text analysis, (Murata et al., 1999) proposed a method of utilizing “Nm no Nh ” phrases for indirect anaphora resolution of diverse relationships. However, they basically used all “Nm no Nh ” phrases from corpora, just excluding some pre-fixed stop words. They confessed that an accurate analysis of “Nm no Nh ” phrases is necessary for the further improvement of indirect anaphora resolution. As a response to these problems and following the work in (Kurohashi and Sakai, 1999), we propose a method to construct Japanese nominal case frames from large corpora, based on an accurate analysis of “Nm no Nh ” phrases using an ordinary di"
C04-1174,poesio-etal-2002-acquiring,0,0.0848411,"Missing"
C04-1174,J99-3001,0,0.0354057,"sport), “club no coach” (obligatory case concerning which institution), and “kyonen ‘last year’ no coach” (optional case). Therefore, the key issue to construct nominal case frames is to analyze “Nh of Nm ” or “Nm no Nh ” phrases to distinguish obligatory case examples and others. Work which addressed indirect anaphora in English texts so far restricts relationships to a small, relatively well-defined set, mainly part-of relation like the above example (2), and utilized hand-crafted heuristic rules or hand-crafted lexical knowledge such as WordNet (Hahn et al., 1996; Vieira and Poesio, 2000; Strube and Hahn, 1999). (Poesio et al., 2002) proposed a method of acquiring lexical knowledge from “Nh of Nm ” phrases, but again concentrated on part-of relation. In case of Japanese text analysis, (Murata et al., 1999) proposed a method of utilizing “Nm no Nh ” phrases for indirect anaphora resolution of diverse relationships. However, they basically used all “Nm no Nh ” phrases from corpora, just excluding some pre-fixed stop words. They confessed that an accurate analysis of “Nm no Nh ” phrases is necessary for the further improvement of indirect anaphora resolution. As a response to these problems and followi"
C04-1174,J00-4003,0,0.0764556,"tory case concerning what sport), “club no coach” (obligatory case concerning which institution), and “kyonen ‘last year’ no coach” (optional case). Therefore, the key issue to construct nominal case frames is to analyze “Nh of Nm ” or “Nm no Nh ” phrases to distinguish obligatory case examples and others. Work which addressed indirect anaphora in English texts so far restricts relationships to a small, relatively well-defined set, mainly part-of relation like the above example (2), and utilized hand-crafted heuristic rules or hand-crafted lexical knowledge such as WordNet (Hahn et al., 1996; Vieira and Poesio, 2000; Strube and Hahn, 1999). (Poesio et al., 2002) proposed a method of acquiring lexical knowledge from “Nh of Nm ” phrases, but again concentrated on part-of relation. In case of Japanese text analysis, (Murata et al., 1999) proposed a method of utilizing “Nm no Nh ” phrases for indirect anaphora resolution of diverse relationships. However, they basically used all “Nm no Nh ” phrases from corpora, just excluding some pre-fixed stop words. They confessed that an accurate analysis of “Nm no Nh ” phrases is necessary for the further improvement of indirect anaphora resolution. As a response to th"
C08-1054,D07-1032,1,0.550722,"ordinated. Such selectional preferences are thought to support the construction of coordinate structures and to yield similarity between conjuncts on the contrary. We present a method of coordination disambiguation without using similarities. Coordinate structures are supported by their surrounding dependency relations that provide selectional preferences. These relations implicitly work as similarities, and thus it is not necessary to use similarities explicitly. In this paper, we focus on Japanese. Coordination disambiguation is integrated in a fullylexicalized generative dependency parser (Kawahara and Kurohashi, 2007). For the selectional preferences, we use lexical knowledge, such as case frames, which is extracted from a large raw corpus. The remainder of this paper is organized as follows. Section 2 summarizes previous work related to coordination disambiguation and its integration into parsing. Section 3 brieﬂy describes the background of this study. Section 4 overviews our idea, and section 5 describes our model in detail. Section 6 is devoted to our experiments. Finally, section 7 gives the conclusions. 425 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), p"
C08-1054,W02-2016,0,0.201047,"Missing"
C08-1054,J94-4001,1,0.578675,"d the effectiveness of our approach, and endorsed our hypothesis. 1 Introduction The interpretation of coordinate structures directly affects the meaning of the text. Addressing coordination ambiguities is fundamental to natural language understanding. Previous studies on coordination disambiguation suggested that conjuncts in coordinate structures have syntactic or semantic similarities, and dealt with coordination ambiguities using (sub-)string matching, part-ofspeech matching, semantic similarities, and so forth (Agarwal and Boggess, 1992). Semantic similarities are acquired from thesauri (Kurohashi and Nagao, 1994; Resnik, 1999) or distributional similarity (Chantree et al., 2005). c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. The above methods detect the similarity between salad and pasta using a thesaurus or distributional similarity, and identify the coordinate structure that conjoins salad and pasta. They do not use the information of the word eat. On the other hand, this coordinate structure can be analyzed by using selectional preference of eat. Since eat is likely"
C08-1054,P98-2127,0,0.0137177,"is large enough to train a supervised parser. 6.2 Discussion We presented a method for coordination disambiguation without using similarities, and this method achieved better performance than the conventional approaches based on similarities. Though we do not use similarities, we implicitly consider similarities between conjuncts. This is because the heads of pre- and post-conjuncts share a case marker and a predicate, and thus they are essentially similar. Our idea is related to the notion of distributional similarity. Chantree et al. (2005) applied the distributional similarity proposed by Lin (1998) to coordination disambiguation. Lin extracted from a corpus dependency triples of two words and the grammatical relationship between them, and considered that similar words are likely to have similar dependency relations. The difference between Chantree et al. (2005) and ours is that their method does not use the information of verbs in the sentence under consideration, but use only the cooccurrence information extracted from a corpus. On the other hand, the disadvantage of our model is that it cannot consider the parallelism of conjuncts, which still seems to exist in especially strong coord"
C08-1054,P92-1003,0,0.115562,"es coordination disambiguation. Experimental results on web sentences indicated the effectiveness of our approach, and endorsed our hypothesis. 1 Introduction The interpretation of coordinate structures directly affects the meaning of the text. Addressing coordination ambiguities is fundamental to natural language understanding. Previous studies on coordination disambiguation suggested that conjuncts in coordinate structures have syntactic or semantic similarities, and dealt with coordination ambiguities using (sub-)string matching, part-ofspeech matching, semantic similarities, and so forth (Agarwal and Boggess, 1992). Semantic similarities are acquired from thesauri (Kurohashi and Nagao, 1994; Resnik, 1999) or distributional similarity (Chantree et al., 2005). c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. The above methods detect the similarity between salad and pasta using a thesaurus or distributional similarity, and identify the coordinate structure that conjoins salad and pasta. They do not use the information of the word eat. On the other hand, this coordinate structur"
C08-1054,H05-1105,0,0.106678,"Missing"
C08-1054,P06-1033,0,0.0200042,"ours is that their method does not use the information of verbs in the sentence under consideration, but use only the cooccurrence information extracted from a corpus. On the other hand, the disadvantage of our model is that it cannot consider the parallelism of conjuncts, which still seems to exist in especially strong coordinate structures. Handling of such parallelism is an open question of our model. The generation process adopted in this work is similar to the design of dependency structure described in Hudson (1990), which lets the conjuncts have a dependency relation to the predicate. Nilsson et al. (2006) mentioned this notion, but did not consider this idea in their experiments of tree transformations for data-driven dependency parsers. In addition, it is not necessary for our method to transform dependency trees in pre- and post-processes, because we just changed the process of generation in the generative parser. 7 Conclusion In this paper, we ﬁrst came up with a hypothesis that coordinate structures are supported by 431 ? ? (1) densya-no hassyaaizu-ya, keitaidenwa-no tyakushinon-madega ongaku-ni train-GEN departure signal cell phone-GEN ring tone-also music-ACC (departure signals of trains"
C08-1054,P05-1022,0,0.0185262,"Missing"
C08-1054,P07-1122,0,0.0473309,"Missing"
C08-1054,P06-1053,0,0.176801,"Missing"
C08-1054,P99-1081,0,0.0512103,"Missing"
C08-1054,P07-1086,0,0.302116,"Missing"
C08-1054,kawahara-kurohashi-2006-case,1,0.929336,"verb sokushin-sareta (stimulated). However, (b) is not appropriate, because we cannot say the nominal compound “jinkou-no osen” (pollution of population). In (c) and (d), the heads of conjuncts, zouka (increase) and taiki (air), are generated from osen (pollution). These cases are also inappropriate, because we cannot say the nominal compound “zouka-no osen” (pollution of increase). Accordingly, in this case, the correct scope, (a), is derived based on the selectional preferences of predicates and nouns. In this framework, we require selectional preferences. We use case frames for predicates (Kawahara and Kurohashi, 2006) and occurrences of noun-noun modiﬁcations for nouns. Both of them are extracted from a large amount of raw text. 5 Our Model of Coordination Disambiguation This section describes an integrated model of coordination disambiguation in a generative parsing framework. First, we describe resources for selectional preferences, and then illustrate our model of coordination disambiguation. 5.1 Resources for Selectional Preferences As the resources of selectional preferences to support coordinate structures, we use automatically constructed case frames and cooccurrences of noun-noun modiﬁcations. 5.1."
C08-1054,C04-1002,0,0.179018,"Missing"
C08-1054,D07-1064,0,0.69978,"Missing"
C08-1054,D07-1063,0,0.0243001,"Missing"
C08-1054,C98-2122,0,\N,Missing
C08-1097,P06-1079,0,0.46045,"Missing"
C08-1097,W03-1024,0,0.427214,"Missing"
C08-1097,N06-1023,1,0.763607,"antecedents. Seki et al. (2002) proposed a probabilistic model for zero pronoun detection and resolution that uses hand-crafted case frames. In order to alleviate the sparseness of hand-crafted case frames, Kawahara and Kurohashi (2004) introduced wide-coverage case frames to zero pronoun detection that are automatically constructed from a large corpus. They use the case frames as selectional restriction for zero pronoun resolution, but do not utilize the frequency of each example of case slots. However, since the frequency is shown to be a good clue for syntactic and case structure analysis (Kawahara and Kurohashi, 2006), we consider the frequency also can beneﬁt zero pronoun detection. Therefore we propose a probabilistic model for zero anaphora resolution that fully utilizes case frames. This model directly considers the frequency and estimates case assignments for overt case components and antecedents of zero pronoun simultaneously. In addition, our model directly links each zero pronoun to an entity, while most existing models link it to a certain mention of an entity. In our model, mentions and zero pronouns are treated similarly and all of them are linked to corresponding entities. In this point, our mo"
C08-1097,J94-4002,0,0.226897,"is problem, we utilize generalized examples. When one mention of an entity is tagged any category or recognized as an NE, we also use the category or the NE class as the content part of the entity. For examples, if an entity {Prius} is recognized as an artifact name and assigned to wo case of the case frame hanbai(1) in Table 1, the system also calculates: P (NE :ARTIFACT |hanbai(1), wo, A0(wo)=1) P (NE : ARTIFACT ) besides: P (P rius|hanbai(1), wo, A0 (wo) = 1) P (P rius) and uses the higher value. 3.3 Salience Score Previous works reported the usefulness of salience for anaphora resolution (Lappin and Leass, 1994; Mitkov et al., 2002). In order to consider salience of an entity, we introduce salience score, which is calculated by the following set of simple rules: • • • • +2 : mentioned with topical marker “wa”. +1 : mentioned without topical marker “wa”. +0.5 : assigned to a zero pronoun. ×0.7 : beginning of each sentence. For examples, we consider the salience score of the entity {Toyota} in (i) in Section 3.1. In the ﬁrst sentence, since {Toyota} is mentioned with topical marker “wa”, the salience score is 2. At the beginning of the second sentence it becomes 1.4, 774 4.2 Experimental Results Table"
C08-1097,N07-1010,0,0.0147514,"ce on Computational Linguistics (Coling 2008), pages 769–776 Manchester, August 2008 examples he, driver, friend, · · · baggage, luggage, hay, · · · car, truck, vessel, seat, · · · player, children, party, · · · experience, knowledge, · · · generalized examples with rate company, Microsoft, ﬁrm, · · · goods, product, ticket, · · · customer, company, user, · · · shop, bookstore, site · · · [NE:ORGANIZATION]:0.16, [CT:ORGANIZATION]:0.13, · · · [CT:ARTIFACT]:0.40, [CT:FOOD]:0.07, · · · [CT:PERSON]:0.28, · · · [CT:FACILITY]:0.40, [CT:LOCATION]:0.39, · · · ... ... the coreference model proposed by Luo (2007) and that proposed by Yang et al. (2008). Due to this characteristic, our model can utilize information beyond a mention and easily consider salience (the importance of an entity). 2 [CT:PERSON]:0.45, [NE:PERSON]:0.08, · · · [CT:ARTIFACT]:0.31, · · · [CT:VEHICLE]:0.32, · · · [CT:PERSON]:0.40, [NE:PERSON]:0.12, · · · [CT:ABSTRACT]:0.47, · · · ... ga (subjective) wo (objective) ni (dative) de (locative) ... hanbai (1) (sell) Table 1: Examples of Constructed Case Frames. ... ... case slot ga (subjective) tsumu (1) wo (objective) (load) ni (dative) tsumu (2) ga (subjective) (accumulate) wo (object"
C08-1097,P05-1020,0,0.0399419,"Missing"
C08-1097,I08-2080,1,0.832631,"JUMAN1 adds to common nouns. In JUMAN, about twenty categories are deﬁned and tagged to common nouns. For example, “ringo (apple),” “inu (dog)” and “byoin (hospital)” are tagged as “FOOD,” “ANIMAL” and “FACILITY,” respectively. For each category, we calculate the rate of categorized example among all case slot examples, and add it to the case slot as “[CT:FOOD]:0.07.” We also generalize NEs. We use a common standard NE deﬁnition for Japanese provided by IREX workshop (1999). IREX deﬁned eight NE classes as shown in Table 2. We ﬁrst recognize NEs in the source corpus by using an NE recognizer (Sasano and Kurohashi, 2008), and then construct case frames from the NE-recognized corpus. 770 1 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html Table 2: Deﬁnition of NE in IREX. NE class ORGANIZATION PERSON LOCATION ARTIFACT DATE TIME MONEY PERCENT Morphological analysis, NE recognition, syntactic analysis and coreference resolution are conducted as pre-processes for zero anaphora resolution. Therefore, the model has already recognized existing entities before zero anaphora resolution. For example, let us consider the following text: Examples NHK Symphony Orchestra Kawasaki Kenjiro Rome, Sinuiju Nobel Prize July 1"
C08-1097,C02-1078,0,0.441458,"n plays an important role in discourse analysis. Zero anaphora resolution can be divided into two phases. The ﬁrst phase is zero pronoun detection and the second phase is zero pronoun resolution. Zero pronoun resolution is similar to corefc 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. * Research Fellow of the Japan Society for the Promotion of Science (JSPS) Zero pronouns are not expressed in a text and have to be detected prior to identifying their antecedents. Seki et al. (2002) proposed a probabilistic model for zero pronoun detection and resolution that uses hand-crafted case frames. In order to alleviate the sparseness of hand-crafted case frames, Kawahara and Kurohashi (2004) introduced wide-coverage case frames to zero pronoun detection that are automatically constructed from a large corpus. They use the case frames as selectional restriction for zero pronoun resolution, but do not utilize the frequency of each example of case slots. However, since the frequency is shown to be a good clue for syntactic and case structure analysis (Kawahara and Kurohashi, 2006),"
C08-1097,J01-4004,0,0.116189,"Missing"
C08-1097,P08-1096,0,0.0171136,"(Coling 2008), pages 769–776 Manchester, August 2008 examples he, driver, friend, · · · baggage, luggage, hay, · · · car, truck, vessel, seat, · · · player, children, party, · · · experience, knowledge, · · · generalized examples with rate company, Microsoft, ﬁrm, · · · goods, product, ticket, · · · customer, company, user, · · · shop, bookstore, site · · · [NE:ORGANIZATION]:0.16, [CT:ORGANIZATION]:0.13, · · · [CT:ARTIFACT]:0.40, [CT:FOOD]:0.07, · · · [CT:PERSON]:0.28, · · · [CT:FACILITY]:0.40, [CT:LOCATION]:0.39, · · · ... ... the coreference model proposed by Luo (2007) and that proposed by Yang et al. (2008). Due to this characteristic, our model can utilize information beyond a mention and easily consider salience (the importance of an entity). 2 [CT:PERSON]:0.45, [NE:PERSON]:0.08, · · · [CT:ARTIFACT]:0.31, · · · [CT:VEHICLE]:0.32, · · · [CT:PERSON]:0.40, [NE:PERSON]:0.12, · · · [CT:ABSTRACT]:0.47, · · · ... ga (subjective) wo (objective) ni (dative) de (locative) ... hanbai (1) (sell) Table 1: Examples of Constructed Case Frames. ... ... case slot ga (subjective) tsumu (1) wo (objective) (load) ni (dative) tsumu (2) ga (subjective) (accumulate) wo (objective) Construction of Case Frames Case fr"
C08-1097,C02-1122,1,\N,Missing
C08-1132,W00-1201,0,0.0796591,"Missing"
C08-1132,W06-2920,0,0.27989,"the left side, and [ pos1 ,..., posn−1 , posn ]r means the pos-tag sequence of the modifiers attached to a head from the right side. We use the 33 pos-tags defined in Penn Chinese Treebank (Xue et al., 2002) to describe a case pattern, and make following modifications: • group common noun, proper noun and pronoun together and mark them as ‘noun’; • group predicative adjective and all the other verbs together and mark them as ‘verb’; • only regard comma, pause, colon and semi-colon as punctuations and mark them as ‘punc’, and neglect other punctuations. 2 UAS means unlabeled attachment score (Buchholz and Marsi, 2006). The sentences used for this evaluation are from Penn Chinese Treebank with gold word segmentation and pos-tag. 1050 • group cardinal number and ordinal number together and mark them as ‘num’; • keep the original definition for other postags but label them by new tags, such as labeling ‘P’ as ’prep’ and labeling ‘AD’ as ‘adv’. The task of case pattern construction is to extract cpi for each head from both the tagged corpus and the raw corpus. As we will introduce later, the Chinese dependency parser using case structures applies CKY algorithm for decoding. Thus the following substrings of cpi"
C08-1132,W06-2927,0,0.0538281,"Missing"
C08-1132,P96-1025,0,0.0610114,"the sentences are parsed by the same Chinese deterministic parser used for Chinese Gigaword analysis. The correct dependency relations created by the P(cmi |wi , cpi ) = ∏ P ( D j |wi , cpi ) j = ∏ P( w j , dis j , comma j |wi , cpi ) j 3 1051 http://chasen.org/~taku/software/TinySVM/ (4) Finally, P(wj,disj,commaj |wi,cpi) is divided as P( w j , dis j , comma j |wi , cpi ) = P( w j |wi , cpi ) × P(dis j , comma j |wi , w j , cpi ) (5) Pˆraw (dis j , comma j |wi , w j , cpi ) ; Maximum likelihood estimation is used to estimate P(wROOT|S) on training data set with the smoothing method used in (Collins, 1996). The estimation of P(cpi|wi), P(wj|wi,cpi), and P(disj, commaj |wi,wj,cpi) will be introduced in the following subsections. 3.2 • Estimate the two probabilities only by the case elements from the raw corpus, and represent and them as Pˆraw ( w j |wi , cpi ) • Estimate P(wj|wi,cpi) and P(disj, commaj |wi,wj,cpi) by equation 8 and equation 9. Pˆ ( w j |wi , cpi ) = λelement × Pˆtagged ( w j |wi , cpi ) + (1 − λelement ) × Pˆraw ( w j |wi , cpi ) Estimating P(cpi|wi) by Case Patterns Three steps are used to estimate P(cpi|wi) by maximum likelihood estimation using the constructed case patterns:"
C08-1132,D07-1097,0,0.0338055,"Missing"
C08-1132,C04-1104,0,0.019314,"Missing"
C08-1132,I08-4010,0,0.0423187,"Missing"
C08-1132,N06-1023,1,0.829634,"Missing"
C08-1132,kawahara-kurohashi-2006-case,1,0.8977,"Missing"
C08-1132,W02-2016,0,0.159082,"Missing"
C08-1132,E06-1011,0,0.328393,"ancing good parse selection on the analysis of raw corpus, and integrating pos-tagging into parsing model. Related Work To our current knowledge, there were few works about using case structures in Chinese parsing, except for the work of Wu (2003) and Han et al. (2004). Compared with them, our proposed approach presents a new type of case structures for all kinds of head-modifier pairs, which not only recognizes bi-lexical dependency but also remembers the parsing history of a head node. Parsing history has been used to improve parsing accuracy by many researchers (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006). Yamada and Matsumoto (2003) showed that keeping a small amount of parsing history was useful to improve parsing performance in a shiftreduce parser. McDonald and Pereira (2006) expanded their first-order spanning tree model to be second-order by factoring the score of the tree into the sum of adjacent edge pair scores. In our proposed approach, the case patterns remember the neighboring modifiers for a head node like McDonald and Pereira’s work. But it keeps all the parsing histories of a head, which is different from only keeping adjacent two modifiers in (McDonald and Pereira, 2006). Besid"
C08-1132,D07-1013,0,0.0594151,"Missing"
C08-1132,P07-1052,0,0.023439,"Missing"
C08-1132,D07-1111,0,0.0767765,"(McDonald and Pereira, 2006). Besides, to use the parsing histories in CKY decoding, our approach applies horizontal Markovization during case pattern construction. In general, the success of using case patterns in Chinese parsing in his paper proves again that keeping parsing history is crucial to improve parsing performance, no matter in which way and to which parsing model it is applied. There were also some works that handled lexical preference for Chinese parsing in other ways. For example, Cheng et al. (2006) and Hall et al. (2007) applied shift-reduce deterministic parsing to Chinese. Sagae and Tsujii (2007) generalized the standard deterministic framework to probabilistic parsing by using a best-first search strategy. In these works, lexical preferences were introduced as features for predicting parsing action. Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank. Wang et al. (2005) proposed a completely lexicalized bottom-up generative parsing model to parse Chinese, in which a word-similarity-based smooth7 Conclusion and Future Work References T.Abekawa and M.Okumura. 2006. Japanese Dependency Parsing Using Co-occurrence Informa"
C08-1132,W05-1516,0,0.0749552,"Missing"
C08-1132,W03-1717,0,0.233813,"uracy significantly. Besides, although we only apply the proposed approach to Chinese dependency parsing currently, the same idea could be adapted to other languages easily because it doesn’t use any language specific knowledge. There are several future works under consideration, such as modifying the representation of case patterns to make it more robust, enhancing good parse selection on the analysis of raw corpus, and integrating pos-tagging into parsing model. Related Work To our current knowledge, there were few works about using case structures in Chinese parsing, except for the work of Wu (2003) and Han et al. (2004). Compared with them, our proposed approach presents a new type of case structures for all kinds of head-modifier pairs, which not only recognizes bi-lexical dependency but also remembers the parsing history of a head node. Parsing history has been used to improve parsing accuracy by many researchers (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006). Yamada and Matsumoto (2003) showed that keeping a small amount of parsing history was useful to improve parsing performance in a shiftreduce parser. McDonald and Pereira (2006) expanded their first-order spanning tree"
C08-1132,C02-1145,0,0.0791919,"Missing"
C08-1132,W03-1707,0,0.0440487,"Missing"
C08-1132,W03-3023,0,0.242349,"to make it more robust, enhancing good parse selection on the analysis of raw corpus, and integrating pos-tagging into parsing model. Related Work To our current knowledge, there were few works about using case structures in Chinese parsing, except for the work of Wu (2003) and Han et al. (2004). Compared with them, our proposed approach presents a new type of case structures for all kinds of head-modifier pairs, which not only recognizes bi-lexical dependency but also remembers the parsing history of a head node. Parsing history has been used to improve parsing accuracy by many researchers (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006). Yamada and Matsumoto (2003) showed that keeping a small amount of parsing history was useful to improve parsing performance in a shiftreduce parser. McDonald and Pereira (2006) expanded their first-order spanning tree model to be second-order by factoring the score of the tree into the sum of adjacent edge pair scores. In our proposed approach, the case patterns remember the neighboring modifiers for a head node like McDonald and Pereira’s work. But it keeps all the parsing histories of a head, which is different from only keeping adjacent two modifiers in (McDon"
C08-1132,W06-1604,0,0.0404282,"Missing"
C08-1132,W04-1116,0,0.0819482,"Missing"
C08-1132,N07-2051,1,0.888322,"Missing"
C08-1132,P07-2055,0,0.060802,"Missing"
C08-1132,J04-4004,0,\N,Missing
C08-1132,J03-4003,0,\N,Missing
C08-1132,P06-1105,0,\N,Missing
C10-2061,P08-1118,0,0.145329,"Missing"
C10-2061,C08-1031,0,0.0321247,"Missing"
C10-2061,N09-2029,0,0.0304626,"zing contradictions. For example, Harabagiu et al. (2006) used negative expressions, antonyms and contrast discourse relations to recognize contradictions. These methods only detect relations between given sentences, and do not create a bird’s-eye view. To create a kind of bird’s-eye view, Kawahara et al. (2008), Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010) identified various relations between statements including contradictory relations, but do not handle contrastive relations, which are one of the important relations for taking a bird’s-eye view on a topic. Lerman and McDonald (2009) proposed a method for generating contrastive summaries about given two entities on the basis of KL-divergence. This study is related to ours in the aspect of extracting implicit contrasts, but contrastive summaries are different from contrastive relations between statements in our study. 3 Our Method We propose a method for grasping overall information on the Web on a given query (topic). This method extracts and presents statements that are relevant to a given topic, including direct contrastive statements and contradictory/contrastive relations between these statements. As a unit for statem"
C10-2061,P98-2127,0,0.0187212,"lowing patterns. They are extracted from the statement candidates. (patent system of America is different from φ of Japan · · ·) In this sentence, “nihon” (Japan) has a meaning of “nihon-no tokkyo seido” (patent system of Japan). That is to say, “tokkyo seido” (patent system), which is the attribute of comparison, is omitted. In this study, in addition to patterns of contrastive constructs, we use checking and filtering on the basis of similarity. The use of similarity is inspired by the semantic parallelism between contrasted keywords. As this similarity, we employ distributional similarity (Lin, 1998), which is calculated using automatic dependency parses of 100 million Japanese Web pages. By searching similar keywords from the above sentence, we successfully extract a contrastive keyword pair, “amerika” (America) and “nihon” (Japan), and the above sentence as a direct contrastive statement. Similarly, a target of comparison can be omitted as in the following sentence. (4) nedan-wa gosei senzai-yori takaidesu price-TOP synthetic detergent-ABL high (price of φ is higher than synthetic detergent) 7-Eleven-wa hokano konbini-to 7-Eleven-TOP other convenience store-ABL • X-wa Y-to {chigau |koto"
C10-2061,D08-1002,0,0.193277,"olarity. To aggregate statements and detect relations between them, one of important modules is recognition of synonymous, entailed, contradictory and contrastive statements. Studies on rhetorical structure theory (Mann and Thompson, 1988) and recognizing textual entailment (RTE) deal with these relations. In particular, evaluative workshops on RTE have been held and this kind of research has been actively studied (Bentivogli et al., 2009). The recent workshops of this series set up a task that recognizes contradictions. Harabagiu et al. (2006), de Marneffe et al. (2008), Voorhees (2008), and Ritter et al. (2008) focused on recognizing contradictions. For example, Harabagiu et al. (2006) used negative expressions, antonyms and contrast discourse relations to recognize contradictions. These methods only detect relations between given sentences, and do not create a bird’s-eye view. To create a kind of bird’s-eye view, Kawahara et al. (2008), Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010) identified various relations between statements including contradictory relations, but do not handle contrastive relations, which are one of the important relations for taking a bird’s-ey"
C10-2061,I08-2110,1,0.909525,"ly learned probabilities of polarity. To aggregate statements and detect relations between them, one of important modules is recognition of synonymous, entailed, contradictory and contrastive statements. Studies on rhetorical structure theory (Mann and Thompson, 1988) and recognizing textual entailment (RTE) deal with these relations. In particular, evaluative workshops on RTE have been held and this kind of research has been actively studied (Bentivogli et al., 2009). The recent workshops of this series set up a task that recognizes contradictions. Harabagiu et al. (2006), de Marneffe et al. (2008), Voorhees (2008), and Ritter et al. (2008) focused on recognizing contradictions. For example, Harabagiu et al. (2006) used negative expressions, antonyms and contrast discourse relations to recognize contradictions. These methods only detect relations between given sentences, and do not create a bird’s-eye view. To create a kind of bird’s-eye view, Kawahara et al. (2008), Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010) identified various relations between statements including contradictory relations, but do not handle contrastive relations, which are one of the"
C10-2061,I08-1025,1,0.833222,"s. Then, we explain our method of extracting direct contrastive statements with contrastive keyword pairs, and identifying contradictory and contrastive relations in detail. 536 3.1 Extraction and Aggregation of Predicate-argument Structures (1) A predicate-argument structure consists of a predicate and one or more arguments that have a dependency relation to the predicate. We extract predicate-argument structures from automatic parses of Web pages on a given topic by using the method of Kawahara et al. (2008). We apply the following procedure to Web pages that are retrieved from the TSUBAKI (Shinzato et al., 2008) open search engine infrastructure, by inputting the topic as a query. 1. Extract important sentences from each Web page. Important sentences are defined as sentences neighboring the topic word(s). 2. Obtain results of morphological analysis (JUMAN1 ) and dependency parsing (KNP2 ) of the important sentences, and extract predicate-argument structures from them. 3. Filter out functional and meaningless predicate-argument structures, which are not relevant to the topic. Pointwise mutual information between the entire Web and the target Web pages for a topic is used. Note that the analyses in ste"
C10-2061,P09-1026,0,0.258539,"omatically identify such pairs. Ganapathibhotla and Liu (2008) proposed a method for detecting which entities (“target” and “basis”) in a direct contrastive statement are preferred by its author. There is also related work that focuses on noncontrastive sentences. Ohshima et al. (2006) extracted coordinated terms, which are semantically broader than our contrastive keyword pairs, using hit counts from a search engine. They made use of syntactic parallelism among coordinated terms. Their task was to input one of coordinated terms as a query, which is different from ours. Somasundaran and Wiebe (2009) presented a method for recognizing a stance in online debates. They formulated this task as debate-side classification and solved it by using automatically learned probabilities of polarity. To aggregate statements and detect relations between them, one of important modules is recognition of synonymous, entailed, contradictory and contrastive statements. Studies on rhetorical structure theory (Mann and Thompson, 1988) and recognizing textual entailment (RTE) deal with these relations. In particular, evaluative workshops on RTE have been held and this kind of research has been actively studied"
C10-2061,P08-1008,0,0.150366,"ed probabilities of polarity. To aggregate statements and detect relations between them, one of important modules is recognition of synonymous, entailed, contradictory and contrastive statements. Studies on rhetorical structure theory (Mann and Thompson, 1988) and recognizing textual entailment (RTE) deal with these relations. In particular, evaluative workshops on RTE have been held and this kind of research has been actively studied (Bentivogli et al., 2009). The recent workshops of this series set up a task that recognizes contradictions. Harabagiu et al. (2006), de Marneffe et al. (2008), Voorhees (2008), and Ritter et al. (2008) focused on recognizing contradictions. For example, Harabagiu et al. (2006) used negative expressions, antonyms and contrast discourse relations to recognize contradictions. These methods only detect relations between given sentences, and do not create a bird’s-eye view. To create a kind of bird’s-eye view, Kawahara et al. (2008), Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010) identified various relations between statements including contradictory relations, but do not handle contrastive relations, which are one of the important relati"
C10-2061,P09-2039,0,0.0162946,"issue. Contrastive relations are the relations between statements in which two entities or issues are contrasted. In particular, we have the following two novel contributions. • We identify contrastive relations between statements, which consist of in-document and cross-document implicit relations. These relations complement direct contrastive statements, which are explicitly mentioned in a single sentence. • We precisely extract direct contrastive statements and contrastive keyword pairs in an unsupervised manner, whereas most previous studies used supervised methods (Jindal and Liu, 2006b; Yang and Ko, 2009). Our system focuses on the Japanese language. For example, Figure 1 shows examples of extracted statements on the topic “gosei senzai” (synthetic detergent). Rounded rectangles represent statements relevant to this topic. The first statement is a direct contrastive statement, which refers to a contrastive keyword pair, “gosei senzai” (synthetic detergent) and “sekken” (soap). The pairs of statements connected with a broad arrow have contradictory relations. The pairs of statements connected with a thin arrow have contrastive relations. Users not only can see what is written on this topic at a"
C10-2061,W07-1401,0,\N,Missing
C10-2061,C98-2122,0,\N,Missing
C10-2061,P09-4001,1,\N,Missing
C14-1027,bethard-etal-2008-building,0,0.0271323,"the average number of clauses in a document is 3.9. The total number of clause pairs is 59,426. 3.1.2 Discourse Relation Tagset One of our supposed applications of discourse parsing is to automatically generate a bird’s eye view of a controversial topic as in Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010), which identify various relations between statements, including contradictory relations. We assume that expansion relations, such as elaboration and restatement, and temporal relations are not important for this purpose. This setting is similar to the work of Bethard et al. (2008), which annotated temporal relations independently of causal relations. We also suppose that temporal relations can be annotated separately for NLP applications that require temporal information. We determined the tagset of discourse relations 271 Upper type CONTINGENCY COMPARISON OTHER Lower type Example Cause/Reason 【ボタンを押したので】【お湯が出た。】 [since (I) pushed the button] [hot water was turned on] Purpose 【試験に受かるために】【必死に勉強した。】 [to pass the exam] [(I) studied a lot] Condition 【ボタンを押せば】【お湯が出る。】 [if (you) push the button] [hot water will be turned on] Ground 【ここにカバンがあるから】【まだ社内にいるだろう。】 [here is his/her"
C14-1027,P13-2013,0,0.0147805,"1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especially in a short period if possible. 3 Development of Corpus with Discourse Annotations using Crowdsourcing 3.1 Corpus Specifications We develop a tagged corpus in which pairs of discourse units are ann"
C14-1027,W01-1605,0,0.230039,"the precise relations between these text fragments. This kind of analysis is called discourse parsing or discourse structure analysis, and is an important and fundamental task in natural language processing (NLP). Systems for discourse parsing are, however, available only for major languages, such as English, owing to the lack of corpora with discourse annotations. For English, several corpora with discourse annotations have been developed manually, consuming a great deal of time and cost in the process. These include the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). Discourse parsers trained on these corpora have also been developed and practically used. To create the same resource-rich environment for another language, a quicker method than the conventional time-consuming framework should be sought. One possible approach is to use crowdsourcing, which has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008; Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource the difficult judgments for discourse annotati"
C14-1027,W11-0401,0,0.0183636,"otation for English, such as the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). These corpora were developed from English newspaper articles. Several attempts have been made to manually create corpora with discourse annotations for languages other than English. These include the Potsdam Commentary Corpus (Stede, 2004) for German (newspaper; 2,900 sentences), Rhetalho (Pardo et al., 2004) for Portuguese (scientific papers; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse p"
C14-1027,P12-1007,0,0.0123901,"l., 2004) for Portuguese (scientific papers; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especially in a short period if possible. 3 Development of Corpus with Discourse Annotations using Crowdsourcing 3.1 Corpus Specifications We de"
C14-1027,P13-2130,0,0.0809301,"These include the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). Discourse parsers trained on these corpora have also been developed and practically used. To create the same resource-rich environment for another language, a quicker method than the conventional time-consuming framework should be sought. One possible approach is to use crowdsourcing, which has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008; Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource the difficult judgments for discourse annotations, which typically consists of two steps: finding a pair of spans with a certain relation and identifying the relation between the pair. In this paper, we propose a method for crowdsourcing discourse annotations that simplifies the procedure by dividing it into two steps. The point is that by simplifying the annotation task it is suitable for crowdsourcing, but does not skew the annotations for use in practical discourse parsing. First, finding a discourse unit for the span is a costly process, and thus"
C14-1027,Y12-1058,1,0.853354,"ing discourse corpora, the target documents were mainly newspaper texts, such as the Wall Street Journal for English. However, discourse parsers trained on such newspaper corpora usually have a problem of domain adaptation. That is to say, while discourse parsers trained on newspaper corpora are good at analyzing newspaper texts, they generally cannot perform well on texts of other domains. To address this problem, we set out to create an annotated corpus covering a variety of domains. Since the web contains many documents across a variety of domains, we use the Diverse Document Leads Corpus (Hangyo et al., 2012), which was extracted from the web. Each document in this corpus consists of the first three sentences of a Japanese web page, making these short documents suitable for our discourse annotation method based on crowdsourcing. We adopt the clause as a discourse unit, since spans are too fine-grained to annotate using crowdsourcing and sentences are too coarse-grained to capture discourse relations. Clauses, which are automatically identified, do not need to be manually modified since they are thought to be reliable enough. Clause identification is performed using the rules of Shibata and Kurohas"
C14-1027,W11-0404,0,0.0723435,"d cost in the process. These include the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). Discourse parsers trained on these corpora have also been developed and practically used. To create the same resource-rich environment for another language, a quicker method than the conventional time-consuming framework should be sought. One possible approach is to use crowdsourcing, which has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008; Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource the difficult judgments for discourse annotations, which typically consists of two steps: finding a pair of spans with a certain relation and identifying the relation between the pair. In this paper, we propose a method for crowdsourcing discourse annotations that simplifies the procedure by dividing it into two steps. The point is that by simplifying the annotation task it is suitable for crowdsourcing, but does not skew the annotations for use in practical discourse parsing. First, finding a discourse unit for the span is a c"
C14-1027,D12-1083,0,0.0149086,"se (scientific papers; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especially in a short period if possible. 3 Development of Corpus with Discourse Annotations using Crowdsourcing 3.1 Corpus Specifications We develop a tagged corp"
C14-1027,P13-1048,0,0.0168993,"rs; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especially in a short period if possible. 3 Development of Corpus with Discourse Annotations using Crowdsourcing 3.1 Corpus Specifications We develop a tagged corpus in which pairs o"
C14-1027,W14-0705,0,0.097662,"Missing"
C14-1027,P13-1047,0,0.0145803,"RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especially in a short period if possible. 3 Development of Corpus with Discourse Annotations using Crowdsourcing 3.1 Corpus Specifications We develop a tagged corpus in which pairs of discourse units are annotated with discour"
C14-1027,D11-1062,0,0.101904,"reat deal of time and cost in the process. These include the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). Discourse parsers trained on these corpora have also been developed and practically used. To create the same resource-rich environment for another language, a quicker method than the conventional time-consuming framework should be sought. One possible approach is to use crowdsourcing, which has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008; Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource the difficult judgments for discourse annotations, which typically consists of two steps: finding a pair of spans with a certain relation and identifying the relation between the pair. In this paper, we propose a method for crowdsourcing discourse annotations that simplifies the procedure by dividing it into two steps. The point is that by simplifying the annotation task it is suitable for crowdsourcing, but does not skew the annotations for use in practical discourse parsing. First, finding a discourse un"
C14-1027,P09-2004,0,0.0313816,"e include the Potsdam Commentary Corpus (Stede, 2004) for German (newspaper; 2,900 sentences), Rhetalho (Pardo et al., 2004) for Portuguese (scientific papers; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especially in a short period if p"
C14-1027,P09-1077,0,0.0140033,"er than English. These include the Potsdam Commentary Corpus (Stede, 2004) for German (newspaper; 2,900 sentences), Rhetalho (Pardo et al., 2004) for Portuguese (scientific papers; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especia"
C14-1027,prasad-etal-2008-penn,0,0.712547,"to understand text, it is essential to capture the precise relations between these text fragments. This kind of analysis is called discourse parsing or discourse structure analysis, and is an important and fundamental task in natural language processing (NLP). Systems for discourse parsing are, however, available only for major languages, such as English, owing to the lack of corpora with discourse annotations. For English, several corpora with discourse annotations have been developed manually, consuming a great deal of time and cost in the process. These include the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). Discourse parsers trained on these corpora have also been developed and practically used. To create the same resource-rich environment for another language, a quicker method than the conventional time-consuming framework should be sought. One possible approach is to use crowdsourcing, which has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008; Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource"
C14-1027,I05-1066,1,0.82951,"Missing"
C14-1027,D08-1027,0,0.22439,"Missing"
C14-1027,W04-0213,0,0.0438613,"urse annotations, which are our target. To the best of our knowledge, there have been no attempts to crowdsource discourse annotations. There are several manually-crafted corpora with discourse annotation for English, such as the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). These corpora were developed from English newspaper articles. Several attempts have been made to manually create corpora with discourse annotations for languages other than English. These include the Potsdam Commentary Corpus (Stede, 2004) for German (newspaper; 2,900 sentences), Rhetalho (Pardo et al., 2004) for Portuguese (scientific papers; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009;"
C14-1027,N09-1064,0,0.0419951,"Missing"
C14-1027,J05-2005,0,0.430652,"nts. This kind of analysis is called discourse parsing or discourse structure analysis, and is an important and fundamental task in natural language processing (NLP). Systems for discourse parsing are, however, available only for major languages, such as English, owing to the lack of corpora with discourse annotations. For English, several corpora with discourse annotations have been developed manually, consuming a great deal of time and cost in the process. These include the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). Discourse parsers trained on these corpora have also been developed and practically used. To create the same resource-rich environment for another language, a quicker method than the conventional time-consuming framework should be sought. One possible approach is to use crowdsourcing, which has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008; Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource the difficult judgments for discourse annotations, which typically consists of two steps: findi"
C14-1027,C10-1140,0,0.067365,"Missing"
C14-1027,I11-1038,0,0.0554212,"Missing"
C16-1029,W09-2307,0,0.0222709,"rvation, these tokens have the ability to determine the syntactic role of the entire compound. For example, any compound that end with a nominal suffix “度” (degree) always act as nouns in a sentence. It should be noted that because of this characteristic of suffixes, we can tag the children of suffixes in compounds based on their meaning but not their syntactic roles. We show some examples in Table 2 to illustrate our POS tagging strategy for compounds. In Table 4 we present a dependency label set developed based on the Stanford Dependencies (De Marneffe et al., 2006) and its Chinese version (Chang et al., 2009), which defines 45 dependency relations for Chinese sentences. This label set is also closely related to the Universal Dependency1 with many of their labels compatible with each other. We explain the major characteristics of our label set in the following subsection. 3.1 Chinese Specific Labels dislocated The label “dislocated” is originally defined in the universal dependencies for languages such as Japanese to describe the syntactic relation of words in a topic–comment structure, but is not defined for Chinese. However, in Chinese it is frequent to see the topic–comment structure in a senten"
C16-1029,W02-1001,0,0.112792,"lower percentage of unknown words and unknown word-POS pairs found in the corresponding test set. This is consistent with our observation that compounds with internal structures are one of the major sources of OOV words. 4.2 Morphological Analysis Experiments We compared the performance of a state-of-the-art joint word segmentation and part-of-speech tagging system (Kruengkrai et al., 2009) on the original and our re-annotated CTB5. We used the position-ofcharacter (POC) tagset and the baseline feature set described in (Shen et al., 2014). We trained all models using the averaged perceptron (Collins, 2002), which is an efficient and stable online learning algorithm. The models applied on all test sets are those that result in the best performance on the dev sets. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 2 times. We use precision, recall and the F-score to measure the performance of the systems. Precision (P) is defined as the percentage of output tokens that are consistent with the gold standard test data, and recall (R) is the percentage of tokens in the gold standard test data that are recognize"
C16-1029,N06-1023,1,0.566142,"phrase based statistical machine translation toolkit Moses (Koehn et al., 2007) with default options. We trained the 5-gram language models on the target side of the parallel corpora using the SRILM toolkit4 with interpolated Kneser-Ney discounting. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. In the second set of experiments, we used the same morphological analyzers to segment and tag the POS of Japanese and Chinese sentences as in the first set. We further parsed the dependency structures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default options. We report results on the test set using BLEU-4 score, which was evaluated using the multi-bleu.perl script in Moses based on Juman segmentations. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). In Table 7 we compare the performance of three Moses model"
C16-1029,W03-1722,0,0.036952,"is paper, we propose a new annotation approach to Chinese word segmentation, part-ofspeech (POS) tagging and dependency labelling that aims to overcome the two major issues in traditional morphology-based annotation: Inconsistency and data sparsity. We re-annotate the Penn Chinese Treebank 5.0 (CTB5) and demonstrate the advantages of this approach compared to the original CTB5 annotation through word segmentation, POS tagging and machine translation experiments. 1 Introduction The definition of “word” is an open problem in Chinese linguistics. In previous studies of Chinese corpus annotation (Duan et al., 2003; Huang et al., 1997; Xia, 2000), the judgement of word-hood of a meaningful string is based on the analysis of morphology: A morpheme in Chinese is defined as the smallest combination of meaning and phonetic sound in Chinese language, which can be classified into two major types: 1). Free morphemes, which can either be words by themselves or form words with other morphemes; and 2). Bound morphemes, which can only form words by attaching to other morphemes. An issue with word definition using morpheme classification is that, it potentially undermines the consistency of the representation of wo"
C16-1029,O97-4003,0,0.219847,"e a new annotation approach to Chinese word segmentation, part-ofspeech (POS) tagging and dependency labelling that aims to overcome the two major issues in traditional morphology-based annotation: Inconsistency and data sparsity. We re-annotate the Penn Chinese Treebank 5.0 (CTB5) and demonstrate the advantages of this approach compared to the original CTB5 annotation through word segmentation, POS tagging and machine translation experiments. 1 Introduction The definition of “word” is an open problem in Chinese linguistics. In previous studies of Chinese corpus annotation (Duan et al., 2003; Huang et al., 1997; Xia, 2000), the judgement of word-hood of a meaningful string is based on the analysis of morphology: A morpheme in Chinese is defined as the smallest combination of meaning and phonetic sound in Chinese language, which can be classified into two major types: 1). Free morphemes, which can either be words by themselves or form words with other morphemes; and 2). Bound morphemes, which can only form words by attaching to other morphemes. An issue with word definition using morpheme classification is that, it potentially undermines the consistency of the representation of words. For example, “论"
C16-1029,P08-1102,0,0.0702105,"Missing"
C16-1029,C08-1049,0,0.0408866,"Missing"
C16-1029,W04-3250,0,0.12997,"tures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default options. We report results on the test set using BLEU-4 score, which was evaluated using the multi-bleu.perl script in Moses based on Juman segmentations. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). In Table 7 we compare the performance of three Moses models: In “Character” we used a simple segmentation strategy for the Chinese sentences where we treated each character as a token; in “Original” and “Re-annotated” we segmented the Chinese sentences using the corresponding models described in 2 ? < 0.05 in McNemar’s test. http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 4 http://www.speech.sri.com/projects/srilm 5 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KyotoEBMT 3 306 the last subsection. The results show, with the underlying machine translation system being the same, the segmenter trained wit"
C16-1029,P09-1058,0,0.0604459,"Missing"
C16-1029,de-marneffe-etal-2006-generating,0,0.0183273,"Missing"
C16-1029,P03-1021,0,0.012317,"irs for tuning and testing, respectively. In the first set of experiments, we segmented the Japanese sentences using JUMAN (Kurohashi et al., 1994), and the Chinese sentences using the same morphological analyzer described in the last subsection. For decoding, we used the state-of-the-art phrase based statistical machine translation toolkit Moses (Koehn et al., 2007) with default options. We trained the 5-gram language models on the target side of the parallel corpora using the SRILM toolkit4 with interpolated Kneser-Ney discounting. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. In the second set of experiments, we used the same morphological analyzers to segment and tag the POS of Japanese and Chinese sentences as in the first set. We further parsed the dependency structures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default opti"
C16-1029,W15-5006,1,0.786007,"mum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. In the second set of experiments, we used the same morphological analyzers to segment and tag the POS of Japanese and Chinese sentences as in the first set. We further parsed the dependency structures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default options. We report results on the test set using BLEU-4 score, which was evaluated using the multi-bleu.perl script in Moses based on Juman segmentations. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). In Table 7 we compare the performance of three Moses models: In “Character” we used a simple segmentation strategy for the Chinese sentences where we treated each character as a token; in “Original” and “Re-annotated” we segmented the Chinese sentences using the corresponding models described in 2 ? < 0.05 in McNemar’s test. http"
C16-1029,Y12-1033,1,0.855948,"he parallel corpora using the SRILM toolkit4 with interpolated Kneser-Ney discounting. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. In the second set of experiments, we used the same morphological analyzers to segment and tag the POS of Japanese and Chinese sentences as in the first set. We further parsed the dependency structures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default options. We report results on the test set using BLEU-4 score, which was evaluated using the multi-bleu.perl script in Moses based on Juman segmentations. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). In Table 7 we compare the performance of three Moses models: In “Character” we used a simple segmentation strategy for the Chinese sentences where we treated each character as a token; in “Original” and “Re-ann"
C16-1029,P14-2042,1,0.938798,"621 466 451 258 245 33 12 6 0 0 0 Proposed annotation 137,816 75,935 75,033 35,922 30,985 17,721 21,493 18,091 13,898 12,346 12,145 4,524 0 7,518 7,134 6,646 5,970 5,521 4,033 2,980 1,661 1,316 0 888 756 627 466 451 258 245 391 17 6 13,212 438 129 Table 3. Proposed tagset for part-of-speech tagging. The underlined characters in the examples correspond to the tags on the left-most column. The CTB POS are also shown. The key in our method to define the boundaries of common words is the character-level POS pattern. Character-level POS has been introduced in previous studies (Zhang et al., 2013; Shen et al., 2014) which captures the grammatical roles of Chinese characters inside words; we further develop this idea and use it as a criterion in word definition. We treat a meaningful disyllabic strings as a word if it falls into one of the character-level POS patterns listed in Table 1. The reason we focus on disyllabic patterns instead of other polysyllabic ones is that, based on our observation, meaningful strings with 3 or more syllables (other than names and idioms) are always compounds in Chinese, and therefore can be segmented into a sequence of monosyllabic and disyllabic tokens based on their inte"
C16-1029,P11-1139,0,0.0380763,"Missing"
C16-1029,P13-1013,0,0.0165029,"1,316 1,287 888 751 621 466 451 258 245 33 12 6 0 0 0 Proposed annotation 137,816 75,935 75,033 35,922 30,985 17,721 21,493 18,091 13,898 12,346 12,145 4,524 0 7,518 7,134 6,646 5,970 5,521 4,033 2,980 1,661 1,316 0 888 756 627 466 451 258 245 391 17 6 13,212 438 129 Table 3. Proposed tagset for part-of-speech tagging. The underlined characters in the examples correspond to the tags on the left-most column. The CTB POS are also shown. The key in our method to define the boundaries of common words is the character-level POS pattern. Character-level POS has been introduced in previous studies (Zhang et al., 2013; Shen et al., 2014) which captures the grammatical roles of Chinese characters inside words; we further develop this idea and use it as a criterion in word definition. We treat a meaningful disyllabic strings as a word if it falls into one of the character-level POS patterns listed in Table 1. The reason we focus on disyllabic patterns instead of other polysyllabic ones is that, based on our observation, meaningful strings with 3 or more syllables (other than names and idioms) are always compounds in Chinese, and therefore can be segmented into a sequence of monosyllabic and disyllabic tokens"
C16-1029,D10-1082,0,0.0356933,"Missing"
C18-1128,P13-1133,0,0.0849537,"Missing"
C18-1128,S17-2002,0,0.0442787,"Missing"
C18-1128,C16-1276,0,0.10794,"e source language has a similar vector representation to its target-side counterpart, assuming each concept in the source language corresponds to exactly one concept in the target language. There is a rich body of work on sense embedding, which allows one surface form of a word to have sense-specific vectors (Neelakantan et al., 2014; Iacobacci et al., 2015). However, to the best of our knowledge, previous studies in this field do not target sense vectors of concepts for cross-lingual knowledge projection. Several studies proposed methods for one-to-one projection of facts (Kuo and Hsu, 2010; Feng et al., 2016). The work by Feng et al. (2016) is the most related to our study. Their model learns mappings between English and Chinese facts by manually annotated alignments. Their experimental result showed the model successfully resolved the projection ambiguity. Their experiment was, however, limited to a narrow domain due to the cost of manual annotations, indicating the difficulty of obtaining sufficient resources for learning a model. Various types of commonsense is vital to understanding languages in a wide range of tasks such as recognizing textual entailment (LoBue and Yates, 2011). Researchers h"
C18-1128,P15-1010,0,0.0242526,"released code. knowledge projection. Klein et al. (2017) and Chen et al. (2017) represented concepts in multiple languages in a unified vector space, and built knowledge base completion models based on vector representations. Their methods ensure a concept in the source language has a similar vector representation to its target-side counterpart, assuming each concept in the source language corresponds to exactly one concept in the target language. There is a rich body of work on sense embedding, which allows one surface form of a word to have sense-specific vectors (Neelakantan et al., 2014; Iacobacci et al., 2015). However, to the best of our knowledge, previous studies in this field do not target sense vectors of concepts for cross-lingual knowledge projection. Several studies proposed methods for one-to-one projection of facts (Kuo and Hsu, 2010; Feng et al., 2016). The work by Feng et al. (2016) is the most related to our study. Their model learns mappings between English and Chinese facts by manually annotated alignments. Their experimental result showed the model successfully resolved the projection ambiguity. Their experiment was, however, limited to a narrow domain due to the cost of manual anno"
C18-1128,E17-2083,0,0.0613434,"Missing"
C18-1128,P16-1137,0,0.0164472,"of a Japanese fact (koumori, CapableOf, tobu) given an English fact (bat, CapableOf, fly). We first obtain language expressions of the facts using hand-crafted templates and compute a translation probability with the translation model. 4.2 Knowledge Base Completion (KBC) KBC models evaluate the plausibility of a given fact based on existing information on the KB. For example, if we already know many animals with wings can fly and bats have wings, we can imagine that bats also can fly. We train a KBC model on the target-side KB. We use a bilinear model used in several previous studies (e.g., (Li et al., 2016)) as a component of our model, where concepts and relations are represented as vectors and matrices, respectively. This component can also be replaced with other KBC models.4 Given a fact f t = (e1 , r, e2 ), the bilinear model outputs the value of plausibility as follows. xKBC (f t ) = σ(uT1 Mr u2 ), (3) where σ is a sigmoid function, ui ∈ Rd (i = 1, 2) corresponds to vectors of concepts e1 and e2 , Mr ∈ Rd×d corresponds to a matrix of relation r, and d is a hyper parameter. We construct concept vectors by averaging pre-trained d0 -dimensional word embeddings as several previous studies did t"
C18-1128,P11-2057,0,0.0211961,"Kuo and Hsu, 2010; Feng et al., 2016). The work by Feng et al. (2016) is the most related to our study. Their model learns mappings between English and Chinese facts by manually annotated alignments. Their experimental result showed the model successfully resolved the projection ambiguity. Their experiment was, however, limited to a narrow domain due to the cost of manual annotations, indicating the difficulty of obtaining sufficient resources for learning a model. Various types of commonsense is vital to understanding languages in a wide range of tasks such as recognizing textual entailment (LoBue and Yates, 2011). Researchers have compiled resources to maintain such knowledge. Cyc (Lenat, 1995) is a seminal big project that aims to organize commonsense in logical forms. Logical forms are suitable for disambiguating the meaning of language, but we need high expertise to acquire or utilize them. In contrast, ConceptNet (Liu and Singh, 2004b; Speer et al., 2017) adopted natural language expressions such as words and phrases that may have ambiguities to represent knowledge, which made it possible to collect millions of commonsense facts in multiple languages via crowdsourcing. 3 Problem Setting Suppose we"
C18-1128,D15-1276,1,0.891939,"Missing"
C18-1128,D14-1113,0,0.0161577,"plates can be found in the released code. knowledge projection. Klein et al. (2017) and Chen et al. (2017) represented concepts in multiple languages in a unified vector space, and built knowledge base completion models based on vector representations. Their methods ensure a concept in the source language has a similar vector representation to its target-side counterpart, assuming each concept in the source language corresponds to exactly one concept in the target language. There is a rich body of work on sense embedding, which allows one surface form of a word to have sense-specific vectors (Neelakantan et al., 2014; Iacobacci et al., 2015). However, to the best of our knowledge, previous studies in this field do not target sense vectors of concepts for cross-lingual knowledge projection. Several studies proposed methods for one-to-one projection of facts (Kuo and Hsu, 2010; Feng et al., 2016). The work by Feng et al. (2016) is the most related to our study. Their model learns mappings between English and Chinese facts by manually annotated alignments. Their experimental result showed the model successfully resolved the projection ambiguity. Their experiment was, however, limited to a narrow domain due t"
C18-1128,P16-1009,0,0.0115522,"Methods We compare the performance of our proposed methods with the following baselines. • PPMI: Positive pointwise mutual information of two concepts consisting of a target-side fact f t . We count the co-occurrence of the concepts in the 200 million web sentences for Japanese, and in the Chinese Gigaword Fifth Edition for Chinese concepts. • MT: The neural MT model with an attention mechanism, which computes xMT in the proposed methods. We used an implementation by Neubig (2015) and train a model on 3.25M (en-ja) and 2.97M (en-zh) sentence pairs from dictionaries and newswire corpora. BPE (Sennrich et al., 2016) was used to reduce the vocabulary size. • KBC: The target-side bilinear KBC model which was used as the component to produce xKBC . The Japanese and Chinese models were trained on 59,274 and 318,361 facts, respectively. • MTransE: The multi-lingual translation-based KBC model (Chen et al., 2017) which learns TransE (Bordes et al., 2013) and concept-to-concept alignment jointly. Chen et al. (2017) proposed five different alignment models and reported the fourth variant performed best in their experiments. Thus we use the variant in our experiments. The proposed methods LIN and MLP use estimate"
C18-1128,speer-havasi-2012-representing,0,0.437381,"e gap across languages by cross-lingual 2 https://github.com/notani/CLKP-MTKBC 1509 Relation e1 e2 English Japanese Chinese AtLocation CapableOf MadeOf UsedFor NP NP NP NP NP VP NP VP You are likely to find e1 in e2 . e1 can e2 e1 is made of e2 . You can use e1 to e2 . e2 e1 e1 e1 Ni keyi zai e2 zhaodao e1 . e1 hui e1 . e1 keyi yong e2 zhi cheng. e2 de shihou keneng hui yong dao e1 . de e1 wo miru koto ga aru. wa e2 koto ga dekiru . wa e2 kara tsukurareru. wa e2 tame ni tsukawareru. Table 1: Examples of templates for converting facts into sentences. Constraints of part-of-speech on e1 and e2 (Speer and Havasi, 2012) are also presented. Some templates were developed by the ConceptNet organizers. The rest of the templates can be found in the released code. knowledge projection. Klein et al. (2017) and Chen et al. (2017) represented concepts in multiple languages in a unified vector space, and built knowledge base completion models based on vector representations. Their methods ensure a concept in the source language has a similar vector representation to its target-side counterpart, assuming each concept in the source language corresponds to exactly one concept in the target language. There is a rich body"
C18-1128,P03-1010,0,0.054756,"Missing"
D07-1032,P05-1022,0,0.0527499,"ltaneously evaluate not only syntactic relations but also indirect relations, such as ellipses and anaphora. This kind of mismatch also occurred for the detection of coordinate structures. Another errors were caused by an inherent characteristic of generative models. Generative models have some advantages, such as their application to language models. However, it is difﬁcult to incorporate various features that seem to be useful for addressing syntactic and coordinate ambiguity. We plan to apply discriminative reranking to the n-best parses produced by our generative model in the same way as (Charniak and Johnson, 2005). 7 Conclusion This paper has described an integrated probabilistic model for coordination disambiguation and syntactic/case structure analysis. This model takes advantage of lexical preference of a huge raw corpus and large-scale case frames and performs coordination disambiguation and syntactic/case analysis simultaneously. The experiments indicated the effectiveness of our model. Our future work involves incorporating ellipsis resolution to develop an integrated model for syntactic, case, and ellipsis analysis. Acknowledgment This research is partially supported by special coordination fund"
D07-1032,P06-1053,0,0.164169,"Missing"
D07-1032,P99-1081,0,0.433146,"ute of Information and Communications Technology, 3-5 Hikaridai Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan dk@nict.go.jp Abstract more than two conjuncts, commas, which have various usages, also function like coordinate conjunctions. Recognizing true coordinate conjunctions from such possible coordinate conjunctions is a task of coordination disambiguation (Kurohashi, 1995). The other is the task of identifying the range of coordinate phrases or clauses. Previous work on coordination disambiguation has focused on the task of addressing the scope ambiguity (e.g., (Agarwal and Boggess, 1992; Goldberg, 1999; Resnik, 1999; Chantree et al., 2005)). Kurohashi and Nagao proposed a similarity-based method to resolve both of the two tasks for Japanese (Kurohashi and Nagao, 1994). Their method, however, heuristically detects coordinate conjunctions by considering only similarities between possible conjuncts, and thus cannot disambiguate the following cases1 : This paper describes a probabilistic model for coordination disambiguation integrated into syntactic and case structure analysis. Our model probabilistically assesses the parallelism of a candidate coordinate structure using syntactic/semantic sim"
D07-1032,kawahara-kurohashi-2006-case,1,0.856964,"divided by the square root of the number of bunsetsus covered by the path for normalization The score of each path is calculated using a dynamic programming method. We consider each path as a candidate of pre- and post-conjuncts. 5 Dependency structure T1 ,T2 Integrated Probabilistic Model for Syntactic, Coordinate and Case Structure Analysis bentou-wa tabete-te This section describes a method of integrating coordination disambiguation into a probabilistic parsing model. The integrated model is based on a fullylexicalized probabilistic model for Japanese syntactic and case structure analysis (Kawahara and Kurohashi, 2006b). 5.1 Outline of the Model T1 : D T2 : C 0 (lunchbox) tabete-te (eat) kaet-ta kaet-ta (go home) (1) T3 : D T4 : C 0 EOS (go home) (3) EOS P(kaet − ta, D |EOS) P(bentou − wa kaet − ta, D |EOS) P(bentou − wa tabe − te, D |kaet − ta) P(tabe − te, D |bentou − wa kaet − ta) (2) (4) P(kaet − ta, D |EOS) P(bentou − wa kaet − ta, D |EOS) P(bentou − wa tabe − te, C 0 |kaet − ta) P(tabe − te, C 0 |bentou − wa kaet − ta) Figure 3: Example of probability calculation. This model gives a probability to each possible dependency structure, T , and case structure, L, of the input sentence, S, and outputs the"
D07-1032,N06-1023,1,0.9302,"divided by the square root of the number of bunsetsus covered by the path for normalization The score of each path is calculated using a dynamic programming method. We consider each path as a candidate of pre- and post-conjuncts. 5 Dependency structure T1 ,T2 Integrated Probabilistic Model for Syntactic, Coordinate and Case Structure Analysis bentou-wa tabete-te This section describes a method of integrating coordination disambiguation into a probabilistic parsing model. The integrated model is based on a fullylexicalized probabilistic model for Japanese syntactic and case structure analysis (Kawahara and Kurohashi, 2006b). 5.1 Outline of the Model T1 : D T2 : C 0 (lunchbox) tabete-te (eat) kaet-ta kaet-ta (go home) (1) T3 : D T4 : C 0 EOS (go home) (3) EOS P(kaet − ta, D |EOS) P(bentou − wa kaet − ta, D |EOS) P(bentou − wa tabe − te, D |kaet − ta) P(tabe − te, D |bentou − wa kaet − ta) (2) (4) P(kaet − ta, D |EOS) P(bentou − wa kaet − ta, D |EOS) P(bentou − wa tabe − te, C 0 |kaet − ta) P(tabe − te, C 0 |bentou − wa kaet − ta) Figure 3: Example of probability calculation. This model gives a probability to each possible dependency structure, T , and case structure, L, of the input sentence, S, and outputs the"
D07-1032,W02-2016,0,0.201989,"Missing"
D07-1032,J94-4001,1,0.553519,"commas, which have various usages, also function like coordinate conjunctions. Recognizing true coordinate conjunctions from such possible coordinate conjunctions is a task of coordination disambiguation (Kurohashi, 1995). The other is the task of identifying the range of coordinate phrases or clauses. Previous work on coordination disambiguation has focused on the task of addressing the scope ambiguity (e.g., (Agarwal and Boggess, 1992; Goldberg, 1999; Resnik, 1999; Chantree et al., 2005)). Kurohashi and Nagao proposed a similarity-based method to resolve both of the two tasks for Japanese (Kurohashi and Nagao, 1994). Their method, however, heuristically detects coordinate conjunctions by considering only similarities between possible conjuncts, and thus cannot disambiguate the following cases1 : This paper describes a probabilistic model for coordination disambiguation integrated into syntactic and case structure analysis. Our model probabilistically assesses the parallelism of a candidate coordinate structure using syntactic/semantic similarities and cooccurrence statistics. We integrate these probabilities into the framework of fully-lexicalized parsing based on largescale case frames. This approach si"
D07-1032,1995.iwpt-1.17,1,0.743726,"Missing"
D07-1032,C04-1002,0,0.200229,"Missing"
D07-1032,J03-4003,0,\N,Missing
D07-1032,P92-1003,0,\N,Missing
D08-1104,W03-1812,0,0.129562,"the occasion of (something).” The former is the literal interpretation and the latter is the idiomatic interpretation of the compound functional expression. 4 993 The SAID dataset5 provides data about the syntactic flexibility of English idioms. It does not concern itself with idiom token identification. However, as in Hashimoto et al. (2006b), Hashimoto et al. (2006a) and Cook et al. (2007) among others, the syntactic behavior of idioms is an important clue to idiom token identification. Previous studies have mostly focused on the idiom type identification (Lin, 1999; Krenn and Evert, 2001; Baldwin et al., 2003; Shudo et al., 2004; Fazly and Stevenson, 2006). However, there has been a growing interest in idiom token identification in recent times (Katz and Giesbrecht, 2006; Hashimoto et al., 2006b; Hashimoto et al., 2006a; Birke and Sarkar, 2006; Cook et al., 2007). Katz and Giesbrecht (2006) compared the word vector of an idiom in context and that of the constituent words of the idiom using LSA in order to determine if the expression is idiomatic. Hashimoto et al. (2006b) and Hashimoto et al. (2006a) (HSU henceforth) focused their attention on the differences in grammatical constraints imposed on i"
D08-1104,E06-1042,0,0.279257,"ional Linguistics ther “punch,” “steal” or “make moves on.” This kind of ambiguity should be placed on the agenda. We do not tackle the problem of what constitutes the notion of “idiom.” We simply regard phrases listed in Sato (2007) as idioms. The reminder of this paper is organized as follows. In §2 we present related works. §3 shows the target idioms. After the idiom corpus is described in §4, we detail our idiom identification method and experiment in §5. Finally §6 concludes the paper. 2 Related Work There have only been a few works on the construction of an idiom corpus. In this regard, Birke and Sarkar (2006) and Cook et al. (2008) are notable exceptions. Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally). They targeted 50 expressions and collected about 6,600 examples. They call the corpus TroFi Example Base, which is available on the Web.2 Cook et al. (2008) compiled a corpus of English verbnoun combinations (VNCs) tokens. Their corpus deals with 53 VNC expressions and consists of about 3,000 example sentences. Like ours, they assigned each example with a label indicating whether an expression in the example is used"
D08-1104,W07-1106,0,0.747906,"resents a verbal negation morpheme. 2 http://www.cs.sfu.ca/∼anoop/students/jbirke/ 3 http://nlp.iit.tsukuba.ac.jp/must/ For example, (something)-ni-atatte ((something)-DATrun.into) means either “run into (something)” or “on the occasion of (something).” The former is the literal interpretation and the latter is the idiomatic interpretation of the compound functional expression. 4 993 The SAID dataset5 provides data about the syntactic flexibility of English idioms. It does not concern itself with idiom token identification. However, as in Hashimoto et al. (2006b), Hashimoto et al. (2006a) and Cook et al. (2007) among others, the syntactic behavior of idioms is an important clue to idiom token identification. Previous studies have mostly focused on the idiom type identification (Lin, 1999; Krenn and Evert, 2001; Baldwin et al., 2003; Shudo et al., 2004; Fazly and Stevenson, 2006). However, there has been a growing interest in idiom token identification in recent times (Katz and Giesbrecht, 2006; Hashimoto et al., 2006b; Hashimoto et al., 2006a; Birke and Sarkar, 2006; Cook et al., 2007). Katz and Giesbrecht (2006) compared the word vector of an idiom in context and that of the constituent words of th"
D08-1104,E06-1043,0,0.0714314,"is the literal interpretation and the latter is the idiomatic interpretation of the compound functional expression. 4 993 The SAID dataset5 provides data about the syntactic flexibility of English idioms. It does not concern itself with idiom token identification. However, as in Hashimoto et al. (2006b), Hashimoto et al. (2006a) and Cook et al. (2007) among others, the syntactic behavior of idioms is an important clue to idiom token identification. Previous studies have mostly focused on the idiom type identification (Lin, 1999; Krenn and Evert, 2001; Baldwin et al., 2003; Shudo et al., 2004; Fazly and Stevenson, 2006). However, there has been a growing interest in idiom token identification in recent times (Katz and Giesbrecht, 2006; Hashimoto et al., 2006b; Hashimoto et al., 2006a; Birke and Sarkar, 2006; Cook et al., 2007). Katz and Giesbrecht (2006) compared the word vector of an idiom in context and that of the constituent words of the idiom using LSA in order to determine if the expression is idiomatic. Hashimoto et al. (2006b) and Hashimoto et al. (2006a) (HSU henceforth) focused their attention on the differences in grammatical constraints imposed on idioms and their literal counterparts such as the"
D08-1104,P07-2035,1,0.887657,"Missing"
D08-1104,P08-2018,1,0.871896,"Missing"
D08-1104,P06-2046,1,0.675395,"d for the Japanese counterparts of from and to. NEG represents a verbal negation morpheme. 2 http://www.cs.sfu.ca/∼anoop/students/jbirke/ 3 http://nlp.iit.tsukuba.ac.jp/must/ For example, (something)-ni-atatte ((something)-DATrun.into) means either “run into (something)” or “on the occasion of (something).” The former is the literal interpretation and the latter is the idiomatic interpretation of the compound functional expression. 4 993 The SAID dataset5 provides data about the syntactic flexibility of English idioms. It does not concern itself with idiom token identification. However, as in Hashimoto et al. (2006b), Hashimoto et al. (2006a) and Cook et al. (2007) among others, the syntactic behavior of idioms is an important clue to idiom token identification. Previous studies have mostly focused on the idiom type identification (Lin, 1999; Krenn and Evert, 2001; Baldwin et al., 2003; Shudo et al., 2004; Fazly and Stevenson, 2006). However, there has been a growing interest in idiom token identification in recent times (Katz and Giesbrecht, 2006; Hashimoto et al., 2006b; Hashimoto et al., 2006a; Birke and Sarkar, 2006; Cook et al., 2007). Katz and Giesbrecht (2006) compared the word vector of an idiom"
D08-1104,isahara-etal-2008-development,0,0.0490112,"Missing"
D08-1104,W06-1203,0,0.555751,"993 The SAID dataset5 provides data about the syntactic flexibility of English idioms. It does not concern itself with idiom token identification. However, as in Hashimoto et al. (2006b), Hashimoto et al. (2006a) and Cook et al. (2007) among others, the syntactic behavior of idioms is an important clue to idiom token identification. Previous studies have mostly focused on the idiom type identification (Lin, 1999; Krenn and Evert, 2001; Baldwin et al., 2003; Shudo et al., 2004; Fazly and Stevenson, 2006). However, there has been a growing interest in idiom token identification in recent times (Katz and Giesbrecht, 2006; Hashimoto et al., 2006b; Hashimoto et al., 2006a; Birke and Sarkar, 2006; Cook et al., 2007). Katz and Giesbrecht (2006) compared the word vector of an idiom in context and that of the constituent words of the idiom using LSA in order to determine if the expression is idiomatic. Hashimoto et al. (2006b) and Hashimoto et al. (2006a) (HSU henceforth) focused their attention on the differences in grammatical constraints imposed on idioms and their literal counterparts such as the possibility of passivization, and developed handcrafted rules for Japanese idiom identification. Although their task"
D08-1104,kawahara-kurohashi-2006-case,1,0.873587,"Missing"
D08-1104,W02-1006,0,0.0929092,"Missing"
D08-1104,P99-1041,0,0.236057,"ther “run into (something)” or “on the occasion of (something).” The former is the literal interpretation and the latter is the idiomatic interpretation of the compound functional expression. 4 993 The SAID dataset5 provides data about the syntactic flexibility of English idioms. It does not concern itself with idiom token identification. However, as in Hashimoto et al. (2006b), Hashimoto et al. (2006a) and Cook et al. (2007) among others, the syntactic behavior of idioms is an important clue to idiom token identification. Previous studies have mostly focused on the idiom type identification (Lin, 1999; Krenn and Evert, 2001; Baldwin et al., 2003; Shudo et al., 2004; Fazly and Stevenson, 2006). However, there has been a growing interest in idiom token identification in recent times (Katz and Giesbrecht, 2006; Hashimoto et al., 2006b; Hashimoto et al., 2006a; Birke and Sarkar, 2006; Cook et al., 2007). Katz and Giesbrecht (2006) compared the word vector of an idiom in context and that of the constituent words of the idiom using LSA in order to determine if the expression is idiomatic. Hashimoto et al. (2006b) and Hashimoto et al. (2006a) (HSU henceforth) focused their attention on the differ"
D08-1104,W04-0405,0,0.0165888,"thing).” The former is the literal interpretation and the latter is the idiomatic interpretation of the compound functional expression. 4 993 The SAID dataset5 provides data about the syntactic flexibility of English idioms. It does not concern itself with idiom token identification. However, as in Hashimoto et al. (2006b), Hashimoto et al. (2006a) and Cook et al. (2007) among others, the syntactic behavior of idioms is an important clue to idiom token identification. Previous studies have mostly focused on the idiom type identification (Lin, 1999; Krenn and Evert, 2001; Baldwin et al., 2003; Shudo et al., 2004; Fazly and Stevenson, 2006). However, there has been a growing interest in idiom token identification in recent times (Katz and Giesbrecht, 2006; Hashimoto et al., 2006b; Hashimoto et al., 2006a; Birke and Sarkar, 2006; Cook et al., 2007). Katz and Giesbrecht (2006) compared the word vector of an idiom in context and that of the constituent words of the idiom using LSA in order to determine if the expression is idiomatic. Hashimoto et al. (2006b) and Hashimoto et al. (2006a) (HSU henceforth) focused their attention on the differences in grammatical constraints imposed on idioms and their lite"
D08-1104,D07-1050,1,0.814697,"ment, we dealt with 90 idioms for which more than 50 examples for both idiomatic and literal usages were available.19 We conducted experiments for each idiom. The performance measure is the accuracy. Accuracy = • tyousyu-no mune-o utu utukusi uta audience-GEN chest-ACC hit beautiful song ‘A beautiful song that impresses the audience’ f6 and f7 are available from JUMAN’s output. For example, the hypernym of tyousyu (audience) is human and its domain is culture/media. Those of uta (song) are abstract-thing and culture/recreation. They are not used in LN, but they are known to be useful for WSD (Tanaka et al., 2007; Magnini et al., 2002). f8 indicates whether a nominal constituent of an idiom, if any, undergoes adnominal modification. f9 indicates whether one of Japanese topic case markers is attached to a nominal constituent of an idiom, if any. f10 is turned on when a passive or causative suffix is attached to a verbal constituent of an idiom, if any.17 f11 and f12 are similar to f10. The former is used for negated forms and the latter for volitional modality suffixes of a predicate part of an idiom, if any.18 Volitional modality includes expressions like order, request, permission, prohibition, and v"
D13-1095,kawahara-etal-2002-construction,1,\N,Missing
D13-1095,poesio-etal-2010-creating,0,\N,Missing
D13-1095,C08-1097,1,\N,Missing
D13-1095,N06-1023,1,\N,Missing
D13-1095,P09-2022,0,\N,Missing
D13-1095,P06-1079,0,\N,Missing
D13-1095,D10-1086,0,\N,Missing
D13-1095,E12-1072,0,\N,Missing
D13-1095,I11-1085,1,\N,Missing
D13-1095,Y12-1058,1,\N,Missing
D13-1121,Y00-1002,0,0.0604865,"ons. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasing a simple sentence. They manually introduced case alternation rules on the basis of verb types and case patterns and transformed passive sentences into active sentences. Murata et al. (2006) developed a machinelearning-based method for Japanese ca"
D13-1121,J10-4006,0,0.019436,"pus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasin"
D13-1121,W07-1522,0,0.160739,"Missing"
D13-1121,C02-1122,1,0.790053,"al ablative genitive Table 1: Examples of Japanese postpositional case particles and their typical grammatical functions. and tagged their cases in the active voice. Then, they trained SVM classiﬁers using the tagged corpus. Their features for training SVM were made by using several lexical resources such as IPAL (IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo (NLRI, 1993), and the output of Kondo et al.’s method. 3 Lexicalized Case Frames To acquire knowledge for case alternation, we exploit lexicalized case frames that are automatically constructed from 6.9 billion Web sentences by using Kawahara and Kurohashi (2002)’s method. In short, their method ﬁrst parses the input sentences, and then constructs case frames by collecting reliable modiﬁer-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003), and for each voice. However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames deﬁned in FrameNet are included in these case frames. Each case frame describes surface cases that each predicate has and instances"
D13-1121,N06-1023,1,0.94546,"ned from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and ﬁnds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness. 1 Introduction Predicate-argument structure analysis is one of the fundamental techniques for many natural language applications such as recognition of textual entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations."
D13-1121,kawahara-etal-2004-toward,1,0.755809,"al entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations. Instead, postpositional case particles function as case markers. (1) 女が woman-ga 男に man-ni 突き落とされた． was pushed down (A woman was pushed down by a man.) (2) 男が man-ga 女を woman-wo 突き落とした． pushed down (A man pushed down a woman.) Both representations have their own advantages. Surface case analysis is easier than normalized-case analysis, especially when we consider omitted arguments, which are also called zero ana"
D13-1121,korhonen-etal-2006-large,0,0.175816,"mes by collecting reliable modiﬁer-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003), and for each voice. However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames deﬁned in FrameNet are included in these case frames. Each case frame describes surface cases that each predicate has and instances that can ﬁll a case slot, which is fully lexicalized like the subcategorization lexicon VALEX (Korhonen et al., 2006). We list some Japanese postpositional case particles with their typical grammatical functions in Table 1 and show examples of case frames in Table 2.4 Ideally, one case frame is constructed for each meaning and voice of the target predicate. However, since Kawahara and Kurohashi’s method is unsupervised, several case frames are actually constructed 4 Niyotte in Table 2 is a Japanese functional phrase that indicates agent in this case. We treat niyotte as a case particle in this paper for the sake of simplicity. 1215 Case Frame: “突き落とされる-5 (be pushed down-5)” { 京子 (Kyoko):3, 監督 (manager):1, ·"
D13-1121,J04-1003,0,0.029428,"panese. Our method leverages several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did"
D13-1121,P08-1050,0,0.01896,"on patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alter"
D13-1121,P98-2127,0,0.150639,"ces of the ga-case of the case frame “突き落とされ る-5 (be pushed down-5)” and the wo-case of the case frame “突き落とす-4 (push down-4),” which are considered to be aligned and represent patient, are similar. Thus, we exploit semantic similarity simSEM between the instances of the corresponding cases. We ﬁrst deﬁne an asymmetric similarity measure between C1 and C2 , each of which is a set of case slot instances, as follows: sima (C1 , C2 ) = 1  max (sim(i1 , i2 )), i2 ∈C2 |C1 | i1 ∈C1 where sim(i1 , i2 ) is the similarity between instances. In this study, we apply a distributional similarity measure (Lin, 1998), which was computed from the Web corpus used to construct the case frames. We next deﬁne a symmetric similarity measure between C1 and C2 as an average of sima (C1 , C2 ) and sima (C2 , C1 ). 1 sims (C1 , C2 ) = (sima (C1 , C2 )+sima (C2 , C1 )). 2 Then we deﬁne semantic similarity of a case alignment A between case frames CF1 and CF2 . simSEM (A) = 3. Preference of alternation patterns: fP P . N 1  sims (C1,i , C2,a(i) ), N i=1 1217  ,GHQWLID FRUUHVSRQGLQJ FDVHIUDPH where N denotes the number of case slots of CF1 , C1,i denotes a set of instances of the i-th case slot of CF1 , and C"
D13-1121,A00-2034,0,0.0687522,"ve voices in Japanese. Our method leverages several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-craft"
D13-1121,P06-2076,0,0.133723,"passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasing a simple sentence. They manually introduced case alternation rules on the basis of verb types and case patterns and transformed passive sentences into active sentences. Murata et al. (2006) developed a machinelearning-based method for Japanese case alternation. They extracted 3,576 case particles in passive sentences from the Kyoto University Text Corpus Case particle ga wo ni de kara no Case Frame: “突き落とされる-4 (be pushed down-4)” { 女性 (woman):5, 僕 (I):2, 女 (woman):2, · · · }-ga { 海 (sea):229, 川 (bottom):115, 池 (pond):51, · · · }-ni { 継母(stepmother):2, ペガサス(Pegasus):2, · · · }-niyotte ··· Grammatical function nominative accusative dative locative, instrumental ablative genitive Table 1: Examples of Japanese postpositional case particles and their typical grammatical functions. an"
D13-1121,P98-2151,0,0.0145837,"f the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations. Instead, postpositional case particles function as case markers. (1) 女が woman-ga 男に man-ni 突き落とされた． was pushed down (A woman was pushed down by a man.) (2) 男が man-ga 女を woman-wo 突き落とした． pushed down (A man pushed down a woman.) Both representations have their own advantages. Surface case analysis is easier than normalized-case analysis, especially when we consider omitted arguments, which are also called zero anaphors (Nagao and Hasida, 1998). In Japanese, zero anaphora frequently occurs, and the omitted unnormalizedcase of a zero anaphor is often the same as the surface case of its antecedent (Sasano and Kurohashi, 2011). Therefore, surface case analysis suits zero anaphora resolution. On the other hand, when 2 Ga, wo, and ni are typical Japanese postpositional case particles. In most cases, they indicate nominative, accusative, and dative, respectively. 1213 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213–1223, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for"
D13-1121,J05-1004,0,0.0714087,"several lexical resources such as IPAL (IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo (NLRI, 1993), and the output of Kondo et al.’s method. 3 Lexicalized Case Frames To acquire knowledge for case alternation, we exploit lexicalized case frames that are automatically constructed from 6.9 billion Web sentences by using Kawahara and Kurohashi (2002)’s method. In short, their method ﬁrst parses the input sentences, and then constructs case frames by collecting reliable modiﬁer-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003), and for each voice. However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames deﬁned in FrameNet are included in these case frames. Each case frame describes surface cases that each predicate has and instances that can ﬁll a case slot, which is fully lexicalized like the subcategorization lexicon VALEX (Korhonen et al., 2006). We list some Japanese postpositional case particles with their typical grammatical functions in Table 1 and show examples of case frames in Table 2."
D13-1121,I11-1085,1,0.838446,"nstead, postpositional case particles function as case markers. (1) 女が woman-ga 男に man-ni 突き落とされた． was pushed down (A woman was pushed down by a man.) (2) 男が man-ga 女を woman-wo 突き落とした． pushed down (A man pushed down a woman.) Both representations have their own advantages. Surface case analysis is easier than normalized-case analysis, especially when we consider omitted arguments, which are also called zero anaphors (Nagao and Hasida, 1998). In Japanese, zero anaphora frequently occurs, and the omitted unnormalizedcase of a zero anaphor is often the same as the surface case of its antecedent (Sasano and Kurohashi, 2011). Therefore, surface case analysis suits zero anaphora resolution. On the other hand, when 2 Ga, wo, and ni are typical Japanese postpositional case particles. In most cases, they indicate nominative, accusative, and dative, respectively. 1213 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213–1223, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics we focus on the resulting predicate argument structures, the normalized-case structure is more useful. Speciﬁcally, since a normalized-case structure repres"
D13-1121,P08-1057,0,0.0164512,"raints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dea"
D13-1121,D09-1067,0,0.100645,"xical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the pass"
D13-1121,D08-1055,0,0.165115,"ur method aligns a case frame in the passive voice to a corresponding case frame in the active voice and ﬁnds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness. 1 Introduction Predicate-argument structure analysis is one of the fundamental techniques for many natural language applications such as recognition of textual entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations. Instead, postpositio"
D13-1121,I11-1126,0,0.146631,"ase frame in the passive voice to a corresponding case frame in the active voice and ﬁnds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness. 1 Introduction Predicate-argument structure analysis is one of the fundamental techniques for many natural language applications such as recognition of textual entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations. Instead, postpositional case particles functi"
D13-1121,C98-2122,0,\N,Missing
D13-1121,C98-2146,0,\N,Missing
D15-1276,Y12-1058,1,0.850756,"Missing"
D15-1276,D14-1011,0,0.0376258,"alysis. To solve this problem, we do not use language models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis. 1 (1) Introduction In contrast to space-delimited languages like English, word segmentation is the first and most crucial step for natural language processing (NLP) in unsegmented languages like Japanese, Chinese, and Thai (Kudo et al., 2004; Kaji and Kitsuregawa, 2014; Shen et al., 2014; Kruengkrai et al., 2006). Word segmentation is usually performed jointly with related analysis: POS tagging for Chinese, and POS tagging and lemmatization (analysis of inflected words) for Japanese. Morphological analysis including word segmentation has been widely and actively studied, and for example, Japanese word segmentation accuracy is in the high 90s. However, we often observe that strange outputs of downstream NLP applications such as machine translation and question answering come from incorrect word segmentations. For example, the state-of-the-art and popular Jap"
D15-1276,kawahara-kurohashi-2006-case,1,0.834187,"Missing"
D15-1276,kawahara-etal-2002-construction,1,0.269195,"Missing"
D15-1276,kruengkrai-etal-2006-conditional,0,0.0255249,"nguage models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis. 1 (1) Introduction In contrast to space-delimited languages like English, word segmentation is the first and most crucial step for natural language processing (NLP) in unsegmented languages like Japanese, Chinese, and Thai (Kudo et al., 2004; Kaji and Kitsuregawa, 2014; Shen et al., 2014; Kruengkrai et al., 2006). Word segmentation is usually performed jointly with related analysis: POS tagging for Chinese, and POS tagging and lemmatization (analysis of inflected words) for Japanese. Morphological analysis including word segmentation has been widely and actively studied, and for example, Japanese word segmentation accuracy is in the high 90s. However, we often observe that strange outputs of downstream NLP applications such as machine translation and question answering come from incorrect word segmentations. For example, the state-of-the-art and popular Japanese morphological analyzers, JUMAN a. 外国 /"
D15-1276,W04-3230,0,0.189553,"nary of JUMAN and an additional dictionary comprising 0.8 million words, both of which have lemma, POS and inflection information. The additional dictionary mainly consists of itemizations in articles and article titles in Japanese Wikipedia. We define the scoring function as follows: scoreB (y) = Φ(y) · w,  (1) where y is a tagged word sequence, Φ(y) is a feature vector for y, and w  is a weight vector. Each element in w  gives a weight to its corresponding feature in Φ(y). We use the unigram and the bigram features composed from word base form, POS and inflection described in Kudo et al. (2004). We also use additional lexical features such as character type, and trigram features used in Zhang and Clark (2008). To learn the weight vector, we adopt exact soft confidence-weighted learning (Wang et al., 2012). To consider out-of-vocabulary (OOV) words that are not found in the dictionary, we automatically generate words at the lookup step by segmenting the input string by character types2 . For training, we regard words that are not found in the dictionary but found in the training corpus as OOV words to learn their weights. 3.3 RNNLM Integrated Model Based on retrained RNNLM, we calcul"
D15-1276,I13-1181,0,0.106274,"s to normalize informally spelled words in microblogs. Therefore, their objective is different from ours. Some studies have used character-based language models for Chinese word segmentation and POS tagging (Zheng et al., 2013; Liu et al., 2014). Although their approaches have no drawbacks of learning incorrect segmentations, they only capture more local information than word-based language models. Word embeddings have been also used for morphological analysis. Neural network based models have been proposed for Chinese word segmentation and POS tagging (Pei et al., 2014) or word segmentation (Mansur et al., 2013). These methods acquire word embeddings from a corpus, and then use them as the input of the neural networks. Our proposed model learns word embeddings via RNNLM, and these embeddings are used for scoring word transitions in morphological analysis. Our usage of word embeddings is different from the previous studies. 3 Proposed Method We propose a new morphological analysis model that considers semantic plausibility of word sequences by using RNNLM. We integrate RNNLM into morphological analysis (Figure 2). We train the RNNLM using both an automatically analyzed corpus and a manually labeled co"
D15-1276,D13-1061,0,0.0242531,"of morphological analysis. They also divided Ngram frequencies into three binned features: highfrequency, middle-frequency and low-frequency. Such coarse features cannot express slight differences in the likelihood of language models. Kaji and Kitsuregawa (2014) used a bigram language model feature for Japanese word segmentation and POS tagging. Their objective of using a language model is to normalize informally spelled words in microblogs. Therefore, their objective is different from ours. Some studies have used character-based language models for Chinese word segmentation and POS tagging (Zheng et al., 2013; Liu et al., 2014). Although their approaches have no drawbacks of learning incorrect segmentations, they only capture more local information than word-based language models. Word embeddings have been also used for morphological analysis. Neural network based models have been proposed for Chinese word segmentation and POS tagging (Pei et al., 2014) or word segmentation (Mansur et al., 2013). These methods acquire word embeddings from a corpus, and then use them as the input of the neural networks. Our proposed model learns word embeddings via RNNLM, and these embeddings are used for scoring w"
D15-1276,P14-1028,0,0.0729368,"ir objective of using a language model is to normalize informally spelled words in microblogs. Therefore, their objective is different from ours. Some studies have used character-based language models for Chinese word segmentation and POS tagging (Zheng et al., 2013; Liu et al., 2014). Although their approaches have no drawbacks of learning incorrect segmentations, they only capture more local information than word-based language models. Word embeddings have been also used for morphological analysis. Neural network based models have been proposed for Chinese word segmentation and POS tagging (Pei et al., 2014) or word segmentation (Mansur et al., 2013). These methods acquire word embeddings from a corpus, and then use them as the input of the neural networks. Our proposed model learns word embeddings via RNNLM, and these embeddings are used for scoring word transitions in morphological analysis. Our usage of word embeddings is different from the previous studies. 3 Proposed Method We propose a new morphological analysis model that considers semantic plausibility of word sequences by using RNNLM. We integrate RNNLM into morphological analysis (Figure 2). We train the RNNLM using both an automaticall"
D15-1276,P14-2042,1,0.826308,"m, we do not use language models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis. 1 (1) Introduction In contrast to space-delimited languages like English, word segmentation is the first and most crucial step for natural language processing (NLP) in unsegmented languages like Japanese, Chinese, and Thai (Kudo et al., 2004; Kaji and Kitsuregawa, 2014; Shen et al., 2014; Kruengkrai et al., 2006). Word segmentation is usually performed jointly with related analysis: POS tagging for Chinese, and POS tagging and lemmatization (analysis of inflected words) for Japanese. Morphological analysis including word segmentation has been widely and actively studied, and for example, Japanese word segmentation accuracy is in the high 90s. However, we often observe that strange outputs of downstream NLP applications such as machine translation and question answering come from incorrect word segmentations. For example, the state-of-the-art and popular Japanese morphological"
D15-1276,P99-1023,0,0.0242946,"nt model makes decoding harder than nonrecurrent neural network language models. However, we use RNNLM because the model outperforms other NNLMs (Mikolov, 2012) and the result suggests that the model is more likely to capture semantic plausibility. Since a sentence rarely contains ambiguous and semantically appropriate word sequences, we think that beam search with enough beam size is able to keep the ambiguous candidates of word sequences. In the case of nonrecurrent NNLMs and the base model, which uses trigram features, we can conduct exact decoding using the second-order Viterbi algorithm (Thede and Harper, 1999). 4 4.1 Experiments words (Cp ) as 5, which is the default value in the implementation of Mikolov et al. (2011). We tuned the parameters of our proposed model and the baseline model (α and Lp ) and the parameters of language models using grid search on the development data. We set α = 0.3, Lp =1.5 for the proposed model (“ Base + RNNLMretrain ”).3 We measured the performance of the baseline models and the proposed model by F-value of word segmentation and F-value of joint evaluation of word segmentation and POS tagging. We calculated F-value for the two corpora (news and web) and the merged co"
D15-1276,I11-1035,0,0.0155168,"incorrect analyses which support such a semantically strange sequence. This would prefer analysis toward semantically appropriate word sequences. When a morphological analyzer utilizes such a generalized and reasonable language model, it can penalize strange segmentations like “外国 (foreign)/人参 (carrot)/ 政権 (regime),” leading to better accuracy. We furthermore retrain RNNLM using an annotated corpus of manually segmented 45k sentences, which further improves morphological analysis. 2 Related Work There have been several studies that have integrated language models into morphological analysis. Wang et al. (2011) improved Chinese word segmentation and POS tagging by using N-gram features learned from an automatically segmented corpus. However, since the auto-segmented corpus inevitably contains segmentation errors, frequent N-grams are not always correct and thus this problem might affect the performance of morphological analysis. They also divided Ngram frequencies into three binned features: highfrequency, middle-frequency and low-frequency. Such coarse features cannot express slight differences in the likelihood of language models. Kaji and Kitsuregawa (2014) used a bigram language model feature fo"
D15-1276,P08-1101,0,0.0244808,"S and inflection information. The additional dictionary mainly consists of itemizations in articles and article titles in Japanese Wikipedia. We define the scoring function as follows: scoreB (y) = Φ(y) · w,  (1) where y is a tagged word sequence, Φ(y) is a feature vector for y, and w  is a weight vector. Each element in w  gives a weight to its corresponding feature in Φ(y). We use the unigram and the bigram features composed from word base form, POS and inflection described in Kudo et al. (2004). We also use additional lexical features such as character type, and trigram features used in Zhang and Clark (2008). To learn the weight vector, we adopt exact soft confidence-weighted learning (Wang et al., 2012). To consider out-of-vocabulary (OOV) words that are not found in the dictionary, we automatically generate words at the lookup step by segmenting the input string by character types2 . For training, we regard words that are not found in the dictionary but found in the training corpus as OOV words to learn their weights. 3.3 RNNLM Integrated Model Based on retrained RNNLM, we calculate an RNNLM score (scoreR (y)) to be integrated into the base model. The RNNLM score is defined as the log probabili"
D15-1276,zhang-etal-2004-interpreting,0,0.0211283,"Missing"
D16-1049,W13-2201,0,0.479219,"i, Sakyo-ku, Kyoto, Japan 2 Japan Science and Technology Agency, Kawaguchi-shi, Saitama, Japan otani.naoki.65v@st.kyoto-u.ac.jp nakazawa@pa.jst.jp Abstract Thus, many previous studies focused on pairwise comparisons instead of absolute evaluations. The same task is given to multiple workers, and their responses are aggregated to obtain a reliable answer. We must, therefore, develop methods that robustly estimate the MT performance based on many pairwise comparisons. Some aggregation methods have been proposed for MT competitions hosted by the Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013; Hopkins and May, 2013; Sakaguchi et al., 2014), where a ranking of the submitted systems is produced by aggregating many manual judgments of pairwise comparisons of system outputs. However, existing methods do not consider the following important issues. Recent work on machine translation has used crowdsourcing to reduce costs of manual evaluations. However, crowdsourced judgments are often biased and inaccurate. In this paper, we present a statistical model that aggregates many manual pairwise comparisons to robustly measure a machine translation system’s performance. Our method applies gra"
D16-1049,J15-2005,0,0.0181214,"resented by a Gaussian distribution. Sakaguchi et al. (2014) applied TrueSkill (Herbrich et al., 2006) to reduce the number of comparisons to reach the final estimate based on an active learning strategy. The same model was recently used for grammatical error correction (Grundkiewicz et al., 2015; Napoles et al., 2015). These methods acquire the final system-level scores, whereas our model also estimates segment specific and judge specific parameters. The Bradley–Terry (BT) model was the result of a seminal study on aggregating pairwise comparisons (Bradley and Terry, 1952; Chen et al., 2013; Dras, 2015). Recently, Chen et al. (2013) explicitly incorporated the quality of judges into the BT model, and applied it to quality control in crowdsourcing. The previously mentioned methods focused on pairwise comparisons of all combination of the MT systems, and thus, the number of comparisons increases rapidly as the number of systems increases. 3 Problem Setting We first describe the problem setting, as shown in Figure 1. Assume that there are a group of systems I indexed by i, a set of segments J indexed by j, and a set of judges K indexed by k. Before a manual evaluation, we fix an arbitrary basel"
D16-1049,goto-etal-2014-crowdsourcing,0,0.0380694,"Missing"
D16-1049,D15-1052,0,0.0127855,"used to produce the WMT13 official rankings (Bojar et al., 2013), considering statistical significance of the results (Koehn, 2012). Hopkins and May (2013) noted that we should consider the relative matchup difficulty, and proposed a statistical aggregation model. Their model assumes that the quality of each system can be represented by a Gaussian distribution. Sakaguchi et al. (2014) applied TrueSkill (Herbrich et al., 2006) to reduce the number of comparisons to reach the final estimate based on an active learning strategy. The same model was recently used for grammatical error correction (Grundkiewicz et al., 2015; Napoles et al., 2015). These methods acquire the final system-level scores, whereas our model also estimates segment specific and judge specific parameters. The Bradley–Terry (BT) model was the result of a seminal study on aggregating pairwise comparisons (Bradley and Terry, 1952; Chen et al., 2013; Dras, 2015). Recently, Chen et al. (2013) explicitly incorporated the quality of judges into the BT model, and applied it to quality control in crowdsourcing. The previously mentioned methods focused on pairwise comparisons of all combination of the MT systems, and thus, the number of comparisons"
D16-1049,P13-1139,0,0.382213,"Japan 2 Japan Science and Technology Agency, Kawaguchi-shi, Saitama, Japan otani.naoki.65v@st.kyoto-u.ac.jp nakazawa@pa.jst.jp Abstract Thus, many previous studies focused on pairwise comparisons instead of absolute evaluations. The same task is given to multiple workers, and their responses are aggregated to obtain a reliable answer. We must, therefore, develop methods that robustly estimate the MT performance based on many pairwise comparisons. Some aggregation methods have been proposed for MT competitions hosted by the Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013; Hopkins and May, 2013; Sakaguchi et al., 2014), where a ranking of the submitted systems is produced by aggregating many manual judgments of pairwise comparisons of system outputs. However, existing methods do not consider the following important issues. Recent work on machine translation has used crowdsourcing to reduce costs of manual evaluations. However, crowdsourced judgments are often biased and inaccurate. In this paper, we present a statistical model that aggregates many manual pairwise comparisons to robustly measure a machine translation system’s performance. Our method applies graded response model from"
D16-1049,2012.iwslt-papers.5,0,0.085833,"nts to produce reliable rankings. 1 We also show that our method accurately replicated the WMT13 official system scores using a few comparisons. However, this is not the main focus of this paper. 512 Figure 1: Illustration of manual pairwise comparison. Each system yields translations. Judges compare them with a baseline translation and report their preferences. Our goal is to aggregate the judgments to determine the performance of each system. Frequency based approaches were used to produce the WMT13 official rankings (Bojar et al., 2013), considering statistical significance of the results (Koehn, 2012). Hopkins and May (2013) noted that we should consider the relative matchup difficulty, and proposed a statistical aggregation model. Their model assumes that the quality of each system can be represented by a Gaussian distribution. Sakaguchi et al. (2014) applied TrueSkill (Herbrich et al., 2006) to reduce the number of comparisons to reach the final estimate based on an active learning strategy. The same model was recently used for grammatical error correction (Grundkiewicz et al., 2015; Napoles et al., 2015). These methods acquire the final system-level scores, whereas our model also estima"
D16-1049,P15-2097,0,0.0240859,"official rankings (Bojar et al., 2013), considering statistical significance of the results (Koehn, 2012). Hopkins and May (2013) noted that we should consider the relative matchup difficulty, and proposed a statistical aggregation model. Their model assumes that the quality of each system can be represented by a Gaussian distribution. Sakaguchi et al. (2014) applied TrueSkill (Herbrich et al., 2006) to reduce the number of comparisons to reach the final estimate based on an active learning strategy. The same model was recently used for grammatical error correction (Grundkiewicz et al., 2015; Napoles et al., 2015). These methods acquire the final system-level scores, whereas our model also estimates segment specific and judge specific parameters. The Bradley–Terry (BT) model was the result of a seminal study on aggregating pairwise comparisons (Bradley and Terry, 1952; Chen et al., 2013; Dras, 2015). Recently, Chen et al. (2013) explicitly incorporated the quality of judges into the BT model, and applied it to quality control in crowdsourcing. The previously mentioned methods focused on pairwise comparisons of all combination of the MT systems, and thus, the number of comparisons increases rapidly as t"
D16-1049,W14-3301,0,0.0892015,"Missing"
D16-1049,W15-3001,0,\N,Missing
D18-2010,D15-1276,1,0.72855,"Daisuke Kawahara Arseny Tolmachev Graduate School Graduate School Graduate School of Informatics of Informatics of Informatics Kyoto University Kyoto University Kyoto University Yoshida-honmachi, Sakyo-ku Yoshida-honmachi, Sakyo-ku Yoshida-honmachi, Sakyo-ku Kyoto, 606-8501, Japan Kyoto, 606-8501, Japan Kyoto, 606-8501, Japan dk@i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp arseny@kotonoha.ws Abstract open domain data (like web texts) the accuracy decreases, and it is difficult to improve that accuracy without creating costly annotations by trained experts. The Juman++ Japanese morphological analyzer (Morita et al. 2015, referred to also as V1), which uses a combination of a linear model and a neural network-based language model (RNNLM) to compute a semantic plausibility of a segmentation. Juman++ has achieved state-of-the-art analysis accuracy on Jumandic (the JUMAN dictionary and segmentation standard (Kurohashi and Kawahara, 2012)) based corpora, and drastically reduced the number of intolerable analysis errors. Unfortunately, its execution speed was extremely slow and this has limited the practical usage of Juman++. We have developed a morphological analysis toolkit consisting of three components: a morp"
D18-2010,P11-2093,0,0.0214977,"ctable to compute scores for all paths through the lattice. Instead, we use beam search. Additionally, because the RNNLM is computationally-heavy, we compute the RNN score only for the paths which remain in the beam of the special end-of-string token. The overall design is to cut off improbable analysis results with a simple model and then re-rank by RNNLM. edge, a similar set of tools has never been developed before. Juman++ V2 and the development tools are language and segmentation standard independent and are released under the permissive Apache 2 open source license. 2 Related Work KyTea (Neubig et al., 2011) is a similar tool that can perform morphological analysis for languages with the continuous script. It can also be trained using partial annotation data and output point-wise confidence scores for the analysis result which were used for creating partially annotated data in an active learning scenario. Still, by using a pointwise approach and estimating auxiliary tags (like POS) after computing segmentation, KyTea trades off accuracy for simplicity. Juman++ is faster, has better accuracy, does tag estimation jointly with segmentation, uses an online learning approach and can use longer context"
D18-2010,D08-1112,0,0.183796,"Missing"
D19-1241,P17-1105,0,0.0149618,"sed seq2tree for semantic parsing and out performed the seq2seq model. One drawback is that their generation is at token level so it cannot guarantee the result is syntactically correct. Grammar rules were used to solve this problem. Another drawback is that they needed special tokens for predicting branches, which are not necessary for MWPs because all operators are binary operators. The similar framework is also used in code generation (Zhang et al., 2016; Yin and Neubig, 2017). Alvarez-Melis and Jaakkola (2017) presented doubly recurrent neural networks to predict tree topology explicitly. Rabinovich et al. (2017) presented a abstract syntax network that combines edge information for code generation. Convolution neural networks (CNNs) were used for code generation decoding because the output program is much longer than semantic parsing and MWPs, and RNNs suffer from the long dependency problem (Sun et al., 2018). 3 Model Our model consists of two stages as shown in Figure 2: the encoder stage that encodes the input natural language into a sequence of representation vectors and the decoder stage that receives these vectors and decodes the AST of the equation with Equation Quantities Template Prefix temp"
D19-1241,D15-1202,0,0.295332,"single model performance. 2 Related Work Our work synthesizes two strands of research, which are math word problems and seq2tree encoder-decoder architectures. 2.1 Math Word Problems Early approaches hand engineered rule-based systems to solve MWPs (Mukherjee and Garain, 2008). The rules could only cover a limited domain of problems, while math word problems are flexible in real-world settings. There are currently three major research lines in solving MWPs. The first research line maps a problem text into logical forms, and then uses the logical forms to obtain the equation (Shi et al., 2015; Roy and Roth, 2015; Huang et al., 2017; Roy and Roth, 2018). Shi et al. (2015) defined a Dolphin language to connect math word problems and logical forms. The major drawback is that it requires extra human annotation for the logic forms. Another research line uses either a retrieval or classification model to maintain a template, and then fills in the slots with quantities. Kushman et al. (2014) first introduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514 samples. They broug"
D19-1241,N19-1272,0,0.525246,"Missing"
D19-1241,P16-1004,0,0.173413,"ict the operators. However, the topology of the AST is determined in the first step without tree structure information. To our best knowledge, we are the first to explicitly give the model guidance of parent and sibling nodes. 2.2 Seq2Tree Architectures Seq2Tree-style encoder-decoder is mainly used in two fields both of which try to bridge natural language and a tree structured output. Semantic parsing is the task that translates natural language text to formal meaning logical forms or structured queries. Code generation maps a piece of program description to programming language source code. Dong and Lapata (2016) first used recurrent neural networks (RNNs) based seq2tree for semantic parsing and out performed the seq2seq model. One drawback is that their generation is at token level so it cannot guarantee the result is syntactically correct. Grammar rules were used to solve this problem. Another drawback is that they needed special tokens for predicting branches, which are not necessary for MWPs because all operators are binary operators. The similar framework is also used in code generation (Zhang et al., 2016; Yin and Neubig, 2017). Alvarez-Melis and Jaakkola (2017) presented doubly recurrent neural"
D19-1241,Q18-1012,0,0.293648,"Our work synthesizes two strands of research, which are math word problems and seq2tree encoder-decoder architectures. 2.1 Math Word Problems Early approaches hand engineered rule-based systems to solve MWPs (Mukherjee and Garain, 2008). The rules could only cover a limited domain of problems, while math word problems are flexible in real-world settings. There are currently three major research lines in solving MWPs. The first research line maps a problem text into logical forms, and then uses the logical forms to obtain the equation (Shi et al., 2015; Roy and Roth, 2015; Huang et al., 2017; Roy and Roth, 2018). Shi et al. (2015) defined a Dolphin language to connect math word problems and logical forms. The major drawback is that it requires extra human annotation for the logic forms. Another research line uses either a retrieval or classification model to maintain a template, and then fills in the slots with quantities. Kushman et al. (2014) first introduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514 samples. They brought out a twostep pipeline model, which fi"
D19-1241,D14-1058,0,0.0698436,"k, ALG514, which contained 514 samples. They brought out a twostep pipeline model, which first used a classifier to select a template and then mapped the numbers into the slots. One major drawback is that it cannot solve problems beyond the templates in the training data. This two-step pipeline model was further extended with tree-based features, ranking style retrieval models and so on (Upadhyay and Chang, 2017; Roy and Roth, 2017). Huang et al. (2016) released the first large-scale dataset Dophin18K and trained a similar system on it. The third research line directly generates the equation. Hosseini et al. (2014) cast the problem into a State Transition Diagram of verbs and trained a binary classifier that could solve problems with only add and minus operators. Wang et al. (2017) first used a seq2seq model to directly generate the equation template and released a Chinese high-quality large-scale dataset, Math23K. Reinforcement learning was used to further improve the seq2seq framework. Wang et al. (2018b) first extended the seq2seq model by decoding the suffix order sequence of the equations. Wang et al. (2018a) introduced equation normalization techniques that leverage the duplicated template problem"
D19-1241,D17-1084,0,0.15949,"ance. 2 Related Work Our work synthesizes two strands of research, which are math word problems and seq2tree encoder-decoder architectures. 2.1 Math Word Problems Early approaches hand engineered rule-based systems to solve MWPs (Mukherjee and Garain, 2008). The rules could only cover a limited domain of problems, while math word problems are flexible in real-world settings. There are currently three major research lines in solving MWPs. The first research line maps a problem text into logical forms, and then uses the logical forms to obtain the equation (Shi et al., 2015; Roy and Roth, 2015; Huang et al., 2017; Roy and Roth, 2018). Shi et al. (2015) defined a Dolphin language to connect math word problems and logical forms. The major drawback is that it requires extra human annotation for the logic forms. Another research line uses either a retrieval or classification model to maintain a template, and then fills in the slots with quantities. Kushman et al. (2014) first introduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514 samples. They brought out a twostep pip"
D19-1241,P16-1084,0,0.0485036,"ntroduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514 samples. They brought out a twostep pipeline model, which first used a classifier to select a template and then mapped the numbers into the slots. One major drawback is that it cannot solve problems beyond the templates in the training data. This two-step pipeline model was further extended with tree-based features, ranking style retrieval models and so on (Upadhyay and Chang, 2017; Roy and Roth, 2017). Huang et al. (2016) released the first large-scale dataset Dophin18K and trained a similar system on it. The third research line directly generates the equation. Hosseini et al. (2014) cast the problem into a State Transition Diagram of verbs and trained a binary classifier that could solve problems with only add and minus operators. Wang et al. (2017) first used a seq2seq model to directly generate the equation template and released a Chinese high-quality large-scale dataset, Math23K. Reinforcement learning was used to further improve the seq2seq framework. Wang et al. (2018b) first extended the seq2seq model b"
D19-1241,Q15-1042,0,0.0466133,"s to map the problems into several predefined templates in classification style or retrieval style. The major drawback of these approaches is that they are inflexible for new templates and require extra effort to design rules, features and templates. Modeling the tree structure of math equations has been considered as an important factor when building models for MWP. As shown in Figure 1, each equation could be transformed into an abstract syntax tree (AST). Roy and Roth (2017) built an expression tree and combined two classifiers for quantity relevance prediction and operator classification. Koncel-Kedziorski et al. (2015) designed a template ranking function based on the ∗ This denotes equal contribution. + 660 34 Figure 1: One example of MWP. Problem refers to the natural language descriptions. Equation refers to the formal math equation. Prefix refers to the prefix notation of the equation. Answer refers to the final quantity solution. AST refers to the AST of the equation. AST of the equations. However, these approaches are based on traditional methods and require feature engineering. Recently, the appearance of large-scale datasets and the development of neural generative models have opened a new research"
D19-1241,P14-1026,0,0.567564,"real-world settings. There are currently three major research lines in solving MWPs. The first research line maps a problem text into logical forms, and then uses the logical forms to obtain the equation (Shi et al., 2015; Roy and Roth, 2015; Huang et al., 2017; Roy and Roth, 2018). Shi et al. (2015) defined a Dolphin language to connect math word problems and logical forms. The major drawback is that it requires extra human annotation for the logic forms. Another research line uses either a retrieval or classification model to maintain a template, and then fills in the slots with quantities. Kushman et al. (2014) first introduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514 samples. They brought out a twostep pipeline model, which first used a classifier to select a template and then mapped the numbers into the slots. One major drawback is that it cannot solve problems beyond the templates in the training data. This two-step pipeline model was further extended with tree-based features, ranking style retrieval models and so on (Upadhyay and Chang, 2017; Roy and Roth,"
D19-1241,D15-1135,0,0.106382,"state-of-the-art single model performance. 2 Related Work Our work synthesizes two strands of research, which are math word problems and seq2tree encoder-decoder architectures. 2.1 Math Word Problems Early approaches hand engineered rule-based systems to solve MWPs (Mukherjee and Garain, 2008). The rules could only cover a limited domain of problems, while math word problems are flexible in real-world settings. There are currently three major research lines in solving MWPs. The first research line maps a problem text into logical forms, and then uses the logical forms to obtain the equation (Shi et al., 2015; Roy and Roth, 2015; Huang et al., 2017; Roy and Roth, 2018). Shi et al. (2015) defined a Dolphin language to connect math word problems and logical forms. The major drawback is that it requires extra human annotation for the logic forms. Another research line uses either a retrieval or classification model to maintain a template, and then fills in the slots with quantities. Kushman et al. (2014) first introduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514"
D19-1241,E17-1047,0,0.0517011,"with quantities. Kushman et al. (2014) first introduced the idea of ‘equation template’. For example, x = 6*7 and x = 10*5 belong to the same template x = n1 *n2 . They collected the first dataset of this task, ALG514, which contained 514 samples. They brought out a twostep pipeline model, which first used a classifier to select a template and then mapped the numbers into the slots. One major drawback is that it cannot solve problems beyond the templates in the training data. This two-step pipeline model was further extended with tree-based features, ranking style retrieval models and so on (Upadhyay and Chang, 2017; Roy and Roth, 2017). Huang et al. (2016) released the first large-scale dataset Dophin18K and trained a similar system on it. The third research line directly generates the equation. Hosseini et al. (2014) cast the problem into a State Transition Diagram of verbs and trained a binary classifier that could solve problems with only add and minus operators. Wang et al. (2017) first used a seq2seq model to directly generate the equation template and released a Chinese high-quality large-scale dataset, Math23K. Reinforcement learning was used to further improve the seq2seq framework. Wang et al."
D19-1241,D18-1132,0,0.479935,"he equations. However, these approaches are based on traditional methods and require feature engineering. Recently, the appearance of large-scale datasets and the development of neural generative models have opened a new research line for MWP. Wang et al. (2017) cast this task as a sequence generation problem and used a sequence-to-sequence (seq2seq) model to learn the mapping from natural language text to a math equation. Recent approaches use the Reverse Polish notation, also called suffix notation of equations in which operators follow their operands to implicitly model the tree structure (Wang et al., 2018a; Chiang and Chen, 2018). However, these studies lost sight of important information of the math equation ASTs 2370 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2370–2379, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (e.g., parent and siblings of each node), despite of their promising results. Thus, their model has to use extra effort to memorize various pieces of auxiliary information such as the sibling node of the current step"
D19-1241,D17-1088,0,0.577866,"Missing"
D19-1241,P17-1041,0,0.0284643,"Missing"
D19-5814,Y18-1026,1,0.838874,"Missing"
D19-5814,kawahara-etal-2002-construction,1,0.260604,"Missing"
D19-5814,P18-1044,1,0.824968,"rained an MC model using an RC-QA dataset and transfered the pre-trained knowledge to sequence-tosequence models. They used SQuAD 1.1 as the RC-QA dataset and experimented on translation and summarization. While they used different models for pre-training and fine-tuning, we use the same MC model by constructing PAS-QA and RC-QA datasets in the same QA form. 3 Figure 2: An example of RC-QA dataset. focus on the ga case (nominative), the wo case (accusative), and the ni case (dative), which are targeted in the Japanese PAS analysis literature (Shibata et al., 2016; Shibata and Kurohashi, 2018; Kurita et al., 2018; Ouchi et al., 2017). As a source corpus, we use blog articles included in the Driving Experience Corpus (Iwai et al., 2019). We first detect a predicate that has an omitted argument of either of the target three cases by applying the existing PAS analyzer KNP1 to the corpus. KNP tends to overgenerate such predicates, but most erroneous ones are filtered out by the following crowdsourcing step. We extract the sentence that contains the predicate and preceding three sentences as a document. Then, we automatically generate a question using the following template for nominative. • ［述語］の主語は何か？ (W"
D19-5814,N18-2089,0,0.0255049,"te-ofthe-art neural model. • We construct PAS-QA and RC-QA datasets in the driving domain using crowdsourcing. • We improve Japanese PAS analysis by combining the PAS-QA and RC-QA datasets. The current affiliation is Yahoo Japan Corporation. 98 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 98–104 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2 Related Work &apos;؞RFXPHQW  எは右⾞線に移動した。 , PRYHGWRWKHULJKWODQH  ऋ ংॵॡছ⾒شた؛ 120 VDZ WKHUHDUYLHZPLUURU 2.1 QA Dataset Construction FitzGerald et al. (2018) and Michael et al. (2018) constructed QA-SRL Bank 2.0 and QAMRs using crowdsourcing, respectively. They asked crowdworkers to generate question-answer pairs that represent a PAS. These datasets are similar to our PAS-QA dataset, but different in that we focus on omitted arguments and automatically generate questions (see Section 3.1). Many RC-QA datasets have been constructed in recent years. For example, Rajpurkar et al. (2016) constructed SQuAD 1.1, which contains 100K crowdsourced questions and answer spans in a Wikipedia article. Rajpurkar et al. (2018) updated SQuAD 1.1 to 2.0 by adding unanswerable questions. So"
D19-5814,N19-1423,0,0.131266,"g unanswerable questions. Some RC-QA datasets have been built in a specific domain (Welbl et al., 2017; Suster and Daelemans, 2018; Pampari et al., 2018). ؞4XHVWLRQ n⾒た|の主語は何か︖ :KDWLVWKHVXEMHFWRInVDZ|&quot; ؞$QVZHU எ , Figure 1: An example of PAS-QA dataset. &apos; ؞RFXPHQW எの⾞の前をংॖॡपऽञऋढञ ऋढथःञ؛ $SROLFHRIILFHUVWUDGGOLQJKLVELNH ZDVUXQQLQJLQIURQWRIPFDU ؞4XHVWLRQ は何に乗っていた︖ :KDWZDVWKHSROLFHRIILFHUULGLQJ&quot; ؞$QVZHU ংॖॡ KLVELNH 2.2 Machine Comprehension Models Many MC models based on neural networks have been proposed to solve RC-QA datasets. For example, Devlin et al. (2019) proposed an MC model using a language representation model, BERT, which achieved a high-ranked accuracy on the SQuAD 1.1 leaderboard as of September 30, 2019. As a previous study of transfer learning of MC models to other tasks, Pan et al. (2018) pre-trained an MC model using an RC-QA dataset and transfered the pre-trained knowledge to sequence-tosequence models. They used SQuAD 1.1 as the RC-QA dataset and experimented on translation and summarization. While they used different models for pre-training and fine-tuning, we use the same MC model by constructing PAS-QA and RC-QA datasets in the"
D19-5814,D18-1260,0,0.0259608,"ool of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan {ntakahashi, shibata, dk, kuro}@nlp.ist.i.kyoto-u.ac.jp Norio Takahashi Abstract its answer are constructed, and an MC model is trained using these datasets (e.g., Rajpurkar et al. (2016) and Trischler et al. (2017)). MC has made remarkable progress in the last couple of years, and MC models have even exceeded human accuracy in some datasets (Devlin et al., 2019). However, MC accuracy is not necessarily high for documents that contain anaphoric phenomena and those that need external knowledge or inference (Mihaylov et al., 2018; Yang et al., 2018). In this paper, we propose a Japanese PAS analysis method based on the MC framework for a specific domain. In particular, we focus on a challenging task of finding an antecedent of a zero pronoun within PAS analysis. We construct a widecoverage QA dataset for PAS analysis (PAS-QA) in the domain and feed it to an MC model to perform PAS analysis. We also construct a QA dataset for reading comprehension (RC-QA) in the same domain and jointly use the two datasets in the MC model to improve PAS analysis. We consider the domain of blogs on driving because of the following two r"
D19-5814,P18-1191,0,0.0239211,"show its superiority to a state-ofthe-art neural model. • We construct PAS-QA and RC-QA datasets in the driving domain using crowdsourcing. • We improve Japanese PAS analysis by combining the PAS-QA and RC-QA datasets. The current affiliation is Yahoo Japan Corporation. 98 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 98–104 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2 Related Work &apos;؞RFXPHQW  எは右⾞線に移動した。 , PRYHGWRWKHULJKWODQH  ऋ ংॵॡছ⾒شた؛ 120 VDZ WKHUHDUYLHZPLUURU 2.1 QA Dataset Construction FitzGerald et al. (2018) and Michael et al. (2018) constructed QA-SRL Bank 2.0 and QAMRs using crowdsourcing, respectively. They asked crowdworkers to generate question-answer pairs that represent a PAS. These datasets are similar to our PAS-QA dataset, but different in that we focus on omitted arguments and automatically generate questions (see Section 3.1). Many RC-QA datasets have been constructed in recent years. For example, Rajpurkar et al. (2016) constructed SQuAD 1.1, which contains 100K crowdsourced questions and answer spans in a Wikipedia article. Rajpurkar et al. (2018) updated SQuAD 1.1 to 2.0 by adding"
D19-5814,P17-1146,0,0.0248456,"ing an RC-QA dataset and transfered the pre-trained knowledge to sequence-tosequence models. They used SQuAD 1.1 as the RC-QA dataset and experimented on translation and summarization. While they used different models for pre-training and fine-tuning, we use the same MC model by constructing PAS-QA and RC-QA datasets in the same QA form. 3 Figure 2: An example of RC-QA dataset. focus on the ga case (nominative), the wo case (accusative), and the ni case (dative), which are targeted in the Japanese PAS analysis literature (Shibata et al., 2016; Shibata and Kurohashi, 2018; Kurita et al., 2018; Ouchi et al., 2017). As a source corpus, we use blog articles included in the Driving Experience Corpus (Iwai et al., 2019). We first detect a predicate that has an omitted argument of either of the target three cases by applying the existing PAS analyzer KNP1 to the corpus. KNP tends to overgenerate such predicates, but most erroneous ones are filtered out by the following crowdsourcing step. We extract the sentence that contains the predicate and preceding three sentences as a document. Then, we automatically generate a question using the following template for nominative. • ［述語］の主語は何か？ (What is the subject of"
D19-5814,P18-1192,0,0.0624874,"Missing"
D19-5814,D18-1191,0,0.0335015,"Missing"
D19-5814,P18-2124,0,0.0315232,"LUURU 2.1 QA Dataset Construction FitzGerald et al. (2018) and Michael et al. (2018) constructed QA-SRL Bank 2.0 and QAMRs using crowdsourcing, respectively. They asked crowdworkers to generate question-answer pairs that represent a PAS. These datasets are similar to our PAS-QA dataset, but different in that we focus on omitted arguments and automatically generate questions (see Section 3.1). Many RC-QA datasets have been constructed in recent years. For example, Rajpurkar et al. (2016) constructed SQuAD 1.1, which contains 100K crowdsourced questions and answer spans in a Wikipedia article. Rajpurkar et al. (2018) updated SQuAD 1.1 to 2.0 by adding unanswerable questions. Some RC-QA datasets have been built in a specific domain (Welbl et al., 2017; Suster and Daelemans, 2018; Pampari et al., 2018). ؞4XHVWLRQ n⾒た|の主語は何か︖ :KDWLVWKHVXEMHFWRInVDZ|&quot; ؞$QVZHU எ , Figure 1: An example of PAS-QA dataset. &apos; ؞RFXPHQW எの⾞の前をংॖॡपऽञऋढञ ऋढथःञ؛ $SROLFHRIILFHUVWUDGGOLQJKLVELNH ZDVUXQQLQJLQIURQWRIPFDU ؞4XHVWLRQ は何に乗っていた︖ :KDWZDVWKHSROLFHRIILFHUULGLQJ&quot; ؞$QVZHU ংॖॡ KLVELNH 2.2 Machine Comprehension Models Many MC models based on neural networks have been proposed to s"
D19-5814,P16-1162,0,0.0116349,"owledge, and suggest that it is better to construct both PAS-QA and RCQA datasets to develop a PAS analyzer for a new 5.1 Experimental Settings We adopt BERT (Devlin et al., 2019) as an MC model. We split the triplets in the PAS-QA dataset as shown in Table 4. All sentences in these datasets are preprocessed using the Japanese morphological analyzer, JUMAN++3 . We trained a Japanese pre-trained BERT model using Japanese Wikipedia, which consists of approximately 18 million sentences. The input sentences were segmented into words by JUMAN++, and words were broken into subwords by applying BPE (Sennrich et al., 2016). The parameters of BERT are the same as English BERTBASE . The number of epochs for the pre-training was 30. The state-of-the-art baseline PAS analyzer, NN-PAS, was trained using the existing PAS dataset, KWDLC4 (Kyoto University Web Document Leads Corpus), as described in Shibata and Kurohashi (2018). We also trained an NN-PAS model using the PAS-QA dataset in addition to KWDLC (hereafter, NN-PAS′ ). For this training, the PAS-QA dataset was converted to the same format as KWDLC, where questions are deleted, and only answers are used. The PAS-QA test data is used to compare the baseline meth"
D19-5814,P16-1117,1,0.848269,"f MC models to other tasks, Pan et al. (2018) pre-trained an MC model using an RC-QA dataset and transfered the pre-trained knowledge to sequence-tosequence models. They used SQuAD 1.1 as the RC-QA dataset and experimented on translation and summarization. While they used different models for pre-training and fine-tuning, we use the same MC model by constructing PAS-QA and RC-QA datasets in the same QA form. 3 Figure 2: An example of RC-QA dataset. focus on the ga case (nominative), the wo case (accusative), and the ni case (dative), which are targeted in the Japanese PAS analysis literature (Shibata et al., 2016; Shibata and Kurohashi, 2018; Kurita et al., 2018; Ouchi et al., 2017). As a source corpus, we use blog articles included in the Driving Experience Corpus (Iwai et al., 2019). We first detect a predicate that has an omitted argument of either of the target three cases by applying the existing PAS analyzer KNP1 to the corpus. KNP tends to overgenerate such predicates, but most erroneous ones are filtered out by the following crowdsourcing step. We extract the sentence that contains the predicate and preceding three sentences as a document. Then, we automatically generate a question using the f"
D19-5814,P18-1054,1,0.905025,"asks, Pan et al. (2018) pre-trained an MC model using an RC-QA dataset and transfered the pre-trained knowledge to sequence-tosequence models. They used SQuAD 1.1 as the RC-QA dataset and experimented on translation and summarization. While they used different models for pre-training and fine-tuning, we use the same MC model by constructing PAS-QA and RC-QA datasets in the same QA form. 3 Figure 2: An example of RC-QA dataset. focus on the ga case (nominative), the wo case (accusative), and the ni case (dative), which are targeted in the Japanese PAS analysis literature (Shibata et al., 2016; Shibata and Kurohashi, 2018; Kurita et al., 2018; Ouchi et al., 2017). As a source corpus, we use blog articles included in the Driving Experience Corpus (Iwai et al., 2019). We first detect a predicate that has an omitted argument of either of the target three cases by applying the existing PAS analyzer KNP1 to the corpus. KNP tends to overgenerate such predicates, but most erroneous ones are filtered out by the following crowdsourcing step. We extract the sentence that contains the predicate and preceding three sentences as a document. Then, we automatically generate a question using the following template for nominat"
D19-5814,D18-1548,0,0.0565549,"Missing"
D19-5814,N18-1140,0,0.0508556,"Missing"
D19-5814,W17-2623,0,0.0517339,"Missing"
D19-5814,W17-4413,0,0.0307523,"ng, respectively. They asked crowdworkers to generate question-answer pairs that represent a PAS. These datasets are similar to our PAS-QA dataset, but different in that we focus on omitted arguments and automatically generate questions (see Section 3.1). Many RC-QA datasets have been constructed in recent years. For example, Rajpurkar et al. (2016) constructed SQuAD 1.1, which contains 100K crowdsourced questions and answer spans in a Wikipedia article. Rajpurkar et al. (2018) updated SQuAD 1.1 to 2.0 by adding unanswerable questions. Some RC-QA datasets have been built in a specific domain (Welbl et al., 2017; Suster and Daelemans, 2018; Pampari et al., 2018). ؞4XHVWLRQ n⾒た|の主語は何か︖ :KDWLVWKHVXEMHFWRInVDZ|&quot; ؞$QVZHU எ , Figure 1: An example of PAS-QA dataset. &apos; ؞RFXPHQW எの⾞の前をংॖॡपऽञऋढञ ऋढथःञ؛ $SROLFHRIILFHUVWUDGGOLQJKLVELNH ZDVUXQQLQJLQIURQWRIPFDU ؞4XHVWLRQ は何に乗っていた︖ :KDWZDVWKHSROLFHRIILFHUULGLQJ&quot; ؞$QVZHU ংॖॡ KLVELNH 2.2 Machine Comprehension Models Many MC models based on neural networks have been proposed to solve RC-QA datasets. For example, Devlin et al. (2019) proposed an MC model using a language representation model, BERT, which achieved"
D19-5814,D18-1259,0,0.0330184,"to University Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan {ntakahashi, shibata, dk, kuro}@nlp.ist.i.kyoto-u.ac.jp Norio Takahashi Abstract its answer are constructed, and an MC model is trained using these datasets (e.g., Rajpurkar et al. (2016) and Trischler et al. (2017)). MC has made remarkable progress in the last couple of years, and MC models have even exceeded human accuracy in some datasets (Devlin et al., 2019). However, MC accuracy is not necessarily high for documents that contain anaphoric phenomena and those that need external knowledge or inference (Mihaylov et al., 2018; Yang et al., 2018). In this paper, we propose a Japanese PAS analysis method based on the MC framework for a specific domain. In particular, we focus on a challenging task of finding an antecedent of a zero pronoun within PAS analysis. We construct a widecoverage QA dataset for PAS analysis (PAS-QA) in the domain and feed it to an MC model to perform PAS analysis. We also construct a QA dataset for reading comprehension (RC-QA) in the same domain and jointly use the two datasets in the MC model to improve PAS analysis. We consider the domain of blogs on driving because of the following two reasons. Firstly, we"
D19-6014,N16-1014,0,0.0289546,"distribution. Additionally, we extend the CVAE with a reconstruction mechanism (Tu et al., 2017) to alleviate the model’s tendency to concentrate on a small number of highly typical events. Diversity-Promoting Objective Functions In dialogue response generation, seq2seq is known to suffer from the generic response problem: The model often ends up blindly generating uninformative responses such as “I don’t know”. A popular approach to this problem is to rerank the candidate outputs, which are usually produced by beam search, according to the mutual information with the conversational context (Li et al., 2016). 4.1 Objective Function We introduce a probabilistic latent variable z and assume that y depends on both x and z. The conditional log likelihood of y given x is written as: Z log p(y|x) = log pθ (y, z|x)dz (1) z Z = log pθ (y|z, x)pθ (z|x)dz. (2) We notice that the reconstruction mechanism (Tu et al., 2017) serves the same purpose in a more straightforward manner, albeit stemming from a different motivation. The reconstruction mechanism forces the model to reconstruct the input from the hidden states of the decoder. Although it was originally proposed for machine translation to prevent over-t"
D19-6014,K16-1002,0,0.218923,"ormation, VAEs adopt probabilistic generation: a VAE encodes y into the probability distribution of z, instead of a point in a low-dimensional vector space. It then reconstructs the input y from z drawn from 114 the posterior distribution. z is assumed to have a prior distribution, for which a multivariate Gaussian distribution is often used. As straightforward extensions of VAEs, conditional VAEs (CVAEs) let probabilistic distributions be conditioned on a common observed variable x (Kingma et al., 2014; Sohn et al., 2015). In our case, x is a current event while y is a next event to predict. Bowman et al. (2016) applied VAEs to text generation. They constructed VAEs using RNNs as its components and found that VAEs with an RNNbased decoder failed to encode meaningful information to z. To alleviate this problem, they proposed simple but effective heuristics: KL cost annealing and word dropout. We also employ these techniques. Figure 1: The neural network architecture of our event prediction model. ⊕ denotes vector concatenation. If a VAE-based text generation model is conditioned on text, it can be seen as a CVAE-based seq2seq model (Zhao et al., 2017; Serban et al., 2017; Zhang et al., 2016). Since a"
D19-6014,P11-2057,0,0.0118185,"the model from concentrating on highly typical events. To facilitate fair and systematic evaluation of the diversityaware models, we also extend existing evaluation datasets by tying each current event to multiple next events. Experiments show that the CVAE-based models drastically outperform deterministic models in terms of precision and that the reconstruction mechanism improves the recall of CVAE-based models without sacrificing precision.1 1 Introduction Typical event sequences are an important class of commonsense knowledge that enables deep text understanding (Schank and Abelson, 1975; LoBue and Yates, 2011). Following previous work (Nguyen et al., 2017), we work on the task of generating a next event conditioned on a current event, which we call event prediction. For example, we want a computer to recognize that the event “board bus” is typically followed by another event “pay bus fare” and to generate the latter word sequence given the former. 1 The source code and the new test sets are publicly available at https://github.com/hkiyomaru/ diversity-aware-event-prediction. 113 Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 113–122 c Hongkong, Chin"
D19-6014,P08-1090,0,0.572552,"fare” and to generate the latter word sequence given the former. 1 The source code and the new test sets are publicly available at https://github.com/hkiyomaru/ diversity-aware-event-prediction. 113 Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 113–122 c Hongkong, China, November 3, 2019. 2019 Association for Computational Linguistics pre-defined set of candidates for a missing event. A popular strategy is to rank candidates by similarity with the remaining part of the event sequence. Similarity measures include pointwise mutual information (Chambers and Jurafsky, 2008), conditional bigram probability (Jans et al., 2012), and cosine similarities based on latent semantic indexing and word embeddings (Granroth-Wilding and Clark, 2016). For its reliance on pre-defined candidates, however, the classification approach is constrained by its limited flexibility. In the generation task, a model is to directly generate a missing event, usually in the form of a word sequence (Pichotta and Mooney, 2016; Hu et al., 2017; Nguyen et al., 2017), although one previous study adopted a predicate-argument structure-based event representation (Weber et al., 2018). Nguyen et al."
D19-6014,N16-1012,0,0.0220961,"ies memorized event sequences extracted from a corpus and inevitably suffered from low generalization capability and a scalability problem. A promising approach to modeling wide-coverage knowledge is to generalize events by representing them in a continuous space (Granroth-Wilding and Clark, 2016; Nguyen et al., 2017; Hu et al., 2017). Nguyen et al. (2017) generate a next event using the sequenceto-sequence (seq2seq) framework, which was first proposed for machine translation (Bahdanau et al., 2014) and subsequently applied to various NLP tasks including text summarization (Rush et al., 2015; Chopra et al., 2016) and dialog generation (Sordoni et al., 2015; Serban et al., 2016). One limitation of the simple seq2seq models, which are deterministic in nature, is their inability to take into account an important characteristic of events: What can happen after a current event is usually diverse. For the example of “board bus” mentioned above, “get off bus” as well as “pay bus fare” is a valid next event. The inherent diversity makes it difficult to train deterministic models, and during testing, they can hardly generate multiple next events that are both valid and diverse. To address this problem, we firs"
D19-6014,I17-2007,0,0.120154,"ational Autoencoder with Reconstruction Hirokazu Kiyomaru Kazumasa Omura Yugo Murawaki Daisuke Kawahara Sadao Kurohashi Graduate School of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan {kiyomaru, omura, murawaki, dk, kuro}@nlp.ist.i.kyoto-u.ac.jp Abstract Early studies memorized event sequences extracted from a corpus and inevitably suffered from low generalization capability and a scalability problem. A promising approach to modeling wide-coverage knowledge is to generalize events by representing them in a continuous space (Granroth-Wilding and Clark, 2016; Nguyen et al., 2017; Hu et al., 2017). Nguyen et al. (2017) generate a next event using the sequenceto-sequence (seq2seq) framework, which was first proposed for machine translation (Bahdanau et al., 2014) and subsequently applied to various NLP tasks including text summarization (Rush et al., 2015; Chopra et al., 2016) and dialog generation (Sordoni et al., 2015; Serban et al., 2016). One limitation of the simple seq2seq models, which are deterministic in nature, is their inability to take into account an important characteristic of events: What can happen after a current event is usually diverse. For the examp"
D19-6014,P02-1040,0,0.104688,"). Second, the event pairs were sometimes hard to interpret because they were extracted from adjacent descriptions out of context. The results suggest that further studies in this area should use Wikihow with caution. l1 : Strange expression. l2 : No relation. l3 : A and B are related, but one does not happen after the other. l4 : A happens after B. l5 : B happens after A. 3 https://code.google.com/archive/p/ word2vec/ 117 6 Experiments 6.1 where BLEU is the sentence-level variant of a well-known metric that measures the geometric mean of modified n-gram precision with the penalty of brevity (Papineni et al., 2002). The final score was averaged over the entire test set. We refer to the precision and recall as P@N and R@N, respectively. F@N is the harmonic mean of P@N and R@N. We report the scores with N = 5 and 10, in accordance with the average number of next events in our new test sets. For comparison, we also followed the experimental procedure of Nguyen et al. (2017), where event prediction models deterministically output a single next event using greedy decoding. For CVAEs, we did this by setting z at the mean of the predicted Gaussian prior. The outputs were evaluated by BLEU. We refer to the crit"
D19-6014,E14-1024,0,0.158375,"ntly outperformed the simple seq2seq models in terms of precision (i.e., validity) without hurting recall (i.e., diversity) while forcing the simple seq2seq models to generate diverse outputs yielded low precision. The reconstruction mechanism consistently improved recall of the CVAE-based models while keeping or even increasing precision. We also confirmed that the original test sets failed to detect the clear differences between the models. 2 2.1 Related Work Event Prediction There is a growing body of work on learning typical event sequences (Chambers and Jurafsky, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016; Hu et al., 2017; Nguyen et al., 2017). While early studies explicitly store event sequences in a symbolic manner, a recent approach to this task is to train neural network models that implicitly represent event sequence knowledge as continuous model parameters. In both cases, models are usually evaluated by how well they restore a missing portion of an event sequence. We collectively refer to this task as event prediction. Event prediction can be categorized into two tasks: classification and generation. In the classification task,"
D19-6014,P16-1027,0,0.0766384,"event. A popular strategy is to rank candidates by similarity with the remaining part of the event sequence. Similarity measures include pointwise mutual information (Chambers and Jurafsky, 2008), conditional bigram probability (Jans et al., 2012), and cosine similarities based on latent semantic indexing and word embeddings (Granroth-Wilding and Clark, 2016). For its reliance on pre-defined candidates, however, the classification approach is constrained by its limited flexibility. In the generation task, a model is to directly generate a missing event, usually in the form of a word sequence (Pichotta and Mooney, 2016; Hu et al., 2017; Nguyen et al., 2017), although one previous study adopted a predicate-argument structure-based event representation (Weber et al., 2018). Nguyen et al. (2017) worked on the task of generating a next event given a single event, which we follow in this paper. They adopted the seq2seq framework (Sutskever et al., 2014) and investigated how recurrent neural network (RNN) variants, the number of RNN layers, and the presence or absence of an attention mechanism (Bahdanau et al., 2014) affected the performance. Hu et al. (2017) gave a sequence of events to the model to generate the"
D19-6014,E12-1034,0,0.280491,"Missing"
D19-6014,D15-1044,0,0.0159523,"Abstract Early studies memorized event sequences extracted from a corpus and inevitably suffered from low generalization capability and a scalability problem. A promising approach to modeling wide-coverage knowledge is to generalize events by representing them in a continuous space (Granroth-Wilding and Clark, 2016; Nguyen et al., 2017; Hu et al., 2017). Nguyen et al. (2017) generate a next event using the sequenceto-sequence (seq2seq) framework, which was first proposed for machine translation (Bahdanau et al., 2014) and subsequently applied to various NLP tasks including text summarization (Rush et al., 2015; Chopra et al., 2016) and dialog generation (Sordoni et al., 2015; Serban et al., 2016). One limitation of the simple seq2seq models, which are deterministic in nature, is their inability to take into account an important characteristic of events: What can happen after a current event is usually diverse. For the example of “board bus” mentioned above, “get off bus” as well as “pay bus fare” is a valid next event. The inherent diversity makes it difficult to train deterministic models, and during testing, they can hardly generate multiple next events that are both valid and diverse. To address"
D19-6014,D16-1050,0,0.0243173,"predict. Bowman et al. (2016) applied VAEs to text generation. They constructed VAEs using RNNs as its components and found that VAEs with an RNNbased decoder failed to encode meaningful information to z. To alleviate this problem, they proposed simple but effective heuristics: KL cost annealing and word dropout. We also employ these techniques. Figure 1: The neural network architecture of our event prediction model. ⊕ denotes vector concatenation. If a VAE-based text generation model is conditioned on text, it can be seen as a CVAE-based seq2seq model (Zhao et al., 2017; Serban et al., 2017; Zhang et al., 2016). Since a CVAE learns probabilistic generation, it is suitable for tasks where the output is not uniquely determined according to the input. One of the representative applications of CVAE-based text generation is dialogue response generation, or the task of generating possible replies to a human utterance (Zhao et al., 2017; Serban et al., 2017). Applying CVAEs to next event prediction is a natural choice because the task is also characterized by output diversity. 2.3 3 Problem Setting Given a current event x, we are to generate a variety of events, each of which, y, often happens after x. x a"
D19-6014,P17-1061,0,0.116486,"urrent event while y is a next event to predict. Bowman et al. (2016) applied VAEs to text generation. They constructed VAEs using RNNs as its components and found that VAEs with an RNNbased decoder failed to encode meaningful information to z. To alleviate this problem, they proposed simple but effective heuristics: KL cost annealing and word dropout. We also employ these techniques. Figure 1: The neural network architecture of our event prediction model. ⊕ denotes vector concatenation. If a VAE-based text generation model is conditioned on text, it can be seen as a CVAE-based seq2seq model (Zhao et al., 2017; Serban et al., 2017; Zhang et al., 2016). Since a CVAE learns probabilistic generation, it is suitable for tasks where the output is not uniquely determined according to the input. One of the representative applications of CVAE-based text generation is dialogue response generation, or the task of generating possible replies to a human utterance (Zhao et al., 2017; Serban et al., 2017). Applying CVAEs to next event prediction is a natural choice because the task is also characterized by output diversity. 2.3 3 Problem Setting Given a current event x, we are to generate a variety of events, ea"
D19-6014,N15-1020,0,0.0316724,"Missing"
D19-6014,D18-1413,0,0.0199242,"tion (Chambers and Jurafsky, 2008), conditional bigram probability (Jans et al., 2012), and cosine similarities based on latent semantic indexing and word embeddings (Granroth-Wilding and Clark, 2016). For its reliance on pre-defined candidates, however, the classification approach is constrained by its limited flexibility. In the generation task, a model is to directly generate a missing event, usually in the form of a word sequence (Pichotta and Mooney, 2016; Hu et al., 2017; Nguyen et al., 2017), although one previous study adopted a predicate-argument structure-based event representation (Weber et al., 2018). Nguyen et al. (2017) worked on the task of generating a next event given a single event, which we follow in this paper. They adopted the seq2seq framework (Sutskever et al., 2014) and investigated how recurrent neural network (RNN) variants, the number of RNN layers, and the presence or absence of an attention mechanism (Bahdanau et al., 2014) affected the performance. Hu et al. (2017) gave a sequence of events to the model to generate the next one. Accordingly, they worked on hierarchically encoding the given event sequence using word-level and event-level RNNs. All of these models are dete"
E14-1007,P10-1024,0,0.0149542,"hey, we, ...} observe dobj:{effect} nsubj:{he, ...} observe dobj:{result} prep at:{time} nsubj:{you, child, ...} observe dobj:{bird} .. . 2. merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation to get a set of initial frames, and Figure 1: Examples of predicate-argument structures and initial frames for the verb “observe.” 3. apply clustering to the initial frames based on the Chinese Restaurant Process to produce the final semantic frames. frequencies in the applications of semantic frames or the method proposed by Abend and Rappoport (2010). We apply the following processes to extracted predicate-argument structures: Each of these steps is described in the following sections in detail. 3.1 Extracting Predicate-argument Structures from a Raw Corpus • A verb and an argument are lemmatized, and only the head of an argument is preserved for compound nouns. We first apply dependency parsing to a large raw corpus. We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006).2 Collapsed dependencies are adopted to directly extract prepositional phrases. Then, we extract predicate-argument structures from the depende"
E14-1007,kawahara-kurohashi-2006-case,1,0.679562,"Missing"
E14-1007,P98-1013,0,0.304305,"ga-word corpora. Our semantic frames are verb-specific example-based frames that are distinguished according to their senses. We use the Chinese Restaurant Process to automatically induce these frames from a massive amount of verb instances. In our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to"
E14-1007,N06-1023,1,0.835493,"Although these frames are based on typed dependencies and more semantic than subcategorization frames, they are not distinguished in terms of the senses of words filling a case slot. There are hand-crafted semantic frames in the lexicons of FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Corpus Pattern Analysis (CPA) frames (Hanks, 2012) are another manually created repository of patterns for verbs. Each pattern represents a prototypical word usage as extracted by lexicographers from the BNC. Creating CPA is time consuming, but our proposed For other languages than English, Kawahara and Kurohashi (2006a) proposed a method for automatically compiling Japanese semantic frames from a large web corpus. They applied conventional agglomerative clustering to predicateargument structures using word/frame similarity based on a manually-crafted thesaurus. Since Japanese is head-final and has case-marking postpositions, it seems easier to build semantic frames with it than with other languages such as English. They also achieved an improvement in dependency parsing and predicate-argument structure 59 Sentences: They observed the effects of ... This statistical ability to observe an effect ... We did n"
E14-1007,boas-2002-bilingual,0,0.0222164,"our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006"
E14-1007,korhonen-etal-2006-large,0,0.0632328,"nslation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, student, ...} prep in:{classroom, school, ...} ... observe:3 nsubj:{child, people, ...} dobj:{bird, animal, ...} prep at:{range, time, ...} ... 1 In this paper, we use the dependency relation names of the Stanford collapsed dependencies (de Marneffe et al., 2006) as the notations of case slots. For instance, “nsubj” means a nominal subject, “dobj” means a direct object, “iboj” means an indirect object, “ccomp” means a clausal complement and “prep *”"
E14-1007,A97-1052,0,0.111055,"arsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, student, ...} prep in:{classroom, school, ...} ... observe:3 nsubj:{child, people, ...} dobj:{bird, animal, ...} prep at:{range, time, ...} ... 1 In this paper, we use the dependency relation names of the Stanford collapsed dependencies (de Marneffe et al., 2006) as the notations of case slots. For instance, “nsubj” means a nominal subject, “dobj” means a direct object, “iboj” means an indirect object, “ccomp” means a clausal"
E14-1007,N10-1137,0,0.0714345,"., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying thi"
E14-1007,P10-1046,0,0.106572,"the previous approaches, especially considering that no filtering procedures were applied to the induced frames. We anticipate that the results based on the prototypical induced frames with SUMO attributes would be competitive. Our post-analysis revealed that the entropy can be lowered further if an automatic filtering based on frequencies is applied. 4.5 Evaluation of the Quality of Selectional Preferences We also investigated the quality of selectional preferences within the induced semantic frames. The only publicly available test data for selectional preferences, to our knowledge, is from Chambers and Jurafsky (2010). This data consists of quadruples (verb, relation, word, confounder) and does not contain their context.7 A typical way for using our semantic frames is to select an appropriate frame for an input sentence and judge the eligibility of the word uses against the selected frame. However, due to the lack of context for the above data, it is difficult to select a corresponding semantic frame for a test quadruple and thus the induced semantic frames cannot be naturally applied to this data. To investigate the potential for selectional preferences of the semantic frames, we approximately match a qua"
E14-1007,P11-1112,0,0.121541,"orhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large cor"
E14-1007,de-marneffe-etal-2006-generating,0,0.00430938,"Missing"
E14-1007,D11-1122,0,0.144094,"orhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large cor"
E14-1007,W07-1424,0,0.0310914,"imental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, stude"
E14-1007,P12-1044,0,0.0467125,"information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, student, ...} prep in:{classroom, school, ...} ... observe:3 nsubj:{child, people, ...} dobj:{bird, animal, ...} prep at:{range, time, ...} ... 1 In this paper, we use the dependency relation names of the Stanford collapsed dependencies (de Marneffe et al., 2006) as the notations of case slots. For instance, “nsubj” means a nominal subject, “dobj” means a direct object, “iboj” means an indirect object, “ccomp” means a clausal complement and “prep *” means a preposition. 58"
E14-1007,W13-0117,1,0.919031,"the current number of initial frames assigned to the semantic frame fj . α is a hyper-parameter that determines how likely it is for a new semantic frame to be created. In this equation, the first term is the Dirichlet process prior and the second term is the likelihood of vi . P (vi |fj ) is defined based on the DirichletMultinomial distribution as follows: ∏ P (vi |fj ) = P (w|fj )count(vi ,w) , (2) dobj, ccomp, nsubj, prep ∗, iobj. w∈V This selection of a predominant argument order above is justified by relative comparisons of the discriminative power of the different slots for CPA frames (Popescu, 2013). If a predicate-argument structure does not have any of the above slots, it is discarded. Then, the predicate-argument structures that have the same verb and argument pair (slot and where V is the vocabulary in all case slots cooccurring with the verb. It is distinguished by the case slot, and thus consists of pairs of slots and words, e.g., “nsubj:child” and “dobj:bird.” count(vi , w) is the number of w in the initial frame vi . P (w|fj ) is defined as follows: 3 If a predicate-argument structure has multiple prepositional phrases, one of them is randomly selected. P (w|fj ) = ∑ 61 count(fj"
E14-1007,P13-1085,0,0.309287,"urdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, student, ...} prep in:{classroom, school, ...} ... observe:3 nsubj:{child, people, ...} dobj:{bird, animal, ...} prep at:{range, time, ...} ... 1 In this paper, we use the dependency relation names of the Stanford collapsed dependencies (de Marneffe et al., 2006) as the notations of case slots. For instance, “nsubj” means a nominal subject, “dobj” means a direct object, “iboj” means an indirect object, “ccomp” means a clausal complement and “prep *” means a preposition. 58 Proceedings of the 14th Confer"
E14-1007,D13-1121,1,0.84905,"hen, we extract predicate-argument structures from the dependency parses. Dependents that have the following dependency relations to a verb are extracted as arguments: • Phrasal verbs are also distinguished from non-phrasal verbs. For example, “look up” has independent frames from “look.” • The passive voice of a verb is distinguished from the active voice, and thus these have independent frames. Passive voice is detected using the part-of-speech tag “VBN” (past participle). The alignment between frames of active and passive voices will be done after the induction of frames using the model of Sasano et al. (2013) in the future. nsubj, xsubj, dobj, iobj, ccomp, xcomp, prep ∗ Here, we do not distinguish adjuncts from arguments. All extracted dependents of a verb are handled as arguments. This distinction is left for future work, but this will be performed using slot 2 • “xcomp” (open clausal complement) is renamed to “ccomp” (clausal complement) and “xsubj” (controlling subject) is renamed to “nsubj” (nominal subject). This is because http://nlp.stanford.edu/software/lex-parser.shtml 60 word, e.g., “dobj:effect”) are merged into an initial frame (Figure 1). After this process, we discard minor initial f"
E14-1007,D09-1067,0,0.0615122,"sien and Stevenson are related to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a Dirichlet Process model for clustering usages of the verb “get.” Later, Parisien and Stevenson (2010) proposed a Hierarchical Dirichlet Process model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domi"
E14-1007,P03-1002,0,0.0720399,"verage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen"
E14-1007,N13-1051,0,0.278596,"nto, Italy dk@i.kyoto-u.ac.jp, {Daniel.W.Peterson, Martha.Palmer}@colorado.edu, popescu@fbk.eu Abstract argument patterns of verbs and are purely syntactic, expressions that have the same subcategorization frame can have different meanings (e.g., metaphors). Semantics-oriented NLP applications based on frames, such as paraphrase acquisition and machine translation, require consistency in the meaning of each frame, and thus these subcategorization frames are not suitable for these semantic tasks. Recently, there have been a few studies on automatically acquiring semantic frames (Materna, 2012; Materna, 2013). Materna induced semantic frames (called LDA-Frames) from triples of (subject, verb, object) in the British National Corpus (BNC) based on Latent Dirichlet Allocation (LDA) and the Dirichlet Process. LDAFrames capture limited linguistic phenomena of these triples, and are defined across verbs based on probabilistic topic distributions. This paper presents a method for automatically building verb-specific semantic frames from a large raw corpus. Our semantic frames are verbspecific like PropBank and semantically distinguished. A frame has several syntactic case slots, each of which consists of"
E14-1007,P11-1145,0,0.0615372,"rs in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalabilit"
E14-1007,W12-1901,0,0.252813,"rowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalability would be a big problem. The most closely related work to our semantic frames are LDA-Frames, which are probabilistic semantic frames automatically induced from a raw corpus (Mate"
E14-1007,E12-1003,0,0.10559,"articipate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalability would be a big problem. The"
E14-1007,C04-1100,0,0.040668,"rpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent o"
E14-1007,W09-0210,0,0.0675922,"ct, the models of Parisien and Stevenson are related to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a Dirichlet Process model for clustering usages of the verb “get.” Later, Parisien and Stevenson (2010) proposed a Hierarchical Dirichlet Process model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role"
E14-1007,J05-1004,1,0.186197,"es are verb-specific example-based frames that are distinguished according to their senses. We use the Chinese Restaurant Process to automatically induce these frames from a massive amount of verb instances. In our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing"
E14-1007,H93-1052,0,0.534394,"model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. ("
E14-1007,D09-1001,0,0.0389919,"orhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotat"
E14-1007,W06-1601,0,\N,Missing
E14-1007,P07-1028,0,\N,Missing
E14-1007,C98-1013,0,\N,Missing
E14-1007,P93-1032,0,\N,Missing
E14-1007,P12-1090,0,\N,Missing
E17-1054,I13-1124,1,0.935349,"sed on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2014). There were several studies using additional knowledge to improve syntactic and semantic tasks. McClosky et al. (2006) used an addi569 gold standard data and the automatic parses, correct dependencies were collected as positive examples and incorrect dependencies were collected as negative examples. Then selecting high-quality dependencies was regarded as a binary classification problem. To conduct such binary classification, they employed a set of basic features from Yu et al. (2008). In addition to these basic features, Jin et al. (2013) considered context features that are thought to affect parsing performance. Since the input for high-quality dependency selection method is a dependency tree, tree features are used to identify dependency quality. Also, some dependency parsers output the score of each dependency (i.e., edge confidence value) during the parsing process. They used the real value of the score as an additional feature. We first apply this approach to select high-quality dependencies from automatic parses. tactic and semantic knowledge, i.e., case frames. Word embeddings can also be incorporated into our method bu"
E17-1054,jin-etal-2014-framework,1,0.856177,"n for Japanese, there are also ambiguous case makers that can represent multiple cases. The most common one, for example, is “wa” case in Japanese. This case marker always functions as a topic marker. The argument in “wa” case is normally emphasized as the topic of the sentence. It can be equal to either “ga” case or “wo” case, and sometimes “ni” case. To avoid such ambiguous cases, one can simply discard all the instances in “wa” case to make case frames more precise. For languages that lack such case markers (e.g., English and Chinese), case frames are composed of automatic syntactic roles (Jin et al., 2014). Such syntactic roles include “subject”, “direct object”, “indirect object” and “prepositional phrases”. Such surface cases have limitations on case representation especially for Chinese. Take the following sentences as examples. (1) 苹果 (apples) / 我 (I) / 吃了 (eaten) / 很多 (a lot). (2) 我 (I) / 苹果 (apples) / 吃了 (eaten) / 很多 (a lot). (3) 我 (I) / 吃了 (eaten) / 很多(a lot) / 苹果 (apples). (4) 我 (I) / 吃了 (eaten) / 很多(a lot). 2 https://code.google.com/archive/p/ word2vec/source/default/source 571 feature description Freq the co-occurrence frequency of a predicate-argument pair without considering the syn"
E17-1054,W09-1206,0,0.0917277,"Missing"
E17-1054,W15-3119,1,0.282594,"of the human-annotated data is used to apply SRL using the base model. From these results, we acquire training data for semantic role selection by collecting each semantic role. We then judge the correctness of each semantic role according to the gold standard annotations. All correct semantic roles are used as positive examples and the incorrect ones are used as negative examples for semantic role selection. Judging whether an automatic semantic role is reliable can be regarded as a binary classification problem. We use SVMs to solve this problem. We use the feature set for SRL described in Jin et al. (2015) as basic features. It contains predicate features that are extracted from the target predicate; argument features which are extracted from each candidate argument. We also use surface case frames, which have a positive effect on SRL, as additional knowledge. 6.3 Using high-quality deep case frames for SRL Syntactic information such as dependencies is essential for SRL. In Section 4, we used surface 573 feature description SRFreq the co-occurrence frequency of a predicate-argument pair without considering the semantic role of the argument SRPmi the PMI value for each predicate-argument pair wi"
E17-1054,W10-0907,0,0.0729771,"Missing"
E17-1054,N06-1023,1,0.860357,"apers, pages 568–577, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics tional unlabeled corpus to reduce data sparsity. In syntactic level of NLP, rich knowledge, such as predicate-argument structures and case frames, is strong backups for various kinds of tasks. Case frames, which clarify relations between a predicate and its arguments, can support tasks ranging from fundamental analysis, such as dependency parsing and word similarity calculation, to multilingual applications, such as machine translation. Japanese case frames have been successfully compiled (Kawahara and Kurohashi, 2006), where each case slot is represented as its case marker in Japanese such as ‘ga’, ‘wo’, and ‘ni’. For the case frames of other languages such as English and Chinese, because there are no such case markers that can help clarify syntactic structures, instead of using case markers like in Japanese, syntactic surface cases (i.e., subject, object, prepositional phrase, etc.) are used for argument representation (Jin et al., 2014). Case frames can be automatically acquired using a different method such as Chinese Restaurant Process (CRP) (Kawahara et al., 2014) for different languages. In our work,"
E17-1054,E14-1007,1,0.844973,"bject, object, etc.) For each predicate, all the PASs are clustered into different case frames to reflect different semantic usages. We show an example of case frames for the verb ‘谢’ in Table 1, which has multiple meanings. ‘谢(1)’ is the case frame used to represent the sense of ‘withering of flower’. Similarly, the sense of ‘谢’ which means ‘to thank’ is represented by case frame ‘谢(2)’. ‘谢(3)’ is the case frame for the sense of ‘curtain call’. In other words, case frames are knowledge that solves word sense disambiguation (WSD) by clustering the PASs. We applied the CRP method described by (Kawahara et al., 2014) for clustering the high-quality PASs to compile high-quality case frames. 4 Applying high-quality surface case frames to SRL 4.1 High-quality dependency selection Dependency parsing has been widely employed for knowledge acquisition related to predicateargument structures. The dependency parsing performance determines the quality of acquired knowledge, regardless of target languages. Reducing dependency parsing errors and selecting highquality dependencies are of primary importance. Jin et al. (2013) used a single set of dependency labeled corpus (a treebank), a part of which was used to trai"
E17-1054,D09-1003,0,0.0700095,"Missing"
E17-1054,P08-1068,0,0.098446,"Missing"
E17-1054,Y01-1001,0,0.222646,"Missing"
E17-1054,C10-1081,0,0.0714146,"Missing"
E17-1054,E09-1026,0,0.0722539,"Missing"
E17-1054,P06-1043,0,0.0308194,"of framework has the ability to use SRL information in syntactic parsing for improvement, but needs a much larger search space for decoding. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2014). There were several studies using additional knowledge to improve syntactic and semantic tasks. McClosky et al. (2006) used an addi569 gold standard data and the automatic parses, correct dependencies were collected as positive examples and incorrect dependencies were collected as negative examples. Then selecting high-quality dependencies was regarded as a binary classification problem. To conduct such binary classification, they employed a set of basic features from Yu et al. (2008). In addition to these basic features, Jin et al. (2013) considered context features that are thought to affect parsing performance. Since the input for high-quality dependency selection method is a dependency tree, tree features"
E17-1054,W09-1203,0,0.042817,"Missing"
E17-1054,W08-1810,0,0.0664419,"Missing"
E17-1054,W09-1217,0,0.0302214,"presentation can be learned from unlabeled data (Bengio et al., 2003). Deschacht and Moens (2009) used word similarity learned from unlabeled data as additional features for SRL. Word embeddings have also been used in several NLP tasks including SRL (Collobert et al., 2011). Instead of using word-level lexical knowledge, our work uses synRelated work The CoNLL-2009 shared task (Hajiˇc et al., 2009) features a substantial number of studies on SRL that used Propbank as one of the resources. The participating systems can be categorized into two types: joint learning of syntactic parsing and SRL (Tang et al., 2009; Morante et al., 2009), which learns a unique model for syntactic parsing and SRL jointly. This type of framework has the ability to use SRL information in syntactic parsing for improvement, but needs a much larger search space for decoding. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2"
E17-1054,D14-1041,0,0.0174453,"ang et al., 2009; Morante et al., 2009), which learns a unique model for syntactic parsing and SRL jointly. This type of framework has the ability to use SRL information in syntactic parsing for improvement, but needs a much larger search space for decoding. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2014). There were several studies using additional knowledge to improve syntactic and semantic tasks. McClosky et al. (2006) used an addi569 gold standard data and the automatic parses, correct dependencies were collected as positive examples and incorrect dependencies were collected as negative examples. Then selecting high-quality dependencies was regarded as a binary classification problem. To conduct such binary classification, they employed a set of basic features from Yu et al. (2008). In addition to these basic features, Jin et al. (2013) considered context features that are thought to affec"
E17-1054,P09-2019,0,0.0822879,"Missing"
E17-1054,W09-1208,0,0.0338018,"evel lexical knowledge, our work uses synRelated work The CoNLL-2009 shared task (Hajiˇc et al., 2009) features a substantial number of studies on SRL that used Propbank as one of the resources. The participating systems can be categorized into two types: joint learning of syntactic parsing and SRL (Tang et al., 2009; Morante et al., 2009), which learns a unique model for syntactic parsing and SRL jointly. This type of framework has the ability to use SRL information in syntactic parsing for improvement, but needs a much larger search space for decoding. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2014). There were several studies using additional knowledge to improve syntactic and semantic tasks. McClosky et al. (2006) used an addi569 gold standard data and the automatic parses, correct dependencies were collected as positive examples and incorrect dependencies were collected as negative exa"
H01-1043,A97-1052,0,0.0695649,"Missing"
H01-1043,J94-4001,1,0.832916,"tsumu ‘accumulate experience’. Since these couples multiply to millions of combinations, it is diﬃcult to make a wide-coverage case frame dictionary from a small corpus like an analyzed corpus. We, however, use a raw corpus, so that this problem can be addressed. The clustering process is to merge examples which does not have diﬀerent usages but belong to diﬀerent case frames because of diﬀerent closest case components. 2. VARIOUS METHODS FOR CASE FRAME CONSTRUCTION We employ the following procedure of case frame construction from raw corpus (Figure 1): 1. A large raw corpus is parsed by KNP [5], and reliable modiﬁer-head relations are extracted from the parse results. We call these modiﬁer-head relations examples. 2. The extracted examples are distinguished by the verb and its closest case component. We call these data example patterns. 3. The example patterns are clustered based on a thesaurus. We call the output of this process example case frames, which is the ﬁnal result of the system. We call words which compose case components case examples, and a group of case examples case example group. In Figure 1, nimotsu ‘baggage’, busshi 1 In English, several unsupervised methods have b"
H01-1043,W97-0123,0,0.0794265,". However, information quantity of this is equivalent to that of the co-occurrences (II of Figure 1), so verb sense ambiguity becomes a problem as well. We distinguish examples by the verb and its closest case component. Our method can address the two problems above: verb sense ambiguity and sparse data. On the other hand, semantic markers can be used as case components instead of case examples. These we call semantic case frames (IV of Figure 1). Constructing semantic case frames by hand leads to the problem mentioned in Section 1. Utsuro et al. constructed semantic case frames from a corpus [8]. There are three main diﬀerences to our approach: they use an annotated corpus, depend deeply on a thesaurus, and did not resolve verb sense ambiguity. 3. COLLECTING EXAMPLES This section explains how to collect examples shown in Figure 1. In order to improve the quality of collected examples, reliable modiﬁer-head relations are extracted from the parsed corpus. 3.1 Conditions of case components When examples are collected, case markers, case examples, and case components must satisfy the following conditions. Conditions of case markers Case components which have the following case markers (C"
H01-1043,P93-1032,0,\N,Missing
I05-1017,A00-2018,0,0.0162257,". The prepositional phrase in (1a) “with a fork” modiﬁes the verb “ate”, because “with a fork” describes how the salad is eaten. The prepositional phrase in (1b) “with croutons” modiﬁes the noun “the salad”, because “with croutons” describes the salad. To disambiguate such PP-attachment ambiguity, some kind of world knowledge is required. However, it is currently diﬃcult to give such world knowledge to computers, and this situation makes PP-attachment disambiguation diﬃcult. Recent state-of-the-art parsers perform with the practical accuracy, but seem to suﬀer from the PP-attachment ambiguity [2, 3]. For NLP tasks including PP-attachment disambiguation, corpus-based approaches have been the dominant paradigm in recent years. They can be divided into two classes: supervised and unsupervised. Supervised methods automatically learn rules from tagged data, and achieve good performance for many NLP tasks, especially when lexical information, such as words, is given. Such methods, however, cannot avoid the sparse data problem. This is because tagged data are not suﬃcient enough to discriminate a large variety of lexical information. To deal with this problem, many smoothing techniques have bee"
I05-1017,P98-2177,0,0.624239,"Missing"
I05-1017,P00-1014,0,0.724913,"Missing"
I05-1017,H94-1048,0,0.544766,"Missing"
I05-1017,C94-2195,0,0.0573136,"Missing"
I05-1017,W95-0103,0,0.163443,"Missing"
I05-1017,W97-0109,0,0.133315,"Missing"
I05-1017,W97-1016,0,0.0565704,"Missing"
I05-1017,W99-0606,0,0.215158,"Missing"
I05-1017,J93-1005,0,0.397374,"Missing"
I05-1017,C02-1004,0,0.203169,"Missing"
I05-1017,J95-4004,0,0.278935,"Missing"
I05-1017,N01-1025,0,0.0702764,"Missing"
I05-1017,J93-2004,0,\N,Missing
I05-1017,J03-4003,0,\N,Missing
I05-1017,C98-2172,0,\N,Missing
I05-1060,C04-1176,0,0.0660833,". There are several related work which can contribute the modification and extension of our methods. When using a Japanese-English dictionary, if we understand the translation is transliteration, we can utilize the information more effectively, handling inflections. In this sense, work by Knight and Graehl can be incorporated into our method [2]. 692 T. Nakazawa, D. Kawahara, and S. Kurohashi In order to handle spelling variation problems, there have been many methods proposed [3], and we can utilize recently proposed robust treatment of Japanese Katakana spelling variation by Masuyama et al. [5]. Our second method using Japanese-English dictionary and the English corpus can be considered as a translation acquisition method. It is interesting to compare these results with other web-based methods, such as Utsuro et al. [8, 1]. There have been many studies that extract compound nouns. Nakagawa et al. focused on the tendency that most of technical terms are compound nouns, and proposed a method of extracting technical terms by using frequency and variety of its neiboring words [10, 7]. In view of information retrieval, Yamada et al. aimed at imporving information retrieval using matching"
I05-1060,C04-1149,0,0.0204093,"e effectively, handling inflections. In this sense, work by Knight and Graehl can be incorporated into our method [2]. 692 T. Nakazawa, D. Kawahara, and S. Kurohashi In order to handle spelling variation problems, there have been many methods proposed [3], and we can utilize recently proposed robust treatment of Japanese Katakana spelling variation by Masuyama et al. [5]. Our second method using Japanese-English dictionary and the English corpus can be considered as a translation acquisition method. It is interesting to compare these results with other web-based methods, such as Utsuro et al. [8, 1]. There have been many studies that extract compound nouns. Nakagawa et al. focused on the tendency that most of technical terms are compound nouns, and proposed a method of extracting technical terms by using frequency and variety of its neiboring words [10, 7]. In view of information retrieval, Yamada et al. aimed at imporving information retrieval using matching of compounds [9]. It is similar to our study in handling compounds. 8 Conclusion This paper proposed an automatic segmentation method of Japanese Katakana compounds, which makes it possible to construct precise and concise Katakana"
I05-1060,J98-4003,0,\N,Missing
I08-1012,A00-1031,0,0.0519108,"2 in our experiments. We used the same rules for conversion and created the same data split as (Wang et al., 2007): ﬁles 1-270 and 400-931 as training, 271-300 as testing and ﬁles 301-325 as development. We used the gold standard segmentation and POS tags in the CTB. For unlabeled data, we used the PFR corpus 3 . It includes the documents from People’s Daily at 1998 (12 months). There are about 290 thousand sentences and 15 million words in the PFR corpus. To simplify, we used its segmentation. And we discarded the POS tags because PFR and CTB used different POS sets. We used the package TNT (Brants, 2000), a very efﬁcient statistical part-of-speech tagger, to train a POS tagger4 on training data of the CTB. We measured the quality of the parser by the unlabeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD. We reported two types of scores: “UAS without p” is the UAS score without all punctuation tokens and “UAS with p” is the one with all punctuation tokens. 4.1 Experimental results In the experiments, we trained the parsers on training data and tuned the parameters on development data. In the following sessions, “baseline” refers to Basic Parser (the model with basi"
I08-1012,D07-1097,0,0.013766,"case frames. And we represent additional information as the features for learning models while they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input string): • Left-Arc(LA): Add an arc f"
I08-1012,N06-1023,1,0.793343,"successful instance of parsing with self-training by using a re-ranker. As Figure 1 suggests, the dependency parser performs bad for parsing the words with long distances. In our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser. (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. They obtain consistent improvements by penalizing dependencies between two words that are farther apart in the string. The study most relevant to ours is done by (Kawahara and Kurohashi, 2006). They present an integrated probabilistic model for Japanese parsing. They also use partial information after current parser parses the sentences. Our work differs in that we consider general dependency relations while they only consider case frames. And we represent additional information as the features for learning models while they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature sp"
I08-1012,P06-1043,0,0.0775317,"mance. Our study is relative to incorporating unlabeled data into a model for parsing. There are several other studies relevant to ours as described below. A simple method is self-training in which the existing model ﬁrst labels unlabeled data and then the newly labeled data is then treated as hand annotated data for training a new model. But it seems that selftraining is not so effective. (Steedman et al., 2003) reports minor improvement by using self-training for syntactic parsing on small labeled data. The reason may be that errors in the original model would be ampliﬁed in the new model. (McClosky et al., 2006) presents a successful instance of parsing with self-training by using a re-ranker. As Figure 1 suggests, the dependency parser performs bad for parsing the words with long distances. In our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser. (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. They obtain consistent improvements by penalizing dependencies between two words that are farther apart in the string. The study most relevant to ours"
I08-1012,D07-1013,0,0.091155,"Missing"
I08-1012,E06-1011,0,0.0577863,"Missing"
I08-1012,W06-2932,0,0.0408924,"onent for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input string): • Left-Arc(LA): Add an arc from NEXT to TOP; pop the stack. • Right-Arc(RA): Add an arc from TOP to NEXT; push NEXT onto the stack. • Reduce(RE): Pop the stack. •"
I08-1012,W06-2933,0,0.0192393,"elations while they only consider case frames. And we represent additional information as the features for learning models while they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input stri"
I08-1012,W03-3017,0,0.0749943,"t parser parses the sentences. Our work differs in that we consider general dependency relations while they only consider case frames. And we represent additional information as the features for learning models while they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (w"
I08-1012,P06-1072,0,0.0195353,"model. But it seems that selftraining is not so effective. (Steedman et al., 2003) reports minor improvement by using self-training for syntactic parsing on small labeled data. The reason may be that errors in the original model would be ampliﬁed in the new model. (McClosky et al., 2006) presents a successful instance of parsing with self-training by using a re-ranker. As Figure 1 suggests, the dependency parser performs bad for parsing the words with long distances. In our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser. (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. They obtain consistent improvements by penalizing dependencies between two words that are farther apart in the string. The study most relevant to ours is done by (Kawahara and Kurohashi, 2006). They present an integrated probabilistic model for Japanese parsing. They also use partial information after current parser parses the sentences. Our work differs in that we consider general dependency relations while they only consider case frames. And we represent additional information as"
I08-1012,E03-1008,0,0.0855318,"ta for dependency parsing. We use a parser to parse the sentences in unlabeled data. Then another parser makes use of the information on short dependency relations in the newly parsed data to improve performance. Our study is relative to incorporating unlabeled data into a model for parsing. There are several other studies relevant to ours as described below. A simple method is self-training in which the existing model ﬁrst labels unlabeled data and then the newly labeled data is then treated as hand annotated data for training a new model. But it seems that selftraining is not so effective. (Steedman et al., 2003) reports minor improvement by using self-training for syntactic parsing on small labeled data. The reason may be that errors in the original model would be ampliﬁed in the new model. (McClosky et al., 2006) presents a successful instance of parsing with self-training by using a re-ranker. As Figure 1 suggests, the dependency parser performs bad for parsing the words with long distances. In our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser. (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependen"
I08-1012,W05-1516,0,0.0438281,"Missing"
I08-1012,P06-1054,0,0.0376974,"Missing"
I08-1012,N07-3002,0,0.0308211,"Missing"
I08-1012,W03-3023,0,0.171228,"le they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input string): • Left-Arc(LA): Add an arc from NEXT to TOP; pop the stack. • Right-Arc(RA): Add an arc from TOP to NEXT; push NEXT onto the stac"
I08-1012,D07-1096,0,\N,Missing
I08-1025,kawahara-kurohashi-2006-case,1,0.542577,"Missing"
I08-1025,J94-4001,1,0.406601,"example of a web standard format data is shown in Figure 2. Extracted sentences are enclosed by &lt;RawString> tags, and the analyzed results of the sentences are enclosed by &lt;Annotation> tags. Sentences in a web page and their analyzed results can be obtained by looking at these tags in the standard format data corresponding to the page. 2.2 Construction of Web Standard Format Data Collection We have crawled 218 million web pages over three months, May - July in 2007, by using the ShimCrawler,2 and then converted these pages into web standard format data with results of a Japanese parser, KNP (Kurohashi and Nagao, 1994), through our conversion tools. Note that this web page collec2 http://www.logos.t.u-tokyo.ac.jp/crawler/ &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?> &lt;StandardFormat Url=&quot;http://www.kantei.go.jp/jp/koizumiprofile/1_sin nen.html&quot; OriginalEncoding=&quot;Shift_JIS&quot; Time=&quot;2006-08 -14 19:48:51&quot;>&lt;Text Type=&quot;default&quot;> &lt;S Id=&quot;1&quot; Length=&quot;70&quot; Offset=&quot;525&quot;> &lt;RawString>小泉総理の好きな格言のひとつに「無信不立 (信無く ば立たず)」があります。&lt;/RawString> &lt;Annotation Scheme=&quot;KNP&quot;> &lt;![CDATA[* 1D &lt;文 頭>&lt;サ 変>&lt;人 名>&lt;助 詞>&lt;連 体 修 飾>&lt;体 言>&lt;係:ノ格>&lt;区切:0-4>&lt;RID:1056> 小泉 こいずみ 小泉 名詞 6 人名 5 * 0 * 0 NIL &lt;文頭>&lt;漢字>&lt; かな漢字>&lt;名詞相当語>&lt;自立>&lt;タグ単位始>&lt;文節始>&lt;固有キー> ... ま す ま す ま す 接 尾 辞"
I08-2097,P05-1001,0,0.0205575,"machine in spite of the time complexity of O(n3 ). If greater efﬁciency is required, it is possible to apply a pre-ﬁlter that removes long sentences (e.g., longer than 30 words), which are seldom selected by the reliability detector. In addition, our method does not depend on a particular parser, and can be applied to other state-of-theart parsers, such as Malt Parser (Nivre et al., 2006), which is a feature-rich linear-time parser. In general, it is very difﬁcult to improve the accuracy of the best performing systems by using unlabeled data. There are only a few successful studies, such as (Ando and Zhang, 2005) for chunking and (McClosky et al., 2006a; McClosky et al., 2006b) on constituency parsing. We succeeded in boosting the accuracy of the second-order MST parser, which is 713 a state-of-the-art dependency parser, in the CoNLL 2007 domain adaptation task. This was a difﬁcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data (Dredze et al., 2007). The key factor in our success was the extraction of only reliable information from unlabeled data. However, that improvement was not satisfactory. In order to achieve more gains, it is necessary to exp"
I08-2097,J94-4001,0,0.034451,"Missing"
I08-2097,N06-1020,0,0.118392,"of O(n3 ). If greater efﬁciency is required, it is possible to apply a pre-ﬁlter that removes long sentences (e.g., longer than 30 words), which are seldom selected by the reliability detector. In addition, our method does not depend on a particular parser, and can be applied to other state-of-theart parsers, such as Malt Parser (Nivre et al., 2006), which is a feature-rich linear-time parser. In general, it is very difﬁcult to improve the accuracy of the best performing systems by using unlabeled data. There are only a few successful studies, such as (Ando and Zhang, 2005) for chunking and (McClosky et al., 2006a; McClosky et al., 2006b) on constituency parsing. We succeeded in boosting the accuracy of the second-order MST parser, which is 713 a state-of-the-art dependency parser, in the CoNLL 2007 domain adaptation task. This was a difﬁcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data (Dredze et al., 2007). The key factor in our success was the extraction of only reliable information from unlabeled data. However, that improvement was not satisfactory. In order to achieve more gains, it is necessary to exploit a much larger number of unlabeled d"
I08-2097,P06-1043,0,0.0739276,"of O(n3 ). If greater efﬁciency is required, it is possible to apply a pre-ﬁlter that removes long sentences (e.g., longer than 30 words), which are seldom selected by the reliability detector. In addition, our method does not depend on a particular parser, and can be applied to other state-of-theart parsers, such as Malt Parser (Nivre et al., 2006), which is a feature-rich linear-time parser. In general, it is very difﬁcult to improve the accuracy of the best performing systems by using unlabeled data. There are only a few successful studies, such as (Ando and Zhang, 2005) for chunking and (McClosky et al., 2006a; McClosky et al., 2006b) on constituency parsing. We succeeded in boosting the accuracy of the second-order MST parser, which is 713 a state-of-the-art dependency parser, in the CoNLL 2007 domain adaptation task. This was a difﬁcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data (Dredze et al., 2007). The key factor in our success was the extraction of only reliable information from unlabeled data. However, that improvement was not satisfactory. In order to achieve more gains, it is necessary to exploit a much larger number of unlabeled d"
I08-2097,D07-1013,0,0.0883378,"Missing"
I08-2097,W06-2932,0,0.0182422,"for them. We prepared the following three labeled data sets to train the base dependency parser and the reliability detector. PTB base train: training set for the base parser: 14,862 sentences PTB rel train: training set for reliability detector: 2,500 sentences4 BIO rel dev: development set for reliability detector: 200 sentences (= labeled BIO data) PTB base train is used to train the base dependency parser, and PTB rel train is used to train our reliability detector. BIO rel dev is used for tuning the parameters of the reliability detector. 4.1 Base Dependency Parser We used the MSTParser (McDonald et al., 2006), which achieved top results in the CoNLL 2006 (CoNLL-X) shared task, as a base dependency parser. To enable second-order features, the parameter order was set to 2. The other parameters were set to default. We used PTB base train (14,862 sentences) to train this parser. 4.2 Algorithm to Detect Reliable Parses We built a binary classiﬁer for detecting reliable sentences from a set of automatic parses produced by 4 1,215 labeled PTB sentences are left as another development set for the reliability detector, but they are not used in this paper. the base dependency parser. We used support vector"
I08-2097,1991.mtsummit-papers.9,0,0.0246341,", does not deal with domain adaptation of a tagger but focuses on that of a parser. 4 Learning Reliability of Parses Our approach assesses automatic parses of a single parser in order to select only reliable parses from them. We compare automatic parses and their goldstandard ones, and regard accurate parses as positive examples and the remainder as negative examples. Based on these examples, we build a binary classiﬁer that classiﬁes each sentence as reliable or not. To precisely detect reliable parses, we make use of several linguistic features inspired by the notion of controlled language (Mitamura et al., 1991). That is to say, the reliability of parses is judged based on the degree of sentence difﬁculty. Before describing our base dependency parser and the algorithm for detecting reliable parses, we ﬁrst explain the data sets used for them. We prepared the following three labeled data sets to train the base dependency parser and the reliability detector. PTB base train: training set for the base parser: 14,862 sentences PTB rel train: training set for reliability detector: 2,500 sentences4 BIO rel dev: development set for reliability detector: 200 sentences (= labeled BIO data) PTB base train is us"
I08-2097,P02-1063,0,0.0680386,"Missing"
I08-2097,D07-1119,0,0.0265757,"Missing"
I08-2097,W96-0213,0,0.0379723,"Missing"
I08-2097,P07-1052,0,0.0982428,"Missing"
I08-2097,P05-1022,0,0.171,"Missing"
I08-2097,P07-1078,0,0.185469,"Missing"
I08-2097,J05-1003,0,0.0434159,"Missing"
I08-2097,P07-1033,0,0.0327808,"Missing"
I08-2097,D07-1097,0,0.0282794,"Missing"
I08-2097,D07-1111,0,0.296028,"le parser, because speed and efﬁciency are important when processing a massive volume of text. The resulting highly reliable parses would be useful to automatically construct dictionaries and knowledge bases, such as case frames (Kawahara and Kurohashi, 2006). Furthermore, we incorporate the reliable parses we obtained into the dependency parser to achieve domain adaptation. The CoNLL 2007 shared task tackled domain adaptation of dependency parsers for the ﬁrst time (Nivre et al., 2007). Sagae and Tsujii applied an ensemble method to the domain adaptation track and achieved the highest score (Sagae and Tsujii, 2007). They ﬁrst parsed in-domain unlabeled sentences using two parsers trained on out-of-domain labeled data. Then, they extracted identical parses that were produced by the two parsers and added them to the original (out-of-domain) training set to train a domain-adapted model. Dredze et al. yielded the second highest score1 in the domain adaptation track (Dredze et al., 2007). However, their results were obtained without adaptation. They concluded that it is very difﬁcult to substantially improve the target domain performance over that of a state-of-the-art parser. To conﬁrm this, we parsed the t"
I08-2097,W07-2201,0,0.0274237,"Missing"
I08-2097,N07-1049,0,\N,Missing
I08-2097,W06-2933,0,\N,Missing
I08-2097,N06-2033,0,\N,Missing
I08-2097,D07-1112,0,\N,Missing
I08-2097,D07-1096,0,\N,Missing
I11-1051,W02-2016,0,0.602073,"Missing"
I11-1051,J94-4001,1,0.570926,"mpirically proven to be effective for coordination disambiguation. However, a unified approach that combines both clues has not been explored comprehensively. In this paper, we propose a unified framework for coordi• selectional preferences. Syntactic, lexical and semantic parallelism of conjuncts is frequently observed in coordinate structures. For example, Dubey et al. (2005) empirically confirmed syntactic parallelism in coordinate structures. This clue was modeled by string matching, part-of-speech matching, number agreement, semantic similarities, and so forth (Agarwal and Boggess, 1992; Kurohashi and Nagao, 1994; Resnik, 1999; Chantree et al., 2005; 456 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 456–464, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP factors parallelism. Several other studies have considered parallelism in parsing models. Charniak and Johnson (2005) incorporated some features of syntactic parallelism in coordinate structures into their MaxEnt reranking parser. K¨ubler et al. (2009) used a reranking parser with automatically detected scope possibilities to improve German parsing. As for a generative parser, Dubey et al. (2006"
I11-1051,C08-1012,0,0.0266675,"Missing"
I11-1051,P98-2127,0,0.0196256,"ant role in sentence meanings. For instance, examples are distinguished not by verbs (e.g., yaku (bake/broil/have difficulty)), but by pairs (e.g., pan-wo yaku (bake bread), nikuwo yaku (broil meat), and te-wo yaku (have difficulty)). Predicate-argument examples are aggregated in this way, and yield basic case frames. Thereafter, the basic case frames are clustered to merge similar case frames. For example, since pan-wo yaku (bake bread) and niku-wo yaku (broil meat) are similar, they are clustered. The similarity is measured by using a distributional thesaurus based on the study described in Lin (1998). By using this gradual procedure, we constructed case frames from a web corpus. The case frames were obtained from approximately 1.6 billion sentences extracted from the web. They consisted of 43,000 predicates, and the average number of case frames for a verb was 22.2. In Table 1, some examples of the resulting case frames of the verb yaku are listed. 4.3 Our Model We employ the probabilistic generative model of dependency and case structure analysis (Kawahara and Kurohashi, 2008) as a base model. This base model resolves coordination ambiguities only on the basis of selectional preferences"
I11-1051,P05-1022,0,0.0605805,"rved in coordinate structures. For example, Dubey et al. (2005) empirically confirmed syntactic parallelism in coordinate structures. This clue was modeled by string matching, part-of-speech matching, number agreement, semantic similarities, and so forth (Agarwal and Boggess, 1992; Kurohashi and Nagao, 1994; Resnik, 1999; Chantree et al., 2005; 456 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 456–464, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP factors parallelism. Several other studies have considered parallelism in parsing models. Charniak and Johnson (2005) incorporated some features of syntactic parallelism in coordinate structures into their MaxEnt reranking parser. K¨ubler et al. (2009) used a reranking parser with automatically detected scope possibilities to improve German parsing. As for a generative parser, Dubey et al. (2006) proposed an unlexicalized PCFG parser that modified PCFG probabilities to condition the existence of a coordinate structure. Hogan (2007) proposed a generative lexicalized parser that considered the symmetry of part-of-speech tags and phrase categories of conjuncts, which is more shallow information than our synchro"
I11-1051,H05-1105,0,0.0219366,"ectively. These types of parallelism contribute to identifying the coordinate structure that conjoins Caesar salad and Italian pasta. The other clue is selectional preferences, such as eat in the above example. Since eat is likely to have salad and pasta as its objects, it is plausible that salad and pasta are coordinated. Such selectional preferences of predicates are thought to support the construction of coordinate structures, and were used in Japanese dependency parsing by Kawahara and Kurohashi (2008). Selectional preferences of nouns (noun-noun modifications) were used by Resnik (1999), Nakov and Hearst (2005) and Kawahara and Kurohashi (2008). For example, let us see the following examples: 1 Introduction Coordinate structures are a potential source of syntactic ambiguity in natural language. Although many methods have been proposed to resolve the ambiguities of coordinate structures, coordination disambiguation still remains a difficult problem for state-of-the-art parsers. Previous studies on coordination disambiguation used two kinds of clues: (2) a. mail and securities fraud b. corn and peanut butter • parallelism of conjuncts, and In (2a), the coordination of mail and securities is guided by"
I11-1051,H05-1104,0,0.0319942,"ided by the estimation that mail fraud is a salient compound nominal phrase. In (2b), on the contrary, the coordinate structure that conjoins corn and peanut butter is led because corn butter is not a familiar concept. Each clue has been empirically proven to be effective for coordination disambiguation. However, a unified approach that combines both clues has not been explored comprehensively. In this paper, we propose a unified framework for coordi• selectional preferences. Syntactic, lexical and semantic parallelism of conjuncts is frequently observed in coordinate structures. For example, Dubey et al. (2005) empirically confirmed syntactic parallelism in coordinate structures. This clue was modeled by string matching, part-of-speech matching, number agreement, semantic similarities, and so forth (Agarwal and Boggess, 1992; Kurohashi and Nagao, 1994; Resnik, 1999; Chantree et al., 2005; 456 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 456–464, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP factors parallelism. Several other studies have considered parallelism in parsing models. Charniak and Johnson (2005) incorporated some features of synta"
I11-1051,C04-1002,0,0.0414497,"Missing"
I11-1051,D07-1064,0,0.0158621,"e categories of conjuncts, which is more shallow information than our synchronization model. She also used cooccurrence statistics of conjunct heads, which are similar to our modeling of lexical parallelism, but her model did not use selectional preferences. Kurohashi and Nagao (1994) proposed a rulebased method of Japanese dependency parsing that included coordination disambiguation. Their method first detects coordinate structures in a sentence using dynamic programming, and then determines the dependency structure of the sentence under the constraints of the detected coordinate structures. Shimbo and Hara (2007) and Hara et al. (2009) considered many features for coordination disambiguation and automatically optimized their weights, which were heuristically determined in Kurohashi and Nagao (1994), by using a discriminative learning model. nation disambiguation by incorporating both the clues into a generative parser. To capture syntactic parallelism of conjuncts, we formulate the generative process of pre-modifiers of conjuncts in a synchronized manner. In the above example, the generation process of Caesar from salad is synchronized with that of Italian from pasta. An interpretation of an unbalance"
I11-1051,P06-1053,0,0.0219553,"hi and Nagao, 1994; Resnik, 1999; Chantree et al., 2005; 456 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 456–464, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP factors parallelism. Several other studies have considered parallelism in parsing models. Charniak and Johnson (2005) incorporated some features of syntactic parallelism in coordinate structures into their MaxEnt reranking parser. K¨ubler et al. (2009) used a reranking parser with automatically detected scope possibilities to improve German parsing. As for a generative parser, Dubey et al. (2006) proposed an unlexicalized PCFG parser that modified PCFG probabilities to condition the existence of a coordinate structure. Hogan (2007) proposed a generative lexicalized parser that considered the symmetry of part-of-speech tags and phrase categories of conjuncts, which is more shallow information than our synchronization model. She also used cooccurrence statistics of conjunct heads, which are similar to our modeling of lexical parallelism, but her model did not use selectional preferences. Kurohashi and Nagao (1994) proposed a rulebased method of Japanese dependency parsing that included"
I11-1051,W07-2201,0,0.208556,"Missing"
I11-1051,C96-1058,0,0.184382,"sents the situation of the same case slot of the pre-conjunct. In practice, to avoid the data sparseness problem, we interpolate this probability, which is conditioned on case frames, with the probability conditioned on predicates in the same manner as in Collins (1999). This probability is estimated on the basis of the cooccurrence data of coordinated nouns described in section 4.2.3. 4.4 Practical Issue The proposed model considers all the possible dependency structures including coordination ambiguities. To reduce this high computational cost, we introduced the CKY framework to the search (Eisner, 1996). 4.3.2 Generative Probability of Argument Nouns In the base model, the generative probability of argument nouns in a clause is defined as the product of the generative probability of an argument noun P njk : ∏ ∏ (5) njk ∈N sj P njk , sj :A(sj )=Y 5 Experiments 5.1 Experimental Settings where N sj is a set of nouns including a noun filled in the case slot sj and its coordinated nouns. The generative probability of an argument noun is given as follows: P njk = P (njk |CF l , sj ). # of sents. 1,000 1,000 759 (6) In our model, the direct argument noun filled in the case slot sj is generated with"
I11-1051,P09-1109,0,0.0155,"which is more shallow information than our synchronization model. She also used cooccurrence statistics of conjunct heads, which are similar to our modeling of lexical parallelism, but her model did not use selectional preferences. Kurohashi and Nagao (1994) proposed a rulebased method of Japanese dependency parsing that included coordination disambiguation. Their method first detects coordinate structures in a sentence using dynamic programming, and then determines the dependency structure of the sentence under the constraints of the detected coordinate structures. Shimbo and Hara (2007) and Hara et al. (2009) considered many features for coordination disambiguation and automatically optimized their weights, which were heuristically determined in Kurohashi and Nagao (1994), by using a discriminative learning model. nation disambiguation by incorporating both the clues into a generative parser. To capture syntactic parallelism of conjuncts, we formulate the generative process of pre-modifiers of conjuncts in a synchronized manner. In the above example, the generation process of Caesar from salad is synchronized with that of Italian from pasta. An interpretation of an unbalanced coordinate structure"
I11-1051,P07-1086,0,0.024373,", pages 456–464, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP factors parallelism. Several other studies have considered parallelism in parsing models. Charniak and Johnson (2005) incorporated some features of syntactic parallelism in coordinate structures into their MaxEnt reranking parser. K¨ubler et al. (2009) used a reranking parser with automatically detected scope possibilities to improve German parsing. As for a generative parser, Dubey et al. (2006) proposed an unlexicalized PCFG parser that modified PCFG probabilities to condition the existence of a coordinate structure. Hogan (2007) proposed a generative lexicalized parser that considered the symmetry of part-of-speech tags and phrase categories of conjuncts, which is more shallow information than our synchronization model. She also used cooccurrence statistics of conjunct heads, which are similar to our modeling of lexical parallelism, but her model did not use selectional preferences. Kurohashi and Nagao (1994) proposed a rulebased method of Japanese dependency parsing that included coordination disambiguation. Their method first detects coordinate structures in a sentence using dynamic programming, and then determines"
I11-1051,kawahara-kurohashi-2006-case,1,0.810006,"ga (NOM), wo (ACC), ni (DAT) and de (LOC). Example words are expressed only in English due to space limitation. The number following each word denotes its frequency. In (b), P (A(GEN=N)|tibet, Ac (GEN)=Y) means that nothing is generated from tibet, whereas the head of the pre-conjunct has a genitive case. This probability has a small value because of non-synchronization (unbalanced coordinate structure). pendency parser, KNP,4 which is also used as a base model in the following sections. 4.2.1 Automatically Constructed Case Frames 4.2 Resources We employ automatically constructed case frames (Kawahara and Kurohashi, 2006). This section outlines the method for constructing the case frames. A large corpus is automatically parsed, and case As the resources of selectional preferences to support coordinate structures, we use automatically constructed case frames and cooccurrences of noun-noun modifications. As a parser for extracting these resources, we use the Japanese de4 459 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP lect cooccurrences of coordinated nouns from automatic parses of a large corpus. We extracted 54.1 million unique noun pairs from the web corpus of 1.6 billion sentences. frames are constructed"
I11-1051,C08-1054,1,0.614389,"similarity. In addition, syntactic parallelism can be observed; each conjunct has a modifier Caesar and Italian, respectively. These types of parallelism contribute to identifying the coordinate structure that conjoins Caesar salad and Italian pasta. The other clue is selectional preferences, such as eat in the above example. Since eat is likely to have salad and pasta as its objects, it is plausible that salad and pasta are coordinated. Such selectional preferences of predicates are thought to support the construction of coordinate structures, and were used in Japanese dependency parsing by Kawahara and Kurohashi (2008). Selectional preferences of nouns (noun-noun modifications) were used by Resnik (1999), Nakov and Hearst (2005) and Kawahara and Kurohashi (2008). For example, let us see the following examples: 1 Introduction Coordinate structures are a potential source of syntactic ambiguity in natural language. Although many methods have been proposed to resolve the ambiguities of coordinate structures, coordination disambiguation still remains a difficult problem for state-of-the-art parsers. Previous studies on coordination disambiguation used two kinds of clues: (2) a. mail and securities fraud b. corn"
I11-1051,E09-1047,0,0.0245714,"Missing"
I11-1051,J03-4003,0,\N,Missing
I11-1051,C98-2122,0,\N,Missing
I11-1051,P92-1003,0,\N,Missing
I13-1005,P05-1012,0,0.0340903,"a query sentence. One stream is based on linguistically-motivated approaches that exploit natural language analysis to identify dependencies between words. For example, Jones proposed an information retrieval method that exploits linguistically-motivated analysis, especially dependency relations (Jones, 1999). However, Jones noted that dependency relations did not contribute to significantly improving performance due to the low accuracy and robustness of syntactic parsers. Subsequently, both the accuracy and robustness of dependency parsers were dramatically improved (Nivre and Scholz, 2004; McDonald et al., 2005), with such parsers being applied more recently to information retrieval (Lee et al., 2006; Song et al., 2008; ShinIn the area of question answering, predicateargument structures have been used to precisely match a query with a passage in a document (e.g., (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Bilotti et al., 2010)). However, candidate documents to extract an answer are retrieved using conventional search engines without NOM (nominative), ACC (accusative), DAT (dative), ALL (allative), GEN (genitive), CMI (comitative), LOC (locative), ABL (ablative), CMP (comparative), DEL (de"
I13-1005,C04-1100,0,0.0219297,"endency relations (Jones, 1999). However, Jones noted that dependency relations did not contribute to significantly improving performance due to the low accuracy and robustness of syntactic parsers. Subsequently, both the accuracy and robustness of dependency parsers were dramatically improved (Nivre and Scholz, 2004; McDonald et al., 2005), with such parsers being applied more recently to information retrieval (Lee et al., 2006; Song et al., 2008; ShinIn the area of question answering, predicateargument structures have been used to precisely match a query with a passage in a document (e.g., (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Bilotti et al., 2010)). However, candidate documents to extract an answer are retrieved using conventional search engines without NOM (nominative), ACC (accusative), DAT (dative), ALL (allative), GEN (genitive), CMI (comitative), LOC (locative), ABL (ablative), CMP (comparative), DEL (delimitative) and TOP (topic marker). 38 structure analysis normalizes the following linguistic expressions: predicate-argument structures. 3 Information retrieval exploiting predicate-argument structures • relative clause • passive voice (the predicate is normalized to active voice) 3.1"
I13-1005,C04-1010,0,0.0118925,"dencies between words in a query sentence. One stream is based on linguistically-motivated approaches that exploit natural language analysis to identify dependencies between words. For example, Jones proposed an information retrieval method that exploits linguistically-motivated analysis, especially dependency relations (Jones, 1999). However, Jones noted that dependency relations did not contribute to significantly improving performance due to the low accuracy and robustness of syntactic parsers. Subsequently, both the accuracy and robustness of dependency parsers were dramatically improved (Nivre and Scholz, 2004; McDonald et al., 2005), with such parsers being applied more recently to information retrieval (Lee et al., 2006; Song et al., 2008; ShinIn the area of question answering, predicateargument structures have been used to precisely match a query with a passage in a document (e.g., (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Bilotti et al., 2010)). However, candidate documents to extract an answer are retrieved using conventional search engines without NOM (nominative), ACC (accusative), DAT (dative), ALL (allative), GEN (genitive), CMI (comitative), LOC (locative), ABL (ablative), CM"
I13-1005,J05-1004,0,0.0628789,"Missing"
I13-1005,de-marneffe-etal-2006-generating,0,0.0580818,"Missing"
I13-1005,D07-1002,0,0.026795,". However, Jones noted that dependency relations did not contribute to significantly improving performance due to the low accuracy and robustness of syntactic parsers. Subsequently, both the accuracy and robustness of dependency parsers were dramatically improved (Nivre and Scholz, 2004; McDonald et al., 2005), with such parsers being applied more recently to information retrieval (Lee et al., 2006; Song et al., 2008; ShinIn the area of question answering, predicateargument structures have been used to precisely match a query with a passage in a document (e.g., (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Bilotti et al., 2010)). However, candidate documents to extract an answer are retrieved using conventional search engines without NOM (nominative), ACC (accusative), DAT (dative), ALL (allative), GEN (genitive), CMI (comitative), LOC (locative), ABL (ablative), CMP (comparative), DEL (delimitative) and TOP (topic marker). 38 structure analysis normalizes the following linguistic expressions: predicate-argument structures. 3 Information retrieval exploiting predicate-argument structures • relative clause • passive voice (the predicate is normalized to active voice) 3.1 Overview Our key idea i"
I13-1005,I08-1025,1,0.929082,"that in a document, a mismatch of dependency type can cause another problem. This is because previous models did not distinguish dependency types. For example, the dependency “YouTube←acquire” in query sentence (2) can be found in the following irrelevant document. Introduction Most conventional approaches to information retrieval (IR) deal with words as independent terms. In query sentences1 and documents, however, dependencies exist between words.2 To capture these dependencies, some extended IR models have been proposed in the last decade (Jones, 1999; Lee et al., 2006; Song et al., 2008; Shinzato et al., 2008). These models, however, did not achieve consistent significant improvements over models based on independent words. One of the reasons for this is the linguistic variations of syntax, that is, languages are syntactically expressed in various ways. For instance, the same or similar meaning can be expressed using the passive voice or the active voice in a sentence. Previous approaches based on dependencies cannot identify such variations. This is because they use the output of a dependency parser, which generates syntactic (grammatical) dependencies built (3) Google acquired PushLife for $25M ."
I13-1005,N06-1023,1,0.839455,"Missing"
I13-1005,J03-4003,0,\N,Missing
I13-1005,P06-1128,0,\N,Missing
I13-1020,I05-3018,0,0.0286895,"est score on at least one corpus (Tseng et al., 177 F 95.26 95.40 95.73 95.40 95.46 95.65 Table 6. F-measure on CTB7 test set compared with previous work. “+”: semisupervised systems. System Tseng 05 Asahara 05 Chen 05 Best closed Zhang 07 Zhao 07 Baseline MaxSub-S MaxSub-L+ AS 94.7 95.2 94.5 95.2 95.1 95.5 95.07 95.17 95.34 CityU 94.3 94.1 94.0 94.3 95.1 95.6 94.53 94.61 94.79 MSR 96.4 95.8 96.0 96.4 97.2 97.5 96.25 96.42 96.64 PKU 95.0 94.1 95.0 95.0 95.1 95.4 95.13 95.31 95.55 Table 7. F-measure on SIGHAN Bakeoff-2005 test set compared with previous work. “+”: semisupervised systems. 2005; Asahara et al., 2005; Chen et al., 2005). “Best closed” summarizes the best official results on all four corpora. “Zhao 07” and “Zhang 06” represent the supervised segmentation systems in (Zhao and Kit, 2007; Zhang et al., 2006). “Baseline”, “Maxsub-Test” and “MaxSub-U” refer to the same systems as in Table 5. For the unlabeled data, we have used the test sets of corresponding corpora for “MaxSub-Test”, and the Chinese Gigaword for “MaxSub-U”. Other parameters were left unchanged. The results do not indicate that our approach performs better than other systems. However, this is largely because of our baseline not"
I13-1020,W02-1001,0,0.013634,"sult in the best performance on the CTB7 dev set. We have used two different types of unlabeled data. One is the test set itself, which means the system is purely supervised. Another is a largescale dataset, which is the Chinese Gigaword Second Edition (LDC2007T03). This dataset is a collection of news articles from 1991 to 2004 published by Central News Agency (Taiwan), Xinhua News Agency and Lianhe Zaobao Newspaper. It includes a total amount of over 1.2 billion characters in both simplified Chinese and traditional Chinese. We have trained all models using the averaged perceptron algorithm (Collins, 2002), which we selected because of its efficiency and stability. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data with a frequency higher than a threshold, . This threshold was tuned using the development data. In order to use the maximized substring features, we have used training data as unlabeled data for supervised models, and used both the training data and Chinese Gigaword for semi-supervised models. We have applied the same parameters for all models, which are tuned on the CTB7 dev set: , , , and . We have used precision,"
I13-1020,I05-3017,0,0.0426989,"s “normal”, and all other cases are “infrequent”. 176 To evaluate our approach, we have conducted word segmentation experiments on two datasets. The first is Chinese Treebank 7 (CTB7), which is a widely used version of the Penn Chinese Treebank dataset for the evaluations of word segmentation techniques. We have adopted the same setting of data division as (Wang et al., 2011): the training set, dev set and test set. For CTB7, these sets have 31,131, 10,136 and 10,180 sentences respectively. The second dataset is the second international Chinese word segmentation bakeoff (SIGHAN Bakeoff-2005) (Emerson, 2005), which has four independent subsets: the Academia Sinica Corpus (AS), the Microsoft Research Corpus (MSR), the Hong Kong City University Corpus (CityU) and the Peking University Corpus (PKU). Since POS tags are not available in this dataset, we have omitted all templates that include them. The models and parameters applied on all test sets are those that result in the best performance on the CTB7 dev set. We have used two different types of unlabeled data. One is the test set itself, which means the system is purely supervised. Another is a largescale dataset, which is the Chinese Gigaword Se"
I13-1020,I05-3019,0,0.0295139,"one corpus (Tseng et al., 177 F 95.26 95.40 95.73 95.40 95.46 95.65 Table 6. F-measure on CTB7 test set compared with previous work. “+”: semisupervised systems. System Tseng 05 Asahara 05 Chen 05 Best closed Zhang 07 Zhao 07 Baseline MaxSub-S MaxSub-L+ AS 94.7 95.2 94.5 95.2 95.1 95.5 95.07 95.17 95.34 CityU 94.3 94.1 94.0 94.3 95.1 95.6 94.53 94.61 94.79 MSR 96.4 95.8 96.0 96.4 97.2 97.5 96.25 96.42 96.64 PKU 95.0 94.1 95.0 95.0 95.1 95.4 95.13 95.31 95.55 Table 7. F-measure on SIGHAN Bakeoff-2005 test set compared with previous work. “+”: semisupervised systems. 2005; Asahara et al., 2005; Chen et al., 2005). “Best closed” summarizes the best official results on all four corpora. “Zhao 07” and “Zhang 06” represent the supervised segmentation systems in (Zhao and Kit, 2007; Zhang et al., 2006). “Baseline”, “Maxsub-Test” and “MaxSub-U” refer to the same systems as in Table 5. For the unlabeled data, we have used the test sets of corresponding corpora for “MaxSub-Test”, and the Chinese Gigaword for “MaxSub-U”. Other parameters were left unchanged. The results do not indicate that our approach performs better than other systems. However, this is largely because of our baseline not being optimized for"
I13-1020,J04-1004,0,0.199407,"machine learning techniques have boosted the performance of CWS systems. On the other hand, a major difficulty in CWS is the problem of identifying out-of-vocabulary (OOV) words, as the Chinese language is continually and rapidly evolving, particularly with the rapid growth of the internet. A recent line of research to overcome this difficulty is through exploiting characteristics of frequent substrings in unlabeled data. Statistical criteria for measuring the likelihood of a substring being a word have been proposed in previous studies of unsupervised segmentation, such as accessor variety (Feng et al., 2004) and branching entropy (Jin and Tanaka-Ishii, 2006). This kind of criteria has been applied to enhance the performance of supervised segmentation systems (Zhao and Kit, 2007; Zhao and Kit, 2008; Sun and Xu, 2011) by identifying unknown word boundaries. In this paper, instead of investigating statistical characteristics of batched substrings, we propose a novel method that extracts substrings as reliable word boundary estimations. The technique uses large-scale unlabeled data, and processes it on the fly. To illustrate the idea, we first consider the following example taken from a scientific te"
I13-1020,P06-2056,0,0.120261,"the performance of CWS systems. On the other hand, a major difficulty in CWS is the problem of identifying out-of-vocabulary (OOV) words, as the Chinese language is continually and rapidly evolving, particularly with the rapid growth of the internet. A recent line of research to overcome this difficulty is through exploiting characteristics of frequent substrings in unlabeled data. Statistical criteria for measuring the likelihood of a substring being a word have been proposed in previous studies of unsupervised segmentation, such as accessor variety (Feng et al., 2004) and branching entropy (Jin and Tanaka-Ishii, 2006). This kind of criteria has been applied to enhance the performance of supervised segmentation systems (Zhao and Kit, 2007; Zhao and Kit, 2008; Sun and Xu, 2011) by identifying unknown word boundaries. In this paper, instead of investigating statistical characteristics of batched substrings, we propose a novel method that extracts substrings as reliable word boundary estimations. The technique uses large-scale unlabeled data, and processes it on the fly. To illustrate the idea, we first consider the following example taken from a scientific text: “使一致认定界限数的期望值近似于一致正确界限 数的期望值，求得一致认定界限的期望值/认定界 限"
I13-1020,P09-1058,0,0.332013,"The remainder of this paper is organized as follows. Section 2 describes our baseline segmentation system, defines maximized substrings, and proposes an efficient algorithm for retrieving these substrings from unlabeled data. Section 3 172 7 or more BB2B3M...ME introduces the maximized substring features. Section 4 presents the experimental results. Section 5 discusses related work. The final section summarizes our conclusions. 2. Approach 2.1 Baseline Segmentation System We have used a word-character hybrid model as our baseline Chinese word segmentation system (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). As shown in Figure 1, this hybrid model constructs a lattice that consists of wordlevel and character-level nodes from a given input sentence. Word-level nodes correspond to words found in the system’s lexicon, which has been compiled from training data. Characterlevel nodes have special tags called position-ofcharacter (POC) that indicate the word-internal position (Asahara, 2003; Nakagawa, 2004). We have adopted the 6-tag tagset, which (Zhao et al., 2006) reported to be optimal. This tagset is illustrated in Table 2. Previous studies have shown that jointly processing word segmentation and"
I13-1020,J09-4006,0,0.0128228,"r impacts. Although the idea behind co-occurrence sub-sequence is similar with maximized substrings, there are several restrictions: it requires post-processing to remove overlapping instances; sub-sequences are retrievable only from different sentences; and the retrieval is performed only on training and testing data. In (Sun and Xu, 2011), the authors proposed a semi-supervised segmentation system enhanced with multiple statistical criteria. Large-scale unlabeled data were used in their experiments. Li and Sun presented a model to learn features of word delimiters from punctuation marks in (Li and Sun, 2009). Wang et al. proposed a semisupervised word segmentation method that took advantages from auto-analyzed data (Wang et al., 2011). Nakagawa showed the advantage of the hybrid model combining both character-level information and word-level information in Chinese and Japanese word segmentation (Nakagawa, 2004). In (Nakagawa and Uchimoto, 2007) and (Kruengkrai et al., 2009a; 2009b) the researchers presented word-character hybrid models for joint word segmentation and POS tagging, and achieved the state-of-the-art accuracy on Chinese and Japanese datasets. 6. Conclusion We propose a simple yet eff"
I13-1020,C04-1067,0,0.206711,"izes our conclusions. 2. Approach 2.1 Baseline Segmentation System We have used a word-character hybrid model as our baseline Chinese word segmentation system (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). As shown in Figure 1, this hybrid model constructs a lattice that consists of wordlevel and character-level nodes from a given input sentence. Word-level nodes correspond to words found in the system’s lexicon, which has been compiled from training data. Characterlevel nodes have special tags called position-ofcharacter (POC) that indicate the word-internal position (Asahara, 2003; Nakagawa, 2004). We have adopted the 6-tag tagset, which (Zhao et al., 2006) reported to be optimal. This tagset is illustrated in Table 2. Previous studies have shown that jointly processing word segmentation and part-of-speech tagging is preferable to separate processing, which can propagate errors (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). If the training data was annotated by part-of-speech tags, we have combined them with both wordlevel and character-level nodes. XYZ ABC ABC ABCC JQK … XYZ ABC “ABCCKID…” ABC ABCC ABCK “ABCCFAL…” JQK ABCK ABCMN “ABCCTEA…” … ABCCFA “ABCCFAL…” “ABCCTEA…” ABCMN"
I13-1020,P07-2055,0,0.0621851,"Missing"
I13-1020,I11-1035,0,0.101994,"itional source of information, “MaxSub-Test” outperforms the baseline method by 0.14 points in F-score. This indicates that our method of using maximized substrings can enhance the segmentation performance even with a purely supervised approach. The improvement increases to 0.47 points in F-score for “MaxSub-U”, which demonstrates the effectiveness of using largescale unlabeled data. We have compared our approach with previous work in Table 6. Two methods from (Kruengkrai et al., 2009a; 2009b) are referred to as “Kruengkrai 09a” and “Kruengkrai 09b”, and are taken directly from the report of (Wang et al., 2011). “Wang 11” refers to the semi-supervised system in (Wang et al., 2011). We have observed that our system “MaxSub-U” achieves the best segmentation among these systems. Also, although the performance of our baseline is lower than the systems “Kruengkrai 09a” and “Kruengkrai 09b” because of differences in implementation, the system “MaxSub-Test” (which has used no external resource) has achieved a comparable result. The results for the SIGHAN Bakeoff-2005 dataset are shown in Table 7. The first three rows (“Tseng 05”, “Asahara 05” and “Chen 05”) show the results of systems that have reached the"
I13-1020,D11-1090,0,0.093584,"ually and rapidly evolving, particularly with the rapid growth of the internet. A recent line of research to overcome this difficulty is through exploiting characteristics of frequent substrings in unlabeled data. Statistical criteria for measuring the likelihood of a substring being a word have been proposed in previous studies of unsupervised segmentation, such as accessor variety (Feng et al., 2004) and branching entropy (Jin and Tanaka-Ishii, 2006). This kind of criteria has been applied to enhance the performance of supervised segmentation systems (Zhao and Kit, 2007; Zhao and Kit, 2008; Sun and Xu, 2011) by identifying unknown word boundaries. In this paper, instead of investigating statistical characteristics of batched substrings, we propose a novel method that extracts substrings as reliable word boundary estimations. The technique uses large-scale unlabeled data, and processes it on the fly. To illustrate the idea, we first consider the following example taken from a scientific text: “使一致认定界限数的期望值近似于一致正确界限 数的期望值，求得一致认定界限的期望值/认定界 限数的值。” Without any knowledge of the Chinese language one may still notice that some substrings like “一致” and “的期望值”, occur multiple times in the sentence and are"
I13-1020,I05-3027,0,0.089569,"Missing"
I13-1020,Y06-1012,0,0.024426,"n System We have used a word-character hybrid model as our baseline Chinese word segmentation system (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). As shown in Figure 1, this hybrid model constructs a lattice that consists of wordlevel and character-level nodes from a given input sentence. Word-level nodes correspond to words found in the system’s lexicon, which has been compiled from training data. Characterlevel nodes have special tags called position-ofcharacter (POC) that indicate the word-internal position (Asahara, 2003; Nakagawa, 2004). We have adopted the 6-tag tagset, which (Zhao et al., 2006) reported to be optimal. This tagset is illustrated in Table 2. Previous studies have shown that jointly processing word segmentation and part-of-speech tagging is preferable to separate processing, which can propagate errors (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). If the training data was annotated by part-of-speech tags, we have combined them with both wordlevel and character-level nodes. XYZ ABC ABC ABCC JQK … XYZ ABC “ABCCKID…” ABC ABCC ABCK “ABCCFAL…” JQK ABCK ABCMN “ABCCTEA…” … ABCCFA “ABCCFAL…” “ABCCTEA…” ABCMN “ABCCFAT…” “ABCCDEA…” “ABCCDEA…” Hash1 Hash2 Occur(ABCC) Has"
I13-1020,P06-2123,0,0.0449809,"5 Chen 05 Best closed Zhang 07 Zhao 07 Baseline MaxSub-S MaxSub-L+ AS 94.7 95.2 94.5 95.2 95.1 95.5 95.07 95.17 95.34 CityU 94.3 94.1 94.0 94.3 95.1 95.6 94.53 94.61 94.79 MSR 96.4 95.8 96.0 96.4 97.2 97.5 96.25 96.42 96.64 PKU 95.0 94.1 95.0 95.0 95.1 95.4 95.13 95.31 95.55 Table 7. F-measure on SIGHAN Bakeoff-2005 test set compared with previous work. “+”: semisupervised systems. 2005; Asahara et al., 2005; Chen et al., 2005). “Best closed” summarizes the best official results on all four corpora. “Zhao 07” and “Zhang 06” represent the supervised segmentation systems in (Zhao and Kit, 2007; Zhang et al., 2006). “Baseline”, “Maxsub-Test” and “MaxSub-U” refer to the same systems as in Table 5. For the unlabeled data, we have used the test sets of corresponding corpora for “MaxSub-Test”, and the Chinese Gigaword for “MaxSub-U”. Other parameters were left unchanged. The results do not indicate that our approach performs better than other systems. However, this is largely because of our baseline not being optimized for these corpora. Nevertheless, when compared with the baseline, our approach has yielded consistent improvements across the four corpora, and on the PKU corpus we have performed better than"
I13-1124,D07-1111,0,0.0500263,"Missing"
I13-1124,D09-1060,0,0.106189,"uncle nodes are also considered as other features. Similarly, we use leftmost and rightmost uncle nodes. 4 Experiments 4.1 Experimental Settings We first experiment on English, Chinese and Japanese. For English, we employ MSTparser1 as a base dependency parser and use sections 02 to 21 from Wall Street Journal (WSJ) corpus in Penn Treebank (PTB) to train a dependency parsing model. Then, we use section 22 from WSJ to apply the dependency parsing model to acquire the training data for dependency classification. MXPOST2 tagger is used for English automatic POS tagging. For Chinese, we use CNP (Chen et al., 2009) parser to train a dependency parser using section 1 to 270, 400 to 931 and 1001 to 1151 from Penn Chinese Treebank (CTB). Sections 301 to 325 are used to apply dependency parsing to acquire training data for dependency classification. We use MMA (Kruengkrai et al., 2009) to apply both segmentation and POS tagging. Different from the previous two languages which take words as the basic unit, experiments on Japanese are based on the unit of the phrase segments bunsetsu. We first use JUMAN3 for Japanese morphological analysis. Then KNP4 is utilized for Japanese dependency parsing. Section 950112"
I13-1124,W06-1604,0,0.0820275,"Missing"
I13-1124,W11-0314,0,0.0322624,"Missing"
I13-1124,P11-1109,1,0.837391,"Missing"
I13-1124,N06-1023,1,0.740308,"Missing"
I13-1124,kawahara-kurohashi-2010-acquiring,1,0.871315,"Missing"
I13-1124,I08-2097,1,0.901315,"Missing"
I13-1124,P09-1058,0,0.0510456,"use sections 02 to 21 from Wall Street Journal (WSJ) corpus in Penn Treebank (PTB) to train a dependency parsing model. Then, we use section 22 from WSJ to apply the dependency parsing model to acquire the training data for dependency classification. MXPOST2 tagger is used for English automatic POS tagging. For Chinese, we use CNP (Chen et al., 2009) parser to train a dependency parser using section 1 to 270, 400 to 931 and 1001 to 1151 from Penn Chinese Treebank (CTB). Sections 301 to 325 are used to apply dependency parsing to acquire training data for dependency classification. We use MMA (Kruengkrai et al., 2009) to apply both segmentation and POS tagging. Different from the previous two languages which take words as the basic unit, experiments on Japanese are based on the unit of the phrase segments bunsetsu. We first use JUMAN3 for Japanese morphological analysis. Then KNP4 is utilized for Japanese dependency parsing. Section 950112, 950113 and 9509ED from Kyoto Corpus are used to apply dependency parsing and acquire training data for dependency selection. We employ SVM-Light5 with polynomial kernel (degree 3) to solve the binary classification. In order to compare with previous work by Yu et al. (2"
I13-1124,D07-1013,0,0.0180192,"ing. The experimental results on English, Chinese and 947 International Joint Conference on Natural Language Processing, pages 947–951, Nagoya, Japan, 14-18 October 2013. by labeling each dependency according to the gold standard data. All the correct dependencies are defined as reliable and vice versa. 3.2 Features for Dependency Classification Most basic features consider that word pairs are much less likely to have a dependency relation when there are punctuation between them. On the other hand, based on the fact that dependencies with longer distance always show worse parsing performance (McDonald and Nivre, 2007), distance is another important factor that reflects the difficulty of judging whether two words have a dependency relation. Yu et al. (2008) used the features mentioned above and PoS features except the word features and did not use the context features, which are described later. In addition to these basic features, we consider context features that are thought to affect the parsing performance. Table 2 lists these context features. In some more complex cases, it is also necessary to observe larger span of context. In order to learn such linguistic characteristics automatically, besides POS"
I13-1124,P07-1052,0,0.0616152,"Missing"
I13-1124,W09-1120,0,0.0431396,"Missing"
I13-1124,D11-1076,0,\N,Missing
jin-etal-2014-framework,boas-2002-bilingual,0,\N,Missing
jin-etal-2014-framework,N06-1023,1,\N,Missing
jin-etal-2014-framework,P09-1058,0,\N,Missing
jin-etal-2014-framework,P13-1085,0,\N,Missing
jin-etal-2014-framework,P90-1034,0,\N,Missing
jin-etal-2014-framework,P11-1109,1,\N,Missing
jin-etal-2014-framework,D09-1060,0,\N,Missing
jin-etal-2014-framework,P98-2127,0,\N,Missing
jin-etal-2014-framework,C98-2122,0,\N,Missing
jin-etal-2014-framework,D13-1014,0,\N,Missing
jin-etal-2014-framework,I13-1124,1,\N,Missing
jin-etal-2014-framework,korhonen-etal-2006-large,0,\N,Missing
jin-etal-2014-framework,D11-1076,0,\N,Missing
jin-etal-2014-framework,kawahara-kurohashi-2010-acquiring,1,\N,Missing
kawahara-etal-2002-construction,W99-0307,0,\N,Missing
kawahara-etal-2002-construction,H94-1020,0,\N,Missing
kawahara-etal-2002-construction,poesio-2000-annotating,0,\N,Missing
kawahara-etal-2004-toward,kawahara-etal-2002-construction,1,\N,Missing
kawahara-etal-2004-toward,poesio-etal-2002-acquiring,0,\N,Missing
kawahara-etal-2004-toward,C02-1122,1,\N,Missing
kawahara-etal-2004-toward,P99-1062,1,\N,Missing
kawahara-etal-2004-toward,P03-1023,0,\N,Missing
kawahara-kurohashi-2006-case,C02-1122,1,\N,Missing
kawahara-kurohashi-2006-case,N04-1016,0,\N,Missing
kawahara-kurohashi-2006-case,J03-3005,0,\N,Missing
kawahara-kurohashi-2006-case,J03-3001,0,\N,Missing
kawahara-kurohashi-2010-acquiring,I05-1017,1,\N,Missing
kawahara-kurohashi-2010-acquiring,W98-1505,0,\N,Missing
kawahara-kurohashi-2010-acquiring,J93-2004,0,\N,Missing
kawahara-kurohashi-2010-acquiring,H05-1059,0,\N,Missing
kawahara-kurohashi-2010-acquiring,C94-1042,0,\N,Missing
kawahara-kurohashi-2010-acquiring,D08-1093,0,\N,Missing
kawahara-kurohashi-2010-acquiring,kawahara-uchimoto-2008-method,1,\N,Missing
kawahara-kurohashi-2010-acquiring,W02-0907,0,\N,Missing
kawahara-kurohashi-2010-acquiring,A97-1052,0,\N,Missing
kawahara-kurohashi-2010-acquiring,J93-2002,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P07-1052,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P03-1007,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P98-1013,0,\N,Missing
kawahara-kurohashi-2010-acquiring,C98-1013,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P93-1032,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P98-1071,0,\N,Missing
kawahara-kurohashi-2010-acquiring,C98-1068,0,\N,Missing
kawahara-kurohashi-2010-acquiring,J05-1004,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P87-1027,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P99-1051,0,\N,Missing
kawahara-kurohashi-2010-acquiring,korhonen-etal-2006-large,0,\N,Missing
kawahara-kurohashi-2010-acquiring,kawahara-kurohashi-2006-case,1,\N,Missing
kawahara-kurohashi-2010-acquiring,N01-1025,0,\N,Missing
kawahara-palmer-2014-single,de-marneffe-etal-2006-generating,0,\N,Missing
kawahara-palmer-2014-single,W11-0110,1,\N,Missing
kawahara-palmer-2014-single,H05-1111,0,\N,Missing
kawahara-palmer-2014-single,N09-1064,0,\N,Missing
kawahara-palmer-2014-single,C08-1002,0,\N,Missing
kawahara-palmer-2014-single,P98-1013,0,\N,Missing
kawahara-palmer-2014-single,C98-1013,0,\N,Missing
kawahara-palmer-2014-single,J92-4003,0,\N,Missing
kawahara-palmer-2014-single,P08-2008,1,\N,Missing
kawahara-palmer-2014-single,N13-1090,0,\N,Missing
kawahara-palmer-2014-single,P12-1028,1,\N,Missing
kawahara-palmer-2014-single,P10-1040,0,\N,Missing
kawahara-palmer-2014-single,J05-1004,1,\N,Missing
kawahara-palmer-2014-single,C10-1140,0,\N,Missing
kawahara-uchimoto-2008-method,W98-1505,0,\N,Missing
kawahara-uchimoto-2008-method,H05-1059,0,\N,Missing
kawahara-uchimoto-2008-method,C94-1042,0,\N,Missing
kawahara-uchimoto-2008-method,W07-1424,0,\N,Missing
kawahara-uchimoto-2008-method,W02-0907,0,\N,Missing
kawahara-uchimoto-2008-method,A97-1052,0,\N,Missing
kawahara-uchimoto-2008-method,J93-2002,0,\N,Missing
kawahara-uchimoto-2008-method,P03-1007,0,\N,Missing
kawahara-uchimoto-2008-method,P98-1013,0,\N,Missing
kawahara-uchimoto-2008-method,C98-1013,0,\N,Missing
kawahara-uchimoto-2008-method,P93-1032,0,\N,Missing
kawahara-uchimoto-2008-method,W06-2932,0,\N,Missing
kawahara-uchimoto-2008-method,P98-1071,0,\N,Missing
kawahara-uchimoto-2008-method,C98-1068,0,\N,Missing
kawahara-uchimoto-2008-method,P87-1027,0,\N,Missing
kawahara-uchimoto-2008-method,P99-1051,0,\N,Missing
kawahara-uchimoto-2008-method,korhonen-etal-2006-large,0,\N,Missing
kawahara-uchimoto-2008-method,kawahara-kurohashi-2006-case,1,\N,Missing
kawahara-uchimoto-2008-method,W97-0123,0,\N,Missing
L18-1050,O14-4001,0,0.02906,"Missing"
L18-1050,P14-2082,0,0.0160505,"e relationship between event information and time information in text. For example, temporal ordering of events that estimates the temporal relations of event-event and event-time was studied in TempEval 1, 2, 3 (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), and the timeline generation task that links event and time in multiple documents was studied in SemEval 15 (Minard et al., 2015). In order to train models and evaluate results in these tasks, corpora in which event information is correlated with time information in text have been developed (Pustejovsky et al., 2003; Cassidy et al., 2014; Reimers et al., 2016). In these studies, expressions which have clear temporality were annotated, but in order to know how people understand texts from the perspective of time, it is essential to know how the expressions with weak temporality are interpreted. To understand temporal information in text exhaustively, we propose an annotation scheme that anchors various expressions to the time axis, reflecting personal interpretation of text and common sense. Using this scheme, we annotate Kyoto University Text Corpus (Kawahara et al., 2002), which is a Japanese newspaper corpus annotated with"
L18-1050,D16-1005,0,0.0404101,"Missing"
L18-1050,kawahara-etal-2002-construction,1,0.603761,"in text have been developed (Pustejovsky et al., 2003; Cassidy et al., 2014; Reimers et al., 2016). In these studies, expressions which have clear temporality were annotated, but in order to know how people understand texts from the perspective of time, it is essential to know how the expressions with weak temporality are interpreted. To understand temporal information in text exhaustively, we propose an annotation scheme that anchors various expressions to the time axis, reflecting personal interpretation of text and common sense. Using this scheme, we annotate Kyoto University Text Corpus (Kawahara et al., 2002), which is a Japanese newspaper corpus annotated with predicateargument structures and coreference relations. The points of our annotation scheme are two-fold. One of the points is to annotate various expressions that can have temporality. We annotate not only expressions with strong temporality but also expressions with weak temporality. Many previous studies annotate “events” that express situations that happen or occur, which are defined in the guideline of TimeML (Sauri et al., 2006). Therefore, expressions as in the following example are not annotated. (1) Businesses are emerging on the I"
L18-1050,P12-1010,0,0.0463033,"Missing"
L18-1050,S15-2132,0,0.0345274,"Missing"
L18-1050,P16-1207,0,0.285186,"event information and time information in text. For example, temporal ordering of events that estimates the temporal relations of event-event and event-time was studied in TempEval 1, 2, 3 (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), and the timeline generation task that links event and time in multiple documents was studied in SemEval 15 (Minard et al., 2015). In order to train models and evaluate results in these tasks, corpora in which event information is correlated with time information in text have been developed (Pustejovsky et al., 2003; Cassidy et al., 2014; Reimers et al., 2016). In these studies, expressions which have clear temporality were annotated, but in order to know how people understand texts from the perspective of time, it is essential to know how the expressions with weak temporality are interpreted. To understand temporal information in text exhaustively, we propose an annotation scheme that anchors various expressions to the time axis, reflecting personal interpretation of text and common sense. Using this scheme, we annotate Kyoto University Text Corpus (Kawahara et al., 2002), which is a Japanese newspaper corpus annotated with predicateargument struc"
L18-1050,S13-2001,0,0.104484,"large amount of texts, we need an information analysis technology to integrate, summarize and compare related texts. In order to analyze texts written at different times or texts referring to different times, it is necessary to interpret the temporal information implied in the texts. There have been many studies and tasks to understand the relationship between event information and time information in text. For example, temporal ordering of events that estimates the temporal relations of event-event and event-time was studied in TempEval 1, 2, 3 (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), and the timeline generation task that links event and time in multiple documents was studied in SemEval 15 (Minard et al., 2015). In order to train models and evaluate results in these tasks, corpora in which event information is correlated with time information in text have been developed (Pustejovsky et al., 2003; Cassidy et al., 2014; Reimers et al., 2016). In these studies, expressions which have clear temporality were annotated, but in order to know how people understand texts from the perspective of time, it is essential to know how the expressions with weak temporality are interpreted"
L18-1050,S07-1014,0,0.31054,"ract knowledge about a certain topic from this large amount of texts, we need an information analysis technology to integrate, summarize and compare related texts. In order to analyze texts written at different times or texts referring to different times, it is necessary to interpret the temporal information implied in the texts. There have been many studies and tasks to understand the relationship between event information and time information in text. For example, temporal ordering of events that estimates the temporal relations of event-event and event-time was studied in TempEval 1, 2, 3 (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), and the timeline generation task that links event and time in multiple documents was studied in SemEval 15 (Minard et al., 2015). In order to train models and evaluate results in these tasks, corpora in which event information is correlated with time information in text have been developed (Pustejovsky et al., 2003; Cassidy et al., 2014; Reimers et al., 2016). In these studies, expressions which have clear temporality were annotated, but in order to know how people understand texts from the perspective of time, it is essential to know how the exp"
L18-1223,P98-1013,0,0.727279,"Missing"
L18-1223,P09-1068,0,0.0257387,"i et al. (2017), most natural language processing studies on emotion focus on emotion classification (Tokuhisa et al., 2008; Tokuhisa et al., 2009; Li et al., 2015; Zhou et al., 2016) and emotion information extraction (Vu et al., 2014). There are few studies on emotion cause extraction based on relationships between events and emotions caused by the events (Ghazi et al., 2015; Gui et al., 2016; Gui et al., 2017). These studies use explicit keywords in texts to recognize relationships between emotions and events. There are many studies on automatic acquisition of relationships between events (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010; Vanderwende, 2005; Shibata et al., 2014). Knowledge bases constructed in these studies are useful to determine what events happen after other events. However, these studies do not focus on motivations for events or effects caused by the events. As described in the previous section, this approach cannot guarantee quotidian connotational meanings from texts because of the reporting bias. Therefore, it is important to construct knowledge bases of such commonsense knowledge by manual intervention as well as through automatic acquisition. 3. Design of JFCKB Our final"
L18-1223,chambers-jurafsky-2010-database,0,0.0171929,"language processing studies on emotion focus on emotion classification (Tokuhisa et al., 2008; Tokuhisa et al., 2009; Li et al., 2015; Zhou et al., 2016) and emotion information extraction (Vu et al., 2014). There are few studies on emotion cause extraction based on relationships between events and emotions caused by the events (Ghazi et al., 2015; Gui et al., 2016; Gui et al., 2017). These studies use explicit keywords in texts to recognize relationships between emotions and events. There are many studies on automatic acquisition of relationships between events (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010; Vanderwende, 2005; Shibata et al., 2014). Knowledge bases constructed in these studies are useful to determine what events happen after other events. However, these studies do not focus on motivations for events or effects caused by the events. As described in the previous section, this approach cannot guarantee quotidian connotational meanings from texts because of the reporting bias. Therefore, it is important to construct knowledge bases of such commonsense knowledge by manual intervention as well as through automatic acquisition. 3. Design of JFCKB Our final goal is to construct a knowle"
L18-1223,D16-1170,0,0.054801,"Missing"
L18-1223,D17-1167,0,0.0437158,"Missing"
L18-1223,Y12-1058,1,0.845198,"391 case frames (types)) were used. In this study, we constructed a full version of the knowledge base and extended the dataset as follows: (1) for the representation manner of behaviors, we used only triples (increased, decreased, and unchanged) whose values are probabilities. In the trial version, some feature changes were represented by the triples while the other feature changes were represented by pairs (changed and unchanged) whose values are probabilities. (2) In addition to event sentences based on the 200 most frequent verbs in the Kyoto University Web Document Leads Corpus (KWDLC) (Hangyo et al., 2012)1 used in our previous work, we used sentences from other Japanese language resources. The resources are Kyoto University Case Frames (KUCF) (Kawahara and Kurohashi, 2006)2 and the Japanese translated version of the WSC (JWSC) dataset. KWDLC is a Japanese text corpus that comprises 5,000 documents (15,000 sentences) with annotations of morphology, named entities, dependencies, predicate-argument structures including zero anaphora and coreferences. KUCF is a database of case frames automatically constructed from a corpus of 10 billion Japanese sentences taken from the Web. Case frames describe"
L18-1223,P14-2065,0,0.187182,"eacher): 11, hito (person): 8, ... te (hand): 26449 kodomo (child): 168, musuko (son):108, ... daitouryou (president): 1, shidousya (mentor): 1, ... CD:13812, DVD:12200, ... Table 2: Case frame examples. Each row denotes one case frame. In the “Word” column, each number denotes the frequency of the noun in the Web corpus. Category as closely as possible. This decision was based on a traditional emotion study (Plutchik, 1980), Japanese thesauri (Ikehara, 1997; NINJAL, 2004), sentiment analysis studies (Tokuhisa et al., 2008; Tokuhisa et al., 2009), and features used in the VerbCorner project (Hartshorne et al., 2014). Although our final version of the proposed knowledge base will have all the features in Table 3, at present, emotional and sensory features in the table have been mainly investigated through crowdsourcing tasks described in the following section. physical color touch 4. Construction of JFCKB In our previous work (Nakamura and Kawahara, 2016), we constructed a trial version of a knowledge base in which each argument of an event sentence was associated with various feature changes caused by the events. As a result of a subjective evaluation experiment, it was shown that such feature changes ca"
L18-1223,P13-1095,0,0.224557,"tities’ perspective, effect, value, and mental state). To construct Connotation Frames, crowdsourcing and a news corpus were used. Although this knowledge base is useful for understanding information conveyed by event descriptions, it is hard to know details of the emotions associated with events because emotions themselves and associated behaviors are abstracted by polarities in the knowledge base. As for connotational meanings, especially for mental states, there are many studies that exploit traditional emotion models proposed by psychologists (Tokuhisa et al., 2008; Tokuhisa et al., 2009; Hasegawa et al., 2013; Vu et al., 2014). In these studies, Ekman’s Big Six Model (Ekman, 1992) or Plutchik’s wheel of emotions (Plutchik, 1980) are used to automatically extract emotion knowledge from large corpora or Web documents. However, as pointed out in Gui et al. (2017), most natural language processing studies on emotion focus on emotion classification (Tokuhisa et al., 2008; Tokuhisa et al., 2009; Li et al., 2015; Zhou et al., 2016) and emotion information extraction (Vu et al., 2014). There are few studies on emotion cause extraction based on relationships between events and emotions caused by the events"
L18-1223,kawahara-kurohashi-2006-case,1,0.822498,"ation manner of behaviors, we used only triples (increased, decreased, and unchanged) whose values are probabilities. In the trial version, some feature changes were represented by the triples while the other feature changes were represented by pairs (changed and unchanged) whose values are probabilities. (2) In addition to event sentences based on the 200 most frequent verbs in the Kyoto University Web Document Leads Corpus (KWDLC) (Hangyo et al., 2012)1 used in our previous work, we used sentences from other Japanese language resources. The resources are Kyoto University Case Frames (KUCF) (Kawahara and Kurohashi, 2006)2 and the Japanese translated version of the WSC (JWSC) dataset. KWDLC is a Japanese text corpus that comprises 5,000 documents (15,000 sentences) with annotations of morphology, named entities, dependencies, predicate-argument structures including zero anaphora and coreferences. KUCF is a database of case frames automatically constructed from a corpus of 10 billion Japanese sentences taken from the Web. Case frames describe what kinds of nouns are related to each verb. Many Japanese verbs have 1 2 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KWDLC http://www.gsk.or.jp/en/catalog/gsk2008-b/ Sub"
L18-1223,P15-1101,0,0.0571754,"Missing"
L18-1223,W16-1006,1,0.674547,"esults demonstrated the usefulness of our feature change knowledge base. Keywords: emotion, commonsense knowledge, knowledge base, crowdsourcing, anaphora resolution 1. Introduction Commonsense knowledge plays an essential role in our language activities. Such knowledge also plays an important role for computers to understand texts. In the area of language resource development, most studies on commonsense knowledge focus on denotational meanings such as predicate-argument structures (Baker et al., 1998; Palmer et al., 2005). Meanwhile, there are few studies focusing on connotational meanings (Nakamura and Kawahara, 2016; Rashkin et al., 2016). Even in such studies, abstract features such as polarities are used. However, concrete knowledge (that is, finegrained knowledge) is better for computers than abstract knowledge because events actually cause various concrete feature changes to participants in the events or those who know of the events. There are two approaches to acquiring commonsense knowledge. One is the automatic acquisition approach; the other is the manual acquisition approach. The automatic acquisition approach uses machine learning techniques and pattern matching methods. Although this approach"
L18-1223,J05-1004,0,0.154294,"as an antecedent candidate ranking task and used Ranking SVM as the solver. Experimental results demonstrated the usefulness of our feature change knowledge base. Keywords: emotion, commonsense knowledge, knowledge base, crowdsourcing, anaphora resolution 1. Introduction Commonsense knowledge plays an essential role in our language activities. Such knowledge also plays an important role for computers to understand texts. In the area of language resource development, most studies on commonsense knowledge focus on denotational meanings such as predicate-argument structures (Baker et al., 1998; Palmer et al., 2005). Meanwhile, there are few studies focusing on connotational meanings (Nakamura and Kawahara, 2016; Rashkin et al., 2016). Even in such studies, abstract features such as polarities are used. However, concrete knowledge (that is, finegrained knowledge) is better for computers than abstract knowledge because events actually cause various concrete feature changes to participants in the events or those who know of the events. There are two approaches to acquiring commonsense knowledge. One is the automatic acquisition approach; the other is the manual acquisition approach. The automatic acquisiti"
L18-1223,P16-1030,0,0.102098,"lness of our feature change knowledge base. Keywords: emotion, commonsense knowledge, knowledge base, crowdsourcing, anaphora resolution 1. Introduction Commonsense knowledge plays an essential role in our language activities. Such knowledge also plays an important role for computers to understand texts. In the area of language resource development, most studies on commonsense knowledge focus on denotational meanings such as predicate-argument structures (Baker et al., 1998; Palmer et al., 2005). Meanwhile, there are few studies focusing on connotational meanings (Nakamura and Kawahara, 2016; Rashkin et al., 2016). Even in such studies, abstract features such as polarities are used. However, concrete knowledge (that is, finegrained knowledge) is better for computers than abstract knowledge because events actually cause various concrete feature changes to participants in the events or those who know of the events. There are two approaches to acquiring commonsense knowledge. One is the automatic acquisition approach; the other is the manual acquisition approach. The automatic acquisition approach uses machine learning techniques and pattern matching methods. Although this approach is useful when the amou"
L18-1223,shibata-etal-2014-large,0,0.0174345,"emotion classification (Tokuhisa et al., 2008; Tokuhisa et al., 2009; Li et al., 2015; Zhou et al., 2016) and emotion information extraction (Vu et al., 2014). There are few studies on emotion cause extraction based on relationships between events and emotions caused by the events (Ghazi et al., 2015; Gui et al., 2016; Gui et al., 2017). These studies use explicit keywords in texts to recognize relationships between emotions and events. There are many studies on automatic acquisition of relationships between events (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010; Vanderwende, 2005; Shibata et al., 2014). Knowledge bases constructed in these studies are useful to determine what events happen after other events. However, these studies do not focus on motivations for events or effects caused by the events. As described in the previous section, this approach cannot guarantee quotidian connotational meanings from texts because of the reporting bias. Therefore, it is important to construct knowledge bases of such commonsense knowledge by manual intervention as well as through automatic acquisition. 3. Design of JFCKB Our final goal is to construct a knowledge base of various feature changes of arg"
L18-1223,speer-havasi-2012-representing,0,0.0172295,") to exploit the merits of both techniques. After the construction of our feature change knowledge base, we conducted an experiment on anaphora resolution using the knowledge base. In this experiment, we used a Japanese translated version of the Winograd Schema Challenge (WSC) dataset (JWSC) (Levesque, 2011; Shibata et al., 2015). 2. Related Work There are many studies focusing on events themselves. FrameNet (Baker et al., 1998) is a corpus in which deep cases (semantic roles) of predicates are defined. In FrameNet, each argument in an example sentence is labeled with a deep case. ConceptNet (Speer and Havasi, 2012) is a large semantic network constructed in the Open Mind Common Sense project (Singh et al., 2012). It is composed of relationships between concepts, where concepts are noun, verb or adjectival phrases. To date, some knowledge bases have been developed to capture connotational meanings of predicates. Lexical Conceptual Structure (LCS) is a useful way to describe behavior of arguments in event sentences (Jackendoff, 1983). In LCS, not only behaviors of arguments but also relationships between arguments in event sentences are described, for each verb. However, LCS focuses only on direct and exp"
L18-1223,strapparava-valitutti-2004-wordnet,0,0.119836,"predicates. Lexical Conceptual Structure (LCS) is a useful way to describe behavior of arguments in event sentences (Jackendoff, 1983). In LCS, not only behaviors of arguments but also relationships between arguments in event sentences are described, for each verb. However, LCS focuses only on direct and explicit information. It is necessary to develop a method to process indirect and implicit information because such knowledge is frequently used. Moreover, there is a possibility that many predicates are excessively generalized because the information used in LCS is abstract. WordNet-Affect (Strapparava and Valitutti, 2004) is an extended version of WordNet (Fellbaum, 1998). Although synsets of WordNet are associated with some emotions 1398 in WordNet-Affect, it is hard to know who/what is associated with the emotions in events. Connotation Frames are a knowledge base of emotional implications of events (Rashkin et al., 2016). In Connotation Frames, implications of an event are represented by a set of polarities categorized into five types (writers’ perspective, entities’ perspective, effect, value, and mental state). To construct Connotation Frames, crowdsourcing and a news corpus were used. Although this knowl"
L18-1223,C08-1111,0,0.77842,"ized into five types (writers’ perspective, entities’ perspective, effect, value, and mental state). To construct Connotation Frames, crowdsourcing and a news corpus were used. Although this knowledge base is useful for understanding information conveyed by event descriptions, it is hard to know details of the emotions associated with events because emotions themselves and associated behaviors are abstracted by polarities in the knowledge base. As for connotational meanings, especially for mental states, there are many studies that exploit traditional emotion models proposed by psychologists (Tokuhisa et al., 2008; Tokuhisa et al., 2009; Hasegawa et al., 2013; Vu et al., 2014). In these studies, Ekman’s Big Six Model (Ekman, 1992) or Plutchik’s wheel of emotions (Plutchik, 1980) are used to automatically extract emotion knowledge from large corpora or Web documents. However, as pointed out in Gui et al. (2017), most natural language processing studies on emotion focus on emotion classification (Tokuhisa et al., 2008; Tokuhisa et al., 2009; Li et al., 2015; Zhou et al., 2016) and emotion information extraction (Vu et al., 2014). There are few studies on emotion cause extraction based on relationships be"
L18-1223,E14-4025,0,0.013449,"fect, value, and mental state). To construct Connotation Frames, crowdsourcing and a news corpus were used. Although this knowledge base is useful for understanding information conveyed by event descriptions, it is hard to know details of the emotions associated with events because emotions themselves and associated behaviors are abstracted by polarities in the knowledge base. As for connotational meanings, especially for mental states, there are many studies that exploit traditional emotion models proposed by psychologists (Tokuhisa et al., 2008; Tokuhisa et al., 2009; Hasegawa et al., 2013; Vu et al., 2014). In these studies, Ekman’s Big Six Model (Ekman, 1992) or Plutchik’s wheel of emotions (Plutchik, 1980) are used to automatically extract emotion knowledge from large corpora or Web documents. However, as pointed out in Gui et al. (2017), most natural language processing studies on emotion focus on emotion classification (Tokuhisa et al., 2008; Tokuhisa et al., 2009; Li et al., 2015; Zhou et al., 2016) and emotion information extraction (Vu et al., 2014). There are few studies on emotion cause extraction based on relationships between events and emotions caused by the events (Ghazi et al., 20"
L18-1223,D16-1061,0,0.0129969,"nings, especially for mental states, there are many studies that exploit traditional emotion models proposed by psychologists (Tokuhisa et al., 2008; Tokuhisa et al., 2009; Hasegawa et al., 2013; Vu et al., 2014). In these studies, Ekman’s Big Six Model (Ekman, 1992) or Plutchik’s wheel of emotions (Plutchik, 1980) are used to automatically extract emotion knowledge from large corpora or Web documents. However, as pointed out in Gui et al. (2017), most natural language processing studies on emotion focus on emotion classification (Tokuhisa et al., 2008; Tokuhisa et al., 2009; Li et al., 2015; Zhou et al., 2016) and emotion information extraction (Vu et al., 2014). There are few studies on emotion cause extraction based on relationships between events and emotions caused by the events (Ghazi et al., 2015; Gui et al., 2016; Gui et al., 2017). These studies use explicit keywords in texts to recognize relationships between emotions and events. There are many studies on automatic acquisition of relationships between events (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010; Vanderwende, 2005; Shibata et al., 2014). Knowledge bases constructed in these studies are useful to determine what events ha"
L18-1461,Y12-1058,1,0.824741,"ion study (Plutchik, 1980), Japanese thesauri (Ikehara, 1997; NINJAL, 2004), sentiment analysis studies (Tokuhisa et al., 2008; Tokuhisa et al., 2009), and features used in the VerbCorner project (Hartshorne et al., 2014). Although our final version of JFCKB will have all the features listed in Table 2, emotional and sensory features are mainly investigated in the current study. Event sentences for JFCKB were created as follows. Step 1: the 200 most frequent verbs, 1,000 most frequent verbs, and all verbs were respectively extracted from the Kyoto University Web Document Leads Corpus (KWDLC) (Hangyo et al., 2012)1 , Kyoto University Case Frames (KUCF) (Kawahara and Kurohashi, 2006)2 , and the Japanese version of the Winograd Schema Challenge dataset (JWSC) (Levesque, 2011; Shibata et al., 2015). KWDLC is a Japanese text corpus that comprises 5,000 documents (15,000 sentences) with annotations of morphology, named entities, dependencies, predicate-argument structures including zero anaphora and coreferences. KUCF is a database of case frames automatically conJFCKB is composed of three types of information for event sentences, as shown the left three columns (sentence, case, and probabilities) in Table"
L18-1461,P14-2065,0,0.016977,"in each argument has a triple (increased, decreased, and unchanged) whose values are probabilities. We controlled the granularity of knowledge (i.e., features), and designed the 47 features shown in Table 2. These features were designed to correspond with basic level categories in cognitive linguistics (Rosch et al., 1976; Taylor, 1995) as much as possible. This design was based on a traditional emotion study (Plutchik, 1980), Japanese thesauri (Ikehara, 1997; NINJAL, 2004), sentiment analysis studies (Tokuhisa et al., 2008; Tokuhisa et al., 2009), and features used in the VerbCorner project (Hartshorne et al., 2014). Although our final version of JFCKB will have all the features listed in Table 2, emotional and sensory features are mainly investigated in the current study. Event sentences for JFCKB were created as follows. Step 1: the 200 most frequent verbs, 1,000 most frequent verbs, and all verbs were respectively extracted from the Kyoto University Web Document Leads Corpus (KWDLC) (Hangyo et al., 2012)1 , Kyoto University Case Frames (KUCF) (Kawahara and Kurohashi, 2006)2 , and the Japanese version of the Winograd Schema Challenge dataset (JWSC) (Levesque, 2011; Shibata et al., 2015). KWDLC is a Jap"
L18-1461,P13-1095,0,0.0184096,"achine dialogue, it is difficult to understand a speaker’s/replier’s motivations because such corpora do not record a speaker’s/replier’s inner state (in particular, his/her emotions). Even if such corpora include some keywords as clues for inferring a speaker’s/replier’s inner state, it is necessary to develop a method to extract inner state information from the corpora, which are composed of raw text. Dialogue corpora that include various feature changes of arguments in utterances and the reactions to speakers can be used to understand a speaker’s motivations. In the dialogue corpus used in Hasegawa et al. (2013), each utterance is annotated with the addressee’s emotions. Although this corpus is useful for understanding the relationships between utterances and emotions in a conversation, the understandable relationships are limited to the addressee’s direct emotional expressions because the corpus is annotated based on an explicit keyword list. In the keyword list, explicit keywords such as “afraid” and “happy” are manually associated with emotions “fear” and “joy” respectively. There are other relationships between utterances and emotions in conversations, such as relationships that concern the speak"
L18-1461,L16-1502,0,0.0286441,"a spoken dialog system (Kim et al., 2016). While the DSTC corpus is made from manually transcribed Skype dialogues, there are corpora that consist of conversations extracted from SNS websites (Ritter et al., 2010; Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015). There is also a corpus based on a collection of logs extracted from Ubuntu-related chat rooms that is mainly composed of technical support conversations (Lowe et al., 2015; Lowe et al., 2016). The Dialogue Breakdown Detection Challenge database is a corpus used to detect incorrect replies generated by dialogue systems (Higashinaka et al., 2016). Although these corpora are very useful resources for understanding actual human-human dialogue or human-machine dialogue, it is difficult to understand a speaker’s/replier’s motivations because such corpora do not record a speaker’s/replier’s inner state (in particular, his/her emotions). Even if such corpora include some keywords as clues for inferring a speaker’s/replier’s inner state, it is necessary to develop a method to extract inner state information from the corpora, which are composed of raw text. Dialogue corpora that include various feature changes of arguments in utterances and t"
L18-1461,kawahara-kurohashi-2006-case,1,0.635173,"NINJAL, 2004), sentiment analysis studies (Tokuhisa et al., 2008; Tokuhisa et al., 2009), and features used in the VerbCorner project (Hartshorne et al., 2014). Although our final version of JFCKB will have all the features listed in Table 2, emotional and sensory features are mainly investigated in the current study. Event sentences for JFCKB were created as follows. Step 1: the 200 most frequent verbs, 1,000 most frequent verbs, and all verbs were respectively extracted from the Kyoto University Web Document Leads Corpus (KWDLC) (Hangyo et al., 2012)1 , Kyoto University Case Frames (KUCF) (Kawahara and Kurohashi, 2006)2 , and the Japanese version of the Winograd Schema Challenge dataset (JWSC) (Levesque, 2011; Shibata et al., 2015). KWDLC is a Japanese text corpus that comprises 5,000 documents (15,000 sentences) with annotations of morphology, named entities, dependencies, predicate-argument structures including zero anaphora and coreferences. KUCF is a database of case frames automatically conJFCKB is composed of three types of information for event sentences, as shown the left three columns (sentence, case, and probabilities) in Table 1. As shown in the table, for 2916 1 2 http://nlp.ist.i.kyoto-u.ac.jp/"
L18-1461,W15-4640,0,0.0477009,"Missing"
L18-1461,W16-3634,0,0.0129628,"o train and evaluate machine learning methods. For instance, the Dialog State Tracking Challenge (DSTC) dataset is used to estimate a user’s goal in a spoken dialog system (Kim et al., 2016). While the DSTC corpus is made from manually transcribed Skype dialogues, there are corpora that consist of conversations extracted from SNS websites (Ritter et al., 2010; Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015). There is also a corpus based on a collection of logs extracted from Ubuntu-related chat rooms that is mainly composed of technical support conversations (Lowe et al., 2015; Lowe et al., 2016). The Dialogue Breakdown Detection Challenge database is a corpus used to detect incorrect replies generated by dialogue systems (Higashinaka et al., 2016). Although these corpora are very useful resources for understanding actual human-human dialogue or human-machine dialogue, it is difficult to understand a speaker’s/replier’s motivations because such corpora do not record a speaker’s/replier’s inner state (in particular, his/her emotions). Even if such corpora include some keywords as clues for inferring a speaker’s/replier’s inner state, it is necessary to develop a method to extract inner"
L18-1461,W16-1006,1,0.838019,"columns (sentence, case, and probabilities) in JDCFC are the same information of JFCKB. The trigger utterance corresponds to the event sentence. Replies are given probabilities for their reasonableness. In the “Probability” column, symbols +, −, and UNC denote increased, decreased, and unchanged, respectively. In the “Reply” column, symbols ◦, ×, and UNK denote reasonable, unreasonable, and unknown, respectively. a given conversation (i.e., a dialogue). This corpus is for Japanese. 2. Proposed Dialogue Corpus Based on a Feature Change Knowledge Base Since the publication of our previous work (Nakamura and Kawahara, 2016), we have been constructing a knowledge base of argument feature changes in event sentences with controlled granularity. We call this knowledge base JFCKB (Nakamura and Kawahara, 2018). In JFCKB, arguments in event sentences are associated with various feature changes caused by the events. The feature changes of sentence readers (i.e., sentence recognizers) are also associated with the sentences in the current version of JFCKB. For example, in the case of “my wife hits my child,” “my child” is associated with some feature changes, such as increase in pain, increase in anger, increase in disgus"
L18-1461,L18-1223,1,0.892163,"tions (especially in emotional conversations) in addition to the relationships used in Hasegawa et al. (2013). It is necessary to construct corpora designed to treat both of explicit and implicit emotional expressions because explicit emotional expressions are not always used in daily conversations. For example, when someone says “my wife hit my child,” he probably wants to convey some kinds of information about his “surprise,” “anger,” and “disgust.” In this paper, we propose a dialogue corpus constructed based on our knowledge base, called the Japanese Feature Change Knowledge Base (JFCKB) (Nakamura and Kawahara, 2018). In the proposed corpus, feature changes (mainly emotions) of arguments in utterances and those of the utterance recognizers (i.e., utterers and addressees) are associated with the utterances. Because of the lack of large-scale corpora focusing on detailed relationships between emotions and utterances, the dialogue corpora constructed based on JFCKB will be useful for developing robots and software that can handle natural language. To validate the usefulness of our dialogue corpus, we conducted an experiment to investigate whether a machine learning method can recognize the reasonableness of"
L18-1461,N10-1020,0,0.04514,"expressions in conversations. This is partially because there are no large-scale corpora focusing on the detailed relationships between emotions and utterances. Many dialogue corpora have been developed because they are essential language resources needed to train and evaluate machine learning methods. For instance, the Dialog State Tracking Challenge (DSTC) dataset is used to estimate a user’s goal in a spoken dialog system (Kim et al., 2016). While the DSTC corpus is made from manually transcribed Skype dialogues, there are corpora that consist of conversations extracted from SNS websites (Ritter et al., 2010; Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015). There is also a corpus based on a collection of logs extracted from Ubuntu-related chat rooms that is mainly composed of technical support conversations (Lowe et al., 2015; Lowe et al., 2016). The Dialogue Breakdown Detection Challenge database is a corpus used to detect incorrect replies generated by dialogue systems (Higashinaka et al., 2016). Although these corpora are very useful resources for understanding actual human-human dialogue or human-machine dialogue, it is difficult to understand a speaker’s/replier’s motivations"
L18-1461,D11-1054,0,0.0306539,"rsations. This is partially because there are no large-scale corpora focusing on the detailed relationships between emotions and utterances. Many dialogue corpora have been developed because they are essential language resources needed to train and evaluate machine learning methods. For instance, the Dialog State Tracking Challenge (DSTC) dataset is used to estimate a user’s goal in a spoken dialog system (Kim et al., 2016). While the DSTC corpus is made from manually transcribed Skype dialogues, there are corpora that consist of conversations extracted from SNS websites (Ritter et al., 2010; Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015). There is also a corpus based on a collection of logs extracted from Ubuntu-related chat rooms that is mainly composed of technical support conversations (Lowe et al., 2015; Lowe et al., 2016). The Dialogue Breakdown Detection Challenge database is a corpus used to detect incorrect replies generated by dialogue systems (Higashinaka et al., 2016). Although these corpora are very useful resources for understanding actual human-human dialogue or human-machine dialogue, it is difficult to understand a speaker’s/replier’s motivations because such corpora"
L18-1461,P15-1152,0,0.0147847,"re no large-scale corpora focusing on the detailed relationships between emotions and utterances. Many dialogue corpora have been developed because they are essential language resources needed to train and evaluate machine learning methods. For instance, the Dialog State Tracking Challenge (DSTC) dataset is used to estimate a user’s goal in a spoken dialog system (Kim et al., 2016). While the DSTC corpus is made from manually transcribed Skype dialogues, there are corpora that consist of conversations extracted from SNS websites (Ritter et al., 2010; Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015). There is also a corpus based on a collection of logs extracted from Ubuntu-related chat rooms that is mainly composed of technical support conversations (Lowe et al., 2015; Lowe et al., 2016). The Dialogue Breakdown Detection Challenge database is a corpus used to detect incorrect replies generated by dialogue systems (Higashinaka et al., 2016). Although these corpora are very useful resources for understanding actual human-human dialogue or human-machine dialogue, it is difficult to understand a speaker’s/replier’s motivations because such corpora do not record a speaker’s/replier’s inner s"
L18-1461,N15-1020,0,0.0381379,"Missing"
L18-1461,C08-1111,0,0.0360489,"each sentence, arguments in the sentence are associated with various features. Each feature in each argument has a triple (increased, decreased, and unchanged) whose values are probabilities. We controlled the granularity of knowledge (i.e., features), and designed the 47 features shown in Table 2. These features were designed to correspond with basic level categories in cognitive linguistics (Rosch et al., 1976; Taylor, 1995) as much as possible. This design was based on a traditional emotion study (Plutchik, 1980), Japanese thesauri (Ikehara, 1997; NINJAL, 2004), sentiment analysis studies (Tokuhisa et al., 2008; Tokuhisa et al., 2009), and features used in the VerbCorner project (Hartshorne et al., 2014). Although our final version of JFCKB will have all the features listed in Table 2, emotional and sensory features are mainly investigated in the current study. Event sentences for JFCKB were created as follows. Step 1: the 200 most frequent verbs, 1,000 most frequent verbs, and all verbs were respectively extracted from the Kyoto University Web Document Leads Corpus (KWDLC) (Hangyo et al., 2012)1 , Kyoto University Case Frames (KUCF) (Kawahara and Kurohashi, 2006)2 , and the Japanese version of the"
L18-1637,J94-4001,1,0.151172,"us with discourse annotations produced by Kawahara et al. (2014). The target documents are web pages extracted from the Kyoto University Web Leads Corpus (Hangyo et al., 2012). Each document consists of the first three sentences of a Japanese web page. The web pages cover a variety of domains and the first three sentences are long enough to annotate with discourse relations through crowdsourcing. They adopted a clause as the discourse unit. The clause is a span delimited by relatively strong boundaries in a sentence. They are automatically identified with hand-written rules by the KNP parser (Kurohashi and Nagao, 1994). Kawahara et al. (2014) annotated all possible combinations of clauses with discourse annotations. Table 1 shows the discourse relation tagset. This tagset consists of two layers, where the upper layer contains three classes and the lower layer contains seven classes. 4044 Upper type Lower type Cause/Reason Purpose CONTINGENCY Condition Example 【ボタンを押したので】【お湯が出た】 [Since (I) pushed the button] [hot water came out] 【試験に受かるために】【必死に勉強した】 [To pass the exam] [(I) desperately studied] 【ボタンを押せば】【お湯が出る。】 [If (you) push the button] [hot water will be turned on] Ground 【ここにカバンがあるから】【まだ社内にいるだろう。】 [Here i"
L18-1637,prasad-etal-2008-penn,0,0.493981,"Missing"
L18-1637,J14-4007,0,0.0487645,"Missing"
L18-1637,D08-1027,0,0.223774,"Missing"
L18-1637,P12-1008,0,0.0498295,"Missing"
N04-4039,W00-1423,0,0.0380054,"Missing"
N04-4039,J94-4001,0,0.0108357,"ce for marking the theme explic- a stroke co-occurs with the most prominent syllable itly. Topic marking postpositions (or “topic markers”), (Kendon, 1972). Thus, we annotated the stroke time as typically “wa,” mark a nominal phrase as the theme. well as the start and end time of each gesture. This facilitates the use of syntactic analysis to identify Linguistic Analysis: Each bunsetsu unit was automatithe theme of a sentence. Another interesting aspect of cally annotated with linguistic information using a Japa2 information structure is that in English grammar, a wh- nese syntactic analyzer (Kurohashi & Nagao, 1994) . interrogative (what, how, etc.) at the beginning of a The information was determined by asked the following sentence marks the theme and indicates that the content questions for each bunsetsu unit. of the theme is the focus (Halliday, 1967). However, we (a) If it is an NP, is it modified by a clause or a complement? do not know whether such a special type of theme is (b) If it is an NP, what type of postpositional particle more likely to co-occur with a gesture or not. marks its end (e.g., “wa”, “ga”, “wo”)? Given/New: Given and new information demonstrate (c) Is it a wh-interrogative? an a"
N04-4039,P96-1039,0,\N,Missing
N04-4039,J86-3001,0,\N,Missing
N06-1023,P98-1013,0,0.00845536,"Missing"
N06-1023,A00-2031,0,0.0129625,"Missing"
N06-1023,A00-2018,0,0.0421672,"Missing"
N06-1023,W98-1511,0,0.060744,"Missing"
N06-1023,C02-1122,1,0.948491,"and part-of-speech categories (Bikel, 2004). This paper aims at exploiting much more lexical information, and proposes a fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis. Lexical information is extracted not from a small tagged corpus, but from a huge raw corpus as case frames. This model performs case structure analysis by a generative probabilistic model based on the case frames, and selects the syntactic structure that has the highest case structure probability. 2 Automatically Constructed Case Frames We employ automatically constructed case frames (Kawahara and Kurohashi, 2002) for our model of 177 Table 1: Case frame examples (examples are expressed only in English for space limitation.). CS ga youritsu (1) wo (support) ni ga youritsu (2) wo (support) ni .. .. . . itadaku (1) ga wo (have) ga itadaku (2) wo (be given) kara .. .. . . examples <agent>, group, party, · · · <agent>, candidate, applicant <agent>, district, election, · · · <agent> <agent>, member, minister, · · · <agent>, candidate, successor .. . <agent> soup <agent> advice, instruction, address <agent>, president, circle, · · · .. . case structure analysis. This section outlines the method for construct"
N06-1023,kawahara-kurohashi-2006-case,1,0.714718,"es are distinguished not by verbs (e.g., “tsumu” (load/accumulate)), but by couples (e.g., “nimotsu-wo tsumu” (load baggage) and “keiken-wo tsumu” (accumulate experience)). Modifier-head examples are aggregated in this way, and yield basic case frames. Thereafter, the basic case frames are clustered to merge similar case frames. For example, since “nimotsu-wo tsumu” (load baggage) and “busshi-wo tsumu” (load supply) are similar, they are clustered. The similarity is measured using a thesaurus (Ikehara et al., 1997). Using this gradual procedure, we constructed case frames from the web corpus (Kawahara and Kurohashi, 2006). The case frames were obtained from approximately 470M sentences extracted from the web. They consisted of 90,000 verbs, and the average number of case frames for a verb was 34.3. In Figure 1, some examples of the resulting case frames are shown. In this table, ‘CS’ means a case slot. <agent> in the table is a generalized example, which is given to the case slot where half of the examples belong to <agent> in a thesaurus (Ikehara et al., 1997). <agent> is also given to “ga” case slot that has no examples, because “ga” case components are usually agentive and often omitted. 3 Integrated Probab"
N06-1023,kawahara-etal-2002-construction,1,0.879956,"Missing"
N06-1023,P03-1054,0,0.00381633,"Missing"
N06-1023,W02-2016,0,0.479957,"Missing"
N06-1023,J94-4001,1,0.326326,"Missing"
N06-1023,W98-1510,0,0.0598714,"Missing"
N06-1023,2000.iwpt-1.43,0,0.0658874,"Missing"
N06-1023,J04-4004,0,\N,Missing
N06-1023,J03-4003,0,\N,Missing
N06-1023,C98-1013,0,\N,Missing
N09-1059,P06-2004,0,0.0609428,"Missing"
N09-1059,H01-1052,0,0.418193,"lly utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data size improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; they found that the combined system only improved the performance of the parser for small training sets. Brants et al. (2007) varied the amount of language model training data from 13 million to 2 trillion tokens and applied these models to machine translation systems. They reported that translation quality continued to improve"
N09-1059,P01-1005,0,0.487383,"lly utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data size improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; they found that the combined system only improved the performance of the parser for small training sets. Brants et al. (2007) varied the amount of language model training data from 13 million to 2 trillion tokens and applied these models to machine translation systems. They reported that translation quality continued to improve"
N09-1059,D07-1090,0,0.0402824,"e structure analysis, and zero anaphora resolution. We obtained better results by using case frames constructed from larger corpora; the performance was not saturated even with a corpus size of 100 billion words. 1 Introduction Very large corpora obtained from the Web have been successfully utilized for many natural language processing (NLP) applications, such as prepositional phrase (PP) attachment, other-anaphora resolution, spelling correction, confusable word set disambiguation and machine translation (Volk, 2001; Modjeska et al., 2003; Lapata and Keller, 2005; Atterer and Sch¨utze, 2006; Brants et al., 2007). Most of the previous work utilized only the surface information of the corpora, such as n-grams, co-occurrence counts, and simple surface syntax. This may be because these studies did not require structured knowledge, and for such studies, the size of currently available corpora is considered to have been almost enough. For instance, while Brants et al. (2007) reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens, the 521 increase became small at the corpus size of larger than 30 billion tokens. Howev"
N09-1059,C08-1036,0,0.0132129,"machine translation systems. They reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens. Suzuki and Isozaki (2008) provided evidence that the use of more unlabeled data in semisupervised learning could improve the performance of NLP tasks, such as POS tagging, syntactic chunking, and named entities recognition. There are several methods to extract useful information from very large corpora. Search engines, such as Google and Altavista, are often used to obtain Web counts (e.g. (Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from vari522 ety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 bill"
N09-1059,halacsy-etal-2004-creating,0,0.012625,"(Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from vari522 ety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi (2006b) collected 23 million pages and created a corpus with approximately 20 billion words. Google released Japanese n-gram constructed from 20 billion Japanese sentences (Kudo and Kazawa, 2007). Several news"
N09-1059,N06-1023,1,0.817137,"emantic case frames, which describe the cases each predicate has and the types of nouns that can ﬁll a case slot. Note that case frames offer not only the knowledge of the relationships between a predicate and its particular case slot, but also the knowledge of the relationships among a predicate and its multiple case slots. To obtain such knowledge, very large corpora seem to be necessary; however it is still unknown how much corpora would be required to obtain good coverage. For examples, Kawahara and Kurohashi proposed a method for constructing wide-coverage case frames from large corpora (Kawahara and Kurohashi, 2006b), and a model for syntactic and case structure analysis of Japanese that based upon case frames (Kawahara and Kurohashi, 2006a). However, they did not demonstrate whether the coverage of case frames was wide enough for these tasks and how dependent the performance of the model was on the corpus size for case frame construction. This paper aims to address these questions. We collect a very large Japanese corpus consisting of about 100 billion words, or 1.6 billion unique sentences from the Web. Subsets of the corpus are randomly selected to obtain corpora of different sizes ranging from 1.6 m"
N09-1059,kawahara-kurohashi-2006-case,1,0.83525,"Missing"
N09-1059,D07-1032,1,0.900202,"Missing"
N09-1059,kawahara-etal-2004-toward,1,0.836044,"statistics for the constructed case frames. The number of predicates, the average number of examples and unique examples for a case slot, and whole ﬁle size were conﬁrmed to be heavily dependent upon the corpus size. However, the average number of case frames for a predicate and case slots for a case frame did not. 525 5.2.1 Setting In order to investigate the coverage of the resultant case frames, we used a syntactic relation, case structure, and anaphoric relation annotated corpus consisting of 186 web documents (979 sentences). This corpus was manually annotated using the same criteria as Kawahara et al. (2004). There were 2,390 annotated relationships between predicates and their direct (not omitted) case components and 837 zero anaphoric relations in the corpus. We used two evaluation metrics depending upon whether the target case component was omitted or not. For the overt case component of a predicate, we judged the target component was covered by case frames if the target component itself was included in the examples for one of the corresponding case slots of the case frame. For the omitted case component, we checked not only the target component itself but also all mentions that refer to the s"
N09-1059,J03-3001,0,0.0365786,"zes ranging from 1.6 million to 1.6 billion sentences. We construct case frames from each corpus and apply them to syntactic and case structure analysis, and zero anaphora resolution, in order to investigate the Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 521–529, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics relationships between the corpus size and the performance of these analyses. 2 Related Work Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data s"
N09-1059,E06-1030,0,0.0160577,"unking, and named entities recognition. There are several methods to extract useful information from very large corpora. Search engines, such as Google and Altavista, are often used to obtain Web counts (e.g. (Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from vari522 ety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi ("
N09-1059,N06-1020,0,0.0173976,"Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from vari522 ety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi (2006b) collected 23 million pages and created a corpus with approximately 20 billion words. Google released Japanese n-gram constructed from 20 billion Japanese sentences (Kudo and Kazawa, 2007). Several news wires are publicly available consisting of tens of million sentences. Kotonoha project is now constructing a balanced corpus of the presentday written Japanese consisting of 50 million words (Maekawa, 2006). 3 Construction of Case Frames Case frames describe t"
N09-1059,W03-1023,0,0.0493633,"six different sizes. Then, we applied these case frames to syntactic and case structure analysis, and zero anaphora resolution. We obtained better results by using case frames constructed from larger corpora; the performance was not saturated even with a corpus size of 100 billion words. 1 Introduction Very large corpora obtained from the Web have been successfully utilized for many natural language processing (NLP) applications, such as prepositional phrase (PP) attachment, other-anaphora resolution, spelling correction, confusable word set disambiguation and machine translation (Volk, 2001; Modjeska et al., 2003; Lapata and Keller, 2005; Atterer and Sch¨utze, 2006; Brants et al., 2007). Most of the previous work utilized only the surface information of the corpora, such as n-grams, co-occurrence counts, and simple surface syntax. This may be because these studies did not require structured knowledge, and for such studies, the size of currently available corpora is considered to have been almost enough. For instance, while Brants et al. (2007) reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens, the 521 incre"
N09-1059,P08-1052,0,0.0187231,"mputational Linguistics relationships between the corpus size and the performance of these analyses. 2 Related Work Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data size improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; they found that the combined system only improved the performance of the parser for small training sets. Brants et al. (2007) varied the amount of language model training data from 13 mil"
N09-1059,I08-2080,1,0.879813,"Missing"
N09-1059,C08-1097,1,0.538794,"construct case frames from the NErecognized corpus. Similar to the categories, for each NE class, we calculate the NE ratio among all the case slot examples, and add it to the case slot (e.g. [NE:PERSON]:0.12). The generalized examples are also included in Table 1. 4 Discourse Analysis with Case Frames In order to investigate the effect of corpus size on complex NLP tasks, we apply the constructed cases frames to an integrated probabilistic model for Japanese syntactic and case structure analysis (Kawahara and Kurohashi, 2006a) and a probabilistic model for Japanese zero anaphora resolution (Sasano et al., 2008). In this section, we brieﬂy describe these models. 4.1 Model for Syntactic and Case Structure Analysis Kawahara and Kurohashi (2006a) proposed an integrated probabilistic model for Japanese syntactic and case structure analysis based upon case frames. Case structure analysis recognizes predicate argument structures. Their model gives a probability to each possible syntactic structure T and case structure L of the input sentence S, and outputs the syntactic and case structure that have the highest probability. That is to say, the system selects the syntactic structure Tbest and the case struct"
N09-1059,P08-1076,0,0.0205197,"improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; they found that the combined system only improved the performance of the parser for small training sets. Brants et al. (2007) varied the amount of language model training data from 13 million to 2 trillion tokens and applied these models to machine translation systems. They reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens. Suzuki and Isozaki (2008) provided evidence that the use of more unlabeled data in semisupervised learning could improve the performance of NLP tasks, such as POS tagging, syntactic chunking, and named entities recognition. There are several methods to extract useful information from very large corpora. Search engines, such as Google and Altavista, are often used to obtain Web counts (e.g. (Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have"
N18-2041,D14-1181,0,0.00413864,"ng the explicit and implicit sentiment in the financial text, de Kauter et al. (2015) used a fine-grained sentiment annotation scheme. Kumar et al. (2017) used a classical supervised approach based on Support Vector Regression for sentiment analysis in financial domain. Oliveira et al. (2013) relied on multiple regression models. Akhtar et al. (2017) used an ensemble of four different systems for predicting the sentiment. It used a combination of Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), Convolutional Neural Network (CNN) (Kim, 2014) and Support Vector Regression (SVR) (Smola and Sch¨olkopf, 2004). Yang et al. (2016) used a hierarchical attention network to build the document representation incrementally for document classification. Our model focuses on interpretability and usage of knowledge bases. Knowledge bases have been recognized important for natural language understanding tasks (Minsky, 1986). Our main contribution is a two-layered attention network which utilizes background knowledge bases to build good word level representation at the primary level. The secondary attention mechanism works on top of the primary l"
N18-2041,baccianella-etal-2010-sentiwordnet,0,0.0675258,"Neural Network. It handles the long-term dependencies where the current output is dependent on many prior inputs. BiLSTM, in essence, is a combination of two different LSTM - one working in forward and the other working in the backward direction. The contextual information about both past and future helps in determining the current output. αt ∝ hcTt Ws us H= X t αt hbt (4) (5) where Ws is a parameter matrix and us is the context vector to be learned. H is finally fed to a one layer feed forward neural network. 254 2.2 Relevant Terms and Embeddings lexicon (Wilson et al., 2005), SentiWordNet (Baccianella et al., 2010) and Vader sentiment (Gilbert, 2014). From the above lexicons we extracted the agreement score (Rao and Srivastava, 2012) and the count of the number of occurrences of all positive and negative words in the text. - Word embedding: We use the 300-dimensional pre-trained Word2Vec and GloVe embedding. The sentence embedding was obtained by concatenating the embedding for all words in the sentence. External knowledge can provide explicit information for the model which the training data lacks. This helps the model to make better predictions. We relied on Knowledge Graph Embeddings based on WordNet"
N18-2041,W14-4012,0,0.0165288,"Missing"
N18-2041,S17-2152,0,0.0447809,"Missing"
N18-2041,S17-2138,0,0.0160322,"et al., 2014) model. An example of the DT expansion of the word ’touchpad’ is mouse, trackball, joystick and trackpad. 2.3 Experiments https://wordnet.princeton.edu 255 3.3 Models Single systems Mansar et al. (Team Fortia-FBK) Akhtar et al. - LSTM Akhtar et al. - GRU Akhtar et al. - CNN L3 (proposed) Ensembled systems Lan et al. (Team ECNU) Akhtar et al. E1 (proposed) Results We compare our system with the state-of-the-art systems of SemEval 2017 Task 5 and the system proposed by Akhtar et al. (2017). Table 1 shows evaluation of our various models. Team ECNU (Lan et al., 2017) and Fortia-FBK (Mansar et al., 2017) were the top systems for sub-tracks 1 and 2 respectively. Team ECNU and Fortia-FBK reported a cosine similarity of 0.777 and 0.745 for sub-tracks 1 and 2 respectively. Team ECNU employed a number of systems - Support Vector Regression, XGBoost Regressor, AdaBoost Regressor and Bagging Regressor ensembled together. Team Fortia-FBK used a Convolutional Neural Network for this task. The system proposed by Akhtar et al. utilizes an ensemble of LSTM, GRU, CNN and a SVR and reported a cosine similarity of 0.797 and 0.786 for the two sub-tracks. Our proposed system has a cosine similarity of 0.794 a"
N18-2041,D14-1162,0,0.0809956,"d test instances respectively. The task was to predict a regression score in between -1 and 1 indicating the sentiment with -1 being negative and +1 being positive. 2.2.2 Distributional Thesaurus Distributional Thesaurus (DT) (Biemann and Riedl, 2013) is an automatically computed word list which ranks words according to their semantic similarity. We use a pre-trained DT to expand a current word. For each current word, top-4 target words are found which are the relevant terms. The relevant embeddings are obtained by using a 300-dimensional pre-trained Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) model. An example of the DT expansion of the word ’touchpad’ is mouse, trackball, joystick and trackpad. 2.3 Experiments https://wordnet.princeton.edu 255 3.3 Models Single systems Mansar et al. (Team Fortia-FBK) Akhtar et al. - LSTM Akhtar et al. - GRU Akhtar et al. - CNN L3 (proposed) Ensembled systems Lan et al. (Team ECNU) Akhtar et al. E1 (proposed) Results We compare our system with the state-of-the-art systems of SemEval 2017 Task 5 and the system proposed by Akhtar et al. (2017). Table 1 shows evaluation of our various models. Team ECNU (Lan et al., 2017) and Fortia-FBK (Mansar et al."
N18-2041,P13-2005,0,0.0343581,"he-art system at SemEval 2017 Task 5 by 1.7 and 3.7 points for sub-tracks 1 and 2 respectively. 1 Introduction With the rise of microblogging websites, people have access and option to reach to the large crowd using as few words as possible. Microblog and news headlines are one of the common ways to dispense information online. The dynamic nature of these texts can be effectively used in the financial domain to track and predict the stock prices (Goonatilake and Herath, 2007). These can be used by an individual or an organization to make an informed prediction related to any company or stock (Si et al., 2013). This gives rise to an interesting problem of sentiment analysis in financial domain. A study indicates that sentiment analysis of public mood derived from Twitter feeds can be used to eventually forecast movements of individual stock prices (Smailovi´c et al., 2014). An efficient system for sentiment analysis is a core component of a company involved in financial stock market price prediction. 253 Proceedings of NAACL-HLT 2018, pages 253–258 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 → − ← − The two hidden states ht and ht for forward and bac"
N18-2041,H05-1044,0,0.0997265,"LSTM) is a special kind of Recurrent Neural Network. It handles the long-term dependencies where the current output is dependent on many prior inputs. BiLSTM, in essence, is a combination of two different LSTM - one working in forward and the other working in the backward direction. The contextual information about both past and future helps in determining the current output. αt ∝ hcTt Ws us H= X t αt hbt (4) (5) where Ws is a parameter matrix and us is the context vector to be learned. H is finally fed to a one layer feed forward neural network. 254 2.2 Relevant Terms and Embeddings lexicon (Wilson et al., 2005), SentiWordNet (Baccianella et al., 2010) and Vader sentiment (Gilbert, 2014). From the above lexicons we extracted the agreement score (Rao and Srivastava, 2012) and the count of the number of occurrences of all positive and negative words in the text. - Word embedding: We use the 300-dimensional pre-trained Word2Vec and GloVe embedding. The sentence embedding was obtained by concatenating the embedding for all words in the sentence. External knowledge can provide explicit information for the model which the training data lacks. This helps the model to make better predictions. We relied on Kn"
N18-2041,N16-1174,0,0.085616,". (2015) used a fine-grained sentiment annotation scheme. Kumar et al. (2017) used a classical supervised approach based on Support Vector Regression for sentiment analysis in financial domain. Oliveira et al. (2013) relied on multiple regression models. Akhtar et al. (2017) used an ensemble of four different systems for predicting the sentiment. It used a combination of Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), Convolutional Neural Network (CNN) (Kim, 2014) and Support Vector Regression (SVR) (Smola and Sch¨olkopf, 2004). Yang et al. (2016) used a hierarchical attention network to build the document representation incrementally for document classification. Our model focuses on interpretability and usage of knowledge bases. Knowledge bases have been recognized important for natural language understanding tasks (Minsky, 1986). Our main contribution is a two-layered attention network which utilizes background knowledge bases to build good word level representation at the primary level. The secondary attention mechanism works on top of the primary layer to build meaningful sentence representations. This provides a good intuitive wor"
N18-2041,S17-2089,0,\N,Missing
N19-1281,D07-1090,0,0.0345957,"Neural models with word or n2744 Proceedings of NAACL-HLT 2019, pages 2744–2755 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics gram embeddings are even larger, easily reaching gigabytes. This makes it difficult to deploy MA in space-constrained environments such as mobile applications and browsers. It has been shown that simple or straightforward models can match or outperform complex models when using a large number of training data. For example, a straightforward backoff technique rivals a complicated smoothing technique for language models (Brants et al., 2007). Pretraining a bidirectional language model on a large dataset helps to solve a variety of NLP tasks (Devlin et al., 2018). Our approach is inspired by this line of work. Contributions We propose a very straightforward fully-neural morphological analyzer which uses only character unigrams as its input1 . Such an analyzer, when trained only on human-annotated gold data has low accuracy. However, when trained on a large amount of automatically tagged silver data, the analyzer rivals and even outperforms, albeit slightly, the bootstrapping analyzer. We conclude that there is no need for rich inp"
N19-1281,P16-1039,0,0.017769,"and performing heavyweight NN evaluation only after lightweight scoring by the linear model. Direct lattice-based approaches are not very popular for Chinese, but some are lattice-based in spirit. A line of work by Zhang and Clark (2008, 2010) builds the lattice dynamically from partial words, searching paths with a perceptron-based scorer and customized beam search. The dictionary is built dynamically from the training data as frequent word-tag pairs which help the system to prune unlikely POS tags for word candidates. One more variation on lattice-based approaches for Chinese is the work by Cai and Zhao (2016). In this work, a segmentation dictionary is used to construct a subnetwork, which combines character representations into word representations used for computing sentence-wise segmentation scores. This can be seen as explicitly learning dictionary information by a model. Resulting segmentation is still created from the start to the end by growing words one by one while performing beam search. The follow up (Cai et al., 2017) simplifies that model and shows that greedy search can be enough for estimating segmentation when using neural networks. Still, this line of work does not consider POS ta"
N19-1281,I13-1018,0,0.0162083,"tions. This also can be seen as dynamically constructing a lattice while performing the search in it at the same time. Lattice-based approaches are popular for the Japanese language. Most of the time, the lattice is based on words which are present in a segmentation dictionary and a rule-based component for handling out-of-dictionary words. Usually, there are no machine-learning components in lattice creation, but the scoring can be machine-learning based. We believe that the availability of high quality consistent morphological analysis dictionaries is the reason for that. Still, the work of Kaji and Kitsuregawa (2013) is a counterexample of a lattice-based approach for Japanese which uses a machine-learning component for creating the lattice. Traditional lattice-based approaches for Japanese use mostly POS tags or other hidden information accessible from the dictionary to score paths through the lattice. JUMAN (Kurohashi, 1994) is one of the first analyzers, which uses a hidden Markov model with manually-tuned weights for scoring. Lattice path scores are computed using connection weights for each pair of part of speech tags. Probably the most known and used morphological analyzer for Japanese is MeCab (Kud"
N19-1281,P17-2096,0,0.0165623,"frequent word-tag pairs which help the system to prune unlikely POS tags for word candidates. One more variation on lattice-based approaches for Chinese is the work by Cai and Zhao (2016). In this work, a segmentation dictionary is used to construct a subnetwork, which combines character representations into word representations used for computing sentence-wise segmentation scores. This can be seen as explicitly learning dictionary information by a model. Resulting segmentation is still created from the start to the end by growing words one by one while performing beam search. The follow up (Cai et al., 2017) simplifies that model and shows that greedy search can be enough for estimating segmentation when using neural networks. Still, this line of work does not consider POS tagging. Transition-based approaches treat input data (most frequently – characters) as input queue and store a current, possibly incomplete, token in a buffer. Models usually infer whether they should create a new token from a character in the input 2746 2 https://unidic.ninjal.ac.jp/ queue or append an input character to the already existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et"
N19-1281,D18-2012,0,0.0344,"e 1: Proposed model. We encode character unigram embeddings into shared representations for each character. The shared representation is projected into a tag-specific representations from which we independently infer segmentation and per-character tags. Introduction Languages with a continuous script, like Japanese and Chinese, do not have natural word boundaries in most cases. Natural language processing for such languages requires to perform some variation of word segmentation. Although some NLP applications, like neural machine translation, started to use unsupervised segmentation methods (Kudo and Richardson, 2018), resulting segmentation often has decisions which are not natural to humans. Supervised segmentation based on a human-defined standard is essential for applications which are designed for interaction on a word-level granularity, for example, full-text search. Segmentation is commonly done jointly with part of speech (POS) tagging and usually referred to as Morphological Analysis. Modern Japanese Morphological Analyzers (MA) are very accurate, having a >99 segmentation tokenwise F1 score on news domain and a >98.5 F1 on web domain (Tolmachev et al., 2018). They often use segmentation dictionar"
N19-1281,D16-1070,0,0.0255772,"train word and character n-gram embeddings. Yang et al. (2017) pretrain a part of the model on different data sources, including automatically segmented text, but the model itself is trained only on the gold data. Another approach is to use heterogeneous data (annotated in incompatible annotation standards). In addition to corpus statistics from a raw corpus, Zhao and Kit (2008) exploit heterogeneous annotations. Li et al. (2015) use corpora with different annotation standards. They combine tags into “bundles” (e.g. [NN, n]) and infer them at the same time while paying attention to ambiguity. Chen et al. (2016) train a classifier that can annotate several standards jointly. Finally, it is possible to use raw or automaticallyannotated data directly. A study (Suzuki and Isozaki, 2008) is an example of a feature-based algorithm which uses raw data. Tri-training (Zhou and Li, 2005) is a generic way to use raw data. They propose to train on automatically analyzed examples where two of three diverse analyzers agree. Søgaard (2010) show that tri-training helps English POS-tagging with SVM and MaxEntbased approaches. Zhou et al. (2017) use selftraining and tri-training for Chinese word segmentation. They, h"
N19-1281,W04-3230,0,0.0191528,"13) is a counterexample of a lattice-based approach for Japanese which uses a machine-learning component for creating the lattice. Traditional lattice-based approaches for Japanese use mostly POS tags or other hidden information accessible from the dictionary to score paths through the lattice. JUMAN (Kurohashi, 1994) is one of the first analyzers, which uses a hidden Markov model with manually-tuned weights for scoring. Lattice path scores are computed using connection weights for each pair of part of speech tags. Probably the most known and used morphological analyzer for Japanese is MeCab (Kudo et al., 2004), where CRFs were used for learning the scoring. MeCab is very fast: it can analyze almost 50k sentences per second. It also achieves acceptable accuracy, and so the tool is very popular. The speed is realized by precomputing feature weights, but it takes a lot of space when the total number of features gets large. For example, the UniDic model for modern Japanese v2.3.02 takes 5.5GB because it uses many feature templates. There were studies which tried to integrate NN into lattice-based approaches as well. Juman++ (Morita et al., 2015) uses dictionary-based lattice construction with the combi"
N19-1281,N19-1423,0,0.0470242,"Missing"
N19-1281,Y12-1058,1,0.800125,"on and 2 for the first POS tag layer. 4 Experiments We conduct experiments on Japanese morphological analysis. For training we use two data sources. The first is usual human-annotated gold training data. The second is silver data from the results of automatic analysis. We use Juman++ V2 – the current state-of-the-art analyzer for the JUMAN segmentation standard as the bootstrap analyzer. We use two gold corpora. The first is the Kyoto University Text Corpus (Kurohashi and Nagao (2003), referred to as KU), containing newspaper data. The second is the Kyoto University Web Document Leads Corpus (Hangyo et al. (2012), referred to as Leads) which consists of web documents. Corpus statistics are shown in Table 1. We denote models which use gold training data by G. We take raw data to generate our silver annotated data from a crawled web corpus of 9.8B unique sentences. We sample 3B sentences randomly from it and analyze them using the Juman++ baseline model. From it we sample 500M sentences, which become our training silver data, prioritizing sentences which contain at least one not very frequent word. We prepare both top-scored (denoted as T) and non-ambigous in beam (denoted as B) variants of the silver d"
N19-1281,P12-1110,0,0.0271262,"d approaches treat input data (most frequently – characters) as input queue and store a current, possibly incomplete, token in a buffer. Models usually infer whether they should create a new token from a character in the input 2746 2 https://unidic.ninjal.ac.jp/ queue or append an input character to the already existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as features. They precomute the features beforehand and train an analyzer afterwards. In addition to that, Zhang et al. (2013) use a variation of smoothing for handling automatic annotation errors. A lot of neural-based methods"
N19-1281,P17-1111,1,0.788175,"put data (most frequently – characters) as input queue and store a current, possibly incomplete, token in a buffer. Models usually infer whether they should create a new token from a character in the input 2746 2 https://unidic.ninjal.ac.jp/ queue or append an input character to the already existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as features. They precomute the features beforehand and train an analyzer afterwards. In addition to that, Zhang et al. (2013) use a variation of smoothing for handling automatic annotation errors. A lot of neural-based methods pretrain word and cha"
N19-1281,P15-1172,0,0.0273814,"and and train an analyzer afterwards. In addition to that, Zhang et al. (2013) use a variation of smoothing for handling automatic annotation errors. A lot of neural-based methods pretrain word and character n-gram embeddings. Yang et al. (2017) pretrain a part of the model on different data sources, including automatically segmented text, but the model itself is trained only on the gold data. Another approach is to use heterogeneous data (annotated in incompatible annotation standards). In addition to corpus statistics from a raw corpus, Zhao and Kit (2008) exploit heterogeneous annotations. Li et al. (2015) use corpora with different annotation standards. They combine tags into “bundles” (e.g. [NN, n]) and infer them at the same time while paying attention to ambiguity. Chen et al. (2016) train a classifier that can annotate several standards jointly. Finally, it is possible to use raw or automaticallyannotated data directly. A study (Suzuki and Isozaki, 2008) is an example of a feature-based algorithm which uses raw data. Tri-training (Zhou and Li, 2005) is a generic way to use raw data. They propose to train on automatically analyzed examples where two of three diverse analyzers agree. Søgaard"
N19-1281,D11-1090,0,0.0311703,"ready existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as features. They precomute the features beforehand and train an analyzer afterwards. In addition to that, Zhang et al. (2013) use a variation of smoothing for handling automatic annotation errors. A lot of neural-based methods pretrain word and character n-gram embeddings. Yang et al. (2017) pretrain a part of the model on different data sources, including automatically segmented text, but the model itself is trained only on the gold data. Another approach is to use heterogeneous data (annotated in incompatible annotation sta"
N19-1281,D18-1529,0,0.0249978,"Missing"
N19-1281,P08-1076,0,0.0539664,"odel itself is trained only on the gold data. Another approach is to use heterogeneous data (annotated in incompatible annotation standards). In addition to corpus statistics from a raw corpus, Zhao and Kit (2008) exploit heterogeneous annotations. Li et al. (2015) use corpora with different annotation standards. They combine tags into “bundles” (e.g. [NN, n]) and infer them at the same time while paying attention to ambiguity. Chen et al. (2016) train a classifier that can annotate several standards jointly. Finally, it is possible to use raw or automaticallyannotated data directly. A study (Suzuki and Isozaki, 2008) is an example of a feature-based algorithm which uses raw data. Tri-training (Zhou and Li, 2005) is a generic way to use raw data. They propose to train on automatically analyzed examples where two of three diverse analyzers agree. Søgaard (2010) show that tri-training helps English POS-tagging with SVM and MaxEntbased approaches. Zhou et al. (2017) use selftraining and tri-training for Chinese word segmentation. They, however, also pretrain other features like word-context character embeddings, chraracter unigrams and bigrams. 3 Proposed Approach In order for MA to be practical, it should be"
N19-1281,P15-1167,0,0.0198254,"he follow up (Cai et al., 2017) simplifies that model and shows that greedy search can be enough for estimating segmentation when using neural networks. Still, this line of work does not consider POS tagging. Transition-based approaches treat input data (most frequently – characters) as input queue and store a current, possibly incomplete, token in a buffer. Models usually infer whether they should create a new token from a character in the input 2746 2 https://unidic.ninjal.ac.jp/ queue or append an input character to the already existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as fe"
N19-1281,D15-1276,1,0.858442,"nown and used morphological analyzer for Japanese is MeCab (Kudo et al., 2004), where CRFs were used for learning the scoring. MeCab is very fast: it can analyze almost 50k sentences per second. It also achieves acceptable accuracy, and so the tool is very popular. The speed is realized by precomputing feature weights, but it takes a lot of space when the total number of features gets large. For example, the UniDic model for modern Japanese v2.3.02 takes 5.5GB because it uses many feature templates. There were studies which tried to integrate NN into lattice-based approaches as well. Juman++ (Morita et al., 2015) uses dictionary-based lattice construction with the combination of two models for path scoring: the feature-based linear model using soft-confidence weighted learning (Wang et al., 2016) and a recurrent neural network (Mikolov, 2012). It significantly reduced the number of both segmentation and POS tagging errors. However, it was very slow, being able to analyze only about 15 sentences per second, hence the original version was impractical. The following improvement (Tolmachev et al., 2018) greatly increased analysis speed by doing aggressive beam trimming and performing heavyweight NN evalua"
N19-1281,P11-2093,0,0.0265267,"hould be possible to segment a sentence using the dictionary entries only in a single correct way. Such dictionaries are often maintained together with annotated corpora. On the other hand, Chinese-focused systems do not put much focus on dictionaries. Still, almost all aproaches use rich feature templates or additional resources such as pretrained character n-gram or word embeddings, which increase the model size. Pointwise approaches make a segmentation decision independently for each position. They can be seen as a sequence tagging task. Such approaches are more popular for Chinese. KyTea (Neubig et al., 2011) is an example of this approach in Japanese. It makes a binary decision for each character: whether to insert a boundary before it or not. It can be seen as sequence tagging with {B, I} tagset. POS tagging is done after inferring segmentation. The decisions are made by feature-based approaches, using characters, character n-grams, character type information, and dictionary information as features. KyTea can use word features obtained from a dictionary. It checks whether the character sequence before and after the current character forms a word from the dictionary. It also checks whether the cu"
N19-1281,C04-1081,0,0.105546,"ictionary. It checks whether the character sequence before and after the current character forms a word from the dictionary. It also checks whether the current word is inside a word. Neural networks were shown to be useful for Japanese in this paradigm as well (Kitagawa and Komachi, 2017). They use character embeddings, character type embeddings, character n-gram embeddings, and tricks to incorporate dictionary information into the model. Many studies on Chinese adopt the pointwise approach. Often, the segmentation task is reformulated as sequence tagging (Xue, 2003) with {B, I, E, S} tagset. Peng et al. (2004) showed that CRFs help further in this task. This tactic was followed by many subsequent feature-based approaches (Tseng et al., 2005; Zhao et al., 2006; Zhang et al., 2013), using character n-gram, character type and word features. Neural networks were applied to this paradigm as well. Zheng et al. (2013) used a feed-forward network on character and categorical features that were shown to be useful for computing a segmentation score from a fixed window. Qi et al. (2014) used a similar architecture. They predicted not only segmentation but POS tags and performed 2745 named entity recognition a"
N19-1281,I17-1018,0,0.0180553,"sequent feature-based approaches (Tseng et al., 2005; Zhao et al., 2006; Zhang et al., 2013), using character n-gram, character type and word features. Neural networks were applied to this paradigm as well. Zheng et al. (2013) used a feed-forward network on character and categorical features that were shown to be useful for computing a segmentation score from a fixed window. Qi et al. (2014) used a similar architecture. They predicted not only segmentation but POS tags and performed 2745 named entity recognition as well. The character representation was pretrained on a language modeling task. Shao et al. (2017) used a bidirectional recurrent network with GRU cells followed by a CRF layer for joint segmentation and POS tagging. They used pretrained character n-gram embeddings together with sub-character level information extracted by CNNs as features. Using a dictionary with NN is also popular (Zhang et al., 2018b; Liu et al., 2018). Search-based approaches induce a structure over a sentence and perform a search over it. A most frequently used structure is a lattice which contains all possible segmentation tokens. The search then finds the highest scoring path through the lattice. Another branch of s"
N19-1281,P10-2038,0,0.024593,"(2015) use corpora with different annotation standards. They combine tags into “bundles” (e.g. [NN, n]) and infer them at the same time while paying attention to ambiguity. Chen et al. (2016) train a classifier that can annotate several standards jointly. Finally, it is possible to use raw or automaticallyannotated data directly. A study (Suzuki and Isozaki, 2008) is an example of a feature-based algorithm which uses raw data. Tri-training (Zhou and Li, 2005) is a generic way to use raw data. They propose to train on automatically analyzed examples where two of three diverse analyzers agree. Søgaard (2010) show that tri-training helps English POS-tagging with SVM and MaxEntbased approaches. Zhou et al. (2017) use selftraining and tri-training for Chinese word segmentation. They, however, also pretrain other features like word-context character embeddings, chraracter unigrams and bigrams. 3 Proposed Approach In order for MA to be practical, it should be not only accurate, but also fast and have relatively compact models. The speed of search-based approaches is dependent on how computationally heavy a weighting function is. Heavyweight models, like neural networks, require a large number of compu"
N19-1281,D18-2010,1,0.918019,"unsupervised segmentation methods (Kudo and Richardson, 2018), resulting segmentation often has decisions which are not natural to humans. Supervised segmentation based on a human-defined standard is essential for applications which are designed for interaction on a word-level granularity, for example, full-text search. Segmentation is commonly done jointly with part of speech (POS) tagging and usually referred to as Morphological Analysis. Modern Japanese Morphological Analyzers (MA) are very accurate, having a >99 segmentation tokenwise F1 score on news domain and a >98.5 F1 on web domain (Tolmachev et al., 2018). They often use segmentation dictionaries which define possible words. Also, their models are generally large and unwieldy, spanning hundreds of megabytes in case of traditional symbolic featurebased approaches. Neural models with word or n2744 Proceedings of NAACL-HLT 2019, pages 2744–2755 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics gram embeddings are even larger, easily reaching gigabytes. This makes it difficult to deploy MA in space-constrained environments such as mobile applications and browsers. It has been shown that simple or strai"
N19-1281,I05-3027,0,0.0633992,"checks whether the current word is inside a word. Neural networks were shown to be useful for Japanese in this paradigm as well (Kitagawa and Komachi, 2017). They use character embeddings, character type embeddings, character n-gram embeddings, and tricks to incorporate dictionary information into the model. Many studies on Chinese adopt the pointwise approach. Often, the segmentation task is reformulated as sequence tagging (Xue, 2003) with {B, I, E, S} tagset. Peng et al. (2004) showed that CRFs help further in this task. This tactic was followed by many subsequent feature-based approaches (Tseng et al., 2005; Zhao et al., 2006; Zhang et al., 2013), using character n-gram, character type and word features. Neural networks were applied to this paradigm as well. Zheng et al. (2013) used a feed-forward network on character and categorical features that were shown to be useful for computing a segmentation score from a fixed window. Qi et al. (2014) used a similar architecture. They predicted not only segmentation but POS tags and performed 2745 named entity recognition as well. The character representation was pretrained on a language modeling task. Shao et al. (2017) used a bidirectional recurrent ne"
N19-1281,I11-1035,0,0.015456,"Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as features. They precomute the features beforehand and train an analyzer afterwards. In addition to that, Zhang et al. (2013) use a variation of smoothing for handling automatic annotation errors. A lot of neural-based methods pretrain word and character n-gram embeddings. Yang et al. (2017) pretrain a part of the model on different data sources, including automatically segmented text, but the model itself is trained only on the gold data. Another approach is to use heterogeneous data (annotated in incompatible annotation standards). In addition to"
N19-1281,O03-4002,0,0.176261,"n use word features obtained from a dictionary. It checks whether the character sequence before and after the current character forms a word from the dictionary. It also checks whether the current word is inside a word. Neural networks were shown to be useful for Japanese in this paradigm as well (Kitagawa and Komachi, 2017). They use character embeddings, character type embeddings, character n-gram embeddings, and tricks to incorporate dictionary information into the model. Many studies on Chinese adopt the pointwise approach. Often, the segmentation task is reformulated as sequence tagging (Xue, 2003) with {B, I, E, S} tagset. Peng et al. (2004) showed that CRFs help further in this task. This tactic was followed by many subsequent feature-based approaches (Tseng et al., 2005; Zhao et al., 2006; Zhang et al., 2013), using character n-gram, character type and word features. Neural networks were applied to this paradigm as well. Zheng et al. (2013) used a feed-forward network on character and categorical features that were shown to be useful for computing a segmentation score from a fixed window. Qi et al. (2014) used a similar architecture. They predicted not only segmentation but POS tags"
N19-1281,P17-1078,0,0.015059,"that model and shows that greedy search can be enough for estimating segmentation when using neural networks. Still, this line of work does not consider POS tagging. Transition-based approaches treat input data (most frequently – characters) as input queue and store a current, possibly incomplete, token in a buffer. Models usually infer whether they should create a new token from a character in the input 2746 2 https://unidic.ninjal.ac.jp/ queue or append an input character to the already existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as features. They precomute the features bef"
N19-1281,D13-1031,0,0.143383,"de a word. Neural networks were shown to be useful for Japanese in this paradigm as well (Kitagawa and Komachi, 2017). They use character embeddings, character type embeddings, character n-gram embeddings, and tricks to incorporate dictionary information into the model. Many studies on Chinese adopt the pointwise approach. Often, the segmentation task is reformulated as sequence tagging (Xue, 2003) with {B, I, E, S} tagset. Peng et al. (2004) showed that CRFs help further in this task. This tactic was followed by many subsequent feature-based approaches (Tseng et al., 2005; Zhao et al., 2006; Zhang et al., 2013), using character n-gram, character type and word features. Neural networks were applied to this paradigm as well. Zheng et al. (2013) used a feed-forward network on character and categorical features that were shown to be useful for computing a segmentation score from a fixed window. Qi et al. (2014) used a similar architecture. They predicted not only segmentation but POS tags and performed 2745 named entity recognition as well. The character representation was pretrained on a language modeling task. Shao et al. (2017) used a bidirectional recurrent network with GRU cells followed by a CRF l"
N19-1281,D12-1046,0,0.0612836,"Missing"
N19-1281,D17-1079,0,0.0183386,"n]) and infer them at the same time while paying attention to ambiguity. Chen et al. (2016) train a classifier that can annotate several standards jointly. Finally, it is possible to use raw or automaticallyannotated data directly. A study (Suzuki and Isozaki, 2008) is an example of a feature-based algorithm which uses raw data. Tri-training (Zhou and Li, 2005) is a generic way to use raw data. They propose to train on automatically analyzed examples where two of three diverse analyzers agree. Søgaard (2010) show that tri-training helps English POS-tagging with SVM and MaxEntbased approaches. Zhou et al. (2017) use selftraining and tri-training for Chinese word segmentation. They, however, also pretrain other features like word-context character embeddings, chraracter unigrams and bigrams. 3 Proposed Approach In order for MA to be practical, it should be not only accurate, but also fast and have relatively compact models. The speed of search-based approaches is dependent on how computationally heavy a weighting function is. Heavyweight models, like neural networks, require a large number of computations, and we think that it will be very difficult to create a practical search-based fully NN morpholo"
N19-1281,P16-1040,0,0.0155005,"., 2017) simplifies that model and shows that greedy search can be enough for estimating segmentation when using neural networks. Still, this line of work does not consider POS tagging. Transition-based approaches treat input data (most frequently – characters) as input queue and store a current, possibly incomplete, token in a buffer. Models usually infer whether they should create a new token from a character in the input 2746 2 https://unidic.ninjal.ac.jp/ queue or append an input character to the already existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as features. They precomu"
N19-1281,P08-1101,0,0.0439278,"., 2016) and a recurrent neural network (Mikolov, 2012). It significantly reduced the number of both segmentation and POS tagging errors. However, it was very slow, being able to analyze only about 15 sentences per second, hence the original version was impractical. The following improvement (Tolmachev et al., 2018) greatly increased analysis speed by doing aggressive beam trimming and performing heavyweight NN evaluation only after lightweight scoring by the linear model. Direct lattice-based approaches are not very popular for Chinese, but some are lattice-based in spirit. A line of work by Zhang and Clark (2008, 2010) builds the lattice dynamically from partial words, searching paths with a perceptron-based scorer and customized beam search. The dictionary is built dynamically from the training data as frequent word-tag pairs which help the system to prune unlikely POS tags for word candidates. One more variation on lattice-based approaches for Chinese is the work by Cai and Zhao (2016). In this work, a segmentation dictionary is used to construct a subnetwork, which combines character representations into word representations used for computing sentence-wise segmentation scores. This can be seen as"
N19-1281,D10-1082,0,0.0710804,"Missing"
N19-1281,Y06-1012,0,0.0229629,"urrent word is inside a word. Neural networks were shown to be useful for Japanese in this paradigm as well (Kitagawa and Komachi, 2017). They use character embeddings, character type embeddings, character n-gram embeddings, and tricks to incorporate dictionary information into the model. Many studies on Chinese adopt the pointwise approach. Often, the segmentation task is reformulated as sequence tagging (Xue, 2003) with {B, I, E, S} tagset. Peng et al. (2004) showed that CRFs help further in this task. This tactic was followed by many subsequent feature-based approaches (Tseng et al., 2005; Zhao et al., 2006; Zhang et al., 2013), using character n-gram, character type and word features. Neural networks were applied to this paradigm as well. Zheng et al. (2013) used a feed-forward network on character and categorical features that were shown to be useful for computing a segmentation score from a fixed window. Qi et al. (2014) used a similar architecture. They predicted not only segmentation but POS tags and performed 2745 named entity recognition as well. The character representation was pretrained on a language modeling task. Shao et al. (2017) used a bidirectional recurrent network with GRU cell"
P02-1028,S01-1004,0,\N,Missing
P02-1028,J94-4001,1,\N,Missing
P02-1028,S01-1008,0,\N,Missing
P02-1028,H01-1043,1,\N,Missing
P02-1028,P01-1008,0,\N,Missing
P07-2052,A00-2018,0,0.248206,"Missing"
P07-2052,N06-1021,0,0.0246711,"Missing"
P07-2052,P05-1067,0,0.0124204,"rd on just phrasal and part-of-speech categories (Bikel, 2004). This paper describes our investigation into the effectiveness of lexicalization in dependency parsing 205 instead of phrase-structure parsing. Usual dependency parsing cannot utilize phrase categories, and thus relies on word information like parts of speech and lexicalized words. Therefore, we want to know the performance of dependency parsers that have minimal or low lexicalization. Dependency trees have been used in a variety of NLP applications, such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). For such applications, a fast, efﬁcient and accurate dependency parser is required to obtain dependency trees from a large corpus. From this point of view, minimally lexicalized parsers have advantages over fully lexicalized ones in parsing speed and memory consumption. We examined the change in performance of dependency parsing by varying the degree of lexicalization. The degree of lexicalization is speciﬁed by giving a list of words to be lexicalized, which appear in a training corpus. For minimal lexicalization, we used a short list that consists of only high-frequency words, and for maxi"
P07-2052,C04-1040,0,0.0395821,"Missing"
P07-2052,P03-1054,0,0.0584778,"Missing"
P07-2052,N01-1025,0,0.156268,"Missing"
P07-2052,P05-1010,0,0.103473,"Missing"
P07-2052,E06-1011,0,0.0326959,"Missing"
P07-2052,P05-1012,0,0.0511429,"Missing"
P07-2052,C04-1010,0,0.0287052,"Missing"
P07-2052,P06-1055,0,0.088056,"Missing"
P07-2052,N03-1027,0,0.0436103,"Missing"
P07-2052,H05-1059,0,0.0238108,"Missing"
P07-2052,W03-3023,0,0.193547,"Missing"
P07-2052,J04-4004,0,\N,Missing
P07-2052,J03-4003,0,\N,Missing
P07-2052,P04-1054,0,\N,Missing
P09-4001,I08-1025,1,0.821789,"of information credibility. (1) Credibility of information contents, (2) Credibility of the information sender, and (3) Credibility estimated from the document style and superficial characteristics. In order to help people judge the credibility of information from these viewpoints, we have been developing an information analysis system called WISDOM. Figure 1 shows the analysis result of WISDOM on the analysis topic “Is bio-ethanol good for the environment?” Figure 2 shows the system architecture of WISDOM. Given an analysis topic (query), WISDOM sends the query to the search engine TSUBAKI (Shinzato et al., 2008), and TSUBAKI returns a list of the top N relevant Web pages (N is usually set to 1000). Then, those pages are automatically analyzed, and major and contradictory expressions and evaluative expressions are extracted. Furthermore, the information senders of the Web pages, which were analyzed beforehand, are collected and the distribution is calculated. The WISDOM analysis results can be viewed from several viewpoints by changing the tabs using a Web browser. The leftmost tab, “Summary,” shows the summary of the analysis, with major phrases and major/contradictory statements first. http://clusty"
P14-1097,D07-1018,0,0.0264447,"jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007). Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives. The most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering. In particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999). However, the verb itself is still represented as a single data point. Aft"
P14-1097,P03-1009,0,0.103917,"Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy. Our novel contributions are summarize"
P14-1097,korhonen-etal-2006-large,0,0.0813188,"5.70 64.18 94.66 78.64 99.30 77.38 96.25 F1 52.44 53.10 54.40 55.04 52.24 52.59 53.58 54.24 51.76 57.91 60.14 61.97 65.49 47.38 63.84 52.40 68.90 52.62 71.39 53.13 Table 2: Type-level multi-class evaluations. K represents the (average) number of induced classes. “S” denotes the use of slot-only features and “SW” denotes the use of slot-word pair features. For example, “SW-S” means that slot-word pair features are used for semantic frame induction and slotonly features are used for verb class induction. the evaluations of soft clusterings for their future work. For input data, we employ VALEX (Korhonen et al., 2006), which is a publicly-available large-scale subcategorization lexicon.5 By following the method of Korhonen et al. (2003), prepositional phrases (pp) are parameterized for two frequent subcategorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. It is necessary to specify the number of clusters, k, for the IB method beforehand, and we adopt 35 and 42 clusters according to their reported high accuracies. To output multiple classes for each verb, we set a threshold, t, for class attribute probabilities. That is,"
P14-1097,J04-1003,0,0.236831,"im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy. Our novel contributions are summarized as follows: • induce"
P14-1097,de-marneffe-etal-2006-generating,0,0.0182596,"Missing"
P14-1097,P12-1090,0,0.221374,"ng (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consi"
P14-1097,D11-1025,0,0.0208016,"Missing"
P14-1097,N06-1023,1,0.708505,"Missing"
P14-1097,E14-1007,1,0.921209,"wo clustering steps. Our procedure to automatically induce verb classes from verb uses is summarized as follows: 1. induce verb-specific semantic frames by clustering predicate-argument structures for each verb extracted from automatic parses as shown in the lower part of Figure 1, and 2. induce verb classes by clustering the induced semantic frames across verbs as shown in the upper part of Figure 1. Each of these two steps is described in the following sections in detail. 3.2 Inducing Verb-specific Semantic Frames We induce verb-specific semantic frames from verb uses based on the method of Kawahara et al. (2014). Our semantic frames consist of case slots, each of which consists of word instances that can be filled. The procedure for inducing these semantic frames is as follows: 1. apply dependency parsing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses, 2. merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and 1 In our replication experiment, it took a week to perform 70 iterations using Materna’s code and an Intel Xeon E5-2"
P14-1097,P08-1050,0,0.133537,"een used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im W"
P14-1097,P12-1044,0,0.0164185,"nson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering ste"
P14-1097,N13-1051,0,0.23495,"eir models. Schulte im Walde et al. (2008) also applied probabilistic soft clustering to verbs by incorporating subcategorization frames and selectional preferences based on WordNet. This model is based on the Expectation-Maximization algorithm and the Minimum Description Length principle. Since they focused on the incorporation of selectional preferences, they did not evaluate verb classes but evaluated only selectional preferences using a language model-based measure. Materna proposed LDA-frames, which are defined across verbs and can be considered to be a kind of verb class (Materna, 2012; Materna, 2013). LDA-frames are probabilistic semantic frames automatically induced from a raw corpus. He used a model based on latent Dirichlet allocation (LDA; Blei et al. (2003)) and the Dirichlet process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and roles. Both of these are represented as a probabilistic distribution of words across verbs. He applied this method to the BNC and acquired 1,200 frames and 400 roles (Materna, 2012). He did not evaluate the resulting frames as verb classes. In sum, there have been no studies that quantitatively evaluate polysemou"
P14-1097,D09-1138,0,0.0772066,"ed to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007). Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives. The most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods"
P14-1097,W12-1901,0,0.0523786,"ess mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standard test set, where a single class is assigned to a verb. Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features. Parisien and Stevenson (2011) extended their model by adding semantic features. They tried to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a v"
P14-1097,W03-0410,0,0.043177,"basis of their syntactic and semantic behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et a"
P14-1097,N09-1064,0,0.0478714,"Missing"
P14-1097,P13-1085,0,0.0170314,"cea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic fr"
P14-1097,W10-2911,0,0.0208275,"eans that semantic similarity based on word overlap is naturally considered by looking at lexical information. We will compare in our experiments four possible combinations: two feature representations for each of the two clustering steps. 4 Experiments and Evaluations We first describe our experimental settings and define evaluation metrics to evaluate induced soft clusterings of verb classes. Then, we conduct type-level multi-class evaluations, type-level single-class evaluations and token-level multiclass evaluations. These two levels of evaluations are performed by considering the work of Reichart et al. (2010) on clustering evaluation. Finally, we discuss the results of our full experiments. 4.1 Experimental Settings We use two kinds of large-scale corpora: a web corpus and the English Gigaword corpus. To prepare a web corpus, we extracted sentences from crawled web pages that are judged to be written in English based on the encoding information. Then, we selected sentences that consist of at most 40 words, and removed duplicated sentences. From this process, we obtained a corpus of one billion sentences, totaling approximately 20 billion words. We focused on verbs whose frequency in the web corpus"
P14-1097,N09-1059,1,0.845519,"tevenson applied HDP only to a small-scale child speech corpus that contains 170K verb uses to jointly induce subcategorization frames and verb classes (Parisien and Stevenson, 2010; Parisien and Stevenson, 2011). Materna applied an LDA-based method to the BNC, which contains 1.4M verb uses, to induce semantic frames across verbs that can be considered to be verb classes (Materna, 2012; Materna, 2013). However, it would take three months for this experiment using this 100 million word corpus.1 Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models. In this paper, we propose a two-step approach for inducing semantic frames and verb classes. First, we make multiple data points for each verb to deal with verb polysemy (cf. polysemy-aware previous studies still represented a verb as one data point (Korhonen et al., 2003; Miyao and Tsujii, 2009)). To do that, we induce verb-specific semantic frames by clustering verb uses. Then, we induce verb classes by clustering these verbspecific semantic frames across verbs. An interesting point here is that we can use exactly the"
P14-1097,P08-1057,0,0.159318,"8; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy. Our novel contributions are summarized as follows: • induce both semantic frames and verb classes from a massiv"
P14-1097,J06-2001,0,0.0724373,"behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Bre"
P14-1097,C10-1113,0,0.0549359,"Missing"
P14-1097,I13-1072,0,0.0156675,"e precision and recall of a clustering, modified purity and inverse purity (also called collocation or weighted class accuracy) are commonly used in previous studies on verb clustering (e.g., Sun and Korhonen (2009)). However, since these measures are only applicable to a hard clustering, it is necessary to extend them to be applicable to a soft clustering, because in our task a verb can belong to multiple clusters or classes.4 We propose a normalized version of modified purity and inverse purity. This kind of normalization for soft clusterings was performed for other evaluation metrics as in Springorum et al. (2013). To measure the precision of a clustering, a normalized version of modified purity is defined as follows. Suppose K is the set of automatically induced clusters and G is the set of gold classes. Let Ki be the verb vector of the i-th cluster and Gj be the verb vector of the j-th gold class. Each component of these vectors is a normalized frequency, which equals a cluster/class attribute probability given a verb. Where there is no frequency information available for class distribution, such as the gold-standard data described in Section 4.3, we use a uniform distribution across the verb’s class"
P14-1097,D09-1067,0,0.493143,"t need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we prop"
P14-1097,I08-2107,0,0.0199328,"P applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008)"
P14-1097,P13-2129,0,0.0829034,"ing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first indu"
P14-1097,H05-1111,0,0.0376783,"uring the sense of a verb is essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP. Verb classes are one such lexical resource. Manually-crafted verb classes have been developed, such as Levin’s classes (Levin, 1993) and their extension, VerbNet (Kipper-Schuler, 2005), in which verbs are organized into classes on the basis of their syntactic and semantic behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott"
P14-1097,E12-1003,0,0.0683136,"cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standard test set, where a single class is assigned to a verb. Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features. Parisien and Stevenson (2011) extended their model by adding semantic features. They tried to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a verb’s syntactic and semantic features, and learns"
P14-1097,W09-0210,0,0.294698,"tics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised me"
P14-1097,H93-1052,0,0.423358,"wing sections in detail. 3.2 Inducing Verb-specific Semantic Frames We induce verb-specific semantic frames from verb uses based on the method of Kawahara et al. (2014). Our semantic frames consist of case slots, each of which consists of word instances that can be filled. The procedure for inducing these semantic frames is as follows: 1. apply dependency parsing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses, 2. merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and 1 In our replication experiment, it took a week to perform 70 iterations using Materna’s code and an Intel Xeon E5-2680 (2.7GHz) CPU. To reach 1,000 iterations, which are reported to be optimum, it would take three months. 1032 3. apply clustering to the initial frames based on the Chinese Restaurant Process (Aldous, 1985) to produce verb-specific semantic frames. These three steps are briefly described below. 3.2.1 Extracting Predicate-argument Structures from a Raw Corpus We apply dependency parsing to a large raw corpus. We use the Stanford parser with S"
P14-2042,W02-1001,0,0.14194,"undaries. The lower part of the lattice (character-level nodes) represents unknown words, where each node carries a position-of-character tag, in addition to other types of information that can also be found on a wordlevel node. A sequence of character-level nodes are considered as an unknown word if and only if the sequence of POC tags forms one of the cases listed in Table 3. This table also illustrates the permitted transitions between adjacent characterlevel nodes. We use the standard dynamic programming technique to search for the best path in the lattice. We use the averaged perceptron (Collins, 2002), an efficient online learning algorithm, to train the model. 3.2 Features We show the feature templates of our model in Table 4. The features consist of two categories: baseline features, which are modified from the templates proposed in (Kruengkrai et al., 2009); and proposed features, which encode characterlevel POS information. Baseline features: For word-level nodes that represent known words, we use the symbols , and to denote the word form, POS tag and length of the word, respectively. The functions and return the first and last character of . If has only one character, we omit the temp"
P14-2042,P08-1102,0,0.353209,"s, the function returns its character-level POS. The subscript indices 0 and -1 as well as 256 (a) Word Segmentation Results System P R Baseline 97.48 98.44 CharPOS 97.55 98.51 other symbols stand for the same meaning as they are in the baseline features. 4 4.1 Evaluation Settings (b) Joint Segmentation and POS Tagging Results System P R F Baseline 93.01 93.95 93.48 CharPOS 93.42 94.18 93.80 To evaluate our proposed method, we have conducted two sets of experiments on CTB5: word segmentation, and joint word segmentation and word-level POS tagging. We have adopted the same data division as in (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011): the training set, dev set and test set have 18,089, 350 and 348 sentences, respectively. The models applied on all test sets are those that result in the best performance on the CTB5 dev set. We have annotated character-level POS information for all 508,768 word tokens in CTB5. As mentioned in section 2, we blind the annotation in the test set in all the experiments. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 3 time"
P14-2042,C08-1049,0,0.0588611,"s, the function returns its character-level POS. The subscript indices 0 and -1 as well as 256 (a) Word Segmentation Results System P R Baseline 97.48 98.44 CharPOS 97.55 98.51 other symbols stand for the same meaning as they are in the baseline features. 4 4.1 Evaluation Settings (b) Joint Segmentation and POS Tagging Results System P R F Baseline 93.01 93.95 93.48 CharPOS 93.42 94.18 93.80 To evaluate our proposed method, we have conducted two sets of experiments on CTB5: word segmentation, and joint word segmentation and word-level POS tagging. We have adopted the same data division as in (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011): the training set, dev set and test set have 18,089, 350 and 348 sentences, respectively. The models applied on all test sets are those that result in the best performance on the CTB5 dev set. We have annotated character-level POS information for all 508,768 word tokens in CTB5. As mentioned in section 2, we blind the annotation in the test set in all the experiments. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 3 time"
P14-2042,P09-1058,0,0.850882,"haracter-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Part-of-Speech v"
P14-2042,Y06-1012,0,0.108255,"hod that performs character-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Charact"
P14-2042,P13-1013,0,0.0488773,"e focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Part-of-Speech verb + noun noun + verb verb + adjective adjective + verb verb + verb Examples of Verb 投资 (invest : throw + wealth) 心疼 (feel sorry : heart + hurt) 认清 (realize : recognize + clear) 痛恨 (hate : pain + hate) 审查 (inspect : examine + review) Table 1. Character-level POS sequence as a more specif"
P14-2042,D10-1082,0,0.22597,"ces 0 and -1 as well as 256 (a) Word Segmentation Results System P R Baseline 97.48 98.44 CharPOS 97.55 98.51 other symbols stand for the same meaning as they are in the baseline features. 4 4.1 Evaluation Settings (b) Joint Segmentation and POS Tagging Results System P R F Baseline 93.01 93.95 93.48 CharPOS 93.42 94.18 93.80 To evaluate our proposed method, we have conducted two sets of experiments on CTB5: word segmentation, and joint word segmentation and word-level POS tagging. We have adopted the same data division as in (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011): the training set, dev set and test set have 18,089, 350 and 348 sentences, respectively. The models applied on all test sets are those that result in the best performance on the CTB5 dev set. We have annotated character-level POS information for all 508,768 word tokens in CTB5. As mentioned in section 2, we blind the annotation in the test set in all the experiments. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 3 times. We applied a similar strategy in building the lexicon for character"
P14-2042,P11-1141,0,0.0183567,"roduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Part-of-Speech verb + noun noun + verb verb + adjective adjective + verb verb + verb Examples of Verb 投资 (invest : throw + wealth) 心疼 (feel sorry : heart + hurt) 认清 (realize : recognize + clear) 痛恨 (hate : pain + hate) 审查 (inspect : examine + review) Table 1. Chara"
P14-2042,D12-1132,0,0.0193696,"In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Part-of-Speech verb + noun noun + verb verb + adjective adjective + verb verb + verb Examples of Verb 投资 (invest : throw + wealth) 心疼 (feel sorry : heart + hurt) 认清 (realize : recognize + clear) 痛恨 (hate : pain + hate) 审查 (inspect : examine + review) Table 1. Character-level POS sequ"
P14-2042,W04-3236,0,0.260357,"evel POS tagging. We propose a method that performs character-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) point"
P14-2042,C04-1067,0,0.0947917,"We propose a method that performs character-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some"
P14-2042,P07-2055,0,0.0411452,"Missing"
P14-2042,C10-2139,0,0.0156538,"ith word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Part-of-Speech verb + noun noun + verb"
P14-2042,P11-1139,0,0.375647,"256 (a) Word Segmentation Results System P R Baseline 97.48 98.44 CharPOS 97.55 98.51 other symbols stand for the same meaning as they are in the baseline features. 4 4.1 Evaluation Settings (b) Joint Segmentation and POS Tagging Results System P R F Baseline 93.01 93.95 93.48 CharPOS 93.42 94.18 93.80 To evaluate our proposed method, we have conducted two sets of experiments on CTB5: word segmentation, and joint word segmentation and word-level POS tagging. We have adopted the same data division as in (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011): the training set, dev set and test set have 18,089, 350 and 348 sentences, respectively. The models applied on all test sets are those that result in the best performance on the CTB5 dev set. We have annotated character-level POS information for all 508,768 word tokens in CTB5. As mentioned in section 2, we blind the annotation in the test set in all the experiments. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 3 times. We applied a similar strategy in building the lexicon for character-level POS,"
P14-2042,O03-4002,0,0.0598797,"g jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Part-of-Speech verb + noun"
P15-5004,E14-1007,1,0.835082,"Missing"
P15-5004,W13-3830,1,0.898671,"Missing"
P15-5004,S14-2047,1,\N,Missing
P15-5004,jezek-etal-2014-pas,1,\N,Missing
P15-5004,popescu-etal-2014-mapping,1,\N,Missing
P16-1117,D15-1041,0,0.0501244,"Missing"
P16-1117,D14-1082,0,0.0543916,"o anaphora. For identifying inter-sentential zero anaphora, an antecedent has to be searched in a broad search space, and the salience of discourse entities has to be captured. Therefore, the task of identifying inter-sentential zero anaphora is more difficult than that of intra-sentential zero anaphora. Thus, Ouchi et al. (2015) and Iida et al. (2015) focused on only intra-sentential zero anaphora. Following this trend, this paper focuses on intra-sentential zero anaphora. Recently, NN-based approaches have achieved improvement for several NLP tasks. For example, in transition-based parsing, Chen and Manning (2014) proposed an NN-based approach, where the words, POS tags, and dependency labels are first represented by embeddings individually. Then, an NN-based classifier is built to make parsing decisions, where an input layer is a concatenation of embeddings of words, POS tags, and dependency labels. This model has been extended by several studies (Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015). In semantic role labeling, Zhou and Xu (2015) propose an end-to-end approach using recurrent NN, where an original text is the input, and semantic role labeling is performed without any interm"
P16-1117,W02-1001,0,0.459465,"Missing"
P16-1117,P15-1033,0,0.0463766,"Missing"
P16-1117,Y12-1058,1,0.722115,"|1 denotes the Hamming distance between the gold PA graph yˆk and a candidate PA graph yk . Stochastic gradient descent is used for parameter inference. Derivatives with respect to parameters are taken using backpropagation. Adam (Kingma and Ba, 2014) is adopted as the optimizer. For initialization of the embeddings of a predicate/argument, the embeddings of the predicate/argument trained by the method described in Section 4.1 are utilized. The weight matrices are randomly initialized. 5 Experiment 5.1 Experimental Setting The KWDLC (Kyoto University Web Document Leads Corpus) evaluation set (Hangyo et al., 2012) was used for our experiments, because it contains 1240 a wide variety of Web documents, such as news articles and blogs. This evaluation set consists of the first three sentences of 5,000 Web documents. Morphology, named entities, dependencies, PASs, and coreferences were manually annotated. This evaluation set was divided into 3,694 documents (11,558 sents.) for training, 512 documents (1,585 sents.) for development, and 700 documents (2,195 sents.) for testing. Table 1 shows the statistics of the number of arguments in the test set. While “dep argument” means that the argument and a predica"
P16-1117,D13-1095,1,0.828124,"sis, dependency analysis, and named entities were used. The sentences having a predicate that takes multiple arguments in the same case role were excluded from training and test examples, since the base model cannot handle this phenomena (it assumes that each predicate has only one argument with one case role). For example, the following sentence, added as well as a NULL node in a PA graph of the base model. When the argument predication score is calculated for “author” or “reader,” because its lemma does not appear in a document, for each noun in the following noun list of “author”/“reader” (Hangyo et al., 2013), the argument prediction score is calculated, and the maximum score is used as a feature. • author: “私” (I), “我々” (we), “僕” (I), “弊 社” (our company), · · · • reader: “あなた” (you), “客” (customer), “ 君” (you), “皆様”(you all), · · · In the argument prediction model training described in Section 4.1, a Japanese Web corpus consisting of 10M sentences was used. We preformed syntactic parsing with a publicly available Japanese parser, KNP4 . The number of negative samples was 5, and the number of epochs was 10. In the model training described in Section 4.3, the dimensions of both embeddings for predi"
P16-1117,D14-1163,0,0.0301029,"given the predicate “逮捕” (arrest) and its ACC “犯人” (suspect). 4 Proposed Model 4.1 Argument Prediction Model No external knowledge is utilized in the base model. One of the most important types of knowledge in PAS analysis is selectional preferences. Sasano and Kurohashi (2011) and Hangyo et al. (2013) extract knowledge of the selectional preferences in the form of case frames from a raw corpus, and the selectional preference score is used as a feature. In this work, argument prediction model is trained using a neural network from a raw corpus, in a similar way to Titov and Khoddam (2015) and Hashimoto et al. (2014). PASs are first extracted from an automaticallyparsed raw corpus, and in each PAS, the argument ai is generated with the following probability p(ai |P AS−ai ): p(ai |P AS−ai ) = T exp(v T ai Wai (Wpred v pred + Z ∑ j=i Waj v aj )) (4) where P AS−ai represents a PAS excluding the target argument ai , v pred , v ai and v aj represent embeddings of the predicate, argument ai and argument aj , and Wpred , Wai , and Waj represent transformation matrices for a predicate and an argument ai and aj . Z is the partition function. Figure 4 illustrates the argument prediction model. The PAS “警察” (police"
P16-1117,I11-1023,0,0.420368,"Missing"
P16-1117,D15-1260,0,0.226421,"capture interactions between predicates and their arguments, and thus performs the best among the three types. This method is adopted as our base model (see Section 3 for details). Most methods for PAS analysis handle both intra-sentential and inter-sentential zero anaphora. For identifying inter-sentential zero anaphora, an antecedent has to be searched in a broad search space, and the salience of discourse entities has to be captured. Therefore, the task of identifying inter-sentential zero anaphora is more difficult than that of intra-sentential zero anaphora. Thus, Ouchi et al. (2015) and Iida et al. (2015) focused on only intra-sentential zero anaphora. Following this trend, this paper focuses on intra-sentential zero anaphora. Recently, NN-based approaches have achieved improvement for several NLP tasks. For example, in transition-based parsing, Chen and Manning (2014) proposed an NN-based approach, where the words, POS tags, and dependency labels are first represented by embeddings individually. Then, an NN-based classifier is built to make parsing decisions, where an input layer is a concatenation of embeddings of words, POS tags, and dependency labels. This model has been extended by severa"
P16-1117,P09-2022,0,0.359611,"Missing"
P16-1117,P15-1093,0,0.408129,"admissible PA graphs for the input sentence x. Score(x, y) is defined as follows3 : ∑ ∑ scorel (x, e)+ scoreg (x, ei , ej ). e∈E(y) ei ,ej ∈Epair (y) scorel (x, e) = θ l · ϕl (x, e) scoreg (x, ei , ej ) = θ g · ϕg (x, ei , ej ) 1 2 3 4 5 (2) 6 (3) 7 8 where E(y) is the edge set on the candidate graph y, Epair (y) is a set of edge pairs in the edge set E(y), scorel (x, e) and scoreg (x, ei , ej ) represent 2 For example, in the sentence “今日は 暑い” (today-TOP hot), the predicate “暑い” does not take “今日”, which represents time, as an argument. Therefore, these nodes do not always have a relation. 3 Ouchi et al. (2015) introduce two models: Per-Case Joint Model and All-Cases Joint Model. Since All-Cases Joint Model performed better than Per-Case Joint Model, AllCases Joint Model is adopted as our base model. Input: sentence x, parameter θ Output: a locally optimal PA graph y˜ Sample a PA graph y (0) from G(x) t←0 repeat ∪ Y ← N eighborG(y (t) ) y (t) y (t+1) ← argmax Score(x, y; θ) y∈Y t←t+1 until y (t) = y (t+1) return y˜ ← y (t) Figure 3: Hill climbing algorithm for obtaining optimal PA graph. Given N training examples D = {(x, yˆ)}N k , the model parameter θ are estimated. θ is the set of θ l and θ g , a"
P16-1117,I11-1085,1,0.702167,"Missing"
P16-1117,D08-1055,0,0.41578,"Missing"
P16-1117,N15-1001,0,0.0269968,"th the NOM case is predicted given the predicate “逮捕” (arrest) and its ACC “犯人” (suspect). 4 Proposed Model 4.1 Argument Prediction Model No external knowledge is utilized in the base model. One of the most important types of knowledge in PAS analysis is selectional preferences. Sasano and Kurohashi (2011) and Hangyo et al. (2013) extract knowledge of the selectional preferences in the form of case frames from a raw corpus, and the selectional preference score is used as a feature. In this work, argument prediction model is trained using a neural network from a raw corpus, in a similar way to Titov and Khoddam (2015) and Hashimoto et al. (2014). PASs are first extracted from an automaticallyparsed raw corpus, and in each PAS, the argument ai is generated with the following probability p(ai |P AS−ai ): p(ai |P AS−ai ) = T exp(v T ai Wai (Wpred v pred + Z ∑ j=i Waj v aj )) (4) where P AS−ai represents a PAS excluding the target argument ai , v pred , v ai and v aj represent embeddings of the predicate, argument ai and argument aj , and Wpred , Wai , and Waj represent transformation matrices for a predicate and an argument ai and aj . Z is the partition function. Figure 4 illustrates the argument prediction"
P16-1117,P15-1032,0,0.0491291,"Missing"
P16-1117,D14-1109,0,0.0166218,"e and a global score for the edge pair ei and ej , ϕl (x, e) and ϕg (x, ei , ej ) represent local features and global features. While ϕl (x, e) is defined for each edge e, ϕg (x, ei , ej ) is defined for each edge pair ei , ej (i = j) . θ l and θ g represent model parameters for local and global features. By using global scores, the interaction between multiple case assignments of multiple predicates can be considered. 3.2 Inference and Training Since global features make the inference of finding the maximum scoring PA graph more difficult, the randomized hill-climbing algorithm proposed in (Zhang et al., 2014) is adopted. Figure 3 describes the pseudo code for hillclimbing algorithm. First, an initial PA graph y (0) is sampled from the set of admissible PA graph G(x). Then, the union Y is constructed from the set of neighboring graphs N eighborG(y (t) ), which is a set of admissible graphs obtained by changing one edge in y (t) , and the current graph y (t) . The current graph y (t) is updated to a higher scoring graph y (t+1) . This process continues until no more improvement is possible, and finally an optimal graph y˜ can be obtained. (1) where G(x) is a set of admissible PA graphs for the input"
P16-1117,P15-1109,0,0.138911,"Missing"
P17-1111,D15-1159,0,0.0641307,"Missing"
P17-1111,P16-1231,0,0.0930603,"y role in dependency parsing, they cannot be applied directly to the joint task in the previous work. To address this problem, we propose embeddings of character strings, in addition to words. Experiments show that our models outperform existing systems in Chinese word segmentation and POS tagging, and perform preferable accuracies in dependency parsing. We also explore bi-LSTM models with fewer features. 1 Introduction Dependency parsers have been enhanced by the use of neural networks and embedding vectors (Chen and Manning, 2014; Weiss et al., 2015; Zhou et al., 2015; Alberti et al., 2015; Andor et al., 2016; Dyer et al., 2015). When these dependency parsers process sentences in English and other languages that use symbols for word separations, they can be very accurate. However, for languages that do not contain word separation symbols, dependency parsers are used in pipeline processes with word segmentation and POS tagging models, and encounter serious problems because of error propagations. In particular, Chinese word segmentation is notoriously difficult because sentences are written without word dividers and Chinese words are not clearly defined. Hence, the pipeline of word segmentation, POS"
P17-1111,D15-1041,0,0.0182796,"Missing"
P17-1111,D14-1082,0,0.525801,"character strings are available. Otherwise, the embeddings of the character strings are used. mension and are chosen in the neural computation graph. We avoid using the “UNK” vector as far as possible, because this degenerates the information about unknown tokens. However, models use the “UNK” vector if the parser encounters characters that are not in the pre-trained embeddings, though this is quite uncommon. 2.3 Feed-forward Neural Network 2.3.1 Neural Network We present a feed-forward neural network model in Figure 2. The neural network for greedy training is based on the neural networks of Chen and Manning (2014) and Weiss et al. (2015). We add the dynamic generation of the embeddings of character strings for unknown tokens, as described in Section 2.2. This neural network has two hidden layers with 8,000 dimensions. This is larger than Chen and Manning (2014) (200 dimensions) or Weiss et al. (2015) (1,024 or 2,048 dimensions). We use the ReLU for the activation function of the hidden layers (Nair and Hinton, 2010) and the softmax function for the output layer of the greedy 1206 Type Value Type Features Size of h1 ,h2 Initial learning rate Initial learning rate of beam decoding Embedding vocabulary si"
P17-1111,P16-2006,0,0.0133814,"17. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1111 these problems, we propose neural network-based joint models for word segmentation, POS tagging and dependency parsing. We use both character and word embeddings for known tokens and apply character string embeddings for unknown tokens. Another problem in the models of Hatori et al. (2012) and Zhang et al. (2014) is that they rely on detailed feature engineering. Recently, bidirectional LSTM (bi-LSTM) based neural network models with very few feature extraction are proposed (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016). In their models, the bi-LSTM is used to represent the tokens including their context. Indeed, such neural networks can observe whole sentence through the bi-LSTM. This biLSTM is similar to that of neural machine translation models of Bahdanau et al. (2014). As a result, Kiperwasser and Goldberg (2016) achieve competitive scores with the previous state-of-theart models. We also develop joint models with ngram character string bi-LSTM. In the experiments, we obtain state-of-the-art Chinese word segmentation and POS tagging scores, and the pipeline of the dependency model achieves the better de"
P17-1111,P15-1033,0,0.0154611,"Missing"
P17-1111,I11-1136,0,0.04379,"he difference in the training set size. We present the final results with four hidden layers in Table 10. 3.2.7 Bi-LSTM Model We experiment the n-gram bi-LSTMs models with four and eight features listed in Table 3. We summarize the result in Table 11. The greedy biLSTM models perform slightly worse than the previous models, but they do not rely on feature engineering. 4 Related Work Zhang and Clark (2008) propose an incremental joint word segmentation and POS tagging model driven by a single perceptron. Zhang and Clark (2010) improve this model by using both character and word-based decoding. Hatori et al. (2011) propose a transition-based joint POS tagging and dependency parsing model. Zhang et al. (2013) propose a joint model using character structures of words for constituency parsing. Wang et al. (2013) also propose a lattice-based joint model for constituency parsing. Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system. This system requires 1211 Model Seg POS Dep Hatori+12 M. Zhang+14 EAG SegTagDep (g) 97.75 97.76 98.24 94.33 94.36 94.49 81.56 81.70 80.15 Bi-LSTM 4feat.(g) Bi-LSTM 8feat.(g) 97.72 97.70 93.12 93.37 79.03 79.38 Bengio. 2014. Neural machine t"
P17-1111,P12-1110,0,0.112837,"be directly applied to joint models because they use given word segmentations. To solve 1204 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1204–1214 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1111 these problems, we propose neural network-based joint models for word segmentation, POS tagging and dependency parsing. We use both character and word embeddings for known tokens and apply character string embeddings for unknown tokens. Another problem in the models of Hatori et al. (2012) and Zhang et al. (2014) is that they rely on detailed feature engineering. Recently, bidirectional LSTM (bi-LSTM) based neural network models with very few feature extraction are proposed (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016). In their models, the bi-LSTM is used to represent the tokens including their context. Indeed, such neural networks can observe whole sentence through the bi-LSTM. This biLSTM is similar to that of neural machine translation models of Bahdanau et al. (2014). As a result, Kiperwasser and Goldberg (2016) achieve competitive scores with the previous state-"
P17-1111,P08-1102,0,0.221872,"Missing"
P17-1111,Q16-1023,0,0.0637301,", Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1111 these problems, we propose neural network-based joint models for word segmentation, POS tagging and dependency parsing. We use both character and word embeddings for known tokens and apply character string embeddings for unknown tokens. Another problem in the models of Hatori et al. (2012) and Zhang et al. (2014) is that they rely on detailed feature engineering. Recently, bidirectional LSTM (bi-LSTM) based neural network models with very few feature extraction are proposed (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016). In their models, the bi-LSTM is used to represent the tokens including their context. Indeed, such neural networks can observe whole sentence through the bi-LSTM. This biLSTM is similar to that of neural machine translation models of Bahdanau et al. (2014). As a result, Kiperwasser and Goldberg (2016) achieve competitive scores with the previous state-of-theart models. We also develop joint models with ngram character string bi-LSTM. In the experiments, we obtain state-of-the-art Chinese word segmentation and POS tagging scores, and the pipeline of the dependency mode"
P17-1111,W04-0308,0,0.00996663,"des. The words in the stack are formed by the following transition operations. • SH(t) (shift): Shift the first character of the buffer to the top of the stack as a new word. • AP (append): Append the first character of the buffer to the end of the top word of the stack. • RR (reduce-right): Reduce the right word of the top two words of the stack, and make the right child node of the left word. • RL (reduce-left): Reduce the left word of the top two words of the stack, and make the left child node of the right word. The RR and RL operations are the same as those of the arc-standard algorithm (Nivre, 2004a). SH makes a new word whereas AP makes the current word longer by adding one character. The POS tags are attached with the SH(t) transition. In this paper, we explore both greedy models and beam decoding models. This parsing algorithm works in both types. We also develop a joint model of word segmentation and POS tagging, along with a dependency parsing model. The joint model of word segmentation and POS tagging does not have RR and RL transitions. 2.2 Based on Hatori et al. (2012), we use a modified arc-standard algorithm for character transiBuﬀer (character-based) 有 Embeddings of Character"
P17-1111,I11-1035,0,0.103015,"er string embeddings can capture similarities among them. Following the bi-LSTM layer, the feature function extracts the corresponding outputs of the bi-LSTM layer. We summarize the features in Table 3. Finally, MLP and the softmax function outputs the transition probability. We use an MLP with three hidden layers as for the model in Section 2.3. We train this neural network with the loss function for the greedy training. Experimental Settings We use the Penn Chinese Treebank 5.1 (CTB5) and 7 (CTB-7) datasets to evaluate our models, following the splitting of Jiang et al. (2008) for CTB-5 and Wang et al. (2011) for CTB-7. The statistics of datasets are presented in Table 4. We use the Chinese Gigaword Corpus for embedding pre-training. Our model is developed for unlabeled dependencies. The development set is used for parameter tuning. Following Hatori et al. (2012) and Zhang et al. (2014), we use the standard word-level evaluation with F1-measure. The POS tags and dependencies cannot be correct unless the corresponding words are correctly segmented. We trained three models: SegTag, SegTagDep and Dep. SegTag is the joint word segmentation and POS tagging model. SegTagDep is the full joint segmentatio"
P17-1111,P13-2110,0,0.0806643,"ed in Table 3. We summarize the result in Table 11. The greedy biLSTM models perform slightly worse than the previous models, but they do not rely on feature engineering. 4 Related Work Zhang and Clark (2008) propose an incremental joint word segmentation and POS tagging model driven by a single perceptron. Zhang and Clark (2010) improve this model by using both character and word-based decoding. Hatori et al. (2011) propose a transition-based joint POS tagging and dependency parsing model. Zhang et al. (2013) propose a joint model using character structures of words for constituency parsing. Wang et al. (2013) also propose a lattice-based joint model for constituency parsing. Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system. This system requires 1211 Model Seg POS Dep Hatori+12 M. Zhang+14 EAG SegTagDep (g) 97.75 97.76 98.24 94.33 94.36 94.49 81.56 81.70 80.15 Bi-LSTM 4feat.(g) Bi-LSTM 8feat.(g) 97.72 97.70 93.12 93.37 79.03 79.38 Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473. Table 11: Bi-LSTM feature extraction model. “4feat.” and “8feat.” denote the use of four an"
P17-1111,P15-1032,0,0.167578,"ble. Otherwise, the embeddings of the character strings are used. mension and are chosen in the neural computation graph. We avoid using the “UNK” vector as far as possible, because this degenerates the information about unknown tokens. However, models use the “UNK” vector if the parser encounters characters that are not in the pre-trained embeddings, though this is quite uncommon. 2.3 Feed-forward Neural Network 2.3.1 Neural Network We present a feed-forward neural network model in Figure 2. The neural network for greedy training is based on the neural networks of Chen and Manning (2014) and Weiss et al. (2015). We add the dynamic generation of the embeddings of character strings for unknown tokens, as described in Section 2.2. This neural network has two hidden layers with 8,000 dimensions. This is larger than Chen and Manning (2014) (200 dimensions) or Weiss et al. (2015) (1,024 or 2,048 dimensions). We use the ReLU for the activation function of the hidden layers (Nair and Hinton, 2010) and the softmax function for the output layer of the greedy 1206 Type Value Type Features Size of h1 ,h2 Initial learning rate Initial learning rate of beam decoding Embedding vocabulary size Embedding vector size"
P17-1111,P13-1013,0,0.0249876,"Table 10. 3.2.7 Bi-LSTM Model We experiment the n-gram bi-LSTMs models with four and eight features listed in Table 3. We summarize the result in Table 11. The greedy biLSTM models perform slightly worse than the previous models, but they do not rely on feature engineering. 4 Related Work Zhang and Clark (2008) propose an incremental joint word segmentation and POS tagging model driven by a single perceptron. Zhang and Clark (2010) improve this model by using both character and word-based decoding. Hatori et al. (2011) propose a transition-based joint POS tagging and dependency parsing model. Zhang et al. (2013) propose a joint model using character structures of words for constituency parsing. Wang et al. (2013) also propose a lattice-based joint model for constituency parsing. Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system. This system requires 1211 Model Seg POS Dep Hatori+12 M. Zhang+14 EAG SegTagDep (g) 97.75 97.76 98.24 94.33 94.36 94.49 81.56 81.70 80.15 Bi-LSTM 4feat.(g) Bi-LSTM 8feat.(g) 97.72 97.70 93.12 93.37 79.03 79.38 Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR abs/1409.0473. http://arxiv.org/abs"
P17-1111,P14-1125,0,0.623085,"oint models because they use given word segmentations. To solve 1204 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1204–1214 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1111 these problems, we propose neural network-based joint models for word segmentation, POS tagging and dependency parsing. We use both character and word embeddings for known tokens and apply character string embeddings for unknown tokens. Another problem in the models of Hatori et al. (2012) and Zhang et al. (2014) is that they rely on detailed feature engineering. Recently, bidirectional LSTM (bi-LSTM) based neural network models with very few feature extraction are proposed (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016). In their models, the bi-LSTM is used to represent the tokens including their context. Indeed, such neural networks can observe whole sentence through the bi-LSTM. This biLSTM is similar to that of neural machine translation models of Bahdanau et al. (2014). As a result, Kiperwasser and Goldberg (2016) achieve competitive scores with the previous state-of-theart models. We als"
P17-1111,N15-1005,0,0.402275,"cause of the large mini-batch size. The neural network is implemented with Theano. 1209 Model Seg POS Model Hatori+12 SegTag Hatori+12 SegTag(d) Hatori+12 SegTagDep Hatori+12 SegTagDep(d) M. Zhang+14 EAG Y. Zhang+15 97.66 98.18 97.73 98.26 97.76 98.04 93.61 94.08 94.46 94.64 94.36 94.47 SegTag(g) SegTag 98.41 98.60 94.84 94.76 Table 5: Joint segmentation and POS tagging scores. Both scores are in F-measure. In Hatori et al. (2012), (d) denotes the use of dictionaries. (g) denotes greedy trained models. All scores for previous models are taken from Hatori et al. (2012), Zhang et al. (2014) and Zhang et al. (2015). 3.2 3.2.1 First, we evaluate the joint segmentation and POS tagging model (SegTag). Table 5 compares the performance of segmentation and POS tagging using the CTB-5 dataset. We train two modles: a greedy-trained model and a model trained with beams of size 4. We compare our model to three previous approaches: Hatori et al. (2012), Zhang et al. (2014) and Zhang et al. (2015). Our SegTag joint model is superior to these previous models, including Hatori et al. (2012)’s model with rich dictionary information, in terms of both segmentation and POS tagging accuracy. 3.2.2 Joint Segmentation, POS"
P17-1111,D08-1059,0,0.017231,"t-test. tice that the MLP with four hidden layers performs better than the MLP with three hidden layers, but we could not find definite differences in the experiments in CTB-5. We speculate that this is caused by the difference in the training set size. We present the final results with four hidden layers in Table 10. 3.2.7 Bi-LSTM Model We experiment the n-gram bi-LSTMs models with four and eight features listed in Table 3. We summarize the result in Table 11. The greedy biLSTM models perform slightly worse than the previous models, but they do not rely on feature engineering. 4 Related Work Zhang and Clark (2008) propose an incremental joint word segmentation and POS tagging model driven by a single perceptron. Zhang and Clark (2010) improve this model by using both character and word-based decoding. Hatori et al. (2011) propose a transition-based joint POS tagging and dependency parsing model. Zhang et al. (2013) propose a joint model using character structures of words for constituency parsing. Wang et al. (2013) also propose a lattice-based joint model for constituency parsing. Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system. This system requires 1211 Mo"
P17-1111,D10-1082,0,0.0712524,"ind definite differences in the experiments in CTB-5. We speculate that this is caused by the difference in the training set size. We present the final results with four hidden layers in Table 10. 3.2.7 Bi-LSTM Model We experiment the n-gram bi-LSTMs models with four and eight features listed in Table 3. We summarize the result in Table 11. The greedy biLSTM models perform slightly worse than the previous models, but they do not rely on feature engineering. 4 Related Work Zhang and Clark (2008) propose an incremental joint word segmentation and POS tagging model driven by a single perceptron. Zhang and Clark (2010) improve this model by using both character and word-based decoding. Hatori et al. (2011) propose a transition-based joint POS tagging and dependency parsing model. Zhang et al. (2013) propose a joint model using character structures of words for constituency parsing. Wang et al. (2013) also propose a lattice-based joint model for constituency parsing. Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system. This system requires 1211 Model Seg POS Dep Hatori+12 M. Zhang+14 EAG SegTagDep (g) 97.75 97.76 98.24 94.33 94.36 94.49 81.56 81.70 80.15 Bi-LSTM 4feat"
P17-1111,D13-1061,0,0.0688612,"cy parsing. Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system. This system requires 1211 Model Seg POS Dep Hatori+12 M. Zhang+14 EAG SegTagDep (g) 97.75 97.76 98.24 94.33 94.36 94.49 81.56 81.70 80.15 Bi-LSTM 4feat.(g) Bi-LSTM 8feat.(g) 97.72 97.70 93.12 93.37 79.03 79.38 Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473. Table 11: Bi-LSTM feature extraction model. “4feat.” and “8feat.” denote the use of four and eight features. base parsers. In neural joint models, Zheng et al. (2013) propose a neural network-based Chinese word segmentation model based on tag inferences. They extend their models for joint segmentation and POS tagging. Zhu et al. (2015) propose the re-ranking system of parsing results with recursive convolutional neural network. 5 Conclusion We propose the joint parsing models by the feedforward and bi-LSTM neural networks. Both of them use the character string embeddings. The character string embeddings help to capture the similarities of incomplete tokens. We also explore the neural network with few features using n-gram bi-LSTMs. Our SegTagDep joint mode"
P17-1111,P15-1117,0,0.0303434,"Missing"
P17-1111,P15-1112,0,0.019685,"AG SegTagDep (g) 97.75 97.76 98.24 94.33 94.36 94.49 81.56 81.70 80.15 Bi-LSTM 4feat.(g) Bi-LSTM 8feat.(g) 97.72 97.70 93.12 93.37 79.03 79.38 Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473. Table 11: Bi-LSTM feature extraction model. “4feat.” and “8feat.” denote the use of four and eight features. base parsers. In neural joint models, Zheng et al. (2013) propose a neural network-based Chinese word segmentation model based on tag inferences. They extend their models for joint segmentation and POS tagging. Zhu et al. (2015) propose the re-ranking system of parsing results with recursive convolutional neural network. 5 Conclusion We propose the joint parsing models by the feedforward and bi-LSTM neural networks. Both of them use the character string embeddings. The character string embeddings help to capture the similarities of incomplete tokens. We also explore the neural network with few features using n-gram bi-LSTMs. Our SegTagDep joint model achieves better scores of Chinese word segmentation and POS tagging than previous joint models, and our SegTag and Dep pipeline model achieves state-of-the-art score of"
P18-1044,D17-1230,0,0.040184,"bata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017), and text classification (Liu et al., 2017a). 476 3.2 while fixing the validator V . This generator training loss using the validator can be explained as follows. The generator tries to increase the validator scores to 1, while the validator is fixed. If the validator is well-trained, it returns scores close to 1 for correct PAS labels that the generator outputs, and 0 for wrong labels. Therefore, in Equation (6), the generator tries to predict correct labels in order to increase the scores of fixed validator. Note that the validator has a sigmoid function for the output of scores. Therefore"
P18-1044,W17-4712,0,0.0227115,"phora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017), and text classification (Liu et al., 2017a). 476 3.2 while fixing the validator V . This generator training loss using the validator can be explained as follows. The generator tries to increase the validator scores to 1, while the validator is fixed. If the validator is well-trained, it returns scores close to 1 for correct PAS labels that the generator outputs, and 0 for wrong labels. Therefore, in Equation (6), the generator tries to predict correct labels in order to increase the scores of fixed validator. Note that the validator has a sigmoid function"
P18-1044,P17-1001,0,0.313019,"rresponding case argument or that the case argument is not written in the sentence. involved), Our contributions are summarized as follows: (1) a novel adversarial training model for PAS analysis; (2) learning from a raw corpus as a source of external knowledge; and (3) as a result, we achieve state-of-the-art performance on Japanese PAS analysis. ing pre-trained external knowledge in the form of word embeddings has also been ubiquitous. However, such external knowledge is overwritten in the task-specific training. The other approach to using raw corpora for PAS analysis is data augmentation. Liu et al. (2017b) generate pseudo training data from a raw corpus and use them for their zero pronoun resolution model. They generate the pseudo training data by dropping certain words or pronouns in a raw corpus and assuming them as correct antecedents. After generating the pseudo training data, they rely on ordinary supervised training based on neural networks. In this paper, we propose a neural semisupervised model for Japanese PAS analysis. We adopt neural adversarial training to directly exploit the advantage of using a raw corpus. Our model consists of two neural network models: a generator model of Ja"
P18-1044,P17-1010,0,0.385019,"rresponding case argument or that the case argument is not written in the sentence. involved), Our contributions are summarized as follows: (1) a novel adversarial training model for PAS analysis; (2) learning from a raw corpus as a source of external knowledge; and (3) as a result, we achieve state-of-the-art performance on Japanese PAS analysis. ing pre-trained external knowledge in the form of word embeddings has also been ubiquitous. However, such external knowledge is overwritten in the task-specific training. The other approach to using raw corpora for PAS analysis is data augmentation. Liu et al. (2017b) generate pseudo training data from a raw corpus and use them for their zero pronoun resolution model. They generate the pseudo training data by dropping certain words or pronouns in a raw corpus and assuming them as correct antecedents. After generating the pseudo training data, they rely on ordinary supervised training based on neural networks. In this paper, we propose a neural semisupervised model for Japanese PAS analysis. We adopt neural adversarial training to directly exploit the advantage of using a raw corpus. Our model consists of two neural network models: a generator model of Ja"
P18-1044,I17-2022,0,0.791284,"ents are not specified in the article. This is called as exophora. We consider “author” and “reader” arguments as exophora (Hangyo et al., 2013). They are frequently dropped from Japanese natural sentences. Sentence (4) is an example of dropped nominative arguments. In this sentence, the nominative argument is “あなた” (you), but “あ なた” (you) does not appear in the sentence. This is also included in zero anaphora resolution. Except these special arguments of exophora, we focus on intra-sentential anaphora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017), and text classification (Liu et al., 2017a). 476 3.2 while fixing th"
P18-1044,Y12-1058,1,0.875776,"ormances are evaluated with microaveraged F-measure (Shibata et al., 2016). Table 4: KWDLC training data statistics for each case. Case Zero Ouchi+ 2015 Shibata+ 2016 76.5 89.3 42.1 53.4 Gen Gen+Adv 91.5 92.0‡ 56.2 58.4‡ Table 5: The results of case analysis (Case) and zero anaphora resolution (Zero). We use Fmeasure as an evaluation measure. ‡ denotes that the improvement is statistically significant at p &lt; 0.05, compared with Gen using paired t-test. 4 4.1 Experiments Experimental Settings Following Shibata et al. (2016), we use the KWDLC (Kyoto University Web Document Leads Corpus) corpus (Hangyo et al., 2012) for our experiments.1 This corpus contains various Web documents, such as news articles, personal blogs, and commerce sites. In KWDLC, lead three sentences of each document are annotated with PAS structures including zero pronouns. For a raw corpus, we use a Japanese web corpus created by Hangyo et al. (2012), which has no duplicated sentences with KWDLC. This raw corpus is automatically parsed by the Japanese dependency parser KNP. We focus on intra-sentential anaphora resolution, and so we apply a preprocess to KWDLC. We regard the anaphors whose antecedents are in the preceding sentences a"
P18-1044,D13-1095,1,0.948228,"xi), while this does not have direct dependencies to the second predicate. A zero anaphora resolution model has to find “タクシー”(taxi) from the sentence, and assign it to the NOM case of the second predicate. LD = − Ex∼pdata (x) [log D(x)]  +Ez∼pz (z) [log(1 − D(G(z)))] , (1) and they train the discriminator by minimizing this loss while fixing the generator G. Similarly, the generator G is trained through minimizing In the zero anaphora resolution task, some correct arguments are not specified in the article. This is called as exophora. We consider “author” and “reader” arguments as exophora (Hangyo et al., 2013). They are frequently dropped from Japanese natural sentences. Sentence (4) is an example of dropped nominative arguments. In this sentence, the nominative argument is “あなた” (you), but “あ なた” (you) does not appear in the sentence. This is also included in zero anaphora resolution. Except these special arguments of exophora, we focus on intra-sentential anaphora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i"
P18-1044,W07-1522,0,0.029009,"ochs. model of Ouchi et al. (2015). They achieved state-of-the-art results on case analysis and zero anaphora resolution using the KWDLC corpus. They use an external resource to extract selectional preferences. Since our model uses an external resource, we compare our model with the models of Shibata et al. (2016) and Ouchi et al. (2015). Ouchi et al. (2017) proposed a semantic role labeling-based PAS analysis model using GridRNNs. Matsubayashi and Inui (2017) proposed a case label selection model with feature-based neural networks. They conducted their experiments on NAIST Text Corpus (NTC) (Iida et al., 2007, 2016). NTC consists of newspaper articles, and does not include the annotations of author/reader expressions that are common in Japanese natural sentences. However, the Gen+Adv model judged the DAT argument as “author”. Although we cannot specify φ as “author” only from this sentence, “author” is a possible argument depending on the context. 4.4.2 Validator Analysis We also evaluate the performance of the validator during the adversarial training with raw corpora. Figure 3 shows the validator performance and the generator performance of Zero on the development set. The validator score is eva"
P18-1044,P15-1093,0,0.291601,"rvised and unsupervised training of the generator, while we do not use minibatch for validator training. Therefore, we use k = 16 and l = 4. Other parameters are summarized in Table 2. Note that the validator is a simple neural network compared with the generator. The validator has limited inputs of predicates and arguments and no inputs of other words in sentences. This allows the generator to overwhelm the validator during the adversarial training. 479 KWDLC NOM ACC DAT # of dep # of zero 7,224 6,453 1,555 515 448 1,248 reader “あなた” (you), “君” (you), “客” (customer), “皆様” (you all) Following Ouchi et al. (2015) and Shibata et al. (2016), we conduct two kinds of analysis: (1) case analysis and (2) zero anaphora resolution. Case analysis is the task to determine the correct case labels when predicates and their arguments have direct dependencies but their case markers are hidden by surface markers, such as topic markers. Zero anaphora resolution is a task to find certain case arguments that do not have direct dependencies to their predicates in the sentence. Following Shibata et al. (2016), we exclude predicates that the same arguments are filled in multiple cases of a predicate. This is relatively un"
P18-1044,D16-1132,0,0.151108,"ora resolution task, some correct arguments are not specified in the article. This is called as exophora. We consider “author” and “reader” arguments as exophora (Hangyo et al., 2013). They are frequently dropped from Japanese natural sentences. Sentence (4) is an example of dropped nominative arguments. In this sentence, the nominative argument is “あなた” (you), but “あ なた” (you) does not appear in the sentence. This is also included in zero anaphora resolution. Except these special arguments of exophora, we focus on intra-sentential anaphora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017), and text classific"
P18-1044,P17-1146,0,0.565249,", some correct arguments are not specified in the article. This is called as exophora. We consider “author” and “reader” arguments as exophora (Hangyo et al., 2013). They are frequently dropped from Japanese natural sentences. Sentence (4) is an example of dropped nominative arguments. In this sentence, the nominative argument is “あなた” (you), but “あ なた” (you) does not appear in the sentence. This is also included in zero anaphora resolution. Except these special arguments of exophora, we focus on intra-sentential anaphora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017), and text classification (Liu et al., 2"
P18-1044,P16-1113,0,0.0310735,"e concatenated and fed into the bi-LSTM layers. The bi-LSTM layers read these embeddings in forward and backward order and outputs the distributed representations of a predicate and a candidate argument: hpredj and hargi . Note that we also use the exophora entities, i.e., an author and a reader, as argument candidates. Therefore, we use specific embeddings for them. These embeddings are not generated by the biLSTM layers but are directly used in the argument selection model. We also use path embeddings to capture a dependency relation between a predicate and its candidate argument as used in Roth and Lapata (2016). Although Roth and Lapata (2016) use a one-way LSTM layer to represent the dependency path from a predicate to its potential argument, we use a bi-LSTM layer for this purpose. We feed the embeddings of words and POS tags to the bi-LSTM layer. In this way, the resulting path embedding represents both predicate-toargument and argument-to-predicate paths. We concatenate the bidirectional path embeddings to generate hpathij , which represents the dependency relation between the predicate j and its candidate argument i. For the argument selection model, we apply the argument selection model (Zhang"
P18-1044,I11-1085,1,0.926589,"Missing"
P18-1044,P16-1117,1,0.670851,"zing In the zero anaphora resolution task, some correct arguments are not specified in the article. This is called as exophora. We consider “author” and “reader” arguments as exophora (Hangyo et al., 2013). They are frequently dropped from Japanese natural sentences. Sentence (4) is an example of dropped nominative arguments. In this sentence, the nominative argument is “あなた” (you), but “あ なた” (you) does not appear in the sentence. This is also included in zero anaphora resolution. Except these special arguments of exophora, we focus on intra-sentential anaphora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017),"
P18-1044,W17-2629,0,0.170923,"ts of exophora, we focus on intra-sentential anaphora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017), and text classification (Liu et al., 2017a). 476 3.2 while fixing the validator V . This generator training loss using the validator can be explained as follows. The generator tries to increase the validator scores to 1, while the validator is fixed. If the validator is well-trained, it returns scores close to 1 for correct PAS labels that the generator outputs, and 0 for wrong labels. Therefore, in Equation (6), the generator tries to predict correct labels in order to increase the scores of fixed validator. Note"
P18-1044,E17-1063,0,0.0804674,"generating the pseudo training data, they rely on ordinary supervised training based on neural networks. In this paper, we propose a neural semisupervised model for Japanese PAS analysis. We adopt neural adversarial training to directly exploit the advantage of using a raw corpus. Our model consists of two neural network models: a generator model of Japanese PAS analysis and a so-called “validator” model of the generator prediction. The generator neural network is a model that predicts probabilities of candidate arguments of each predicate using RNN-based features and a head-selection model (Zhang et al., 2017). The validator neural network gets inputs from the generator and scores them. This validator can score the generator prediction even when PAS gold labels are not available. We apply supervised learning to the generator and unsupervised learning to the entire network using a raw corpus. 2 Task Description Japanese PAS analysis determines essential case roles of words for each predicate: who did what to whom. In many languages, such as English, case roles are mainly determined by word order. However, in Japanese, word order is highly flexible. In Japanese, major case roles are the nominative ca"
S16-1178,W13-2322,0,0.0782985,"Missing"
S16-1178,W09-1206,0,0.0946195,"Missing"
S16-1178,P13-2131,0,0.133802,"3 0.60 0.63 labels, we apply this transition without asking for the perceptron prediction. Finally, if the network is not confident about the prediction – that is, if the probability for the highest scoring candidate is lower than 0.9 – we disregard the NN prediction and choose the prediction given by the perceptron algorithm. Experiments All the experiments were performed on the LDC2015E86 dataset, provided by the organizers. In our experiments we followed the standard train/dev/test split (16, 833, 1, 368 and 1, 371 sentences, respectively). Parser performance was evaluated with the Smatch (Cai and Knight, 2013) scoring script v2.0.2 4 (Table 2). As expected, using SRL features resulted in better performance compared to the baseline model (roughly a 2 F1 points gain). Conditioning on a wider context was also beneficial – widening the context to include more configuration elements is often a good feature expansion technique (Toutanova et al., 2003; Chen et al., 2014). In contrast to our expectations, the NN classifier did not improve the parser performance. This might be due to the higher complexity of the AMR parsing task or the peculiarities of the underlying parsing algorithm (as mentioned in Secti"
S16-1178,D14-1082,0,0.0538673,"ks. At the time of writing this paper two AMR parsers are publicly available: graph-based JAMR (Flanigan et al., 2014) and transition-based CAMR (Wang et al., 2015a; Wang et al., 2015b). The latter has served as our baseline model, which we tried to improve by incorporating additional features defined for a wider conditioning context and a neural network (NN) classifier. Preprocessing We used the Stanford CoreNLP v3.6.0 toolkit (Manning et al., 2014) to get named entity (NE) and dependency information, the latter in the form of Stanford dependencies was obtained from the NN dependency parser (Chen and Manning, 2014). We used a publicly available semantic role labelling (SRL) system with a predicate disambiguation module (Bj¨orkelund et al., 2009). The system is a part of MATE tools1 , which also include a lemmatizer, a part of speech (POS) tagger, and a dependency parser. We use them to obtain lemmas and POS tags. All the tools were used with the pretrained models. Using MALT dependencies instead of or in tandem with Stanford dependencies did not change the 1 https://storage.googleapis.com/ google-code-archive-downloads/v2/code. google.com/mate-tools 1154 Proceedings of SemEval-2016, pages 1154–1159, c S"
S16-1178,C14-1078,0,0.0301038,"e performed on the LDC2015E86 dataset, provided by the organizers. In our experiments we followed the standard train/dev/test split (16, 833, 1, 368 and 1, 371 sentences, respectively). Parser performance was evaluated with the Smatch (Cai and Knight, 2013) scoring script v2.0.2 4 (Table 2). As expected, using SRL features resulted in better performance compared to the baseline model (roughly a 2 F1 points gain). Conditioning on a wider context was also beneficial – widening the context to include more configuration elements is often a good feature expansion technique (Toutanova et al., 2003; Chen et al., 2014). In contrast to our expectations, the NN classifier did not improve the parser performance. This might be due to the higher complexity of the AMR parsing task or the peculiarities of the underlying parsing algorithm (as mentioned in Section 3, we discarded some actionspecific features due to the difficulty of their integration into the NN model). Further investigation on this matter is required to draw ground conclusions. 8 Conclusion We have performed a range of experiments which resulted in improving the performance of the baseline AMR parsing system. The results show that a richer feature"
S16-1178,W02-1001,0,0.466357,".ac.jp {kuro,dk}@i.kyoto-u.ac.jp Abstract Inspired by the results of Chen and Manning (2014) and Weiss et al. (2015), who obtained state-of-the-art results in transition-based dependency parsing using Feedforward Neural Networks (FFNN), and taking into account the transition nature of the CAMR model, we performed a series of experiments in the same direction. Neural networks have been successfully applied to many NLP fields and we were curious to examine their potential in the task of AMR parsing. Specifically, we investigated the possibility of constraining the averaged perceptron algorithm (Collins, 2002) predictions by those of an FFNN at the initial step of the inference process. This paper describes our contribution to the SemEval 2016 Workshop. We participated in the Shared Task 8 on Meaning Representation parsing using a transition-based approach, which builds upon the system of Wang et al. (2015a) and Wang et al. (2015b), with additions that utilize a Feedforward Neural Network classifier and an enriched feature set. We observed that exploiting Neural Networks in Abstract Meaning Representation parsing is challenging and we could not benefit from it, while the feature enhancements yielde"
S16-1178,dorr-etal-1998-thematic,0,0.232536,"Missing"
S16-1178,P14-1134,0,0.462078,"performance over the baseline model. 1 2 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Dorr et al., 1998) is a semantic formalism which represents sentence meaning in a form of a rooted directed acyclic graph. AMR graph nodes represent concepts, labelled directed edges between the nodes show the relationships between concepts. The AMR formalism was created in order to explore the semantics behind natural language units for further analysis and application in various tasks. At the time of writing this paper two AMR parsers are publicly available: graph-based JAMR (Flanigan et al., 2014) and transition-based CAMR (Wang et al., 2015a; Wang et al., 2015b). The latter has served as our baseline model, which we tried to improve by incorporating additional features defined for a wider conditioning context and a neural network (NN) classifier. Preprocessing We used the Stanford CoreNLP v3.6.0 toolkit (Manning et al., 2014) to get named entity (NE) and dependency information, the latter in the form of Stanford dependencies was obtained from the NN dependency parser (Chen and Manning, 2014). We used a publicly available semantic role labelling (SRL) system with a predicate disambigua"
S16-1178,P14-5010,0,0.00862248,"ips between concepts. The AMR formalism was created in order to explore the semantics behind natural language units for further analysis and application in various tasks. At the time of writing this paper two AMR parsers are publicly available: graph-based JAMR (Flanigan et al., 2014) and transition-based CAMR (Wang et al., 2015a; Wang et al., 2015b). The latter has served as our baseline model, which we tried to improve by incorporating additional features defined for a wider conditioning context and a neural network (NN) classifier. Preprocessing We used the Stanford CoreNLP v3.6.0 toolkit (Manning et al., 2014) to get named entity (NE) and dependency information, the latter in the form of Stanford dependencies was obtained from the NN dependency parser (Chen and Manning, 2014). We used a publicly available semantic role labelling (SRL) system with a predicate disambiguation module (Bj¨orkelund et al., 2009). The system is a part of MATE tools1 , which also include a lemmatizer, a part of speech (POS) tagger, and a dependency parser. We use them to obtain lemmas and POS tags. All the tools were used with the pretrained models. Using MALT dependencies instead of or in tandem with Stanford dependencies"
S16-1178,W04-2705,0,0.062934,"hI (Figure 1 (3) and (4)) . We then conto hE , h 1 0 1 1156 I catenate layers hE 1 and h1 (Figure 1 (5)) and pass the resultant vector to the last hidden layer h3 , applying the tanh function again (Figure 1 (6)). Finally, a softmax layer is added on top of h3 in order to calculate probabilities of the output classes (Figure 1 (7)). 5 Feature Sets We have designed two separate feature sets for the NN and perceptron classifiers. The feature set for the latter is roughly the same as in (Wang et al., 2015b) (Table 1). Following the authors of CAMR, we also make use of the NomBank 1.0 dictionary (Meyers et al., 2004) 2 . Unfortunately, we could not obtain the copy of the same SRL system which was used by the authors. Therefore, we also measure accuracy improvement from incorporating the semantic features defined in the original paper but extracted after processing the data with a different SRL system (marked with a ?). We also measure the improvement from incorporating the features extracted from a wider configuration context – they were not included into the baseline model and are marked with a •. In the case of the NN classifier we follow a standard feature extraction procedure and discard transition-sp"
S16-1178,N03-1033,0,0.111853,"All the experiments were performed on the LDC2015E86 dataset, provided by the organizers. In our experiments we followed the standard train/dev/test split (16, 833, 1, 368 and 1, 371 sentences, respectively). Parser performance was evaluated with the Smatch (Cai and Knight, 2013) scoring script v2.0.2 4 (Table 2). As expected, using SRL features resulted in better performance compared to the baseline model (roughly a 2 F1 points gain). Conditioning on a wider context was also beneficial – widening the context to include more configuration elements is often a good feature expansion technique (Toutanova et al., 2003; Chen et al., 2014). In contrast to our expectations, the NN classifier did not improve the parser performance. This might be due to the higher complexity of the AMR parsing task or the peculiarities of the underlying parsing algorithm (as mentioned in Section 3, we discarded some actionspecific features due to the difficulty of their integration into the NN model). Further investigation on this matter is required to draw ground conclusions. 8 Conclusion We have performed a range of experiments which resulted in improving the performance of the baseline AMR parsing system. The results show th"
S16-1178,N15-1040,0,0.524577,"l, we performed a series of experiments in the same direction. Neural networks have been successfully applied to many NLP fields and we were curious to examine their potential in the task of AMR parsing. Specifically, we investigated the possibility of constraining the averaged perceptron algorithm (Collins, 2002) predictions by those of an FFNN at the initial step of the inference process. This paper describes our contribution to the SemEval 2016 Workshop. We participated in the Shared Task 8 on Meaning Representation parsing using a transition-based approach, which builds upon the system of Wang et al. (2015a) and Wang et al. (2015b), with additions that utilize a Feedforward Neural Network classifier and an enriched feature set. We observed that exploiting Neural Networks in Abstract Meaning Representation parsing is challenging and we could not benefit from it, while the feature enhancements yielded an improved performance over the baseline model. 1 2 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Dorr et al., 1998) is a semantic formalism which represents sentence meaning in a form of a rooted directed acyclic graph. AMR graph nodes represent concepts, labelled dir"
S16-1178,P15-2141,0,0.474588,"l, we performed a series of experiments in the same direction. Neural networks have been successfully applied to many NLP fields and we were curious to examine their potential in the task of AMR parsing. Specifically, we investigated the possibility of constraining the averaged perceptron algorithm (Collins, 2002) predictions by those of an FFNN at the initial step of the inference process. This paper describes our contribution to the SemEval 2016 Workshop. We participated in the Shared Task 8 on Meaning Representation parsing using a transition-based approach, which builds upon the system of Wang et al. (2015a) and Wang et al. (2015b), with additions that utilize a Feedforward Neural Network classifier and an enriched feature set. We observed that exploiting Neural Networks in Abstract Meaning Representation parsing is challenging and we could not benefit from it, while the feature enhancements yielded an improved performance over the baseline model. 1 2 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Dorr et al., 1998) is a semantic formalism which represents sentence meaning in a form of a rooted directed acyclic graph. AMR graph nodes represent concepts, labelled dir"
S16-1178,P15-1032,0,0.0502542,"Missing"
S16-1178,W09-1201,1,\N,Missing
S16-2012,P06-1117,0,0.380816,"Missing"
S16-2012,P14-1097,1,0.452225,"nguistic intuition used in its construction. Its great strength is also its downfall: adding new verbs, new senses, and new classes requires trained linguists - at least, to preserve the integrity of the resource. According to Levin’s hypothesis, knowing the set of allowable syntactic patterns for a verb sense is sufficient to make meaningful semantic classifications. Large-scale corpora provide an extremely comprehensive picture of the possible syntactic realizations for any particular verb. With enough data in the training set, even infrequent verbs have sufficient data to support learning. Kawahara et al. (2014) showed that, using a Dirichlet Process Mixture Model (DPMM), a VerbNet-like clustering of verb senses can be built from counts of syntactic features. We develop a model to extend VerbNet, using a large corpus with machine-annotated dependencies. We build on prior work by adding partial supervision from VerbNet, treating VerbNet classes as additional latent variables. The resulting clusters are more similar to the evaluation set, and each cluster in the DPMM predicts its VerbNet class distribution naturally. Because the technique is data-driven, it is easily adaptable to domainspecific corpora"
S16-2012,P03-1009,0,0.14761,"Missing"
S16-2012,P09-1033,0,0.0585909,"Missing"
S16-2012,D09-1026,0,0.0381471,"the verb sense induction and the clustering of verb senses, the features can be optimized for the distinct tasks. According to (Kawahara et al., 2014), the best features for inducing verb classes are joint slot:token pairs. For the verb clustering task, slot features which ignore the lexical items were the most effective. This aligns with Levin’s hypothesis of diathesis alternations - the syntactic contexts are sufficient for the clustering. In this paper, we re-create the second stage clustering with the same features, but add supervision. Supervised Topic Modeling (Mimno and McCallum, 2008; Ramage et al., 2009) builds on the Bayesian framework by adding, for each item, a Dirichlet Process Mixture Models The DPMM used in Kawahara et al. (2014) is shown in Figure 1. The clusters are drawn from a Dirichlet Process with hyperparameter α and base distribution G. The Dirichlet process prior creates a clustering effect described by the Chinese Restaurant Process. Each cluster is chosen proportionally to the number of elements it already 103 prediction about a variable of interest, which is observed at least some of the time. This encourages the topics to be useful at predicting a supervised signal, as well"
S16-2012,N07-1069,1,0.770948,"Missing"
S16-2012,N06-5006,0,\N,Missing
shinzato-etal-2008-large,J94-4001,1,\N,Missing
shinzato-etal-2008-large,kawahara-kurohashi-2006-case,1,\N,Missing
shinzato-etal-2008-large,I08-1025,1,\N,Missing
W09-1201,burchardt-etal-2006-salsa,1,0.483589,"Missing"
W09-1201,D07-1101,0,0.391748,"Missing"
W09-1201,W09-1202,0,0.0278745,"order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the different subtasks of the CoNLL shared task. The idea is to decompose the joint learning problem into four subtasks – syntactic dependency identification, syntactic dependency labeling, semantic dependency identification and semantic dependency labeling. The initial step is to use a pipeline approach to use the input of one subtask as input to the next, in the order specified. The iterative steps then use additional features that are not available in the initial step to improve the accuracy of the overall system. For ex"
W09-1201,W09-1205,0,0.222475,"al token; and (c), the existence of an edge between each pair of tokens. Subsequently, they combine the (possibly conflicting) output of the three classifiers by a ranking approach to determine the most likely structure that meets all well-formedness constraints. (Llu´ıs et al., 2009) present a joint approach based on an extension of Eisner’s parser to accommodate also semantic dependency labels. This architecture is similar to the one presented by the same authors in the past edition, with the extension to a second-order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the differen"
W09-1201,W09-1212,1,0.83271,"Missing"
W09-1201,S07-1008,1,0.697359,"Missing"
W09-1201,H05-1066,1,0.168004,"Missing"
W09-1201,W04-2705,1,0.527791,"Missing"
W09-1201,W09-1219,0,0.0294123,"Missing"
W09-1201,H05-1108,1,0.506508,"y, adding further manual labels where necessary. Then, we used frequency and grammatical realization information to map the remaining roles onto higher-numbered Arg roles. We considerably simplified the annotations provided by SALSA, which use a rather complex annotation scheme. In particular, we removed annotation for multi-word expressions (which may be non-contiguous), annotations involving multiple frames for the same predicate (metaphors, underspecification), and inter-sentence roles. The out-of-domain dataset was taken from a study on the multi-lingual projection of FrameNet annotation (Pado and Lapata, 2005). It is sampled from the EUROPARL corpus and was chosen to maximize the lexical coverage, i.e., it contains of a large number of infrequent predicates. Both syntactic and semantic structure were annotated manually, in the TIGER and SALSA format, respectively. Since it uses a simplified annotation schemes, we did not have to discard any annotation. For both datasets, we converted the syntactic TIGER (Brants et al., 2002) representations into dependencies with a similar set of head-finding rules used for the preparation of the CoNLL-X shared task German dataset. Minor modifications (for the con1"
W09-1201,C08-1085,1,0.175934,"Missing"
W09-1201,J05-1004,0,0.213522,"nnotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation. For the CoNLL-2008 shared task evaluation, this corpus was extended by the task organizers to cover the subset of the Brown corpus used as a secondary testing dataset. From this corpus we only used NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. • Proposition Bank I (PropBank) – The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (Arg0, Arg1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types of adjuncts (ArgM-TMP, -ADV, etc.). • NomBank – NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns. Differences between PropBank and NomBank stem from differences between noun"
W09-1201,E09-1087,1,0.646818,"Missing"
W09-1201,W08-2121,1,0.597132,"Missing"
W09-1201,taule-etal-2008-ancora,0,0.543017,"Missing"
W09-1201,cmejrek-etal-2004-prague,1,0.63993,"Missing"
W09-1201,kawahara-etal-2002-construction,1,\N,Missing
W09-1201,J93-2004,0,\N,Missing
W09-1201,D07-1096,1,\N,Missing
W09-3817,P06-1105,0,0.022166,"r instance, Koo et al. (2008) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbitrary nodes such as sibling nodes. To solve the problem of knowledge scarcity, they learned knowledge of such predicate-argument structures from a very large number of automatically analyzed corpora (Abekawa and Okumura, 2006; Kawahara and Kurohashi, 2006b). While Abekawa and Okumura (2006) used only cooccurrence statistics of verbal arguments, Kawahara and Kurohashi (2006b) assessed predicateargument structures by checking case frames, which are semantic frames that are automatically compiled for each predicate sense from a large raw corpus. These methods outperformed the accuracy of supervised dependency parsers. In such linguistically-motivated approaches, well-formedness within a clause was considered, but coherence between clauses was not considered. Even if intra-clause relations (i.e., a predicate-argument"
W09-3817,W06-2920,0,0.0186736,"at are consistent within each clause and between clauses. We conﬁrm that this method contributes to the improvement of dependency parsing of Japanese. 1 Introduction The approaches of dependency parsing basically assess the likelihood of a dependency relation between two words or phrases and subsequently collect all the assessments for these pairs as the dependency parse of the sentence. To improve dependency parsing, it is important to consider as broad a context as possible, rather than a word/phrase pair. In the recent evaluation workshops (shared tasks) of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, 108 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 108–116, c Paris, October 2009. 2009 Association for Computational Linguistics ( a 1 ( ) p o i n t o  w a a 2 ( ) o i n t"
W09-3817,E06-1011,0,0.0324589,"Missing"
W09-3817,D07-1101,0,0.0538155,"Missing"
W09-3817,D07-1100,0,0.0263215,"Missing"
W09-3817,C04-1010,0,0.0118624,"quently collect all the assessments for these pairs as the dependency parse of the sentence. To improve dependency parsing, it is important to consider as broad a context as possible, rather than a word/phrase pair. In the recent evaluation workshops (shared tasks) of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, 108 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 108–116, c Paris, October 2009. 2009 Association for Computational Linguistics ( a 1 ( ) p o i n t o  w a a 2 ( ) o i n t  T 3 ) , p p a O o P i p o n t i n o t   w a T , O p o i p P o n t i n o  t  w a T , O P h h i t o t s u  n h i i o n e  D A T o m a o t r g o m a e n i t z t o t n e s u   D n i A i o T e m a e p t a c o m e t t o t n e s u   D n i A T e m a k o t r g o m a e n i t z e 3 3 3 3 3 3 3 3"
W09-3817,W04-3205,0,0.44609,"tion, ... mail, post, postage, ... .. . woman, ... baggage, supply, goods, ... person, Japan, parental house, ... mail, post, courier, ... .. . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 1 In this paper, we use the following abbreviations: NOM (nominative), ACC (accusative), ABL (ablative), CMI ("
W09-3817,P06-1015,0,0.0784278,"2 .. . systems, a lot of effort has been made to develop world knowledge or inference knowledge. For example, in the CYC (Lenat, 1995) and Open Mind (Stork, 1999) projects, such knowledge has been obtained manually, but it is difﬁcult to manually develop broad-coverage knowledge that is sufﬁcient for practical use in NLP applications. On the other hand, the automatic acquisition of such inference knowledge from corpora has attracted much attention in recent years. First, semantic knowledge between entities has been automatically obtained (Girju and Moldovan, 2002; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acquires entity pairs and extraction patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted entailment knowledge using coordina"
W09-3817,N06-1007,0,0.103357,"I, ... mail, message, information, ... friend, address, direction, ... mail, post, postage, ... .. . woman, ... baggage, supply, goods, ... person, Japan, parental house, ... mail, post, courier, ... .. . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 1 In this paper, we use the foll"
W09-3817,P02-1006,0,0.0289784,"edge between Case Frames 12 12 .. . systems, a lot of effort has been made to develop world knowledge or inference knowledge. For example, in the CYC (Lenat, 1995) and Open Mind (Stork, 1999) projects, such knowledge has been obtained manually, but it is difﬁcult to manually develop broad-coverage knowledge that is sufﬁcient for practical use in NLP applications. On the other hand, the automatic acquisition of such inference knowledge from corpora has attracted much attention in recent years. First, semantic knowledge between entities has been automatically obtained (Girju and Moldovan, 2002; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acquires entity pairs and extraction patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted en"
W09-3817,kawahara-kurohashi-2006-case,1,0.391505,"8) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbitrary nodes such as sibling nodes. To solve the problem of knowledge scarcity, they learned knowledge of such predicate-argument structures from a very large number of automatically analyzed corpora (Abekawa and Okumura, 2006; Kawahara and Kurohashi, 2006b). While Abekawa and Okumura (2006) used only cooccurrence statistics of verbal arguments, Kawahara and Kurohashi (2006b) assessed predicateargument structures by checking case frames, which are semantic frames that are automatically compiled for each predicate sense from a large raw corpus. These methods outperformed the accuracy of supervised dependency parsers. In such linguistically-motivated approaches, well-formedness within a clause was considered, but coherence between clauses was not considered. Even if intra-clause relations (i.e., a predicate-argument structure within a clause) are"
W09-3817,C04-1002,0,0.023783,"assessments for these pairs as the dependency parse of the sentence. To improve dependency parsing, it is important to consider as broad a context as possible, rather than a word/phrase pair. In the recent evaluation workshops (shared tasks) of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, 108 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 108–116, c Paris, October 2009. 2009 Association for Computational Linguistics ( a 1 ( ) p o i n t o  w a a 2 ( ) o i n t  T 3 ) , p p a O o P i p o n t i n o t   w a T , O p o i p P o n t i n o  t  w a T , O P h h i t o t s u  n h i i o n e  D A T o m a o t r g o m a e n i t z t o t n e s u   D n i A i o T e m a e p t a c o m e t t o t n e s u   D n i A T e m a k o t r g o m a e n i t z e 3 3 3 3 3 3 3 3 3 e h h t a k u"
W09-3817,N06-1023,1,0.323178,"8) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbitrary nodes such as sibling nodes. To solve the problem of knowledge scarcity, they learned knowledge of such predicate-argument structures from a very large number of automatically analyzed corpora (Abekawa and Okumura, 2006; Kawahara and Kurohashi, 2006b). While Abekawa and Okumura (2006) used only cooccurrence statistics of verbal arguments, Kawahara and Kurohashi (2006b) assessed predicateargument structures by checking case frames, which are semantic frames that are automatically compiled for each predicate sense from a large raw corpus. These methods outperformed the accuracy of supervised dependency parsers. In such linguistically-motivated approaches, well-formedness within a clause was considered, but coherence between clauses was not considered. Even if intra-clause relations (i.e., a predicate-argument structure within a clause) are"
W09-3817,P08-1068,0,0.0155249,"ld and Pereira, 2006; Carreras, 2007; Nakagawa, 2007). It is desirable to consider a wider-range phrase, clause, or a whole sentence, but it is difﬁcult to judge whether the structure of such a wide-range expression is linguistically correct. One of the reasons for this is the scarcity of the knowledge required to make such a judgment. When we use the Penn Treebank (Marcus et al., 1993), which is one of the largest corpora among the available analyzed corpora, as training data, even bi-lexical dependencies cannot be learned sufﬁciently (Bikel, 2004). To circumvent such scarcity, for instance, Koo et al. (2008) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbitrary nodes such as sibling nodes. To solve the problem of knowledge scarcity, they learned knowledge of such predicate-argument structures from a very large number of automatically analyzed corpora (Abekawa and Okumura, 2006; Ka"
W09-3817,C08-1107,0,0.0124509,"ed much attention in recent years. First, semantic knowledge between entities has been automatically obtained (Girju and Moldovan, 2002; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acquires entity pairs and extraction patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted entailment knowledge using coordinated verb pairs and noun-verb co-occurrences. Pekar (2006) also collected entailment knowledge with discourse structure constraints. Zanzotto et al. (2006) obtained entailment knowledge using nominalized verbs. There have been some studies on relations other than entailment relations. Chklovski and Pantel (2004) obtained verb pairs that have one of ﬁve semantic relations by using a search engine. Inui et al. (2005) classiﬁed"
W09-3817,W02-2016,0,0.040659,"words or phrases and subsequently collect all the assessments for these pairs as the dependency parse of the sentence. To improve dependency parsing, it is important to consider as broad a context as possible, rather than a word/phrase pair. In the recent evaluation workshops (shared tasks) of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, 108 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 108–116, c Paris, October 2009. 2009 Association for Computational Linguistics ( a 1 ( ) p o i n t o  w a a 2 ( ) o i n t  T 3 ) , p p a O o P i p o n t i n o t   w a T , O p o i p P o n t i n o  t  w a T , O P h h i t o t s u  n h i i o n e  D A T o m a o t r g o m a e n i t z t o t n e s u   D n i A i o T e m a e p t a c o m e t t o t n e s u   D n i A T e m a k o t r g o m a e n"
W09-3817,N06-1008,0,0.0536864,"... .. . person, I, ... mail, message, information, ... friend, address, direction, ... mail, post, postage, ... .. . woman, ... baggage, supply, goods, ... person, Japan, parental house, ... mail, post, courier, ... .. . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 1 In this paper, we"
W09-3817,J94-4001,1,0.60798,"Missing"
W09-3817,P06-1107,0,0.0442754,"message, information, ... friend, address, direction, ... mail, post, postage, ... .. . woman, ... baggage, supply, goods, ... person, Japan, parental house, ... mail, post, courier, ... .. . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 1 In this paper, we use the following abbreviations: NOM"
W09-3817,P98-2127,0,0.0136007,"ll remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 1 In this paper, we use the following abbreviations: NOM (nominative), ACC (accusative), ABL (ablative), CMI (comitative) and TOP (topic marker). 110 wo tsumu” (load baggage) and “busshi-wo tsumu” (load supply) are similar, they are clustered together. The similarity is measured by using a distributional thesaurus based on the study described in Lin (1998). In this section, we ﬁrst describe our unit of transition knowledge, case frames, brieﬂy. We then detail the acquisition method of the transition knowledge, and report experimental results. Finally, we refer to related work to the acquisition of such knowledge. 3.2 Acquisition of Transition Knowledge from Large Corpus 3.1 The Unit of Transition Knowledge: Case Frames To acquire the transition knowledge, we collect the clause pairs in a large raw corpus that have a dependency relation and represent them as pairs of case frames. For example, from the following sentence, a case frame pair, (mato"
W09-3817,J93-2004,0,0.0317551,", 619-0289, Japan dk@nict.go.jp Sadao Kurohashi Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan kuro@i.kyoto-u.ac.jp Abstract i.e., second-order and higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Nakagawa, 2007). It is desirable to consider a wider-range phrase, clause, or a whole sentence, but it is difﬁcult to judge whether the structure of such a wide-range expression is linguistically correct. One of the reasons for this is the scarcity of the knowledge required to make such a judgment. When we use the Penn Treebank (Marcus et al., 1993), which is one of the largest corpora among the available analyzed corpora, as training data, even bi-lexical dependencies cannot be learned sufﬁciently (Bikel, 2004). To circumvent such scarcity, for instance, Koo et al. (2008) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbit"
W09-3817,J04-4004,0,\N,Missing
W09-3817,J03-4003,0,\N,Missing
W09-3817,C98-2122,0,\N,Missing
W09-3817,I08-1065,0,\N,Missing
W09-3817,D07-1096,0,\N,Missing
W13-5714,candito-etal-2010-statistical,0,0.0627122,"Missing"
W13-5714,W11-3801,0,0.234471,"tep enables arguments for the verb to be correctly dependent on the main verb and the main verb to be dependent on the auxiliary verb in the ambiguous annotation scheme in the SJTree.1 Figure 1 shows the original SJTree phrase structure and its corresponding converted representation in dependency grammars. 3 Parsing Model Our parsing model gives a probability to each possible dependency tree T for a sentence S = e1 , e2 , ..., en , where ei is a Korean word. The model finally selects the dependency tree T ∗ that maximizes P (T |S) as follows: T ∗ = arg max P (T |S). (1) T 1 (Oh and Cha, 2010; Choi and Palmer, 2011) also introduced an conversion algorithm of dependency grammars for the SJTree. (Choi and Palmer, 2011) proposed head percolation rules for the SJTREE. However, we found some errors such as S related rules, where it gives lower priority to S than VP. It would fail to assign a head node correctly for S → VP S. Moreover, they did not consider auxiliary verb constructions annotated as VP in the SJTREE. According to their head rules, arguments for the main verb are dependent on the auxiliary verb instead of the main verb because of the annotation of the corpus (in general, VP → VP VP where the for"
W13-5714,P99-1062,1,0.536574,"inbu has 3 occurrences. 3 common = 0 if either i1 or i2 is not included in CoreNet. P ratioci = P p |ex | p |ey | j=1 i=1 (8) where i is the number of the occurrences of the instance examples of the same case marker and j is the number of the occurrences of the instance examples of the all case marker. 5.2 Constructing the database of N1 ui N2 structures We also integrate lexical information on Korean noun phrases of the form N1 ui N2 , which roughly corresponds to N2 of N1 in English. Even though Korean genitive marker ui does not have a broad usage as much as no in Japanese as described in (Kurohashi and Sakai, 1999), it sometime does not modify the immediate constituent such as Kyungjiui meylonhyang binwuleul (‘Melon-flavored soap of Kyungji’) where Kyungjiui modifies binwuleul instead of meylonhyang. The N1 ui N2 structure is very useful to recognize the meaning of natural language text can improve head-modifier relationships between genitive nouns. 6 Experiment and Results 6.1 Parsing results We use the Sejong Treebank corpus (SJTree) in our experiment.4 We use standard dataset split for training, development and testing. We report here final evaluation results on the baseline unlexicalized parsing and"
W13-5714,W12-3411,1,0.423407,"Missing"
W13-5714,W02-2207,0,0.597908,"Missing"
W13-5714,W10-1406,0,0.243484,"Missing"
W13-5714,N06-1023,1,0.788493,"ng. P (T |S) is defined as the product of probabilities as follows: P (T |S) = Y P (Epa , dist|eh ), (2) Epa ∈T where Epa represents a clause dominated by a predicate or a genitive nominal phrase, eh represents the head Korean word of Epa , and dist is the distance between Epa and eh . Instead of specifying the actual distance, it is classified into three bins: 1, 2 – 5, and 6 –. If the dependent Korean word appears right next to the head, the distance is 1. If it appears between 2 and 5, the distance is 2. If it appears past 6, the distance is 6. P (T |S) is calculated in the similar way as (Kawahara and Kurohashi, 2006a). We describe the outline of this model below. Each probability in equation (2) is decomposed into two ways according to the type of Epa . If Epa is a clause dominated by a predicate, it is decomposed into a predicate-argument structure (content part) P Am and a function part fm . eh is also decomposed into a content part ch and a function part fh . P (Epa , dist|eh ) = P (P Am , fm , dist|ch , fh ) = P (P Am |fm , dist, ch , fh ) × P (fm , dist|ch , fh ) ≈ P (P Am |fm , ch ) × P (fm , dist|fh ) (3) The first term in the last equation represents a fullylexicalized generative probability of t"
W13-5714,kawahara-kurohashi-2006-case,1,0.778624,"ng. P (T |S) is defined as the product of probabilities as follows: P (T |S) = Y P (Epa , dist|eh ), (2) Epa ∈T where Epa represents a clause dominated by a predicate or a genitive nominal phrase, eh represents the head Korean word of Epa , and dist is the distance between Epa and eh . Instead of specifying the actual distance, it is classified into three bins: 1, 2 – 5, and 6 –. If the dependent Korean word appears right next to the head, the distance is 1. If it appears between 2 and 5, the distance is 2. If it appears past 6, the distance is 6. P (T |S) is calculated in the similar way as (Kawahara and Kurohashi, 2006a). We describe the outline of this model below. Each probability in equation (2) is decomposed into two ways according to the type of Epa . If Epa is a clause dominated by a predicate, it is decomposed into a predicate-argument structure (content part) P Am and a function part fm . eh is also decomposed into a content part ch and a function part fh . P (Epa , dist|eh ) = P (P Am , fm , dist|ch , fh ) = P (P Am |fm , dist, ch , fh ) × P (fm , dist|ch , fh ) ≈ P (P Am |fm , ch ) × P (fm , dist|fh ) (3) The first term in the last equation represents a fullylexicalized generative probability of t"
W15-0813,P09-4001,1,0.792485,"ntradictory Event Pairs As mentioned above, the recognition of contradictory event pairs is related to RTE. In some RTE tasks, contradiction is one of the relations between text and hypothesis. For example, Harabagiu et al. (2006) proposed a method to recognize contradictions between texts using negation expressions, antonyms, and discourse analysis. Recognition of contradictory event pairs plays an important role in the systems that detect contradictions between information extracted from web texts. For example, there are several systems that detect contradictory information, such as WISDOM (Akamine et al., 2009), Statement Map (Murakami et al., 2009), and Dispute Finder (Ennals et al., 2010). 2.2 Acquisition of Contradictory Event Pairs Hashimoto et al. (2012) and Kloetzer et al. (2013) proposed methods for acquiring contradictory event pairs. Hashimoto et al. (2012) collected Japanese contradictory and consistent event pairs using templates of semantic polarities that indicate excitatory, inhibitory, and neutral properties. A template consists of a particle and a predicate, such as “を (particle) 破壊する (destroy)” and “を (particle) 進行させ る (develop).” They collected one million contradictory event pairs"
W15-0813,D12-1057,0,0.0743454,"Missing"
W15-0813,izumi-etal-2014-constructing,1,0.777391,"rs simultaneously, contradictions of such pairs have a strong relation with negation, such as ⟨having a meal, not having a meal⟩ and ⟨eating to excess, eating moderately⟩. There are also contradictory event pairs based on sibling relations, such as ⟨being in Tokyo, being in Kyoto⟩, where “Tokyo” and “Kyoto” have a sibling relation. We therefore classify negation and sibling relations into binary (e.g., “single” and “married”), discrete (e.g., “Tokyo” and “Kyoto”), and continuous (e.g., “expensive” and “cheap”). Furthermore, negation has the following two classes that can cause contradictions (Izumi et al., 2014): sequential event relations, such as “getting on” and “getting off,” and counterpart perspective relations, such as “selling” and “buying.” We added these classes to our taxonomy. The subclasses of simultaneous contradictions are detailed below. 1-a. binary When an event pair includes mutually exclusive antonyms (e.g., “single” and “married”) or a predicate and its negation (e.g., “going” and “not going”), these events are contradictory. We call such contradictory event pairs binary. 1-b. discrete When an event pair consists of predicates or arguments that have sibling relations, such as ⟨bei"
W15-0813,D13-1065,0,\N,Missing
W15-1701,D11-1145,1,0.689984,"tion. They judged whether the tweet was posted just after an earthquake using a support vector machine (SVM), and determined the seismic center from the formatted tweets. In addition, they developed a system that raises the alarm about an earthquake from the predicted results. Bollen et al. (2011) extracted the social mood, and predicted the stock price fluctuation N days from the day of observation by using evaluated data of the ’mood-related’ dictionary. As a result, they concluded that they could show the 3 days from the ’calm-mood’ day might be able to predict the stock price fluctuation. Aramaki et al. (2011) predicted an influenza epidemic from tweets. They showed the possibility of information extraction from the tweets that reflects the actual world’s situation by using language processing technologies. Boyd et al. (2010) examined a practice of retweeting as a way by which participants can be “in a conversation.” Paul and Dredze (2011) considered a broader range of public health applications for Twitter and showed quantitative correlations with public health data and qualitative evaluations of model output. Baldwin et al. (2013) explored how linguistically noisy or otherwise it is over a range"
W15-1701,I13-1041,0,0.0132289,"lm-mood’ day might be able to predict the stock price fluctuation. Aramaki et al. (2011) predicted an influenza epidemic from tweets. They showed the possibility of information extraction from the tweets that reflects the actual world’s situation by using language processing technologies. Boyd et al. (2010) examined a practice of retweeting as a way by which participants can be “in a conversation.” Paul and Dredze (2011) considered a broader range of public health applications for Twitter and showed quantitative correlations with public health data and qualitative evaluations of model output. Baldwin et al. (2013) explored how linguistically noisy or otherwise it is over a range of social media sources empirically over popular social media text types, in the form of YouTube comments, Twitter posts, web user forum posts, blog posts and Wikipedia. Yin et al. (2012) constructed a system architecture for leveraging social media to enhance emergency situation awareness with high-speed text streams retrieved from Twitter during natural disasters and crises. In these researches, the location of an SNS document plays an important role in extracting informa3 tion, and in most cases, rely on GPS function connect"
W15-1701,C12-1064,0,0.0329781,"he IP address of past content cannot be accessed, and this approach is becoming increasingly ineffective with the increased use of portable terminals. As a result, location name disambiguation should now focus on procedures that consider the original text. As information references, Web pages and change logs in Wikipedia have been used as the basis of location name disambiguation. These resources are homogeneous and manageable. In contrast, the numerous data on SNS often contain noise, which makes disambiguation unmanageable. A number of studies have investigated location name disambiguation. Han et al. (2012) extracted location-indicative words from tweet data by calculating the information gain ratios. Their paper states that the words improved the estimation performance of the users’ location. They concluded that the procedure requires relatively little memory, is fast, and could potentially be used by lexicographers to extract location-indicative words. Backstrom et al. (2008) developed a probabilistic framework to quantify the spatial variation manifested in search queries. This allowed them to obtain a measure of spatial dispersion that indicates regional information. Adams and Janowicz (2012"
W15-1701,P13-4002,0,0.0143475,"GPS information. 1 “I arrived at Prefectural Office Ave. from Shuri Station!” Introduction As the volume of documents on the Web increases, technologies to extract useful information from them become increasingly essential. For instance, information extracted from social network services (SNS) such as Twitter and Facebook is useful because it contains a lot of location-specific information. To extract such information, it is necessary to identify the location of each location-relevant expression within a document. However, many previous studies on SNS rely only on geo-tagged documents (e.g., (Han et al., 2013; Han et al., 2014)), which include GPS information, In this paper, we propose a method that identifies the locations of location expressions in Twitter tweets on the basis of the following two clues: (1) spatial proximity, and (2) temporal consistency. Spatial proximity assumes that all locations mentioned in a tweet are close to one another. In the above document, for example, we would assume that “Prefectural Office Ave.” is “Prefectural Office Ave. (Okinawa)” using the proximity between “Shuri Station” and “Prefectural Office Ave. (Okinawa)” The other clue is temporal consistency, 1 Semioc"
W15-3119,Y01-1001,0,0.637464,"an event and its participants (i.e., predicates and arguments such as agent, patient, locative, temporal and manner). Unlike syntactic level surface cases (i.e., dependency labels such as subject and object), semantic roles can be regarded as a deep case representation for predicates. Because of its ability to abstract the meaning of a sentence, SRL has been applied to many NLP applications, including information extraction (Christensen et al., 2010), question answering (Pizzato and Moll´a, 2008) and machine translation (Liu and Gildea, 2010). Semantically annotated corpora, such as FrameNet (Fillmore et al., 2001) and PropBank (Kingsbury and Palmer, 2002), make this type of automatic semantic structure analysis feasible by using supervised machine learning methods. Automatic SRL processing has two major drawbacks: 120 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 120–127, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing word similarity learned from unlabeled data as additional features for SRL. Word embeddings have also been used in several NLP tasks including SRL (Collober"
W15-3119,jin-etal-2014-framework,1,0.45784,"dency parsing and word similarity calculation, to multilingual applications, such as machine translation. Japanese case frames have been successfully compiled (Kawahara and Kurohashi, 2006), where each argument is represented as its case marker in Japanese such as ‘ga’, ‘wo’, and ‘ni’. For the case frames of other languages such as English and Chinese, because there are no such case markers that can help clarify syntactic structures, instead of using case markers like in Japanese, syntactic surface cases (i.e., subject, object, prepositional phrase, etc.) are used for argument representation (Jin et al., 2014). Case frames can be automatically acquired using a different method such as Chinese Restaurant Process (CRP) (Kawahara et al., 2014) for different languages. In our work, we employ such syntactic level knowledge, which use surface cases as argument representation, to help SRL task. We refer to this kind of knowledge as syntactic knowledge in this paper. Section 2 contains related work. Section 3 describes the high-quality dependency selection process. Section 4.1 presents a detailed description of our approach, conducted on three languages, along with the results followed by a discussion in S"
W15-3119,H01-1043,1,0.737508,"Missing"
W15-3119,N06-1023,1,0.942083,"onal preferences to improve SRL. This study is similar to our approaches but the quality of selectional preferences was not concerned at all. In syntactic level of NLP, rich knowledge such as predicate-argument structures and case frames are strong backups for various kinds of tasks. A case frame, which clarifies relations between a predicate and its arguments, can support tasks ranging from fundamental analysis, such as syntactic dependency parsing and word similarity calculation, to multilingual applications, such as machine translation. Japanese case frames have been successfully compiled (Kawahara and Kurohashi, 2006), where each argument is represented as its case marker in Japanese such as ‘ga’, ‘wo’, and ‘ni’. For the case frames of other languages such as English and Chinese, because there are no such case markers that can help clarify syntactic structures, instead of using case markers like in Japanese, syntactic surface cases (i.e., subject, object, prepositional phrase, etc.) are used for argument representation (Jin et al., 2014). Case frames can be automatically acquired using a different method such as Chinese Restaurant Process (CRP) (Kawahara et al., 2014) for different languages. In our work,"
W15-3119,W09-1206,0,0.263213,"Missing"
W15-3119,kawahara-kurohashi-2010-acquiring,1,0.835992,"to indicate the proposed features. 3.2 Syntactic knowledge acquisition We constructed two types of syntactic knowledge namely, predicate-argument structures and case frames. set feature of the children of predicate 3.2.1 High-quality predicate-argument structure extraction the concatenation of the dependency labels of predicate’s children Predicate-argument structures (PAS) have been basically acquired from syntactic analyses which varies from phrase chunking to syntactic dependency parsing. For example, English PAS in surface case was acquired in a large scale using a chunking-based system (Kawahara and Kurohashi, 2010). Some phenomena in Chinese, such as omission and complex grammar, make it intractable to automatically extract PAS only using shallow syntactic analysis, such as chunking. Syntactic dependency parsing is applied for Chinese PAS extraction. Arguments are represented by their syntactic dependency labels (i.e., subject, object, etc.) Due to various factors, Chinese syntactic dependency parsing is relatively worse in performance compared to that of English, Japanese, etc. However, using an existing treebank, it is possible to train a classifier to acquire high-quality PAS by only using highly rel"
W15-3119,W10-0907,0,0.320263,"tract a proposition from a sentence about who does what to whom, when, where and why. By using semantic roles, the complex expression of a sentence is then interpreted as an event and its participants (i.e., predicates and arguments such as agent, patient, locative, temporal and manner). Unlike syntactic level surface cases (i.e., dependency labels such as subject and object), semantic roles can be regarded as a deep case representation for predicates. Because of its ability to abstract the meaning of a sentence, SRL has been applied to many NLP applications, including information extraction (Christensen et al., 2010), question answering (Pizzato and Moll´a, 2008) and machine translation (Liu and Gildea, 2010). Semantically annotated corpora, such as FrameNet (Fillmore et al., 2001) and PropBank (Kingsbury and Palmer, 2002), make this type of automatic semantic structure analysis feasible by using supervised machine learning methods. Automatic SRL processing has two major drawbacks: 120 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 120–127, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language"
W15-3119,E14-1007,1,0.770777,"ave been successfully compiled (Kawahara and Kurohashi, 2006), where each argument is represented as its case marker in Japanese such as ‘ga’, ‘wo’, and ‘ni’. For the case frames of other languages such as English and Chinese, because there are no such case markers that can help clarify syntactic structures, instead of using case markers like in Japanese, syntactic surface cases (i.e., subject, object, prepositional phrase, etc.) are used for argument representation (Jin et al., 2014). Case frames can be automatically acquired using a different method such as Chinese Restaurant Process (CRP) (Kawahara et al., 2014) for different languages. In our work, we employ such syntactic level knowledge, which use surface cases as argument representation, to help SRL task. We refer to this kind of knowledge as syntactic knowledge in this paper. Section 2 contains related work. Section 3 describes the high-quality dependency selection process. Section 4.1 presents a detailed description of our approach, conducted on three languages, along with the results followed by a discussion in Section 4.2. Finally, Section 5 contains our conclusions and future work. 2 Related work The CoNLL-2009 shared task (Hajiˇc et al., 20"
W15-3119,D09-1003,0,0.267902,"Missing"
W15-3119,P08-1068,0,0.192546,"Missing"
W15-3119,C10-1081,0,0.156181,"antic roles, the complex expression of a sentence is then interpreted as an event and its participants (i.e., predicates and arguments such as agent, patient, locative, temporal and manner). Unlike syntactic level surface cases (i.e., dependency labels such as subject and object), semantic roles can be regarded as a deep case representation for predicates. Because of its ability to abstract the meaning of a sentence, SRL has been applied to many NLP applications, including information extraction (Christensen et al., 2010), question answering (Pizzato and Moll´a, 2008) and machine translation (Liu and Gildea, 2010). Semantically annotated corpora, such as FrameNet (Fillmore et al., 2001) and PropBank (Kingsbury and Palmer, 2002), make this type of automatic semantic structure analysis feasible by using supervised machine learning methods. Automatic SRL processing has two major drawbacks: 120 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 120–127, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing word similarity learned from unlabeled data as additional features for SRL. Word e"
W15-3119,W09-1203,0,0.37053,"Missing"
W15-3119,W08-1810,0,0.385679,"Missing"
W15-3119,W09-1217,0,0.201904,"s syntactic knowledge in this paper. Section 2 contains related work. Section 3 describes the high-quality dependency selection process. Section 4.1 presents a detailed description of our approach, conducted on three languages, along with the results followed by a discussion in Section 4.2. Finally, Section 5 contains our conclusions and future work. 2 Related work The CoNLL-2009 shared task (Hajiˇc et al., 2009) features a substantial number of studies on SRL that used Propbank as one of the resources. These work can be categorized into two types: joint learning of syntactic parsing and SRL (Tang et al., 2009; Morante et al., 2009), which learns a unique model for syntactic parsing and SRL jointly. This type of framework has the ability to use SRL information in syntactic parsing for improvement, but has a much larger search space during the joint model learning. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and"
W15-3119,D14-1041,0,0.391097,"Morante et al., 2009), which learns a unique model for syntactic parsing and SRL jointly. This type of framework has the ability to use SRL information in syntactic parsing for improvement, but has a much larger search space during the joint model learning. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2014). There were also several studies using semisupervised methods for SRL. One basic idea of semi-supervised SRL is to automatically annotate unlabeled data using a simple classifier trained on original training data (F¨urstenau and Lapata, 2009). Since there is a substantial amount of error propagation in SRL frameworks, the additional automatic semantic roles are not guaranteed to be of good quality. Contrary to this approach, we only rely on syntactic level knowledge which does not suffer too much from error propagation. Also, some studies assume that sentences that are syntactically and lexic"
W15-3119,P09-2019,0,0.477444,"Missing"
W15-3119,W09-1208,0,0.236766,"ins our conclusions and future work. 2 Related work The CoNLL-2009 shared task (Hajiˇc et al., 2009) features a substantial number of studies on SRL that used Propbank as one of the resources. These work can be categorized into two types: joint learning of syntactic parsing and SRL (Tang et al., 2009; Morante et al., 2009), which learns a unique model for syntactic parsing and SRL jointly. This type of framework has the ability to use SRL information in syntactic parsing for improvement, but has a much larger search space during the joint model learning. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2014). There were also several studies using semisupervised methods for SRL. One basic idea of semi-supervised SRL is to automatically annotate unlabeled data using a simple classifier trained on original training data (F¨urstenau and Lapata, 2009). Since there is a substantial amount of error propa"
W15-3119,kingsbury-palmer-2002-treebank,0,\N,Missing
W16-1006,P09-1068,0,0.0607792,"mework for acquiring such knowledge based on 46 Proceedings of the 4th Workshop on Events: Definition, Detection, Coreference, and Representation, pages 46–50, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics both an automatic approach and a manual approach (collective intelligence) that exploits merits of both approaches. That is, we acquire seed knowledge by using collective intelligence and expand the knowledge automatically. 2 tences are associated with FCAs. Related Work Many studies have aimed at automatically acquiring relationships between events (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010; Vanderwende, 2005; Shibata et al., 2014). However, these studies do not focus on the motivation of events or effects caused by the events. Although we can determine which events occur after an event by using the dictionaries constructed in such studies, we cannot understand why the events occur. For example, although we may know the event “a girl cries” happens after another event “a girl gets injured” by using the dictionary, we cannot understand why the former event occurs. To understand the motivation of the former event, we have to know that “the girl feels p"
W16-1006,chambers-jurafsky-2010-database,0,0.236238,"wledge based on 46 Proceedings of the 4th Workshop on Events: Definition, Detection, Coreference, and Representation, pages 46–50, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics both an automatic approach and a manual approach (collective intelligence) that exploits merits of both approaches. That is, we acquire seed knowledge by using collective intelligence and expand the knowledge automatically. 2 tences are associated with FCAs. Related Work Many studies have aimed at automatically acquiring relationships between events (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010; Vanderwende, 2005; Shibata et al., 2014). However, these studies do not focus on the motivation of events or effects caused by the events. Although we can determine which events occur after an event by using the dictionaries constructed in such studies, we cannot understand why the events occur. For example, although we may know the event “a girl cries” happens after another event “a girl gets injured” by using the dictionary, we cannot understand why the former event occurs. To understand the motivation of the former event, we have to know that “the girl feels pain.” Of course, the problem"
W16-1006,Y12-1058,1,0.68567,"d on crowdsourcing because our dictionary is designed to deal with subjective information such as emotions. (PHASE 3) Knowledge is automatically expanded by using the seed knowledge. The sequence of precedures described above is illustrated in Figure 2. We intend to expand the size of our dictionary based on the loop. 4 4.2 Method Experiment We conducted an experiment to validate whether FCAs in event sentences are adequately acquired by using crowdsourcing. 4.1 Data We created event sentences presented to crowdsourcing workers based on the “Kyoto University Web Document Leads Corpus (KWDLC) (Hangyo et al., 2012)”2 and the “Kyoto University Case Frames (KUCF) (Kawahara and Kurohashi, 2006).”3 The KWDLC is a Japanese text corpus that comprises 5,000 documents (15,000 sentences) with annotations of morphology, named entities, dependencies, predicate-argument structures including 2 3 zero anaphora and coreferences. The KUCF is a database of case frames automatically constructed from 1.6 billion Japanese sentences taken from Web pages. The KUCF has about 40,000 predicates, with 13 case frames on average for each predicate. The sentence creation procedure is as follows. (STEP 1) The 200 most frequent verbs"
W16-1006,P14-2065,0,0.132263,"edge from large corpora. However, since these studies do not focus on what happens after evoking various changes of emotions, they are not able to understand complex phenomenon involving such changes. Some dictionaries constructed manually that retain high quality are available. For example, ConceptNet1 built in the Open Mind Common Sense project (Singh, 2002) and a large-scale knowledge database built in the CYC project (Lenat, 1995) are available. The database built in the VerbCorner 1 Figure 1: Example entry in the proposed dictionary. Event senhttp://conceptnet5.media.mit.edu/ 47 project (Hartshorne et al., 2014), expanding of VerbNet (Kipper et al., 2000) through crowdsourcing, is also available. However, these studies do not focus on the granularity of knowledge. For the controlled granularity, we focus on the use of basic level features of arguments in event sentences. Since both of animate beings and inanimate objects can be arguments in sentences, we assume not only physical features but also mental features. The decision criteria for these features are described in Section 3.2. 3 Proposal 3.1 Architecture To construct a knowledge database that keeps an controlled granularity and can be used to u"
W16-1006,P13-1095,0,0.255101,"d why the former event occurs. To understand the motivation of the former event, we have to know that “the girl feels pain.” Of course, the problem can be solved by treating the phenomenon “the girl feels pain” as an event and describing it in the dictionary. However, such a policy requires infinite descriptions. It is preferable to form and maintain the dictionary with an controlled granularity. In the case that participants in events are animated beings, events may influence their emotions. Many studies developing software to process human emotions exploits models proposed by psychologists (Hasegawa et al., 2013; Tokuhisa et al., 2008; Tokuhisa et al., 2009; Vu et al., 2014). In these studies, Ekman’s Big Six Model (Ekman, 1992) and Plutchik’s wheel of emotions (Plutchik, 1980) are used to automatically extract emotion knowledge from large corpora. However, since these studies do not focus on what happens after evoking various changes of emotions, they are not able to understand complex phenomenon involving such changes. Some dictionaries constructed manually that retain high quality are available. For example, ConceptNet1 built in the Open Mind Common Sense project (Singh, 2002) and a large-scale kn"
W16-1006,kawahara-kurohashi-2006-case,1,0.916949,"Missing"
W16-1006,D12-1071,0,0.0133092,"on relations in texts, inference of human activities and their planning, and so on. As for event knowledge, we focus on feature changes of arguments (hereafter, FCAs) in event sentences as knowledge of events because there are several studies of infant cognitive development (Massey and Gelman, 1988; Baillargeon et al., 1989; Spelke et al., 1995) that report that even infants use information about the basic features of participants in events to understand the events. There are some trials suggesting the possibility that dictionaries of FCAs are useful resources for deep understanding of texts (Rahman and Ng, 2012; Goyal et al., 2013). In (Rahman and Ng, 2012), features of relationships between event causalities and polarities (positive / negative) of participants in the events are used as features for anaphora resolution. In (Goyal et al., 2013), a dictionary of various binary effects (success(+/-), motivation(+/-), and so on) on characters caused by events (that is, verbs) are used for automatic story generation. To construct a dictionary of FCAs, we propose a framework for acquiring such knowledge based on 46 Proceedings of the 4th Workshop on Events: Definition, Detection, Coreference, and Represen"
W16-1006,shibata-etal-2014-large,0,0.360425,"op on Events: Definition, Detection, Coreference, and Representation, pages 46–50, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics both an automatic approach and a manual approach (collective intelligence) that exploits merits of both approaches. That is, we acquire seed knowledge by using collective intelligence and expand the knowledge automatically. 2 tences are associated with FCAs. Related Work Many studies have aimed at automatically acquiring relationships between events (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010; Vanderwende, 2005; Shibata et al., 2014). However, these studies do not focus on the motivation of events or effects caused by the events. Although we can determine which events occur after an event by using the dictionaries constructed in such studies, we cannot understand why the events occur. For example, although we may know the event “a girl cries” happens after another event “a girl gets injured” by using the dictionary, we cannot understand why the former event occurs. To understand the motivation of the former event, we have to know that “the girl feels pain.” Of course, the problem can be solved by treating the phenomenon “"
W16-1006,D08-1027,0,0.00988946,"Missing"
W16-1006,C08-1111,0,0.552921,"occurs. To understand the motivation of the former event, we have to know that “the girl feels pain.” Of course, the problem can be solved by treating the phenomenon “the girl feels pain” as an event and describing it in the dictionary. However, such a policy requires infinite descriptions. It is preferable to form and maintain the dictionary with an controlled granularity. In the case that participants in events are animated beings, events may influence their emotions. Many studies developing software to process human emotions exploits models proposed by psychologists (Hasegawa et al., 2013; Tokuhisa et al., 2008; Tokuhisa et al., 2009; Vu et al., 2014). In these studies, Ekman’s Big Six Model (Ekman, 1992) and Plutchik’s wheel of emotions (Plutchik, 1980) are used to automatically extract emotion knowledge from large corpora. However, since these studies do not focus on what happens after evoking various changes of emotions, they are not able to understand complex phenomenon involving such changes. Some dictionaries constructed manually that retain high quality are available. For example, ConceptNet1 built in the Open Mind Common Sense project (Singh, 2002) and a large-scale knowledge database built"
W16-1006,E14-4025,0,0.0846701,"rmer event, we have to know that “the girl feels pain.” Of course, the problem can be solved by treating the phenomenon “the girl feels pain” as an event and describing it in the dictionary. However, such a policy requires infinite descriptions. It is preferable to form and maintain the dictionary with an controlled granularity. In the case that participants in events are animated beings, events may influence their emotions. Many studies developing software to process human emotions exploits models proposed by psychologists (Hasegawa et al., 2013; Tokuhisa et al., 2008; Tokuhisa et al., 2009; Vu et al., 2014). In these studies, Ekman’s Big Six Model (Ekman, 1992) and Plutchik’s wheel of emotions (Plutchik, 1980) are used to automatically extract emotion knowledge from large corpora. However, since these studies do not focus on what happens after evoking various changes of emotions, they are not able to understand complex phenomenon involving such changes. Some dictionaries constructed manually that retain high quality are available. For example, ConceptNet1 built in the Open Mind Common Sense project (Singh, 2002) and a large-scale knowledge database built in the CYC project (Lenat, 1995) are avai"
W16-1316,J15-4004,0,0.0235822,"word association game using a dialog system on smartphones as GWAP. We evaluate the quality of automatically acquired Japanese word associations based on logs obtained from game players. For example, if players correctly answer the keyword “glass” for the given associated words “fragile,” “cup,” and “reflect,” these associated words can be regarded as high quality for the keyword. We use such a game to evaluate automatically acquired word associations at no cost. 2 Related Work In recent years, crowdsourcing has been used for data construction and evaluation in NLP (e.g., (Snow et al., 2008; Hill et al., 2015; Schnabel et al., 2015)). Snow et al. (2008) demonstrated that annotations by crowdworkers have almost identical quality with those by experts in various NLP tasks. The motivation of crowdworkers in crowdsourcing is monetary. Another type of wisdom of crowds is GWAP, for which a player’s motivation differs from that of crowdsourcing. Their motivation is “enjoying the game.” Therefore, we need not pay for the players, and can reduce the number of low-quality or dishonest workers. Many approaches using GWAP have been proposed in the field of NLP, such as anaphora resolution (Hladk´a et al., 200"
W16-1316,P09-2053,0,0.07251,"Missing"
W16-1316,Q14-1035,0,0.0130811,"“enjoying the game.” Therefore, we need not pay for the players, and can reduce the number of low-quality or dishonest workers. Many approaches using GWAP have been proposed in the field of NLP, such as anaphora resolution (Hladk´a et al., 2009), paraphrasing (Poesio et al., 2013), constructing semantic network (Lafourcade, 2007), and word sense disambiguation (Venhuizen et al., 2013). However, they are designed in a text-based style. Text-based games are probably less enjoyable for players. Some studies have specifically examined nontext-based games, i.e., video games (Vannella et al., 2014; Jurgens and Navigli, 2014). Video games are familiar for ordinary people and are much more enjoyable than text-based games. For example, Van87 nella et al. (2014) developed a video game to validate the associations between images and senses. They reported that the annotation quality using the video game is better than crowdsourcing. The enjoyable game design improves the quality of annotations by crowds. However, because playing a video game requires a certain amount of time, it is a bit difficult to play it in one’s spare-time. Furthermore, developing an attractive video game would be a timeconsuming task. Our word as"
W16-1316,C14-1027,1,0.815809,"ave lower precision than manually created ones, which would harm the performance of subsequent NLP applications. To cope with potential difficulties existing both in manual methods and automatic ones, it is necessary to combine the two methods, taking their respective benefits. This paper presents a method for compiling large-scale word association knowledge of high quality by evaluating automatically acquired word associations manually, not by linguistic experts but by the wisdom of crowds. To make use of the wisdom of crowds, crowdsourcing has been employed widely (e.g., (Snow et al., 2008; Kawahara et al., 2014)). Crowdsourcing is a low-cost service that enlists numerous human workers to make judgments that are difficult for computers.3 However, costs are still high when we 2 A keyword can be a word or a phrase, but we call both “keywords.” 3 In this paper, we refer to microtask crowdsourcing as “crowdsourcing.” We present a design for acquiring word association knowledge of high quality on the basis of a game with a purpose (GWAP). We evaluate automatically acquired word associations using a word association game as a GWAP. In the word association game, a player is given a set of associated words as"
W16-1316,P98-2127,0,0.102355,"affiliated with Recruit Lifestyle Co., keyword has an association are designated as associated words. There are manually crafted resources of word associations, such as thesauri and ontologies, which have been compiled by lexicographers and which have contributed to many studies in NLP. However, it takes long times and high costs to create a large thesaurus. Furthermore, it is difficult to update it continuously to adapt to neologisms and the changing use of a word. To reduce the creation cost, methods for automatically acquiring word associations from a large corpus have been studied (e.g., (Lin, 1998; Mikolov et al., 2013)). Although such methods can acquire large-scale resources of word associations, they tend to have lower precision than manually created ones, which would harm the performance of subsequent NLP applications. To cope with potential difficulties existing both in manual methods and automatic ones, it is necessary to combine the two methods, taking their respective benefits. This paper presents a method for compiling large-scale word association knowledge of high quality by evaluating automatically acquired word associations manually, not by linguistic experts but by the wis"
W16-1316,W06-1664,0,0.0422799,"at many players can play easily. Moreover, we need not spend much time to develop it because the game system is simple. 3 Automatic Acquisition of Word Associations We first explain our model for acquiring word associations. Then, we describe a method for clustering the acquired word associations to efficiently make questions for the word association game. 3.1 Definition and Collection of Word Associations In general, many relations exist among words, such as hypernym-hyponym relations and part-whole relations. However, we do not care about the kind of relation but the strength between words. Matsuo et al. (2006) used pointwise mutual information (PMI) and chi-square as the strength measures of relations, and acquired associated words using graph clustering. Our method is based on this method, but uses word frequencies and PMI as the strength measures as proposed by Shin and Kurohashi (2014). We used a Japanese Web corpus of 4.2 million sentences and Japanese Wikipedia to acquire associated words for nouns, verbs, and adjectives. 3.2 Clustering Word Associations Next, we cluster the acquired associated words according to their basic meanings. By clustering associated words, we can not only structure a"
W16-1316,D15-1036,0,0.0160241,"ame using a dialog system on smartphones as GWAP. We evaluate the quality of automatically acquired Japanese word associations based on logs obtained from game players. For example, if players correctly answer the keyword “glass” for the given associated words “fragile,” “cup,” and “reflect,” these associated words can be regarded as high quality for the keyword. We use such a game to evaluate automatically acquired word associations at no cost. 2 Related Work In recent years, crowdsourcing has been used for data construction and evaluation in NLP (e.g., (Snow et al., 2008; Hill et al., 2015; Schnabel et al., 2015)). Snow et al. (2008) demonstrated that annotations by crowdworkers have almost identical quality with those by experts in various NLP tasks. The motivation of crowdworkers in crowdsourcing is monetary. Another type of wisdom of crowds is GWAP, for which a player’s motivation differs from that of crowdsourcing. Their motivation is “enjoying the game.” Therefore, we need not pay for the players, and can reduce the number of low-quality or dishonest workers. Many approaches using GWAP have been proposed in the field of NLP, such as anaphora resolution (Hladk´a et al., 2009), paraphrasing (Poesio"
W16-1316,D08-1027,0,0.0179698,"Missing"
W16-1316,P14-1122,0,0.116193,"g. Their motivation is “enjoying the game.” Therefore, we need not pay for the players, and can reduce the number of low-quality or dishonest workers. Many approaches using GWAP have been proposed in the field of NLP, such as anaphora resolution (Hladk´a et al., 2009), paraphrasing (Poesio et al., 2013), constructing semantic network (Lafourcade, 2007), and word sense disambiguation (Venhuizen et al., 2013). However, they are designed in a text-based style. Text-based games are probably less enjoyable for players. Some studies have specifically examined nontext-based games, i.e., video games (Vannella et al., 2014; Jurgens and Navigli, 2014). Video games are familiar for ordinary people and are much more enjoyable than text-based games. For example, Van87 nella et al. (2014) developed a video game to validate the associations between images and senses. They reported that the annotation quality using the video game is better than crowdsourcing. The enjoyable game design improves the quality of annotations by crowds. However, because playing a video game requires a certain amount of time, it is a bit difficult to play it in one’s spare-time. Furthermore, developing an attractive video game would be a tim"
W16-1316,W13-0215,0,0.0221177,"with those by experts in various NLP tasks. The motivation of crowdworkers in crowdsourcing is monetary. Another type of wisdom of crowds is GWAP, for which a player’s motivation differs from that of crowdsourcing. Their motivation is “enjoying the game.” Therefore, we need not pay for the players, and can reduce the number of low-quality or dishonest workers. Many approaches using GWAP have been proposed in the field of NLP, such as anaphora resolution (Hladk´a et al., 2009), paraphrasing (Poesio et al., 2013), constructing semantic network (Lafourcade, 2007), and word sense disambiguation (Venhuizen et al., 2013). However, they are designed in a text-based style. Text-based games are probably less enjoyable for players. Some studies have specifically examined nontext-based games, i.e., video games (Vannella et al., 2014; Jurgens and Navigli, 2014). Video games are familiar for ordinary people and are much more enjoyable than text-based games. For example, Van87 nella et al. (2014) developed a video game to validate the associations between images and senses. They reported that the annotation quality using the video game is better than crowdsourcing. The enjoyable game design improves the quality of an"
W16-1316,C98-2122,0,\N,Missing
W16-4402,W15-4656,1,0.500564,"e also been exploited to acquire knowledge in Chinese (Kuo et al., 2009; Kuo and Hsu, 2011) and Japanese (Nakahara, 2011; Nakahara and Yamada, 2013). The collected knowledge was registered in ConceptNet. Although it is generally hard to gather many players, our GWAP can reach many more users than previous studies because it is built on a running spoken dialogue system. Spoken dialogue systems on smartphones have been attracting much industrial and research interest in recent years. Several studies report that enjoyable user interactions are beneficial for dialogue systems (Jiang et al., 2015; Kobayashi et al., 2015; Sano et al., 2016). 3 Rapid Knowledge Acquisition from Quiz Game We use a quiz game on a spoken dialogue system to obtain large-scale, high-quality commonsense knowledge from many human players. 3.1 Japanese ConceptNet Our knowledge acquisition method follows the scheme of ConceptNet (Speer and Havasi, 2012). In ConceptNet, knowledge is expressed as a triple of two concepts and a relation linking them, where a concept is a word or a short phrase, and a relation consists of about 30 relations such as IsA, Causes, or Antonym. We call a triple a fact, and the two concepts are called a head and"
W16-4402,P16-1137,0,0.019872,"entities, we focus on commonsense knowledge, which has a wider range. The Cyc Project (Lenat, 1995), an early work on commonsense knowledge acquisition, recruited a small group of annotators to construct a knowledge base. Manually curated resources are accurate but expensive to build. Several studies have attempted to construct knowledge bases automatically. For example, Tandon et al. (2014) extracted knowledge from WordNet and Web texts. However, commonsense knowledge is likely to be omitted from texts because it is assumed that every person knows such knowledge (Gordon and Van Durme, 2013). Li et al. (2016) addressed this problem using knowledge base completion, in which existing knowledge is used to acquire more knowledge. However, their method needs some amount of existing knowledge as a seed. Thus, manual effort is still required. Crowdsourcing, which is a process that requests various tasks of non-expert workers on the Internet, can be used to reduce the cost of the manual process. The OMCS project (Liu and Singh, 2004; Speer and Havasi, 2012) collected commonsense knowledge by recruiting many volunteers on the Internet. The resulting knowledge base is called ConceptNet,2 which we use and ex"
W16-4402,W16-1316,1,0.835681,"on-expert workers on the Internet, can be used to reduce the cost of the manual process. The OMCS project (Liu and Singh, 2004; Speer and Havasi, 2012) collected commonsense knowledge by recruiting many volunteers on the Internet. The resulting knowledge base is called ConceptNet,2 which we use and extend in our study. Some studies transform the manual acquisition process into an enjoyable game, called a GWAP, to motivate players to participate in knowledge acquisition. GWAPs are a form of crowdsourcing3 and have been used for validating (Herda˘gdelen and Barobni, 2012; Vannella et al., 2014; Machida et al., 2016) and collecting (von Ahn et al., 2006; Lieberman et al., 2007; Kuo et al., 2009; Nakahara, 2011; Kuo and Hsu, 2011; Nakahara and Yamada, 2013) language resources. A word-guessing game was designed by von Ahn et al. (2006) to collect a large amount of knowledge within a short time and at a low cost. GWAPs have also been exploited to acquire knowledge in Chinese (Kuo et al., 2009; Kuo and Hsu, 2011) and Japanese (Nakahara, 2011; Nakahara and Yamada, 2013). The collected knowledge was registered in ConceptNet. Although it is generally hard to gather many players, our GWAP can reach many more user"
W16-4402,D15-1276,1,0.87159,"and von Ahn, 2011). 4 From a snapshot taken on Sept. 10th , 2015 at http://conceptnet5.media.mit.edu/downloads/20150910/. 5 http://compling.hss.ntu.edu.sg/wnja/index.en.html 3 13 words and many relations linking them. Thus, collecting more commonsense facts is essential to making them at least as useful in NLP tasks as WordNet is. We only consider the filtered ConceptNet in the rest of this paper. Note that words in heads and tails are normalized into their representative forms (e.g., { みかん mikan, ミカン mikan, 蜜柑 mikan} (orange) → みかん mikan (orange)) given by the morphological analyzer JUMAN++ (Morita et al., 2015). 3.2 Building a Quiz from using ConceptNet We collect commonsense knowledge from many people. To motivate them, we transform the knowledge acquisition process into an enjoyable quiz game, where human players are given several hints about a certain word to be guessed, and we acquire knowledge from players’ guesses. The hints are easily generated from existing knowledge in ConceptNet. Figure 1 shows examples. “This is at supermarkets” and “this is made of milk” are generated from the facts (cake, AtLocation, supermarket) and (cake MadeOf, milk), respectively. The word to be guessed (hereafter k"
W16-4402,P16-1114,1,0.436906,"acquire knowledge in Chinese (Kuo et al., 2009; Kuo and Hsu, 2011) and Japanese (Nakahara, 2011; Nakahara and Yamada, 2013). The collected knowledge was registered in ConceptNet. Although it is generally hard to gather many players, our GWAP can reach many more users than previous studies because it is built on a running spoken dialogue system. Spoken dialogue systems on smartphones have been attracting much industrial and research interest in recent years. Several studies report that enjoyable user interactions are beneficial for dialogue systems (Jiang et al., 2015; Kobayashi et al., 2015; Sano et al., 2016). 3 Rapid Knowledge Acquisition from Quiz Game We use a quiz game on a spoken dialogue system to obtain large-scale, high-quality commonsense knowledge from many human players. 3.1 Japanese ConceptNet Our knowledge acquisition method follows the scheme of ConceptNet (Speer and Havasi, 2012). In ConceptNet, knowledge is expressed as a triple of two concepts and a relation linking them, where a concept is a word or a short phrase, and a relation consists of about 30 relations such as IsA, Causes, or Antonym. We call a triple a fact, and the two concepts are called a head and tail, respectively."
W16-4402,speer-havasi-2012-representing,0,0.386844,"r, commonsense knowledge is so clear for every person that it is often omitted in a text (Gordon and Van Durme, 2013). For instance, we rarely state in a text that strawberries are stored in refrigerators. Rather, we often talk about a major production region for strawberries. Therefore, manual effort is still required to build commonsense knowledge bases. To reduce the cost of manual knowledge acquisition, some studies explored the use of crowdsourcing, a process that requests various tasks of non-expert workers on the Internet. The Open Mind Common Sense (OMCS) project (Liu and Singh, 2004; Speer and Havasi, 2012) recruited volunteers on the Internet and constructed ConceptNet, a large collection of commonsense knowledge such as (cake, AtLocation, supermarket). Whereas participants in the OMCS projects entered the commonsense knowledge in Web forms, some studies have transformed the knowledge acquisition process into a type of enjoyable game, called games with a purpose (GWAP) (von Ahn et al., 2006; Lieberman et al., 2007; Kuo et al., 2009; Nakahara, 2011; Herda˘gdelen and Barobni, 2012; Kuo and Hsu, 2011). The advantage of a GWAP is that it is more attractive to humans than the standard annotation pro"
W16-4402,P14-1122,0,0.0301215,"ests various tasks of non-expert workers on the Internet, can be used to reduce the cost of the manual process. The OMCS project (Liu and Singh, 2004; Speer and Havasi, 2012) collected commonsense knowledge by recruiting many volunteers on the Internet. The resulting knowledge base is called ConceptNet,2 which we use and extend in our study. Some studies transform the manual acquisition process into an enjoyable game, called a GWAP, to motivate players to participate in knowledge acquisition. GWAPs are a form of crowdsourcing3 and have been used for validating (Herda˘gdelen and Barobni, 2012; Vannella et al., 2014; Machida et al., 2016) and collecting (von Ahn et al., 2006; Lieberman et al., 2007; Kuo et al., 2009; Nakahara, 2011; Kuo and Hsu, 2011; Nakahara and Yamada, 2013) language resources. A word-guessing game was designed by von Ahn et al. (2006) to collect a large amount of knowledge within a short time and at a low cost. GWAPs have also been exploited to acquire knowledge in Chinese (Kuo et al., 2009; Kuo and Hsu, 2011) and Japanese (Nakahara, 2011; Nakahara and Yamada, 2013). The collected knowledge was registered in ConceptNet. Although it is generally hard to gather many players, our GWAP c"
W16-5407,C96-2184,0,0.0597353,"nnotation are selected from the LCAS (National Science Library, Chinese Academy of Sciences) corpus provided by Japan Science and Technology Agency (JST). The LCAS corpus consists of Chinese scientific papers of various scientific subdomains. From this corpus, 780k abstracts were manually translated from Chinese to Japanese by JST (most of them also contain English translations). We randomly selected the raw sentences from the parallel part of the LCAS corpus, aiming for not only improving Chinese analysis but also multilingual NLP. 2.2 Annotation Standard Conventional segmentation standards (Huang et al., 1996; Xia et al., 2000; Duan et al., 2003) define words based on the analysis of morphology, which could lead to two problems: inconsistency and data sparsity. For example, based on the conventional segmentation standards, both “使用 (use)” and “使 用者 (user/use person)” in Figure 1 are one words, because “者 (person)” is a bound morpheme that cannot form a word itself. This leads to the inconsistent segmentation of “使用 (use)”, and also makes 3 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?A%20Chinese%20Treebank%20in%20Scientific%20Domain%20%28SCTB%29 60 both words sparse. In this work, we adopt the Chin"
W16-5407,P07-2045,0,0.00552999,"bowl wall section 26 defines an opening 28 and a leachate chamber 29 is located in the wall 24 beneath the opening 28. annular 碗状壁区 section 26 defines opening 28, leach liquid chamber 29 in the wall 24 opening 28 below. the annular bowl-shaped wall section 26 defines opening 28, leach liquid chamber 29 is positioned below the opening 28 in the wall 24. Table 5: An improved MT example. task at the NTCIR-10 workshop10 (Goto et al., 2013). The NTCIR-CE task uses 1,000,000, 2,000, and 2,000 sentences for training, development, and testing, respectively. We used the Moses tree-to-string MT system (Koehn et al., 2007) for all of our MT experiments. In our experiments, Chinese is in the tree format, and Japanese/English is in the string format. For Chinese, we used KyotoMorph for segmentations and the Berkeley parser for joint POS tagging and parsing. We binarized the parsing results for better translation rule extraction. We compared the MT performance of the “Baseline” and “Baseline+SCTB” settings in Section 3.1. For Japanese, we used JUMAN11 (Kurohashi et al., 1994) for the segmentation. For English, we tokenized the sentences using a script in Moses. For the Chinese-to-Japanese MT task, we trained a 5-g"
W16-5407,W04-3250,0,0.13052,"d a 5-gram language model for Japanese, on the training data of the ASPEC-CJ corpus using the KenLM toolkit12 with interpolated Kneser-Ney discounting. For the Chinese-to-English MT task, we trained a 5-gram language model for English, on the training data of the NTCIR-CE corpus using the same method. In all of our experiments, we used the GIZA++ toolkit13 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. Table 4 shows the translation results. The significance tests were performed using the bootstrap resampling method (Koehn, 2004). We can see that the significant improvements on Chinese analysis due to the annotated treebank, also lead to the significant MT performance improvements. Despite the language pair and slight domain difference, similar improvements are observed on both the ASPEC-CJ and NTCIR-CE MT tasks. To further understand the reasons for the improvements, we also investigated the translation results. We found that most of the improvements are due to analysis improvements of the source sentences. Table 5 shows an improved MT example from the NTCIR-CE task. We can see that there is an out-ofvocabulary word"
W16-5407,J93-2004,0,0.0540819,"Missing"
W16-5407,W15-5001,1,0.854089,"ences to the baseline treebanks for training the analyzers. Figure 3 shows the results. We can see that for segmentation and POS tagging, the accuracy improvements slow down when more annotated sentences are used for training the analyzers; while for parsing, there is still a large potential of improvement by annotating more sentences. 3.2 MT Experiments For Chinese-to-Japanese translation, we conducted experiments on the scientific domain MT task on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ)8 (Nakazawa et al., 2016), which is one subtask of the workshop on Asian translation (WAT)9 (Nakazawa et al., 2015). The ASPEC-CJ task uses 672,315, 2,090, and 2,107 sentences for training, development, and testing, respectively. For Chinese-to-English translation, we conducted experiments on the Chinese-English subtask (NTCIR-CE) of the patent MT 8 9 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ http://orchid.kuee.kyoto-u.ac.jp/WAT/ 63 System Baseline Baseline+SCTB ASPEC-CJ 39.12 40.08† NTCIR-CE 33.19 33.90† Table 4: BLEU-4 scores for ASPEC-CJ and NTCIR-CE translation tasks (“†” indicates that the result is significantly better than “Baseline” at p < 0.01). Source Reference Baseline Baseline +SCTB 环形碗状壁区段２６限定了开口"
W16-5407,L16-1262,0,0.021801,"Missing"
W16-5407,P03-1021,0,0.0123856,"d JUMAN11 (Kurohashi et al., 1994) for the segmentation. For English, we tokenized the sentences using a script in Moses. For the Chinese-to-Japanese MT task, we trained a 5-gram language model for Japanese, on the training data of the ASPEC-CJ corpus using the KenLM toolkit12 with interpolated Kneser-Ney discounting. For the Chinese-to-English MT task, we trained a 5-gram language model for English, on the training data of the NTCIR-CE corpus using the same method. In all of our experiments, we used the GIZA++ toolkit13 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. Table 4 shows the translation results. The significance tests were performed using the bootstrap resampling method (Koehn, 2004). We can see that the significant improvements on Chinese analysis due to the annotated treebank, also lead to the significant MT performance improvements. Despite the language pair and slight domain difference, similar improvements are observed on both the ASPEC-CJ and NTCIR-CE MT tasks. To further understand the reasons for the improvements, we also investigated the translation results. We found that most of the improvements"
W16-5407,C14-1026,0,0.0271706,"urce sentence in Table 5 “浸出 (leach ) /液 (liquid) /腔室 (chamber) /２９/位于 (is located) /壁 (wall) /２４/中 (in) /开口 (opening) /２８/之下 (beneath)”. chamber)” into “leach liquid chamber”, this is due to the similar analysis results of both systems, while the correct analysis for this noun phrase should be “(NP (NP 浸出 NN 液 SFN) 腔室 NN) (leachate chamber)”. 4 Related Work Besides the widely used CTB (Xue et al., 2005), there are two other treebanks for Chinese. The Peking University (PKU) annotated a Chinese treebank, firstly only for segmentations and POS tags (Yu et al., 2003), and later also for syntax (Qiu et al., 2014). The Harbin Institute of Technologys (HIT) also annotated a treebank for dependency structures (Che et al., 2012). Besides the difference in annotation standards and syntactic structures, all the three treebanks are in news domain. CTB selected the raw sentences from People’s Daily, Hong Kong newswire, Xinhua newswire etc., and PKU and HIT selected the raw sentences from People’s Daily newswire. To the best of our knowledge, our treebank is the first publicly available Chinese treebank in scientific domain. Three are two types of syntactic grammars for treebanking: phrase structures and depen"
W16-5407,P14-2042,1,0.898261,"Missing"
W16-5407,C16-1029,1,0.912447,"of downstream applications such as text mining and machine translation (MT). Motivated by this, we decide to construct a Chinese treebank in the scientific domain (SCTB) to promote Chinese NLP research in this domain. This paper presents the details of our treebank annotation process and the experiments conducted on the annotated treebank. The raw sentences are selected from Chinese scientific papers. Our annotation process follows that of CTB (Xue et al., 2005) with an exception of the segmentation standard. We apply a Chinese word segmentation standard based on character-level POS patterns (Shen et al., 2016), aiming to circumvent inconsistency and address data 1 2 https://catalog.ldc.upenn.edu/LDC2005T01 Statistics from Japan Patent Office. 59 Proceedings of the 12th Workshop on Asian Language Resources, pages 59–67, Osaka, Japan, December 12 2016. Figure 1: A screenshot of the annotation interface containing an annotation example of a Chinese sentence “烟草 (tobacco) /使用 (use) /是 (is) /当今 (nowadays) /世界 (world) /最大 (biggest) /的 (’s) /可 (can) /预防 (prevention) /死因 (cause of death) /，/烟草 (tobacco) /使用 (use) /者 (person) /中 (among) /近 (about) /一半 (half) /将 (will) /死于 (die) /烟草 (tobacco) /使用 (use) /。” ("
W16-5407,L16-1249,0,0.024799,"rase structures and dependency structures. We adopt the phrase structures used in CTB (Xue et al., 2005), because phrase structures can be converted to dependency structures based on predefined head rules using e.g. the Penn2Malt toolkit.14 Treebanks with multi-view of both phrase structures and dependency structures also have been proposed (Qiu et al., 2014). Recently, with more needs of multilingual NLP, the interests of constructing multilingual treebanks have increased. Multilingual treebanks such as the universal dependency treebank15 (Nivre et al., 2016) and the Asian language treebank (Thu et al., 2016) are being constructed. As the raw sentences of our treebank were selected from parallel data and the translated Japanese and English sentences are available, we leave the potential to develop our treebank to a trilingual one. 5 Conclusion In this paper, we presented the details of the annotation of SCTB: a Chinese treebank in scientific domain. Experiments conducted for Chinese analysis and MT verified the effectiveness of the annotated SCTB. As future work, firstly, we plan to annotate more sentences, and we aim to finish the annotation for 10k sentences within this year. Secondly, we also p"
W16-5407,xia-etal-2000-developing,0,0.591073,"ed from the LCAS (National Science Library, Chinese Academy of Sciences) corpus provided by Japan Science and Technology Agency (JST). The LCAS corpus consists of Chinese scientific papers of various scientific subdomains. From this corpus, 780k abstracts were manually translated from Chinese to Japanese by JST (most of them also contain English translations). We randomly selected the raw sentences from the parallel part of the LCAS corpus, aiming for not only improving Chinese analysis but also multilingual NLP. 2.2 Annotation Standard Conventional segmentation standards (Huang et al., 1996; Xia et al., 2000; Duan et al., 2003) define words based on the analysis of morphology, which could lead to two problems: inconsistency and data sparsity. For example, based on the conventional segmentation standards, both “使用 (use)” and “使 用者 (user/use person)” in Figure 1 are one words, because “者 (person)” is a bound morpheme that cannot form a word itself. This leads to the inconsistent segmentation of “使用 (use)”, and also makes 3 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?A%20Chinese%20Treebank%20in%20Scientific%20Domain%20%28SCTB%29 60 both words sparse. In this work, we adopt the Chinese word segmentat"
W16-5407,zhang-etal-2004-interpreting,0,0.102482,"Missing"
W17-6301,Q13-1034,0,0.0388365,"Missing"
W17-6301,P12-1110,0,0.0284407,"is organized as follows. Section 2 summarizes previous joint models for morphological and dependency analysis. Section 3 describes our method for constructing lexical knowledge. Section 4 illustrates our idea and describes our joint analysis model in detail. Section 5 is devoted to our experiments. Finally, section 6 gives the conclusions. 2 Related Work Some variants of transition-based parsing methods have been proposed for joint POS tagging and parsing (Bohnet and Nivre, 2012; Bohnet et al., 2013; Wang and Xue, 2014) and joint Chinese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual dec"
W17-6301,D09-1060,0,0.076172,"Missing"
W17-6301,N06-1023,1,0.865037,"with Fujitsu Laboratories Ltd. 3 In this paper, we use the following abbreviations: NOM (nominative), ACC (accusative), DAT (dative), LOC (locative), ABL (ablative), and TOP (topic marker). 1 Proceedings of the 15th International Conference on Parsing Technologies, pages 1–10, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics mentation and POS tagging using recurrent neural network language models, but does not perform dependency parsing. We employ this morphological analyzer, JUMAN++4 , as a pre-processor to generate word lattice (described in Section 4.1). Kawahara and Kurohashi (2006) proposed a probabilistic model for Japanese dependency parsing and PAS analysis based on case frames automatically compiled from a large raw corpus, which are also used as a source of selectional preferences in our model (described in Section 3.1). Kudo and Matsumoto (2002), Sassano (2004), Iwatate (2012) and Yoshinaga and Kitsuregawa (2014) proposed supervised models for Japanese dependency parsing without using external knowledge sources. These models need a 1-best output of segmentation and POS tagging as an input, and are not a joint model of morphological analysis and dependency parsing."
W17-6301,1993.eamt-1.1,0,0.508974,"Missing"
W17-6301,P11-2124,0,0.0194077,"transition-based parsing methods have been proposed for joint POS tagging and parsing (Bohnet and Nivre, 2012; Bohnet et al., 2013; Wang and Xue, 2014) and joint Chinese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the randomized greedy algorithm, respectively. Of these methods, Goldberg et al. (2009) incorporated an external morphological lexicon, which does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS tagging, and constituency parsing. They proposed a unified"
W17-6301,kawahara-etal-2002-construction,1,0.312831,"Missing"
W17-6301,E14-1007,1,0.846844,"hich are distinguished for each predicate sense or usage. Although PropBank was elaborated by hand and does not have frequency information, we automatically compile large-scale case frames that reflect real predicate uses. Each predicate has several case frames that are semantically distinguished. Each case frame consists of case slots, each of which consists of word instances that can be filled. Examples of Japanese case frames are shown in Table 1. Case frames are the source of selectional preferences, which are compiled by aggregating PASs for each predicate usage. We adapted the method of Kawahara et al. (2014) to Japanese case frame compilation. They proposed an unsupervised method for compiling English case frames from a large raw corpus. The procedure for inducing case frames is as follows: instances necessity:297865, case:190109, · · · thing:40, me:29, trend:29, · · · <time>:398 interest:34236, confidence:21326, · · · point:702, way:490, me:442, · · · feeling:70, aspect:58, · · · possibility:121867 price:23, myself:20, you:18, · · · step:4, influence:4, · · · .. . person:57, I:13, · · · road:24236, trail:4066, · · · parking:175, station:88, · · · I:35, parade:27, · · · city:13548, town:5336, par"
W17-6301,P08-1043,0,0.0295009,"ts. Finally, section 6 gives the conclusions. 2 Related Work Some variants of transition-based parsing methods have been proposed for joint POS tagging and parsing (Bohnet and Nivre, 2012; Bohnet et al., 2013; Wang and Xue, 2014) and joint Chinese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the randomized greedy algorithm, respectively. Of these methods, Goldberg et al. (2009) incorporated an external morphological lexicon, which does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word"
W17-6301,E09-1038,0,0.0184339,"the conclusions. 2 Related Work Some variants of transition-based parsing methods have been proposed for joint POS tagging and parsing (Bohnet and Nivre, 2012; Bohnet et al., 2013; Wang and Xue, 2014) and joint Chinese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the randomized greedy algorithm, respectively. Of these methods, Goldberg et al. (2009) incorporated an external morphological lexicon, which does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS taggi"
W17-6301,P08-1068,0,0.0355769,"ich does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS tagging, and constituency parsing. They proposed a unified decoding algorithm that combines the scores from these three models. This is a purely supervised method that does not use lexical knowledge. As dependency parsing models using lexical knowledge, there have been semi-supervised approaches that use knowledge of word classes, lexical preferences or selectional preferences acquired from raw corpora (e.g., (van Noord, 2007; Koo et al., 2008; Chen et al., 2009; Zhou et al., 2011; Bansal and Klein, 2011)). However, these dependency parsing models cannot be applied to joint morphological and dependency analysis. For Japanese, Morita et al. (2015) proposed a morphological analyzer that jointly performs seg3 Lexical Knowledge Acquisition In our joint analysis model, we use the following three types of lexical knowledge automatically acquired from a large raw corpus: case frames, cooccurrence probabilities of noun-noun / predicatepredicate dependencies, and word embeddings. We deeply utilize case frames in our joint model 4 http://nlp"
W17-6301,W02-2016,0,0.0969971,"Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics mentation and POS tagging using recurrent neural network language models, but does not perform dependency parsing. We employ this morphological analyzer, JUMAN++4 , as a pre-processor to generate word lattice (described in Section 4.1). Kawahara and Kurohashi (2006) proposed a probabilistic model for Japanese dependency parsing and PAS analysis based on case frames automatically compiled from a large raw corpus, which are also used as a source of selectional preferences in our model (described in Section 3.1). Kudo and Matsumoto (2002), Sassano (2004), Iwatate (2012) and Yoshinaga and Kitsuregawa (2014) proposed supervised models for Japanese dependency parsing without using external knowledge sources. These models need a 1-best output of segmentation and POS tagging as an input, and are not a joint model of morphological analysis and dependency parsing. We adopt KNP5 and CaboCha6 as baseline dependency parsers, which are implementations of Kawahara and Kurohashi (2006) and Sassano (2004), respectively.7 Tawara et al. (2015) proposed a joint model for Japanese morphological analysis and dependency parsing without lexical kn"
W17-6301,C10-1045,0,0.0413105,"Missing"
W17-6301,P14-1125,0,0.0205822,"ws. Section 2 summarizes previous joint models for morphological and dependency analysis. Section 3 describes our method for constructing lexical knowledge. Section 4 illustrates our idea and describes our joint analysis model in detail. Section 5 is devoted to our experiments. Finally, section 6 gives the conclusions. 2 Related Work Some variants of transition-based parsing methods have been proposed for joint POS tagging and parsing (Bohnet and Nivre, 2012; Bohnet et al., 2013; Wang and Xue, 2014) and joint Chinese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the ra"
W17-6301,J05-1004,0,0.0199068,"technique is complementary to our model and can be incorporated into our model in the future. 2 CS が ある:1 に (exist:1) time が ある:2 に (exist:2) で が ある:3 に (exist:3) で .. .. . . が あるく:1 を (walk:1) から が あるく:2 を (walk:2) で が あるく:3 を (walk:3) で .. .. . . and also consider these resources as features in our scoring function described in Section 4.2. Below, we describe the methods for constructing each of the resources, which are basically based on previous work. 3.1 Case Frames We use case frames to evaluate the plausibility of PASs. Case frames are predicate-specific semantic frames like PropBank (Palmer et al., 2005), which are distinguished for each predicate sense or usage. Although PropBank was elaborated by hand and does not have frequency information, we automatically compile large-scale case frames that reflect real predicate uses. Each predicate has several case frames that are semantically distinguished. Each case frame consists of case slots, each of which consists of word instances that can be filled. Examples of Japanese case frames are shown in Table 1. Case frames are the source of selectional preferences, which are compiled by aggregating PASs for each predicate usage. We adapted the method"
W17-6301,N15-1005,0,0.0130215,"POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the randomized greedy algorithm, respectively. Of these methods, Goldberg et al. (2009) incorporated an external morphological lexicon, which does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS tagging, and constituency parsing. They proposed a unified decoding algorithm that combines the scores from these three models. This is a purely supervised method that does not use lexical knowledge. As dependency parsing models using lexica"
W17-6301,D12-1046,0,0.0221564,"n proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the randomized greedy algorithm, respectively. Of these methods, Goldberg et al. (2009) incorporated an external morphological lexicon, which does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS tagging, and constituency parsing. They proposed a unified decoding algorithm that combines the scores from these three models. This is a purely supervised method that does not use lexical knowledge. As dependency parsing models using lexical knowledge, there have been semi-supervised approaches that use knowledge of word classes, lexical preferences or selectional preferences acquired from raw corpora (e.g., (van Noord, 2007; Koo et al., 2008; Chen et al., 2009; Zhou et al., 2011; Bansal and Klein, 2011)). However, these dependency p"
W17-6301,P11-1156,0,0.0181279,"erences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS tagging, and constituency parsing. They proposed a unified decoding algorithm that combines the scores from these three models. This is a purely supervised method that does not use lexical knowledge. As dependency parsing models using lexical knowledge, there have been semi-supervised approaches that use knowledge of word classes, lexical preferences or selectional preferences acquired from raw corpora (e.g., (van Noord, 2007; Koo et al., 2008; Chen et al., 2009; Zhou et al., 2011; Bansal and Klein, 2011)). However, these dependency parsing models cannot be applied to joint morphological and dependency analysis. For Japanese, Morita et al. (2015) proposed a morphological analyzer that jointly performs seg3 Lexical Knowledge Acquisition In our joint analysis model, we use the following three types of lexical knowledge automatically acquired from a large raw corpus: case frames, cooccurrence probabilities of noun-noun / predicatepredicate dependencies, and word embeddings. We deeply utilize case frames in our joint model 4 http://nlp.ist.i.kyoto-u.ac.jp/EN/?JUMAN++ http:"
W17-6301,C04-1002,0,0.215265,". 2017 Association for Computational Linguistics mentation and POS tagging using recurrent neural network language models, but does not perform dependency parsing. We employ this morphological analyzer, JUMAN++4 , as a pre-processor to generate word lattice (described in Section 4.1). Kawahara and Kurohashi (2006) proposed a probabilistic model for Japanese dependency parsing and PAS analysis based on case frames automatically compiled from a large raw corpus, which are also used as a source of selectional preferences in our model (described in Section 3.1). Kudo and Matsumoto (2002), Sassano (2004), Iwatate (2012) and Yoshinaga and Kitsuregawa (2014) proposed supervised models for Japanese dependency parsing without using external knowledge sources. These models need a 1-best output of segmentation and POS tagging as an input, and are not a joint model of morphological analysis and dependency parsing. We adopt KNP5 and CaboCha6 as baseline dependency parsers, which are implementations of Kawahara and Kurohashi (2006) and Sassano (2004), respectively.7 Tawara et al. (2015) proposed a joint model for Japanese morphological analysis and dependency parsing without lexical knowledge. However"
W17-6301,W07-2201,0,0.0920682,"Missing"
W17-6301,P14-1069,0,0.0278021,"nowledge which includes selectional preferences. pipeline models. The remainder of this paper is organized as follows. Section 2 summarizes previous joint models for morphological and dependency analysis. Section 3 describes our method for constructing lexical knowledge. Section 4 illustrates our idea and describes our joint analysis model in detail. Section 5 is devoted to our experiments. Finally, section 6 gives the conclusions. 2 Related Work Some variants of transition-based parsing methods have been proposed for joint POS tagging and parsing (Bohnet and Nivre, 2012; Bohnet et al., 2013; Wang and Xue, 2014) and joint Chinese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice,"
W17-6301,P13-2110,0,0.0238419,"ese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the randomized greedy algorithm, respectively. Of these methods, Goldberg et al. (2009) incorporated an external morphological lexicon, which does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS tagging, and constituency parsing. They proposed a unified decoding algorithm that combines the scores from these three models. This is a purely supervised method that does not use lexical knowledge. As dependency par"
W17-6301,C14-1103,0,0.0286068,"Missing"
W17-6301,D12-1133,0,\N,Missing
W17-6301,P11-1070,0,\N,Missing
W17-6301,D15-1276,1,\N,Missing
W17-6301,Y12-1058,1,\N,Missing
W19-6704,H92-1086,0,0.135738,"-items, we conducted a websurvey and statistical analysis to identify the fivefactor structure, calculate reliability, and examine validity. Exploratory Factor Analysis (EFA) is a statistical approach to extract common factors across measured variables based on correlation coefficients (Fabrigar et al., 1999). In constructing a psychological questionnaire, it is important to evaluate reliability and validity. Reliability indicates how responses are reliably produced. Internal consistency assumes that a person tends to similarly answer items within the same trait, which Cronbach’s α indicates (Cronbach, 1951). Furthermore, a psychological questionnaire must measure the targeted concepts, which is named as validity. One method to assess validity is criterion-validity. Criterion-validity investigates correlations between the latent variables in the newly constructed questionnaire and the corresponding latent variables in a “criterion” questionnaire. The correlations are expected to be high between similar latent variables and low between unrelated latent variables. We use TIPI-J (Oshio et al., 2012) for this validity evaluation. 4.1 Web-Survey Participants: We conducted a web-survey on registrants o"
Y12-1033,P05-1022,0,0.845516,"plore more expressive features and have proven significant improvement on parsing accuracy. However, the power of higherorder models comes with the cost of expensive computation and sometimes it requires aggressive pruning in the pre-processing. Another line of research that explores complex feature representations is parse reranking. In its general framework, a K-best list of parse tree candidates is first produced from the base parser; a reranker is then applied to pick up the best parse among these candidates. For constituent parsing, successful results has been reported in (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). For dependency parsing, the efficient algorithms for produce K-best list for graph-based parsers have been proposed in (Huang and Chiang, 2005) for projective parsing and in (Hall, 2007) for nonprojective parsing; Improvements on dependency accuracy has been achieved in (Hall, 2007; Hayashi et al., 2011). However, the feature sets in these studies explored a relatively small context, either by emulating the feature set in the constituent parse reranking, or by factorizing the search space. A desirable approach for the K-best list reranking is to encode features on subtrees extr"
Y12-1033,W02-1001,0,0.0495527,"a weight vector : ( ) ( ). (3) The feature vector is defined on the factor which means it is only able to capture tree-structure information from a small context. This can be seen as the off-set for performing exact inference. The goal of training a parser is to learn a weight vector that assigns scores to effectively discriminate good parses from bad parses. We use the edge factorization and the sibling factorization models described in (McDonald et al., 2005; McDonald and Pereira, 2006) to construct our base parsers. We learn the weight vector by applying the averaged perceptron algorithm (Collins, 2002) for its efficiency and stable performance. An illustration for generic perceptron algorithm is shown in Pseudocode 1. Pseudocode 1: Generic perceptron learning ) 1 for training data ( 2 for iteration 3  ( ) ( ) 4 if  5 ( ) ( ) 6 end 7 End 3. Parse Reranking In this section, we describe our reranking approach and introduce the feature set consists of three different types. 3.1 Overview of Parse Reranking The task of reranking is similar with that of parsing instead of that the searching of parse tree is performed on a K-best list with selected parse candidates rather than the entire search"
Y12-1033,D09-1060,0,0.0178368,"ollecting linguistic evidence, and efficient feature back-off strategy is proposed to relieve data sparsity. Through experiment we confirmed the effectiveness and efficiency of our method, and observed significant 316 improvement over the base system as well as other known systems. To further improve the proposed method, we mention several possibilities for our future work. An advantage of the reranking framework we used is that it has no overlap with many of the semisupervised parsing methods, such as word clustering (Koo et al., 2008) and subtree features integration using auto-parsed data (Chen et al., 2009). We are interested in the performance of our system when combining with these methods. Another interesting approach is to incorporate information from large-scale structured data, such as case frame (Kawahara and Kurohashi, 2006), which provides lexical predicate-argument selection preference and is an effective way to help to overcome data sparse problem in discriminative learning. While the relatively complex data structure in the case frame prohibits its incorporation in any existing factorization methods, it can be well utilized in the reranking framework with the proposed feature set. Re"
Y12-1033,C96-1058,0,0.00888878,"re unbalanced. Typically, when computing the trimmed subtree features, a candidate parse with most nodes being leaves will provide little information except on the root node, while on another parse that has fewer leaves and more depth we can have a bunch of features that give more information. This defect makes the comparison between candidates be “unfair” and thus less reliable. Therefore, it is natural to raise the question the other way round—whether a node is a good head for a subtree. To answer this question, we consider a dynamic programming structure called complete span introduced in (Eisner, 1996). A complete span consists of a head node and all its descendants on one side, which can also be transfer money from S2 to funds funds … … S1 … … S0 Figure 4. A complete span for the clause “transfer money from the new funds to other investment funds” where we omitted some of the details. This structure functions as a relatively independent and complete component in the entire parse tree. Features are encoded over the tuples: &lt;transfer, ,s2&gt;, &lt;transfer, s2,s1&gt;, &lt;transfer, s1,s0&gt;, &lt;transfer, s0,-&gt;. considered as a head node and sibling subtrees shown in Figure 4. In our observation, a complete"
Y12-1033,P07-1050,0,0.0849741,"uning in the pre-processing. Another line of research that explores complex feature representations is parse reranking. In its general framework, a K-best list of parse tree candidates is first produced from the base parser; a reranker is then applied to pick up the best parse among these candidates. For constituent parsing, successful results has been reported in (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). For dependency parsing, the efficient algorithms for produce K-best list for graph-based parsers have been proposed in (Huang and Chiang, 2005) for projective parsing and in (Hall, 2007) for nonprojective parsing; Improvements on dependency accuracy has been achieved in (Hall, 2007; Hayashi et al., 2011). However, the feature sets in these studies explored a relatively small context, either by emulating the feature set in the constituent parse reranking, or by factorizing the search space. A desirable approach for the K-best list reranking is to encode features on subtrees extracted from the candidate parse with arbitrary 308 Copyright 2012 by Mo Shen, Daisuke Kawahara, and Sadao Kurohashi Copyright 2012 by Mo Shen, Daisuke Kawahara, and Sadao Kurohashi 26th Pacific Asia Conf"
Y12-1033,D11-1137,0,0.0381078,"Missing"
Y12-1033,W05-1506,0,0.0384085,"ive computation and sometimes it requires aggressive pruning in the pre-processing. Another line of research that explores complex feature representations is parse reranking. In its general framework, a K-best list of parse tree candidates is first produced from the base parser; a reranker is then applied to pick up the best parse among these candidates. For constituent parsing, successful results has been reported in (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). For dependency parsing, the efficient algorithms for produce K-best list for graph-based parsers have been proposed in (Huang and Chiang, 2005) for projective parsing and in (Hall, 2007) for nonprojective parsing; Improvements on dependency accuracy has been achieved in (Hall, 2007; Hayashi et al., 2011). However, the feature sets in these studies explored a relatively small context, either by emulating the feature set in the constituent parse reranking, or by factorizing the search space. A desirable approach for the K-best list reranking is to encode features on subtrees extracted from the candidate parse with arbitrary 308 Copyright 2012 by Mo Shen, Daisuke Kawahara, and Sadao Kurohashi Copyright 2012 by Mo Shen, Daisuke Kawahara,"
Y12-1033,P08-1067,0,0.324314,"es and have proven significant improvement on parsing accuracy. However, the power of higherorder models comes with the cost of expensive computation and sometimes it requires aggressive pruning in the pre-processing. Another line of research that explores complex feature representations is parse reranking. In its general framework, a K-best list of parse tree candidates is first produced from the base parser; a reranker is then applied to pick up the best parse among these candidates. For constituent parsing, successful results has been reported in (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). For dependency parsing, the efficient algorithms for produce K-best list for graph-based parsers have been proposed in (Huang and Chiang, 2005) for projective parsing and in (Hall, 2007) for nonprojective parsing; Improvements on dependency accuracy has been achieved in (Hall, 2007; Hayashi et al., 2011). However, the feature sets in these studies explored a relatively small context, either by emulating the feature set in the constituent parse reranking, or by factorizing the search space. A desirable approach for the K-best list reranking is to encode features on subtrees extracted from the"
Y12-1033,kawahara-kurohashi-2006-case,1,0.871017,"Missing"
Y12-1033,P10-1001,0,0.144659,"tructures to perform an efficient dynamic programming search. This treatment however restricts the representation of features to in a local context which can be, for example, single edges or adjacent edges. Such restriction prohibits the model from exploring large or complex structures for linguistic evidence, which can be considered as the major drawback of the graphbased approach. Attempts have been made in developing more complex factorization techniques and corresponding decoding methods. Higher-order models that use grand-child, grand-sibling or trisibling factorization were proposed in (Koo and Collins, 2010) to explore more expressive features and have proven significant improvement on parsing accuracy. However, the power of higherorder models comes with the cost of expensive computation and sometimes it requires aggressive pruning in the pre-processing. Another line of research that explores complex feature representations is parse reranking. In its general framework, a K-best list of parse tree candidates is first produced from the base parser; a reranker is then applied to pick up the best parse among these candidates. For constituent parsing, successful results has been reported in (Collins,"
Y12-1033,D10-1004,0,0.029001,"Missing"
Y12-1033,P05-1012,0,0.676516,"ubtrees, and can be encoded efficiently. It exhaustively explores a candidate parse tree for features from the most simple to the most expressive while maintaining the efficiency in the sense that it does not add additional complexities over the K-best parsing. We choose the K-best list reranking framework rather than the forest reranking in (Huang, 2008) because an explicit representation of parse trees is needed in order to compute the features for reranking. We implemented an edge-factored parser and a second-order sibling-factored parser which emulate models in the MSTParser described in (McDonald et al., 2005; McDonald and Pereira, 2006) as our base parsers. In the rest part of this paper, we first give a brief description of the dependency parsing, then we describe the feature set for reranking, which is the major contribution of this paper. Finally, we present a set of experiment for the evaluation of our method. 2. Dependency Parsing The task of dependency parsing is to find a tree structure for a sentence in which edges represent the head-modifier relationship between words: each word is linked to a unique “head” such that the link forms a semantic dependency while the main predicate of the se"
Y12-1033,E06-1011,0,0.194514,"oded efficiently. It exhaustively explores a candidate parse tree for features from the most simple to the most expressive while maintaining the efficiency in the sense that it does not add additional complexities over the K-best parsing. We choose the K-best list reranking framework rather than the forest reranking in (Huang, 2008) because an explicit representation of parse trees is needed in order to compute the features for reranking. We implemented an edge-factored parser and a second-order sibling-factored parser which emulate models in the MSTParser described in (McDonald et al., 2005; McDonald and Pereira, 2006) as our base parsers. In the rest part of this paper, we first give a brief description of the dependency parsing, then we describe the feature set for reranking, which is the major contribution of this paper. Finally, we present a set of experiment for the evaluation of our method. 2. Dependency Parsing The task of dependency parsing is to find a tree structure for a sentence in which edges represent the head-modifier relationship between words: each word is linked to a unique “head” such that the link forms a semantic dependency while the main predicate of the sentence is linked to a dummy “"
Y12-1033,W96-0213,0,0.336353,"of branch of a node from its head. 4. Evaluation We present our experimental results on two languages, English and Chinese. For English experiment, we use the Penn Treebank WSJ part. We convert the constituent structure in the Treebank into dependency structure with the tool Penn2Malt and the head-extraction rule identical with that in (Yamada and Matsumoto, 2003). To align with previous work, we use the standard data division: section 02-21 for training, section 24 for development, and section 23 for testing. As our system assumes Part-of-Speech tags as input, we use MXPOST, a MaxEnt tagger (Ratnaparkhi, 1996) to automatically tag the test data. The tagger is trained on the same training data. For Chinese, we use the Chinese Treebank 5.0 with the following data division: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We use Penn2Malt to convert the Treebank into dependency structure and the set of head-extraction rules for Chinese is identical with the one in (Zhang and Clark, 2008). Moreover, for Chinese we use the gold standard Part-of-Speech tags in evaluation. We apply unlabeled attachment score (UAS) to measure the effectiveness of our"
Y12-1033,W09-3839,0,0.0531774,"Missing"
Y12-1033,D09-1058,0,0.013012,"parser in the training. It is much slower than the base parser in parsing new sentences, which is mainly due to the time required for outputting the 50-best candidates list; this can be seen as an unavoidable trade-off to obtain high accuracy in the reranking framework. 5. Related Work McDonald (2005, 2006) proposed an edge-factored parser and a second-order parser that both trained by discriminative online learning methods. Huang (2005) proposed the efficient algorithm for produce K-best list for graph-based parsers, which add a factor of to the parsing complexity of the base parser. Sangati (2009) has shown that a discriminative parser is very effective at filtering out bad parses from a factorized search space which agreed with the conclusion in (Hall, 2007) that an edge-factored model can reach good oracle performance when generating relatively small Kbest list. Successful results have been reported for constituent parse reranking in (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008), in which feature sets defined on constituent parses have been proposed that are able to capture rich non-local information. These feature sets, however, cannot be directly applied to parse tree un"
Y12-1033,W03-3023,0,0.0629629,"in each time, while for all nodes we encode their grammatical and positional information. Thus for the subtree (e) in Figure 5, a feature can appear as: 〈( s w )( )( )( )〉 A binary value, here we denote as “left” and “right”, is used to indicate the direction of branch of a node from its head. 4. Evaluation We present our experimental results on two languages, English and Chinese. For English experiment, we use the Penn Treebank WSJ part. We convert the constituent structure in the Treebank into dependency structure with the tool Penn2Malt and the head-extraction rule identical with that in (Yamada and Matsumoto, 2003). To align with previous work, we use the standard data division: section 02-21 for training, section 24 for development, and section 23 for testing. As our system assumes Part-of-Speech tags as input, we use MXPOST, a MaxEnt tagger (Ratnaparkhi, 1996) to automatically tag the test data. The tagger is trained on the same training data. For Chinese, we use the Chinese Treebank 5.0 with the following data division: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We use Penn2Malt to convert the Treebank into dependency structure and the se"
Y12-1033,C08-1132,1,0.900961,"Missing"
Y12-1033,D08-1059,0,0.0428817,"tion 02-21 for training, section 24 for development, and section 23 for testing. As our system assumes Part-of-Speech tags as input, we use MXPOST, a MaxEnt tagger (Ratnaparkhi, 1996) to automatically tag the test data. The tagger is trained on the same training data. For Chinese, we use the Chinese Treebank 5.0 with the following data division: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We use Penn2Malt to convert the Treebank into dependency structure and the set of head-extraction rules for Chinese is identical with the one in (Zhang and Clark, 2008). Moreover, for Chinese we use the gold standard Part-of-Speech tags in evaluation. We apply unlabeled attachment score (UAS) to measure the effectiveness of our method, which is the percentage of words that correctly identified their heads. For all experiments conducted, we use the parameters tuned in the development set. We train two base parsers which are the reimplementation of the first-order and second-order parsers in the MSTParser (McDonald et al., 2005; McDonald and Pereira, 2006) with 10 iterations on English and Chinese training dataset. We use 30way cross-validation on the identica"
Y12-1033,P11-2033,0,\N,Missing
Y12-1033,P08-1068,0,\N,Missing
Y12-1058,W07-1522,0,0.0901418,"pus that consists of document leads, we expect to raise the accuracy of the analysis of both document leads and the document as a whole. In this paper, we describe related work in Section 2. We describe the documents that the corpus consists of in Section 3 and the annotation criteria in Section 4. In Section 5 we discuss the statistics and properties of the corpus and conclude in Section 6. 536 2 Related Work Existing corpora that are annotated with predicateargument structures and anaphoric relations include the Kyoto University Text Corpus (Kawahara et al., 2002) and the Naist Text Corpus (Iida et al., 2007). These corpora are based on Mainich Newspaper articles from 1995 and annotated with predicateargument structures and anaphoric relations. Since there are only reports and editorial articles in the newspaper, the writing styles are consistent, making it difficult to adapt a semantic analysis system based on this corpus to texts other than newspaper articles. Corpora that consist of documents from various genres include the Balanced Corpus of Contemporary Written Japanese (BCCWJ)2 . BCCWJ includes publications such as books and magazines and text from the Internet. BCCWJ has publications form v"
Y12-1058,kawahara-etal-2002-construction,1,0.85845,"agate to the following analyses. By building a corpus that consists of document leads, we expect to raise the accuracy of the analysis of both document leads and the document as a whole. In this paper, we describe related work in Section 2. We describe the documents that the corpus consists of in Section 3 and the annotation criteria in Section 4. In Section 5 we discuss the statistics and properties of the corpus and conclude in Section 6. 536 2 Related Work Existing corpora that are annotated with predicateargument structures and anaphoric relations include the Kyoto University Text Corpus (Kawahara et al., 2002) and the Naist Text Corpus (Iida et al., 2007). These corpora are based on Mainich Newspaper articles from 1995 and annotated with predicateargument structures and anaphoric relations. Since there are only reports and editorial articles in the newspaper, the writing styles are consistent, making it difficult to adapt a semantic analysis system based on this corpus to texts other than newspaper articles. Corpora that consist of documents from various genres include the Balanced Corpus of Contemporary Written Japanese (BCCWJ)2 . BCCWJ includes publications such as books and magazines and text fr"
Y12-1058,rodriguez-etal-2010-anaphoric,0,0.0476145,"Missing"
Y12-1058,M95-1005,0,0.0202762,"phoric relations Table 8: Number of zero anaphora/exophora Author 602 8 78 23 711 Anaphora 2201 3185 757 6143 = NO  Total Author 100 256 31 387 Reader 29 96 24 149 Others 2072 2833 702 5607 Total 2201 3185 757 6143 Table 12: Breakdown of anaphoric relations indicates that most reference relations are anaphoric relations regardless of types. Since NO relations are more than , more bridging references can be rephrased as the form “A ͷ B.” The inter-annotator agreements are shown in Table 14 and Table 157 . Only the agreement of coreference, is annotated by “=,” is calculated by the MUC score (Vilain et al., 1995). For the agreement of other cases, we show only representative cases and “Total” includes cases that are omitted from the table. In Table 15, although the agreements of GA and WO are very high, that of GA2 is very low. It is because that GA2-case sometimes can be rephrased to other cases. For example, since it is possible to rephrase Example(11) to both (12) and (13), there are two annotation candidates, (11a) and (11b). We had set up a criterion that a case marker other than GA2 to which the target expression can be paraphrased is preferred to GA2. However, the judgment on such paraphrasing"
Y18-1026,P16-1101,0,0.0300583,"s feelings of being understood (Oishi et al., 2012). Drivers’ feelings of being understood by the system enhance the reliability and trustworthiness of the system, and improve the likelihood of following the system’s messages or cautions. For our future work, we are currently working on (1) expanding the corpus size with human annotations, (2) conducting further CRF experiments with more features such as modality, polarity, pronouns, zero-anaphora and emotions, and (3) conducting experiments with more recent machine learning methods such as Bidirectional LSTM-CRF (e.g., Huang, Xu, & Yu, 2015; Ma & Hovy, 2016; Reimers & Gurevych, 2017). The errors suggest that simple sequential tagging is not good enough to represent contexts that are located a far distance away. 6 References Conclusions and Future Work We constructed a Driving Experience Corpus by annotating behaviors and subjectivity. With the tagged textual data, we constructed a model that sought to understand driving scenes, behaviors as actors or observers and their subjectivity as humans recognize them, which helps the system to communicate with its users in a more colloquial and human-like manner. The annotation scheme and guidelines refle"
Y18-1026,P06-2059,0,0.0526234,"rs, including the present and the past, the date, the time, and the place. This tag refers to the evaluation of the behaviors. It refers to the emotions, cognitions, thoughts, judgments, and predictions held by the author as a result of actions. Subjectivity (SJ) Table 1: Definitions of the tags Polarity Evaluation (polarity per sentence)2, (2) the ACP corpus, (3) the Kyoto University and NTT blog corpus and (4) the Phrase Polarity Corpus. The Tsukuba Corpus consists of 4,309 sentences in the travel services domain with the polarity per sentence1. The ACP corpus was automatically constructed (Kaji & Kitsuregawa, 2006). This corpus is large (126,610 sentences), but it was not filtered by humans. Hashimoto et al. (2011) constructed the Kyoto University and NTT Blog Corpus of 249 blog articles (4,186 sentences) with sentiment information. These blogs were written by 81 university students in the following four themes: sight-seeing in Kyoto, cellphones, sports or gourmet food. Nakazawa et al. (2018) constructed a phrase-sentiment review corpus (59,758 phrases) with all manually annotated polarity information from the Tsukuba Corpus. These corpora do not focus on driving, thinking and cognition. Our corpus is d"
Y18-1026,D17-1035,0,0.0116143,"ng understood (Oishi et al., 2012). Drivers’ feelings of being understood by the system enhance the reliability and trustworthiness of the system, and improve the likelihood of following the system’s messages or cautions. For our future work, we are currently working on (1) expanding the corpus size with human annotations, (2) conducting further CRF experiments with more features such as modality, polarity, pronouns, zero-anaphora and emotions, and (3) conducting experiments with more recent machine learning methods such as Bidirectional LSTM-CRF (e.g., Huang, Xu, & Yu, 2015; Ma & Hovy, 2016; Reimers & Gurevych, 2017). The errors suggest that simple sequential tagging is not good enough to represent contexts that are located a far distance away. 6 References Conclusions and Future Work We constructed a Driving Experience Corpus by annotating behaviors and subjectivity. With the tagged textual data, we constructed a model that sought to understand driving scenes, behaviors as actors or observers and their subjectivity as humans recognize them, which helps the system to communicate with its users in a more colloquial and human-like manner. The annotation scheme and guidelines reflect the ideas and knowledge"
Y18-1026,C08-1103,0,0.0412832,"on and Driving Memo Previous studies related to driving in Japan basically focused only on driving actions, traffic rules and ontologies (e.g., Takayama et al., 2017; Kawabe et al., 2015; Suzuki et al., 2015; Taira et al., 2014). These datasets, however, are not publicly available. Furthermore, human-centered or human experiential foci are lacking. 2.3 Annotation 1/1 Regarding subjectivity, subjective analysis is one important and active domain. Subjectivity in this analysis refers to “information about any attitudes, beliefs, emotions, opinions, evaluations and sentiment expressed in texts” (Stoyanov & Cardie, 2008, p. 817). The previous studies of subjectivity analysis include emotions (e.g., Laskowski & Burger, 2006; (Tokuhisa et al., 2008, 2009), opinions (e.g., Jakob & Guvebychi, 2008), and sentiments (e.g., Hashimoto et al., 2011; Nakazawa et al., 2018). Regarding annotated corpora in this domain, four Japanese corpora have been constructed: (1) the University of Tsukuba Corpus Tagged with 1/1 223 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors 2 PACLIC 32 Tags Driving Experience (DE) Definitions A car driving experien"
