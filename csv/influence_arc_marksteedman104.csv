2007.sigdial-1.47,P95-1019,0,0.108462,"Missing"
2007.sigdial-1.47,P94-1009,0,0.0814573,"Missing"
2007.sigdial-1.47,P91-1007,0,0.183426,"Missing"
2007.sigdial-1.47,A00-2001,0,0.0573847,"Missing"
2007.sigdial-1.47,W94-0302,0,0.129288,"Missing"
2020.acl-main.378,N15-1006,1,0.875603,"Missing"
2020.acl-main.378,P16-1231,0,0.357808,", and show how it can be improved, to come as close as possible to the nonincremental version that uses the same embeddings, which has accuracy 87.5 F1, changing only the method of training, keeping the network architecture and embeddings the same. 3 3.1 Three sources of bias Label bias Label bias is a frequent bias present in some types of locally normalised models. It was first recognised by Bottou (1991), but became more widely known with the publication of CRFs (Lafferty et al., 2001). Here we give an explanation of labelbias in incremental parsing context. For a more formal treatment see Andor et al. (2016). In a general non-incremental setting, a discriminative parsing model assigns a probability to the whole transition sequence as p(y|x) where y = [y0 , y1 , . . . , ym ] is sequence of parsing actions and x = [x0 , x1 , . . . , xn ] is a sequence of words. Since the model is locally normalised we can express this conditional probability as the product of conditional probabilities of each parsing action: Q p(y|x) = i p(yi |y<i , x). In the non-incremental version of the parser there are no independence assumptions, so every parsing action can condition on the whole sequence of words x. However,"
2020.acl-main.378,J07-4004,0,0.312664,"se they are in the beam) and it gives significant improvements in parsing accuracy. 5 Experiments We have conducted experiments on English CCGbank (Hockenmaier and Steedman, 2007). For evaluation we use F1 score over labelled-directed and unlabelled-undirected dependencies. The parser is implemented in Scala and uses DyNet (Neubig et al., 2017) for the neural computation. The code is available on github.5 There are two dependency types often used in CCG parsing research: first one from (Clark et al., 2002) which is much closer to the typical CCG notion of dependencies and the second one from (Clark and Curran, 2007) which is more formalism-neutral but less expressive. The only implementation of the second method is the one in C&C parser and is not able to handle all the categories that come from CCGbank. This is the rea5 https://github.com/stanojevic/ Rotating-CCG/tree/incremental_max_margin son why most previous work on incremental CCG parsing has used the dependencies of Clark et al. (2002). In order to be able to compare to them we use the same dependencies. 5.1 Models tested We have tested the following methods: Disc Incremental discriminative model (the baseline). Disc-REINFORCE Discriminative model"
2020.acl-main.378,P02-1042,1,0.286363,"n training in any significant manner (we already have a forward pass for all the additional hypotheses because they are in the beam) and it gives significant improvements in parsing accuracy. 5 Experiments We have conducted experiments on English CCGbank (Hockenmaier and Steedman, 2007). For evaluation we use F1 score over labelled-directed and unlabelled-undirected dependencies. The parser is implemented in Scala and uses DyNet (Neubig et al., 2017) for the neural computation. The code is available on github.5 There are two dependency types often used in CCG parsing research: first one from (Clark et al., 2002) which is much closer to the typical CCG notion of dependencies and the second one from (Clark and Curran, 2007) which is more formalism-neutral but less expressive. The only implementation of the second method is the one in C&C parser and is not able to handle all the categories that come from CCGbank. This is the rea5 https://github.com/stanojevic/ Rotating-CCG/tree/incremental_max_margin son why most previous work on incremental CCG parsing has used the dependencies of Clark et al. (2002). In order to be able to compare to them we use the same dependencies. 5.1 Models tested We have tested"
2020.acl-main.378,P04-1015,0,0.168721,"some approximate methods to global models. For instance, instead of enumerating all hypotheses to compute normalization we could use a beam search as an approximation. This was done for CRF objective in (Zhou et al., 2015; Andor et al., 2016) and for (single-violation) M3 N objective in (Wiseman and Rush, 2016). They all need to compare in some way the gold hypothesis to the rest of the beam, but the issue arises when the gold hypothesis falls out of the beam. For that situation they use different heuristics. CRF approximation of Zhou et al. (2015) and Andor et al. (2016) uses Early update of Collins and Roark (2004). During training with Early update, the beam search is stopped when the gold hypothesis falls out of the beam and the parameter update is performed. In the BeamSearch Optimization (BSO) method of Wiseman and Rush (2016) an alternative heuristic is used from Daum´e III and Marcu (2005) called LaSO. LaSO does the update at the same point as Early but, unlike Early, it continues decoding by removing all elements of the beam except for the gold one. This will potentially result in another update for the same training instance. We have implemented most of these methods in attempt to improve increm"
2020.acl-main.378,D19-1106,0,0.0204979,"Missing"
2020.acl-main.378,D16-1001,0,0.0218442,"independence assumptions. Exposure bias happens because model is not exposed to its errors during training time. With dynamic oracle (Goldberg and Nivre, 2012) parser is trained on its own prediced history instead of the gold sequence of actions (static oracle). Whenever the model is in some sampled state (which is not necessarily a good state), we train the model to pick the transition that is a beginning of a path that would lead the parser to the ending state with the highest achievable metric score from that state. Finding such a transition is not trivial for all systems and all metrics (Cross and Huang, 2016). To this date there have been no proposals for a dynamic oracle for CCG parsing with F1 metric over CCG dependency structures and it is not even clear if there is a polynomial solution to this problem.3 Therefore this is not an option that we can use. An alternative is to use a reinforcement learning algorithm REINFORCE (Williams, 1992). REINFORCE samples derivations for training just like dynamic-oracle, but does not require design of a task-specific oracle extraction algorithm. Instead, it implicitly minimises the expected error of the desired metric.4 Fried and Klein (2018) have shown that"
2020.acl-main.378,J13-4008,0,0.146882,"is highly incremental, we argue below that there is no unequivocal evidence that it is more incremental than would be allowed under the Strict Competence Hypothesis (SCH) which states that the parser cannot construct any structure that is not licensed by the competence grammar, given CCG’s generalized notion of constituency (Steedman, 1989). Most research in incremental parsing has been directed at finding the right parsing algorithm (Abney and Johnson, 1991; Resnik, 1992; Hale, 2014; Stanojevi´c and Stabler, 2018) or grammar formalism (Steedman, 1989; Stabler, 1991; Sturt and Lombardo, 2005; Demberg et al., 2013; Stanojevi´c et al., 2020), but not much has been done in addressing the issue of finding the right oracle. Early approaches to this problem were lateclosure and minimal-attachment heuristics (Frazier, 1979; Pereira, 1985) which do not appear to be language universal (Cuetos and Mitchell, 1988). Altmann and Steedman (1988) have shown that these heuristics are overruled by human parser if the context gives evidence for a particular interpretation, in itself further evidence for processing incrementality at all levels. It seems natural to model the non-deterministic decision by using a probabil"
2020.acl-main.378,P15-1033,0,0.0168693,"to say that all unwanted biases are removed—for instance, beam search is still a biased search. However, the biases that remain do not have the drastic effect on performance of the three identified above. 2 Baseline model The parser of Stanojevi´c and Steedman (2019) already offers a fully incremental transition system with a non-incremental probabilistic model that gives state of the art accuracy in recovering predicate-argument dependencies. The parser encodes words using ELMo (Peters et al., 2018) and BiLSTM (Graves et al., 2005), sub-trees with tree encoders and the stack with Stack-LSTM (Dyer et al., 2015). This provides the encoding of the whole configuration together with the buffer, because the buffer is implicitly encoded via ELMo and Bi-LSTM, which look at the whole sentence. Given the hidden vector representation of the configuration, the parser uses a feed-forward network to determine the probability of the next action. 4112 There are three main types of transitions: • Parsing actions: shift and reduce(X) where X is a unary or binary combinatory rule; • Supertagging actions: tag(X) where X is one of the lexical supertags from English CCGbank (Hockenmaier and Steedman, 2007); • Right-adju"
2020.acl-main.378,N16-1024,0,0.0260404,"parser if the context gives evidence for a particular interpretation, in itself further evidence for processing incrementality at all levels. It seems natural to model the non-deterministic decision by using a probabilistic model which will condition on words and possibly on the context. Oracles of the modern broad coverage incremental parsers are without exception statistical in nature. The most typical statistical oracle is a locally normalised generative model ether in the form of simple PCFG (Stolcke, 1995; Hale, 2001), feature based (Roark and Johnson, 1999; Roark, 2001) or neural model (Dyer et al., 2016; Hale et al., 2018). RNNG (Dyer et al., 2016) is the main contemporary representative of this approach. RNNG is a top-down parser which in its first version used a non-incremental discriminative locallynormalised model. To make the parser fully incremental Dyer et al. (2016) exchanged the discriminative model for a generative one. This was not enough to get a working single-model incremental parser. Stern et al. (2017) added a couple more modifications to the search, namely wordsynchronous beams with a very large number of hypotheses, that gave good results. Could we just apply these same tec"
2020.acl-main.378,N01-1021,0,0.267349,"ltmann and Steedman (1988) have shown that these heuristics are overruled by human parser if the context gives evidence for a particular interpretation, in itself further evidence for processing incrementality at all levels. It seems natural to model the non-deterministic decision by using a probabilistic model which will condition on words and possibly on the context. Oracles of the modern broad coverage incremental parsers are without exception statistical in nature. The most typical statistical oracle is a locally normalised generative model ether in the form of simple PCFG (Stolcke, 1995; Hale, 2001), feature based (Roark and Johnson, 1999; Roark, 2001) or neural model (Dyer et al., 2016; Hale et al., 2018). RNNG (Dyer et al., 2016) is the main contemporary representative of this approach. RNNG is a top-down parser which in its first version used a non-incremental discriminative locallynormalised model. To make the parser fully incremental Dyer et al. (2016) exchanged the discriminative model for a generative one. This was not enough to get a working single-model incremental parser. Stern et al. (2017) added a couple more modifications to the search, namely wordsynchronous beams with a ve"
2020.acl-main.378,P18-1254,0,0.0147124,"xt gives evidence for a particular interpretation, in itself further evidence for processing incrementality at all levels. It seems natural to model the non-deterministic decision by using a probabilistic model which will condition on words and possibly on the context. Oracles of the modern broad coverage incremental parsers are without exception statistical in nature. The most typical statistical oracle is a locally normalised generative model ether in the form of simple PCFG (Stolcke, 1995; Hale, 2001), feature based (Roark and Johnson, 1999; Roark, 2001) or neural model (Dyer et al., 2016; Hale et al., 2018). RNNG (Dyer et al., 2016) is the main contemporary representative of this approach. RNNG is a top-down parser which in its first version used a non-incremental discriminative locallynormalised model. To make the parser fully incremental Dyer et al. (2016) exchanged the discriminative model for a generative one. This was not enough to get a working single-model incremental parser. Stern et al. (2017) added a couple more modifications to the search, namely wordsynchronous beams with a very large number of hypotheses, that gave good results. Could we just apply these same techniques to the CCG p"
2020.acl-main.378,P18-2075,0,0.0127401,"nd all metrics (Cross and Huang, 2016). To this date there have been no proposals for a dynamic oracle for CCG parsing with F1 metric over CCG dependency structures and it is not even clear if there is a polynomial solution to this problem.3 Therefore this is not an option that we can use. An alternative is to use a reinforcement learning algorithm REINFORCE (Williams, 1992). REINFORCE samples derivations for training just like dynamic-oracle, but does not require design of a task-specific oracle extraction algorithm. Instead, it implicitly minimises the expected error of the desired metric.4 Fried and Klein (2018) have shown that in some circumstances REINFORCE can give results almost as good as dynamic oracle, but it requires using additional techniques to compensate for high variance of the training method. The method of applying REINFORCE to the discriminative parser is straightforward because sampling trees from the discriminative parser is easy. However, that is not the case for the generative model from which we have to sample both trees and sentences at the same time. That is why we will apply REINFORCE only to the discriminative model. Imbalanced probability causes a search bias so the way it w"
2020.acl-main.378,P17-2025,0,0.0168149,"al models on all metrics. 6 Other relevant work The incremental CCG parser of Ambati (2016) uses the linear model trained with a structured perceptron objective and the early update heuristic. Given the simplicity of that model, it performs surprisingly well. The reason is the fact that the structured perceptron addresses all the biases identified in our paper. Our work has been an attempt to Non-Incremental 5.4 Incremental mental models. With MBR they all manage to outperform the non-incremental model. However, we should not credit this right away to the quality of the incremental models. As Fried et al. (2017) point out, improvements in reranking with a different model could be a result of model ensembling. Hassan et al. (2008) beam= 1 Ambati (2016) beam= 1 this work beam= 1 Goyal et al. (2019) beam= 5 this work beam= 5 Ambati (2016) beam=16 this work beam=16 this work beam=64 Lewis and Steedman (2014) Ambati et al. (2015) Hockenmaier (2003) Zhang and Clark (2011) Xu et al. (2016) Clark and Curran (2007) Stanojevi´c and Steedman (2019) this work MBR reranking Tag — 74.6 78.8 85.5 90.1 90.8 91.4 92.0 93.0 91.2 92.2 93.1 93.9 94.3 95.4 95.6 UF 59.0 67.5 69.9 — 92.2 88.3 91.5 92.3 88.6 89.0 92.0 — — 9"
2020.acl-main.378,C12-1059,0,0.0368444,") advocated using conditional random fields (CRF), which are globally normalised probabilistic models, but margin-based alternatives like Max-Margin Markov Networks (M3 N) (Taskar et al., 2004) and Structured SVM (Tsochantaridis et al., 2004) could be used in the same way. These particular solutions are not applicable here because they require (implicitly) enu4114 merating all possible derivations which is not possible with a model like ours that makes only few independence assumptions. Exposure bias happens because model is not exposed to its errors during training time. With dynamic oracle (Goldberg and Nivre, 2012) parser is trained on its own prediced history instead of the gold sequence of actions (static oracle). Whenever the model is in some sampled state (which is not necessarily a good state), we train the model to pick the transition that is a beginning of a path that would lead the parser to the ending state with the highest achievable metric score from that state. Finding such a transition is not trivial for all systems and all metrics (Cross and Huang, 2016). To this date there have been no proposals for a dynamic oracle for CCG parsing with F1 metric over CCG dependency structures and it is n"
2020.acl-main.378,P96-1024,0,0.662977,"Missing"
2020.acl-main.378,N19-1171,0,0.0740644,"sing combinatory rules such as function composition. In this way we can form (mostly) left-branching derivation trees that can be parsed incrementally even with simple transition mechanisms such as shift-reduce parsers. Still, left branching structures are not sufficient to solve all the problems of incremental sentence processing. Right adjuncts are particularly problematic. They appear on the right of the head that they modify which means that they need to be predicted, but at the same time they are optional which makes it impossible to predict them with confidence. Stanojevi´c and Steedman (2019) tackle this issue by using incremental tree-rotation and revealing operations that allow adjuncts not to be predicted, but still be easy to attach to the head in case they appear. They show great improvement in the incrementality of this approach as measured by connectedness (the average stack size). However, Stanojevi´c and Steedman (2019) parser is not fully incremental because its oracle (the function that decides which transition to take in case of non-determinism)1 is a probabilistic model that looks at the whole sentence. It does so using bi-directional ELMo embeddings with the addition"
2020.acl-main.378,J07-3004,1,0.687636,"the stack with Stack-LSTM (Dyer et al., 2015). This provides the encoding of the whole configuration together with the buffer, because the buffer is implicitly encoded via ELMo and Bi-LSTM, which look at the whole sentence. Given the hidden vector representation of the configuration, the parser uses a feed-forward network to determine the probability of the next action. 4112 There are three main types of transitions: • Parsing actions: shift and reduce(X) where X is a unary or binary combinatory rule; • Supertagging actions: tag(X) where X is one of the lexical supertags from English CCGbank (Hockenmaier and Steedman, 2007); • Right-adjunction actions: adjoin(X) where X is one of the nodes to which the adjunct can be adjoined. We refer the reader to (Stanojevi´c and Steedman, 2019) for more detail on the original neural model and transition system, which are not of particular relevance here. What matters is only that (1) the number of tagging actions is much bigger than the number of possible parsing actions and (2) that the buffer is implicitly encoded with ELMo and Bi-LSTM. To make the parsing model fully incremental first we modify ELMo embeddings: instead of using full ELMo embeddings we use only the forward"
2020.acl-main.378,N04-1022,0,0.40051,"Missing"
2020.acl-main.378,D14-1107,1,0.834284,"rly-Single BSO-Early-All CRF-LaSO 70 60 50 1 4 8 16 Beam size 32 64 Figure 2: Influence of beam size on the dev results. Results: Test set performance Table 1 compares our strongest method on the test set against all the previously published incremental CCG models. The results show that it outperforms all the previous incremental models when using beams of the same size. The improvement is even bigger with the larger beam. Even thought our primary goal is not to compete with nonincremental parsers, our incremental model outperforms some widely used non-incremental CCG parsers such as EasyCCG (Lewis and Steedman, 2014). The result is particularly good for unlabelled dependencies. We also report the results of applying MBR reranking using incremental model over the samples generated by the non-incremental model. This model outperforms other incremental and non-incremental models on all metrics. 6 Other relevant work The incremental CCG parser of Ambati (2016) uses the linear model trained with a structured perceptron objective and the early update heuristic. Given the simplicity of that model, it performs surprisingly well. The reason is the fact that the structured perceptron addresses all the biases identi"
2020.acl-main.378,D19-1233,0,0.0270344,"Missing"
2020.acl-main.378,D15-1005,1,0.888359,"Missing"
2020.acl-main.378,N19-1020,1,0.901421,"Missing"
2020.acl-main.378,D17-1178,0,0.530224,"racle is a locally normalised generative model ether in the form of simple PCFG (Stolcke, 1995; Hale, 2001), feature based (Roark and Johnson, 1999; Roark, 2001) or neural model (Dyer et al., 2016; Hale et al., 2018). RNNG (Dyer et al., 2016) is the main contemporary representative of this approach. RNNG is a top-down parser which in its first version used a non-incremental discriminative locallynormalised model. To make the parser fully incremental Dyer et al. (2016) exchanged the discriminative model for a generative one. This was not enough to get a working single-model incremental parser. Stern et al. (2017) added a couple more modifications to the search, namely wordsynchronous beams with a very large number of hypotheses, that gave good results. Could we just apply these same techniques to the CCG parser of Stanojevi´c and Steedman (2019) and replace non-incremental probabilistic model with an incremental one? The short answer is no. As it will be shown later, a straightforward adaptation of the beam search and switching to a generative model does indeed improve accuracy over the model that does not do that, but not enough to make the incremental parser competitive. We provide an explanation fo"
2020.acl-main.378,N18-1202,0,0.0189327,"ves results that outperform all previous incremental parsing models even with a relatively small beam. This is not to say that all unwanted biases are removed—for instance, beam search is still a biased search. However, the biases that remain do not have the drastic effect on performance of the three identified above. 2 Baseline model The parser of Stanojevi´c and Steedman (2019) already offers a fully incremental transition system with a non-incremental probabilistic model that gives state of the art accuracy in recovering predicate-argument dependencies. The parser encodes words using ELMo (Peters et al., 2018) and BiLSTM (Graves et al., 2005), sub-trees with tree encoders and the stack with Stack-LSTM (Dyer et al., 2015). This provides the encoding of the whole configuration together with the buffer, because the buffer is implicitly encoded via ELMo and Bi-LSTM, which look at the whole sentence. Given the hidden vector representation of the configuration, the parser uses a feed-forward network to determine the probability of the next action. 4112 There are three main types of transitions: • Parsing actions: shift and reduce(X) where X is a unary or binary combinatory rule; • Supertagging actions: t"
2020.acl-main.378,J95-2002,0,0.572281,"chell, 1988). Altmann and Steedman (1988) have shown that these heuristics are overruled by human parser if the context gives evidence for a particular interpretation, in itself further evidence for processing incrementality at all levels. It seems natural to model the non-deterministic decision by using a probabilistic model which will condition on words and possibly on the context. Oracles of the modern broad coverage incremental parsers are without exception statistical in nature. The most typical statistical oracle is a locally normalised generative model ether in the form of simple PCFG (Stolcke, 1995; Hale, 2001), feature based (Roark and Johnson, 1999; Roark, 2001) or neural model (Dyer et al., 2016; Hale et al., 2018). RNNG (Dyer et al., 2016) is the main contemporary representative of this approach. RNNG is a top-down parser which in its first version used a non-incremental discriminative locallynormalised model. To make the parser fully incremental Dyer et al. (2016) exchanged the discriminative model for a generative one. This was not enough to get a working single-model incremental parser. Stern et al. (2017) added a couple more modifications to the search, namely wordsynchronous be"
2020.acl-main.378,C92-1032,0,0.330658,"er that we use is a CCG bottom-up parser. This choice is intentional—even though there is clear evidence that human sentence processing is highly incremental, we argue below that there is no unequivocal evidence that it is more incremental than would be allowed under the Strict Competence Hypothesis (SCH) which states that the parser cannot construct any structure that is not licensed by the competence grammar, given CCG’s generalized notion of constituency (Steedman, 1989). Most research in incremental parsing has been directed at finding the right parsing algorithm (Abney and Johnson, 1991; Resnik, 1992; Hale, 2014; Stanojevi´c and Stabler, 2018) or grammar formalism (Steedman, 1989; Stabler, 1991; Sturt and Lombardo, 2005; Demberg et al., 2013; Stanojevi´c et al., 2020), but not much has been done in addressing the issue of finding the right oracle. Early approaches to this problem were lateclosure and minimal-attachment heuristics (Frazier, 1979; Pereira, 1985) which do not appear to be language universal (Cuetos and Mitchell, 1988). Altmann and Steedman (1988) have shown that these heuristics are overruled by human parser if the context gives evidence for a particular interpretation, in i"
2020.acl-main.378,J01-2004,0,0.152162,"istics are overruled by human parser if the context gives evidence for a particular interpretation, in itself further evidence for processing incrementality at all levels. It seems natural to model the non-deterministic decision by using a probabilistic model which will condition on words and possibly on the context. Oracles of the modern broad coverage incremental parsers are without exception statistical in nature. The most typical statistical oracle is a locally normalised generative model ether in the form of simple PCFG (Stolcke, 1995; Hale, 2001), feature based (Roark and Johnson, 1999; Roark, 2001) or neural model (Dyer et al., 2016; Hale et al., 2018). RNNG (Dyer et al., 2016) is the main contemporary representative of this approach. RNNG is a top-down parser which in its first version used a non-incremental discriminative locallynormalised model. To make the parser fully incremental Dyer et al. (2016) exchanged the discriminative model for a generative one. This was not enough to get a working single-model incremental parser. Stern et al. (2017) added a couple more modifications to the search, namely wordsynchronous beams with a very large number of hypotheses, that gave good results."
2020.acl-main.378,P99-1054,0,0.358033,"ave shown that these heuristics are overruled by human parser if the context gives evidence for a particular interpretation, in itself further evidence for processing incrementality at all levels. It seems natural to model the non-deterministic decision by using a probabilistic model which will condition on words and possibly on the context. Oracles of the modern broad coverage incremental parsers are without exception statistical in nature. The most typical statistical oracle is a locally normalised generative model ether in the form of simple PCFG (Stolcke, 1995; Hale, 2001), feature based (Roark and Johnson, 1999; Roark, 2001) or neural model (Dyer et al., 2016; Hale et al., 2018). RNNG (Dyer et al., 2016) is the main contemporary representative of this approach. RNNG is a top-down parser which in its first version used a non-incremental discriminative locallynormalised model. To make the parser fully incremental Dyer et al. (2016) exchanged the discriminative model for a generative one. This was not enough to get a working single-model incremental parser. Stern et al. (2017) added a couple more modifications to the search, namely wordsynchronous beams with a very large number of hypotheses, that gave"
2020.acl-main.378,W06-1666,0,0.100973,"Missing"
2020.acl-main.378,W18-2809,1,0.895241,"Missing"
2020.acl-main.378,Q16-1014,0,0.0193763,". (2016) Clark and Curran (2007) Stanojevi´c and Steedman (2019) this work MBR reranking Tag — 74.6 78.8 85.5 90.1 90.8 91.4 92.0 93.0 91.2 92.2 93.1 93.9 94.3 95.4 95.6 UF 59.0 67.5 69.9 — 92.2 88.3 91.5 92.3 88.6 89.0 92.0 — — 93.0 95.8 95.9 LF — 57.5 55.8 — 82.1 80.8 82.3 83.4 81.3 81.4 84.4 85.5 86.4 87.6 90.2 90.6 Table 1: Results on the test set. The results of NonIncremental parsers are shown only as a reference. bring these benefits to more modern neural models. Another interesting approach to tackle labelbias while keeping the probabilistic interpretation is the error-states model of Vaswani and Sagae (2016). This model in its original formulation would not be computationally efficient in our setting because there are too many instances of errorstates to be trained on in CCG parsing caused by large number of transitions. Possibly some modification based on sampling could remedy this. There has also been some recent work on reducing the imbalanced probability bias. Mabona et al. (2019) propose an algorithmic solution for organising beam search into buckets that have the same number of expensive transitions. Crabb´e et al. (2019) propose a sampling based approach with 4118 the same motivation of co"
2020.acl-main.378,D16-1137,0,0.175383,"Missing"
2020.acl-main.378,N16-1025,0,0.0141604,"it was addressed by Stern et al. (2017) is to modify the search itself. Stern et al. (2017) introduced a word synchronous beam search (WordSync) in which all the hypotheses that are competing with each other are guaranteed to have the same number of expensive actions. 3 In parsers that do not use a normal-form there is an additional source of exposure bias arrising from alternative gold derivations. For this source of exposure bias there is a dynamic oracle by Xu et al. (2014) that works on a subset of CCG dependencies. 4 A related, but different, objective is minimum expected error training (Xu et al., 2016). Most of these methods are either not applicable (exact CRF, exact M3 N, dynamic oracle), or they solve only some subset of the previously mentioned biases. However, we can resort to some approximate methods to global models. For instance, instead of enumerating all hypotheses to compute normalization we could use a beam search as an approximation. This was done for CRF objective in (Zhou et al., 2015; Andor et al., 2016) and for (single-violation) M3 N objective in (Wiseman and Rush, 2016). They all need to compare in some way the gold hypothesis to the rest of the beam, but the issue arises"
2020.acl-main.378,P14-1021,0,0.0195635,"ame time. That is why we will apply REINFORCE only to the discriminative model. Imbalanced probability causes a search bias so the way it was addressed by Stern et al. (2017) is to modify the search itself. Stern et al. (2017) introduced a word synchronous beam search (WordSync) in which all the hypotheses that are competing with each other are guaranteed to have the same number of expensive actions. 3 In parsers that do not use a normal-form there is an additional source of exposure bias arrising from alternative gold derivations. For this source of exposure bias there is a dynamic oracle by Xu et al. (2014) that works on a subset of CCG dependencies. 4 A related, but different, objective is minimum expected error training (Xu et al., 2016). Most of these methods are either not applicable (exact CRF, exact M3 N, dynamic oracle), or they solve only some subset of the previously mentioned biases. However, we can resort to some approximate methods to global models. For instance, instead of enumerating all hypotheses to compute normalization we could use a beam search as an approximation. This was done for CRF objective in (Zhou et al., 2015; Andor et al., 2016) and for (single-violation) M3 N object"
2020.acl-main.378,P11-1069,0,0.034624,"Missing"
2020.acl-main.378,P15-1117,0,0.0118074,"or this source of exposure bias there is a dynamic oracle by Xu et al. (2014) that works on a subset of CCG dependencies. 4 A related, but different, objective is minimum expected error training (Xu et al., 2016). Most of these methods are either not applicable (exact CRF, exact M3 N, dynamic oracle), or they solve only some subset of the previously mentioned biases. However, we can resort to some approximate methods to global models. For instance, instead of enumerating all hypotheses to compute normalization we could use a beam search as an approximation. This was done for CRF objective in (Zhou et al., 2015; Andor et al., 2016) and for (single-violation) M3 N objective in (Wiseman and Rush, 2016). They all need to compare in some way the gold hypothesis to the rest of the beam, but the issue arises when the gold hypothesis falls out of the beam. For that situation they use different heuristics. CRF approximation of Zhou et al. (2015) and Andor et al. (2016) uses Early update of Collins and Roark (2004). During training with Early update, the beam search is stopped when the gold hypothesis falls out of the beam and the parameter update is performed. In the BeamSearch Optimization (BSO) method of"
2020.coling-main.401,W19-1806,1,0.903884,"y, we contribute a dataset of human–human conversations annotated with lexical aspect and present experiments that show the correlation of telicity with genre and discourse goals. 1 Introduction One of the fascinating aspects of studying aspectual class of verbs in English is its relation with nonverbal categories. Thus, although in origin a property of the verb, the aspectual class interacts in a tight-knit fashion with other words in a sentence. Previous research has discussed the importance of predicting the aspectual classes of verbs for predicting coherence relations in text and imagery (Alikhani and Stone, 2019), predicting links in entailment graphs (Hosseini et al., 2019) and interpreting sign languages (Wilbur, 2003). In addition, knowledge about the aspectual class of a verb phrase, and its influence on the temporal extent and entailments that it licenses, has been leveraged in the past for a number of natural language understanding tasks such as temporal relation extraction (Costa and Branco, 2012), event ordering (Chambers et al., 2014; Modi and Titov, 2014), and statistical machine translation (Lo´aiciga and Grisot, 2016). The Aktionsart (Vendler, 1957) of a verb determines the temporal extent"
2020.coling-main.401,P10-1124,0,0.0258215,"not observe any meaningful performance difference when replacing our bagof-embeddings approach with ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). While this work was done on English, we aim to use our methodology in a multilingual setup in future work as distributional approaches scale well with growing amounts of data and across languages. Aspect, alongside tense, is a crucial indicator of the temporal extent of a verb as well as the entailments it licenses. In future work we plan to integrate aspectual information for improving the unsupervised construction of entailment graphs (Berant et al., 2010; Hosseini et al., 2018), as well as temporal reasoning, which has been shown recently to be difficult for distributional semantic models (Kober et al., 2019). Aspectual information can be utilised for directional entailment detection by inferring that the event of buying something entails the state of owning that thing, but not the other way round. Determining the telicity of an event also enables fine-grained inferences about whether an event caused a change of state. For example, while the telic context of writing a sonnet in fifteen minutes entails a change to a state where a finished sonn"
2020.coling-main.401,D15-1075,0,0.0165189,"d verbs, such as get, see or know, which have a 3 The annotation protocol is published with the dataset at https://go.rutgers.edu/cb6le5c1. 4548 Figure 1: Frequency distribution of the 10 most frequent verbs (left) and associated label distribution for the 10 most frequent verbs (right). clear majority class, whereas do or look exhibit a much more balanced, and therefore ambiguous label distribution. 4 Experiments The utility of distributional semantic word representations has been shown in a large body of works in recent years (Weeds et al. (2014), Nguyen et al. (2017), Socher et al. (2013), Bowman et al. (2015); passim). In order to compose a verb with its context we apply pointwise addition as a simple distributional composition function. Pointwise addition in neural word embeddings approximates the intersection of their contexts4 (Tian et al., 2017), and has been shown to be an efficient function for contextualising a word in a phrase (Arora et al., 2016; Kober et al., 2017). 4.1 Distributional Models for Predicational Aspect Following previous work on modelling the aspectual class of a verb (Siegel and McKeown, 2000; Friedrich and Palmer, 2014), we treat the problem as a supervised classification"
2020.coling-main.401,Q14-1022,0,0.0719297,"sentence. Previous research has discussed the importance of predicting the aspectual classes of verbs for predicting coherence relations in text and imagery (Alikhani and Stone, 2019), predicting links in entailment graphs (Hosseini et al., 2019) and interpreting sign languages (Wilbur, 2003). In addition, knowledge about the aspectual class of a verb phrase, and its influence on the temporal extent and entailments that it licenses, has been leveraged in the past for a number of natural language understanding tasks such as temporal relation extraction (Costa and Branco, 2012), event ordering (Chambers et al., 2014; Modi and Titov, 2014), and statistical machine translation (Lo´aiciga and Grisot, 2016). The Aktionsart (Vendler, 1957) of a verb determines the temporal extent of the predication as well as whether it causes a change of state for the entities involved (Filip, 2012). As Aktionsart typically refers to the lexical aspect of a verb in isolation, we adopt the terminology of Verkuyl (2005), and refer to the compositionally formed Aktionsart of a verb phrase as predicational aspect. One of the most important distinctions of the predicational aspect of a verb is between states, such as to know or t"
2020.coling-main.401,E12-1027,0,0.624017,"tight-knit fashion with other words in a sentence. Previous research has discussed the importance of predicting the aspectual classes of verbs for predicting coherence relations in text and imagery (Alikhani and Stone, 2019), predicting links in entailment graphs (Hosseini et al., 2019) and interpreting sign languages (Wilbur, 2003). In addition, knowledge about the aspectual class of a verb phrase, and its influence on the temporal extent and entailments that it licenses, has been leveraged in the past for a number of natural language understanding tasks such as temporal relation extraction (Costa and Branco, 2012), event ordering (Chambers et al., 2014; Modi and Titov, 2014), and statistical machine translation (Lo´aiciga and Grisot, 2016). The Aktionsart (Vendler, 1957) of a verb determines the temporal extent of the predication as well as whether it causes a change of state for the entities involved (Filip, 2012). As Aktionsart typically refers to the lexical aspect of a verb in isolation, we adopt the terminology of Verkuyl (2005), and refer to the compositionally formed Aktionsart of a verb phrase as predicational aspect. One of the most important distinctions of the predicational aspect of a verb"
2020.coling-main.401,N19-1423,0,0.0139807,"in a verb phrase, tend to be very reliable indicators of the verb’s aspectual class (Vendler (1957), Dowty (1979), Moens and Steedman (1988), passim). Our model setup was intentionally kept simple as we were primarily concerned with the question whether predicational aspect can be captured with a distributional semantics approach in principle. We note that using more sophisticated models might yield even stronger results, although in preliminary tests, we did not observe any meaningful performance difference when replacing our bagof-embeddings approach with ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). While this work was done on English, we aim to use our methodology in a multilingual setup in future work as distributional approaches scale well with growing amounts of data and across languages. Aspect, alongside tense, is a crucial indicator of the temporal extent of a verb as well as the entailments it licenses. In future work we plan to integrate aspectual information for improving the unsupervised construction of entailment graphs (Berant et al., 2010; Hosseini et al., 2018), as well as temporal reasoning, which has been shown recently to be difficult for distributional semantic models"
2020.coling-main.401,P97-1020,0,0.514241,"fferent supervised machine learning algorithms to classify the extracted feature vectors into either states or events, or telic or atelic events. Siegel and McKeown (2000) show that their method substantially improves over a majority-class baseline. The first approach to include features derived from a distributional semantic model has been proposed by Friedrich and Palmer (2014). In addition to the linguistic indicator features of Siegel and McKeown (2000), Friedrich and Palmer (2014) extract representative stative, dynamic or mixed verbs from the lexical conceptual structure (LCS) database (Dorr and Olsen, 1997) and subsequently use distributional representations to derive similarity scores for the mined verbs. Another extension to the work of Friedrich and Palmer (2014) has been proposed by Heuschkel (2016), who refines the distributional similarity features by first contextualising a target verb with its subject or object, and only then computing the distributional similarities to the set of representative verbs from the LCS database as in Friedrich and Palmer (2014). All else being equal, Heuschkel (2016) shows that contextualising the distributional representations improves performance on the Asp"
2020.coling-main.401,S16-2002,0,0.350641,"rom the LCS database as in Friedrich and Palmer (2014). All else being equal, Heuschkel (2016) shows that contextualising the distributional representations improves performance on the Asp-ambig dataset of Friedrich and Palmer (2014). In contrast to this line of research we do not make explicit use of any hand-engineered linguistic indicator features but show that these can be picked up in an unsupervised way by composing distributional semantic word representations. The linguistic indicators are furthermore frequently collected on the verb type level instead of on the token level. Similar to Falk and Martin (2016), we are concerned with classifying verb readings; however, we do not use engineered features as Falk and Martin (2016) do, but directly leverage local contextual information in the form of distributional representations. Our approach is also not reliant on the availability of a parallel corpus as in Friedrich and Gateva (2017). The major difference between our approach of using distributional word representations and previous approaches is that we are using the word representations directly for classification, rather than indirectly by computing similarity scores and using these as features."
2020.coling-main.401,D17-1271,0,0.418239,"inguistic indicator features but show that these can be picked up in an unsupervised way by composing distributional semantic word representations. The linguistic indicators are furthermore frequently collected on the verb type level instead of on the token level. Similar to Falk and Martin (2016), we are concerned with classifying verb readings; however, we do not use engineered features as Falk and Martin (2016) do, but directly leverage local contextual information in the form of distributional representations. Our approach is also not reliant on the availability of a parallel corpus as in Friedrich and Gateva (2017). The major difference between our approach of using distributional word representations and previous approaches is that we are using the word representations directly for classification, rather than indirectly by computing similarity scores and using these as features. This furthermore liberates us from the requirement of having a representative seed set of verbs per class to compute the distributional similarities from. 3 Dataset We introduce a new dataset, DIASPORA,2 of human-human conversations annotated with predicational aspect, representing the first dialogue dataset annotated with aspe"
2020.coling-main.401,P14-2085,0,0.759901,"avans and Chodorov (1992), and collected linguistic indicators for lexical aspect from a large corpus. These include the presence of in- or for-adverbials, the tense of the verb or its frequency. Siegel and McKeown (2000) subsequently applied different supervised machine learning algorithms to classify the extracted feature vectors into either states or events, or telic or atelic events. Siegel and McKeown (2000) show that their method substantially improves over a majority-class baseline. The first approach to include features derived from a distributional semantic model has been proposed by Friedrich and Palmer (2014). In addition to the linguistic indicator features of Siegel and McKeown (2000), Friedrich and Palmer (2014) extract representative stative, dynamic or mixed verbs from the lexical conceptual structure (LCS) database (Dorr and Olsen, 1997) and subsequently use distributional representations to derive similarity scores for the mined verbs. Another extension to the work of Friedrich and Palmer (2014) has been proposed by Heuschkel (2016), who refines the distributional similarity features by first contextualising a target verb with its subject or object, and only then computing the distributiona"
2020.coling-main.401,P16-1166,0,0.711369,"assess the suitability of distributional representations for distinguishing states from events (§ 5.1), and telic from atelic events (§ 5.2). Only a completed and telic event licenses a new consequent state. Therefore, modelling predicational aspect is important for deeper text understanding, for example for modelling cause and effect, and especially for inferring consequent states. 5.1 Experiment 1 — States vs. Events For the distinction between states and events we perform experiments on 5 datasets in total. We use the Asp-ambig dataset by Friedrich and Palmer (2014), the SitEnt dataset by Friedrich et al. (2016), our own sub-sampled version of the SitEnt dataset, the Captions dataset by Alikhani and Stone (2019), and our own DIASPORA dataset, proposed in this work. The Asp-ambig dataset is sampled from the Brown corpus (Francis and Kucera, 1979) and is based on 20 frequently occurring verbs whose predicational aspect changes depending on context. For each verb, Friedrich and Palmer (2014) collected 138 sentences, resulting in 2760 examples in total. The dataset 7 If the target verb is the root, then no context is used for that verb. 4550 contains the annotations of whether the verb in context express"
2020.coling-main.401,P14-5010,0,0.00337872,"rough a logistic regression classifier in order to predict the aspectual class of v. This model aims to capture the compositional nature of predicational aspect by integrating local contextual information into the model. Types of Context We investigate two different kinds of context: simple linear context windows of varying length and firstorder dependency contexts. For example for the sentence in Figure 2, a linear context window of size 1 would extract Jane and to for the target verb decided, whereas a dependency-based context would extract Jane and leave. We used the Stanford NLP pipeline (Manning et al., 2014) with default settings for parsing the sentences in our datasets. root xcomp nsubj Jane aux decided to advmod leave early Figure 2: With a linear context window of size 1, Jane and to would be extracted as contexts for the verb decided. With a dependency-based context, Jane and leave would be extracted. For linear context windows we use sizes {1, 2, 3, 5, 10}, and for first-order dependency-based contexts we experiment with using only the head7 of the verb, only its children, or the full first-order context. Incorporating the Full Sentence We furthermore test a model that incorporates the whol"
2020.coling-main.401,W14-1606,0,0.0298338,"arch has discussed the importance of predicting the aspectual classes of verbs for predicting coherence relations in text and imagery (Alikhani and Stone, 2019), predicting links in entailment graphs (Hosseini et al., 2019) and interpreting sign languages (Wilbur, 2003). In addition, knowledge about the aspectual class of a verb phrase, and its influence on the temporal extent and entailments that it licenses, has been leveraged in the past for a number of natural language understanding tasks such as temporal relation extraction (Costa and Branco, 2012), event ordering (Chambers et al., 2014; Modi and Titov, 2014), and statistical machine translation (Lo´aiciga and Grisot, 2016). The Aktionsart (Vendler, 1957) of a verb determines the temporal extent of the predication as well as whether it causes a change of state for the entities involved (Filip, 2012). As Aktionsart typically refers to the lexical aspect of a verb in isolation, we adopt the terminology of Verkuyl (2005), and refer to the compositionally formed Aktionsart of a verb phrase as predicational aspect. One of the most important distinctions of the predicational aspect of a verb is between states, such as to know or to love, and events, suc"
2020.coling-main.401,J88-2003,1,0.597345,"r representation. The approach simply uses all words from a given sentence and composes their corresponding word2vec representations as in Equation 1 above to create an embedding for the whole sentence. Embedding a sentence by adding word vectors has been shown to be an effective method for other NLP tasks such as sentiment analysis (Iyyer et al., 2015) and recognising textual entailment (Wieting et al., 2016). The underlying rationale behind this approach is that the aspectual class of a verb is a function of the sentence as a whole, rather than dependent on local context alone (Moens, 1987; Moens and Steedman, 1988; Dowty, 1991). 5 Experiments We perform experiments that assess the suitability of distributional representations for distinguishing states from events (§ 5.1), and telic from atelic events (§ 5.2). Only a completed and telic event licenses a new consequent state. Therefore, modelling predicational aspect is important for deeper text understanding, for example for modelling cause and effect, and especially for inferring consequent states. 5.1 Experiment 1 — States vs. Events For the distinction between states and events we perform experiments on 5 datasets in total. We use the Asp-ambig datas"
2020.coling-main.401,D17-1022,0,0.0481987,"Missing"
2020.coling-main.401,J88-2005,0,0.606945,"ing distinctions of a verb that relate to its aspectual class should be reflected in its distribution when composed with its context. We therefore intersect word vectors with their context in order to determine a VP’s predicational aspect, and show that we achieve a new state-of-the-art on two datasets. We further evaluate our approach on two new genres: image captions and situated human–human conversations, thereby extending the validity of our findings across a variety of genres. 2 Related Work An early approach to classifying the lexical aspectual class of a verb in context was proposed by Passonneau (1988), who applied a decompositional analysis of the verb to determine the aspectual class for verb occurrences in a restricted domain. The first general-purpose study was conducted by Siegel and McKeown (2000), who built up on earlier work by Klavans and Chodorov (1992), and collected linguistic indicators for lexical aspect from a large corpus. These include the presence of in- or for-adverbials, the tense of the verb or its frequency. Siegel and McKeown (2000) subsequently applied different supervised machine learning algorithms to classify the extracted feature vectors into either states or eve"
2020.coling-main.401,N18-1202,0,0.0337896,"of prepositions or particles in a verb phrase, tend to be very reliable indicators of the verb’s aspectual class (Vendler (1957), Dowty (1979), Moens and Steedman (1988), passim). Our model setup was intentionally kept simple as we were primarily concerned with the question whether predicational aspect can be captured with a distributional semantics approach in principle. We note that using more sophisticated models might yield even stronger results, although in preliminary tests, we did not observe any meaningful performance difference when replacing our bagof-embeddings approach with ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019). While this work was done on English, we aim to use our methodology in a multilingual setup in future work as distributional approaches scale well with growing amounts of data and across languages. Aspect, alongside tense, is a crucial indicator of the temporal extent of a verb as well as the entailments it licenses. In future work we plan to integrate aspectual information for improving the unsupervised construction of entailment graphs (Berant et al., 2010; Hosseini et al., 2018), as well as temporal reasoning, which has been shown recently to be difficult for"
2020.coling-main.401,J00-4004,0,0.924192,"to determine a VP’s predicational aspect, and show that we achieve a new state-of-the-art on two datasets. We further evaluate our approach on two new genres: image captions and situated human–human conversations, thereby extending the validity of our findings across a variety of genres. 2 Related Work An early approach to classifying the lexical aspectual class of a verb in context was proposed by Passonneau (1988), who applied a decompositional analysis of the verb to determine the aspectual class for verb occurrences in a restricted domain. The first general-purpose study was conducted by Siegel and McKeown (2000), who built up on earlier work by Klavans and Chodorov (1992), and collected linguistic indicators for lexical aspect from a large corpus. These include the presence of in- or for-adverbials, the tense of the verb or its frequency. Siegel and McKeown (2000) subsequently applied different supervised machine learning algorithms to classify the extracted feature vectors into either states or events, or telic or atelic events. Siegel and McKeown (2000) show that their method substantially improves over a majority-class baseline. The first approach to include features derived from a distributional"
2020.coling-main.401,D13-1170,0,0.00459162,"are some highly skewed verbs, such as get, see or know, which have a 3 The annotation protocol is published with the dataset at https://go.rutgers.edu/cb6le5c1. 4548 Figure 1: Frequency distribution of the 10 most frequent verbs (left) and associated label distribution for the 10 most frequent verbs (right). clear majority class, whereas do or look exhibit a much more balanced, and therefore ambiguous label distribution. 4 Experiments The utility of distributional semantic word representations has been shown in a large body of works in recent years (Weeds et al. (2014), Nguyen et al. (2017), Socher et al. (2013), Bowman et al. (2015); passim). In order to compose a verb with its context we apply pointwise addition as a simple distributional composition function. Pointwise addition in neural word embeddings approximates the intersection of their contexts4 (Tian et al., 2017), and has been shown to be an efficient function for contextualising a word in a phrase (Arora et al., 2016; Kober et al., 2017). 4.1 Distributional Models for Predicational Aspect Following previous work on modelling the aspectual class of a verb (Siegel and McKeown, 2000; Friedrich and Palmer, 2014), we treat the problem as a sup"
2020.coling-main.401,J86-2003,0,0.24002,"elations in text and imagery (Alikhani and Stone, 2019), predicting links in entailment graphs (Hosseini et al., 2019) and interpreting sign languages (Wilbur, 2003). In addition, knowledge about the aspectual class of a verb phrase, and its influence on the temporal extent and entailments that it licenses, has been leveraged in the past for a number of natural language understanding tasks such as temporal relation extraction (Costa and Branco, 2012), event ordering (Chambers et al., 2014; Modi and Titov, 2014), and statistical machine translation (Lo´aiciga and Grisot, 2016). The Aktionsart (Vendler, 1957) of a verb determines the temporal extent of the predication as well as whether it causes a change of state for the entities involved (Filip, 2012). As Aktionsart typically refers to the lexical aspect of a verb in isolation, we adopt the terminology of Verkuyl (2005), and refer to the compositionally formed Aktionsart of a verb phrase as predicational aspect. One of the most important distinctions of the predicational aspect of a verb is between states, such as to know or to love, and events, such as visit or swim. This distinction is important for identifying the entailments that a given ver"
2020.coling-main.401,C14-1212,0,0.0243232,"the 10 most frequent verbs shows that there are some highly skewed verbs, such as get, see or know, which have a 3 The annotation protocol is published with the dataset at https://go.rutgers.edu/cb6le5c1. 4548 Figure 1: Frequency distribution of the 10 most frequent verbs (left) and associated label distribution for the 10 most frequent verbs (right). clear majority class, whereas do or look exhibit a much more balanced, and therefore ambiguous label distribution. 4 Experiments The utility of distributional semantic word representations has been shown in a large body of works in recent years (Weeds et al. (2014), Nguyen et al. (2017), Socher et al. (2013), Bowman et al. (2015); passim). In order to compose a verb with its context we apply pointwise addition as a simple distributional composition function. Pointwise addition in neural word embeddings approximates the intersection of their contexts4 (Tian et al., 2017), and has been shown to be an efficient function for contextualising a word in a phrase (Arora et al., 2016; Kober et al., 2017). 4.1 Distributional Models for Predicational Aspect Following previous work on modelling the aspectual class of a verb (Siegel and McKeown, 2000; Friedrich and"
2020.emnlp-main.642,N18-1007,0,0.0417865,"Missing"
2020.emnlp-main.642,N09-2021,0,0.0327803,"achine learning models (Wightman and Ostendorf, 1994; Levow, 2005; Gregory and Altun, 2004), and neural models (Fernandez et al., 2017; Stehwien and Vu, 2017; Stehwien et al., 2018). Stehwien and Vu (2017) and Stehwien et al. (2018) (henceforth, SVS18) showed that neural methods can perform comparably to traditional methods using a relatively small amount of speech context— just a single word on either side of the target word. However, since pitch accents are deviations from a speaker’s average pitch, intensity, and duration, we hypothesize that, as in some non-neural models (e.g. Levow 2005; Rosenberg and Hirschberg 2009), a wider input context will allow the model to better determine the speaker’s baseline for these features and therefore improve its ability to detect deviations. In addition, we hypothesize that a recurrent model (rather than the CNN used by SVS18) will also improve performance, since it is better adapted to processing long-distance dependencies. In this paper, we test these hypotheses by building a new neural pitch accent prediction model that takes in prosodic speech features, text features, or both. Our main contribution is showing that these context-enhancing innovations in the speechonly"
2020.emnlp-main.642,P04-1086,0,\N,Missing
2020.emnlp-main.642,D14-1162,0,\N,Missing
2020.findings-emnlp.199,W13-2322,0,0.325217,"speak receive-01 :ARG0 I :ARG1 :ARG1 instruct-01 :ARG2 :ARG0 Verbalization I received instructions to act act-02 Table 1: Several linguistic phenomena causing reentrancies in AMR. 2200 in which the node it has two incoming edges, creating a reentrancy. As mentioned before, one would expect relative clauses to be one of the syntactic reentrancy triggers, because the noun involved has a semantic role in both the main and relative clause: (8) I saw the womani who i won. In the example above, the woman is the object of seeing and the subject of winning. However, according to the AMR guidelines (Banarescu et al., 2013) relative clauses should be annotated as attaching to the noun with an inverse role, thereby avoiding a reentrancy (see Table 1). Pragmatic triggers Human annotators resolve coreferences even in the absence of definite syntactic clues, giving rise to pragmatically triggered reentrancies. To this class belong for instance the cases of pronominal anaphora resolution where the anaphora is not syntactically bound (unlike in 1). While coreference is, in general, a discourse phenomenon (Hobbs, 1979), it is also applicable to individual sentences such as those in the AMR corpora: (9) The coach of FC"
2020.findings-emnlp.199,P02-1018,0,0.387757,"Missing"
2020.findings-emnlp.199,P17-4012,0,0.0136849,".00) O RACLE +10.3 (0.00) R ANDOM SEQ 2 SEQ -4.2 (0.06) -0.1 (0.25) Table 4: Relative improvements in reentrancy prediction scores on the test set of LDC2017T10, obtained by the oracle and the proposed baselines. VANILLA are the scores obtained by Lyu and Titov (2018). 5 Automatic Error Correction 7 We further provide baseline systems that learn when to apply A DD, the most impactful action. First, we experiment with a system that randomly selects two nodes in the predicted graph that are not connected by any edge and add an edge with ARG0, the most frequent label. We also train a OpenNMT-py (Klein et al., 2017) sequence-tosequence model (Bahdanau et al., 2015) with a copy mechanism (Gulcehre et al., 2016). The input sequence is the predicted graph and the output sequence is the sequence of edges to add. For each edge, the output contains three tokens: the parent node, the child node, and the edge label. Table 4 shows that the baselines do not improve the predictions of the original parsers (VANILLA). While sequence modeling of the output is convenient, other options can be attempted. We are also only exploiting the input AMR parse but not the input sentence. We leave it to future work to address the"
2020.findings-emnlp.199,Q15-1040,0,0.029067,", where the node I has two parents. Reentrancies complicate AMR parsing and require the addition of specific transitions in transition-based parsing (Wang et al., 2015; Damonte et al., 2017) or of pre- and post-processing steps in sequence-to-sequence parsing (van Noord and Bos, 2017). Enabling AMR parsers to predict † you :ARG1 Introduction ∗ :ARG1 :ARG0 reentrancy structures correctly is of particular importance because it separates AMR parsing from semantic parsing based on tree structures (Steedman, 2000; Liang, 2013; Cheng et al., 2017). Reentrancy is however not an AMR-specific problem (Kuhlmann and Jonsson, 2015), and other formalisms can benefit from a better understanding of how to parse such structures. Nevertheless, to our knowledge, the AMR literature lacks any detailed discussion of the types and linguistic causes of reentrant structures. We aim to fill the gap by describing the phenomena causing reentrancies and quantifying their prevalence in the AMR corpus. We identify sources of reentrancy which have not been acknowledged in the AMR literature such as adjunct control, verbalization, and pragmatics. AMR parsers are evaluated using Smatch (Cai and Knight, 2013), which however does not explicit"
2020.findings-emnlp.199,J95-4004,0,0.586869,"g good AMR parsing performance. Related Work Our classification of phenomena causing reentrancies extends previous work in this direction (Groschwitz et al., 2017). van Noord and Bos (2017) previously attempted to improve the prediction of reentrancies in a neural parser. They experiment with several pre- and post-processing techniques and showed that co-indexing reentrancies nodes in the AMR annotations yields the best results. Transformation-based learning (Brill, 1993) inspired the idea of correcting existing parses. This approach has been mostly used for tagging (Ramshaw and Marcus, 1999; Brill, 1995; Nguyen et al., 2016) but it has also shown promises for semantic parsing (Jurˇc´ıcˇ ek et al., 2009). A similar approach has been also used to add empty nodes in constituent parses (Johnson, Conclusions Building upon previous observations that AMR parsers do not perform well at recovering reentrancies, we analyzed the linguistic phenomena responsible for reentrancies in AMR. We found sources of reentrancies which have not been acknowledged in the AMR literature such as adjunct control, verbalization, and pragmatics. The inclusion of reentrancies due to pragmatics is controversial; we hope th"
2020.findings-emnlp.199,J16-4009,0,0.0341884,"Missing"
2020.findings-emnlp.199,2020.acl-main.119,0,0.181779,"s. Our heuristics fail to detect the causes of many reentrancies. For a more precise estimate of the most common causes of reentrancies, it is necessary to manually annotate the reentrancies in the AMR corpora. Our oracle experiments show that there is room for improvement in predicting reentrancies, which in turn can translate to better parsing results. Stronger baselines that can learn how to correct the errors automatically are left to future work. While the parser we experimented with no longer gives state-of-the-art results (but also not far from them), newer parsers (Zhang et al., 2019; Cai and Lam, 2020) also report relatively low accuracy on reentrancies (using the metrics from Damonte et al. 2017), and as such we believe our work is relevant to these parsers. Acknowledgments The authors would like to thank anoymous reviewers, Adam Lopez, Bonnie Webber, Nathan Schneider, Sameer Bansal, and Yevgen Matusevych for their help and comments. This research was supported by a grant from Bloomberg as well as by the European Union H2020 project SUMMA, under grant agreement 688139 and the project SEMANTAX, which has received funding from the European Research Council (ERC) under the European 2206 Union"
2020.findings-emnlp.199,J13-2005,0,0.0275263,"ence of nodes with multiple parents, called reentrancies, as demonstrated in Figure 1, where the node I has two parents. Reentrancies complicate AMR parsing and require the addition of specific transitions in transition-based parsing (Wang et al., 2015; Damonte et al., 2017) or of pre- and post-processing steps in sequence-to-sequence parsing (van Noord and Bos, 2017). Enabling AMR parsers to predict † you :ARG1 Introduction ∗ :ARG1 :ARG0 reentrancy structures correctly is of particular importance because it separates AMR parsing from semantic parsing based on tree structures (Steedman, 2000; Liang, 2013; Cheng et al., 2017). Reentrancy is however not an AMR-specific problem (Kuhlmann and Jonsson, 2015), and other formalisms can benefit from a better understanding of how to parse such structures. Nevertheless, to our knowledge, the AMR literature lacks any detailed discussion of the types and linguistic causes of reentrant structures. We aim to fill the gap by describing the phenomena causing reentrancies and quantifying their prevalence in the AMR corpus. We identify sources of reentrancy which have not been acknowledged in the AMR literature such as adjunct control, verbalization, and pragm"
2020.findings-emnlp.199,P13-2131,0,0.0877441,"t an AMR-specific problem (Kuhlmann and Jonsson, 2015), and other formalisms can benefit from a better understanding of how to parse such structures. Nevertheless, to our knowledge, the AMR literature lacks any detailed discussion of the types and linguistic causes of reentrant structures. We aim to fill the gap by describing the phenomena causing reentrancies and quantifying their prevalence in the AMR corpus. We identify sources of reentrancy which have not been acknowledged in the AMR literature such as adjunct control, verbalization, and pragmatics. AMR parsers are evaluated using Smatch (Cai and Knight, 2013), which however does not explicitly assess the parsers’ ability to recover reentrancies. Damonte et al. (2017) introduced a measure of reentrancy prediction, which computes the Smatch score of the AMR subgraphs containing reentrancies. It was observed that the performance of parsers at recovering reentrancy structures is generally poor. We analyze errors made by the parsers and use an oracle to demonstrate that correcting reentrancy-related errors leads to parsing score improvement. Our contributions are as follows: Equal contribution Work done while at University of Edinburgh • We classify th"
2020.findings-emnlp.199,P18-1037,0,0.556171,"05.7 (3.21) 281.3 (5.51) 572.3 (4.04) 224.7 (3.06) +1.7 +0.7 +0.4 +0.2 +10.3 +3.1 -0.1 +0.8 M ERGE M ERGE - RMN S PLIT S PLIT- ADDN 187.3 (1.53) 94.3 (1.15) 574.7 (3.21) 333.0 (1.00) +0.4 +0.3 +1.2 +0.9 +1.6 +1.0 +1.8 -0.2 193.3 (3.06) 84.0 (2.00) 541.3 (4.16) 347.3 (3.79) +0.4 +0.2 +1.1 +0.9 +1.7 +0.9 +1.7 -0.0 A DD - SIB A DD - SIB - ADDN R M - SIB R M - SIB - RMN 128.0 (1.00) 99.7 (3.06) 69.3 (0.58) 0.0 (0.00) +0.2 +0.1 +0.1 +0.0 +1.3 -0.1 +0.2 -0.1 119.7 (1.15) 104.3 (1.53) 89.3 (0.58) 0.0 (0.00) +0.1 +0.1 +0.0 +0.0 +1.2 -0.0 +0.2 +0.0 Table 3: Relative Smatch improvements with respect to Lyu and Titov (2018) of all actions on the test split of LDC2015E86 and LDC2017T10. Freq. is the number of times the action could be applied, Smatch is the parsing score and Reent. is the reentrancies prediction score. A LL is the combination of all actions. VANILLA are the scores obtained by the original parsers. In parentheses, we report the standard deviation of the actions’ frequency. The standard deviation for the Smatch and reentrancy prediction scores is less or equal than 0.12. sa a) sb sc sa b) tb tb sa c) sb sc sb tc tb tc ta R EMOVE - SIB - RMN sc Results are shown in Table 3.9 While the largest improv"
2020.findings-emnlp.199,P17-1005,0,0.0210633,"with multiple parents, called reentrancies, as demonstrated in Figure 1, where the node I has two parents. Reentrancies complicate AMR parsing and require the addition of specific transitions in transition-based parsing (Wang et al., 2015; Damonte et al., 2017) or of pre- and post-processing steps in sequence-to-sequence parsing (van Noord and Bos, 2017). Enabling AMR parsers to predict † you :ARG1 Introduction ∗ :ARG1 :ARG0 reentrancy structures correctly is of particular importance because it separates AMR parsing from semantic parsing based on tree structures (Steedman, 2000; Liang, 2013; Cheng et al., 2017). Reentrancy is however not an AMR-specific problem (Kuhlmann and Jonsson, 2015), and other formalisms can benefit from a better understanding of how to parse such structures. Nevertheless, to our knowledge, the AMR literature lacks any detailed discussion of the types and linguistic causes of reentrant structures. We aim to fill the gap by describing the phenomena causing reentrancies and quantifying their prevalence in the AMR corpus. We identify sources of reentrancy which have not been acknowledged in the AMR literature such as adjunct control, verbalization, and pragmatics. AMR parsers ar"
2020.findings-emnlp.199,E17-1051,1,0.942412,"graphs (Banarescu et al., 2013) — rooted and directed acyclic graphs where nodes represent concepts and edges represent semantic relations between them. The AMR for the sentence I want you to believe me is shown in Figure 1. One of the main properties of AMR, and the reason why sentences are represented as graphs rather than trees, is the presence of nodes with multiple parents, called reentrancies, as demonstrated in Figure 1, where the node I has two parents. Reentrancies complicate AMR parsing and require the addition of specific transitions in transition-based parsing (Wang et al., 2015; Damonte et al., 2017) or of pre- and post-processing steps in sequence-to-sequence parsing (van Noord and Bos, 2017). Enabling AMR parsers to predict † you :ARG1 Introduction ∗ :ARG1 :ARG0 reentrancy structures correctly is of particular importance because it separates AMR parsing from semantic parsing based on tree structures (Steedman, 2000; Liang, 2013; Cheng et al., 2017). Reentrancy is however not an AMR-specific problem (Kuhlmann and Jonsson, 2015), and other formalisms can benefit from a better understanding of how to parse such structures. Nevertheless, to our knowledge, the AMR literature lacks any detail"
2020.findings-emnlp.199,P14-1134,0,0.123863,"ism rather than by what is actually expressed by the sentence. There are other conventions besides verbalization which introduce reentrancies, in particular if inverse roles were normalized2 . Our choice to nor normalize edge direcionality was partially motivated by a desire to avoid including those phenomena in our analysis. 3 Quantifying Reentrancy Causes In order to assess the prevalence of the various reentrancy triggers, we designed heuristics to assign each reentrancy in the AMR corpus to one of the above phenomena. We automatically align AMR graphs to their source sentences using JAMR (Flanigan et al., 2014) and identify the spans of words associated with re-entrant nodes.3 Heuristics based on Universal Dependency (UD) parses (Manning et al., 2014) and automatic coreference resolution are applied to the spans and the AMR subgraphs containing the reentrancy to classify the cause.4 We use the NeuralCoref project for coreference resolution.5 We recognize syntactic reentrancy triggers primarily with UD-based heuristics. For prototypical cases of control we look for common con2 representation of ”-er” nouns with their corresponding predicate and a person node; the convention for representing governmen"
2020.findings-emnlp.199,W17-6810,0,0.281805,"s or ”-er” nouns), with relative clauses admittedly being an exception. With that in mind, we classify reentrancy triggers into three broad types: syntactic, pragmatic, and AMR-specific. Syntactic triggers We consider a reentrancy as syntactically triggered if the syntactic structure of a sentence forces an interpretation in which one entity performs more than one semantic role. Below we illustrate the syntactic triggers which are commonly discussed in the AMR literature: some types of pronominal anaphora resolution (1), prototypical subject and object control (3 and 4), and coordination (2) (Groschwitz et al., 2017; van Noord and Bos, 2017). (1) The mani saw himselfi in the mirror. (2) Shei ate and i drank. (3) Theyi want i to believe. (4) I asked youi i to sing. In addition to those, our inspection of the AMR data revealed that other kinds of control structures, primarily adjunct control, are frequent reentrancy triggers. In adjunct control, the clause which lacks a subject is an adjunct of the main clause, as in the following examples: (5) Ii went home before i eating. (6) Shei left the room i crying. Such adjuncts express various additional information regarding the main clause, for example the"
2020.findings-emnlp.199,P16-1014,0,0.0208376,"ments in reentrancy prediction scores on the test set of LDC2017T10, obtained by the oracle and the proposed baselines. VANILLA are the scores obtained by Lyu and Titov (2018). 5 Automatic Error Correction 7 We further provide baseline systems that learn when to apply A DD, the most impactful action. First, we experiment with a system that randomly selects two nodes in the predicted graph that are not connected by any edge and add an edge with ARG0, the most frequent label. We also train a OpenNMT-py (Klein et al., 2017) sequence-tosequence model (Bahdanau et al., 2015) with a copy mechanism (Gulcehre et al., 2016). The input sequence is the predicted graph and the output sequence is the sequence of edges to add. For each edge, the output contains three tokens: the parent node, the child node, and the edge label. Table 4 shows that the baselines do not improve the predictions of the original parsers (VANILLA). While sequence modeling of the output is convenient, other options can be attempted. We are also only exploiting the input AMR parse but not the input sentence. We leave it to future work to address these issues and achieve better results. 6 2002), with considerable success. The SEQ 2 SEQ baseline"
2020.findings-emnlp.199,P14-5010,0,0.00301277,"n particular if inverse roles were normalized2 . Our choice to nor normalize edge direcionality was partially motivated by a desire to avoid including those phenomena in our analysis. 3 Quantifying Reentrancy Causes In order to assess the prevalence of the various reentrancy triggers, we designed heuristics to assign each reentrancy in the AMR corpus to one of the above phenomena. We automatically align AMR graphs to their source sentences using JAMR (Flanigan et al., 2014) and identify the spans of words associated with re-entrant nodes.3 Heuristics based on Universal Dependency (UD) parses (Manning et al., 2014) and automatic coreference resolution are applied to the spans and the AMR subgraphs containing the reentrancy to classify the cause.4 We use the NeuralCoref project for coreference resolution.5 We recognize syntactic reentrancy triggers primarily with UD-based heuristics. For prototypical cases of control we look for common con2 representation of ”-er” nouns with their corresponding predicate and a person node; the convention for representing government; special frames for roles 3 https://github.com/jflanigan/jamr 4 https://stanfordnlp.github.io/CoreNLP 5 https://github.com/huggingface/neural"
2020.findings-emnlp.199,W17-7306,0,0.176461,"Missing"
2020.findings-emnlp.199,P15-2141,0,0.0444994,"sentences into AMR graphs (Banarescu et al., 2013) — rooted and directed acyclic graphs where nodes represent concepts and edges represent semantic relations between them. The AMR for the sentence I want you to believe me is shown in Figure 1. One of the main properties of AMR, and the reason why sentences are represented as graphs rather than trees, is the presence of nodes with multiple parents, called reentrancies, as demonstrated in Figure 1, where the node I has two parents. Reentrancies complicate AMR parsing and require the addition of specific transitions in transition-based parsing (Wang et al., 2015; Damonte et al., 2017) or of pre- and post-processing steps in sequence-to-sequence parsing (van Noord and Bos, 2017). Enabling AMR parsers to predict † you :ARG1 Introduction ∗ :ARG1 :ARG0 reentrancy structures correctly is of particular importance because it separates AMR parsing from semantic parsing based on tree structures (Steedman, 2000; Liang, 2013; Cheng et al., 2017). Reentrancy is however not an AMR-specific problem (Kuhlmann and Jonsson, 2015), and other formalisms can benefit from a better understanding of how to parse such structures. Nevertheless, to our knowledge, the AMR lite"
2020.findings-emnlp.199,P19-1009,0,0.0732168,"Missing"
2020.iwpt-1.12,W10-4407,0,0.436233,"Missing"
2020.iwpt-1.12,N19-1018,0,0.318852,"continuous Penn Treebank (DPTB) (Evang and Kallmeyer, 2011). The treebanks were preprocessed using standard practice described in (Maier, 2015) by using the treetools5 package. For evaluation we have used the discodop6 package with the standard settings (van Cranenburgh et al., 2016). parameter word-embeddings dim. char bi-lstm dim. sentence-level bi-lstm layers sentence-level bi-lstm dim. compression MLP layers compression MLP dim. Adam optimiser lr. batch size value 32 100 2 200 2 200 0.001 1 sentence The architecture and hyper-parameters of the neural model are chosen to be the same as in (Coavoux and Cohen, 2019) to obtain a relatively fair comparison. That is, we use a combination of character bi-LSTM to embed each word. This embedding is concatenated with the lookup table embedding for each word and passed through a two-layer bi-LSTM that runs over the whole sentence. In case of MLP model we score labels for each span by passing two bi-LSTM vectors at borders of the spans through a two-layer MLP. In the case of the bi-affine model we compress bi-LSTM vectors with a specialised MLP for left and right index, analogous to the specialisation for head and dependent vector in Dozat and Manning (2017), and"
2020.iwpt-1.12,E17-1118,0,0.404622,"Missing"
2020.iwpt-1.12,Q19-1005,0,0.557478,"Missing"
2020.iwpt-1.12,D17-1172,0,0.432026,"Missing"
2020.iwpt-1.12,W13-5701,0,0.151522,"Missing"
2020.iwpt-1.12,D16-1001,0,0.376213,"ir full potential. Grammarbased methods have more difficulty incorporating rich probabilistic models due to the necessary independence assumptions needed for exact dynamic programming algorithms like CKY. Another disadvantage of grammar based models is that, even though their parsing algorithms are polynomial, they are significantly slower in practice due the high polynomial degrees and a large grammar constant. In this work we try to improve both speed and accuracy of chart-based parsers. The accuracy is improved by using a modified version of neural span-based scoring of non-terminal nodes (Cross and Huang, 2016; Stern et al., 2017) which does not break the independence assumptions needed for efficient parsing. Speed is improved by restricting the set of acceptable trees to the ones recognizable with an LCFRS-2 grammar formalism, but no explicit grammar is used, removing the grammar constant from the worst-case complexity.3 Additionally, the parser is implemented using an imperative approach to Viterbi CKY parsing (as opposed to deductive approach), similar to standard CFG CKY implementations with embedded loops. By avoiding the usage of standard weighted deductive parsing (Shieber et al., 1995; Nede"
2020.iwpt-1.12,W11-2913,0,0.879148,"rojective, i.e. there would be crossed dependencies (Nivre et al., 2016). In constituency treebanks this is modelled either by using traces that are co-indexed with the moving element, as in English Penn treebank (Marcus et al., 1993), or by having a direct discontinuous constituent, as in German Negra and Tiger treebanks (Brants et al., 2004). Here we adopt the discontinuous constituency approach because of its well defined formal properties, but the results are also relevant for other representations. The Penn treebank trace representation can be converted to a discontinuous representation (Evang and Kallmeyer, 2011)1 and nonprojective dependency trees can be interpreted as lexicalised versions of discontinuous constituency trees (Kuhlmann and M¨ohl, 2007). There are two different approaches to predicting discontinuous constituency structure directly.2 The first approach, usually grammar-based chart parsing, limits the type of trees that are acceptable (for example TAG (Joshi, 1985) or LCFRS (Vijay-Shanker et al., 1987; Seki et al., 1991)) and searches for the best tree with an exact search algorithm like CKY. The second approach, usually transition-based, does not limit the type of trees but searches for"
2020.iwpt-1.12,P15-1147,0,0.316381,"Missing"
2020.iwpt-1.12,N18-1091,0,0.0459152,"Missing"
2020.iwpt-1.12,P07-1021,0,0.121937,"Missing"
2020.iwpt-1.12,N10-1035,0,0.0512938,"Missing"
2020.iwpt-1.12,P06-2066,0,0.205967,"dard CFG and a very efficient parser. If the fan-out is unrestricted (as big as the sentence being parsed), we could process any discontinuous structure but will get a non-polynomial parser. Clearly, we need to restrict fan-out to some small constant number. Maier et al. (2012) suggested that restricting fan-out to 2 is sufficient to process a large portion of discontinuous structures in German treebanks. We adopt this proposal and show the consequences of it in the experiments section. We will refer to this grammar as LCFRS-2. Another useful restriction of LCFRS is a wellnestedness property (Kuhlmann and Nivre, 2006; Maier and Lichte, 2009). For any LCFRS rule which contains some non-terminals A and B on its right-hand side we say that it is well-nested 112 if there are no components A1 and A2 from A, and B1 and B2 from B that form a linear order A1 < B1 < A2 < B2 . This property allows for efficient parsing (G´omez-Rodr´ıguez et al., 2010), but in our case of a binary LCFRS with fan-out 2, the effect of well-nestedness will be only proportional to some constant. Maier and Lichte (2009) state that well-nestedness holds for the majority of constituents in German treebanks. We will test it in our experimen"
2020.iwpt-1.12,W13-3003,0,0.058433,"4, 5) . An LCFRS rule that forms this constituent can be expressed as: PP(X, Y ) → WH(X) P(Y ) These individual spans are often called components and the number of them per non-terminal is called the fan-out of the non-terminal. The fan-out of an LCFRS grammar is defined by the maximal fan-out of its non-terminals. The fan-out of the grammar has significant consequences to its expressivity and the parsing complexity. For binary LCFRS the worst-case parsing complexity is O(G · n3φ ) where G is the grammar constant (total number of LCFRS rules) and φ is the grammar’s fan-out (Seki et al., 1991; Kallmeyer, 2013). If fan-out is 1 we get only the power of a standard CFG and a very efficient parser. If the fan-out is unrestricted (as big as the sentence being parsed), we could process any discontinuous structure but will get a non-polynomial parser. Clearly, we need to restrict fan-out to some small constant number. Maier et al. (2012) suggested that restricting fan-out to 2 is sufficient to process a large portion of discontinuous structures in German treebanks. We adopt this proposal and show the consequences of it in the experiments section. We will refer to this grammar as LCFRS-2. Another useful re"
2020.iwpt-1.12,C10-1061,0,0.136834,"binarization of a gold tree to be a positive class. 4 Direct CKY Parsing Algorithm The algorithms for LCFRS are usually presented, and implemented, as deductive rules. These deductive rules, combined with a deduction engine of Shieber et al. (1995) can form a conceptually simple mechanism for parsing. In case of weighted deductive rules the modification of Nederhof (2003) can be used. It modifies the method of search to explore the most probable search space first by implementing the agenda as a priority queue. Almost all Probabilistic LCFRS (PLCFRS) parsers have been implemented in this way (Kallmeyer and Maier, 2010; Maier et al., 2012; van Cranenburgh et al., 2016). However, there are many reasons not to use this approach with our span-based model. First, implementing the agenda as a priority queue adds a O(log n) multiplicative term to the worst-case complexity. Second, the multiplicative grammar constant that exists in PLCFRS approaches does not exist in ours since there is no explicit grammar, and the optimal label for each span is independent of the other spans. Third, because of the difficulty of implementing optimal chart lookup under deductive approaches means that most PLCFRS parsers optimise lo"
2020.iwpt-1.12,P18-1249,0,0.0615122,"ation task and train the model to predict the probability of that triple being part of the gold tree. For training we use not only the triples from the gold tree but all possible triples for a given sentence. We consider the probability of the tree to Neural Span-Based Model We borrow and modify some ideas already popular in CFG parsing to improve LCFRS-2 chart parsing. In particular, span-based scoring is a popular approach for modelling scoring of parse trees without breaking the dynamic programming assumptions of chart parsers (Cross and Huang, 2016; Stern et al., 2017; Gaddy et al., 2018; Kitaev and Klein, 2018a,b). In this approach words are first encoded with biLSTM (Hochreiter and Schmidhuber, 1997; Graves et al., 2005). These word encodings are afterwards used to score spans. For each span we take encodings of two words that are at its borders and pass them through feed-forward (Cross and Huang, 2016; Gaddy et al., 2018) or bi-affine classifier (Dozat and Manning, 2017; Stern et al., 2017) that predicts the score for each possible label (nonterminal) occupying that span. Unaries are all collapsed into a single non-terminal to simplify scoring. The score of a whole tree is defined as a sum of the"
2020.iwpt-1.12,E09-1053,0,0.090397,"Missing"
2020.iwpt-1.12,P15-1116,0,0.2968,"of the approaches because we handle unary rules in a different way as previously described. 5 The parser is implemented in Scala using DyNet (Neubig et al., 2017) and is available on github.4 Experiments are conducted on German and English discontinuous constituency treebanks. The reported development results are on the German Negra treebank. The test set results, in addition to German Negra, also contain German Tiger treebank (Brants et al., 2004) and English Discontinuous Penn Treebank (DPTB) (Evang and Kallmeyer, 2011). The treebanks were preprocessed using standard practice described in (Maier, 2015) by using the treetools5 package. For evaluation we have used the discodop6 package with the standard settings (van Cranenburgh et al., 2016). parameter word-embeddings dim. char bi-lstm dim. sentence-level bi-lstm layers sentence-level bi-lstm dim. compression MLP layers compression MLP dim. Adam optimiser lr. batch size value 32 100 2 200 2 200 0.001 1 sentence The architecture and hyper-parameters of the neural model are chosen to be the same as in (Coavoux and Cohen, 2019) to obtain a relatively fair comparison. That is, we use a combination of character bi-LSTM to embed each word. This em"
2020.iwpt-1.12,W12-4615,0,0.734999,"-out of the grammar has significant consequences to its expressivity and the parsing complexity. For binary LCFRS the worst-case parsing complexity is O(G · n3φ ) where G is the grammar constant (total number of LCFRS rules) and φ is the grammar’s fan-out (Seki et al., 1991; Kallmeyer, 2013). If fan-out is 1 we get only the power of a standard CFG and a very efficient parser. If the fan-out is unrestricted (as big as the sentence being parsed), we could process any discontinuous structure but will get a non-polynomial parser. Clearly, we need to restrict fan-out to some small constant number. Maier et al. (2012) suggested that restricting fan-out to 2 is sufficient to process a large portion of discontinuous structures in German treebanks. We adopt this proposal and show the consequences of it in the experiments section. We will refer to this grammar as LCFRS-2. Another useful restriction of LCFRS is a wellnestedness property (Kuhlmann and Nivre, 2006; Maier and Lichte, 2009). For any LCFRS rule which contains some non-terminals A and B on its right-hand side we say that it is well-nested 112 if there are no components A1 and A2 from A, and B1 and B2 from B that form a linear order A1 < B1 < A2 < B2"
2020.iwpt-1.12,J93-2004,0,0.0700531,"University of Edinburgh steedman@inf.ed.ac.uk that are a major concern of most syntactic theories. Take for instance the sentence in Figure 1. It contains a long range dependency between “on” and “what”. This is represented differently across syntactic theories. In dependency parsing, there would be a direct arc between these two words that would cause the dependency tree to be non-projective, i.e. there would be crossed dependencies (Nivre et al., 2016). In constituency treebanks this is modelled either by using traces that are co-indexed with the moving element, as in English Penn treebank (Marcus et al., 1993), or by having a direct discontinuous constituent, as in German Negra and Tiger treebanks (Brants et al., 2004). Here we adopt the discontinuous constituency approach because of its well defined formal properties, but the results are also relevant for other representations. The Penn treebank trace representation can be converted to a discontinuous representation (Evang and Kallmeyer, 2011)1 and nonprojective dependency trees can be interpreted as lexicalised versions of discontinuous constituency trees (Kuhlmann and M¨ohl, 2007). There are two different approaches to predicting discontinuous c"
2020.iwpt-1.12,J03-1006,0,0.41532,"2016; Stern et al., 2017) which does not break the independence assumptions needed for efficient parsing. Speed is improved by restricting the set of acceptable trees to the ones recognizable with an LCFRS-2 grammar formalism, but no explicit grammar is used, removing the grammar constant from the worst-case complexity.3 Additionally, the parser is implemented using an imperative approach to Viterbi CKY parsing (as opposed to deductive approach), similar to standard CFG CKY implementations with embedded loops. By avoiding the usage of standard weighted deductive parsing (Shieber et al., 1995; Nederhof, 2003). we avoid the need to maintain heap property of the agenda, further reducing the worst-case parsing complexity. This results in a fast chart-based LCFRS-2 parser that outperforms all previous chart-based parsers for discontinuous structures, and gives performance that is on par with the best transition-based parsers. 2 LCFRS-2 Trees LCFRS (Vijay-Shanker et al., 1987; Seki et al., 1991; Kallmeyer, 2010) is a grammar formalism that works in a similar way to CFG: it applies a series of recursive rewriting rules that eventually generate a sentence. What makes it different from CFG is that it allo"
2020.iwpt-1.12,N19-1016,0,0.119182,"m, but for discontinuous constituents that is sometimes not the case. To test the limitations that the formalism puts on our model further we did oracle experiments that would show what would results be if we had an Parsing speed Chart parsers have often been avoided for expressive formalisms like LCFRS because of their high worst-case complexity. Most previous work using them has either constrained sentences to those less than 30 words in length, or used length filtering in combination with heavy pruning (Evang and Kallmeyer, 2011; van Cranenburgh and Bod, 2013; van Cranenburgh et al., 2016; Ruprecht and Denkinger, 2019) it is therefore important to compare our parser with previous approaches not only in accuracy but also in speed. 117 conversion transition-based LCFRS parser this work Evang and Kallmeyer (2011) ≤ 25 gold POS van Cranenburgh et al. (2016) ≤ 40 Ruprecht and Denkinger (2019) ≤ 30 gold POS Coavoux and Cohen (2019) Coavoux et al. (2019) Stanojevi´c and G. Alhama (2017) Stanojevi´c and G. Alhama (2017) gold POS Maier (2015) gold POS Coavoux and Crabb´e (2017) gold POS Corro et al. (2017) Corro et al. (2017) gold POS Versley (2016) Fern´andez-Gonz´alez and Martins (2015) Fern´andez-Gonz´alez and G´"
2020.iwpt-1.12,D17-1174,1,0.720755,"Missing"
2020.iwpt-1.12,P17-1076,0,0.202183,"marbased methods have more difficulty incorporating rich probabilistic models due to the necessary independence assumptions needed for exact dynamic programming algorithms like CKY. Another disadvantage of grammar based models is that, even though their parsing algorithms are polynomial, they are significantly slower in practice due the high polynomial degrees and a large grammar constant. In this work we try to improve both speed and accuracy of chart-based parsers. The accuracy is improved by using a modified version of neural span-based scoring of non-terminal nodes (Cross and Huang, 2016; Stern et al., 2017) which does not break the independence assumptions needed for efficient parsing. Speed is improved by restricting the set of acceptable trees to the ones recognizable with an LCFRS-2 grammar formalism, but no explicit grammar is used, removing the grammar constant from the worst-case complexity.3 Additionally, the parser is implemented using an imperative approach to Viterbi CKY parsing (as opposed to deductive approach), similar to standard CFG CKY implementations with embedded loops. By avoiding the usage of standard weighted deductive parsing (Shieber et al., 1995; Nederhof, 2003). we avoid"
2020.iwpt-1.12,W04-3201,0,0.46096,"each X. This decomposes the discontinuous constituent scoring as scoring of three continuous constituents. The labelling complexity with l labels is still O(l n4 ) but the neural matrix multiplication will be done only O(n2 ) times just like in CFG case of Stern et al. (2017). In Section 5.3 we will show that most of the time is spent in the neural component and span combination, and that the labelling component takes a negligible proportion of time. The second aspect of span-based models that we needed to change is the objective function. The original max-margin parsing objective proposed by Taskar et al. (2004b) maximised the margin between the gold tree and all other trees. Because that approach was too slow in practice it is usually approximated by maximising only the margin between the gold and the highest scoring tree, in case highest scoring tree is not the gold tree. This approach gave good results in CFG parsing (Stern et al., 2017), but it was very unstable in our tests. The reason for this may be in the difference between the number of possible hypotheses between CFG and LCFRS-2 which increases quadratically from the order of O(n3 ) to O(n6 ). In this case, optimising for just a single mar"
2020.iwpt-1.12,W16-0907,0,0.367038,"Missing"
2020.iwpt-1.12,P87-1015,0,0.867351,"f its well defined formal properties, but the results are also relevant for other representations. The Penn treebank trace representation can be converted to a discontinuous representation (Evang and Kallmeyer, 2011)1 and nonprojective dependency trees can be interpreted as lexicalised versions of discontinuous constituency trees (Kuhlmann and M¨ohl, 2007). There are two different approaches to predicting discontinuous constituency structure directly.2 The first approach, usually grammar-based chart parsing, limits the type of trees that are acceptable (for example TAG (Joshi, 1985) or LCFRS (Vijay-Shanker et al., 1987; Seki et al., 1991)) and searches for the best tree with an exact search algorithm like CKY. The second approach, usually transition-based, does not limit the type of trees but searches for the best tree only approximately 1 This is a lossy conversion because the discontinuous representation does not contain information about the initial location of the constituent before it was displaced, nor the attachment point in the surface tree. 2 Indirect approaches work by conversion to some other simpler formalism, parsing in the simpler formalism, and then converting the result back. They are not th"
2020.starsem-1.15,N19-1423,0,0.087264,"Missing"
2020.starsem-1.15,P16-1047,0,0.0383028,"Missing"
2020.starsem-1.15,E17-2010,0,0.0232697,"Missing"
2020.starsem-1.15,C10-1076,0,0.0462387,"Missing"
2020.starsem-1.15,N13-1090,0,0.117105,"Missing"
2020.starsem-1.15,S12-1035,0,0.0795836,"Missing"
2020.starsem-1.15,P14-1007,0,0.0396819,"Missing"
2020.starsem-1.15,S12-1041,0,0.0616489,"Missing"
2020.starsem-1.15,N19-1020,1,0.631439,"Missing"
2020.starsem-1.15,W08-0606,0,0.0621914,"1989; Horn and Wansing, 2017), such as in ¬SOLVE(Sherlock, case). Further, in truth-theoretic logic the meaning of negation is simply the inverted truth value of this expression. Capturing the meaning of a negation cue in language can thus be understood as simply identifying the negated expression. This is the task of Negation Scope Detection (NSD): given a negation cue in a sentence, identify the sentence tokens which make up the negated expression. Negation Scope Detection Task The task of NSD has been approached in several formulations using different annotation schemes (Kim et al., 2008; Szarvas et al., 2008), but these older tasks do not define negation semantically, in contrast to the commonly accepted *SEM2012 Shared Task competition (*SEM, 2012). Under the *SEM definition cues scope over events and participants, either directly or via predicate arguments and complements. Linguistic theory explains negation semantics in terms of events, which are built from syntactic phrase structures (Huddleston and Pullum, 2005), i.e. that negation doesn’t just scope over individual words, but rather whole phrases and clauses which make up events. The following examples drawn from the *SEM2012 dataset of Cona"
2020.textgraphs-1.7,P11-1062,0,0.644967,"pitfall of learning non-entailments such as win 6→ lose. We evaluate our model on a manually constructed dataset, showing that incorporating time intervals and applying a temporal window around them, are effective strategies. 1 Introduction Recognising textual entailment and paraphrases is core to many downstream NLP applications such as question answering and semantic parsing. In the case of open-domain question answering over unstructured data, the answer to a question may not be explicitly stated in the text, but may be recovered via paraphrases and/or entailment rules. Entailment graphs (Berant et al., 2011; Berant et al., 2015; Hosseini et al., 2018), in which nodes represent predicates and edges are entailment relations, have been proposed as a means to answer such questions. They can be mined using unsupervised methods applied over large collections of text, by keeping track of which entity pairs occur with which predicates. One common error made by these graphs, however, is that they assert spurious associations between similar but temporally distinct events that occur with the same entity pairs. For example, both the predicates beat and lost against will apply to sports team entity pairs su"
2020.textgraphs-1.7,J15-2003,0,0.271857,"non-entailments such as win 6→ lose. We evaluate our model on a manually constructed dataset, showing that incorporating time intervals and applying a temporal window around them, are effective strategies. 1 Introduction Recognising textual entailment and paraphrases is core to many downstream NLP applications such as question answering and semantic parsing. In the case of open-domain question answering over unstructured data, the answer to a question may not be explicitly stated in the text, but may be recovered via paraphrases and/or entailment rules. Entailment graphs (Berant et al., 2011; Berant et al., 2015; Hosseini et al., 2018), in which nodes represent predicates and edges are entailment relations, have been proposed as a means to answer such questions. They can be mined using unsupervised methods applied over large collections of text, by keeping track of which entity pairs occur with which predicates. One common error made by these graphs, however, is that they assert spurious associations between similar but temporally distinct events that occur with the same entity pairs. For example, both the predicates beat and lost against will apply to sports team entity pairs such as (Arsenal, Man U"
2020.textgraphs-1.7,D15-1075,0,0.0410157,"ombines the desirable behaviors of symmetric and directional measures. We adapt and examine each of these similarity measures using our evaluation dataset. Alternatively, Hosseini et al. (2019) performed link prediction on the set of relation triples extracted from the text, and showed improvements over BInc by augmenting the data with additional predicted triples. We consider this link prediction model to be beyond the scope of this work. 2.2 Evaluating Entailment Graphs The construction of entailment datasets has been framed as a number of manual annotation tasks including image captioning (Bowman et al., 2015) and question answering (Levy and Dagan, 2016). The dataset creation method used by Levy and Dagan (2016) aims to address the bias towards real world knowledge. They ask human annotators to mark possible answers to questions as True/False (entailment/non-entailment), with entities in the answer replaced using tokens representing their type (e.g. London becomes city). The method also aims to address the bias of other datasets such as Zeichner’s dataset (Zeichner et al., 2012) and the SherLIic dataset (Schmitt and Sch¨utze, 2019), in which candidate entailments were automatically pre-selected fo"
2020.textgraphs-1.7,chang-manning-2012-sutime,0,0.0261532,"Missing"
2020.textgraphs-1.7,P05-1014,0,0.112325,"(Berant et al., 2011; Hosseini et al., 2018), and eventualities (Yu et al., 2020). In this work we use typed predicates, leveraging the second level in the FIGER hierarchy (Ling and Weld, 2012), to enable a close examination of events that take place between two sports teams. Whether predicates in the graph entail each other may be determined using a variety of similarity measures. These are inspired by the Distributional Inclusion Hypothesis, which states that a predicate p entails another predicate q if for any context in which p can be used, q may be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005). They include the symmetric Lin’s similarity measure (Lin, 1998), the directional Weeds’ precision and recall measures (Weeds and Weir, 2003), and the Balanced Inclusion score (BInc) (Szpektor and Dagan, 2008). BInc, the geometric mean of Lin’s similarity and Weed’s precision, combines the desirable behaviors of symmetric and directional measures. We adapt and examine each of these similarity measures using our evaluation dataset. Alternatively, Hosseini et al. (2019) performed link prediction on the set of relation triples extracted from the text, and showed improvements over BInc by augment"
2020.textgraphs-1.7,Q18-1048,1,0.155495,"as win 6→ lose. We evaluate our model on a manually constructed dataset, showing that incorporating time intervals and applying a temporal window around them, are effective strategies. 1 Introduction Recognising textual entailment and paraphrases is core to many downstream NLP applications such as question answering and semantic parsing. In the case of open-domain question answering over unstructured data, the answer to a question may not be explicitly stated in the text, but may be recovered via paraphrases and/or entailment rules. Entailment graphs (Berant et al., 2011; Berant et al., 2015; Hosseini et al., 2018), in which nodes represent predicates and edges are entailment relations, have been proposed as a means to answer such questions. They can be mined using unsupervised methods applied over large collections of text, by keeping track of which entity pairs occur with which predicates. One common error made by these graphs, however, is that they assert spurious associations between similar but temporally distinct events that occur with the same entity pairs. For example, both the predicates beat and lost against will apply to sports team entity pairs such as (Arsenal, Man United). This is likely t"
2020.textgraphs-1.7,P19-1468,1,0.846673,"cate p entails another predicate q if for any context in which p can be used, q may be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005). They include the symmetric Lin’s similarity measure (Lin, 1998), the directional Weeds’ precision and recall measures (Weeds and Weir, 2003), and the Balanced Inclusion score (BInc) (Szpektor and Dagan, 2008). BInc, the geometric mean of Lin’s similarity and Weed’s precision, combines the desirable behaviors of symmetric and directional measures. We adapt and examine each of these similarity measures using our evaluation dataset. Alternatively, Hosseini et al. (2019) performed link prediction on the set of relation triples extracted from the text, and showed improvements over BInc by augmenting the data with additional predicted triples. We consider this link prediction model to be beyond the scope of this work. 2.2 Evaluating Entailment Graphs The construction of entailment datasets has been framed as a number of manual annotation tasks including image captioning (Bowman et al., 2015) and question answering (Levy and Dagan, 2016). The dataset creation method used by Levy and Dagan (2016) aims to address the bias towards real world knowledge. They ask hum"
2020.textgraphs-1.7,W19-0409,1,0.894684,"Missing"
2020.textgraphs-1.7,P16-2041,0,0.250603,"and directional measures. We adapt and examine each of these similarity measures using our evaluation dataset. Alternatively, Hosseini et al. (2019) performed link prediction on the set of relation triples extracted from the text, and showed improvements over BInc by augmenting the data with additional predicted triples. We consider this link prediction model to be beyond the scope of this work. 2.2 Evaluating Entailment Graphs The construction of entailment datasets has been framed as a number of manual annotation tasks including image captioning (Bowman et al., 2015) and question answering (Levy and Dagan, 2016). The dataset creation method used by Levy and Dagan (2016) aims to address the bias towards real world knowledge. They ask human annotators to mark possible answers to questions as True/False (entailment/non-entailment), with entities in the answer replaced using tokens representing their type (e.g. London becomes city). The method also aims to address the bias of other datasets such as Zeichner’s dataset (Zeichner et al., 2012) and the SherLIic dataset (Schmitt and Sch¨utze, 2019), in which candidate entailments were automatically pre-selected for manual annotation according to a similarity"
2020.textgraphs-1.7,W14-1610,0,0.0248504,"ences (left) and their resulting (collapsed) entailment/non-entailment graph (right) The contributions of this work are: 1) a model for incorporating relation-level time intervals into an entailment graph mining procedure, outperforming non-temporal models, and 2) a manually constructed evaluation dataset of sports domain predicates. To our knowledge this is the first attempt to incorporate temporal information for learning entailment graphs. 2 2.1 Related Work Entailment Graphs Entailment graphs have been constructed for a range of domains, including newswire (Hosseini et al., 2018), health (Levy et al., 2014), and commonsense (Yu et al., 2020). In order to leverage temporal information, our work focuses on the news domain, in which each article has a known publication date and temporal expressions are commonly used. A range of node representations have been explored, including Open-IE propositions (Levy et al., 2014), typed predicates (Berant et al., 2011; Hosseini et al., 2018), and eventualities (Yu et al., 2020). In this work we use typed predicates, leveraging the second level in the FIGER hierarchy (Ling and Weld, 2012), to enable a close examination of events that take place between two spor"
2020.textgraphs-1.7,Q13-1015,1,0.848439,"Missing"
2020.textgraphs-1.7,P19-1086,0,0.026677,"Missing"
2020.textgraphs-1.7,N19-1020,1,0.770989,"Missing"
2020.textgraphs-1.7,C08-1107,0,0.911416,"close examination of events that take place between two sports teams. Whether predicates in the graph entail each other may be determined using a variety of similarity measures. These are inspired by the Distributional Inclusion Hypothesis, which states that a predicate p entails another predicate q if for any context in which p can be used, q may be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005). They include the symmetric Lin’s similarity measure (Lin, 1998), the directional Weeds’ precision and recall measures (Weeds and Weir, 2003), and the Balanced Inclusion score (BInc) (Szpektor and Dagan, 2008). BInc, the geometric mean of Lin’s similarity and Weed’s precision, combines the desirable behaviors of symmetric and directional measures. We adapt and examine each of these similarity measures using our evaluation dataset. Alternatively, Hosseini et al. (2019) performed link prediction on the set of relation triples extracted from the text, and showed improvements over BInc by augmenting the data with additional predicted triples. We consider this link prediction model to be beyond the scope of this work. 2.2 Evaluating Entailment Graphs The construction of entailment datasets has been fram"
2020.textgraphs-1.7,S13-2001,0,0.0691019,"Missing"
2020.textgraphs-1.7,W03-1011,0,0.368862,"level in the FIGER hierarchy (Ling and Weld, 2012), to enable a close examination of events that take place between two sports teams. Whether predicates in the graph entail each other may be determined using a variety of similarity measures. These are inspired by the Distributional Inclusion Hypothesis, which states that a predicate p entails another predicate q if for any context in which p can be used, q may be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005). They include the symmetric Lin’s similarity measure (Lin, 1998), the directional Weeds’ precision and recall measures (Weeds and Weir, 2003), and the Balanced Inclusion score (BInc) (Szpektor and Dagan, 2008). BInc, the geometric mean of Lin’s similarity and Weed’s precision, combines the desirable behaviors of symmetric and directional measures. We adapt and examine each of these similarity measures using our evaluation dataset. Alternatively, Hosseini et al. (2019) performed link prediction on the set of relation triples extracted from the text, and showed improvements over BInc by augmenting the data with additional predicted triples. We consider this link prediction model to be beyond the scope of this work. 2.2 Evaluating Ent"
2020.textgraphs-1.7,P12-2031,0,0.0194155,"The construction of entailment datasets has been framed as a number of manual annotation tasks including image captioning (Bowman et al., 2015) and question answering (Levy and Dagan, 2016). The dataset creation method used by Levy and Dagan (2016) aims to address the bias towards real world knowledge. They ask human annotators to mark possible answers to questions as True/False (entailment/non-entailment), with entities in the answer replaced using tokens representing their type (e.g. London becomes city). The method also aims to address the bias of other datasets such as Zeichner’s dataset (Zeichner et al., 2012) and the SherLIic dataset (Schmitt and Sch¨utze, 2019), in which candidate entailments were automatically pre-selected for manual annotation according to a similarity measure. Entailments that exist, but are not captured by these similarity measures will therefore be excluded. There has been very little work on the specific problem of evaluating entailment of a temporal nature. The FraCas test suite (Cooper et al., 1996) contains a small section of which only a few examples are 61 entailments between predicates. The TEA dataset (Kober et al., 2019) consists of pairs of sentences in which tempo"
2020.textgraphs-1.7,D13-1183,0,0.641764,"Missing"
2021.acl-long.79,H89-2017,0,0.537777,"ackets is called the reparandum, which is immediately followed by the interruption point. Disfluencies in SWBD-NXT are marked in the constituency parse annotation, where the reparandum is marked as a constituent with the label EDITED. The interruption point is the right edge of this constituent. Our analysis draws on the work of Shriberg (2001), who described the prosodic features of the interruption point and the reparandum based on an analysis of three English conversational and taskbased dialogue corpora — the Switchboard Corpus (which we use a subset of), ATIS (Hirschman, 1992), and AMEX (Kowtko and Price, 1989). Pauses. Although pauses may be the most intuitive potential cue to SU boundaries, previous work suggests that long pauses also characterize interruption points (Wagner and Watson, 2010; Shriberg, 2001). Indeed, our analysis shows that longer pauses (&gt; 0.05s) are over-represented in both locations. If pause types were distributed uniformly, 16 percent of both SU boundaries and interruption The second duration feature is the token length normalized by the maximum length of any token in the input, to normalize for speaking rate. Initially, this feature looks helpful: SU-final words have mean va"
2021.acl-long.79,N13-1023,0,0.0328193,"Roark et al. (2006)). However, they show that having access to gold SU boundaries increases the SParseval score from 78.5 to 82.3, which shows that parsing without gold SU boundaries is difficult. However, in some research areas, prosody is less frequently used for SU detection. Some ASR corpora and applications segment at relatively arbitrary boundaries such as long silences or even regular intervals (e.g., Jain et al. (2020)). Other applications, such as speech translation, do require syntactically coherent input, but even there, systems targeting SUs have often used only textual features (Sridhar et al., 2013; Wan et al., 2020). D (ASR) transcript; we plan to use ASR output in future work. We build on the work of Tran et al. (2018) and Tran et al. (2019), considering two different experimental conditions for each model: inputting text features only and inputting both text and prosodic features. Using the Switchboard corpus of English conversational dialogue, we find that when only transcripts are used, the turn-based parser performs considerably worse than the SU-based parser, which is not surprising given that it needs to perform two tasks instead of one. However, when prosodic features are inclu"
2021.acl-long.79,D12-1096,0,0.30946,"sody boosts SU segmentation accuracy to near-perfect levels, which explains why the parser performance is similar (and much better than without prosody). Comparing the two text-only models reveals a more interesting pattern: while the pipeline model achieves much better segmentation F1, its parsing performance is worse. This is unexpected, as parsing and segmentation performance are usually correlated. This effect seems to arise because the two models err in different directions on segmentation: The pipeline model under-segments turns (corre5.1 Error types We use the Berkeley Parser Analyser (Kummerfeld et al., 2012) to determine what types of errors each of the SU-based and end-to-end turn-based models makes. Figure 2 summarizes the output of the Analyser. Overall, the SU-based parser shows only small effects from prosody, but the turn-based model does significantly worse on certain error types without prosody. Even for the turn-based model, prosody only affects error types that have to do with the shape of the tree. The different label category shows errors where two identically shaped trees have different constituent labels, and prosody has no effect on these. For the turn-based model, poor SU segmenta"
2021.acl-long.79,D14-1162,0,0.0839201,"Missing"
2021.acl-long.79,roark-etal-2006-sparseval,0,0.104552,"e important include Gotoh and Renals (2000); Kol´aˇr et al. (2006); Kahn et al. (2004); Kahn and Ostendorf (2012), who all used traditional statistical models (e.g., HMMs, finite state machines, and decision trees), and Xu et al. (2014), who used a neural model. Kahn et al. (2004) and Kahn and Ostendorf (2012) also looked at downstream parsing accuracy on the same corpus we use. Like us, Kahn and Ostendorf (2012) don’t use gold SU boundaries, but direct comparison is impossible because they use ASR output instead of human transcriptions and a different metric for parse performance (SParseval; Roark et al. (2006)). However, they show that having access to gold SU boundaries increases the SParseval score from 78.5 to 82.3, which shows that parsing without gold SU boundaries is difficult. However, in some research areas, prosody is less frequently used for SU detection. Some ASR corpora and applications segment at relatively arbitrary boundaries such as long silences or even regular intervals (e.g., Jain et al. (2020)). Other applications, such as speech translation, do require syntactically coherent input, but even there, systems targeting SUs have often used only textual features (Sridhar et al., 2013"
2021.acl-long.79,P17-1076,0,0.0207485,"om seeds. For the test set, we use the model that has the median dev. set performance out of 10 randomly seeded models. D Text only Text+pros. ∆ Input length (# tokens) 1 2–8 9–22 23–255 98.36 93.00 89.22 84.30 99.18 94.91 92.74 89.80 0.82 1.91 3.52 5.5 Table 2: F1 performance of the text-only and text+prosody turn-based models on inputs of various lengths in the development set. The inputs are divided into bins of approximately equal size by token length. RE TR AC TE parser based on Kitaev and Klein (2018)’s textonly parser, with a transformer-based encoder and a chart-style decoder based on Stern et al. (2017) and Gaddy et al. (2018). This encoder-decoder is augmented with a CNN on the input side that handles prosodic features (Tran et al., 2019). For further description of the model and hyperparameters, see Appendices A.1 and A.2. The text is encoded using 300-dimensional GloVe embeddings (Pennington et al., 2014).4 Of the four types of prosodic features described in Section 3, pause and duration features are already token-level. However, pitch and intensity features are extracted from the speech signal at the frame level. In order to map from these frame-level features to a token-level representa"
2021.acl-long.79,N18-1007,0,0.0523023,"Missing"
2021.acl-long.79,2020.clssts-1.11,0,0.0341009,"However, they show that having access to gold SU boundaries increases the SParseval score from 78.5 to 82.3, which shows that parsing without gold SU boundaries is difficult. However, in some research areas, prosody is less frequently used for SU detection. Some ASR corpora and applications segment at relatively arbitrary boundaries such as long silences or even regular intervals (e.g., Jain et al. (2020)). Other applications, such as speech translation, do require syntactically coherent input, but even there, systems targeting SUs have often used only textual features (Sridhar et al., 2013; Wan et al., 2020). D (ASR) transcript; we plan to use ASR output in future work. We build on the work of Tran et al. (2018) and Tran et al. (2019), considering two different experimental conditions for each model: inputting text features only and inputting both text and prosodic features. Using the Switchboard corpus of English conversational dialogue, we find that when only transcripts are used, the turn-based parser performs considerably worse than the SU-based parser, which is not surprising given that it needs to perform two tasks instead of one. However, when prosodic features are included, there is no di"
2021.case-1.6,W12-3010,0,0.0515951,"Missing"
2021.case-1.6,P15-1034,0,0.0920519,"labelled, a number of small datasets exist for specific domains including biomedical text (Thompson et al., 2011), news (Thompson et al., 2017), reviews (Konstantinova et al., 2012), and web-crawled text comprising news, web pages, blogs and Wikipedia (Morante and Daelemans, 2012). 2.3 Event Extraction Since the introduction of the Open Information Extraction (OIE) task by Banko et al. (2007), a range of open-domain information extraction systems have been proposed for the extraction of relation tuples from text. OIE systems make use of patterns, which may be hand-crafted (Fader et al., 2011; Angeli et al., 2015) or learned through methods such as bootstrapping (Wu and Weld, 2010; Mausam et al., 2012). These patterns may be applied at the sentence level, or to semantically simplified independent clauses identified during a pre-processing step (Del Corro and Gemulla, 2013; Angeli et al., 2015). The majority of systems are restricted to the extraction of binary relations (i.e. relation triples consisting of a predicate and two arguments), but systems have also been proposed for the extraction of n-ary relations (Akbik and L¨oser, 2012; Mesquita et al., 2013). Our system is a form of n-ary event extracti"
2021.case-1.6,baker-etal-2010-modality,0,0.454563,"rain tomorrow.), deontic necessity (You must go.), and deontic possibility (You may enter.) (Van Der Auwera and Ammann, 2005). Sometimes a lexical trigger of modality is ambiguous between categories; English may, for example, is ambiguous between an epistemic possibility reading (It may rain tomorrow.) and a deontic possibility reading (You may enter.) These definitions have brought about a variety of annotation schemes in practice. Prabhakaran et al. (2012) propose five classes of modality: ability, effort, intention, success, and want, and train a classifier on crowd-sourced annotated data. Baker et al. (2010) extend the number of modality classes to include requirement, permission, and belief, and combine these with negation. Pe˜nas et al. (2011) take a coarser, epistemic approach, asking whether events are asserted, negated, or speculated, and Saurı et al. (2006) enrich the TimeML specification language with yet other categories (e.g. evidentiality, conditionality). In English, modality can be expressed in a vaCounterfactuality: In the counterfactual construction a more complicated semantic relation is established between antecedent and consequent, as in the example: Had they protested, they woul"
2021.case-1.6,Q18-1048,1,0.318409,"in 2 the 1 election Figure 2: CCG dependency graph for Johnson doubts that Labour will win the election; marked paths from doubts (blue, dotted) and will (orange, solid) to win. tion 6 for a comparison of our system with OpenIE and OLLIE.) We therefore construct our own event extraction system. Our system takes as input a text document, and for each sentence outputs a set of event relations. An event relation tuple consists of a predicate and either one, or two arguments (e.g. (The) protest-ended, Angela Merkel-addressed-NPD protesters). We use a pipeline approach similar to that described by Hosseini et al. (2018), which allows us to extract open-domain relations. Each sentence in the document is parsed using the RotatingCCG parser (Stanojevi´c and Steedman, 2019) over which we construct a CCG dependency graph using a method similar to the one proposed by Clark et al. (2002). (See Figure 2 for an example of a dependency graph and Figure 1 for the CCG parse tree from which it was extracted.) CCG dependency graphs are more expressive than standard dependency trees because they can encode long-range dependencies, coordination and reentrancies. We traverse the dependency graph, starting from verb and prepo"
2021.case-1.6,konstantinova-etal-2012-review,0,0.0898339,"Missing"
2021.case-1.6,P02-1042,1,0.448248,"n event extraction system. Our system takes as input a text document, and for each sentence outputs a set of event relations. An event relation tuple consists of a predicate and either one, or two arguments (e.g. (The) protest-ended, Angela Merkel-addressed-NPD protesters). We use a pipeline approach similar to that described by Hosseini et al. (2018), which allows us to extract open-domain relations. Each sentence in the document is parsed using the RotatingCCG parser (Stanojevi´c and Steedman, 2019) over which we construct a CCG dependency graph using a method similar to the one proposed by Clark et al. (2002). (See Figure 2 for an example of a dependency graph and Figure 1 for the CCG parse tree from which it was extracted.) CCG dependency graphs are more expressive than standard dependency trees because they can encode long-range dependencies, coordination and reentrancies. We traverse the dependency graph, starting from verb and preposition nodes, until an 34 Lemma Category succeed shall conceivably impossible as long as concede reckon MOD MOD MOD MOD COND ATT SAY ATT THINK POS-tag Strength VB MD RB JJ RB VB VB 4 3 2 0 2 4 2 Algorithm 1 Tagging Modal Events 1: procedure TAG M ODAL E VENTS(senten"
2021.case-1.6,N19-1423,0,0.00575395,"henomenon to handle in this domain. Future Work An obvious limitation of our approach is that it does not take into account the context in which events and trigger words occur. Modality is a contextdependent phenomenon, so using the sentential context would improve accuracy. For example, the word unbelievable is ambiguous between an unlikely and an amazing, and happened reading. Relatedly, our concept of epistemic strength is highly context-sensitive, and requires further development. A promising avenue is to develop a pre-training procedure for a modality-aware contextualised language model (Devlin et al., 2019; Zhou et al., 2020). We plan to use our modal lexicon to identify sentences with modality triggers. We will then gather human annotations of the certainty that each event happened, and use this annotated data to train a modality-aware language model able to classify event uncertainty. Such a system might eventually even tackle the long-tail of modal examples mentioned in Section 2.1. We will also investigate the application of zero shot and few shot learning to the problem of detecting modality and negation. This could provide a way to leverage a large pre-trained language model together with"
2021.case-1.6,D11-1142,0,0.387738,"n which modality is labelled, a number of small datasets exist for specific domains including biomedical text (Thompson et al., 2011), news (Thompson et al., 2017), reviews (Konstantinova et al., 2012), and web-crawled text comprising news, web pages, blogs and Wikipedia (Morante and Daelemans, 2012). 2.3 Event Extraction Since the introduction of the Open Information Extraction (OIE) task by Banko et al. (2007), a range of open-domain information extraction systems have been proposed for the extraction of relation tuples from text. OIE systems make use of patterns, which may be hand-crafted (Fader et al., 2011; Angeli et al., 2015) or learned through methods such as bootstrapping (Wu and Weld, 2010; Mausam et al., 2012). These patterns may be applied at the sentence level, or to semantically simplified independent clauses identified during a pre-processing step (Del Corro and Gemulla, 2013; Angeli et al., 2015). The majority of systems are restricted to the extraction of binary relations (i.e. relation triples consisting of a predicate and two arguments), but systems have also been proposed for the extraction of n-ary relations (Akbik and L¨oser, 2012; Mesquita et al., 2013). Our system is a form o"
2021.case-1.6,P14-5010,0,0.00594641,"e TAG M ODAL E VENTS(sentence s, events e, lexicon l) 2: G, event nodes ← CCG dep parse(s, e) 3: trigger nodes ← [ ] 4: for n in G do 5: if check lexicon(n,l) or check cf(n,G) then 6: trigger nodes.add(n) 7: end if 8: end for 9: for e n in event nodes do 10: for t n in trigger nodes do 11: if path between(e n, t n) then 12: e n ← update(e n,t n.tag) 13: end if 14: end for 15: e n.tag ← tag precedence(e n) 16: event nodes.update(e n) 17: end for 18: return event nodes 19: end procedure Table 2: Example lexicon entries Arguments are classified as either a Named Entity (extracted by the CoreNLP (Manning et al., 2014) Named Entity recogniser), or a general entity (all other nouns and noun phrases). Arguments are mapped to types by linking to their Freebase (Bollacker et al., 2008) IDs using AIDA-Light (Nguyen et al., 2014), and subsequently mapping these IDs to their fine-grained FIGER types (Ling and Weld, 2012). For example, Angela Merkel would be mapped to person/politician and NPD (Nationaldemokratische Partei Deutschland) to government/political party. The type system may be leveraged to identify events belonging to specific domains, for example, to identify and track political events such as election"
2021.case-1.6,2020.textgraphs-1.7,1,0.81718,"Missing"
2021.case-1.6,D12-1048,0,0.535919,"(Thompson et al., 2011), news (Thompson et al., 2017), reviews (Konstantinova et al., 2012), and web-crawled text comprising news, web pages, blogs and Wikipedia (Morante and Daelemans, 2012). 2.3 Event Extraction Since the introduction of the Open Information Extraction (OIE) task by Banko et al. (2007), a range of open-domain information extraction systems have been proposed for the extraction of relation tuples from text. OIE systems make use of patterns, which may be hand-crafted (Fader et al., 2011; Angeli et al., 2015) or learned through methods such as bootstrapping (Wu and Weld, 2010; Mausam et al., 2012). These patterns may be applied at the sentence level, or to semantically simplified independent clauses identified during a pre-processing step (Del Corro and Gemulla, 2013; Angeli et al., 2015). The majority of systems are restricted to the extraction of binary relations (i.e. relation triples consisting of a predicate and two arguments), but systems have also been proposed for the extraction of n-ary relations (Akbik and L¨oser, 2012; Mesquita et al., 2013). Our system is a form of n-ary event extraction; we extract both binary and unary relations, and relations of higher valencies can be i"
2021.case-1.6,J07-3004,1,0.657537,"1. We will also investigate the application of zero shot and few shot learning to the problem of detecting modality and negation. This could provide a way to leverage a large pre-trained language model together with a small annotated corpus. Our system was developed for English, but work is already underway to develop event extraction systems for other languages including German and Chinese. Extending to other languages would allow us to apply our methods to multilingual and crosslingual NLP tasks. Finally, most CCG parsers, including the one used in this work, are trained on English CCGbank (Hockenmaier and Steedman, 2007). This makes them perform well on news text, but accuracy suffers on out-of-domain sentences, primarily those involving questions. The results could be improved by retraining the parser on the CCG annotated questions dataset (Rimell and Clark, 2008; Yoshikawa et al., 2019), allowing us to apply our system to the task of open-domain Question Answering in an extrinsic evaluation. 10 Acknowledgments This work was funded by the ERC H2020 Advanced Fellowship GA 742137 SEMANTAX and a grant from The University of Edinburgh and Huawei Technologies. The authors would like to thank Mark Johnson, Ian Woo"
2021.case-1.6,2020.starsem-1.15,1,0.757305,"r further counterfactual patterns. We can then decide whether an event node should be tagged, by checking whether there is a path in the dependency graph from the trigger nodes to the event node (lines 9-12). Figure 2 illustrates the intuition behind walking the dependency graph. The graph shows a path from both doubt and will to win. This works because the existence of a path between a trigger node and an event node corresponds to the trigger node taking syntactic scope over the event node. The semantic phenomena we handle all rely heavily on this syntactic process (for example negation, see McKenna and Steedman (2020)). We highlight the capabilities of our system on five example sentences, comparing with two existing event extraction systems: OpenIE (Angeli et al., 2015) and OLLIE (Mausam et al., 2012). Note that this is not intended as a conclusive evaluation of systems, but rather as a high-level overview of the phenomena captured by each of the systems. See Table 3 for a comparison of the relations extracted by M O NTEE, OpenIE and OLLIE. The examples are all naturally occurring sentences from the news domain, obtained by a web search targeted to the modality categories discussed in this paper. To enabl"
2021.case-1.6,D08-1050,0,0.0683307,"Missing"
2021.case-1.6,D13-1043,0,0.0111618,"which may be hand-crafted (Fader et al., 2011; Angeli et al., 2015) or learned through methods such as bootstrapping (Wu and Weld, 2010; Mausam et al., 2012). These patterns may be applied at the sentence level, or to semantically simplified independent clauses identified during a pre-processing step (Del Corro and Gemulla, 2013; Angeli et al., 2015). The majority of systems are restricted to the extraction of binary relations (i.e. relation triples consisting of a predicate and two arguments), but systems have also been proposed for the extraction of n-ary relations (Akbik and L¨oser, 2012; Mesquita et al., 2013). Our system is a form of n-ary event extraction; we extract both binary and unary relations, and relations of higher valencies can be inferred by combining sets of binary relations. A comprehensive survey of OIE systems is provided by Niklaus et al. (2018). Modality Taggers and Annotated Datasets A number of approaches have been proposed for the automatic tagging of modality in text. These differ in both the granularity of the classes of modality that the model tags, and the model design. At the lowest granularity all modality classes are collapsed into a single label. This strategy was emplo"
2021.case-1.6,2007.sigdial-1.5,0,0.0486828,"eech (tag ATT SAY, e.g. say, state) and attitudes of thought (tag ATT THINK, e.g. suspect, assume). More phrases expressing uncertainty are found in a data set of news domain sentences describing conflicting events, such as a win and a loss (Guillou et al., 2020). Such sentences often contained descriptions of events that didn’t actually happen. Yet more related words were found by generating each entry’s WordNet synonyms and antonyms (Miller, 1995). We filtered and annotated these manually to obtain just under another 200 phrases, and added these to the lexicon. We also took inspiration from Somasundaran et al. (2007), especially for conditionals. In aggregate, this work resulted in a resource of 530 phrases. Lexicon Since many of the phenomena we capture involve lexical trigger items, we opt for a lexicon-based approach. Triggers identified using the lexicon can then be linked to event nodes in the CCG dependency graph. Entries in the lexicon cover modality, lexical negation, propositional attitude, and conditionality, with counterfactuality handled separately. Each entry contains the lemma, the categories that it covers, the POS-tag and an estimate of the epistemic strength that the word would normally i"
2021.case-1.6,N19-1020,1,0.621352,"Missing"
2021.case-1.6,C18-1326,0,0.0129377,"uses identified during a pre-processing step (Del Corro and Gemulla, 2013; Angeli et al., 2015). The majority of systems are restricted to the extraction of binary relations (i.e. relation triples consisting of a predicate and two arguments), but systems have also been proposed for the extraction of n-ary relations (Akbik and L¨oser, 2012; Mesquita et al., 2013). Our system is a form of n-ary event extraction; we extract both binary and unary relations, and relations of higher valencies can be inferred by combining sets of binary relations. A comprehensive survey of OIE systems is provided by Niklaus et al. (2018). Modality Taggers and Annotated Datasets A number of approaches have been proposed for the automatic tagging of modality in text. These differ in both the granularity of the classes of modality that the model tags, and the model design. At the lowest granularity all modality classes are collapsed into a single label. This strategy was employed in the pilot task on modality and negation detection at CLEF 2012, in which participants were asked to automatically label a set of events/states as negated, modal, neither, or both (Morante and Daelemans, 2012). The submitted systems were either purely"
2021.case-1.6,J12-2004,0,0.0755547,"Missing"
2021.case-1.6,C08-1107,0,0.0368103,"win the election doubts 1 Johnson argument node is reached. The traversed nodes, which are used to form the predicate strings, may include (non-auxiliary) verbs, verb particles, adjectives, and prepositions. The CCG argument slot position, corresponding to the grammatical case of the argument (e.g. 1 for nominative, 2 for accusative), is appended to the predicate. Our focus is on the extraction of binary and unary relations. Binary relations may be extracted from dependency paths between two entities. Extraction of unary relations, which have only one such endpoint, poses a harder challenge (Szpektor and Dagan, 2008) – we must decide whether they are truly a unary relation, or form part of a binary relation. Therefore linguistic knowledge must be carefully applied to extract meaningful unary relations. We extract unary relations for the following cases: verbs with a single argument including intransitives (bombs exploded) and passivised transitives (protests were held), and copular constructions (Greta Thunberg is a climate activist). In addition to binary and unary relations we also extract n-ary relations which combine two binary relations via prepositional attachment. These are of the form: arg1-predic"
2021.case-1.6,P10-1013,0,0.0129922,"ng biomedical text (Thompson et al., 2011), news (Thompson et al., 2017), reviews (Konstantinova et al., 2012), and web-crawled text comprising news, web pages, blogs and Wikipedia (Morante and Daelemans, 2012). 2.3 Event Extraction Since the introduction of the Open Information Extraction (OIE) task by Banko et al. (2007), a range of open-domain information extraction systems have been proposed for the extraction of relation tuples from text. OIE systems make use of patterns, which may be hand-crafted (Fader et al., 2011; Angeli et al., 2015) or learned through methods such as bootstrapping (Wu and Weld, 2010; Mausam et al., 2012). These patterns may be applied at the sentence level, or to semantically simplified independent clauses identified during a pre-processing step (Del Corro and Gemulla, 2013; Angeli et al., 2015). The majority of systems are restricted to the extraction of binary relations (i.e. relation triples consisting of a predicate and two arguments), but systems have also been proposed for the extraction of n-ary relations (Akbik and L¨oser, 2012; Mesquita et al., 2013). Our system is a form of n-ary event extraction; we extract both binary and unary relations, and relations of hig"
2021.case-1.6,P19-1013,0,0.0232063,"Missing"
2021.case-1.6,D13-1183,0,0.490404,"–6, 2021, ©2021 Association for Computational Linguistics Category ∅ Negation Lexical negation Modal operator Conditional Counterfactual Propositional attitude riety of ways. The modal auxiliaries (e.g. might, should, can) are commonly used, but modality can be lexicalised in many other trigger words. Nouns (e.g. possibility), adjectives (e.g. obligatory), adverbs (e.g. probably) and verbs (e.g. presume that) can all indicate modality. In the long tail, speakers have access to vastly productive phrases that indicate their attitude. The following examples occurred naturally in the news domain (Zhang and Weld, 2013): That’s how close they were to ..., I cannot come up with a scenario that has..., That’s based on the world wide assumption that.... Example Protesters attacked the police Protesters did not attack the police Protesters refrained from attacking the police Protesters may have attacked the police If protesters attack the police... Had protesters attacked the police... Journalists said that protesters attacked the police Table 1: Modality and negation categories 2 2.1 Background Conditionality: A conditional sentence is composed of a subordinate clause (which we will refer to as the antecedent)"
2021.case-1.6,2020.acl-main.678,0,0.0204411,"n this domain. Future Work An obvious limitation of our approach is that it does not take into account the context in which events and trigger words occur. Modality is a contextdependent phenomenon, so using the sentential context would improve accuracy. For example, the word unbelievable is ambiguous between an unlikely and an amazing, and happened reading. Relatedly, our concept of epistemic strength is highly context-sensitive, and requires further development. A promising avenue is to develop a pre-training procedure for a modality-aware contextualised language model (Devlin et al., 2019; Zhou et al., 2020). We plan to use our modal lexicon to identify sentences with modality triggers. We will then gather human annotations of the certainty that each event happened, and use this annotated data to train a modality-aware language model able to classify event uncertainty. Such a system might eventually even tackle the long-tail of modal examples mentioned in Section 2.1. We will also investigate the application of zero shot and few shot learning to the problem of detecting modality and negation. This could provide a way to leverage a large pre-trained language model together with a small annotated c"
2021.emnlp-main.840,J15-2003,0,0.35235,"Missing"
2021.emnlp-main.840,P10-1124,0,0.671559,"es. Previous models learn predicates of a single valency, the number and types of arguments controlled by the predicate. Commonly these are binary graphs, which cannot model single-argument predicates like the entity states “is dead” or “is an author.” This means they miss a variety of entailments in text that could be used to answer questions such as our example. The Distributional Inclusion Hypothesis (DIH) (Dagan et al., 1999; Kartsaklis and Sadrzadeh, 2016) is a theory which has been used effectively in unsupervised learning of these same-valency entailment graphs (Geffet and Dagan, 2005; Berant et al., 2010; Hosseini, 2021). In this work the DIH is reinterpreted in a way which supports learning entailments between predicates of different valencies such as KILL(Mustard, Boddy)  DIE(Boddy). We extend the work of Hosseini et al. (2018) and develop a new Multivalent Entailment Graph (MGraph) where vertices may be predicates of different valencies. This results in new kinds of entailments that answer a broader range of questions including entity state. We further pose a true-false question answering task generated automatically from news text. Our model draws inferences across propositions of differ"
2021.emnlp-main.840,2021.case-1.6,1,0.681848,"Missing"
2021.emnlp-main.840,N19-1300,0,0.0279143,"ents of unary predicates (UU). WIN . IN (:person, BE . WINNER (:person) 5 Evaluation: Question Answering We pose an automatically generated QA task to evaluate our model explicitly for directional inference between binary and unary predicates, as we are not aware of any standard datasets for this probBE . WINNER (:person1) lem. Our task is to answer true-false questions about real events that are discussed in the news, for example, “Was Biden elected?” These types of OBLITERATE(:person1, :person2) questions are surprisingly difficult and frequently Bivalent Graphs require inference to answer (Clark et al., 2019). For this, entailment is especially useful: we must decide if the question (hypothesis) is true given a list Univalent Graphs of propositions from limited news text (premises), which are all likely to be phrased differently. Person Graph Person Person Graph Graph This task is designed independently of the BE . WINNER (:person) MGraph as a challenge in information retrieval. Positive questions made from binary and unary BE . CHAMPION (:person) predicates are selected directly from the news text using special criteria, and are then removed. From Figure 2: Bivalent graphs model entailments from"
2021.emnlp-main.840,N19-1423,0,0.029097,"the kill/die example, which are easy for humans. We generalize the DIH to learn entailments within and across valencies. Typing is very helpful for entailment graph learning (Berant et al., 2010; Lewis and Steedman, 2013; Hosseini et al., 2018). Inducing a type for each entity such as “person,” “location,” etc. enables generalized learning across instances and disambiguates word sense, e.g. “running a company” has different entailments than “running code.” We compare our model to several baselines, including strong pretrained language models in an unsupervised setting using similarity. BERT (Devlin et al., 2019) generates impressive word representations, even unsupervised (Petroni et al., 2019), which we compare with on a task of predicate inference. We further test RoBERTa (Liu et al., 2019) to show the impact of robust in-domain pretraining on the same architecture. These non-directional similarity models provide a strong baseline for evaluating directional entailment graphs. 3 Multivalent Distributional Inclusion Hypothesis tailments about one or more of the arguments arise from their roles in this eventuality. We may infer that “Mr. Boddy died” due to his role as a direct object in the killing/mu"
2021.emnlp-main.840,Q18-1048,1,0.901754,"dead” or “is an author.” This means they miss a variety of entailments in text that could be used to answer questions such as our example. The Distributional Inclusion Hypothesis (DIH) (Dagan et al., 1999; Kartsaklis and Sadrzadeh, 2016) is a theory which has been used effectively in unsupervised learning of these same-valency entailment graphs (Geffet and Dagan, 2005; Berant et al., 2010; Hosseini, 2021). In this work the DIH is reinterpreted in a way which supports learning entailments between predicates of different valencies such as KILL(Mustard, Boddy)  DIE(Boddy). We extend the work of Hosseini et al. (2018) and develop a new Multivalent Entailment Graph (MGraph) where vertices may be predicates of different valencies. This results in new kinds of entailments that answer a broader range of questions including entity state. We further pose a true-false question answering task generated automatically from news text. Our model draws inferences across propositions of different valencies to answer more questions than using same-valence entailment graphs. We also compare with several baselines, including unsupervised pretrained language models, and show that our directional entailment graphs succeed ov"
2021.emnlp-main.840,P19-1468,1,0.693194,"2010; Hosseini et al., 2018). These graphs frequently rely on the DIH for the local learning step to learn initial predicate entailments. The DIH states that for some predicates p and q, if the contextual features of p are included in those of q, then p entails q (Geffet and Dagan, 2005). In previous work predicate arguments are successfully used as these contextual features, but only predicates of the same valency are considered (e.g. binary predicates entail binary; unary entail unary), and further research computes additional edges in these same-valency graphs such as with link prediction (Hosseini et al., 2019). However, this leaves out crucial inferences that cross valencies such as the kill/die example, which are easy for humans. We generalize the DIH to learn entailments within and across valencies. Typing is very helpful for entailment graph learning (Berant et al., 2010; Lewis and Steedman, 2013; Hosseini et al., 2018). Inducing a type for each entity such as “person,” “location,” etc. enables generalized learning across instances and disambiguates word sense, e.g. “running a company” has different entailments than “running code.” We compare our model to several baselines, including strong pret"
2021.emnlp-main.840,C16-1268,0,0.0596099,"Missing"
2021.emnlp-main.840,P16-2041,0,0.337834,"Missing"
2021.emnlp-main.840,Q13-1015,1,0.905296,"previous work predicate arguments are successfully used as these contextual features, but only predicates of the same valency are considered (e.g. binary predicates entail binary; unary entail unary), and further research computes additional edges in these same-valency graphs such as with link prediction (Hosseini et al., 2019). However, this leaves out crucial inferences that cross valencies such as the kill/die example, which are easy for humans. We generalize the DIH to learn entailments within and across valencies. Typing is very helpful for entailment graph learning (Berant et al., 2010; Lewis and Steedman, 2013; Hosseini et al., 2018). Inducing a type for each entity such as “person,” “location,” etc. enables generalized learning across instances and disambiguates word sense, e.g. “running a company” has different entailments than “running code.” We compare our model to several baselines, including strong pretrained language models in an unsupervised setting using similarity. BERT (Devlin et al., 2019) generates impressive word representations, even unsupervised (Petroni et al., 2019), which we compare with on a task of predicate inference. We further test RoBERTa (Liu et al., 2019) to show the impa"
2021.emnlp-main.840,P98-2127,0,0.623102,"he argument pair ai ∈ {(em , en ) |em ∈ Et1 , en ∈ Et2 }. Here t1 , t2 ∈ T , and Et is the subset of entities of type t. For example, the predicate BUILD(:company, :thing) might have some feature f37 , the PMI of “build” with argument pair (Apple, iPhone). A Balanced Inclusion (BInc) score is calculated for the directed entailment from one predicate to another (Szpektor and Dagan, 2008). BInc is the geometric mean of two subscores: a directional score, Weeds Precision (Weeds and Weir, 2003), measuring how much one vector’s features “cover” the other’s; and a symmetrical score, Lin Similarity (Lin, 1998), which downweights infrequent predicates that cause spurious false positives. In this work we compute local binary graphs following Hosseini et al. (2018) and leverage the new MDIH to compute additional entailments for unaries and between valencies. To do this we compute a vector for each argument slot respecting its position in the predicate. For a predicate p, a slot vector (s) v(s) for s ∈ {1, 2} consists of features fi . We define τ (p, s) = t, the type of slot s in predicate p. (s) Each fi is the PMI of p and the argument in slot (s) s, ai ∈ Et . Slot vectors are computed for the slot in"
2021.emnlp-main.840,P05-1045,0,0.0198899,"G argument position which corresponds to its case (e.g. 1 for nominative, 2 for accusative), is appended to the predicate. Passive predicates are mapped to active ones. Modifiers such as negation and predicates like “planned to” as in “Professor Plum planned to attend” are also extracted in the predicate. We pay special attention to copular constructions, which always introduce stative predicates, rather than events (Vendler, 1967). These are interesting for modeling the properties of entities. 4.2 Learning Local Graphs Identified by the CoreNLP Named Entitiy Recogniser (Manning et al., 2014; Finkel et al., 2005). Here we number the typed arguments for demonstration to show which :person argument is in the entailment. 10761 includes Bivalent Graphs which contain the entailments of binary predicates (BB and BU edges), and separate Univalent Graphs which contain the entailments of unary predicates (only UU edges, since we do not allow a unary to entail a binary). We follow previous research and learn separate disjoint subgraphs for each typing, up to |T |2 bivalent and |T |univalent subgraphs given enough data. For example, we learn a bivalent (:person, :location) graph containing binary predicates such"
2021.emnlp-main.840,P05-1014,0,0.474229,"resented as directed edges. Previous models learn predicates of a single valency, the number and types of arguments controlled by the predicate. Commonly these are binary graphs, which cannot model single-argument predicates like the entity states “is dead” or “is an author.” This means they miss a variety of entailments in text that could be used to answer questions such as our example. The Distributional Inclusion Hypothesis (DIH) (Dagan et al., 1999; Kartsaklis and Sadrzadeh, 2016) is a theory which has been used effectively in unsupervised learning of these same-valency entailment graphs (Geffet and Dagan, 2005; Berant et al., 2010; Hosseini, 2021). In this work the DIH is reinterpreted in a way which supports learning entailments between predicates of different valencies such as KILL(Mustard, Boddy)  DIE(Boddy). We extend the work of Hosseini et al. (2018) and develop a new Multivalent Entailment Graph (MGraph) where vertices may be predicates of different valencies. This results in new kinds of entailments that answer a broader range of questions including entity state. We further pose a true-false question answering task generated automatically from news text. Our model draws inferences across p"
2021.emnlp-main.840,P14-5010,0,0.00376382,"s are stripped. The CCG argument position which corresponds to its case (e.g. 1 for nominative, 2 for accusative), is appended to the predicate. Passive predicates are mapped to active ones. Modifiers such as negation and predicates like “planned to” as in “Professor Plum planned to attend” are also extracted in the predicate. We pay special attention to copular constructions, which always introduce stative predicates, rather than events (Vendler, 1967). These are interesting for modeling the properties of entities. 4.2 Learning Local Graphs Identified by the CoreNLP Named Entitiy Recogniser (Manning et al., 2014; Finkel et al., 2005). Here we number the typed arguments for demonstration to show which :person argument is in the entailment. 10761 includes Bivalent Graphs which contain the entailments of binary predicates (BB and BU edges), and separate Univalent Graphs which contain the entailments of unary predicates (only UU edges, since we do not allow a unary to entail a binary). We follow previous research and learn separate disjoint subgraphs for each typing, up to |T |2 bivalent and |T |univalent subgraphs given enough data. For example, we learn a bivalent (:person, :location) graph containing"
2021.emnlp-main.840,P15-2070,0,0.0732372,"Missing"
2021.emnlp-main.840,D19-1250,0,0.0350132,"tailments within and across valencies. Typing is very helpful for entailment graph learning (Berant et al., 2010; Lewis and Steedman, 2013; Hosseini et al., 2018). Inducing a type for each entity such as “person,” “location,” etc. enables generalized learning across instances and disambiguates word sense, e.g. “running a company” has different entailments than “running code.” We compare our model to several baselines, including strong pretrained language models in an unsupervised setting using similarity. BERT (Devlin et al., 2019) generates impressive word representations, even unsupervised (Petroni et al., 2019), which we compare with on a task of predicate inference. We further test RoBERTa (Liu et al., 2019) to show the impact of robust in-domain pretraining on the same architecture. These non-directional similarity models provide a strong baseline for evaluating directional entailment graphs. 3 Multivalent Distributional Inclusion Hypothesis tailments about one or more of the arguments arise from their roles in this eventuality. We may infer that “Mr. Boddy died” due to his role as a direct object in the killing/murdering event. No other information is needed, including who murdered Mr. Boddy, whe"
2021.emnlp-main.840,D09-1001,0,0.0603039,"d the news text within this 3-day window is used as evidence to answer them. We ask questions as if happening presently in this time window to control for the variable of time, so we can ask ambiguous questions like “Did the Patriots win the Superbowl?” which may be “true” or not depending on the date and timespan. The small 3-day window size was chosen so multiple news stories about an event appear together, increasing the chances of finding question answers. Within each partition we do relation extraction in a process mirroring §4.1. 2. Selecting Positives. We adapt a selection process from Poon and Domingos (2009) to choose good questions which are interesting to a human and answerable from the partition text. First, we identify repeated entities that star in the events of the articles; these will yield interesting questions as well as ample textual evidence for answering them. In each partition we count the mentions of each entity pair (from binary propositions) and single 5.2 Question Answering Models entities (from unary and binary ones). The most frequent entities and pairs mentioned more than In each partition, models receive factual proposi5 times in the partition are selected. Predicates tions e"
2021.emnlp-main.840,N19-1020,1,0.858231,"Missing"
2021.emnlp-main.840,C08-1107,0,0.822054,"reading T would infer that H is most likely true.” From here, research has moved in ∗ Now at Google Research. 1 several directions. We study predicates, including The murder mystery board game Clue (also known as Cluedo) lends inspiration to this project. verbs and phrases that apply to arguments. 10758 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10758–10768 c November 7–11, 2021. 2021 Association for Computational Linguistics Research in predicate entailment graphs has evolved from “local” learning of entailment rules (Geffet and Dagan, 2005; Szpektor and Dagan, 2008) to later work on joint learning of “globalized” rules, overcoming sparsity in local graphs (Berant et al., 2010; Hosseini et al., 2018). These graphs frequently rely on the DIH for the local learning step to learn initial predicate entailments. The DIH states that for some predicates p and q, if the contextual features of p are included in those of q, then p entails q (Geffet and Dagan, 2005). In previous work predicate arguments are successfully used as these contextual features, but only predicates of the same valency are considered (e.g. binary predicates entail binary; unary entail unary)"
2021.emnlp-main.840,W03-1011,0,0.47742,"predicate p with corresponding vector v, v consists of features fi which are the pointwise mutual information (PMI) of p and the argument pair ai ∈ {(em , en ) |em ∈ Et1 , en ∈ Et2 }. Here t1 , t2 ∈ T , and Et is the subset of entities of type t. For example, the predicate BUILD(:company, :thing) might have some feature f37 , the PMI of “build” with argument pair (Apple, iPhone). A Balanced Inclusion (BInc) score is calculated for the directed entailment from one predicate to another (Szpektor and Dagan, 2008). BInc is the geometric mean of two subscores: a directional score, Weeds Precision (Weeds and Weir, 2003), measuring how much one vector’s features “cover” the other’s; and a symmetrical score, Lin Similarity (Lin, 1998), which downweights infrequent predicates that cause spurious false positives. In this work we compute local binary graphs following Hosseini et al. (2018) and leverage the new MDIH to compute additional entailments for unaries and between valencies. To do this we compute a vector for each argument slot respecting its position in the predicate. For a predicate p, a slot vector (s) v(s) for s ∈ {1, 2} consists of features fi . We define τ (p, s) = t, the type of slot s in predicate"
2021.emnlp-main.840,D13-1183,0,0.470293,"that the former entails being an author, while the latter entails being a programmer. 4 binary is a window into its higher-valency predicate, allowing higher-valency predicates to entail lower binaries and unaries. To learn these new kinds of connections we develop a method of local entailment rule learning using the MDIH. As in §2, the local step learns the initial directed edges of the entailment graph, which are further improved with global learning. Our local step learns entailments by machine-reading the NewsSpike corpus (2.3GB), which contains 550K news articles, or over 20M sentences (Zhang and Weld, 2013). NewsSpike consists of multi-source news articles collected within a fixed timeframe, and due to these properties the articles frequently discuss the same events but phrased in different ways, providing appropriate training evidence. Learning Multivalent Graphs We define an Entailment Graph as a directed graph of predicates and their entailments, G = (V, E). The vertices V are the set of predicates, where each argument has a type from the set of 49 FIGER base types T , e.g. TRAVEL . TO(:person, :location) ∈ V , and :person, :location ∈ T . The directed edges are E = {(v1 , v2 ) |v1 , v2 ∈ V i"
2021.emnlp-main.87,2021.emnlp-main.468,0,0.0383686,"uage Modelling, XDM - Cross-lingual Dialogue Modelling, RM - Response Masking. Italics is the response in the given chat. Both XDM and RM are new designs for intermediate tasks, tailored for cross-lingual dialogue tasks. We also experimented with combining monolingual and cross-lingual objectives but our pilot experiments did not show any considerable improvement over the individual objectives. For tasks where combining multiple objectives has worked, those tasks required higher reasoning and inference capabilities like coreference resolution or question answering (Pruksachatkun et al., 2020; Aghajanyan et al., 2021). Such highly specific task data is not available for all languages and even further limited for conversational tasks. We will explore this direction in future. Similarly, our initial experiments suggested that simply combining data from multiple languages for a multilingual intermediate task has lower performance than individual crosslingual intermediate tasks. Thus, designing multilingual intermediate tasks is far from trivial and we will also explore this in future. Multilingual WoZ dataset (Mrkši´c et al., 2017b). As the datasets vary in difficulty and languages, we choose a different amou"
2021.emnlp-main.87,Q19-1038,0,0.0224936,"h limited labelled data in the target language for the MultiWoZ dataset over the baseline. 3. We propose two new intermediate tasks: Crosslingual dialogue modelling (XDM) and Response masking (RM) that can be extended to other crosslingual dialogue tasks. 2 Related Work Intermediate fine-tuning of large language models: Training deep neural networks on large unlabelled text data to learn meaningful representations has shown remarkable success on several downstream tasks. These representations can be monolingual (Qiu et al., 2020) or multilingual (Devlin et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019) depending on the underlying training data. These representations are 1 further refined to suit the downstream task by fineOur code is available at https://github.com/ nikitacs16/xlift_dst tuning the pretrained model on related data and/or 1138 tasks. This “intermediate” fine-tuning is done before fine-tuning the task-specific architecture on the downstream task. In adaptive intermediate fine-tuning, a pretrained model is fine-tuned with the same objectives used during pretraining on data that is closer to the distribution of the target task. This is referred to as task adaptive pretraining (T"
2021.emnlp-main.87,2020.lrec-1.53,0,0.108542,"all the submissions in the shared task used the translated version of the dataset and treated the problem as a monolingual dialogue state tracking setup. We use the Multilingual WoZ dataset and the parallel MultiWoZ dataset to demonstrate the effectiveA popular benchmark for cross-lingual dialogue ness of our methods. As there are no existing bench1139 marks for cross-lingual dialogue state tracking for the parallel MultiWoZ dataset, we use the slotutterance matching belief tracker (SUMBT) (Lee et al., 2019) as our baseline, which was the state-ofthe-art for the English MultiWoZ 2.1 dataset (Eric et al., 2020). The SUMBT model uses BERT encoder to obtain contextual semantic vectors for the utterances, slot-names, and slot values. It then uses a multi-head attention network to learn the relationship between slot-names and slot-values appearing in the text to predict the dialogue states. Monolingual dialogue modelling (MonoDM): Dialogue history is an important component of any dialogue task. We select K continuous subtitles from the monolingual subtitles data where K is chosen randomly between 2 to 15 for every example. By choosing a random K, we ensure that the examples contain varied length dialogu"
2021.emnlp-main.87,2021.eacl-main.270,0,0.0671976,"Missing"
2021.emnlp-main.87,2020.acl-main.740,0,0.032183,"Missing"
2021.emnlp-main.87,D19-1433,0,0.0493058,"Missing"
2021.emnlp-main.87,P18-1031,0,0.0645101,"Missing"
2021.emnlp-main.87,P19-1546,0,0.0278763,"Missing"
2021.emnlp-main.87,L16-1147,0,0.478897,"termediate fine-tuning also becomes an important addition in the intermediate fine-tuning literature which has largely focused on related monolingual tasks. Our best method leads to an impressive performance on the standard benchmark of the Multilingual WoZ 2.0 dataset (Mrkši´c et al., 2017b) and the recently released parallel MultiWoZ 2.1 dataset (Gunasekara et al., 2020). It uses dialogue history and parallel conversational context confirming that our design principles based on conversation history and cross-lingual conversations are important. Our methods use 200K parallel movie subtitles (Lison and Tiedemann, 2016) for intermediate training and this data is already available for 1782 language pairs allowing extension to new language pairs. 1 Our contributions can be summarized as follows: 1. To the best of our knowledge, this is the first work to use parallel data for intermediate finetuning of multilingual models for multilingual dialogue tasks. We provide strong empirical evidence on four language directions in two datasets for lowresource and zero-shot data scenarios. 2. Our proposed intermediate fine-tuning techniques produce data-efficient target language dialogue state trackers. We achieve state-o"
2021.emnlp-main.87,P19-1373,0,0.0517639,"Missing"
2021.emnlp-main.87,P17-1163,0,0.0487528,"Missing"
2021.emnlp-main.87,Q17-1022,0,0.0606135,"Missing"
2021.emnlp-main.87,2020.aacl-main.56,0,0.0327191,"-lingual intermediate fine-tuning of pretrained multilingual language models for the task of cross-lingual dialogue state tracking. We experimented with existing intermediate tasks and introduced two new cross-lingual intermediate tasks based on the parallel and dialogue-level nature of the movie subtitles corpus. Our best method had significant improvement in performance for the parallel MultiWoZ dataset and Multilingual WoZ dataset. We also demonstrated the data efficiency of our methods. Our intermediate tasks were trained on a generic dataset unlike the related high resource tasks used in Phang et al. (2020). As OpenSubtitles is available for 1782 language pairs, we speculate that using these cross-lingual intermediate tasks will be effective for languages where a collection of large training datasets for dialogue tasks is not feasible. We speculate that this setup can be useful for crosslingual domain transfer too - when such benchmark becomes available for dialogue tasks. We hope that our method can serve as a strong baseline for future work in multilingual dialogue. We emphasized using dialogue history nformation while designing intermediate tasks. For ablation Acknowledgements studies, we fin"
2021.emnlp-main.87,2020.acl-main.467,0,0.061459,"Missing"
2021.emnlp-main.87,D18-1299,0,0.0376903,"Missing"
2021.emnlp-main.87,N19-1380,0,0.0467241,"Missing"
2021.emnlp-main.87,E17-1042,0,0.0741207,"Missing"
2021.emnlp-main.87,2020.tacl-1.19,0,0.042181,"Missing"
2021.emnlp-main.87,2020.acl-demos.19,0,0.0354218,"Missing"
2021.findings-emnlp.238,D19-1522,0,0.13997,"ph because the corresponding triple was not found in the text (Hosseini et al., 2019; Broscheit et al., 2020). Figure 1a shows part of an example open-domain KG, in which the triple (Apple, own, Beats) is missing, but can be inferred using link prediction over all entities in the complete KG. 2790 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2790–2802 November 7–11, 2021. ©2021 Association for Computational Linguistics 's pu Previous work has applied standard link prediction methods such as TransE (Bordes et al., 2013), ConvE (Dettmers et al., 2018), or TuckER (Balazevic et al., 2019) to open-domain triples. These methods have been shown to be effective in learning the KG structure, but they are sub-optimal for open-domain link prediction because they ignore the textual context of the triples. Since the triples are extracted from text, they can be automatically grounded back to their contexts. Hence, in addition to the KG structure, the triple contexts can be used as input to the link prediction task. Figure 1b shows the context sentences that have given rise to the partial KG in Figure 1a.1 There are multiple clues in the contexts such as deal, $, cash, stock, and Financi"
2021.findings-emnlp.238,J15-2003,0,0.0282748,"Missing"
2021.findings-emnlp.238,P10-1124,0,0.0462528,"dd missing relations to the KG (e.g., own in in Figure 1a) by predicting the relations that hold between the entities of triple mentions in context (e.g., the context c1 in Figure 1b). Our experiments show that the proposed model for the contextual link prediction task significantly outperforms standard link prediction in open-domain KG completion. In addition, we investigate the interplay between contextual link prediction and context-independent entailments between relations, in the form of entailment graphs (EG). An EG has typed relations as nodes and entailment relation as directed edges (Berant et al., 2010, 2011, 2015; Hosseini et al., 2018; Hosseini, 2021). The type of each relation is determined by the types of its two entities. EGs are by definition context-independent, but they use relation types as a proxy of the context. Figure 1c shows a fragment of an EG showing that for example acquire entails own. Similar to opendomain KGs, EGs are constructed based on extracted triples from text. The entailment between two relations is predicted by computing a directional entailment score between them. It has been recently shown that the two tasks of open-domain link prediction and EG learning are co"
2021.findings-emnlp.238,P11-1062,0,0.0326858,"r training, but has two main differences: First, our contextual link prediction model outputs a directional score between relations in context (e.g., acquire in Figure 1) and hypothesis relations (e.g., own), while MTB learns a symmetric similarity score. Second, we can predict a score for any hypothesis relation as long as the relation is previously observed somewhere in the corpus with any other entities (§3.2). Relational Entailment Graphs. Earlier attempts take a local approach and predict entailment relations independently from each other (Lin and Pantel, 2001; Szpektor and Dagan, 2008). Berant et al. (2011, 2015) and Hosseini et al. (2018) propose a global approach where the dependencies between the entailment relations are taken into account. They first build a local typed EG for any plausible type pair. They then build global EGs that satisfy soft or hard constraints such as the transitivity of entailment. The constraints consider the structures both across typed EGs and inside each graph. In this work, we improve the local entailment scores, which in turn improves the global EGs. Hosseini et al. (2019) perform standard link prediction to add more coverage to the EGs by adding 3 Contextual Li"
2021.findings-emnlp.238,D19-1651,0,0.0538611,"Missing"
2021.findings-emnlp.238,2020.acl-main.209,0,0.0364414,"knowledge graph (KG) is constituted by a set of (subject, relation, object) triples such as (Apple, acquire, Beats). KGs have entities (subjects and objects) as nodes and relations as labeled edges. Manually-built KGs such as Freebase (Bollacker et al., 2008), Wikidata (Vrandeˇci´c and Krötzsch, 2014), or DBPedia (Lehmann et al., 2015) have a known set of hand-built relations. In contrast, the relation-labels of open-domain KGs are obtained from text rather than fixed. Open-domain KGs can be constructed by applying parsers or openinformation extraction methods to text (Hosseini et al., 2019; Broscheit et al., 2020). ∗ (a) Figure 1: a) Part of an example KG. The relation own is missing, but can be predicted from the rest of the KG and the triple contexts using contextual link prediction. b) The contexts c1 and c2 from which we have extracted the KG triples. The relation tokens are boldfaced and entities are italic. The contextual link prediction task predicts relations that hold between the entitypair in a grounded triple. For example, we predict that the relation own should be added between Apple and Beats. c) An example EG of type Organization, Organization. The contextual link prediction and EG learni"
2021.findings-emnlp.238,N19-1423,0,0.0397277,"is because these clues could have been seen around occurrences of other entity-pairs of the same type (e.g., Facebook and Whatsapp) that are connected by acquire, ’s purchase of, and own relations. In this paper, we propose the new task of contextual link prediction for such open-domain graphs: Given a triple (e1 , r, e2 ) grounded in context with the relation r holding between the entities e1 and e2 , our goal is to predict all the other relations that hold between the two entities. We present a model that uses contextualized relation embeddings to predict new relations. We start with BERT (Devlin et al., 2019) pre-trained embeddings and fine-tune them with a novel unsupervised contextual link prediction objective function. After training the contextual link prediction model, we can add missing relations to the KG (e.g., own in in Figure 1a) by predicting the relations that hold between the entities of triple mentions in context (e.g., the context c1 in Figure 1b). Our experiments show that the proposed model for the contextual link prediction task significantly outperforms standard link prediction in open-domain KG completion. In addition, we investigate the interplay between contextual link predic"
2021.findings-emnlp.238,2020.textgraphs-1.7,1,0.829589,"Missing"
2021.findings-emnlp.238,2021.eacl-main.316,0,0.0236904,"Missing"
2021.findings-emnlp.238,P19-1468,1,0.820203,"arch. headphone Apple A knowledge graph (KG) is constituted by a set of (subject, relation, object) triples such as (Apple, acquire, Beats). KGs have entities (subjects and objects) as nodes and relations as labeled edges. Manually-built KGs such as Freebase (Bollacker et al., 2008), Wikidata (Vrandeˇci´c and Krötzsch, 2014), or DBPedia (Lehmann et al., 2015) have a known set of hand-built relations. In contrast, the relation-labels of open-domain KGs are obtained from text rather than fixed. Open-domain KGs can be constructed by applying parsers or openinformation extraction methods to text (Hosseini et al., 2019; Broscheit et al., 2020). ∗ (a) Figure 1: a) Part of an example KG. The relation own is missing, but can be predicted from the rest of the KG and the triple contexts using contextual link prediction. b) The contexts c1 and c2 from which we have extracted the KG triples. The relation tokens are boldfaced and entities are italic. The contextual link prediction task predicts relations that hold between the entitypair in a grounded triple. For example, we predict that the relation own should be added between Apple and Beats. c) An example EG of type Organization, Organization. The contextual link"
2021.findings-emnlp.238,2020.tacl-1.28,0,0.0471479,"kens, whereas we use with unequal types, we do not need the flag as the the natural text associated with the triples. order is obvious and set o(r) = 0. For relations Extracting Factual Knowledge from Prewith identical types, we set o(r) = 0 if the entities Trained Language Models. These works form are in the original order and o(r) = 1, otherwise. a prompt where an entity is missing (e.g., Apple A triple mention is a triple grounded in its texacquire [MASK] ), and ask the language models tual context. We define a triple mention as a tuto predict the masked entity (Petroni et al., 2019, 2020; Jiang et al., 2020; Bouraoui et al., 2020; Ha- 3 For brevity, we drop the types when they are obvious. 2792 Model ple m = (e1 , r, e2 , c, s), where r ∈ R is a relation and e1 , e2 ∈ E are entities. The sub-word token sequence c = [c0 , . . . , cn ] is the textual context of the triple including the surface form of the relation and entity-pair.4 The pair s = (s1 , s2 ) indicates the indices of the first and last relation tokens. An example triple mention in Figure 1b is (Apple,acquire,Beats,c   2 ,[9, 11]). We denote by D= (ei,1 , ri , ei,2 , ci , si ) i∈{1,...,N } the set of all triple mentions. We define th"
2021.findings-emnlp.238,W19-4002,0,0.0628762,"Missing"
2021.findings-emnlp.238,2021.eacl-main.108,0,0.0439871,"Missing"
2021.findings-emnlp.238,P16-2041,0,0.0517452,"Missing"
2021.findings-emnlp.238,P19-1279,0,0.0277664,"es as well as predicted ones from contextual link prediction. We build state-of-the-art EGs. • We show that EGs in turn improve contextual link prediction. • We release a dataset containing the extracted triples grounded in context, for future research. 2791 Our code and data are available at https://github. com/mjhosseini/open_contextual_link_ pred. 2 Related Work viv et al., 2021). These models do not probe for relations because a) They face technical challenges in processing multi-token relations; and b) Relations can be expressed in many different ways. The matching-the-blank (MTB) model (Soares et al., 2019) learns relation embeddings by encouraging relations that share the same entity-pairs to have similar embeddings. This is similar to our training, but has two main differences: First, our contextual link prediction model outputs a directional score between relations in context (e.g., acquire in Figure 1) and hypothesis relations (e.g., own), while MTB learns a symmetric similarity score. Second, we can predict a score for any hypothesis relation as long as the relation is previously observed somewhere in the corpus with any other entities (§3.2). Relational Entailment Graphs. Earlier attempts"
2021.findings-emnlp.238,N15-1098,0,0.0298638,"upervised R(t1 , t2 ) as the set of relations with types t1 , t2 , model that fine-tunes pre-trained LMs directly on a or t2 , t1 . For example, R(Person, Location) intraining portion of entailment datasets. They report better results than EGs, but our focus is different. cludes born in(Person,Location), birthplace of (Location,Person), etc. Similarly, we define R(e1 , e2 ) Unlike their method, our approach is unsupervised as the set of relations r ∈ R such that (e1 , r, e2 ) is and is not capable of learning potential artifacts a valid (extracted) triple. For example, R(Barack from datasets (Levy et al., 2015). In addition, we Obama, Hawaii) includes born in3 , visit, etc. explicitly build EGs by doing machine-reading over Link prediction and entailment can hold between large text corpora, and hence can explain the basis relations with the same entity order or the reverse for the beliefs captured in them. order. When the two entity types are identical, we Pre-trained LMs for Link Prediction. KGkeep two copies of the relations one for each entity BERT (Yao et al., 2019) uses contextual represenorder. For example, acquire(Org1 ,Org2 ) predicts tations for KG completion. However, they form be part of"
2021.findings-emnlp.238,P98-2127,0,0.0947191,"Missing"
2021.findings-emnlp.238,2021.ccl-1.108,0,0.0396328,"Missing"
2021.findings-emnlp.238,2021.emnlp-main.840,1,0.762734,"Missing"
2021.findings-emnlp.238,C08-1107,0,0.294776,"ings. This is similar to our training, but has two main differences: First, our contextual link prediction model outputs a directional score between relations in context (e.g., acquire in Figure 1) and hypothesis relations (e.g., own), while MTB learns a symmetric similarity score. Second, we can predict a score for any hypothesis relation as long as the relation is previously observed somewhere in the corpus with any other entities (§3.2). Relational Entailment Graphs. Earlier attempts take a local approach and predict entailment relations independently from each other (Lin and Pantel, 2001; Szpektor and Dagan, 2008). Berant et al. (2011, 2015) and Hosseini et al. (2018) propose a global approach where the dependencies between the entailment relations are taken into account. They first build a local typed EG for any plausible type pair. They then build global EGs that satisfy soft or hard constraints such as the transitivity of entailment. The constraints consider the structures both across typed EGs and inside each graph. In this work, we improve the local entailment scores, which in turn improves the global EGs. Hosseini et al. (2019) perform standard link prediction to add more coverage to the EGs by a"
2021.findings-emnlp.238,D13-1183,0,0.0319067,"Missing"
2021.findings-emnlp.238,D19-1250,0,0.0573115,"Missing"
2021.insights-1.16,J15-2003,0,0.0551277,"Missing"
2021.insights-1.16,P11-1062,0,0.0408107,"lity-unaware: the model learns from both asserted and modal predications. Our contributions are 1) a comparison of Entailment Graphs learned from modal and nonmodal data, showing (counterintuitively) that ignoring modal distinctions in fact improves Entailment Graph-learning, and 2) insights as to whether this effect applies uniformly across different subdomains. 2 Background Entailment rules specify directional inferences between linguistic predicates (Szpektor and Dagan, 2008), and can be stored in an Entailment Graph, whose global structural properties can be used to learn more accurately (Berant et al., 2011, 2015). * Equal contribution They are defined as a directed graph G = {N, E}, 1 Assuming a democratic election. We use the typical defin which the nodes N are typed predicates and inition of the premise most likely entailing the hypothesis (Dagan et al., 2006) edges E represent the entailment relation. The lex110 Proceedings of the Second Workshop on Insights from Negative Results in NLP, pages 110–116 November 10, 2021. ©2021 Association for Computational Linguistics ISBN 978-1-954085-93-0 Category ∅ Modal operator Conditional Counterfactual Propositional attitude Example Protesters attacked"
2021.insights-1.16,2021.case-1.6,1,0.845788,"Missing"
2021.insights-1.16,S14-1009,0,0.0215041,"2016; Adel and Schütze, 2017). Recently, Bijl de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs. ical entailment knowledge stored within them is useful for Question Answering (McKenna et al., 2021), as well as other tasks such as email categorisation (Eichler et al., 2014), relation extraction (Eichler et al., 2017) and link prediction (Hosseini et al., 2019). A subgraph containing predicates of a type-pair (e.g. PERSON-LOCATION) can be learned in an unsupervised way from collections of multiplysourced text. A vector of argument-pair counts for every predicate is first machine read from the 3 Methods corpus. Typically, relation extraction systems used for reading these corpora ignore modal modifiers, We extend relation extraction to pay attention to possibly introducing noise in the graph. Next, a modality, so that we can distinguish modal and non(directed) sim"
2021.insights-1.16,S17-1026,0,0.0239934,"l de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs. ical entailment knowledge stored within them is useful for Question Answering (McKenna et al., 2021), as well as other tasks such as email categorisation (Eichler et al., 2014), relation extraction (Eichler et al., 2017) and link prediction (Hosseini et al., 2019). A subgraph containing predicates of a type-pair (e.g. PERSON-LOCATION) can be learned in an unsupervised way from collections of multiplysourced text. A vector of argument-pair counts for every predicate is first machine read from the 3 Methods corpus. Typically, relation extraction systems used for reading these corpora ignore modal modifiers, We extend relation extraction to pay attention to possibly introducing noise in the graph. Next, a modality, so that we can distinguish modal and non(directed) similarity score (e.g. DIRT (Lin and Pan- modal"
2021.insights-1.16,Q18-1048,1,0.686663,"scriptions of eventualities in the news, observing directional co-occurrences of typed predicates and their arguments. For example, we expect to observe all the arguments of being president, such as Biden and Obama, also to be encountered in a sufficiently large multiply-sourced body of text as arguments of running for president, but not the other way around (Hillary Clinton will run but not be president). However, if all the reports of Clinton might be president are extracted as be_president(Clinton), one might expect the learning signal to be confusing to the algorithm. We use the method of Hosseini et al. (2018) combined with a modality parser (Bijl de Vroe et al., 2021) to construct typed Entailment Graphs from raw text corpora under two different settings. Modality-aware: modal predications are removed from the data entirely, and modality-unaware: the model learns from both asserted and modal predications. Our contributions are 1) a comparison of Entailment Graphs learned from modal and nonmodal data, showing (counterintuitively) that ignoring modal distinctions in fact improves Entailment Graph-learning, and 2) insights as to whether this effect applies uniformly across different subdomains. 2 Bac"
2021.insights-1.16,P19-1468,1,0.825632,"imilar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs. ical entailment knowledge stored within them is useful for Question Answering (McKenna et al., 2021), as well as other tasks such as email categorisation (Eichler et al., 2014), relation extraction (Eichler et al., 2017) and link prediction (Hosseini et al., 2019). A subgraph containing predicates of a type-pair (e.g. PERSON-LOCATION) can be learned in an unsupervised way from collections of multiplysourced text. A vector of argument-pair counts for every predicate is first machine read from the 3 Methods corpus. Typically, relation extraction systems used for reading these corpora ignore modal modifiers, We extend relation extraction to pay attention to possibly introducing noise in the graph. Next, a modality, so that we can distinguish modal and non(directed) similarity score (e.g. DIRT (Lin and Pan- modal relations in the Entailment Graph mining te"
2021.insights-1.16,W08-0607,0,0.0666705,"NLP, pages 110–116 November 10, 2021. ©2021 Association for Computational Linguistics ISBN 978-1-954085-93-0 Category ∅ Modal operator Conditional Counterfactual Propositional attitude Example Protesters attacked the police Protesters may have attacked the police If protesters attack the police... Had protesters attacked the police... Journalists said that protesters attacked the police Table 1: Modality categories Answering and Knowledge Base Population (Karttunen and Zaenen, 2005; Morante and Daelemans, 2012b). Early approaches to detecting modality focused on lexicon design (Szarvas, 2008; Kilicoglu and Bergler, 2008; Baker et al., 2010), with later approaches using machine learning over annotated corpora (Morante and Daelemans, 2009; Rei and Briscoe, 2010; Jean et al., 2016; Adel and Schütze, 2017). Recently, Bijl de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs"
2021.insights-1.16,P16-2041,0,0.0303899,"Missing"
2021.insights-1.16,2021.emnlp-main.840,1,0.759046,"Missing"
2021.insights-1.16,P07-1125,0,0.0738546,"Missing"
2021.insights-1.16,W09-1304,0,0.0511293,"Modal operator Conditional Counterfactual Propositional attitude Example Protesters attacked the police Protesters may have attacked the police If protesters attack the police... Had protesters attacked the police... Journalists said that protesters attacked the police Table 1: Modality categories Answering and Knowledge Base Population (Karttunen and Zaenen, 2005; Morante and Daelemans, 2012b). Early approaches to detecting modality focused on lexicon design (Szarvas, 2008; Kilicoglu and Bergler, 2008; Baker et al., 2010), with later approaches using machine learning over annotated corpora (Morante and Daelemans, 2009; Rei and Briscoe, 2010; Jean et al., 2016; Adel and Schütze, 2017). Recently, Bijl de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs. ical entailment knowledge stored within them is useful for Question Answering (McKenna et al., 2021), as well as othe"
2021.insights-1.16,W10-3008,0,0.0218177,"ounterfactual Propositional attitude Example Protesters attacked the police Protesters may have attacked the police If protesters attack the police... Had protesters attacked the police... Journalists said that protesters attacked the police Table 1: Modality categories Answering and Knowledge Base Population (Karttunen and Zaenen, 2005; Morante and Daelemans, 2012b). Early approaches to detecting modality focused on lexicon design (Szarvas, 2008; Kilicoglu and Bergler, 2008; Baker et al., 2010), with later approaches using machine learning over annotated corpora (Morante and Daelemans, 2009; Rei and Briscoe, 2010; Jean et al., 2016; Adel and Schütze, 2017). Recently, Bijl de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs. ical entailment knowledge stored within them is useful for Question Answering (McKenna et al., 2021), as well as other tasks such as email c"
2021.insights-1.16,N06-1005,0,0.102783,"rante and Daelemans, 2012b). Early approaches to detecting modality focused on lexicon design (Szarvas, 2008; Kilicoglu and Bergler, 2008; Baker et al., 2010), with later approaches using machine learning over annotated corpora (Morante and Daelemans, 2009; Rei and Briscoe, 2010; Jean et al., 2016; Adel and Schütze, 2017). Recently, Bijl de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs. ical entailment knowledge stored within them is useful for Question Answering (McKenna et al., 2021), as well as other tasks such as email categorisation (Eichler et al., 2014), relation extraction (Eichler et al., 2017) and link prediction (Hosseini et al., 2019). A subgraph containing predicates of a type-pair (e.g. PERSON-LOCATION) can be learned in an unsupervised way from collections of multiplysourced text. A vector of argument-pair counts for every predicate is first"
2021.insights-1.16,N19-1020,1,0.868453,"Missing"
2021.insights-1.16,P08-1033,0,0.0120881,"ive Results in NLP, pages 110–116 November 10, 2021. ©2021 Association for Computational Linguistics ISBN 978-1-954085-93-0 Category ∅ Modal operator Conditional Counterfactual Propositional attitude Example Protesters attacked the police Protesters may have attacked the police If protesters attack the police... Had protesters attacked the police... Journalists said that protesters attacked the police Table 1: Modality categories Answering and Knowledge Base Population (Karttunen and Zaenen, 2005; Morante and Daelemans, 2012b). Early approaches to detecting modality focused on lexicon design (Szarvas, 2008; Kilicoglu and Bergler, 2008; Baker et al., 2010), with later approaches using machine learning over annotated corpora (Morante and Daelemans, 2009; Rei and Briscoe, 2010; Jean et al., 2016; Adel and Schütze, 2017). Recently, Bijl de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect"
2021.insights-1.16,C08-1107,0,0.204037,"ent Graphs from raw text corpora under two different settings. Modality-aware: modal predications are removed from the data entirely, and modality-unaware: the model learns from both asserted and modal predications. Our contributions are 1) a comparison of Entailment Graphs learned from modal and nonmodal data, showing (counterintuitively) that ignoring modal distinctions in fact improves Entailment Graph-learning, and 2) insights as to whether this effect applies uniformly across different subdomains. 2 Background Entailment rules specify directional inferences between linguistic predicates (Szpektor and Dagan, 2008), and can be stored in an Entailment Graph, whose global structural properties can be used to learn more accurately (Berant et al., 2011, 2015). * Equal contribution They are defined as a directed graph G = {N, E}, 1 Assuming a democratic election. We use the typical defin which the nodes N are typed predicates and inition of the premise most likely entailing the hypothesis (Dagan et al., 2006) edges E represent the entailment relation. The lex110 Proceedings of the Second Workshop on Insights from Negative Results in NLP, pages 110–116 November 10, 2021. ©2021 Association for Computational Li"
2021.insights-1.16,W03-1011,0,0.0155796,"predicates of a type-pair (e.g. PERSON-LOCATION) can be learned in an unsupervised way from collections of multiplysourced text. A vector of argument-pair counts for every predicate is first machine read from the 3 Methods corpus. Typically, relation extraction systems used for reading these corpora ignore modal modifiers, We extend relation extraction to pay attention to possibly introducing noise in the graph. Next, a modality, so that we can distinguish modal and non(directed) similarity score (e.g. DIRT (Lin and Pan- modal relations in the Entailment Graph mining tel, 2001), Weed’s score (Weeds and Weir, 2003) algorithm. This allows us to investigate the impact or BInc (Szpektor and Dagan, 2008)) is computed of modalised predicate data on the accuracy of between the vectors, producing a local entailment learned entailment edges. score between each predicate pair. Then a globaliWe extract binary relations of the form arg1sation process such as the soft constraints algorithm predicate-arg2 using M O NTEE, an open-domain of Hosseini et al. (2018), which transfers informa- modality-aware relation extraction system (Bijl de tion both within and between type-pair subgraphs, Vroe et al., 2021). M O NTEE u"
2021.insights-1.16,D13-1183,0,0.0742463,"Missing"
2021.insights-1.7,D19-1607,0,0.026517,"ee table 1). The hierarchical typing model has the tendency Related Work To the best of our knowledge there is no work that compares annotation projection directly against zero-shot cross-lingual transfer. While annotation projection has been used in a variety of tasks, there has not been a study of a case where this approach fails. Authors admit that the quality of the annotating system plays role (e.g. Ehrmann et al. (2011); Ni et al. (2017)), but they don’t specify model properties that are necessary for the approach to work, instead focusing on ways to mitigate noise. Pires et al. (2019); Hsu et al. (2019) and Artetxe and Schwenk (2019) show the strengths of zeroshot cross-lingual transfer on a variety of different NLP tasks, but they do not address fine-grained entity typing. Zhao et al. (2020) conclude that zeroshot performance can be improved by choosing a small amount of high quality training data from the target language. We test their approach for the FET scenario, but arrive at unclear results. 3 Experimental Setup Method In this work we use the hierarchical typing model of Chen et al. (2020) trained on English gold data for the zero-shot approach and also to annotate the English side of"
2021.insights-1.7,J03-1002,0,0.0417078,"Missing"
2021.insights-1.7,2020.acl-main.749,0,0.0612356,"Missing"
2021.insights-1.7,P19-1493,0,0.0232523,"e training corpora (see table 1). The hierarchical typing model has the tendency Related Work To the best of our knowledge there is no work that compares annotation projection directly against zero-shot cross-lingual transfer. While annotation projection has been used in a variety of tasks, there has not been a study of a case where this approach fails. Authors admit that the quality of the annotating system plays role (e.g. Ehrmann et al. (2011); Ni et al. (2017)), but they don’t specify model properties that are necessary for the approach to work, instead focusing on ways to mitigate noise. Pires et al. (2019); Hsu et al. (2019) and Artetxe and Schwenk (2019) show the strengths of zeroshot cross-lingual transfer on a variety of different NLP tasks, but they do not address fine-grained entity typing. Zhao et al. (2020) conclude that zeroshot performance can be improved by choosing a small amount of high quality training data from the target language. We test their approach for the FET scenario, but arrive at unclear results. 3 Experimental Setup Method In this work we use the hierarchical typing model of Chen et al. (2020) trained on English gold data for the zero-shot approach and also to annotate"
2021.insights-1.7,P19-4007,0,0.023921,"eebase (Bollacker et al., 2008). They are both interpretable 42 Proceedings of the Second Workshop on Insights from Negative Results in NLP, pages 42–48 November 10, 2021. ©2021 Association for Computational Linguistics ISBN 978-1-954085-93-0 Another approach to the same problem is zeroshot cross-lingual transfer, in which a model built on multilingual word embeddings and trained on high-resource language data is applied to test data in a different language. Because the English FET model used in this work (Chen et al., 2020) relies on contextualised multilingual word embeddings (XLM-RoBERTa) (Conneau et al., 2019) it is possible to train it on English data and to test it on German. We compare the two approaches and show that the annotation projection approach amplifies the model’s tendency to underpredict level 2 types, which lowers model performance. We also introduce three new test sets for German FET1 on which zero-shot cross-lingual transfer performs better than models trained with German or a mix of German and English data. 2 In the hierarchical typing model the entity and its context are encoded using multilingual XLMRoBERTa (Conneau et al., 2019). For each type in the FIGER ontology the model le"
2021.insights-1.7,2020.acl-demos.14,0,0.0232395,"typing. In our study of fine-grained entity typing with the FIGER type ontology for German, we show that annotation projection amplifies the English model’s tendency to underpredict level 2 labels and is beaten by zero-shot cross-lingual transfer on three novel test sets. 1 Figure 1: An example of fine-grained entity typing with the FIGER ontology. Correct types are highlighted. by humans and useful in NLP applications such as relation extraction (Kuang et al., 2020). There are systems for named entity recognition and coarse-grained entity typing in languages other than English (e.g. Stanza (Qi et al., 2020)), but systems for FET with FIGER types are only available in English, due to the lack of FIGER annotated data in other languages. Because manual annotation is time consuming and expensive, various methods have been proposed to expand NLP models to other languages without additional manual annotation. The method of annotation projection (Yarowsky and Ngai, 2001) uses parallel text to automatically create annotated corpora. Annotations from the resource-rich language are transferred to the resource-poor language using word alignment between translated sentences. Annotation projection has been u"
2021.insights-1.7,N13-1073,0,0.105492,"Missing"
2021.insights-1.7,R11-1017,0,0.0389334,"for annotation projection. For details of the selection process refer to appendix A. An important point for our experiments is the label distribution in the training corpora (see table 1). The hierarchical typing model has the tendency Related Work To the best of our knowledge there is no work that compares annotation projection directly against zero-shot cross-lingual transfer. While annotation projection has been used in a variety of tasks, there has not been a study of a case where this approach fails. Authors admit that the quality of the annotating system plays role (e.g. Ehrmann et al. (2011); Ni et al. (2017)), but they don’t specify model properties that are necessary for the approach to work, instead focusing on ways to mitigate noise. Pires et al. (2019); Hsu et al. (2019) and Artetxe and Schwenk (2019) show the strengths of zeroshot cross-lingual transfer on a variety of different NLP tasks, but they do not address fine-grained entity typing. Zhao et al. (2020) conclude that zeroshot performance can be improved by choosing a small amount of high quality training data from the target language. We test their approach for the FET scenario, but arrive at unclear results. 3 Experi"
2021.insights-1.7,N01-1026,0,0.414946,"ect types are highlighted. by humans and useful in NLP applications such as relation extraction (Kuang et al., 2020). There are systems for named entity recognition and coarse-grained entity typing in languages other than English (e.g. Stanza (Qi et al., 2020)), but systems for FET with FIGER types are only available in English, due to the lack of FIGER annotated data in other languages. Because manual annotation is time consuming and expensive, various methods have been proposed to expand NLP models to other languages without additional manual annotation. The method of annotation projection (Yarowsky and Ngai, 2001) uses parallel text to automatically create annotated corpora. Annotations from the resource-rich language are transferred to the resource-poor language using word alignment between translated sentences. Annotation projection has been used successfully for the task of coarse-grained named entity typing in conjunction with named entity recognition (Agerri et al., 2018; Li et al., 2021; Ni et al., 2017). We follow these examples by using a parallel English-German corpus, automatic named entity recognition and a state of the art English FET model (Chen et al., 2020) to assign FIGER type labels on"
2021.iwpt-1.4,D13-1160,0,0.105633,"Missing"
2021.iwpt-1.4,P14-1133,0,0.0631626,"Missing"
2021.iwpt-1.4,P16-1004,0,0.0574668,"Missing"
2021.iwpt-1.4,D13-1161,0,0.0594703,"Missing"
2021.iwpt-1.4,2021.emnlp-main.707,0,0.197581,"tps: //github.com/Teddy-Li/SemiAuto_Data_ Text_SQL 39 trained on it still suffer a sharp performance drop when generalizing to unseen domains3 . Thus, additional resource for domain transfer is appealing. However, in this more complex multitable Text-to-SQL task, previous semi-automatic dataset construction methods face an even greater challenge. With multi-table SQL queries with more complex clauses, exhaustive enumeration is intractable in size and prone to mismatches. More recently, various methods of Text-to-SQL dataset construction have been proposed (Yu et al., 2020; Zhong et al., 2020; Zhang et al., 2021), further automating the SQL-to-NL step with neural question generation. However, for query construction, they either do vanilla grammar-based SQL query sampling (Zhang et al., 2021) or use template sketches from existing datasets (Zhong et al., 2020; Yu et al., 2020). On the other hand, we focus instead on the context-dependent construction of SQL queries that both generalize beyond existing datasets and remain realistic. 3 Figure 2: The grammar for generating SQL queries, listed iteratively. PREDEFINED TOPIC is the topic set with method in section 3.1; terminal nodes are in red, non-terminal"
2021.iwpt-1.4,J13-2005,0,0.025746,"sitionally, converted them to rigid pseudo natural language (Pseudo-NL) questions with rules, then crowd-sourced those Pseudo-NL questions into NL questions. Cheng et al. (2018) further broke down the pseudo-NL questions into question sequences to make them more digestible for crowd workers. While these approaches shed light on the methodology of semi-automatic construction of semantic parsing datasets, applying them to collect broad-coverage Text-to-SQL data for domain transfer is not trivial. Firstly, SQL language has a much larger variety of realistic queries than Lambda-DCS logical forms (Liang, 2013), which were the focus of earlier work. Blind enumeration-up-to-a-certaindepth from a CFG is therefore intractable in size. Secondly, Herzig and Berant (2019) have discovered a mismatch between semi-automatically constructed queries and real-world queries, in terms of the distribution of logical form and the style of natural language expressions. As achieving accuracy gains in domain transfer demands high quality for the in-domain data, narrowing these mismatches is crucial. In this paper, we propose a novel semi-automatic pipeline for robust construction of Text-SQL pairs as training data in"
2021.iwpt-1.4,Q14-1030,1,0.88278,"Missing"
2021.iwpt-1.4,2020.acl-main.677,0,0.245314,"transfer performance for SOTA semantic parsers. 1 Introduction Due to the broad use of SQL in real-world databases, the task of mapping natural language questions to SQL queries (Text-to-SQL) has drawn considerable attention. Several large-scale crossdomain Text-to-SQL datasets have been manually constructed and advanced the development of Textto-SQL semantic parsing (Zhong et al., 2017; Yu et al., 2018). While these datasets are built for domain-general semantic parsing, current state-of-the-art (SOTA) semantic parsers still suffer sharp performance drop when generalising to unseen domains (Wang et al., 2020; Guo et al., 2019; Zhang et al., 2019). This could be attributed to the observation that the mapping of Text-to-SQL vary vastly across different domains, particularly in terms of the expressions of predicates1 . It is very difficult for models to generalize to those variations in a zero-shot fashion. Thus, additional in-domain data is desirable when applying semantic parsers to novel domains. 1 An example illustrating such difference is presented in Appendix A 38 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 38–49 Bangkok, Thailand (online), Augus"
2021.iwpt-1.4,2020.emnlp-main.558,0,0.0419277,"ll be released at https: //github.com/Teddy-Li/SemiAuto_Data_ Text_SQL 39 trained on it still suffer a sharp performance drop when generalizing to unseen domains3 . Thus, additional resource for domain transfer is appealing. However, in this more complex multitable Text-to-SQL task, previous semi-automatic dataset construction methods face an even greater challenge. With multi-table SQL queries with more complex clauses, exhaustive enumeration is intractable in size and prone to mismatches. More recently, various methods of Text-to-SQL dataset construction have been proposed (Yu et al., 2020; Zhong et al., 2020; Zhang et al., 2021), further automating the SQL-to-NL step with neural question generation. However, for query construction, they either do vanilla grammar-based SQL query sampling (Zhang et al., 2021) or use template sketches from existing datasets (Zhong et al., 2020; Yu et al., 2020). On the other hand, we focus instead on the context-dependent construction of SQL queries that both generalize beyond existing datasets and remain realistic. 3 Figure 2: The grammar for generating SQL queries, listed iteratively. PREDEFINED TOPIC is the topic set with method in section 3.1; terminal nodes are"
2021.iwpt-1.4,P15-1129,0,0.047473,"Missing"
2021.iwpt-1.4,P14-1090,0,0.0733521,"Missing"
2021.textgraphs-1.14,2020.acl-main.749,0,0.0362246,"Missing"
2021.textgraphs-1.14,P18-1009,0,0.0400363,"Missing"
2021.textgraphs-1.14,2020.acl-main.747,0,0.191107,"reate a fine-grained entity typing system that is capable of typing both GE and NE in English by integrating GEs into their training data. Their approach relies on large amounts of manually annotated data, and is therefore not feasible for our case. Moreover they propose a new type hierarchy, while we stick to the widely used FIGER type hierarchy, to make the output of our system consistent with that of other systems for tasks like multilingual knowledge graph construction. Recent advances in typing NE in English have harnessed the power of contextualized word embeddings (Peters et al., 2018; Conneau et al., 2020) to encode entities and their context. These approaches use the AIDA, BNN, OntoNotes and FIGER ontologies, which come with their own human annotated data sets (Chen et al., 2020; Dai et al., 2019; López et al., 2019). By choosing to use the model of (Chen et al., 2020), we build upon their strengths to enable GE typing in German. German NE typing suffers from a lack of manually annotated resources. Two recent approaches by by Ruppenhofer et al. (2020) and Leitner et al. (2020) use manually annotated data from biographic interviews and court proceedings. Owing to the specific domains, the autho"
2021.textgraphs-1.14,D19-1643,0,0.0210638,"data, and is therefore not feasible for our case. Moreover they propose a new type hierarchy, while we stick to the widely used FIGER type hierarchy, to make the output of our system consistent with that of other systems for tasks like multilingual knowledge graph construction. Recent advances in typing NE in English have harnessed the power of contextualized word embeddings (Peters et al., 2018; Conneau et al., 2020) to encode entities and their context. These approaches use the AIDA, BNN, OntoNotes and FIGER ontologies, which come with their own human annotated data sets (Chen et al., 2020; Dai et al., 2019; López et al., 2019). By choosing to use the model of (Chen et al., 2020), we build upon their strengths to enable GE typing in German. German NE typing suffers from a lack of manually annotated resources. Two recent approaches by by Ruppenhofer et al. (2020) and Leitner et al. (2020) use manually annotated data from biographic interviews and court proceedings. Owing to the specific domains, the authors modify existing type onthologies (OntoNotes in the case of biographic interviews) or come up with their own type ontology (in the case of court proceedings). This limits the way their models c"
2021.textgraphs-1.14,W97-0800,0,0.511095,"strates this method. We use this method to generate German GE training data and as our rule-based baseline. We use this GE training data in addition to German NE typing data to train the hierarchical typing model of Chen et al. (2020). In this model the entity and its context are encoded using XLMRoBERTa (Conneau et al., 2020). For each type in the FIGER ontology the model learns a type embedding. We pass the concatenated entity and context vector trough a 2-layer feed-forward network that maps into the same space as the type 3 Method embedding. The score is an inner product between GermaNet (Hamp and Feldweg, 1997) is a broad- the transformed entity and context vector and the coverage lexical-semantic net for German which type embedding. For further model details refer to contains 16.000 words and is modelled after the En- Chen et al. (2020). 139 4 4.1 Experimental setup Data sets As a NE training set we use the German finegrained entity typing corpus of Weber and Steedman (2021, under submission). This data set was generated from the WikiMatrix corpus by Schwenk et al. (2019) using annotation projection. To create the GE training data, we use the German portion of the WikiMatrix corpus. By using the sa"
2021.textgraphs-1.14,P19-1468,1,0.872773,"Missing"
2021.textgraphs-1.14,2020.lrec-1.551,0,0.0285098,"cent advances in typing NE in English have harnessed the power of contextualized word embeddings (Peters et al., 2018; Conneau et al., 2020) to encode entities and their context. These approaches use the AIDA, BNN, OntoNotes and FIGER ontologies, which come with their own human annotated data sets (Chen et al., 2020; Dai et al., 2019; López et al., 2019). By choosing to use the model of (Chen et al., 2020), we build upon their strengths to enable GE typing in German. German NE typing suffers from a lack of manually annotated resources. Two recent approaches by by Ruppenhofer et al. (2020) and Leitner et al. (2020) use manually annotated data from biographic interviews and court proceedings. Owing to the specific domains, the authors modify existing type onthologies (OntoNotes in the case of biographic interviews) or come up with their own type ontology (in the case of court proceedings). This limits the way their models can be applied to other domains or used for multilingual tasks. Weber and Steedman (2021, under submission) use annotation projection to create a training data set of Wikipedia text annotated with FIGER types. We build upon their data set to create a German model that types both NEs and"
2021.textgraphs-1.14,W19-4319,0,0.012698,"fore not feasible for our case. Moreover they propose a new type hierarchy, while we stick to the widely used FIGER type hierarchy, to make the output of our system consistent with that of other systems for tasks like multilingual knowledge graph construction. Recent advances in typing NE in English have harnessed the power of contextualized word embeddings (Peters et al., 2018; Conneau et al., 2020) to encode entities and their context. These approaches use the AIDA, BNN, OntoNotes and FIGER ontologies, which come with their own human annotated data sets (Chen et al., 2020; Dai et al., 2019; López et al., 2019). By choosing to use the model of (Chen et al., 2020), we build upon their strengths to enable GE typing in German. German NE typing suffers from a lack of manually annotated resources. Two recent approaches by by Ruppenhofer et al. (2020) and Leitner et al. (2020) use manually annotated data from biographic interviews and court proceedings. Owing to the specific domains, the authors modify existing type onthologies (OntoNotes in the case of biographic interviews) or come up with their own type ontology (in the case of court proceedings). This limits the way their models can be applied to othe"
2021.textgraphs-1.14,N18-1202,0,0.0236761,"Choi et al. (2018) create a fine-grained entity typing system that is capable of typing both GE and NE in English by integrating GEs into their training data. Their approach relies on large amounts of manually annotated data, and is therefore not feasible for our case. Moreover they propose a new type hierarchy, while we stick to the widely used FIGER type hierarchy, to make the output of our system consistent with that of other systems for tasks like multilingual knowledge graph construction. Recent advances in typing NE in English have harnessed the power of contextualized word embeddings (Peters et al., 2018; Conneau et al., 2020) to encode entities and their context. These approaches use the AIDA, BNN, OntoNotes and FIGER ontologies, which come with their own human annotated data sets (Chen et al., 2020; Dai et al., 2019; López et al., 2019). By choosing to use the model of (Chen et al., 2020), we build upon their strengths to enable GE typing in German. German NE typing suffers from a lack of manually annotated resources. Two recent approaches by by Ruppenhofer et al. (2020) and Leitner et al. (2020) use manually annotated data from biographic interviews and court proceedings. Owing to the spec"
2021.textgraphs-1.14,2020.lrec-1.566,0,0.0203384,"owledge graph construction. Recent advances in typing NE in English have harnessed the power of contextualized word embeddings (Peters et al., 2018; Conneau et al., 2020) to encode entities and their context. These approaches use the AIDA, BNN, OntoNotes and FIGER ontologies, which come with their own human annotated data sets (Chen et al., 2020; Dai et al., 2019; López et al., 2019). By choosing to use the model of (Chen et al., 2020), we build upon their strengths to enable GE typing in German. German NE typing suffers from a lack of manually annotated resources. Two recent approaches by by Ruppenhofer et al. (2020) and Leitner et al. (2020) use manually annotated data from biographic interviews and court proceedings. Owing to the specific domains, the authors modify existing type onthologies (OntoNotes in the case of biographic interviews) or come up with their own type ontology (in the case of court proceedings). This limits the way their models can be applied to other domains or used for multilingual tasks. Weber and Steedman (2021, under submission) use annotation projection to create a training data set of Wikipedia text annotated with FIGER types. We build upon their data set to create a German mod"
2021.textgraphs-1.14,D16-1015,0,0.0696626,"Missing"
C04-1180,briscoe-carroll-2002-robust,0,0.0181404,"Missing"
C04-1180,E99-1042,0,0.00750892,"Missing"
C04-1180,2003.mtsummit-papers.6,0,0.0315977,"Missing"
C04-1180,A00-2018,0,0.0299319,"Missing"
C04-1180,W03-1013,1,0.567339,"mma and returns a sentential modifier of the same type. Type-raising is applied to the categories NP, PP and S adj NP (adjectival phrase), and is implemented by adding the relevant set of type-raised categories to the chart whenever an NP, PP or S adj NP is present. The sets of type-raised categories are based on the most commonly used typeraising rule instantiations in sections 2-21 of CCGbank, and currently contain 8 type-raised categories for NP and 1 each for PP and S adj NP. For a given sentence, the automatically extracted grammar can produce a very large number of derivations. Clark and Curran (2003) and Clark and Curran (2004b) describe how a packed chart can be used to efficiently represent the derivation space, and also efficient algorithms for finding the most probable derivation. The parser uses a log-linear model over normal-form derivations.3 Features are defined in terms of the local trees in the derivation, including lexical head information and word-word dependencies. The normal-form derivations in CCGbank provide the gold standard training data. For a given sentence, the output of the parser is a set of syntactic dependencies corresponding to the 3 most probable derivation. How"
C04-1180,C04-1041,1,0.441407,"es are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the target word’s context to dec"
C04-1180,P04-1014,1,0.669575,"es are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the target word’s context to dec"
C04-1180,P02-1042,1,0.779112,"of the derivation and of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger us"
C04-1180,hockenmaier-steedman-2002-acquiring,1,0.878485,"nd of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the tar"
C04-1180,P02-1043,1,0.826238,"nd of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the tar"
C04-1180,N04-1013,0,0.0413167,"Missing"
C04-1180,C02-1105,0,0.0414764,"Missing"
C04-1180,J03-4003,0,\N,Missing
D09-1085,de-marneffe-etal-2006-generating,0,0.240733,"Missing"
D09-1085,H91-1060,0,0.0344846,"Missing"
D09-1085,W01-0521,0,0.0557389,"Missing"
D09-1085,J07-3004,1,0.896337,"e removed. 3.2 The parsers The parsers that we chose to evaluate are the C & C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C & C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation because CCG was designed to capture the unbounded dependencies being considered. The Enju parser was designed with a similar motivation to C & C, and is also based on an automatically extracted grammar derived from the PTB, but the grammar formalism is HPSG rather than CCG . Both parsers produce head-word dependencies reflecting the underlying predicate-argument structure of a sentence, and so in theory should be straightforward to evaluate. The RASP parser is based on a manually constructed POS tag-sequence grammar"
D09-1085,E03-1005,0,0.0607821,"Missing"
D09-1085,P08-1067,0,0.0408422,"Missing"
D09-1085,P06-4020,0,0.0556321,"it does not produce explicit dependencies. However, the DCU post-processor is ideal because it does produce dependencies in a GR format. It has also obtained competitive scores on general GR evaluation corpora (Cahill et al., 2004). Table 3: Distance between head and dependent. subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed. 3.2 The parsers The parsers that we chose to evaluate are the C & C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C & C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation because CCG was designed to capture the unbounded dependencies being considered. The Enju p"
D09-1085,P02-1018,0,0.309025,"resented. Fourth, we show that current parsing technology is very poor at representing some important elements of the argument structure of sentences, and argue for a more focused constructionbased parser evaluation as a complement to existing grammatical relation-based evaluations. We also perform an error-analysis for one of the more successful parsers. There has been some prior work on evaluating parsers on long-range dependencies, but no work we are aware of that has the scope and focus of this paper. Clark et al. (2004) evaluated a CCG parser on a small corpus of object extraction cases. Johnson (2002) began the body of work on inserting traces into the output of Penn Treebank (PTB) parsers, followed by Levy and Manning (2004), among others. This PTB work focused heavily on the representation in the Treebank, evaluating against patterns in the trace annotation. In this paper we have tried to be more “formalismindependent” and construction focused. 2 2.1 in object position has apparently been extracted: the paper which I wrote. Our corpus includes cases where the extracted word is (semantically) the object of a preposition in the verb phrase: the agency that I applied to. Object extraction f"
D09-1085,P04-1041,0,0.125932,"Missing"
D09-1085,P03-1054,0,0.0330272,"However, the DCU post-processor is ideal because it does produce dependencies in a GR format. It has also obtained competitive scores on general GR evaluation corpora (Cahill et al., 2004). Table 3: Distance between head and dependent. subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed. 3.2 The parsers The parsers that we chose to evaluate are the C & C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C & C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation because CCG was designed to capture the unbounded dependencies being considered. The Enju parser was designed with a similar motivation to"
D09-1085,W08-2102,0,0.0339446,"Missing"
D09-1085,P04-1042,0,0.104635,"argument structure of sentences, and argue for a more focused constructionbased parser evaluation as a complement to existing grammatical relation-based evaluations. We also perform an error-analysis for one of the more successful parsers. There has been some prior work on evaluating parsers on long-range dependencies, but no work we are aware of that has the scope and focus of this paper. Clark et al. (2004) evaluated a CCG parser on a small corpus of object extraction cases. Johnson (2002) began the body of work on inserting traces into the output of Penn Treebank (PTB) parsers, followed by Levy and Manning (2004), among others. This PTB work focused heavily on the representation in the Treebank, evaluating against patterns in the trace annotation. In this paper we have tried to be more “formalismindependent” and construction focused. 2 2.1 in object position has apparently been extracted: the paper which I wrote. Our corpus includes cases where the extracted word is (semantically) the object of a preposition in the verb phrase: the agency that I applied to. Object extraction from a reduced relative clause is essentially the same, except that there is no overt relative pronoun: the paper I wrote; the a"
D09-1085,N06-1020,0,0.109981,"Missing"
D09-1085,P05-1022,0,0.186351,"Missing"
D09-1085,A00-2018,0,0.148425,"he dependencies in our corpus; for example, the tagsequence grammar has no explicit representation of verb subcategorisation, and so may not know that there is a missing object in the case of extraction from a relative clause (though it does recover some of these dependencies). However, RASP is a popular parser used in a number of applications, and it returns dependencies in a suitable format for evaluation, and so we considered it to be an appropriate and useful member of our parser set. The Stanford parser is representative of a large number of PTB parsers, exemplified by Collins (1997) and Charniak (2000). The Parseval scores reported for the Stanford parser are not the highest in the literature, but are competitive enough for our purposes. The advantage of the Stanford parser is that it returns dependencies in a suitable format for our evaluation. The dependencies are obtained by a set of manually defined rules operating over the phrase-structure trees returned by the parser (de Marneffe et al., 2006). Like RASP, the Stanford parser has not been designed to capture unbounded dependencies; in particular it does not make use of any of the trace information in the PTB. However, we wanted to incl"
D09-1085,P05-1012,0,0.0354573,"bounded dependencies being considered. The Enju parser was designed with a similar motivation to C & C, and is also based on an automatically extracted grammar derived from the PTB, but the grammar formalism is HPSG rather than CCG . Both parsers produce head-word dependencies reflecting the underlying predicate-argument structure of a sentence, and so in theory should be straightforward to evaluate. The RASP parser is based on a manually constructed POS tag-sequence grammar, with a statistical parse selection component and a robust 3.3 1 One obvious omission is any form of dependency parser (McDonald et al., 2005; Nivre and Scholz, 2004). However, the dependencies returned by these parsers are local, and it would be non-trivial to infer from a series of links whether a long-range dependency had been correctly represented. Also, dependency parsers are not significantly better at recovering head-based dependencies than constituent parsers based on the PTB (McDonald et al., 2005). Parser evaluation The parsers were run essentially out-of-the-box when parsing the test sentences. The one exception was C & C, which required some minor adjusting of parameters, as described in the parser documentation, to obt"
D09-1085,J07-4004,1,0.953553,"representation directly, such as Johnson (2002), is difficult for us to evaluate because it does not produce explicit dependencies. However, the DCU post-processor is ideal because it does produce dependencies in a GR format. It has also obtained competitive scores on general GR evaluation corpora (Cahill et al., 2004). Table 3: Distance between head and dependent. subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed. 3.2 The parsers The parsers that we chose to evaluate are the C & C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C & C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation beca"
D09-1085,P05-1011,0,0.136149,"), is difficult for us to evaluate because it does not produce explicit dependencies. However, the DCU post-processor is ideal because it does produce dependencies in a GR format. It has also obtained competitive scores on general GR evaluation corpora (Cahill et al., 2004). Table 3: Distance between head and dependent. subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed. 3.2 The parsers The parsers that we chose to evaluate are the C & C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C & C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation because CCG was designed to capture the unbounded d"
D09-1085,W04-3215,1,0.872505,"cause of different conventions across the parsers as to how the underlying grammar is represented. Fourth, we show that current parsing technology is very poor at representing some important elements of the argument structure of sentences, and argue for a more focused constructionbased parser evaluation as a complement to existing grammatical relation-based evaluations. We also perform an error-analysis for one of the more successful parsers. There has been some prior work on evaluating parsers on long-range dependencies, but no work we are aware of that has the scope and focus of this paper. Clark et al. (2004) evaluated a CCG parser on a small corpus of object extraction cases. Johnson (2002) began the body of work on inserting traces into the output of Penn Treebank (PTB) parsers, followed by Levy and Manning (2004), among others. This PTB work focused heavily on the representation in the Treebank, evaluating against patterns in the trace annotation. In this paper we have tried to be more “formalismindependent” and construction focused. 2 2.1 in object position has apparently been extracted: the paper which I wrote. Our corpus includes cases where the extracted word is (semantically) the object of"
D09-1085,C04-1010,0,0.0309107,"ing considered. The Enju parser was designed with a similar motivation to C & C, and is also based on an automatically extracted grammar derived from the PTB, but the grammar formalism is HPSG rather than CCG . Both parsers produce head-word dependencies reflecting the underlying predicate-argument structure of a sentence, and so in theory should be straightforward to evaluate. The RASP parser is based on a manually constructed POS tag-sequence grammar, with a statistical parse selection component and a robust 3.3 1 One obvious omission is any form of dependency parser (McDonald et al., 2005; Nivre and Scholz, 2004). However, the dependencies returned by these parsers are local, and it would be non-trivial to infer from a series of links whether a long-range dependency had been correctly represented. Also, dependency parsers are not significantly better at recovering head-based dependencies than constituent parsers based on the PTB (McDonald et al., 2005). Parser evaluation The parsers were run essentially out-of-the-box when parsing the test sentences. The one exception was C & C, which required some minor adjusting of parameters, as described in the parser documentation, to obtain close to full coverag"
D09-1085,N07-1051,0,0.0681442,"Missing"
D09-1085,P97-1003,0,0.154492,"o capture many of the dependencies in our corpus; for example, the tagsequence grammar has no explicit representation of verb subcategorisation, and so may not know that there is a missing object in the case of extraction from a relative clause (though it does recover some of these dependencies). However, RASP is a popular parser used in a number of applications, and it returns dependencies in a suitable format for evaluation, and so we considered it to be an appropriate and useful member of our parser set. The Stanford parser is representative of a large number of PTB parsers, exemplified by Collins (1997) and Charniak (2000). The Parseval scores reported for the Stanford parser are not the highest in the literature, but are competitive enough for our purposes. The advantage of the Stanford parser is that it returns dependencies in a suitable format for our evaluation. The dependencies are obtained by a set of manually defined rules operating over the phrase-structure trees returned by the parser (de Marneffe et al., 2006). Like RASP, the Stanford parser has not been designed to capture unbounded dependencies; in particular it does not make use of any of the trace information in the PTB. Howeve"
D09-1085,D08-1050,1,0.903466,"clause is a so-called small clause, i.e. one with a null copula verb: the plan that she considered foolish, where plan is the semantic subject of foolish. 2.2 With the exception of the question construction, all sentences were taken from the PTB, with roughly half from the WSJ sections (excluding 2-21 which provided the training data for many The data The corpus consists of approximately 100 sentences for each of the seven constructions; 80 of 815 of the parsers in our set) and half from Brown (roughly balanced across the different sections). The questions were taken from the question data in Rimell and Clark (2008), which was obtained from various years of the TREC QA track. We chose to use the PTB as the main source because the use of traces in the PTB annotation provides a starting point for the identification of unbounded dependencies. Sentences were selected for the corpus by a combination of automatic and manual processes. A regular expression applied to PTB trees, searching for appropriate traces for a particular construction, was first used to extract a set of candidate sentences. All candidates were manually reviewed and, if selected, annotated with one or more grammatical relations representing"
D09-1085,J03-4003,0,\N,Missing
D10-1056,N10-1083,0,0.565401,"Missing"
D10-1056,P06-3002,0,0.106796,"this improves the corpus probability. [clark]: Class-based n-grams with morphology (Clark, 2003). This system uses a similar model to the previous one, and also clusters word types (rather than tokens, as the rest of the systems do). The main differences between the systems are that clark uses a slightly different approximate search procedure, and that he augments the probabilistic model with a prior that prefers clusterings where morphologically similar words are clustered together. The morphology component is implemented as a single-order letter HMM. [cw]: Chinese Whispers graph clustering (Biemann, 2006). Unlike the other systems we consider, this one induces the value of |C |rather than taking it as an input parameter.2 The system uses a graph clustering algorithm called Chinese Whispers that is based on contextual similarity. The algorithm works in two stages. The first clusters the most frequent 10,000 words (target words) based on their context statistics, with contexts formed from the most frequent 150-250 words (feature words) that appear ei1 Implementations were obtained from: brown: http://www.cs.berkeley.edu/∼pliang/ software/brown-cluster-1.2.zip (Percy Liang), clark: http://www.cs."
D10-1056,J92-4003,0,0.676257,"in many cases, being more stable across different numbers of found and true clusters, and avoiding several of the problems with another commonly used entropy-based measure, Variation of Information (Meilˇa, 2003). Using V-Measure along with several other evaluation measures, we compare the performance of the different induction systems on both WSJ (the data on which most systems were developed and tested) and Multext East, a corpus of parallel texts in eight different languages. We find that for virtually all measures and datasets, older systems using relatively simple models and algorithms (Brown et al., 1992; Clark, 2003) work as well or better than systems using newer and often far more sophisticated and time-consuming machine learning methods (Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010). Thus, although these newer methods have introduced potentially useful machine learning techniques, they should not be assumed to provide the best performance for unsupervised POS induction. In addition to our review and comparison, we introduce a new way to both evaluate and potentially improve a POS induction system. Our method is based on the prototype-driv"
D10-1056,E03-1009,0,0.810921,"g more stable across different numbers of found and true clusters, and avoiding several of the problems with another commonly used entropy-based measure, Variation of Information (Meilˇa, 2003). Using V-Measure along with several other evaluation measures, we compare the performance of the different induction systems on both WSJ (the data on which most systems were developed and tested) and Multext East, a corpus of parallel texts in eight different languages. We find that for virtually all measures and datasets, older systems using relatively simple models and algorithms (Brown et al., 1992; Clark, 2003) work as well or better than systems using newer and often far more sophisticated and time-consuming machine learning methods (Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010). Thus, although these newer methods have introduced potentially useful machine learning techniques, they should not be assumed to provide the best performance for unsupervised POS induction. In addition to our review and comparison, we introduce a new way to both evaluate and potentially improve a POS induction system. Our method is based on the prototype-driven learning sy"
D10-1056,erjavec-2004-multext,0,0.0126186,"Section 2. We first present results for the same WSJ corpus used above. However, because most of the systems were initially developed on this corpus, and often evaluated only on it, there is a question of whether their methods and/or hyperparameters are overly specific to the domain or to the English language. This is a particularly pertinent question since a primary argument in favor of unsupervised systems is that they are easier to port to a new language or domain than supervised systems. To address this question, we evaluate all the systems as well on the multilingual Multext East corpus (Erjavec, 2004), without changing any of the parameter settings. |C| was set to 45 for all of the experiments reported in this section. Based on our assessment of evaluation 580 Figure 2: Performance of the different systems on WSJ, using three different measures [|C|:45, |T |:45] system brown clark cw bhmm vbhmm pr feat runtime e10 min. e40 min. e10 min. e4 hrs. e10 hrs. e10 hrs.* e40 hrs.* Table 2: Runtimes for the different systems on WSJ [|C|:45]. *pr and feat have multithreading implementations and ran on 16 cores. measures above, we report VM scores as the most reliable measure across different systems"
D10-1056,D08-1036,0,0.0869431,"one mapping accuracy (also known as cluster purity) maps each cluster to the gold standard tag that is most common for the words in that cluster (henceforth, the preferred tag), and then computes the proportion of words tagged correctly. More than one cluster may be mapped to the same gold standard tag. This is the most commonly used metric across the literature as it is intuitive and creates a meaningful POS sequence out of the cluster identifiers. However, it tends to yield higher scores as |C |increases, making comparisons difficult when |C |can vary. [crossval]: Cross-validation accuracy (Gao and Johnson, 2008) is intended to address the problem with many-to-one accuracy which is that assigning each word to its own class yields a perfect score. In this measure, the first half of the corpus is used to obtain the many-to-one mapping of clusters to tags, and this mapping is used to compute the accuracy of the clustering on the second half of the corpus. [1-to-1]: One-to-one mapping accuracy (Haghighi and Klein, 2006) constrains the mapping from clusters to tags, so that at most one cluster can be mapped to any tag. The mapping is performed greedily. In general, as the number of clusters increases, fewe"
D10-1056,P07-1094,1,0.932131,"based measure, Variation of Information (Meilˇa, 2003). Using V-Measure along with several other evaluation measures, we compare the performance of the different induction systems on both WSJ (the data on which most systems were developed and tested) and Multext East, a corpus of parallel texts in eight different languages. We find that for virtually all measures and datasets, older systems using relatively simple models and algorithms (Brown et al., 1992; Clark, 2003) work as well or better than systems using newer and often far more sophisticated and time-consuming machine learning methods (Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010). Thus, although these newer methods have introduced potentially useful machine learning techniques, they should not be assumed to provide the best performance for unsupervised POS induction. In addition to our review and comparison, we introduce a new way to both evaluate and potentially improve a POS induction system. Our method is based on the prototype-driven learning system of Haghighi and Klein (2006), which achieves very good performance by using a hand-selected list of prototypes for each syntactic cluster. We instead u"
D10-1056,N06-1041,0,0.297038,"cludes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know how well we can expect them to work. Part-of-speech (POS) induction is one of the most popular tasks in research on unsupervised NLP. Many different methods have been proposed, yet comparisons are difficult to make sinc"
D10-1056,D07-1031,0,0.703065,"stos.c@ed.ac.uk sgwater@inf.ed.ac.uk steedman@inf.ed.ac.uk Abstract the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know ho"
D10-1056,J93-2004,0,0.0413613,"ard tagging to compute. This is normally available during development of a system, but if the system is deployed on a novel language a gold standard may not be available. 578 We mentioned a few strengths and weaknesses of each evaluation method above; in this section we present some empirical results to expand on these claims. First, we examine the effects of varying |C| on the behavior of the evaluation measures, while keeping the number of gold standard tags the same (|T |= 45). Results were obtained by training and evaluating each system on the full WSJ portion of the Penn Treebank corpus (Marcus et al., 1993). Figure 1 shows the results from the Brown system for |C |ranging from 1 to 200; the same trends were observed for all other systems.3 In addition, Table 1 provides results for the two extremes of |C |= 1 (all words assigned to the same cluster) and |C |equal to the size of the corpus (a single word per cluster), as 3 The results reported in this paper are only a fraction of the total from our experiments; given the number of parameters, models and measures tested, we obtained over 15000 results. The full set of results can be found at http://homepages.inf.ed.ac.uk/s0787820/pos/. Figure 1: Sc"
D10-1056,J94-2001,0,0.620472,"h University of Edinburgh christos.c@ed.ac.uk sgwater@inf.ed.ac.uk steedman@inf.ed.ac.uk Abstract the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight"
D10-1056,P09-1057,0,0.25444,"steedman@inf.ed.ac.uk Abstract the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know how well we can expect them to work. Part-of-s"
D10-1056,D07-1043,0,0.128472,"l entropy of each clustering conditioned on the other. More formally, V I(C, T ) = H(T |C) + H(C|T ) = H(C) + H(T ) − 2I(C, T ), where H(.) is the entropy function and I(.) is the mutual information. VI and other entropy-based measures have been argued to be superior to accuracy-based measures such as those above, because they consider not only the majority tag in each cluster, but also whether the remainder of the cluster is more or less homogeneous. Unlike the other measures we consider, lower scores are better (since VI measures the difference between clusterings in bits). [vm]: V-Measure (Rosenberg and Hirschberg, 2007) is another entropy-based measure that is designed to be analogous to F-measure, in that it is defined as the weighted harmonic mean of two values, homogeneity (h, the precision analogue) and completeness (c, the recall analogue): H(T |C) H(T ) H(C|T ) c = 1− H(C) (1 + β)hc VM = (βh) + c h = 1− In addition, there is the question of whether the gold standard itself is “correct”. Recently, Frank et al. (2009) proposed this novel evaluation measure that requires no gold standard, instead using the concept of substitutability to evaluate performance. Instead of comparing the system’s clusters C to"
D10-1056,P05-1044,0,0.311309,"Abstract the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know how well we can expect them to work. Part-of-speech (POS) induction is"
D10-1056,D09-1071,0,0.0460356,"Missing"
D10-1056,W09-0210,0,0.0708737,"Missing"
D10-1119,C04-1180,1,0.305945,"(Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon. 7 Experimental Setup Features We use two types of features in our model. First, we include a set of lexical features: For each lexical item L ∈ Λ, we include a feature φL that fires when L is used. Second, we include semantic features that are functions of the output logical expression z. Each time a predicate p in z takes an argument a with type T (a) in position i it triggers two binary indicator features: φ(p,a,i) for the predicate-argument relation; and φ(p,T (a"
D10-1119,W03-1013,0,0.0248429,"ntries are iteratively refined with a restricted higher-order unification procedure (Huet, 1975) that defines all possible ways to subdivide them, consistent with the requirement that each training sentence can still be parsed to yield its labeled meaning. For the data sets we consider, the space of possible grammars is too large to explicitly enumerate. The induced grammar is also typically highly ambiguous, producing a large number of possible analyses for each sentence. Our approach discriminates between analyses using a log-linear CCG parsing model, similar to those used in previous work (Clark & Curran, 2003, 2007), but differing in that the syntactic parses are treated as a hidden variable during training, following the approach of Zettlemoyer & Collins (2005, 2007). We present an algorithm that incrementally learns the parameters of this model while simultaneously exploring the space of possible grammars. The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence. We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography (Zelle & Mooney, 1996"
D10-1119,J07-4004,0,0.145444,"r S|N P/N P λyλx.state(x) ∧ next to(x, y) &gt; S|N P λx.state(x) ∧ next to(x, tex) &gt; texas S|N P/(S|N P ) SN P/N P λf λx.state(x)∧f (x) λyλx.next to(x, y) S λx.state(x) ∧ next to(x, tex) NP tex &gt;B &gt; &gt; Figure 1: Two examples of CCG parses with different logical form representations. defined as: eθ·φ(x,y,z) θ·φ(x,y 0 ,z 0 ) (y 0 ,z 0 ) e P (y, z|x; θ, Λ) = P (1) Section 7 defines the features used in the experiments, which include, for example, lexical features that indicate when specific lexical items in Λ are used in the parse y. For parsing and parameter estimation, we use standard algorithms (Clark & Curran, 2007), as described below. The parsing, or inference, problem is to find the most likely logical form z given a sentence x, assuming the parameters θ and lexicon Λ are known: f (x) = arg max p(z|x; θ, Λ) z (2) where the probability of the logical form is found by summing over all parses that produce it: X p(z|x; θ, Λ) = p(y, z|x; θ, Λ) (3) y In this approach the distribution over parse trees y is modeled as a hidden variable. The sum over parses in Eq. 3 can be calculated efficiently using the inside-outside algorithm with a CKY-style parsing algorithm. To estimate the parameters themselves, we use"
D10-1119,P06-2034,0,0.0924891,"007) developed a variant of WASP (λ-WASP ) specifically designed for this alternate representation. Zettlemoyer & Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. Our approach eliminates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) consider the challenging problem of constructing broad-coverage semantic representatio"
D10-1119,P06-1115,0,0.935826,"g representations, or both. Here, we develop an approach that can learn to map any natural language to a wide variety of logical representations of linguistic meaning. In addition to data like the above, this approach can also learn from examples such as: Sentence: Meaning: Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: hangi eyaletin texas ye siniri vardir answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008). The reason for generalizing to multiple languages is obvious. The need to learn over multiple representat"
D10-1119,D08-1082,1,0.902357,"natural language to a wide variety of logical representations of linguistic meaning. In addition to data like the above, this approach can also learn from examples such as: Sentence: Meaning: Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: hangi eyaletin texas ye siniri vardir answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008). The reason for generalizing to multiple languages is obvious. The need to learn over multiple representations arises from the fact that there is no standard representation for logical f"
D10-1119,P96-1008,0,0.0802861,"en designed to recover lambda-calculus representations. For example, Wong & Mooney (2007) developed a variant of WASP (λ-WASP ) specifically designed for this alternate representation. Zettlemoyer & Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. Our approach eliminates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) con"
D10-1119,J03-1002,0,0.00259079,"tures: For each lexical item L ∈ Λ, we include a feature φL that fires when L is used. Second, we include semantic features that are functions of the output logical expression z. Each time a predicate p in z takes an argument a with type T (a) in position i it triggers two binary indicator features: φ(p,a,i) for the predicate-argument relation; and φ(p,T (a),i) for the predicate argument-type relation. Initialization The weights for the semantic features are initialized to zero. The weights for the lexical features are initialized according to coocurrance statistics estimated with the Giza++ (Och & Ney, 2003) implementation of IBM Model 1. We compute translation scores for (word, constant) pairs that cooccur in examples in the training data. The initial weight for each φL is set to ten times the average score over the (word, constant) pairs in L, except for the weights of seed lexical entries in ΛN P which are set to 10 (equivalent to the highest possible coocurrence score). We used the learning rate α0 = 1.0 and cooling rate c = 10−5 in all training scenarios, and ran the algorithm for T = 20 iterations. These values were selected with cross validation on 1230 the Geo880 development set, describe"
D10-1119,N06-1056,0,0.88739,"both. Here, we develop an approach that can learn to map any natural language to a wide variety of logical representations of linguistic meaning. In addition to data like the above, this approach can also learn from examples such as: Sentence: Meaning: Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: hangi eyaletin texas ye siniri vardir answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008). The reason for generalizing to multiple languages is obvious. The need to learn over multiple representations arises from the"
D10-1119,P07-1121,0,0.766071,"Λ) [φ(xi , y, zi )] −Ep(y,z|xi ;θ,Λ) [φ(xi , y, z)] • Set θ = θ + γ∆ Output: Lexicon Λ and parameters θ. Figure 2: The UBL learning algorithm. WASP system (Wong & Mooney, 2006) uses statistical machine translation techniques to learn synchronous context free grammars containing both words and logic. Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation. These algorithms are all language independent but representation specific. Other algorithms have been designed to recover lambda-calculus representations. For example, Wong & Mooney (2007) developed a variant of WASP (λ-WASP ) specifically designed for this alternate representation. Zettlemoyer & Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. Our approach eliminates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge &"
D10-1119,D07-1071,1,0.85741,"Missing"
D10-1119,P09-1110,1,0.900752,"Missing"
D11-1059,D10-1083,0,\N,Missing
D11-1059,D07-1031,0,\N,Missing
D11-1059,J93-2004,0,\N,Missing
D11-1059,N10-1083,0,\N,Missing
D11-1059,E03-1009,0,\N,Missing
D11-1059,W06-2920,0,\N,Missing
D11-1059,N06-1041,0,\N,Missing
D11-1059,P07-1094,1,\N,Missing
D11-1059,P10-2040,0,\N,Missing
D11-1059,D10-1056,1,\N,Missing
D11-1059,P08-1084,0,\N,Missing
D11-1059,J92-4003,0,\N,Missing
D11-1059,P02-1017,0,\N,Missing
D11-1059,P05-1044,0,\N,Missing
D11-1059,D07-1043,0,\N,Missing
D11-1059,P09-1057,0,\N,Missing
D11-1059,P06-3002,0,\N,Missing
D11-1059,J03-1002,0,\N,Missing
D11-1059,E95-1020,0,\N,Missing
D11-1059,dzeroski-etal-2006-towards,0,\N,Missing
D11-1059,erjavec-2004-multext,0,\N,Missing
D11-1115,W03-1013,0,0.0785745,"Missing"
D11-1115,J07-4004,0,0.305177,"Missing"
D11-1115,W04-3215,1,0.687832,"Missing"
D11-1115,J07-3004,1,0.931163,"Missing"
D11-1115,A94-1018,0,0.201786,"Missing"
D11-1115,J93-2004,0,0.0364022,"Missing"
D11-1115,N06-1020,0,0.0930292,"(S[dcl]NP),8 and S[wq]/(S[q]/NP)/N.9 For this experiment we focus on the subject WH-element extraction category (S[wq]/(S[dcl]NP))/N, as in Which cat is the grandmother?. This particular category was chosen as a point of investigation because it is OOL in CCGBank and is common enough to meaningfully evaluate. 5.2 Methods The baseline is the original StatCCG parser and lexicon. We also employ self-training (Charniak, 1997), in which a parser is used to parse a set of sentences, and then retrained using those output trees. Selftraining has had very little success in CCG applications hitherto. McClosky et al (2006) attribute success in self-training to a confluence of circum7 Object question category as in What is the Keystone State? question category as in What lays blue eggs? 9 Object WH-element extraction category as in What continent is Scotland in? 8 Subject 1253 stances particular to their learning setting, which has the benefit of a discriminative re-ranker, both in the parsing case and in the learning case (McClosky et al., 2008). We follow their recommendations that the best performance is achieved when all the training sentences are parsed at once, rather than incrementally. We evaluate the su"
D11-1115,C08-1071,0,0.0190156,"n which a parser is used to parse a set of sentences, and then retrained using those output trees. Selftraining has had very little success in CCG applications hitherto. McClosky et al (2006) attribute success in self-training to a confluence of circum7 Object question category as in What is the Keystone State? question category as in What lays blue eggs? 9 Object WH-element extraction category as in What continent is Scotland in? 8 Subject 1253 stances particular to their learning setting, which has the benefit of a discriminative re-ranker, both in the parsing case and in the learning case (McClosky et al., 2008). We follow their recommendations that the best performance is achieved when all the training sentences are parsed at once, rather than incrementally. We evaluate the success of CI in bootstrapping Wh-question categories from the out-of-domain corpus in two ways. First, we compare the CI output to the gold standard categories labelled in Rimmell and Clark (2008a). Second, we add the parsed questions into the training set, then retrain and finally retest the parser. The parser was initially trained on CCGBank §0221 with a word frequency threshold of 5.10 It produces partial parse charts in the"
D11-1115,P89-1013,0,0.460318,"et of partial parse conditions, and further to that, they do not allow for partial parses to be reanalysed within the learning framework. To that end, we have developed a learning algorithm that is capable of operating within the oneunknown-word-per-sentence learning setting established by the two baseline systems, that is able to invent new category types, and that is able to take advantage of the full generality of CCG. This section shows that it performs as well as the previous two systems on a toy corpus, and the next section proves that it more readily scales to natural language domains. Mellish (1989) established a two-stage bidirectional chart parser for diagnosing errors in input text. His method relied heavily on heuristic rules, and the only evaluation he did was on number of cycles needed for each type of error, and number of solutions produced. His method was designed for use in producing parses where the original parser failed, dealing with omissions, insertions, and misspelled/unknown words. The only method used to rank the possible solutions was heuristic scores. Kato (1994) implemented a revised system that used a generalised top-down parser, rather than a chart, and was able to"
D11-1115,D08-1050,0,0.655398,"r the three systems on the McGuffey corpus, training and testing on MG1. BF RB CI P 70.83 16.13 61.90 Top One R F 53.13 60.72 15.63 15.87 40.63 49.06 P 83.33 29.03 76.19 Top Ten R F 62.50 71.43 28.13 28.57 50.00 60.38 Table 2: Exp. III: Category match results for the three systems on the McGuffey corpus, training on MG1 and testing on MG2. Common categories (N, NP, N/N) are overrepresented in the test data, leading to higher BF scores. POS tags not available for MG2, so no POS baseline is reported. forces the fact that this is a domain-adaptation task. We use the same 500-sentence test set as Rimell and Clark (2008b). The test corpus consists of 488 questions, each starting with What, When, How, Who or Where. The learning corpus contains 1328 questions in a similar distribution. Only three out of the five categories needed to parse What-questions are present in the CCGBank seed lexicon: S[wq]/(S[q]/NP),7 , S[wq]/(S[dcl]NP),8 and S[wq]/(S[q]/NP)/N.9 For this experiment we focus on the subject WH-element extraction category (S[wq]/(S[dcl]NP))/N, as in Which cat is the grandmother?. This particular category was chosen as a point of investigation because it is OOL in CCGBank and is common enough to meanin"
D11-1115,W08-1307,0,0.0532579,"r the three systems on the McGuffey corpus, training and testing on MG1. BF RB CI P 70.83 16.13 61.90 Top One R F 53.13 60.72 15.63 15.87 40.63 49.06 P 83.33 29.03 76.19 Top Ten R F 62.50 71.43 28.13 28.57 50.00 60.38 Table 2: Exp. III: Category match results for the three systems on the McGuffey corpus, training on MG1 and testing on MG2. Common categories (N, NP, N/N) are overrepresented in the test data, leading to higher BF scores. POS tags not available for MG2, so no POS baseline is reported. forces the fact that this is a domain-adaptation task. We use the same 500-sentence test set as Rimell and Clark (2008b). The test corpus consists of 488 questions, each starting with What, When, How, Who or Where. The learning corpus contains 1328 questions in a similar distribution. Only three out of the five categories needed to parse What-questions are present in the CCGBank seed lexicon: S[wq]/(S[q]/NP),7 , S[wq]/(S[dcl]NP),8 and S[wq]/(S[q]/NP)/N.9 For this experiment we focus on the subject WH-element extraction category (S[wq]/(S[dcl]NP))/N, as in Which cat is the grandmother?. This particular category was chosen as a point of investigation because it is OOL in CCGBank and is common enough to meanin"
D11-1115,W99-0909,0,0.103003,"vised, it has access to an essentially unlimited amount of unlabelled data, and it can afford to skip any sentence that does not conform to the one-unseen-word restriction. Attempting two or more OOL words at a time from one sentence would compound the search space and the error rate. We do not address the much harder problem of hypothesising missing categories for known words, which should presumably be handled by quite other methods, such as prior offline generalization of the lexicon. 2.1 A Brute-force System One of the early lexical acquisition systems using Categorial Grammar was that of Watkinson and Manandhar (1999; 2000; 2001a; 2001b). This system attempted to simultaneously learn a CG lexicon and annotate unlabelled text with parse derivations. Using a stripped-down parser that only utilised the forward- and backward-application rules, they iteratively learned the lexicon from the feedback from online parsing. The system decided which parse was best based on the lexicon, and then decided which additions to the lexicon to make based on principles of compression. After each change, the system reexamined the parses for previous sentences and updated them to reflect the new lexicon. They report fully conv"
D11-1115,W01-0720,0,0.0492467,"Missing"
D11-1115,W03-2316,0,0.0301159,"agnosed the errors correctly, or whether the solution they offered was accurate. They also had to deal with cases where the error was ambiguous, for example, where an inserted word could be interpreted as a misspelling or vice-versa. Where Mellish uses the two-stage parsing process to complete malformed parses, we use it to diagnose unknown lexical items. In addition, we work on the scale of a full grammar and wide-coverage parser, using modern lexical corpora. Our method is a wrapper for a naive generative CCG parser StatOpenCCG (Christodoulopoulos, 2008), a statistical extension to OpenCCG (White and Baldridge, 2003). In the general case, the parser is trained on all the labelled data available in a particular learning setting, then the learner discovers new lexical items from unlabelled text. Like the brute force and rule-based systems, it is vulnerable CCG Combinator A/B B → A (&gt;) B AB → A (&lt;) A/C C/B → A/B (&gt;B) CB AC → AB (&lt;B) Inverse Combinator X B → A A/B X → A X AB → A B X → A X C/B → A/B A/C X → A/B X AC → AB CB X → AB ⇒ ⇒ ⇒ ⇒ ⇒ ⇒ ⇒ ⇒ X = A/B X=B X=B X = AB X = A/C X = C/B X = CB X = AC if v(B) ≤ 1 if v(B) ≤ 1 Figure 2: Derivation of inverse combinators  P(HeadRight|R) P(HeadLe f t|R)"
D11-1140,C04-1180,1,0.278766,"s learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the problem and an overview of the learning approach. Problem We will learn a semantic parser that takes a sentences x and returns a logical form z representing its underlying meaning. We assume we have input data {(xi , zi )|i = 1 . . . n} containing sentences xi and logical forms zi , for example xi =“Show me flights to Boston” and zi = λ x. f light(x) ∧to(x, bos). Model We will represent"
D11-1140,P10-1129,1,0.585752,"e recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified"
D11-1140,W03-1013,0,0.00562958,"y available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the problem and an overview of the learning approach. Problem We will learn a semantic parser that takes a sentences x and returns a logical form z r"
D11-1140,J07-4004,0,0.026062,"c parsing. Following previous work (Kwiatkowski et al., 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. However, instead of constructing fully specified lexical items for the learned grammar, we automatically generate sets of lexemes and lexical templates to model each example. This is a difficult learning problem, since the CCG analyses that 1A related tactic is commonly used in wide-coverage CCG parsers derived from treebanks, such as work by Hockenmaier and Steedman (2002) and Clark and Curran (2007). These parsers make extensive use of category-changing unary rules, to avoid data sparsity for systematically related categories (such as those related by type-raising). We will automatically learn to represent these types of generalizations in the factored lexicon. 1513 are required to construct the final meaning representations are not explicitly labeled in the training data. Instead, we model them with hidden variables and develop an online learning approach that simultaneously estimates the parameters of a log-linear parsing model, while inducing the factored lexicon. We evaluate the appr"
D11-1140,W10-2903,0,0.551773,"ncluding ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and fro"
D11-1140,P06-2034,0,0.0236428,"guage-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. The learning approach we develop for inducing factored lexicons is also language independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on uns"
D11-1140,P11-1149,0,0.158983,"ls (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a met"
D11-1140,P02-1043,1,0.652263,"obabilistic CCG grammars for semantic parsing. Following previous work (Kwiatkowski et al., 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. However, instead of constructing fully specified lexical items for the learned grammar, we automatically generate sets of lexemes and lexical templates to model each example. This is a difficult learning problem, since the CCG analyses that 1A related tactic is commonly used in wide-coverage CCG parsers derived from treebanks, such as work by Hockenmaier and Steedman (2002) and Clark and Curran (2007). These parsers make extensive use of category-changing unary rules, to avoid data sparsity for systematically related categories (such as those related by type-raising). We will automatically learn to represent these types of generalizations in the factored lexicon. 1513 are required to construct the final meaning representations are not explicitly labeled in the training data. Instead, we model them with hidden variables and develop an online learning approach that simultaneously estimates the parameters of a log-linear parsing model, while inducing the factored l"
D11-1140,D10-1119,1,0.165291,"ction for Semantic Parsing Tom Kwiatkowski∗ Luke Zettlemoyer† Sharon Goldwater∗ Mark Steedman∗ t.m.kwiatkowksi@sms.ed.ac.uk lsz@cs.washington.edu sgwater@inf.ed.ac.uk steedman@inf.ed.ac.uk † Computer ∗ School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Science & Engineering University of Washington Seattle, WA 98195 Abstract and the goal is to learn a grammar that can map new, unseen, sentences onto their corresponding meanings, or logical forms. One approach to this problem has developed algorithms for leaning probabilistic CCG grammars (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010). These grammars are well-suited to the task of semantic parsing, as they closely link syntax and semantics. They can be used to model a wide range of complex linguistic phenomena and are strongly lexicalized, storing all language-specific grammatical information directly with the words in the lexicon. For example, a typical learned lexicon might include entries such as: We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. Traditional CCG lexicons list lexical items that pai"
D11-1140,P09-1011,0,0.0264637,"ations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the"
D11-1140,P11-1060,0,0.573776,"chine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon a"
D11-1140,D08-1082,1,0.938058,"ed CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring. 1 Introduction Semantic parsers automatically recover representations of meaning from natural language sentences. Recent work has focused on learning such parsers directly from corpora made up of sentences paired with logical meaning representations (Kate et al., 2005; Kate and Mooney, 2006; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2005, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). For example, in a flight booking domain we might have access to training examples such as: Sentence: Meaning: I want flights from Boston λ x. f light(x) ∧ f rom(x, bos) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) f light ` N : λ x. f light(x) f light ` N/(S|NP) : λ f λ x. f light(x) ∧ f (x) f light ` NN : λ f λ x. f light(x) ∧ f (x) f are ` N : λ x.cost(x) f are ` N/(S|NP) : λ f λ x.cost(x) ∧ f (x) f are ` NN : λ f λ x.cost(x) ∧ f (x) Boston ` NP : bos Boston ` NN : λ f λ x. f rom(x, bos) ∧ f (x) New York ` NP : nyc New York ` NN : λ f λ x. f rom(x, nyc) ∧ f (x) A"
D11-1140,P96-1008,0,0.0661923,"d an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. The learning approach we develop for inducing factored lexicons is also language independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011"
D11-1140,P06-2080,0,0.0157424,"ge independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environme"
D11-1140,J03-1002,0,0.00538065,"elation; and φ(p,Ty(a),i) for the predicate argument-type relation. Boolean operator features look at predicates that occurr together in conjunctions and disjunctions. For each variable vi that fills argument slot i in two conjoined predicates p1 and p2 we introduce a binary indicator feature φcon j(i,p1 ,p2 ) . We introduce similar features φdis j(i,p1 ,p2 ) for variables vi that are shared by predicates in a disjunction. Initialization The weights for lexeme features are initialized according to coocurrance statistics between words and logical constants. These are estimated with the Giza++ (Och and Ney, 2003) implementation of IBM Model 1. The initial weights for templates are set by adding −0.1 for each slash in the syntactic category and −2 if the template contains logical constants. Features on lexeme-template pairs and all parse features are initialized to zero. Systems We compare performance to all recentlypublished, directly-comparable results. For GeoQuery, this includes the ZC05, ZC07 (Zettlemoyer System ZC07 UBL FUBL Exact Match Rec. Pre. F1 74.4 87.3 80.4 65.6 67.1 66.3 81.9 82.1 82.0 System ZC07 HY06 UBL FUBL Exact Match Rec. Pre. F1. 84.6 85.8 85.2 71.4 72.1 71.7 82.8 82.8 82.8 Partial"
D11-1140,D09-1001,0,0.0645657,"(2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the problem and an overview of the learning approach."
D11-1140,P10-1031,0,0.0256343,"Missing"
D11-1140,P06-1115,0,0.933771,"odel systematic variation in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring. 1 Introduction Semantic parsers automatically recover representations of meaning from natural language sentences. Recent work has focused on learning such parsers directly from corpora made up of sentences paired with logical meaning representations (Kate et al., 2005; Kate and Mooney, 2006; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2005, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). For example, in a flight booking domain we might have access to training examples such as: Sentence: Meaning: I want flights from Boston λ x. f light(x) ∧ f rom(x, bos) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) f light ` N : λ x. f light(x) f light ` N/(S|NP) : λ f λ x. f light(x) ∧ f (x) f light ` NN : λ f λ x. f light(x) ∧ f (x) f are ` N : λ x.cost(x) f are ` N/(S|NP) : λ f λ x.cost(x) ∧ f (x) f are ` NN : λ f λ x.cost(x) ∧ f (x) Boston ` NP : bos Boston ` NN : λ f λ x. f rom(x,"
D11-1140,W00-1317,0,0.0167049,"od, but their approach does not scale well to challenging, unedited sentences. The learning approach we develop for inducing factored lexicons is also language independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), inductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006). More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific i"
D11-1140,P10-1083,0,0.0173826,"d on training semantic parsers without supervision in the form of logical-form annotations. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers which are more easily available. Goldwasser et al. (2011) present work on unsupervised learning of logical form structure. However, all of these systems require significantly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the tra"
D11-1140,W99-0909,0,0.0192112,"antly more domain and language specific initialization than the approach presented here. Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010). There is also related work that uses the CCG grammar formalism. Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal–form parse trees. Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons. Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully–specified CCG derivations in the training data. 3 Overview of the Approach Here we give a formal definition of the problem and an overview of the learning approach. Problem We will learn a semantic parser that takes a sentences x and returns a logical form z representing its underlying meaning. We assume we have input data {(xi , zi )|i = 1 . . . n} containing sentences xi and logical forms zi , for example xi =“"
D11-1140,N06-1056,0,0.926806,"on in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring. 1 Introduction Semantic parsers automatically recover representations of meaning from natural language sentences. Recent work has focused on learning such parsers directly from corpora made up of sentences paired with logical meaning representations (Kate et al., 2005; Kate and Mooney, 2006; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2005, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). For example, in a flight booking domain we might have access to training examples such as: Sentence: Meaning: I want flights from Boston λ x. f light(x) ∧ f rom(x, bos) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) f light ` N : λ x. f light(x) f light ` N/(S|NP) : λ f λ x. f light(x) ∧ f (x) f light ` NN : λ f λ x. f light(x) ∧ f (x) f are ` N : λ x.cost(x) f are ` N/(S|NP) : λ f λ x.cost(x) ∧ f (x) f are ` NN : λ f λ x.cost(x) ∧ f (x) Boston ` NP : bos Boston ` NN : λ f λ x. f rom(x, bos) ∧ f (x) New York"
D11-1140,P07-1121,0,0.892548,"rsing datasets: GeoQuery, which is made up of natural language queries to a database of geographical information; and Atis, which contains natural language queries to a flight booking system. The Geo880 dataset has 880 (English-sentence, logicalform) pairs split into a training set of 600 pairs and a test set of 280. The Geo250 data is a subset of the Geo880 sentences that have been translated into Japanese, Spanish and Turkish as well as the original English. We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007). The Atis dataset contains 5410 (sentence, logical-form) pairs split into a 4480 example training set, a 480 example development set and a 450 example test set. 1519 Evaluation Metrics We report exact match Recall (percentage of sentences for which the correct logical-form was returned), Precision (percentage of returned logical-forms that are correct) and F1 (harmonic mean of Precision and Recall). For Atis we also report partial match Recall (percentage of correct literals returned), Precision (percentage of returned literals that are correct) and F1, computed as described by Zettlemoyer an"
D11-1140,D07-1071,1,0.893493,"three lexical items separately is inefficient, since each word of this class (such as “fare”) will require three similarly structured lexical entries differing only in predicate name. There may also be systemtatic semantic variation between entries for a certain class of words. For example, in (6) “Boston” is paired with the constant bos that represents its meaning. However, item (7) also adds the predicate from to the logical form. This might be used to analyse somewhat elliptical, unedited sentences such as “Show me flights Boston to New York,” which can be challenging for semantic parsers (Zettlemoyer and Collins, 2007). This paper builds upon the insight that a large proportion of the variation between lexical items for a given class of words is systematic. Therefore it should be represented once and applied to a small set of basic lexical units. 1 We develop a factored lexicon that captures this insight by distinguishing lexemes, which pair words with logical constants, from lexical templates, which map lexemes to full lexical items. As we will see, this can lead to a significantly more compact lexicon that can be learned from less data. Each word or phrase will be associated with a few lexemes that can be"
D11-1140,P09-1110,1,0.855677,"ree of success on all of these datasets. 2 Related work There has been significant previous work on learning semantic parsers from training sentences labelled with logical form meaning representations. We extend a line of research that has addressed this problem by developing CCG grammar induction techniques. Zettlemoyer and Collins (2005, 2007) presented approaches that use hand generated, English-language specific rules to generate lexical items from logical forms as well as English specific type-shifting rules and relaxations of the CCG combinators to model spontaneous, unedited sentences. Zettlemoyer and Collins (2009) extends this work to the case of learning in context dependent environments. Kwiatkowski et al. (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. The learning approach we develop for inducing factored lexicons is also language independent, but scales well to these challenging sentences. There have been a number of other approaches for learning semantic parsers, including ones based on machine translat"
D12-1007,P08-1071,0,0.0201407,"ata, and compute similarity measures between these synthetically produced dialogs and real dialogs. Georgila et al. (2006) use a scoring function to evaluate synthetic dialogs using accuracy, precision, recall, and perplexity, while Schatzmann et al. (2007b) rely on dialog completion rates. Williams (2008) use a Cramer–von Mises test, a hypothesis test to determine whether simulated and real dialogs are significantly different, while Janarthanam and Lemon (2009) use Kullback Leibler Divergence between the empirical distributions over acts in real and simulated dialogs. Singh et al. (2000) and Ai and Litman (2008) judge the consistency of human quality ranked synthetic dialogs generated by different simulators interacting with the IT-SPOKE dialog system. Schatzmann et al. (2007b) use a simulator to train a statistical dialog manager and then evaluate the learned policy. Because this only indirectly evaluates the simulator, it is inappropriate as a sole measure of quality. There has been far less evaluation of simulators without a dialog manager. The main approach is to compute precision and recall on an utterance ba73 sis, which is intended to measure the similarity between real user responses in the c"
D12-1007,W09-3950,0,0.1649,"experimental biases. We demonstrate the efficacy of our model on two tasks, and compare it to two other approaches. Firstly we use a standard bigram model as conceived by Eckert et al. (1997) and Levin and Pieraccini (2000); secondly we compare to a probabilistic goalbased simulator where the goals are string literals, as envisaged by Scheffler and Young (2002) and Schatzmann et al. (2007b). We demonstrate substantial improvement over these models in terms of predicting heldout data on two standard dialog resources: DARPA Communicator (Levin et al., 2000; Georgila et al., 2005b) and Let’s Go (Black and Eskenazi, 2009). 2 2.1 Related Work Related Work on User Simulation User simulation as a stochastic process was first envisioned by Eckert et al. (1997): their Bigram model conditions user utterances exclusively on the preceding machine utterance. This was extended by Levin and Pieraccini (2000), who manually restrict 72 the model to estimating “sensible” pairs of user and machine utterances by assigning all others probability zero. Bigram models ensure that a locally sensible response to a machine utterance is provided by the simulator; however, they do not ensure that it provides responses consistent with"
D12-1007,W09-3916,0,0.0141331,"gh the interaction of the user simulator with some dialog manager. Schatzmann et al. (2005) hand-craft a simple deterministic dialog manager based on finite automata, and compute similarity measures between these synthetically produced dialogs and real dialogs. Georgila et al. (2006) use a scoring function to evaluate synthetic dialogs using accuracy, precision, recall, and perplexity, while Schatzmann et al. (2007b) rely on dialog completion rates. Williams (2008) use a Cramer–von Mises test, a hypothesis test to determine whether simulated and real dialogs are significantly different, while Janarthanam and Lemon (2009) use Kullback Leibler Divergence between the empirical distributions over acts in real and simulated dialogs. Singh et al. (2000) and Ai and Litman (2008) judge the consistency of human quality ranked synthetic dialogs generated by different simulators interacting with the IT-SPOKE dialog system. Schatzmann et al. (2007b) use a simulator to train a statistical dialog manager and then evaluate the learned policy. Because this only indirectly evaluates the simulator, it is inappropriate as a sole measure of quality. There has been far less evaluation of simulators without a dialog manager. The m"
D12-1007,W10-4323,0,0.026233,"Missing"
D12-1007,2005.sigdial-1.6,0,0.442845,"restrict the variability in user behaviour that can be accommodated. Furthermore, because these approaches do not define a complete probability distribution over user behaviour, they restrict possibilities for their evaluation, a point to which we now turn. 2.2 Related Work on Simulator Evaluation No standardised metric of evaluation has been established for user simulators largely because they have been so inextricably linked to dialog managers. The most popular method of evaluation relies on generating synthetic dialogs through the interaction of the user simulator with some dialog manager. Schatzmann et al. (2005) hand-craft a simple deterministic dialog manager based on finite automata, and compute similarity measures between these synthetically produced dialogs and real dialogs. Georgila et al. (2006) use a scoring function to evaluate synthetic dialogs using accuracy, precision, recall, and perplexity, while Schatzmann et al. (2007b) rely on dialog completion rates. Williams (2008) use a Cramer–von Mises test, a hypothesis test to determine whether simulated and real dialogs are significantly different, while Janarthanam and Lemon (2009) use Kullback Leibler Divergence between the empirical distribu"
D12-1007,N07-2038,0,0.569612,"babilistic formulation such as we propose allows us to evaluate our models intrinsically using standard machine learning metrics, and without reference to a specific manager, thus breaking the circularity, and guarding against such experimental biases. We demonstrate the efficacy of our model on two tasks, and compare it to two other approaches. Firstly we use a standard bigram model as conceived by Eckert et al. (1997) and Levin and Pieraccini (2000); secondly we compare to a probabilistic goalbased simulator where the goals are string literals, as envisaged by Scheffler and Young (2002) and Schatzmann et al. (2007b). We demonstrate substantial improvement over these models in terms of predicting heldout data on two standard dialog resources: DARPA Communicator (Levin et al., 2000; Georgila et al., 2005b) and Let’s Go (Black and Eskenazi, 2009). 2 2.1 Related Work Related Work on User Simulation User simulation as a stochastic process was first envisioned by Eckert et al. (1997): their Bigram model conditions user utterances exclusively on the preceding machine utterance. This was extended by Levin and Pieraccini (2000), who manually restrict 72 the model to estimating “sensible” pairs of user and machi"
D12-1007,2007.sigdial-1.48,0,0.68233,"babilistic formulation such as we propose allows us to evaluate our models intrinsically using standard machine learning metrics, and without reference to a specific manager, thus breaking the circularity, and guarding against such experimental biases. We demonstrate the efficacy of our model on two tasks, and compare it to two other approaches. Firstly we use a standard bigram model as conceived by Eckert et al. (1997) and Levin and Pieraccini (2000); secondly we compare to a probabilistic goalbased simulator where the goals are string literals, as envisaged by Scheffler and Young (2002) and Schatzmann et al. (2007b). We demonstrate substantial improvement over these models in terms of predicting heldout data on two standard dialog resources: DARPA Communicator (Levin et al., 2000; Georgila et al., 2005b) and Let’s Go (Black and Eskenazi, 2009). 2 2.1 Related Work Related Work on User Simulation User simulation as a stochastic process was first envisioned by Eckert et al. (1997): their Bigram model conditions user utterances exclusively on the preceding machine utterance. This was extended by Levin and Pieraccini (2000), who manually restrict 72 the model to estimating “sensible” pairs of user and machi"
D12-1007,P08-1000,0,\N,Missing
D13-1064,W13-2322,0,0.0588092,"mation retrieval. Regardless of whether it is even possible to create such a semantics, we show that an incomplete version can be useful for downstream tasks. Semantic machine translation aims to map a source language to a language-independent meaning representation, and then generate the target language translation from this. It is hoped this would alleviate the difficulties of simpler models when translating between languages with very different word ordering and syntax (Vauquois, 1968). Despite many attempts to define interlingual representations (Mitamura et al., 1991; Beale et al., 1995; Banarescu et al., 2013), state-of-the-art machine translation still uses phrase-based models (Koehn et al., 2007). The major obstacle to defining interlinguas has been devising a meaning representation that is languageindependent, but capable of expressing the limitless number of meanings that natural languages can express (Dorr et al., 2004). Our approach avoids this problem by utilizing the methods of distributional semantics. Recent work has shown that paraphrases of expressions can be learned by clustering those with similar arguments (Poon and Domingos, 2009; Yao et al., 2011; Lewis and Steedman, 2013)—for exam"
D13-1064,P05-1074,0,0.0320226,"d computing their overall similarity. Like us, they compute similarity between expressions in different languages based on named-entity arguments and clustering (unlike us, they also rely on machine translation for computing similarity). A key difference is that their system aims to understand the overall relation between an entity-pair based on many observations, whereas our approach attempts to understand each sentence individually (as is required for tasks such as translation). Various recent papers have explored the relationship between translation and monolingual paraphrases —for example Bannard and Callison-Burch (2005) create paraphrases by pivoting through a foreign translation, and Callison-Burch et al. (2006) show that including monolingual paraphrases improves the quality of translation by reducing sparsity. The success of these approaches depends on the many-to-many relationship between equivalent expressions in different languages. Our approach aims to model this relationship explicitly by clustering all equivalent paraphrases in different languages. Current state-of-the-art machine translation systems circumvent the problem of full semantic interpretation, by using phrase-based models learnt 689 from"
D13-1064,P11-1062,0,0.0791571,"-entity mentions, we perform some simple heuristic co-reference—based on word-overlap with previously mentioned entities in the document, whether the mention name is the title of a Wikipedia article, or whether the mention name is a Freebase (Bollacker et al., 2008) alias of an entity. We emphasise that this does not mean our approach is only applicable to the Wikipedia corpus. 4.2 Entity Typing It has become standard in clustering approaches to distributional semantics to assign types to predicates before clustering, and only cluster predicates with the same type (Schoenmackers et al., 2010; Berant et al., 2011; Yao et al., 2012). This is useful for resolving ambiguity—for example the phrase born in may express a place-of-birth or date-of-birth relation depending on whether its second argument has a LOC or DAT type. Ambiguous expressions may translate differently in other languages—for example, the two interpretations of was born in translate in French as est n´e a` and est n´e en respectively. The type of a predicate is determined by the type of its arguments, and predicates with different types are treated as distinct. Lewis and Steedman (2013) induce an unsupervised model of entity types using La"
D13-1064,W06-3812,0,0.0631066,"even if predicates in two different languages are truth-conditionally equivalent, the language biases the sample of entity-pairs found in a corpus. For example, the French verb e´ crire may contain more French author/book pairs than the English equivalent write. This difference can make the verbs appear to represent different predicates to the clustering algorithm. Our two-step approach also means that advances in monolingual clustering should directly lead to improved crosslingual clusters. 5.1 Monolingual Clustering Following Lewis and Steedman (2013), we use the Chinese Whispers algorithm (Biemann, 2006) for monolingual clustering—summarized in Algorithm 1. The algorithm is non-parametric, meaning that the number of relation clusters is induced from the data, and highly scalable. We create a separate graph for each type of predicate in each language—for example, predicates between types AUTHOR and BOOK in French (so only predicates with the same type will be clustered). We create one node per predicate in the graph, and edges represent the distributional similarity between the predicates. The distributional similarity between a pair of predicates is calculated as the cosine-similarity of thei"
D13-1064,W09-0434,0,0.0233945,"ess of these approaches depends on the many-to-many relationship between equivalent expressions in different languages. Our approach aims to model this relationship explicitly by clustering all equivalent paraphrases in different languages. Current state-of-the-art machine translation systems circumvent the problem of full semantic interpretation, by using phrase-based models learnt 689 from large parallel corpora (Brown et al., 1993). Although this approach has been very successful, it has significant limitations—for example, when translating between languages with very different wordorders (Birch et al., 2009), or with little parallel text. Semantic machine translation aims to map the source language to an interlingual semantic representation, and then generate the target language sentence from this. Jones et al. (2012) show how this can be done on a small dataset using hyperedge replacement grammars. A major obstacle to this is designing a suitable meaning representation, which involves choosing a set of primitive concepts which are abstract enough to be capable of expressing meaning in any language (Dorr et al., 2004). A recent proposal for this is the Abstract Meaning Representation (Banarescu e"
D13-1064,J93-2003,0,0.019304,"voting through a foreign translation, and Callison-Burch et al. (2006) show that including monolingual paraphrases improves the quality of translation by reducing sparsity. The success of these approaches depends on the many-to-many relationship between equivalent expressions in different languages. Our approach aims to model this relationship explicitly by clustering all equivalent paraphrases in different languages. Current state-of-the-art machine translation systems circumvent the problem of full semantic interpretation, by using phrase-based models learnt 689 from large parallel corpora (Brown et al., 1993). Although this approach has been very successful, it has significant limitations—for example, when translating between languages with very different wordorders (Birch et al., 2009), or with little parallel text. Semantic machine translation aims to map the source language to an interlingual semantic representation, and then generate the target language sentence from this. Jones et al. (2012) show how this can be done on a small dataset using hyperedge replacement grammars. A major obstacle to this is designing a suitable meaning representation, which involves choosing a set of primitive conce"
D13-1064,W06-2920,0,0.00535739,"43(william shakespeare, macbeth) Figure 1: Example showing how our system can map sentences in different languages to the same meaning represen. tation, assuming we have clustered the equivalent predicates writearg0:PER,arg1:BOOK and e´ criresub j:PER,ob j:BOOK Lewis and Steedman (2013), clusters derived from the output from the parser can be integrated into the lexicon, allowing us to build logical forms which capture both operator and lexical semantics. Accurate CCG syntactic parsers are currently only available for English, whereas dependency treebanks and parsers exist for many languages (Buchholz and Marsi, 2006). Consequently, for French we use the dependency path representation, which captures the nodes and edges connecting two named entities in a dependency parse. The extraction of these paths is language-independent, and does not depend on the dependency grammar used, which means our approach could be adapted to new languages with minimal work. 4 4.1 Entity Semantics Entity Linking As discussed, our approach assumes that semantically similar predicates will have similar argument entities. This requires us to be able to identify coreferring entities across languages during training. In 683 the mono"
D13-1064,N06-1003,0,0.0325134,"ent languages based on named-entity arguments and clustering (unlike us, they also rely on machine translation for computing similarity). A key difference is that their system aims to understand the overall relation between an entity-pair based on many observations, whereas our approach attempts to understand each sentence individually (as is required for tasks such as translation). Various recent papers have explored the relationship between translation and monolingual paraphrases —for example Bannard and Callison-Burch (2005) create paraphrases by pivoting through a foreign translation, and Callison-Burch et al. (2006) show that including monolingual paraphrases improves the quality of translation by reducing sparsity. The success of these approaches depends on the many-to-many relationship between equivalent expressions in different languages. Our approach aims to model this relationship explicitly by clustering all equivalent paraphrases in different languages. Current state-of-the-art machine translation systems circumvent the problem of full semantic interpretation, by using phrase-based models learnt 689 from large parallel corpora (Brown et al., 1993). Although this approach has been very successful,"
D13-1064,candito-etal-2010-statistical,0,0.0440034,"Missing"
D13-1064,P04-1014,0,0.116492,"same argument types. 6 (r1 ,r2 )∈RL1 ×RL2 A ←− A ∪ {(r1, r2)}; RL1 ←− RL1 /{r1}; RL2 ←− RL2 /{r2}; end Algorithm 2: Cluster alignment algorithm English X invades Y r 5.2 Data: Sets of monolingual relation clusters RL1 and RL2 Result: An alignment between the monolingual clusters A A ←− {}; while RL1 6= {} ∧ RL2 6= {} do (r1, r2) ←− arg max sim(r1, r2); Cross Lingual Question Answering Experiments X is a skyscraper in Y X is a novel by Y X joins Y X is a member of Y Table 1: Some example cross-lingual clusters. Predicates are given in a human-readable form, and predicate types are suppressed. (Clark and Curran, 2004). The French corpus is tagged with MElt (Denis et al., 2009) and parsed with MaltParser (Nivre et al., 2007), trained on the French Treebank (Candito et al., 2010). Wikipedia markup is filtered using Wikiprep (Gabrilovich and Markovitch, 2007)—replacing internal links with the name of their target article, to help entity linking. Some example clusters learnt by our model are shown in Table 1. We find that the cross-lingual clusters typically contain more French expressions than English, possibly due to the differing sizes of the corpora—adjusting the parameters in Section 5 results in larger c"
D13-1064,W04-3215,1,0.690906,"antically equivalent between languages. It is possible for entities to have multiple types (see Section 4.2), and answers are ranked by the number of types in which the entailment relation is predicted to hold. 3 Questions are given in a declarative form, to make the tasks simpler for the machine translation baseline. We found the machine translation performed poorly on questions such as What is Obama the president of?, as inverted word-orders and long-range dependencies are difficult to handle with re-ordering models and language models (though are straightforward to handle for a CCG system (Clark et al., 2004)). We find that machine translation performs much better on declarative equivalents, such as: Obama is the president of X. 686 6.2 Baseline Our baseline makes use of the Moses machine translation system (Koehn et al., 2007), and is similar to previous approaches to cross-lingual question answering such as Ahn et al. (2004). We train a Moses model on the Europarl corpus (Koehn, 2005). First, the question is translated from language L to L’, taking the 50-best translations. As the questions are typically shorter than corpus sentences, this is substantially easier for the machine-translation than"
D13-1064,Y09-1013,0,0.0195296,"Missing"
D13-1064,D11-1142,0,0.0555461,"uire parallel text—but it does impose some additional constraints on language resources. Our approach requires: • A large amount of factual text, as we rely on the same facts being expressed in different languages. We use Wikipedia, which contains ar682 ticles in 250 languages, including 121 with at least 10,000 articles.1 Other domains, such as Newswire, may also be effective. • A method for extracting binary relations from sentences. This is straightforward from dependency parses, which are available for many languages. It is also possible without a parser, with some language-specific work (Fader et al., 2011). We describe our approach in Section 3. • A method for linking entities in the training data to some canonical representation. McNamee et al. (2011) report good results on this task in 21 languages. We describe our method for this in Section 4.1. 3 Predicate Extraction Our method relies on extracting binary predicates between entities from sentences. Various representations have been suggested for binary predicates, such as Reverb patterns (Fader et al., 2011), dependency paths (Lin and Pantel, 2001; Yao et al., 2011), and binarized predicate-argument relations derived from a CCG-parse (Lewis"
D13-1064,C12-1083,0,0.0124315,"es in different languages. Current state-of-the-art machine translation systems circumvent the problem of full semantic interpretation, by using phrase-based models learnt 689 from large parallel corpora (Brown et al., 1993). Although this approach has been very successful, it has significant limitations—for example, when translating between languages with very different wordorders (Birch et al., 2009), or with little parallel text. Semantic machine translation aims to map the source language to an interlingual semantic representation, and then generate the target language sentence from this. Jones et al. (2012) show how this can be done on a small dataset using hyperedge replacement grammars. A major obstacle to this is designing a suitable meaning representation, which involves choosing a set of primitive concepts which are abstract enough to be capable of expressing meaning in any language (Dorr et al., 2004). A recent proposal for this is the Abstract Meaning Representation (Banarescu et al., 2013), which uses English verbs as a set of predicates. This is a less abstract form of semantic interpretation than our proposal, as semantically equivalent paraphrases may be given a different representati"
D13-1064,E12-1014,0,0.0152799,"ss abstract form of semantic interpretation than our proposal, as semantically equivalent paraphrases may be given a different representation. Such an approach also relies on annotating large amounts of text with the semantic representation—whereas our unsupervised approach offers a way to build such an interlingua using only a method for extracting predicates from sentences. Whilst almost all recent work on machinetranslation has relied on parallel text, there have been several interesting approaches that do not. Rapp (1999) learns to translate words based on small seed bilingual dictionary. Klementiev et al. (2012a) exploit a variety of interesting indirect sources of information to learn a lexicon—for example assuming that equivalent Wikipedia articles in different languages will use semantically similar words. The Polylingual Topic Model (Mimno et al., 2009) makes use of similar intuitions. Whilst we exploit equivalent Wikipedia articles for entity linking, we do not require aligned articles. Incorporating such techniques into our model would be a natural next step, allowing us to learn a more complete lexicon. To our knowledge, ours is the first approach to learn to translate semantic relations, rat"
D13-1064,C12-1089,0,0.0177127,"ss abstract form of semantic interpretation than our proposal, as semantically equivalent paraphrases may be given a different representation. Such an approach also relies on annotating large amounts of text with the semantic representation—whereas our unsupervised approach offers a way to build such an interlingua using only a method for extracting predicates from sentences. Whilst almost all recent work on machinetranslation has relied on parallel text, there have been several interesting approaches that do not. Rapp (1999) learns to translate words based on small seed bilingual dictionary. Klementiev et al. (2012a) exploit a variety of interesting indirect sources of information to learn a lexicon—for example assuming that equivalent Wikipedia articles in different languages will use semantically similar words. The Polylingual Topic Model (Mimno et al., 2009) makes use of similar intuitions. Whilst we exploit equivalent Wikipedia articles for entity linking, we do not require aligned articles. Incorporating such techniques into our model would be a natural next step, allowing us to learn a more complete lexicon. To our knowledge, ours is the first approach to learn to translate semantic relations, rat"
D13-1064,W06-3114,0,0.0203476,"ally supply a translation of the named-entity in the question (based on the Freebase entity name translation), to avoid penalizing the translation system for failing to translate named-entities that have not been seen in its training data. These patterns are then used to find answers to the questions. Answers are ranked by the score of the best translation that produced the pattern. Figure 2 illustrates this pipeline. The choice of languages is very favourable to the machine-translation system, English and French have similar word-order, and there is a large amount of parallel text available (Koehn and Monz, 2006). Our system works with any word-order, and does not require parallel text for training, so we would expect better performance relative to machine-translation on other language pairs. Future work will experiment with more diverse languages. The sentences to be translated are also very short, reducing the potential for error. 6.3 Results Results are shown in Table 3, based on a sample of 100 answers from the output of each of the systems. Unsurprisingly, the machine-translation has high accuracy on this task, given the choice of languages and the short queries. Pleasingly, our clusters achieve"
D13-1064,P07-2045,0,0.00373569,"w that an incomplete version can be useful for downstream tasks. Semantic machine translation aims to map a source language to a language-independent meaning representation, and then generate the target language translation from this. It is hoped this would alleviate the difficulties of simpler models when translating between languages with very different word ordering and syntax (Vauquois, 1968). Despite many attempts to define interlingual representations (Mitamura et al., 1991; Beale et al., 1995; Banarescu et al., 2013), state-of-the-art machine translation still uses phrase-based models (Koehn et al., 2007). The major obstacle to defining interlinguas has been devising a meaning representation that is languageindependent, but capable of expressing the limitless number of meanings that natural languages can express (Dorr et al., 2004). Our approach avoids this problem by utilizing the methods of distributional semantics. Recent work has shown that paraphrases of expressions can be learned by clustering those with similar arguments (Poon and Domingos, 2009; Yao et al., 2011; Lewis and Steedman, 2013)—for example learning that X wrote Y and X is the author of Y are equivalent if they appear in a co"
D13-1064,2005.mtsummit-papers.11,0,0.015049,"hat is Obama the president of?, as inverted word-orders and long-range dependencies are difficult to handle with re-ordering models and language models (though are straightforward to handle for a CCG system (Clark et al., 2004)). We find that machine translation performs much better on declarative equivalents, such as: Obama is the president of X. 686 6.2 Baseline Our baseline makes use of the Moses machine translation system (Koehn et al., 2007), and is similar to previous approaches to cross-lingual question answering such as Ahn et al. (2004). We train a Moses model on the Europarl corpus (Koehn, 2005). First, the question is translated from language L to L’, taking the 50-best translations. As the questions are typically shorter than corpus sentences, this is substantially easier for the machine-translation than translating the corpus. These are then parsed, and patterns are extracted (as in Section 3). We also manually supply a translation of the named-entity in the question (based on the Freebase entity name translation), to avoid penalizing the translation system for failing to translate named-entities that have not been seen in its training data. These patterns are then used to find an"
D13-1064,P13-1117,0,0.0282399,"imilar intuitions. Whilst we exploit equivalent Wikipedia articles for entity linking, we do not require aligned articles. Incorporating such techniques into our model would be a natural next step, allowing us to learn a more complete lexicon. To our knowledge, ours is the first approach to learn to translate semantic relations, rather than words and phrases. Several other recent papers have learnt crosslingual word clusters, and used these to improve cross-lingual tasks such as document-classification (Klementiev et al., 2012b), parsing (T¨ackstr¨om et al., 2012) and semantic role labelling (Kozhevnikov and Titov, 2013) in resource-poor languages. Crosslingual word clusters are learnt by aligning monolingual clusters on the basis of parallel text—in language-pairs where parallel text is available, this offers an interesting complement to our method of clustering based on named entities. 9 Conclusions and Future Work We have demonstated that our previous work on monolingual distributional semantics can simply be extended to learn a language-independent semantics of relations from unlabelled text, and that this semantics is powerful enough to aid applications such as question answering and translation rerankin"
D13-1064,Q13-1015,1,0.249799,"al., 1995; Banarescu et al., 2013), state-of-the-art machine translation still uses phrase-based models (Koehn et al., 2007). The major obstacle to defining interlinguas has been devising a meaning representation that is languageindependent, but capable of expressing the limitless number of meanings that natural languages can express (Dorr et al., 2004). Our approach avoids this problem by utilizing the methods of distributional semantics. Recent work has shown that paraphrases of expressions can be learned by clustering those with similar arguments (Poon and Domingos, 2009; Yao et al., 2011; Lewis and Steedman, 2013)—for example learning that X wrote Y and X is the author of Y are equivalent if they appear in a corpus with similar (X, Y) argumentpairs such as {(Shakespeare, Macbeth), (Dickens, Oliver Twist)}. We extend this to the multilingual case, aiming to also map the French equivalents X a e´ crit Y and Y est un roman de X on to the same cluster as the English paraphrases. Conceptually, we treat a foreign expression as a paraphrase of an English expression. The cluster identifier can be used as a predicate in a logical form, suggesting that the fundamental predicates of an interlingua can be learnt i"
D13-1064,I11-1029,0,0.124072,"s we rely on the same facts being expressed in different languages. We use Wikipedia, which contains ar682 ticles in 250 languages, including 121 with at least 10,000 articles.1 Other domains, such as Newswire, may also be effective. • A method for extracting binary relations from sentences. This is straightforward from dependency parses, which are available for many languages. It is also possible without a parser, with some language-specific work (Fader et al., 2011). We describe our approach in Section 3. • A method for linking entities in the training data to some canonical representation. McNamee et al. (2011) report good results on this task in 21 languages. We describe our method for this in Section 4.1. 3 Predicate Extraction Our method relies on extracting binary predicates between entities from sentences. Various representations have been suggested for binary predicates, such as Reverb patterns (Fader et al., 2011), dependency paths (Lin and Pantel, 2001; Yao et al., 2011), and binarized predicate-argument relations derived from a CCG-parse (Lewis and Steedman, 2013). Our approach is formalism-independent, and is compatible with any method of expressing binary predicates. We choose the CCG-bas"
D13-1064,D09-1092,0,0.0568126,"Missing"
D13-1064,P09-1113,0,0.0451595,"a clustering. In this paper we focus on learning binary relations between named entities. This problem is much simpler than attempting complete interlingual semantic 681 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 681–692, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics interpretation, but the approach could be generalized. This class of expressions has proved extremely useful in the monolingual case, with direct applications for question answering and relation extraction (Poon and Domingos, 2009; Mintz et al., 2009), and we demonstrate how to use them to improve machine translation. It is important to be able to extract knowledge across languages, as many facts will not be expressed in all languages—either due to lesscomplete encyclopedias being available in some languages, or facts being most relevant to a single country. In contrast to most previous work on machine translation and cross-lingual clustering, our method requires no parallel text (see Section 8 for discussion of some exceptions). It instead exploits an alignment between named-entities in different languages. The limited size of parallel co"
D13-1064,1991.mtsummit-papers.9,0,0.464236,"ummarization, question answering, and information retrieval. Regardless of whether it is even possible to create such a semantics, we show that an incomplete version can be useful for downstream tasks. Semantic machine translation aims to map a source language to a language-independent meaning representation, and then generate the target language translation from this. It is hoped this would alleviate the difficulties of simpler models when translating between languages with very different word ordering and syntax (Vauquois, 1968). Despite many attempts to define interlingual representations (Mitamura et al., 1991; Beale et al., 1995; Banarescu et al., 2013), state-of-the-art machine translation still uses phrase-based models (Koehn et al., 2007). The major obstacle to defining interlinguas has been devising a meaning representation that is languageindependent, but capable of expressing the limitless number of meanings that natural languages can express (Dorr et al., 2004). Our approach avoids this problem by utilizing the methods of distributional semantics. Recent work has shown that paraphrases of expressions can be learned by clustering those with similar arguments (Poon and Domingos, 2009; Yao et"
D13-1064,D09-1001,0,0.667041,"entations (Mitamura et al., 1991; Beale et al., 1995; Banarescu et al., 2013), state-of-the-art machine translation still uses phrase-based models (Koehn et al., 2007). The major obstacle to defining interlinguas has been devising a meaning representation that is languageindependent, but capable of expressing the limitless number of meanings that natural languages can express (Dorr et al., 2004). Our approach avoids this problem by utilizing the methods of distributional semantics. Recent work has shown that paraphrases of expressions can be learned by clustering those with similar arguments (Poon and Domingos, 2009; Yao et al., 2011; Lewis and Steedman, 2013)—for example learning that X wrote Y and X is the author of Y are equivalent if they appear in a corpus with similar (X, Y) argumentpairs such as {(Shakespeare, Macbeth), (Dickens, Oliver Twist)}. We extend this to the multilingual case, aiming to also map the French equivalents X a e´ crit Y and Y est un roman de X on to the same cluster as the English paraphrases. Conceptually, we treat a foreign expression as a paraphrase of an English expression. The cluster identifier can be used as a predicate in a logical form, suggesting that the fundamental"
D13-1064,P99-1067,0,0.0429165,"scu et al., 2013), which uses English verbs as a set of predicates. This is a less abstract form of semantic interpretation than our proposal, as semantically equivalent paraphrases may be given a different representation. Such an approach also relies on annotating large amounts of text with the semantic representation—whereas our unsupervised approach offers a way to build such an interlingua using only a method for extracting predicates from sentences. Whilst almost all recent work on machinetranslation has relied on parallel text, there have been several interesting approaches that do not. Rapp (1999) learns to translate words based on small seed bilingual dictionary. Klementiev et al. (2012a) exploit a variety of interesting indirect sources of information to learn a lexicon—for example assuming that equivalent Wikipedia articles in different languages will use semantically similar words. The Polylingual Topic Model (Mimno et al., 2009) makes use of similar intuitions. Whilst we exploit equivalent Wikipedia articles for entity linking, we do not require aligned articles. Incorporating such techniques into our model would be a natural next step, allowing us to learn a more complete lexicon"
D13-1064,J03-3002,0,0.0277319,"nslation. It is important to be able to extract knowledge across languages, as many facts will not be expressed in all languages—either due to lesscomplete encyclopedias being available in some languages, or facts being most relevant to a single country. In contrast to most previous work on machine translation and cross-lingual clustering, our method requires no parallel text (see Section 8 for discussion of some exceptions). It instead exploits an alignment between named-entities in different languages. The limited size of parallel corpora is a significant bottleneck for machine translation (Resnik and Smith, 2003), whereas our approach can be used on much larger monolingual corpora. This means it is potentially useful for language-pairs where little parallel text is available, for domain adaptation, or for semisupervised approaches. 2 Basic Approach Our work builds on clustering-based approaches to monolingual distributional semantics, aiming to create clusters of semantically equivalent predicates, based on their arguments in a corpus. In each language, we first map each sentence in a large monolingual corpus onto a simple logical form, by extracting binary predicates between named entities. Then, we"
D13-1064,D10-1106,0,0.0180461,"article. For unlinked named-entity mentions, we perform some simple heuristic co-reference—based on word-overlap with previously mentioned entities in the document, whether the mention name is the title of a Wikipedia article, or whether the mention name is a Freebase (Bollacker et al., 2008) alias of an entity. We emphasise that this does not mean our approach is only applicable to the Wikipedia corpus. 4.2 Entity Typing It has become standard in clustering approaches to distributional semantics to assign types to predicates before clustering, and only cluster predicates with the same type (Schoenmackers et al., 2010; Berant et al., 2011; Yao et al., 2012). This is useful for resolving ambiguity—for example the phrase born in may express a place-of-birth or date-of-birth relation depending on whether its second argument has a LOC or DAT type. Ambiguous expressions may translate differently in other languages—for example, the two interpretations of was born in translate in French as est n´e a` and est n´e en respectively. The type of a predicate is determined by the type of its arguments, and predicates with different types are treated as distinct. Lewis and Steedman (2013) induce an unsupervised model of"
D13-1064,N12-1052,0,0.0592664,"Missing"
D13-1064,D12-1018,0,0.00812594,"from the data, and highly scalable. We create a separate graph for each type of predicate in each language—for example, predicates between types AUTHOR and BOOK in French (so only predicates with the same type will be clustered). We create one node per predicate in the graph, and edges represent the distributional similarity between the predicates. The distributional similarity between a pair of predicates is calculated as the cosine-similarity of their argument pair vectors in the corpus. Many more sophisticated approaches to determining similarity have been proposed (Kotlerman et al., 2010; Weisman et al., 2012), and future work should explore these. We prune nodes with less than 25 occurrences, edges of weight less than 0.05, and a short list of stop predicates. We find many of our French dependency paths do not have a clear semantic interpretation, so add the requirement that dependency paths contain at least one content word, contain at most 5 edges, and that one of the dependencies connected to the root is subject, object or the French preposition de. Data: Set of predicates P Result: A cluster assignment r p for all p ∈ P ∀p ∈ P : r p ←− unique cluster identifier; while not converged do randomiz"
D13-1064,D11-1135,0,0.39611,"., 1991; Beale et al., 1995; Banarescu et al., 2013), state-of-the-art machine translation still uses phrase-based models (Koehn et al., 2007). The major obstacle to defining interlinguas has been devising a meaning representation that is languageindependent, but capable of expressing the limitless number of meanings that natural languages can express (Dorr et al., 2004). Our approach avoids this problem by utilizing the methods of distributional semantics. Recent work has shown that paraphrases of expressions can be learned by clustering those with similar arguments (Poon and Domingos, 2009; Yao et al., 2011; Lewis and Steedman, 2013)—for example learning that X wrote Y and X is the author of Y are equivalent if they appear in a corpus with similar (X, Y) argumentpairs such as {(Shakespeare, Macbeth), (Dickens, Oliver Twist)}. We extend this to the multilingual case, aiming to also map the French equivalents X a e´ crit Y and Y est un roman de X on to the same cluster as the English paraphrases. Conceptually, we treat a foreign expression as a paraphrase of an English expression. The cluster identifier can be used as a predicate in a logical form, suggesting that the fundamental predicates of an"
D13-1064,P12-1075,0,0.0469592,"perform some simple heuristic co-reference—based on word-overlap with previously mentioned entities in the document, whether the mention name is the title of a Wikipedia article, or whether the mention name is a Freebase (Bollacker et al., 2008) alias of an entity. We emphasise that this does not mean our approach is only applicable to the Wikipedia corpus. 4.2 Entity Typing It has become standard in clustering approaches to distributional semantics to assign types to predicates before clustering, and only cluster predicates with the same type (Schoenmackers et al., 2010; Berant et al., 2011; Yao et al., 2012). This is useful for resolving ambiguity—for example the phrase born in may express a place-of-birth or date-of-birth relation depending on whether its second argument has a LOC or DAT type. Ambiguous expressions may translate differently in other languages—for example, the two interpretations of was born in translate in French as est n´e a` and est n´e en respectively. The type of a predicate is determined by the type of its arguments, and predicates with different types are treated as distinct. Lewis and Steedman (2013) induce an unsupervised model of entity types using Latent Dirichlet Allo"
D14-1107,P13-1023,0,0.0340102,"Missing"
D14-1107,P11-1048,0,0.0395903,"urface-compositional semantics. The fastest dependency parsers use an easy-first strategy, in which edges are added greedily in order of their score, with O(nlog(n)) complexity (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011). This strategy is reminiscent of our A∗ search, which expands the chart in a bestfirst order. A∗ has higher asymptotic complexity, but finds a globally optimal solution. ing model. Kummerfeld et al. (2010) showed that the speed of the C&C parser can be improved with domain-specific self-training—similar improvements may be possible applying this technique to our model. Auli and Lopez (2011a) have achieved the best CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A∗ parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A∗ heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&C parser, at the cost of a 30"
D14-1107,P11-1158,0,0.218517,"urface-compositional semantics. The fastest dependency parsers use an easy-first strategy, in which edges are added greedily in order of their score, with O(nlog(n)) complexity (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011). This strategy is reminiscent of our A∗ search, which expands the chart in a bestfirst order. A∗ has higher asymptotic complexity, but finds a globally optimal solution. ing model. Kummerfeld et al. (2010) showed that the speed of the C&C parser can be improved with domain-specific self-training—similar improvements may be possible applying this technique to our model. Auli and Lopez (2011a) have achieved the best CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A∗ parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A∗ heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&C parser, at the cost of a 30"
D14-1107,E03-1036,0,0.0140975,"hat unlexicalized parsers were only slightly less accurate than their lexicalized counterparts. Col6 Future Work We believe that our model opens several interesting directions for future research. One interesting angle would be to increase the amount of information in CCGBank’s lexical entries, to further reduce the search space for the parser. For example, PP categories could be distinguished with the relevant preposition as a feature; punctuation and coordination could be given more detailed categories to avoid needing their own combinators, and slashes could be extended 997 References with Baldridge and Kruijff (2003)’s multi-modal extensions to limit over-generation. Honnibal and Curran (2009) show how unary rules can be lexicalized in CCG. Such improvements may improve both the speed and accuracy of our model. Because our parser is factored on a unigram tagging model, it can be trained from isolated annotated words, and does not require annotated parse trees or full sentences. Reducing the requirements for training data eases the task for human annotators. It may also make the model more amenable to semi-supervised approaches to CCG parsing, which have typically focused on extending the lexicon (Thomford"
D14-1107,W13-2322,0,0.024317,"Missing"
D14-1107,D13-1160,0,0.046944,"large corpora. 40 30 20 10 0 20 40 60 80 Sentence Length 100 4.4 Semantic Parsing A major motivation for CCG parsing is to exploit its transparent interface to the semantics, allowing syntactic parsers to do much of the work of semantic parsers. Therefore, perhaps the most relevant measure of the performance of a CCG parser is its effect on the accuracy of downstream applications. We experimented with a supervised version of Reddy et al. (2014)’s model for questionanswering on Freebase (i.e. without using Reddy et al.’s lexicon derived from unlabelled text), using the W EB Q UESTIONS dataset (Berant et al., 2013)3. The model learns to map CCG parses to database queries. We compare the performance of the QA system using both our parser and C&C, taking the 10-best parses from each parser for each sentence. Syntactic question parsing models were trained from the combination of 10 copies of Rimell and Clark (2008)’s question dataset and one copy of the CCGBank The accuracy of Reddy et al. (2014)’s model varies significantly between iterations of the training data. Rather than tune the number of iterations, we instead measure the accuracy after each iteration. We experimented with the models’ 1-best answer"
D14-1107,W08-2222,0,0.812627,"sible outside context, rather than the actual sentence. We introduce a new heuristic which gives a tighter upper bound on the outside probability. Existing CCG Parsing Models The seminal C&C parser is by far the most popular choice of CCG parser (Clark and Curran, 2007). It showed that it was possible to parse to an expressive linguistic formalism with high speed and accuracy. The performance of the parser has enabled large-scale logic-based distributional research (Harrington, 2010; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Reddy et al., 2014), and it is a key component of Boxer (Bos, 2008). The C&C parser uses CKY chart parsing, with a log-linear model to rank parses. The vast number of possible parses means that computing the complete chart is impractical. To resolve this prob3 3.1 Model Lexical Category Model As input, our parser takes a distribution over all CCG lexical categories for each word in the sentence. These distributions are assigned using Lewis and Steedman (2014)’s semi-supervised supertagging model. The supertagger is a unigram log-linear classifier that uses features of the ±3 word context window surrounding a word. The 991 a house in Paris in France NP (NP NP"
D14-1107,J07-4004,0,0.144021,"lexicalized grammatical formalism, in which the vast majority of the decisions made during interpretation involve choosing the correct definitions of words. We explore the effect of modelling this explicitly in a parser, by only using a probabilistic model of lexical categories (based on a local context window), rather than modelling the derivation or dependencies. Existing state-of-the-art CCG parsers use complex pipelines of POS-tagging, supertagging and parsing—each with its own feature sets and parameters (and sources of error)—together with further parameters governing their integration (Clark and Curran, 2007). We show that much simpler models can achieve high performance. Our model predicts lexical categories based on a tiny feature set of word embeddings, capitalization, and 2character suffixes—with no parsing model beyond a small set of CCG combinators, and no POS1 Available from mikelewis0/easyccg https://github.com/ 990 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 990–1000, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics lem, a supertagger is first run over the sentence to prune the set of lexical categ"
D14-1107,P03-1054,0,0.151883,"inatory rule types (we use 10 binary and 13 unary rule schemata). This means that, aside from the lexicon, the grammar is small enough to be handcoded—which allows us, in this paper, to confine the entire statistical model to the lexicon. CCG’s generalized notion of constituency means that many derivations are possible for a given a set of lexical categories. However, most of these derivations will be semantically equivalent—for example, deriving the same dependency structures—in which case the actual choice of derivation is unimportant. Such ambiguity is often called spurious. 2.2 A∗ Parsing Klein and Manning (2003a) introduce A∗ parsing for PCFGs. The parser maintains a chart and an agenda, which is a priority queue of items to add to the chart. The agenda is sorted based on the items’ inside probability, and a heuristic upper-bound on the outside probability—to give an upper bound on the probability of the complete parse. The chart is then expanded in best-first order, until a complete parse for the sentence is found. Klein and Manning calculate an upper bound on the outside probability of a span based on a summary of the context. For example, the summary for the SX heuristic is the category of the sp"
D14-1107,W11-2916,1,0.881985,"Missing"
D14-1107,P14-1112,0,0.00935562,"a globally optimal solution. ing model. Kummerfeld et al. (2010) showed that the speed of the C&C parser can be improved with domain-specific self-training—similar improvements may be possible applying this technique to our model. Auli and Lopez (2011a) have achieved the best CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A∗ parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A∗ heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&C parser, at the cost of a 300-fold speed decrease. Zhang and Clark (2011) and Xu et al. (2014) explored shift-reduce CCG parsing, but despite the use of a linear-time algorithm, parsing speed in practice is significantly slower than the C&C parser. Parsers based on supertagging models have previously been applied to other strongly lexicalized formalisms, such as to LTAG (Bangalore and Joshi, 1999) and to H"
D14-1107,E14-1014,1,0.873645,"Missing"
D14-1107,P10-1036,0,0.0279528,"has focused on building fast parsers for web-scale processing, typically using dependency grammars (e.g. Nivre (2003)). CCG has some advantages over dependency grammars, such as supporting surface-compositional semantics. The fastest dependency parsers use an easy-first strategy, in which edges are added greedily in order of their score, with O(nlog(n)) complexity (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011). This strategy is reminiscent of our A∗ search, which expands the chart in a bestfirst order. A∗ has higher asymptotic complexity, but finds a globally optimal solution. ing model. Kummerfeld et al. (2010) showed that the speed of the C&C parser can be improved with domain-specific self-training—similar improvements may be possible applying this technique to our model. Auli and Lopez (2011a) have achieved the best CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A∗ parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A∗ heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be imp"
D14-1107,P96-1011,0,0.445207,"all possible categories. Note that the motivation for introducing pruning here is fundamentally different from for the C&C pipeline. The C&C supertagger prunes the the categories so that the parser can build the complete set of derivations given those categories. In contrast, our parser can efficiently search large (or infinite) spaces of categories, but pruning is helpful for making supertagging itself more efficient, and for building the initial agenda. We therefore implemented the following strategies to improve efficiency: cal category accuracy. We also use Eisner Normal Form Constraints (Eisner, 1996), and Hockenmaier and Bisk’s (2010) Constraint 5, which automatically rule out certain spuriously equivalent derivations, improving parsing speed. We add a hard constraint that the root category of the sentence must be a declarative sentence, a question, or a noun-phrase. This grammar is smaller and cleaner than that used by the C&C parser, which uses 32 unary rules (some of which are semantically dubious, such as S[dcl] → N P N P ), and non-standard binary combinators such as merging two N P s into an N P . The C&C parser also has a large number of special case rules for handling punctuation"
D14-1107,P10-1035,0,0.0312631,"ved with domain-specific self-training—similar improvements may be possible applying this technique to our model. Auli and Lopez (2011a) have achieved the best CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A∗ parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A∗ heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&C parser, at the cost of a 300-fold speed decrease. Zhang and Clark (2011) and Xu et al. (2014) explored shift-reduce CCG parsing, but despite the use of a linear-time algorithm, parsing speed in practice is significantly slower than the C&C parser. Parsers based on supertagging models have previously been applied to other strongly lexicalized formalisms, such as to LTAG (Bangalore and Joshi, 1999) and to HPSG (Ninomiya et al., 2006). A major contribution of our work over these is showing that factoring models o"
D14-1107,D13-1064,1,0.845318,"daptive supertagging. One reason is that the heuristic estimate is rather coarse, as it deals with the best possible outside context, rather than the actual sentence. We introduce a new heuristic which gives a tighter upper bound on the outside probability. Existing CCG Parsing Models The seminal C&C parser is by far the most popular choice of CCG parser (Clark and Curran, 2007). It showed that it was possible to parse to an expressive linguistic formalism with high speed and accuracy. The performance of the parser has enabled large-scale logic-based distributional research (Harrington, 2010; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Reddy et al., 2014), and it is a key component of Boxer (Bos, 2008). The C&C parser uses CKY chart parsing, with a log-linear model to rank parses. The vast number of possible parses means that computing the complete chart is impractical. To resolve this prob3 3.1 Model Lexical Category Model As input, our parser takes a distribution over all CCG lexical categories for each word in the sentence. These distributions are assigned using Lewis and Steedman (2014)’s semi-supervised supertagging model. The supertagger is a unigram log-linear classifier that uses feature"
D14-1107,N10-1115,0,0.0295778,"niques. We achieve state-of-the-art results using the same feature set and a simpler model by exploiting CCG’s lexicalized nature, which makes it more natural to delegate parsing decisions to a tagging model. Other parsing research has focused on building fast parsers for web-scale processing, typically using dependency grammars (e.g. Nivre (2003)). CCG has some advantages over dependency grammars, such as supporting surface-compositional semantics. The fastest dependency parsers use an easy-first strategy, in which edges are added greedily in order of their score, with O(nlog(n)) complexity (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011). This strategy is reminiscent of our A∗ search, which expands the chart in a bestfirst order. A∗ has higher asymptotic complexity, but finds a globally optimal solution. ing model. Kummerfeld et al. (2010) showed that the speed of the C&C parser can be improved with domain-specific self-training—similar improvements may be possible applying this technique to our model. Auli and Lopez (2011a) have achieved the best CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b)"
D14-1107,Q14-1026,1,0.271791,"he performance of the parser has enabled large-scale logic-based distributional research (Harrington, 2010; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Reddy et al., 2014), and it is a key component of Boxer (Bos, 2008). The C&C parser uses CKY chart parsing, with a log-linear model to rank parses. The vast number of possible parses means that computing the complete chart is impractical. To resolve this prob3 3.1 Model Lexical Category Model As input, our parser takes a distribution over all CCG lexical categories for each word in the sentence. These distributions are assigned using Lewis and Steedman (2014)’s semi-supervised supertagging model. The supertagger is a unigram log-linear classifier that uses features of the ±3 word context window surrounding a word. The 991 a house in Paris in France NP (NP NP )/NP NP (NP NP )/NP NP key feature is word embeddings, initialized with the 50-dimensional embeddings trained in Turian et al. (2010), and fine-tuned during supervised training. The model also uses 2-character suffixes and capitalization features. The use of word embeddings, which are trained on a large unlabelled corpus, allows the supertagger to generalize well to words not present in the"
D14-1107,C10-2041,0,0.0125638,"is combined with adaptive supertagging. One reason is that the heuristic estimate is rather coarse, as it deals with the best possible outside context, rather than the actual sentence. We introduce a new heuristic which gives a tighter upper bound on the outside probability. Existing CCG Parsing Models The seminal C&C parser is by far the most popular choice of CCG parser (Clark and Curran, 2007). It showed that it was possible to parse to an expressive linguistic formalism with high speed and accuracy. The performance of the parser has enabled large-scale logic-based distributional research (Harrington, 2010; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Reddy et al., 2014), and it is a key component of Boxer (Bos, 2008). The C&C parser uses CKY chart parsing, with a log-linear model to rank parses. The vast number of possible parses means that computing the complete chart is impractical. To resolve this prob3 3.1 Model Lexical Category Model As input, our parser takes a distribution over all CCG lexical categories for each word in the sentence. These distributions are assigned using Lewis and Steedman (2014)’s semi-supervised supertagging model. The supertagger is a unigram log-linear cl"
D14-1107,W06-1619,0,0.0582926,"Missing"
D14-1107,J07-3004,1,0.287227,"Missing"
D14-1107,W03-3017,0,0.0520961,"rformed at high accuracy using a small feature set based on vectorspace word embeddings. However, the extension of this work to phrase-structure parsing (Collobert, 2011) required a more complex model, and did not match the performance of traditional parsing techniques. We achieve state-of-the-art results using the same feature set and a simpler model by exploiting CCG’s lexicalized nature, which makes it more natural to delegate parsing decisions to a tagging model. Other parsing research has focused on building fast parsers for web-scale processing, typically using dependency grammars (e.g. Nivre (2003)). CCG has some advantages over dependency grammars, such as supporting surface-compositional semantics. The fastest dependency parsers use an easy-first strategy, in which edges are added greedily in order of their score, with O(nlog(n)) complexity (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011). This strategy is reminiscent of our A∗ search, which expands the chart in a bestfirst order. A∗ has higher asymptotic complexity, but finds a globally optimal solution. ing model. Kummerfeld et al. (2010) showed that the speed of the C&C parser can be improved with domain-specific self-training—si"
D14-1107,D09-1126,0,0.0118582,"d counterparts. Col6 Future Work We believe that our model opens several interesting directions for future research. One interesting angle would be to increase the amount of information in CCGBank’s lexical entries, to further reduce the search space for the parser. For example, PP categories could be distinguished with the relevant preposition as a feature; punctuation and coordination could be given more detailed categories to avoid needing their own combinators, and slashes could be extended 997 References with Baldridge and Kruijff (2003)’s multi-modal extensions to limit over-generation. Honnibal and Curran (2009) show how unary rules can be lexicalized in CCG. Such improvements may improve both the speed and accuracy of our model. Because our parser is factored on a unigram tagging model, it can be trained from isolated annotated words, and does not require annotated parse trees or full sentences. Reducing the requirements for training data eases the task for human annotators. It may also make the model more amenable to semi-supervised approaches to CCG parsing, which have typically focused on extending the lexicon (Thomforde and Steedman, 2011; Deoskar et al., 2014). Finally, it may make it easier to"
D14-1107,Q14-1030,1,0.283761,"estimate is rather coarse, as it deals with the best possible outside context, rather than the actual sentence. We introduce a new heuristic which gives a tighter upper bound on the outside probability. Existing CCG Parsing Models The seminal C&C parser is by far the most popular choice of CCG parser (Clark and Curran, 2007). It showed that it was possible to parse to an expressive linguistic formalism with high speed and accuracy. The performance of the parser has enabled large-scale logic-based distributional research (Harrington, 2010; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Reddy et al., 2014), and it is a key component of Boxer (Bos, 2008). The C&C parser uses CKY chart parsing, with a log-linear model to rank parses. The vast number of possible parses means that computing the complete chart is impractical. To resolve this prob3 3.1 Model Lexical Category Model As input, our parser takes a distribution over all CCG lexical categories for each word in the sentence. These distributions are assigned using Lewis and Steedman (2014)’s semi-supervised supertagging model. The supertagger is a unigram log-linear classifier that uses features of the ±3 word context window surrounding a wor"
D14-1107,N03-1016,0,0.871412,"inatory rule types (we use 10 binary and 13 unary rule schemata). This means that, aside from the lexicon, the grammar is small enough to be handcoded—which allows us, in this paper, to confine the entire statistical model to the lexicon. CCG’s generalized notion of constituency means that many derivations are possible for a given a set of lexical categories. However, most of these derivations will be semantically equivalent—for example, deriving the same dependency structures—in which case the actual choice of derivation is unimportant. Such ambiguity is often called spurious. 2.2 A∗ Parsing Klein and Manning (2003a) introduce A∗ parsing for PCFGs. The parser maintains a chart and an agenda, which is a priority queue of items to add to the chart. The agenda is sorted based on the items’ inside probability, and a heuristic upper-bound on the outside probability—to give an upper bound on the probability of the complete parse. The chart is then expanded in best-first order, until a complete parse for the sentence is found. Klein and Manning calculate an upper bound on the outside probability of a span based on a summary of the context. For example, the summary for the SX heuristic is the category of the sp"
D14-1107,D08-1050,0,0.0640844,"rformance of a CCG parser is its effect on the accuracy of downstream applications. We experimented with a supervised version of Reddy et al. (2014)’s model for questionanswering on Freebase (i.e. without using Reddy et al.’s lexicon derived from unlabelled text), using the W EB Q UESTIONS dataset (Berant et al., 2013)3. The model learns to map CCG parses to database queries. We compare the performance of the QA system using both our parser and C&C, taking the 10-best parses from each parser for each sentence. Syntactic question parsing models were trained from the combination of 10 copies of Rimell and Clark (2008)’s question dataset and one copy of the CCGBank The accuracy of Reddy et al. (2014)’s model varies significantly between iterations of the training data. Rather than tune the number of iterations, we instead measure the accuracy after each iteration. We experimented with the models’ 1-best answers, and the oracle accuracy of their 100 best answers. The oracle accuracy gives a better indication of the performance of the parser, by mitigating errors caused by the semantic component. Results are shown in Figure 3, and demonstrate that using E ASY CCG can lead to better downstream performance than"
D14-1107,D11-1115,1,0.917962,"Missing"
D14-1107,D11-1116,0,0.0502919,"the-art results using the same feature set and a simpler model by exploiting CCG’s lexicalized nature, which makes it more natural to delegate parsing decisions to a tagging model. Other parsing research has focused on building fast parsers for web-scale processing, typically using dependency grammars (e.g. Nivre (2003)). CCG has some advantages over dependency grammars, such as supporting surface-compositional semantics. The fastest dependency parsers use an easy-first strategy, in which edges are added greedily in order of their score, with O(nlog(n)) complexity (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011). This strategy is reminiscent of our A∗ search, which expands the chart in a bestfirst order. A∗ has higher asymptotic complexity, but finds a globally optimal solution. ing model. Kummerfeld et al. (2010) showed that the speed of the C&C parser can be improved with domain-specific self-training—similar improvements may be possible applying this technique to our model. Auli and Lopez (2011a) have achieved the best CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A"
D14-1107,P10-1040,0,0.00583379,"means that computing the complete chart is impractical. To resolve this prob3 3.1 Model Lexical Category Model As input, our parser takes a distribution over all CCG lexical categories for each word in the sentence. These distributions are assigned using Lewis and Steedman (2014)’s semi-supervised supertagging model. The supertagger is a unigram log-linear classifier that uses features of the ±3 word context window surrounding a word. The 991 a house in Paris in France NP (NP NP )/NP NP (NP NP )/NP NP key feature is word embeddings, initialized with the 50-dimensional embeddings trained in Turian et al. (2010), and fine-tuned during supervised training. The model also uses 2-character suffixes and capitalization features. The use of word embeddings, which are trained on a large unlabelled corpus, allows the supertagger to generalize well to words not present in the labelled data. It does not use a POS-tagger, which avoids problems caused by POS-tagging errors. Our methods could be applied to any supertagging model, but we find empirically that this model gives higher performance than the C&C supertagger. 3.2 NP NP NP &gt; &lt; NP NP &gt; &lt; NP (a) A standard derivation of a house in Paris in France, with a"
D14-1107,P14-1021,0,0.151807,"allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A∗ parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A∗ heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&C parser, at the cost of a 300-fold speed decrease. Zhang and Clark (2011) and Xu et al. (2014) explored shift-reduce CCG parsing, but despite the use of a linear-time algorithm, parsing speed in practice is significantly slower than the C&C parser. Parsers based on supertagging models have previously been applied to other strongly lexicalized formalisms, such as to LTAG (Bangalore and Joshi, 1999) and to HPSG (Ninomiya et al., 2006). A major contribution of our work over these is showing that factoring models on lexical categories allows fast and exact A∗ parsing, without the need for beam search. Our parsing approach could be applied to any strongly lexicalized formalism. Our work fit"
D14-1107,P11-1069,0,0.195852,"st CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A∗ parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A∗ heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&C parser, at the cost of a 300-fold speed decrease. Zhang and Clark (2011) and Xu et al. (2014) explored shift-reduce CCG parsing, but despite the use of a linear-time algorithm, parsing speed in practice is significantly slower than the C&C parser. Parsers based on supertagging models have previously been applied to other strongly lexicalized formalisms, such as to LTAG (Bangalore and Joshi, 1999) and to HPSG (Ninomiya et al., 2006). A major contribution of our work over these is showing that factoring models on lexical categories allows fast and exact A∗ parsing, without the need for beam search. Our parsing approach could be applied to any strongly lexicalized fo"
D14-1107,N07-1051,0,\N,Missing
D14-1107,J99-2004,0,\N,Missing
D14-1107,C10-1053,0,\N,Missing
D14-1107,Q13-1015,1,\N,Missing
D16-1214,D13-1160,0,0.126124,"realization is potentially a correct derivation. For example, the Freebase logical form for the example sentence in Figure 1 is shown below, and none of its derivations are penalized if they could result in this logical form. λe. business.acquisition(e) ∧ acquiring company(e, G OOGLE) ∧ company acquired(e, N EST) (1) ∧ date(e, 2014) Since grammar induction systems are traditionally trained on declarative sentences, we would ideally require declarative sentences paired with Freebase logical forms. But such datasets do not exist in the Freebase semantic parsing literature (Cai and Yates, 2013; Berant et al., 2013). To alleviate this prob1 Please see Bisk and Hockenmaier (2013) for more details. 2023 lem, and yet perform Freebase semantic parsing, we propose an entity slot-filling task. Entity Slot-Filling Task. Given a declarative sentence containing mentions of Freebase entities, we randomly remove one of the mentions to create a blank slot. The task is to fill this slot by translating the declarative sentence into a Freebase query. Consider the following sentence where the entity Nest has been removed: Google acquired which was founded in Palo Alto To correctly fill in the blank, one has to query Fre"
D16-1214,P15-1135,1,0.941398,"nois.edu, steedman@inf.ed.ac.uk, Abstract We compare the effectiveness of four different syntactic CCG parsers for a semantic slotfilling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems. 1 Introduction The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015). But how useful are unsupervised syntactic parsers for downstream NLP tasks? What phenomena are they able to capture, and where would additional annotation be required? Instead of standard intrinsic evaluations – attachment scores that depend strongly on the particular annotation styles of the gold treebank – we examine the utility of unsupervised and weakly supervised parsers for semantics. We perform an extrinsic evaluation of unsupervised and weakly supervised CCG parsers on a grounded semantic parsing task that will shed light on the extent to which these systems recover semantic informat"
D16-1214,C04-1180,1,0.682913,"gold answer is the same as the predicted answer. 4 Sentences to Freebase Logical Forms CCG provides a clean interface between syntax and semantics, i.e. each argument of a words syntactic category corresponds to an argument of the lambda expression that defines its semantic interpretation (e.g., the lambda expression corresponding to the category (SNP)/NP of the verb acquired is λf.λg.λe.∃x.∃y.acquired(e) ∧ f (x) ∧ g(y) ∧ arg1 (e, y) ∧ arg2 (e, x)), and the logical form for the complete sentence can be constructed by composing word level lambda expressions following the syntactic derivation (Bos et al., 2004). In Figure 2 we show two syntactic derivations for the same sentence, and the corresponding logical forms and equivalent graph representations derived by G RAPH PARSER (Reddy et al., 2014). The graph representations are possible because G RAPH PARSER assumes access to coindexations of input CCG categories. We provide acquired NP (SNP)/NP hblanki NP which was founded in PA (NPNP)/(SNP) SNP founded. in.arg2 > NPNP < NP > SNP x target < S e2 fo u in n d .a e d rg . 1 Google acquired. arg2 e1 Palo Alto acquired. arg1 Google λe1 .∃xe2 . TARGET(x) ∧ acquired(e1 ) ∧ arg1 (e1 , Google) ∧ arg2 ("
D16-1214,S13-1045,0,0.0184229,"at could lead to this realization is potentially a correct derivation. For example, the Freebase logical form for the example sentence in Figure 1 is shown below, and none of its derivations are penalized if they could result in this logical form. λe. business.acquisition(e) ∧ acquiring company(e, G OOGLE) ∧ company acquired(e, N EST) (1) ∧ date(e, 2014) Since grammar induction systems are traditionally trained on declarative sentences, we would ideally require declarative sentences paired with Freebase logical forms. But such datasets do not exist in the Freebase semantic parsing literature (Cai and Yates, 2013; Berant et al., 2013). To alleviate this prob1 Please see Bisk and Hockenmaier (2013) for more details. 2023 lem, and yet perform Freebase semantic parsing, we propose an entity slot-filling task. Entity Slot-Filling Task. Given a declarative sentence containing mentions of Freebase entities, we randomly remove one of the mentions to create a blank slot. The task is to fill this slot by translating the declarative sentence into a Freebase query. Consider the following sentence where the entity Nest has been removed: Google acquired which was founded in Palo Alto To correctly fill in the blank"
D16-1214,P04-1061,0,0.063815,"ign ybisk@isi.edu, siva.reddy@ed.ac.uk, blitzer@google.com, juliahmr@illinois.edu, steedman@inf.ed.ac.uk, Abstract We compare the effectiveness of four different syntactic CCG parsers for a semantic slotfilling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems. 1 Introduction The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015). But how useful are unsupervised syntactic parsers for downstream NLP tasks? What phenomena are they able to capture, and where would additional annotation be required? Instead of standard intrinsic evaluations – attachment scores that depend strongly on the particular annotation styles of the gold treebank – we examine the utility of unsupervised and weakly supervised parsers for semantics. We perform an extrinsic evaluation of unsupervised and weakly supervised CCG parsers on a grounded semantic parsing task that w"
D16-1214,D14-1107,1,0.890059,"Missing"
D16-1214,N10-1116,0,0.0189303,"eddy@ed.ac.uk, blitzer@google.com, juliahmr@illinois.edu, steedman@inf.ed.ac.uk, Abstract We compare the effectiveness of four different syntactic CCG parsers for a semantic slotfilling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems. 1 Introduction The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015). But how useful are unsupervised syntactic parsers for downstream NLP tasks? What phenomena are they able to capture, and where would additional annotation be required? Instead of standard intrinsic evaluations – attachment scores that depend strongly on the particular annotation styles of the gold treebank – we examine the utility of unsupervised and weakly supervised parsers for semantics. We perform an extrinsic evaluation of unsupervised and weakly supervised CCG parsers on a grounded semantic parsing task that will shed light on the ext"
D16-1214,P12-1063,0,0.0558732,"Missing"
D17-1009,E17-2039,0,0.0131179,"Missing"
D17-1009,P15-1039,0,0.0304249,"ttempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded repres"
D17-1009,P14-1133,0,0.21499,"Missing"
D17-1009,Q15-1039,0,0.154417,"Missing"
D17-1009,W13-3520,0,0.0208057,"ty linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input to G RAPH PARSER, leaving the final disambiguation to the semantic parsing problem. Table 2 shows the 1-best and 10-best entity disambiguation F1 -scores for each language and dataset. Features We use features similar to Reddy et al. (2016): basic features of words and Freebase relations, and graph features crossing ungrounded events wi"
D17-1009,W09-1206,0,0.0126084,"e English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1 score to their use of the more fine-grained PTB tag set and Stanford Dependencies. CCGG RAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English. UD EP L AMBDA SRL This is similar to UD EP L AMBDA except that instead of assuming nsubj, dobj and nsubjpass correspond to arg1 , arg2 and arg2 , we employ semantic role labeling to identify the correct interpretation. We used the systems of Roth and Woodsend (2014) for English and German and Bjrkelund et al. (2009) for Spanish trained on the CoNLL-2009 dataset (Haji et al., 2009).8 4.5 GraphQ. WebQ. Results Table 3 shows the performance of G RAPH PARSER with these different representations. Here and in what follows, we use average F1 -score of predicted answers (Berant et al., 2013) as the evaluation metric. We first observe that UD EP L AMBDA consistently outperforms the S INGLE E VENT and D EP T REE representations in all languages. For English, performance is on par with CCGG RAPH, which suggests that UD EP L AMBDA does not sacrifice too much specificity for universality. With both datasets, results"
D17-1009,P02-1041,0,0.341625,"Missing"
D17-1009,C04-1180,1,0.821524,"Missing"
D17-1009,W13-2322,0,0.0191232,", some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. We discuss how to handle quantifiers in this framework in the supplementary material. Although in the current setup UD EP L AMBDA rules are hand-coded, the number of rules are only proportional to the number of UD labels, making rule-writing manageable.5 Moreover, we view UD EP L AMBDA as a first step towards learning rules for converting UD to richer semantic representations such as PropBank, AMR, or the Parallel Meaning Bank (Palmer et al., 2005; Banarescu et al., 2013; Abzianidze et al., 2017).. 4 Semantic Parsing as Graph Matching Cross-lingual Semantic Parsing To study the multilingual nature of UD EP L AMBDA, we conduct an empirical evaluation on question answering against Freebase in three different languages: English, Spanish, and German. Before discussing the details of this experiment, we briefly outline the semantic parsing framework employed. 4.2 Datasets We evaluate our approach on two public benchmarks of question answering against Freebase: WebQuestions (Berant et al., 2013), a widely used benchmark consisting of English questions and their ans"
D17-1009,W02-1001,0,0.0458946,"problem with the goal of finding the Freebase graphs that are structurally isomorphic to an ungrounded graph and rank them according to a model. To account for structural mismatches, G RAPH PARSER uses two graph transformations: CONTRACT and EXPAND . In Figure 3(a) there are two edges between x and Ghana. CONTRACT collapses one of these edges to create a graph isomorphic to Freebase. EXPAND, in contrast, adds edges to connect the graph in the case of disconnected components. The search space is explored by beam search and model parameters are estimated with the averaged structured perceptron (Collins, 2002) from training data consisting of question-answer pairs, using answer F1 -score as the objective. Other constructions that require lexical information are quantifiers like every, some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. We discuss how to handle quantifiers in this framework in the supplementary material. Although in the current setup UD EP L AMBDA rules are hand-coded, the number of rules are only proportional to the number of UD labels, making rule-writing manageable.5 Moreover, we view UD"
D17-1009,W15-0128,0,0.0171085,"2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in"
D17-1009,D15-1127,0,0.0138165,"Missing"
D17-1009,C16-1056,0,0.096081,"ave mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurth"
D17-1009,jakob-etal-2010-mapping,0,0.0578418,"Missing"
D17-1009,D16-1086,0,0.0201594,"emantic types coupled with strong type constraints, which can be very informative but unavoidably language specific. Instead, UD EP L AMBDA relies on generic unlexicalized information present in dependency treebanks and uses a simple type system (one type for dependency labels, and one for words) along with a combinatory mechanism, which avoids type collisions. Earlier attempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) l"
D17-1009,C14-1122,0,0.0632483,"Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit ap"
D17-1009,P14-1134,0,0.024364,"for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to li"
D17-1009,P12-1051,0,0.0794212,"and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this frame"
D17-1009,E03-1030,0,0.105881,"Missing"
D17-1009,P15-1143,0,0.0127531,"tly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common"
D17-1009,N16-1088,0,0.0302269,"motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched"
D17-1009,Q16-1023,0,0.0120801,"stions de es 82.8 91.2 86.7 94.0 GraphQuestions en de es 47.2 56.9 39.9 48.4 39.5 51.6 Here we provide details on the syntactic analyzers employed, our entity resolution algorithm, and the features used by the grounding model. Table 2: Structured perceptron k-best entity linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input to G RAPH PARSER, leaving the final disambiguation to the semantic parsing prob"
D17-1009,Q15-1019,0,0.0329059,"d Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012"
D17-1009,D10-1119,1,0.865489,"al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to ex"
D17-1009,D13-1161,0,0.0259018,"nde et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less)"
D17-1009,levy-andrew-2006-tregex,0,0.0175027,"the in English have different semantics, despite being both determiners) and are not encoded in the UD schema. Furthermore, some cross-linguistic phenomena, such as long-distance dependencies, are not part of the core UD representation. To circumvent this limitation, a simple enhancement step enriches the original UD representation before binarization takes place (Section 3.1). This step adds to the dependency tree missing syntactic information and long-distance dependencies, thereby creating a graph. Whereas D EP L AMBDA is not able to handle graph-structured input, UD EP - 2 We use Tregex (Levy and Andrew, 2006) for substitution mappings and Cornell SPF (Artzi, 2013) as the lambdacalculus implementation. For example, in running horse, the tregex /label:amod/=target < /postag:verb/ matches amod to its INVERT expression λ f gx. ∃y. f (x) ∧ g(y) ∧ amodi (ye , xa ). 3 In what follows, all references to UD are to UD v1.3. 91 L AMBDA is designed to work with dependency graphs as well (Section 3.2). Finally, several constructions differ in structure between UD and SD, which requires different handling in the semantic interface (Section 3.3). 3.1 xcomp nsubj Anna wants to dobj marry Kristoff nsubj (a) With l"
D17-1009,N15-1114,0,0.00825081,"P L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit applications such as summarization (Liu et al., 2015) and machine reading (Sachan and Xing, 2016). Acknowledgements This work greatly benefited from discussions with Michael Collins, Dipanjan Das, Federico Fancellu, Julia Hockenmaier, Tom Kwiatkowski, Adam Lopez, Valeria de Paiva, Martha Palmer, Fernando Pereira, Emily Pitler, Vijay Saraswat, Nathan Schneider, Bonnie Webber, Luke Zettlemoyer, and the members of ILCC Edinburgh University, the Microsoft Research Redmond NLP group, the Stanford NLP group, and the UW NLP and Linguistics group. We thank Reviewer 2 for useful feedback. The authors would also like to thank the Universal Dependencies co"
D17-1009,Q14-1030,1,0.830946,"ikipedia, or SimpleQuestions (Bordes et al., 2015) are shown in parentheses. On GraphQuestions, we achieve a new state-of-the-art result with a gain of 4.8 F1 points over the previously reported best result. On WebQuestions we are 2.1 points below the best model using comparable resources, and 3.8 points below the state of the art. Most related to our work is the English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1 score to their use of the more fine-grained PTB tag set and Stanford Dependencies. CCGG RAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English. UD EP L AMBDA SRL This is similar to UD EP L AMBDA except that instead of assuming nsubj, dobj and nsubjpass correspond to arg1 , arg2 and arg2 , we employ semantic role labeling to identify the correct interpretation. We used the systems of Roth and Woodsend (2014) for English and German and Bjrkelund et al. (2009) for Spanish trained on the CoNLL-2009 dataset (Haji et al., 2009).8 4.5 GraphQ. WebQ. Results Table 3 shows the performance of G RAPH PARSER with these different representations. Here and in what follows, we use average F1 -score o"
D17-1009,J93-2004,0,0.0647121,"ng-distance dependencies in relative clauses and control constructions. We follow Schuster and Manning (2016) and find these using the labels acl (relative) and xcomp (control). Figure 2(a) shows the long-distance dependency in the sentence Anna wants to marry Kristoff. Here, marry is provided with its missing nsubj (dashed arc). Second, UD conflates all coordinating constructions to a single dependency label, conj. To obtain the correct coordination scope, we refine conj to conj:verb, conj:vp, conj:sentence, conj:np, and conj:adj, similar to Reddy et al. (2016). Finally, unlike the PTB tags (Marcus et al., 1993) used by SD, the UD part-of-speech tags do not distinguish question words. Since these are crucial to question-answering, we use a small lexicon to refine the tags for determiners (DET), adverbs (ADV) and pronouns (PRON) to DET: WH, ADV: WH and PRON : WH, respectively. Specifically, we use a list of 12 (English), 14 (Spanish) and 35 (German) words, respectively. This is the only part of UD EP L AMBDA that relies on language-specific information. We hope that, as the coverage of morphological features in UD improves, this refinement can be replaced by relying on morphological features, such as"
D17-1009,D14-1045,0,0.0131176,"the state of the art. Most related to our work is the English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1 score to their use of the more fine-grained PTB tag set and Stanford Dependencies. CCGG RAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English. UD EP L AMBDA SRL This is similar to UD EP L AMBDA except that instead of assuming nsubj, dobj and nsubjpass correspond to arg1 , arg2 and arg2 , we employ semantic role labeling to identify the correct interpretation. We used the systems of Roth and Woodsend (2014) for English and German and Bjrkelund et al. (2009) for Spanish trained on the CoNLL-2009 dataset (Haji et al., 2009).8 4.5 GraphQ. WebQ. Results Table 3 shows the performance of G RAPH PARSER with these different representations. Here and in what follows, we use average F1 -score of predicted answers (Berant et al., 2013) as the evaluation metric. We first observe that UD EP L AMBDA consistently outperforms the S INGLE E VENT and D EP T REE representations in all languages. For English, performance is on par with CCGG RAPH, which suggests that UD EP L AMBDA does not sacrifice too much specifi"
D17-1009,P16-2079,0,0.0223299,"versal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit applications such as summarization (Liu et al., 2015) and machine reading (Sachan and Xing, 2016). Acknowledgements This work greatly benefited from discussions with Michael Collins, Dipanjan Das, Federico Fancellu, Julia Hockenmaier, Tom Kwiatkowski, Adam Lopez, Valeria de Paiva, Martha Palmer, Fernando Pereira, Emily Pitler, Vijay Saraswat, Nathan Schneider, Bonnie Webber, Luke Zettlemoyer, and the members of ILCC Edinburgh University, the Microsoft Research Redmond NLP group, the Stanford NLP group, and the UW NLP and Linguistics group. We thank Reviewer 2 for useful feedback. The authors would also like to thank the Universal Dependencies community for the treebanks and documentation."
D17-1009,L16-1376,0,0.089339,"(Artzi, 2013) as the lambdacalculus implementation. For example, in running horse, the tregex /label:amod/=target < /postag:verb/ matches amod to its INVERT expression λ f gx. ∃y. f (x) ∧ g(y) ∧ amodi (ye , xa ). 3 In what follows, all references to UD are to UD v1.3. 91 L AMBDA is designed to work with dependency graphs as well (Section 3.2). Finally, several constructions differ in structure between UD and SD, which requires different handling in the semantic interface (Section 3.3). 3.1 xcomp nsubj Anna wants to dobj marry Kristoff nsubj (a) With long-distance dependency. Enhancement Both Schuster and Manning (2016) and Nivre et al. (2016) note the necessity of an enhanced UD representation to enable semantic applications. However, such enhancements are currently only available for a subset of languages in UD. Instead, we rely on a small number of enhancements for our main application—semantic parsing for questionanswering—with the hope that this step can be replaced by an enhanced UD representation in the future. Specifically, we define three kinds of enhancements: (1) long-distance dependencies; (2) types of coordination; and (3) refined question word tags. These correspond to line 2 in Algorithm 1. Fi"
D17-1009,J05-1004,0,0.0829545,"uantifiers like every, some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. We discuss how to handle quantifiers in this framework in the supplementary material. Although in the current setup UD EP L AMBDA rules are hand-coded, the number of rules are only proportional to the number of UD labels, making rule-writing manageable.5 Moreover, we view UD EP L AMBDA as a first step towards learning rules for converting UD to richer semantic representations such as PropBank, AMR, or the Parallel Meaning Bank (Palmer et al., 2005; Banarescu et al., 2013; Abzianidze et al., 2017).. 4 Semantic Parsing as Graph Matching Cross-lingual Semantic Parsing To study the multilingual nature of UD EP L AMBDA, we conduct an empirical evaluation on question answering against Freebase in three different languages: English, Spanish, and German. Before discussing the details of this experiment, we briefly outline the semantic parsing framework employed. 4.2 Datasets We evaluate our approach on two public benchmarks of question answering against Freebase: WebQuestions (Berant et al., 2013), a widely used benchmark consisting of English"
D17-1009,P15-1142,0,0.0101447,"in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multi"
D17-1009,D14-1162,0,0.116047,"tic analyzers employed, our entity resolution algorithm, and the features used by the grounding model. Table 2: Structured perceptron k-best entity linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input to G RAPH PARSER, leaving the final disambiguation to the semantic parsing problem. Table 2 shows the 1-best and 10-best entity disambiguation F1 -scores for each language and dataset. Features We"
D17-1009,P16-2067,0,0.0225596,"ir translations. k en 1 10 Implementation Details 89.6 95.7 WebQuestions de es 82.8 91.2 86.7 94.0 GraphQuestions en de es 47.2 56.9 39.9 48.4 39.5 51.6 Here we provide details on the syntactic analyzers employed, our entity resolution algorithm, and the features used by the grounding model. Table 2: Structured perceptron k-best entity linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input t"
D17-1009,R11-1065,0,0.0249782,"ntic interfaces is the reliance on rich typed feature structures or semantic types coupled with strong type constraints, which can be very informative but unavoidably language specific. Instead, UD EP L AMBDA relies on generic unlexicalized information present in dependency treebanks and uses a simple type system (one type for dependency labels, and one for words) along with a combinatory mechanism, which avoids type collisions. Earlier attempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base"
D17-1009,D16-1054,0,0.127651,"uctions such as control. The different treatments of various linguistic constructions in UD compared to SD also require different handling in UD EP L AMBDA (Section 3.3). Our experiments focus on Freebase semantic parsing as a testbed for evaluating the framework’s multilingual appeal. We convert natural language to logical forms which in turn are converted to machine interpretable formal meaning representations for retrieving answers to questions from Freebase. To facilitate multilingual evaluation, we provide translations of the English WebQuestions (Berant et al., 2013) and GraphQuestions (Su et al., 2016) datasets to German and Spanish. We demonstrate that UD EP L AMBDA can be used to derive logical forms for these languages using a minimal amount of language-specific knowledge. Aside from developing the first multilingual semantic parsing tool for Freebase, we also experimentally show that UD EP L AMBDA outperforms strong baselines across Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. Howe"
D17-1009,N15-3006,0,0.0206547,"g semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et a"
D17-1009,D16-1177,0,0.0268016,"Missing"
D17-1009,P16-1220,1,0.172318,"Missing"
D17-1009,N15-3014,0,0.0204028,"Missing"
D17-1009,P14-1090,0,0.131516,"Missing"
D17-1009,D16-1015,0,0.0546486,"Missing"
D17-1009,P15-1128,0,0.0788276,"Missing"
D18-1545,P17-2090,0,0.31702,"like image classification (Ciresan et al., 2012; Krizhevsky et al., 2012). Similarly, speech recognition systems made use of augmentation techniques like changing the tone and speed of the audio (Ko et al., 2015; Ragni et al., 2014), noise addition (Hartmann et al., 2016) and synthetic audio generation (Takahashi et al., 2016). Comparable techniques for data augmentation are less obvious for NLP tasks, due to structural differences among languages. There are only a small number of studies that tackle data augmentation techniques for NLP, such as Zhang et al. (2015) for text classification and Fadaee et al. (2017) for machine translation. In this work, we focus on languages with small training datasets, that are made available by the Universal Dependency (UD) project. These languages are dominantly from Uralic, Turkic, Slavic and Baltic language families, which are known to have extensive morphological case-marking systems and relatively free word order. With these languages in mind, we propose an easily adaptable, multilingual text augmentation technique based on dependency trees, inspired from two common augmentation methods from image processing: cropping and rotating. As images are cropped to focus"
D18-1545,W15-2112,0,0.0168064,"in Fig. 1b: her father wrote) and the objects (second and third row) by removing all dependency links other than the focus (with its subtree). Obviously, cropping may cause semantic shifts on a sentence-level. However it preserves local syntactic tags and even shallow semantic labels. Images are rotated around a chosen center with a certain degree to enhance the training data. Similarly, we choose the root as the center of the sentence and rotate the flexible tree fragments around the root for augmentation. Flexible fragments are usually defined by the morphological typology of the language (Futrell et al., 2015). For instance, languages close to analytical typology such as English, rarely have inflectional morphemes. They do not mark the objects/subjects, therefore words have to follow a strict order. For such languages, sentence rotation would mostly introduce noise. On the other hand, large number of languages such as Latin, Greek, Persian, Romanian, Assyrian, Turkish, Finnish and Basque have no strict word order (though there is a preferred order) due (1) (2) (3) (4) Babası yazdı bir mektup ona (SVOIO) Yazdı babası ona bir mektup (VSIOO) Bir mektup yazdı babası ona (OVSIO) Ona bir mektup yazdı bab"
D18-1545,P08-1040,0,0.0373772,". (2017) chooses low-frequency words instead of a random word, and generate synthetic sentence pairs that contain those rare words. 5 Discussion Unlike majority of previous NLP augmentation techniques, the proposed methods are meaningpreserving, i.e., they preserve the fundamental meaning of the sentence for most of the tested languages. Therefore can be used for variety of problems such as semantic role labeling, sentiment analysis, text classification. Instead of those problems, we evaluate the idea on the simplest possible task (POS) for the following reasons: Similar to sentence cropping, Vickrey and Koller (2008) define transformation rules to simplify sentences (e.g., I was not given a chance to eat - I 5007 • It gets harder to measure the impact of the idea as the system/task gets complicated due to large number of parameters. m zoo Figure 2: Treebank size versus gain by augmentation • POS tagging performance is a good indicator of performances of other structured prediction tasks, since POS tags are crucial features for higher-level NLP tasks. Our research interest was to observe which augmentation technique would improve which language, rather than finding one good model. Therefore we have not use"
D18-1545,P16-1002,0,0.0353142,"e improved accuracies of the augmented models. We would expect a similar result for Telugu. However Telugu treebank is entirely composed of sentences from a grammar book which may not be expressive and diverse. 4 Related Work ate) and shows that enrichening training set with simplified sentences improves the results of semantic role labeling. One of the first studies in text augmentation (Zhang et al., 2015), replaces a randomly chosen word with its randomly chosen synonym extracted from a thesaurus. They report improved test scores when a large neural model is trained with the augmented set. Jia and Liang (2016) induce grammar from semantic parsing training data and generate new data points by sampling to feed a sequence to sequence RNN model. Fadaee et al. (2017) chooses low-frequency words instead of a random word, and generate synthetic sentence pairs that contain those rare words. 5 Discussion Unlike majority of previous NLP augmentation techniques, the proposed methods are meaningpreserving, i.e., they preserve the fundamental meaning of the sentence for most of the tested languages. Therefore can be used for variety of problems such as semantic role labeling, sentiment analysis, text classifica"
D18-1545,D15-1176,0,0.0416076,"aracter-level model to address the rare word problem and to learn morphological regularities among words. For each sentence s, we produce a label sequence ~l, where lt refers to POS tag for the t-th token. Given g as gold labels and θ as model parameters we find the values that minimize the negative log likelihood of the sequence: ! n X θˆ = arg min − log(p(gt |θ, s)) (1) θ t=1 To calculate p(lt |θ, s), we first calculate a word embedding, w, ~ for each word. We consider words as a sequence of characters c0 , c1 , .., cn and use a bi-LSTM unit to compose the character sequence into w, ~ as in Ling et al. (2015): 2 Focus should not be confused with the grammatical category FOC. 5005 ~ f , hw ~ b = bi-LSTM(c0 , c1 , .., cn ) hw ~ f + Wb · hw ~b+b w ~ = Wf · hw (2) (3) Later, these embeddings are passed onto another bi-LSTM unit: h~f , h~b = bi-LSTM(w ~t ) (4) Hidden states from both directions are concatenated and mapped by a linear layer to the label space. Then label probabilities are calculated by a softmax function: p(~lt |s, p) = softmax(Wl · [h~f ; h~b ] + b~l ) (5) Finally the label with the highest probability is assigned to the input. 3 Experiments and Results We use the data provided by Univ"
D19-5321,C10-1096,0,0.0362489,"arity method as a baseline. We expect relying on embedding similarity to result in better candidate sets and in reduced number of merging and addition errors. Regardless of the embedding method, candidate selection requires us to pick k closest neighbours for a given node from all of the nodes of the KG. To do that we use random projection-based approximate nearest neighbour search algorithm implemented in the Annoy library5 . For candidate selection in the original string-based method we use the SimString library which performs approximate string matching according to the method proposed in (Okazaki and Tsujii, 2010), and we define the similarity between nd and nKG as the edit distance divided by the length of the shorter of the names. We use the development set to set k to 7 and string similarity threshold to 0.36 . For each test document, we build a DG using the original AskNET method. Each node in that graph is associated with a set of names and ELMo large and ELMo small embeddings. Using the GraphSAGE and hybrid models trained on the base KG we assign each node in DG a GraphSAGE and hybrid embeddings. Then, for each DG node we find five CKG sets, one for each method, and run the rest of the AskNEt alg"
D19-5321,N18-1202,0,0.0158434,"0.36 . For each test document, we build a DG using the original AskNET method. Each node in that graph is associated with a set of names and ELMo large and ELMo small embeddings. Using the GraphSAGE and hybrid models trained on the base KG we assign each node in DG a GraphSAGE and hybrid embeddings. Then, for each DG node we find five CKG sets, one for each method, and run the rest of the AskNEt algorithm for each set to Word-based embeddings Another approach to obtaining node embeddings is through word embeddings. We make use of of the ELMo model for deep contextualized word representation (Peters et al., 2018). We represent a node in a KG as an average of the word embeddings of every entity that has been resolved to that node during graph building. In other words, given a document we generate ELMo embeddings for every mention of every entity4 , and in the DG a node representing an entity is assigned an embedding which is an average of all the mentions of that entity. When nodes are merged during integration of the DG into the KG, the embedding of the KG node is updated so that it is always an average of the embeddings of all document-level nodes resolved to that KG node. For the purposes of our exp"
D19-5321,Q14-1030,1,0.82218,"We chose top n named entities such that when a KG is created of all the documents mentioning those n entities, the graph has approximately 15k nodes.2 Documents in our dataset are relatively short, averaging 338 words. The average number of named entities per document is 10.3. We held out 10 randomly selected documents each for development and test sets, and the rest forms the training set. 2.3 Base KG The KG used in our experiments is built out of the training set documents using the original AskNET algorithm. To obtain graphs representing individual sentences we use the semantic parser of (Reddy et al., 2014) and extract binary relations from its output. We only use relations which involve at least one named entity. The nodes in the graph are labeled with a set containing all strings from the original documents which have been linked to that node (e.g. a set containing Rowan Williams, Williams, Rev. Williams, Archbishop of Canterbury). The edges are labeled with relation names, where the relation set is open and can include any two place predicate used in the source documents. 1 Semantic parses used in AskNET follow a version of Discourse Representation Theory as implemented by the Boxer parser; i"
D19-5321,D13-1183,0,0.0416029,"Missing"
E03-1008,H91-1060,0,0.0320313,"Missing"
E03-1008,W99-0613,0,0.0609689,"Missing"
E03-1008,W01-0521,0,0.161063,"Missing"
E03-1008,J93-2004,0,0.0278308,"Missing"
E03-1008,W01-0501,0,0.185139,"Missing"
E03-1008,N01-1023,1,0.616825,"Missing"
E03-1008,P95-1026,0,0.323358,"Missing"
E03-1008,J03-4003,0,\N,Missing
E03-1008,P02-1046,0,\N,Missing
E12-1024,D11-1131,0,0.0825834,"Missing"
E12-1024,D10-1119,1,0.517337,"learning and that the learning algorithm must be strictly incremental: it sees each training instance sequentially and exactly once. We define a Bayesian model of parse structure with Dirichlet process priors and train this on a set of (utterance, meaning-candidates) pairs derived from the CHILDES corpus (MacWhinney, 2000) using online variational Bayesian EM. We evaluate the learnt grammar in three ways. First, we test the accuracy of the trained model in parsing unseen utterances onto gold standard annotations of their meaning. We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al., 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al.). We then examine the learning curves of some individual words, showing that the model can learn word meanings on the basis of a single exposure, similar to the fast mapping phenomenon observed in children (Carey and Bartlett, 1978). Finally, we show that our 1 Similar to referential uncertainty but relating to propositions rather than referents. 234 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguis"
E12-1024,D11-1140,1,0.744611,"Missing"
E12-1024,D08-1082,1,0.28321,"learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2 This linguistic use of the term ”parameter” is distinct from the statistical use found elsewhere in this paper. pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations (lists of noun phrases and additional corpus statistics), all of which we dispense with here. In particular, our approac"
E12-1024,sagae-etal-2004-adding,0,0.0577893,"ex function to read all of the lexical items off from the derivations in each {t}0 . In the parameter update step, the training algorithm updates the pseudocounts associated with each of the productions a → b that have ever been seen during training according to Equation (17). Only non-zero pseudocounts are stored in our model. The count vector is expanded with a new entry every time a new production is used. While Data The Eve corpus, collected by Brown (1973), contains 14, 124 English utterances spoken to a single child between the ages of 18 and 27 months. These have been hand annotated by Sagae et al. (2004) with labelled syntactic dependency graphs. An example annotation is shown in Figure 3. While these annotations are designed to represent syntactic information, the parent-child relationships in the parse can also be viewed as a proxy for the predicate-argument structure of the semantics. We developed a template based deterministic procedure for mapping this predicateargument structure onto logical expressions of the type discussed in Section 2.1. For example, the dependency graph in Figure 3 is automatically transformed into the logical expression λe.have(you,another(y, cookie(y)), e) (18) ∧"
E12-1024,N06-1056,0,0.0359022,"een data. Models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2 This linguistic use of the term ”parameter” is distinct from the statistical use found elsewhere in this paper. pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations (lists of noun phrases and additional corpus statistics), all of which we dispense with here. I"
E12-1024,P07-1121,0,0.25359,"Missing"
E12-1024,D07-1071,1,0.775604,"Missing"
E12-1024,P09-1110,1,0.394634,"Missing"
E14-1014,J07-4004,0,0.151191,"y lexicalized formalisms, the problem of grammar generalization can be cast largely as a problem of lexical extension. The present paper focuses on learning lexical categories for words that are unseen or lowfrequency in labeled data, from unlabeled data. Since lexical categories in a strongly lexicalized formalism are complex, fine-grained (and far more numerous than simple part-of-speech tags), they are relatively sparse in labeled data. Despite performing at state-of-the-art levels, a major source of error made by CCG parsers is related to unseen and low-frequency words (Hockenmaier, 2003; Clark and Curran, 2007; Thomforde and Steedman, 2011). The unseen words for which we learn categories are surprisingly commonplace words of English; examples are conquered, apprehended, subdivided, scoring, denotes, hunted, obsessed, residing, migrated (Wikipedia). Correctly learning to parse the predicate-argument structures associated with such words (expressed as lexical categories in the case of CCG), is important for opendomain parsing, not only for CCG but indeed for any parser. We show that a simple self-training method, Viterbi-EM (Neal and Hinton, 1998) when used to enhance the lexicon of a strongly-lexica"
E14-1014,W04-3215,1,0.816077,"Missing"
E14-1014,D08-1050,0,0.435833,"ts, we used sentences from the unlabeled WSJ portion of the ACL/DCI corpus (LDC93T1, 1993), and the WSJ portion of the ANC corpus (Reppen et al., 2005), limited to sentences containing 20 words or less, creating datasets of approximately 10, 20 and 40 million words each. Additionally, we have a dataset of 140 million words – 40M WSJ words plus an additional 100M from the New York Times. For domain-adaptation experiments, we use two different datasets. The first one consists of question-sentences – 1328 unlabeled questions, obtained by removing the manual annotation of the question corpus from Rimell and Clark (2008). The second out-of-domain dataset consists of Wikipedia data, approximately 40 million words in size, with sentence length &lt; 20 words. 5.2 Experimental setup We ran our semi-supervised method using our parser with a smoothed lexicon (from §4.1.1) as the initial model, on unlabeled data of different sizes/domains. For comparison, we also ran experiments using a POS-backed off parser (the original Hockenmaier and Steedman (2002) LexCat model) as the initial model. Viterbi-EM converged at 4-5 iterations. We then parsed various test sets using the semi-supervised lexicons thus obtained. In all ex"
E14-1014,P10-1152,0,0.0235071,"are now in the lexicon. Lexical entries for words in the parse are determined not by the POS-tag from a tagger, but directly by the parsing model, thus making the parse less susceptible to tagging errors. 5 Semi-supervised Learning We use Viterbi-EM (Neal and Hinton, 1998) as the self-training method. Viterbi-EM is an alternative to EM where instead of using the model parameters to find a true posterior from unlabeled data, a posterior based on the single maximumprobability (Viterbi) parse is used. Viterbi-EM has been used in various NLP tasks before and often performs better than classic EM (Cohen and Smith, 2010; Goldwater and Johnson, 2005; Spitkovsky et al., 2010). In practice, a given parsing model is used to obtain Viterbi parses of unlabeled sentences. The Viterbi parses are then treated as training data for a new model. This process is iterated until convergence. Since we are interested in learning the lexicon, we only consider lexical counts from Viterbi parses of the unlabeled sentences. Other parameters of the model are held at their supervised values. We conducted some experiments where we 3 For instance, we find that assigning all categories to unseen verbs gives a lexical category accurac"
E14-1014,W10-2902,0,0.0505347,"Missing"
E14-1014,P97-1003,0,0.328252,"an (2002) and Hockenmaier (2003)1 , except for the treatment of unseen and low-frequency words. We use a model (the LexCat model in Hockenmaier (2003)) that conditions the generation of constituents in the parse tree on the lexical category of the head word of the constituent, but not on the head word itself. While fully-lexicalized models that condition on words (and thus model wordto-word dependencies) are more accurate than unlexicalized ones like the LexCat model, we use an unlexicalized model2 for two reasons: first, 1 These generative models are similar to the Collins’ headbased models (Collins, 1997), where for every node, a head is generated first, and then a sister conditioned on the head. Details of the models are in Hockenmaier and Steedman (2002) and Hockenmaier 2003:pg 166. 2 A terminological clarification: unlexicalized here refers to the model, in the sense that head-word information is not used for rule-expansion. The formalism itself (CCG) is referred to as strongly-lexicalized, as used in the title of the paper. Formalisms like CCG and LTAG are considered strongly-lexicalized since linguistic knowledge (functions mapping words to syntactic structures/semantic interpretations) i"
E14-1014,C08-1025,1,0.765131,"about the word (for instance, its part-of-speech). Based on the part-of-speech of an unseen word in the unlabeled or test corpus, we add an entry to the lexicon of the word with the top n categories that have been seen with that part-of-speech in the labeled data. Each new entry of (w, cat), where w is a word and cat is a CCG category, is associated with a count c(w, cat), obtained as described below. Once all (w, cat) entries are added to the lexicon along with their counts, a probability model P (w|cat) is calculated over the entire lexicon. Our smoothing method is based on a method used in Deoskar (2008) for smoothing a PCFG lexicon. Eq. 1 and 2 apply it to CCG entries for unseen and rare words. In the first step, an outof-the-box POS tagger is used to tag the unlabeled or test corpus (we use the C&C tagger). Counts of words and POS-tags ccorpus (w, T ) are obtained from the tagged corpus. For the CCG lexicon, we ultimately need a count for a word w and a CCG category cat. To get this count, we split the count of a word and POS-tag amongst all categories seen with that tag in the supervised data in the same ratio as the ratio of the categories in the supervised data. In Eq. 1, this ratio is c"
E14-1014,W01-0521,0,0.030695,"een words. 1 Introduction An important open problem in natural language parsing is to generalize supervised parsers, which are trained on hand-labeled data, using unlabeled data. The problem arises because further handlabeled data in the amounts necessary to significantly improve supervised parsers are very unlikely to be made available. Generalization is also necessary in order to achieve good performance on parsing in textual domains other than the domain of the available labeled data. For example, parsers trained on Wall Street Journal (WSJ) data suffer a fall in accuracy on other domains (Gildea, 2001). In this paper, we use self-training to generalize the lexicon of a Combinatory Categorial Grammar (CCG) (Steedman, 2000) parser. CCG is a strongly lexicalized formalism, in which every word is associated with a syntactic category (similar to an elementary syntactic structure) indicat126 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 126–134, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Charniak, 1993). We found that in order for performance to improve, unlabeled data should be used o"
E14-1014,D09-1058,0,0.0187492,"n word is derived from partial parses of sentences with just that one word unknown. The method is capable of inducing unseen categories types (the third kind of sparsity mentioned in §2.1), but due to algorithmic and efficiency issues, it did not achieve the broad-coverage needed for grammar generalisation of a high-end parser. It is more relevant for low-resource languages which do not have substantial labeled data and category type discovery is important. Some notable positive results for non-CCG parsers are McClosky et al. (2006) who use a parser-reranker combination. Koo et al. (2008) and Suzuki et al. (2009) use unsupervised wordclusters as features in a dependency parser to get lexical dependencies. This has some notional similarity to categories, since, like categories, clusters are less fine-grained than words but more finegrained than POS-tags. 4 our lexicon smoothing procedure (described in the next section) introduces new words and new categories for words into the lexicon. Lexical categories are added to the lexicon for seen and unseen words, but no new category types are introduced. Since the LexCat model conditions rule expansions on lexical categories, but not on words, it is still able"
E14-1014,P02-1043,1,0.856429,"s in fact more generally relevant than for CCG parsers alone—the dependence of parsers on POS-taggers was cited as one of the problems in domain-adaptation of parsers in the NAACL2012 shared task on parsing the web (Petrov and McDonald, 2012). Lease and Charniak (2005) obtained an improvement in the accuracy of the Charniak (2000) parser on a biomedical domain simply by training a new POS tagger model. In the following section, we describe an alternative smoothing-based approach to handling unSupervised Parser The CCG parser used in this paper is a reimplementation of the generative parser of Hockenmaier and Steedman (2002) and Hockenmaier (2003)1 , except for the treatment of unseen and low-frequency words. We use a model (the LexCat model in Hockenmaier (2003)) that conditions the generation of constituents in the parse tree on the lexical category of the head word of the constituent, but not on the head word itself. While fully-lexicalized models that condition on words (and thus model wordto-word dependencies) are more accurate than unlexicalized ones like the LexCat model, we use an unlexicalized model2 for two reasons: first, 1 These generative models are similar to the Collins’ headbased models (Collins,"
E14-1014,D11-1115,1,0.823079,", the problem of grammar generalization can be cast largely as a problem of lexical extension. The present paper focuses on learning lexical categories for words that are unseen or lowfrequency in labeled data, from unlabeled data. Since lexical categories in a strongly lexicalized formalism are complex, fine-grained (and far more numerous than simple part-of-speech tags), they are relatively sparse in labeled data. Despite performing at state-of-the-art levels, a major source of error made by CCG parsers is related to unseen and low-frequency words (Hockenmaier, 2003; Clark and Curran, 2007; Thomforde and Steedman, 2011). The unseen words for which we learn categories are surprisingly commonplace words of English; examples are conquered, apprehended, subdivided, scoring, denotes, hunted, obsessed, residing, migrated (Wikipedia). Correctly learning to parse the predicate-argument structures associated with such words (expressed as lexical categories in the case of CCG), is important for opendomain parsing, not only for CCG but indeed for any parser. We show that a simple self-training method, Viterbi-EM (Neal and Hinton, 1998) when used to enhance the lexicon of a strongly-lexicalized parser can be an effectiv"
E14-1014,W09-3306,0,0.419633,"erent sizes of unlabeled data on accuracy of unseen verbs for the two testsets TEST- HOV and TEST-4 SEC . Improvements are monotonic with increasing unlabeled data sizes, up to 40M words. The additional 100M words of NYT also improve the models but to a lesser degree, possibly due to the difference in domain. The graphs indicate that the method will lead to more improvements as more unlabeled data (especially WSJ data) is added. Wikipedia We obtain statistically significant improvements in overall scores over a testset consisting of Wikipedia sentences hand-annotated with CCG categories (from Honnibal et al. (2009)) (Table 4). We also obtained improvements in lexical category accuracy on unseen words, and on unseen verbs alone (not shown), but could not prove significance. This testset contains only 200 sentences, and counts for unseen words are too small for significance tests, although there are numeric improvements. However, the overall improvement is statistically significantly, showing that adapting the lexicon alone is effective for a new domain. 7 This could be because verbs in the Zipfian tail have more idiosyncratic subcategorization patterns than mid-frequency verbs, and thus are harder for a"
E14-1014,P08-1068,0,0.0504195,"category for an unknown word is derived from partial parses of sentences with just that one word unknown. The method is capable of inducing unseen categories types (the third kind of sparsity mentioned in §2.1), but due to algorithmic and efficiency issues, it did not achieve the broad-coverage needed for grammar generalisation of a high-end parser. It is more relevant for low-resource languages which do not have substantial labeled data and category type discovery is important. Some notable positive results for non-CCG parsers are McClosky et al. (2006) who use a parser-reranker combination. Koo et al. (2008) and Suzuki et al. (2009) use unsupervised wordclusters as features in a dependency parser to get lexical dependencies. This has some notional similarity to categories, since, like categories, clusters are less fine-grained than words but more finegrained than POS-tags. 4 our lexicon smoothing procedure (described in the next section) introduces new words and new categories for words into the lexicon. Lexical categories are added to the lexicon for seen and unseen words, but no new category types are introduced. Since the LexCat model conditions rule expansions on lexical categories, but not o"
E14-1014,I05-1006,0,0.0253009,"rategy has given good performance in general for CCG parsers, but it has the disadvantage that POS tagging errors are propagated. The parser can never recover from a tagging error, a problem that is serious for words in the Zipfian tail, where these words might also be unseen for the POS tagger and hence more likely to be tagged incorrectly. This issue is in fact more generally relevant than for CCG parsers alone—the dependence of parsers on POS-taggers was cited as one of the problems in domain-adaptation of parsers in the NAACL2012 shared task on parsing the web (Petrov and McDonald, 2012). Lease and Charniak (2005) obtained an improvement in the accuracy of the Charniak (2000) parser on a biomedical domain simply by training a new POS tagger model. In the following section, we describe an alternative smoothing-based approach to handling unSupervised Parser The CCG parser used in this paper is a reimplementation of the generative parser of Hockenmaier and Steedman (2002) and Hockenmaier (2003)1 , except for the treatment of unseen and low-frequency words. We use a model (the LexCat model in Hockenmaier (2003)) that conditions the generation of constituents in the parse tree on the lexical category of the"
E14-1014,N06-1020,0,0.343354,"data is that of Thomforde and Steedman (2011), in which a CCG category for an unknown word is derived from partial parses of sentences with just that one word unknown. The method is capable of inducing unseen categories types (the third kind of sparsity mentioned in §2.1), but due to algorithmic and efficiency issues, it did not achieve the broad-coverage needed for grammar generalisation of a high-end parser. It is more relevant for low-resource languages which do not have substantial labeled data and category type discovery is important. Some notable positive results for non-CCG parsers are McClosky et al. (2006) who use a parser-reranker combination. Koo et al. (2008) and Suzuki et al. (2009) use unsupervised wordclusters as features in a dependency parser to get lexical dependencies. This has some notional similarity to categories, since, like categories, clusters are less fine-grained than words but more finegrained than POS-tags. 4 our lexicon smoothing procedure (described in the next section) introduces new words and new categories for words into the lexicon. Lexical categories are added to the lexicon for seen and unseen words, but no new category types are introduced. Since the LexCat model co"
E14-1014,J94-2001,0,0.0411992,"parser. We show that a simple self-training method, Viterbi-EM (Neal and Hinton, 1998) when used to enhance the lexicon of a strongly-lexicalized parser can be an effective strategy for self-training and domain-adaptation. Our learnt lexicons improve on the lexical category accuracy of two supervised CCG parsers (Hockenmaier (2003) and the Clark and Curran (2007) parser, C&C) on within-domain (WSJ) and out-of-domain test sets (a question corpus and a Wikipedia corpus). In most prior work, when EM was initialized based on labeled data, its performance did not improve over the supervised model (Merialdo, 1994; Statistical parsers trained on labeled data suffer from sparsity, both grammatical and lexical. For parsers based on strongly lexicalized grammar formalisms (such as CCG , which has complex lexical categories but simple combinatory rules), the problem of sparsity can be isolated to the lexicon. In this paper, we show that semi-supervised Viterbi-EM can be used to extend the lexicon of a generative CCG parser. By learning complex lexical entries for low-frequency and unseen words from unlabeled data, we obtain improvements over our supervised model for both indomain (WSJ) and out-of-domain (q"
E14-1014,Q13-1015,1,\N,Missing
E14-1014,J07-3004,1,\N,Missing
E14-1014,W05-0615,0,\N,Missing
E14-1066,D12-1007,1,0.808172,"Missing"
E14-1066,P07-1102,0,0.0752834,"Missing"
E14-1066,P08-2019,0,0.0301654,"ction Automated dialog management is an area of research that has undergone rapid advancement in the last decade. The driving force of this innovation has been the rise of the statistical paradigm for monitoring dialog state, reasoning about the effects of possible dialog moves, and planning future actions (Young et al., 2013). Statistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational 1.1 Spatial Goals of Users Users in task-oriented domains are goal-directed, with a persistent notion of what they wish to accomplish from the dialog. In slot-filling domains, goals are comprised of a group of categorical entities, represented as slot-value pairs. These entities can be placed directly into the user’s utteranc"
E14-1066,I11-1067,0,0.0232779,"njunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to potentially interesting states. An alternative approach to dealing with the lack of consistency is to extend N-grams to explicitly model user goals and condition utterances on them (Pietquin, 2004; Cuay´ahuitl et al., 2005; Pietquin and Dutoit, 2006; Rossignol et al., 2010; Rossignol et al., 2011; Eshky et al., 2012). 3 (1) p (g, u|f, W ) = p(u) p(g|u) p(f |g) p(W |u) P 0 0 0 0 0 g 0 u0 p(u ) p(g |u ) p(f |g ) p(W |u ) (2) requiring four distributions: p(u), p(g|u), p(f |g), and p(W |u). The first three become the semantic component of our model, to which we dedicate Section 3.1. The fourth is the spatio-semantic component, to which we dedicate Sections 3.2– 3.4. The Model 2 We align g and u in a preprocessing step, and store the names of landmarks which the units abstract away from. 3 Further advancements to this work would investigate the effects of relaxing the conditional independ"
E14-1066,P00-1013,0,0.0980346,"differ significantly from the behaviour of real users. 1 Introduction Automated dialog management is an area of research that has undergone rapid advancement in the last decade. The driving force of this innovation has been the rise of the statistical paradigm for monitoring dialog state, reasoning about the effects of possible dialog moves, and planning future actions (Young et al., 2013). Statistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational 1.1 Spatial Goals of Users Users in task-oriented domains are goal-directed, with a persistent notion of what they wish to accomplish from the dialog. In slot-filling domains, goals are comprised of a group of categorical entities, represented as slot-v"
E14-1066,H93-1005,0,0.22726,"Missing"
E14-1066,P10-1083,0,0.0346081,"current goal on which they base their behaviour. Because the routes are drawn on the Giver’s maps, we treat W as observed. To model some of the interaction between the Giver and the Follower, we additionally consider in our model the previous dialog act of the Follower, which could for example be: Related Work Related Work on the Map Task To our knowledge, there are no attempts to model instruction Givers as users in the Map Task domain. Two studies model the Follower, in the context of understanding natural language instructions and interpreting them by drawing a route (Levit and Roy, 2007; Vogel and Jurafsky, 2010). Both studies exclude dialog from their modelling. Although their work is not directly comparable to ours, they provide a corpus suitable for our task. 2.2 f = Acknowledge Given point-set W and preceding Follower act f , as the giver, we need to determine a procedure for choosing which dialog act g and semantic unit u to produce. In other words, we are interested in the following distribution: p (g, u|f, W ) Related Work on User Simulation which says that, as the Giver, we select our utterances on the basis of what the Follower says, and on the set of waypoints we next wish to describe. To fo"
E14-1066,W07-0301,0,0.0248638,"has been the rise of the statistical paradigm for monitoring dialog state, reasoning about the effects of possible dialog moves, and planning future actions (Young et al., 2013). Statistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational 1.1 Spatial Goals of Users Users in task-oriented domains are goal-directed, with a persistent notion of what they wish to accomplish from the dialog. In slot-filling domains, goals are comprised of a group of categorical entities, represented as slot-value pairs. These entities can be placed directly into the user’s utterance. For example, in a flight booking domain, if a user’s goal is to fly to London from New York on the 3rd of November, then the goal takes t"
E14-1066,W12-1619,0,\N,Missing
E14-1066,P13-1163,0,\N,Missing
E14-4031,P13-2107,1,0.720018,"Missing"
E14-4031,J93-2004,0,0.045459,"or Hindi, Malt uses the arc-standard parsing algorithm with a LIBLINEAR learner. MST uses 2nd-order features, nonprojective algorithm with 5-best MIRA training. For English, we assigned POS-tags using a perceptron tagger (Collins, 2002). For Hindi, we also did all our experiments using automatic features Treebanks In English dependency parsing literature, Stanford and CoNLL dependency schemes are widely popular. We used the Stanford parser’s built-in converter (with the basic projective option) to generate Stanford dependencies and Penn2Malt1 to generate CoNLL dependencies from Penn Treebank (Marcus et al., 1993). We used standard splits, training (sections 02-21), development (section 22) and testing (section 23) for our experiments. For Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 2012 Shared Task (Bharati et al., 2012). HDT contains 12,041 training, 1,233 development and 1,828 testing sentences. We used the English (Hockenmaier and Steedman, 2007) and Hindi CCGbanks (Ambati et al., 1 Supertaggers http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html 160 Language English Hindi Stanford Baseline Stanford + CCG Malt UAS LAS 90.32 87.87 90.56** (2.5) 88.16** (2.5)"
E14-4031,P05-1012,0,0.149734,"Missing"
E14-4031,W06-2920,0,0.193309,"Missing"
E14-4031,P05-2013,0,0.0905221,"Missing"
E14-4031,C04-1041,0,0.315142,"Missing"
E14-4031,P07-1079,0,0.0430903,"Missing"
E14-4031,W02-1001,0,0.0302037,"and Nivre, 2012; Bharati et al., 2012) and treat them as our baseline. In the case of English, Malt uses arc-standard and stack-projective parsing algorithms for CoNLL and Stanford schemes respectively and LIBLINEAR learner (Fan et al., 2008) for both the schemes. MST uses 1st-order features, and a projective parsing algorithm with 5-best MIRA training for both the schemes. For Hindi, Malt uses the arc-standard parsing algorithm with a LIBLINEAR learner. MST uses 2nd-order features, nonprojective algorithm with 5-best MIRA training. For English, we assigned POS-tags using a perceptron tagger (Collins, 2002). For Hindi, we also did all our experiments using automatic features Treebanks In English dependency parsing literature, Stanford and CoNLL dependency schemes are widely popular. We used the Stanford parser’s built-in converter (with the basic projective option) to generate Stanford dependencies and Penn2Malt1 to generate CoNLL dependencies from Penn Treebank (Marcus et al., 1993). We used standard splits, training (sections 02-21), development (section 22) and testing (section 23) for our experiments. For Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 20"
E14-4031,P13-2108,1,0.893725,"Missing"
E14-4031,P13-1103,0,0.0366524,"Missing"
E14-4031,de-marneffe-etal-2006-generating,0,0.03341,"Missing"
E14-4031,P11-2033,0,0.107597,"Missing"
E14-4031,C12-2136,0,0.0243632,"Missing"
E14-4031,J07-3004,1,0.676056,"NLL dependency schemes are widely popular. We used the Stanford parser’s built-in converter (with the basic projective option) to generate Stanford dependencies and Penn2Malt1 to generate CoNLL dependencies from Penn Treebank (Marcus et al., 1993). We used standard splits, training (sections 02-21), development (section 22) and testing (section 23) for our experiments. For Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 2012 Shared Task (Bharati et al., 2012). HDT contains 12,041 training, 1,233 development and 1,828 testing sentences. We used the English (Hockenmaier and Steedman, 2007) and Hindi CCGbanks (Ambati et al., 1 Supertaggers http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html 160 Language English Hindi Stanford Baseline Stanford + CCG Malt UAS LAS 90.32 87.87 90.56** (2.5) 88.16** (2.5) MST UAS LAS 90.36 87.18 90.93** (5.9) 87.73** (4.3) CoNLL Baseline CoNLL + CCG 89.99 90.38** (4.0) 88.73 89.19** (4.1) 90.94 91.48** (5.9) 89.69 90.23** (5.3) Baseline Fine CCG Coarse CCG 88.67 88.93** (2.2) 89.04** (3.3) 83.04 83.23* (1.1) 83.35* (1.9) 90.52 90.97** (4.8) 90.88** (3.8) 80.67 80.94* (1.4) 80.73* (0.4) Experiment Table 1: Impact of CCG categories from a supertagger"
E14-4031,C12-1088,0,0.036192,"Missing"
E14-4031,D07-1096,0,\N,Missing
E93-1039,C90-2025,0,0.234302,"Missing"
E93-1039,P87-1012,1,0.812244,"Missing"
E93-1039,P90-1002,1,0.93639,"Missing"
E93-1039,P91-1010,1,\N,Missing
H89-1038,P87-1012,1,\N,Missing
H89-1038,P83-1020,0,\N,Missing
H89-1038,P87-1011,0,\N,Missing
H94-1035,P88-1023,0,0.0269588,"e g r a t e s t h e n o t i o n s of s y n t a c t i c constituency, prosodic p h r a s i n g and i n f o r m a t i o n s t r u c t u r e . T h e m o d e l d e t e r m i n e s accent locations within p h r a s e s on t h e basis of c o n t r a s t i v e sets derived f r o m t h e discourse s t r u c t u r e and a d o m a i n - i n d e p e n d e n t knowledge base. 1. I n t r o d u c t i o n Previous work in the area of intonation generation includes an early study by Young and Fallside ([26]), and studies by Terken ([24]), Houghton, Isard and Pearson (cf. [11, 12]), Davis and Hirschberg (cf. [4, 10]), and Zacharski et ah ([27]). The present proposal differs from the earlier studies in the accent assignment rules, and in the representation of information structure and its relation to syntax and semantics. In the CCG framework, the information units that are delineated by intonation are directly represented, complete with semantic interpretations. These interpretations are utilized in making accent placement decisions on the basis of contrastive properties rather than previous-mention heuristics: While such heuristics have proven quite effective in the earlier studies, we believe the model"
H94-1035,C90-2025,0,0.0564587,"Missing"
H94-1035,E93-1039,1,0.841221,"ies rather than previous-mention heuristics: While such heuristics have proven quite effective in the earlier studies, we believe the model-theoretic approach taken here will eventually lead to the development of similar heuristics for handling a wider range of examples involving contrastive stress. The remainder of the paper discusses the contrastive stress model, describes the implemented system, and presents results demonstrating the system's ability to generate a variety of intonational possibilities for a given *Preliminary versions of some sections in the present paper were published as [17] and [lS]. We are grateful to the audiences at those meetings, to AT&T Bell Laboratories for allowing us access to the TTS speech synthesizer, to Mark Beutnagel, Julia Hirschberg, and Richard Sproat for patient advice on its use, to Abigail Gertner for advice on TraumAID, and to Janet Pierrehumbert for discussions on notation. The usual disclaimers apply. The research was supported in part by NSF grant nos. IRI90-18513, IRI9016592, IRI91-17110 and CISE IIP-CDA-88-22719, DARPA grant no. N00014-90-J-1863, ARO grant no. DAAL03-89-C0031, and grant no. R01-LM05217 from the National Library of Medic"
H94-1035,C92-1038,0,0.0420871,"Missing"
hockenmaier-steedman-2002-acquiring,J91-3003,0,\N,Missing
hockenmaier-steedman-2002-acquiring,E89-1002,0,\N,Missing
hockenmaier-steedman-2002-acquiring,J03-4003,0,\N,Missing
hockenmaier-steedman-2002-acquiring,P96-1011,0,\N,Missing
hockenmaier-steedman-2002-acquiring,P90-1024,0,\N,Missing
I11-1049,P09-1041,0,0.0901078,"Missing"
I11-1049,P11-1048,0,0.0128064,"f error are in adverbial and prepositional attachment, and NP structural ambiguity, which are also problematic for supervised parsing. Future work remains as follows. First, we are looking forward to improving the capability of our syntactic prototype to also handle free word ordered languages by generating syntactic categories with more flexible combination and restraining the search space with inflectional information. Second, we plan to experiment on grammar induction from untagged words by decomposing the model into tagging and parsing subproblems (Ganchev et al., 2009; Rush et al., 2010; Auli and Lopez, 2011). Third and finally, we will experiment on longer sentences to show the scalability of our approach in dealing with larger data. Conclusion We have demonstrated an efficient approach to grammar induction using linguistic prior knowledge encoded as a prototype or lexical schema. This prior knowledge was used to capture frequent linguistic phenomena. To integrate the strength of constituent and dependency models, Categorial Dependency Grammar was used as the backbone formalism. We also proposed a category penalty score preferring less complex categories, based on the observation of the Zipfian d"
I11-1049,P10-2036,0,0.0274373,"4! 79.04! 1! Figure 3: Effects of category penalty on the directed and undirected dependency accuracy. dard produced by Collins’s parser. We notice that both accuracy scores saturate at the penalty constant of 10−15 and slightly decay afterwards. 4.3.3 Multilingual Experiments We also conducted multilingual experiments on Chinese, Czech, German, and Japanese to show the stability of the approach. We ran ten-fold cross validation on each language and calculated the average F1 scores. Our baseline systems are as follows: (Naseem et al., 2010) for English, (Snyder et al., 2009) for Chinese, and (Gillenwater et al., 2010) for Czech, German, and Japanese. In Table 3, our system significantly outperforms almost all the baselines, except in the Czech experiment. We believe that the under-performance of our system on Czech is caused by the data sparsity issue. Designed based on rather fixed word ordered languages, the syntactic prototype needs to generate almost all possible syntactic categories to capture Czech’s free word orderedness. Although its POS 443 Table 2: Undirected and directed dependency accuracy of grammar induction on English Penn Treebank. The baseline for English is (Naseem et al., 2010). Undirect"
I11-1049,P06-1111,0,0.125276,"Missing"
I11-1049,N09-1012,0,0.188241,"Missing"
I11-1049,W06-2920,0,0.0414727,"of trees from WSJ part of PTB (Marcus et al., 1993) whose sentence length does not exceed ten words after taking out punctuation marks and empty elements. In stead of surface forms, we used a set of POS sequences taken from all WSJ10’s trees to avoid the data sparsity issue. We converted the Penn Treebank into dependency structures with Collins (1999)’s head percolation heuristics. Following the literature, we trained the system with sections 2–22 and evaluated the resultant model on section 23. For multilingual experiments, we made use of dependency corpora from the 2006 CoNLL X Shared Task (Buchholz and Marsi, 2006). Shown in Table 1, Chinese (Keh-Liann and Hsieh, 2004), 442 4.3 Results Effects of constraint sizes towards accuracy! This section presents results of English and multilingual experiments using syntactic prototypes as a guide to grammar induction. Depedency Accuracy! 90! 4.3.1 Experiments on English Table 2 shows experiment results of English grammar induction on WSJ10. In the beginning, we compared the produced trees against the gold standard produced from Collins’s parser. We trained the system with Sections 2–22 of WSJ10 and tested it on Section 23. In decoding, we set the category penalty"
I11-1049,J93-2004,0,0.0453644,"Missing"
I11-1049,P03-1046,0,0.0293193,"ate expected counts using Dynamic Programming (Baker, 1979; Lari and Young, 1990). To further avoid the data over-fitting issue, we smoothed the probability of each substructure with the additive smoothing technique (Lidstone, 1920; Johnson, 1932; Jeffreys, 1948). An approximated parameter π ˆ (τ ) is calculated by We extend the probabilistic context-free grammar with role-emission probabilities, defined as the product of the probability of each daughter category performing as a head or a dependent in a derivation. This model was motivated by Collins (1999)’s head-outward dependency model and Hockenmaier (2003)’s generative model for parsing CCG. Given a CDG G, we define the probability of a tree t having the constituent type C : w by: = P t∈Q Parsing Model P (t|s, G) = P (4) πHE (w|C, G)#t (C:w) C:w∈N (t) where Z is a normalization constant and each production α contains H : w and D : w0 , and H : w and D : w0 are the head and the dependent, respectively. There are four types of parameters as follows. π ˆ (τ ) = 1. πexp (α|C : w, G): probability of the type C : w generating a production α. π(τ ) +  1+ (8) where  is a small constant value. In our experiments, we chose  = 10−25 . 2. πhead (C : w|"
I11-1049,D10-1120,0,0.526081,"has been proposed. Instead of enumerating every possibility, syntactic structures are cautiously constructed regarding some syntactic constraints. Haghighi and Klein (2006) proposed the use of bracketing rules extracted from WSJ10 in CCM and considerably improved accuracy. Druck et al. (2009) used dependency formation rules handcrafted by linguists to improve the accuracy of DMV. Snyder et al. (2009) do semisupervised grammar induction from bilingual text with the help of a supervised parser on one side and word alignment. However, bilingual corpora are not available for many language pairs. Naseem et al. (2010) proposed the use of crosslinguistic knowledge represented as a set of allowable head-dependent pairs. However, this method still requires provision of language-specific rules to boost accuracy. If language-specific rules are necessary to achieve accuracy, we need more efficient ways to encode this knowledge. This paper proposes a method for inducing language-specific word order regularities capturWe present an efficient technique to incorporate a small number of cross-linguistic parameter settings defining default word orders to otherwise unsupervised grammar induction. A syntactic prototype,"
I11-1049,D10-1001,0,0.0119621,"the main sources of error are in adverbial and prepositional attachment, and NP structural ambiguity, which are also problematic for supervised parsing. Future work remains as follows. First, we are looking forward to improving the capability of our syntactic prototype to also handle free word ordered languages by generating syntactic categories with more flexible combination and restraining the search space with inflectional information. Second, we plan to experiment on grammar induction from untagged words by decomposing the model into tagging and parsing subproblems (Ganchev et al., 2009; Rush et al., 2010; Auli and Lopez, 2011). Third and finally, we will experiment on longer sentences to show the scalability of our approach in dealing with larger data. Conclusion We have demonstrated an efficient approach to grammar induction using linguistic prior knowledge encoded as a prototype or lexical schema. This prior knowledge was used to capture frequent linguistic phenomena. To integrate the strength of constituent and dependency models, Categorial Dependency Grammar was used as the backbone formalism. We also proposed a category penalty score preferring less complex categories, based on the obser"
I11-1049,P09-1009,0,0.0190527,"79.2! 79.27! 79.13! 79.03! 79.11! 79.14! 79.04! 1! Figure 3: Effects of category penalty on the directed and undirected dependency accuracy. dard produced by Collins’s parser. We notice that both accuracy scores saturate at the penalty constant of 10−15 and slightly decay afterwards. 4.3.3 Multilingual Experiments We also conducted multilingual experiments on Chinese, Czech, German, and Japanese to show the stability of the approach. We ran ten-fold cross validation on each language and calculated the average F1 scores. Our baseline systems are as follows: (Naseem et al., 2010) for English, (Snyder et al., 2009) for Chinese, and (Gillenwater et al., 2010) for Czech, German, and Japanese. In Table 3, our system significantly outperforms almost all the baselines, except in the Czech experiment. We believe that the under-performance of our system on Czech is caused by the data sparsity issue. Designed based on rather fixed word ordered languages, the syntactic prototype needs to generate almost all possible syntactic categories to capture Czech’s free word orderedness. Although its POS 443 Table 2: Undirected and directed dependency accuracy of grammar induction on English Penn Treebank. The baseline fo"
I11-1049,N10-1116,0,0.0408041,"Missing"
I11-1049,P02-1017,0,0.0999797,"n of F1), and is not significantly different from the baseline for Czech10. 1 Introduction Unsupervised grammar induction has gained general interest for several decades, offering the possibility of building practical syntactic parsers by reducing the labor of constructing a treebank from scratch. One approach is to exploit the Inside/Outside Algorithm (Baker, 1979; Carroll and Charniak, 1992), a variation of EM algorithm for PCFG, to estimate the parameters of the parser’s language models. More recent advances in this approach are the constituent-context model (CCM) (Klein and Manning, 2001; Klein and Manning, 2002), dependency model with valence (DMV) based on Collin’s head dependency model (1999), and the CCM+DMV mixture (Klein and Manning, 2004; Klein, 2005). Several search techniques and 438 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 438–446, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP be assigned the category np/&gt; np, while a transitive verb can be assigned s&gt; np/&lt; np. CDG differs from PF-CCG (Koller and Kuhlmann, 2009) in that dependency direction is specified independently from the order of function and argument, while theirs is deter"
I11-1049,P04-1061,0,0.134122,"neral interest for several decades, offering the possibility of building practical syntactic parsers by reducing the labor of constructing a treebank from scratch. One approach is to exploit the Inside/Outside Algorithm (Baker, 1979; Carroll and Charniak, 1992), a variation of EM algorithm for PCFG, to estimate the parameters of the parser’s language models. More recent advances in this approach are the constituent-context model (CCM) (Klein and Manning, 2001; Klein and Manning, 2002), dependency model with valence (DMV) based on Collin’s head dependency model (1999), and the CCM+DMV mixture (Klein and Manning, 2004; Klein, 2005). Several search techniques and 438 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 438–446, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP be assigned the category np/&gt; np, while a transitive verb can be assigned s&gt; np/&lt; np. CDG differs from PF-CCG (Koller and Kuhlmann, 2009) in that dependency direction is specified independently from the order of function and argument, while theirs is determined by slash directionality. For them such an adjective has the implicit category np/&gt; np and acts as the head of the noun phrase. T"
I11-1049,E09-1053,0,0.0271229,"language models. More recent advances in this approach are the constituent-context model (CCM) (Klein and Manning, 2001; Klein and Manning, 2002), dependency model with valence (DMV) based on Collin’s head dependency model (1999), and the CCM+DMV mixture (Klein and Manning, 2004; Klein, 2005). Several search techniques and 438 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 438–446, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP be assigned the category np/&gt; np, while a transitive verb can be assigned s&gt; np/&lt; np. CDG differs from PF-CCG (Koller and Kuhlmann, 2009) in that dependency direction is specified independently from the order of function and argument, while theirs is determined by slash directionality. For them such an adjective has the implicit category np/&gt; np and acts as the head of the noun phrase. The derivation rules for context-free CDG are listed below: ing cross-linguistically frequent constructions to constrain unsupervised grammar induction. We use the notion of syntactic prototype, a set of grammar rules automatically generated for such constructions. Categorial Dependency Grammar (CDG), which combines rules of constituency and depe"
I11-1049,J03-4003,0,\N,Missing
J07-3004,P90-1024,0,0.228488,"ence, modifiers of used would also receive different categories depending on what occurrence of used they modify. This is undesirable, because we are only guaranteed to acquire a complete lexicon if we have seen all participles (and their possible modifiers) in all their possible surface positions. Similar regularities have been recognized and given a categorial analysis by Carpenter (1992), who advocates lexical rules to account for the use of predicatives as adjuncts. In a statistical model, the parameters for such lexical rules are difficult to estimate. We therefore follow the approach of Aone and Wittenburg (1990) and implement these type-changing 367 Computational Linguistics Volume 33, Number 3 Figure 3 Type-changing rules reduce the number of lexical category types required for complex adjuncts. operations in the derivational syntax, where these generalizations are captured in a few rules. If these rules apply recursively to their own output, they can generate an infinite set of category types, leading to a shift in generative power from context-free to recursively enumerable (Carpenter 1991, 1992). Like Aone and Wittenburg, we therefore consider only a finite number of instantiations of these type-"
J07-3004,A00-2031,0,0.0116897,"inguistic variation exhibits a Zipfian distribution, where a very small proportion of the available alternatives accounts for most of the data. This creates a temptation to concentrate on capturing the few high-frequency cases at the top end of the distribution, and to ignore the “long tail” of rare events such as non-local dependencies. Despite the fact that these occur in a large number of sentences, they affect only a small number of words, and have thus a small impact on overall dependency recovery. Although there is now a sizable literature on trace and function-tag insertion algorithms (Blaheta and Charniak 2000; Johnson 2002; Campbell 2004), and integrated parsing with function tags or null elements (Dienes and Dubey 2003a, 2003b; Merlo and Musillo 2005; Gabbard, Kulick, and Marcus 2006), such approaches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output. A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable. Such representations are provided by grammar formalisms that are more expressive than simp"
J07-3004,C04-1180,1,0.711358,"rresponding predicate–argument structure or logical form can therefore be directly obtained from any derivation if the semantic interpretation of each lexical entry is known. In this article and in CCGbank, we approximate such semantic interpretations with dependency graphs that include most semantically relevant non-anaphoric local and long-range dependencies. Although certain decisions taken by the builders of the original Penn Treebank mean that the syntactic derivations that can be obtained from the Penn Treebank are not always semantically correct (as we will discuss), subsequent work by Bos et al. (2004) and Bos (2005) has demonstrated that the output of parsers trained on CCGbank can also be directly translated into logical forms such as Discourse Representation Theory structures (Kamp and Reyle 1993), which can then be used as input to a theorem prover in applications like question answering and textual entailment recognition. Translating the Treebank into this more demanding formalism has revealed certain sources of noise and inconsistency in the original annotation that have had to be corrected in order to permit induction of a linguistically correct grammar. Because of this preprocessing"
J07-3004,H05-1079,0,0.0174005,"ier and Steedman CCGbank ing those that arise under relativization and coordination. Although these dependencies are only an approximation of the full semantic interpretation that can in principle be obtained from a CCG, they may prove useful for tasks such as summarization and question answering (Clark, Steedman, and Curran 2004). Furthermore, Bos et al. (2004) and Bos (2005) have demonstrated that the output of CCGbank parsers can be successfully translated into Kamp and Reyle’s (1993) Discourse Representation Theory structures, to support question answering and the textual entailment task (Bos and Markert 2005). We hope that these results can be ported to other corpora and other similarly expressive grammar formalisms. We also hope that our experiences will be useful in designing guidelines for future treebanks. Although implementational details will differ across formalisms, similar problems and questions to those that arose in our work will be encountered in any attempt to extract expressive grammars from annotated corpora. Because CCGbank preserves most of the linguistic information in the Treebank in a somewhat less noisy form, we hope that others will find it directly helpful for inducing gramm"
J07-3004,P98-1025,0,0.0261249,"eveloped as a “near-contextfree” theory of natural language grammar, with a very free definition of derivational structure adapted to the analysis of coordination and unbounded dependency without movement or deletion transformations. It has been successfully applied to the analysis of coordination, relative clauses and related constructions, intonation structure, binding and control, and quantifier scope alternation, in a number of languages—see Steedman and Baldridge (2006) for a recent review. Extensions of CCG to other languages and word-orders are discussed by Hoffman (1995), Kang (1995), Bozsahin (1998), Komagata (1999), Steedman (2000), Trechsel (2000), Baldridge (2002), and C ¸ akıcı (2005). The derivations in CCGbank follow the analyses of Steedman (1996, 2000), except where noted. 2.1 Lexical Categories Categorial Grammars are strongly lexicalized, in the sense that the grammar is entirely defined by a lexicon in which words (and other lexical items) are associated with one or more specific categories which completely define their syntactic behavior. The set of categories consists of basic categories (e.g., S, NP, PP) and complex categories of the form X/Y or XY, representing functors w"
J07-3004,P04-1041,0,0.00757058,"Missing"
J07-3004,P05-2013,0,0.18086,"on of derivational structure adapted to the analysis of coordination and unbounded dependency without movement or deletion transformations. It has been successfully applied to the analysis of coordination, relative clauses and related constructions, intonation structure, binding and control, and quantifier scope alternation, in a number of languages—see Steedman and Baldridge (2006) for a recent review. Extensions of CCG to other languages and word-orders are discussed by Hoffman (1995), Kang (1995), Bozsahin (1998), Komagata (1999), Steedman (2000), Trechsel (2000), Baldridge (2002), and C ¸ akıcı (2005). The derivations in CCGbank follow the analyses of Steedman (1996, 2000), except where noted. 2.1 Lexical Categories Categorial Grammars are strongly lexicalized, in the sense that the grammar is entirely defined by a lexicon in which words (and other lexical items) are associated with one or more specific categories which completely define their syntactic behavior. The set of categories consists of basic categories (e.g., S, NP, PP) and complex categories of the form X/Y or XY, representing functors with (basic or complex) argument category Y and result category X. Functor categories of the"
J07-3004,P04-1082,0,0.00611989,"stribution, where a very small proportion of the available alternatives accounts for most of the data. This creates a temptation to concentrate on capturing the few high-frequency cases at the top end of the distribution, and to ignore the “long tail” of rare events such as non-local dependencies. Despite the fact that these occur in a large number of sentences, they affect only a small number of words, and have thus a small impact on overall dependency recovery. Although there is now a sizable literature on trace and function-tag insertion algorithms (Blaheta and Charniak 2000; Johnson 2002; Campbell 2004), and integrated parsing with function tags or null elements (Dienes and Dubey 2003a, 2003b; Merlo and Musillo 2005; Gabbard, Kulick, and Marcus 2006), such approaches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output. A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable. Such representations are provided by grammar formalisms that are more expressive than simple phrase-structure grammar, l"
J07-3004,J91-3003,0,0.0255629,"parameters for such lexical rules are difficult to estimate. We therefore follow the approach of Aone and Wittenburg (1990) and implement these type-changing 367 Computational Linguistics Volume 33, Number 3 Figure 3 Type-changing rules reduce the number of lexical category types required for complex adjuncts. operations in the derivational syntax, where these generalizations are captured in a few rules. If these rules apply recursively to their own output, they can generate an infinite set of category types, leading to a shift in generative power from context-free to recursively enumerable (Carpenter 1991, 1992). Like Aone and Wittenburg, we therefore consider only a finite number of instantiations of these type-changing rules, namely those which arise when we extend the category assignment procedure in the following way: For any sentential or verb phrase modifier (an adjunct with label S or SBAR with null complementizer, or VP) to which the original algorithm assigns category X|X, apply the following type-changing rule (given in bottom-up notation) in reverse: S$ ⇒ X|X (16) where S$ is the category that this constituent obtains if it is treated like a head node by the basic algorithm. S$ has"
J07-3004,A00-2018,0,0.155441,"that has become the de facto standard training and test data for statistical parsers. Its annotation, which is based on generic phrasestructure grammar (with coindexed traces and other null elements indicating non-local dependencies) and function tags on nonterminal categories providing (a limited degree of) syntactic role information, is designed to facilitate the extraction of the underlying predicate–argument structure. Statistical parsing on the Penn Treebank has made great progress by focusing on the machine-learning or algorithmic aspects (Magerman 1994; Ratnaparkhi 1998; Collins 1999; Charniak 2000; Henderson 2004; McDonald, Crammer, and Pereira 2005). However, this has often resulted in parsing models and evaluation measures that are both based on reduced representations which simplify or ignore the linguistic information represented by function tags and null elements in the original Treebank. (One exception is Collins 1999, whose Model 2 includes a distinction between arguments and adjuncts, and whose Model 3 additionally captures wh-movement in relative clauses with a GPSG-like “slash-feature-passing” mechanism.) The reasons for this shift away from linguistic adequacy are easy to tr"
J07-3004,P00-1058,0,0.00798912,"; XTAG-group 1999; Copestake and Flickinger 2000; OpenCCG1 [White and Baldridge 2003; White 2006]). Because treebank annotation for individual formalisms is prohibitively expensive, there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs, from the Penn Treebank (Xia 1999; Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000; Xia 2001; Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2004; O’Donovan et al. 2005; Shen and Joshi 2005; Chen, Bangalore, and Vijay-Shanker 2006). Statistical parsers that are trained on these TAG and HPSG corpora have been presented by Chiang (2000) and Miyao and Tsujii (2005), whereas the LFG parsing system of Cahill et al. (2004) uses 1 Open CCG, the successor of Grok (Hockenmaier et al. 2004), is available from http://openccg. sourceforge.net. 356 Hockenmaier and Steedman CCGbank a postprocessing step on the output of a Treebank parser to recover predicate–argument dependencies. In this article we present an algorithmic method for obtaining a corpus of CCG derivations and dependency structures from the Penn Treebank, together with some observations that we believe carry wider implications for similar attempts with other grammar formal"
J07-3004,P04-1014,0,0.0192347,"tprocessing step on the output of a Treebank parser to recover predicate–argument dependencies. In this article we present an algorithmic method for obtaining a corpus of CCG derivations and dependency structures from the Penn Treebank, together with some observations that we believe carry wider implications for similar attempts with other grammar formalisms and corpora. Earlier versions of the resulting corpus, CCGbank, have already been used to build a number of wide-coverage statistical parsers (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002; Hockenmaier 2003b, 2003a; Clark and Curran 2004, 2007), which recover both local and long-range dependencies directly and in a single pass. CCG is a linguistically expressive, but efficiently parseable, lexicalized grammar formalism that was specifically designed to provide a base-generative account of coordinate and relativized constructions like the following: a. pay HealthVest $5 million right away and additional amounts in the future b. the parched Franco years, the everyday poverty and stagnant atmosphere of which he described in brutally direct, vivid prose c. Who is, and who should be, making the criminal law here? (1) CCG directly"
J07-3004,P07-1032,0,0.103266,"Missing"
J07-3004,P02-1042,1,0.905724,"Missing"
J07-3004,W04-3215,1,0.810022,"Missing"
J07-3004,P97-1003,0,0.016668,"Missing"
J07-3004,copestake-flickinger-2000-open,0,0.00491162,"ions are provided by grammar formalisms that are more expressive than simple phrase-structure grammar, like Lexical-Functional Grammar (LFG) (Kaplan and Bresnan 1982), Head-driven Phrase-Structure Grammar (HPSG) (Pollard and Sag 1994), Tree-Adjoining Grammar (TAG) (Joshi and Schabes 1992), Minimalist Program–related Grammars (Stabler 2004), or Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000). However, until very recently, only handwritten grammars, which lack the wide coverage and robustness of Treebank parsers, were available for these formalisms (Butt et al. 1999; XTAG-group 1999; Copestake and Flickinger 2000; OpenCCG1 [White and Baldridge 2003; White 2006]). Because treebank annotation for individual formalisms is prohibitively expensive, there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs, from the Penn Treebank (Xia 1999; Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000; Xia 2001; Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2004; O’Donovan et al. 2005; Shen and Joshi 2005; Chen, Bangalore, and Vijay-Shanker 2006). Statistical parsers that are trained on these TAG and HPSG corpora have been presented by Chiang (2000) and Miyao and Tsujii (2005), where"
J07-3004,W03-1005,0,0.0232639,"Missing"
J07-3004,P03-1055,0,0.0123941,"unts for most of the data. This creates a temptation to concentrate on capturing the few high-frequency cases at the top end of the distribution, and to ignore the “long tail” of rare events such as non-local dependencies. Despite the fact that these occur in a large number of sentences, they affect only a small number of words, and have thus a small impact on overall dependency recovery. Although there is now a sizable literature on trace and function-tag insertion algorithms (Blaheta and Charniak 2000; Johnson 2002; Campbell 2004), and integrated parsing with function tags or null elements (Dienes and Dubey 2003a, 2003b; Merlo and Musillo 2005; Gabbard, Kulick, and Marcus 2006), such approaches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output. A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable. Such representations are provided by grammar formalisms that are more expressive than simple phrase-structure grammar, like Lexical-Functional Grammar (LFG) (Kaplan and Bresnan 1982), Head-driven Phrase-"
J07-3004,P96-1011,0,0.230637,"yxz : them NP : I >T S/(SNP ) : λf .f I (S/NP )/NP : λxλy.give yxI >B 2 > S/NP : λy.give y them I NN : λqλx.give x them I ∧ qx N : λx.give x them I ∧ money x > < We will see further examples of their use later. Such rules induce additional derivational ambiguity, even in canonical sentences like (4). However, our translation 4 Application of the two rules is indicated by underlines distinguished as < and >, respectively. 359 Computational Linguistics Volume 33, Number 3 algorithm yields normal form derivations (Hepple and Morrill 1989; Wittenburg and ¨ Wall 1991; Konig 1994; Eisner 1996), which use composition and type-raising only when syntactically necessary. For coordination, we will use a binarized version of the following ternary rule schema:5 Coordination: X: f conj X: g ⇒& X: λx.fx ∧ gx (7) For further explanation and linguistics and computational motivation for this theory of grammar, the reader is directed to Steedman (1996, 2000). 2.3 Head-Dependency Structure in CCGbank The syntactic derivations in CCGbank are accompanied with bilexical head-dependency structures, which are defined in terms of the lexical heads of functor categories and their arguments. The derivat"
J07-3004,N06-1024,0,0.0162215,"Missing"
J07-3004,P04-1013,0,0.00860915,"e the de facto standard training and test data for statistical parsers. Its annotation, which is based on generic phrasestructure grammar (with coindexed traces and other null elements indicating non-local dependencies) and function tags on nonterminal categories providing (a limited degree of) syntactic role information, is designed to facilitate the extraction of the underlying predicate–argument structure. Statistical parsing on the Penn Treebank has made great progress by focusing on the machine-learning or algorithmic aspects (Magerman 1994; Ratnaparkhi 1998; Collins 1999; Charniak 2000; Henderson 2004; McDonald, Crammer, and Pereira 2005). However, this has often resulted in parsing models and evaluation measures that are both based on reduced representations which simplify or ignore the linguistic information represented by function tags and null elements in the original Treebank. (One exception is Collins 1999, whose Model 2 includes a distinction between arguments and adjuncts, and whose Model 3 additionally captures wh-movement in relative clauses with a GPSG-like “slash-feature-passing” mechanism.) The reasons for this shift away from linguistic adequacy are easy to trace. The very he"
J07-3004,E89-1002,0,0.313491,"λpλqλx.px ∧ qx N give them (6) ((SNP )/NP )/NP NP : λxλyλz.give yxz : them NP : I >T S/(SNP ) : λf .f I (S/NP )/NP : λxλy.give yxI >B 2 > S/NP : λy.give y them I NN : λqλx.give x them I ∧ qx N : λx.give x them I ∧ money x > < We will see further examples of their use later. Such rules induce additional derivational ambiguity, even in canonical sentences like (4). However, our translation 4 Application of the two rules is indicated by underlines distinguished as < and >, respectively. 359 Computational Linguistics Volume 33, Number 3 algorithm yields normal form derivations (Hepple and Morrill 1989; Wittenburg and ¨ Wall 1991; Konig 1994; Eisner 1996), which use composition and type-raising only when syntactically necessary. For coordination, we will use a binarized version of the following ternary rule schema:5 Coordination: X: f conj X: g ⇒& X: λx.fx ∧ gx (7) For further explanation and linguistics and computational motivation for this theory of grammar, the reader is directed to Steedman (1996, 2000). 2.3 Head-Dependency Structure in CCGbank The syntactic derivations in CCGbank are accompanied with bilexical head-dependency structures, which are defined in terms of the lexical heads"
J07-3004,P03-1046,1,0.56553,"and Steedman CCGbank a postprocessing step on the output of a Treebank parser to recover predicate–argument dependencies. In this article we present an algorithmic method for obtaining a corpus of CCG derivations and dependency structures from the Penn Treebank, together with some observations that we believe carry wider implications for similar attempts with other grammar formalisms and corpora. Earlier versions of the resulting corpus, CCGbank, have already been used to build a number of wide-coverage statistical parsers (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002; Hockenmaier 2003b, 2003a; Clark and Curran 2004, 2007), which recover both local and long-range dependencies directly and in a single pass. CCG is a linguistically expressive, but efficiently parseable, lexicalized grammar formalism that was specifically designed to provide a base-generative account of coordinate and relativized constructions like the following: a. pay HealthVest $5 million right away and additional amounts in the future b. the parched Franco years, the everyday poverty and stagnant atmosphere of which he described in brutally direct, vivid prose c. Who is, and who should be, making the crimi"
J07-3004,P06-1064,1,0.606323,"grammars Mel’ˇcuk and Pertsov 1987; Hudson 1984) and parsers (McDonald, Crammer, and Pereira 2005) could be trained and tested with little extra work on the dependencies in CCGbank. Finally, we believe that existing methods for translating the Penn Treebank from scratch into other grammar formalisms will benefit from including preprocessing similar to that described here. As some indication of the relative ease with which these techniques transfer, we offer the observation that the 900K-word German Tiger dependency corpus has recently been translated into CCG using very similar techniques by Hockenmaier (2006), and C ¸ akıcı (2005) has derived a Turkish lexicon from the a similarly preprocessed version of the METU-Sabanc¸ı Turkish dependency treebank (Oflazer et al. 2003). A fundamental assumption behind attempts at the automatic translation of syntactically annotated corpora into different grammatical formalisms such as CCG, TAG, HPSG, or LFG is that the analyses that are captured in the original annotation can be mapped directly (or, at least, without too much additional work) into the desired analyses in the target formalism. This can only hold if all constructions that are treated in a similar"
J07-3004,P02-1043,1,0.79729,"urceforge.net. 356 Hockenmaier and Steedman CCGbank a postprocessing step on the output of a Treebank parser to recover predicate–argument dependencies. In this article we present an algorithmic method for obtaining a corpus of CCG derivations and dependency structures from the Penn Treebank, together with some observations that we believe carry wider implications for similar attempts with other grammar formalisms and corpora. Earlier versions of the resulting corpus, CCGbank, have already been used to build a number of wide-coverage statistical parsers (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002; Hockenmaier 2003b, 2003a; Clark and Curran 2004, 2007), which recover both local and long-range dependencies directly and in a single pass. CCG is a linguistically expressive, but efficiently parseable, lexicalized grammar formalism that was specifically designed to provide a base-generative account of coordinate and relativized constructions like the following: a. pay HealthVest $5 million right away and additional amounts in the future b. the parched Franco years, the everyday poverty and stagnant atmosphere of which he described in brutally direct, vivid prose c. Who is, and who should be"
J07-3004,P02-1018,0,0.00643982,"s a Zipfian distribution, where a very small proportion of the available alternatives accounts for most of the data. This creates a temptation to concentrate on capturing the few high-frequency cases at the top end of the distribution, and to ignore the “long tail” of rare events such as non-local dependencies. Despite the fact that these occur in a large number of sentences, they affect only a small number of words, and have thus a small impact on overall dependency recovery. Although there is now a sizable literature on trace and function-tag insertion algorithms (Blaheta and Charniak 2000; Johnson 2002; Campbell 2004), and integrated parsing with function tags or null elements (Dienes and Dubey 2003a, 2003b; Merlo and Musillo 2005; Gabbard, Kulick, and Marcus 2006), such approaches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output. A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable. Such representations are provided by grammar formalisms that are more expressive than simple phrase-stru"
J07-3004,C92-2066,0,0.247061,"aches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output. A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable. Such representations are provided by grammar formalisms that are more expressive than simple phrase-structure grammar, like Lexical-Functional Grammar (LFG) (Kaplan and Bresnan 1982), Head-driven Phrase-Structure Grammar (HPSG) (Pollard and Sag 1994), Tree-Adjoining Grammar (TAG) (Joshi and Schabes 1992), Minimalist Program–related Grammars (Stabler 2004), or Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000). However, until very recently, only handwritten grammars, which lack the wide coverage and robustness of Treebank parsers, were available for these formalisms (Butt et al. 1999; XTAG-group 1999; Copestake and Flickinger 2000; OpenCCG1 [White and Baldridge 2003; White 2006]). Because treebank annotation for individual formalisms is prohibitively expensive, there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs, from the Penn Treebank (Xia 1999; Chen a"
J07-3004,kinyon-prolo-2002-identifying,0,0.022997,"Missing"
J07-3004,H94-1020,0,0.012887,"fy the correct analysis among the many alternatives that such a wide-coverage grammar will generate even for the simplest sentences. Given our current machine learning techniques, such parsing models typically need to be trained on relatively large treebanks—that is, text corpora hand-labeled with detailed syntactic structures. Because such annotation requires linguistic expertise, and is therefore difficult to produce, we are currently limited to at most a few treebanks per language. One of the largest and earliest such efforts is the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993; Marcus et al. 1994), which contains a one-million word ∗ Institute for Research in Cognitive Science, University of Pennsylvania, 3401 Walnut Street, Suite 400A, Philadelphia, PA 19104-6228, USA. E-mail: juliahr@cis.upenn.edu. ∗∗ School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: steedman@inf.ed.ac.uk. Submission received: 16 July 2005; revised submission received: 24 January 2007; accepted for publication: 21 February 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 3 subcorpus of Wall Street Journal text that has b"
J07-3004,J93-2004,0,0.0463466,"Missing"
J07-3004,P05-1012,0,0.0304631,"Missing"
J07-3004,H05-1078,0,0.00695377,"creates a temptation to concentrate on capturing the few high-frequency cases at the top end of the distribution, and to ignore the “long tail” of rare events such as non-local dependencies. Despite the fact that these occur in a large number of sentences, they affect only a small number of words, and have thus a small impact on overall dependency recovery. Although there is now a sizable literature on trace and function-tag insertion algorithms (Blaheta and Charniak 2000; Johnson 2002; Campbell 2004), and integrated parsing with function tags or null elements (Dienes and Dubey 2003a, 2003b; Merlo and Musillo 2005; Gabbard, Kulick, and Marcus 2006), such approaches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output. A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable. Such representations are provided by grammar formalisms that are more expressive than simple phrase-structure grammar, like Lexical-Functional Grammar (LFG) (Kaplan and Bresnan 1982), Head-driven Phrase-Structure Grammar (HPSG) (Pollar"
J07-3004,P05-1011,0,0.00465607,"Copestake and Flickinger 2000; OpenCCG1 [White and Baldridge 2003; White 2006]). Because treebank annotation for individual formalisms is prohibitively expensive, there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs, from the Penn Treebank (Xia 1999; Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000; Xia 2001; Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2004; O’Donovan et al. 2005; Shen and Joshi 2005; Chen, Bangalore, and Vijay-Shanker 2006). Statistical parsers that are trained on these TAG and HPSG corpora have been presented by Chiang (2000) and Miyao and Tsujii (2005), whereas the LFG parsing system of Cahill et al. (2004) uses 1 Open CCG, the successor of Grok (Hockenmaier et al. 2004), is available from http://openccg. sourceforge.net. 356 Hockenmaier and Steedman CCGbank a postprocessing step on the output of a Treebank parser to recover predicate–argument dependencies. In this article we present an algorithmic method for obtaining a corpus of CCG derivations and dependency structures from the Penn Treebank, together with some observations that we believe carry wider implications for similar attempts with other grammar formalisms and corpora. Earlier ve"
J07-3004,J05-3003,0,0.00595653,"Missing"
J07-3004,J05-1004,0,0.134314,"Missing"
J07-3004,W03-2316,0,0.00769266,"hat are more expressive than simple phrase-structure grammar, like Lexical-Functional Grammar (LFG) (Kaplan and Bresnan 1982), Head-driven Phrase-Structure Grammar (HPSG) (Pollard and Sag 1994), Tree-Adjoining Grammar (TAG) (Joshi and Schabes 1992), Minimalist Program–related Grammars (Stabler 2004), or Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000). However, until very recently, only handwritten grammars, which lack the wide coverage and robustness of Treebank parsers, were available for these formalisms (Butt et al. 1999; XTAG-group 1999; Copestake and Flickinger 2000; OpenCCG1 [White and Baldridge 2003; White 2006]). Because treebank annotation for individual formalisms is prohibitively expensive, there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs, from the Penn Treebank (Xia 1999; Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000; Xia 2001; Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2004; O’Donovan et al. 2005; Shen and Joshi 2005; Chen, Bangalore, and Vijay-Shanker 2006). Statistical parsers that are trained on these TAG and HPSG corpora have been presented by Chiang (2000) and Miyao and Tsujii (2005), whereas the LFG parsing system of Cahill"
J07-3004,W00-1307,0,0.0633456,"Missing"
J07-3004,W97-1010,0,\N,Missing
J07-3004,W96-0213,0,\N,Missing
J07-3004,J03-4003,0,\N,Missing
J07-3004,C98-1025,0,\N,Missing
J08-1008,D07-1090,0,0.0210116,"ounds depressingly like getting better and better at recalling what is already well-known, and understanding what has often been said before. They point out that, in the long run, ﬁnite state methods alone may simply not work. Accuracy in most areas (WER in ASR, BLEU score in SMT, Eval-b for parsers) is at best 143 Computational Linguistics Volume 34, Number 1 linear in the logarithm of the amount of training data. Even optimistic extrapolation of current learning curves suggests truly incredible amounts of data will be needed (Lamel, Gauvain, and Adda 2002; Moore 2003; Knight and Koehn 2004; Brants et al. 2007). Moreover, the more of the local stuff we get right, the more users will come to trust the software, and hence the more noticeable long range dependencies will become, and the more upset people will get if they are deceived by a wrong analysis. What this should tell us is that the Long Tail is not mocked. Long-range dependencies of the kind investigated above are semantically crucial. Ignoring them disrupts all the other dependencies in those examples. (They are also more frequent in genres like questions.) So we need to remember—and above all, teach our students—what our discipline tells us"
J18-4001,J15-2003,0,0.0245338,". g. Verizon will buy Yahoo or Yazoo. (“Yes”) (“Yes”) (“Yes”) (“Yes”) (“No”) (“Maybe”) (“Maybe not”) (17) To arrive at a meaning representation language that is form-independent (and ultimately language-independent), we are using CCG parsers to machine-read the Web for relations between typed named entities. in order to detect consistent patterns of entailment between relations over named entities of the same types, using directional similarity over entity vectors representing relations. We then build an entailment graph (cleaning it up and closing it under relations such as transitivity [cf. Berant et al. 2015]). Cliques of mutually entailing relations in the entailment graph then constitute paraphrases that can be collapsed to a single relation identifier (Lewis and Steedman 2013a). (This can be done across text from multiple languages [Lewis and Steedman 2013b].) We can then replace the original naive semantics for relation expressions with the relevant paraphrase cluster identifiers, and reparse the entire corpus using this now both form-independent and language-independent semantic representation, building an enormous knowledge graph, with the entities as nodes, and the paraphrase identifiers a"
J18-4001,P11-1103,0,0.0568149,"Missing"
J18-4001,P04-1014,0,0.109241,"constituents also show up under coordination, and as intonational phrases. So any grammar with the same coverage as CCG will engender the same degree of nondeterminism in the parser (because it is there in the grammar). In fact, this is just another drop in the ocean of derivational ambiguity that faces all natural language processors, and can be handled by exactly the same statistical models as other varieties. In particular, the head-word dependency models pioneered by Don Hindle, Mats Rooth, Mike Collins, and Eugene Charniak are straightforwardly applicable (Hockenmaier and Steedman 2002b; Clark and Curran 2004). CCG is also particularly well-adapted to parsing with “supertagger” front ends, which can be optimized using embeddings and long short-term memory (LSTM) (Lewis and Steedman 2014; Lewis, Lee, and Zettlemoyer 2016). CCG is now quite widely used in applications, especially those that call for transparency between semantic and syntactic processing, and/or dislocation, such as machine translation (Birch and Osborne 2011; Mehay and Brew 2012), machine reading (Krishnamurthy and Mitchell 2014), incremental parsing (Pareschi and Steedman 1987; Niv 1994; Xu, Clark, and Zhang 2014; Ambati et al. 2015"
J18-4001,P18-1132,0,0.0239148,"ht to be using both in semantic parser induction, and in building knowledge graphs. Because the FreeBase Query language is quite unlike any kind of linguistic logical form, let alone like the universal language of mind, it may well be more practically effective with small and idiosyncratic data sets to induce semantic parsers for them by end-to-end deep neural brute force, rather than by CCG semantic parser induction. But is it really possible that the problem of parsing could be completely solved by recurrent neural network (RNN)/LSTM, perhaps augmented by attention/a stack (He et al., 2017; Kuncoro et al., 2018)? Do semantic-parsing-as-end-to-end-translation learners actually learn syntax, as has been claimed? It seems likely that end-to-end semantic role labeler parsers and neural machine translation will continue to have difficulty with long-range wh-dependencies, because the evidence for their detailed idiosyncrasies is so sparse. For example, both English and French treat embedded subject extraction as a special case, either involving special bare complement verb categories (English) or a special complementizer “qui” (French). a. A woman who I believe (*that) won b. Une femme que je crois qui/*qu"
J18-4001,D10-1119,1,0.900501,"short-term memory (LSTM) (Lewis and Steedman 2014; Lewis, Lee, and Zettlemoyer 2016). CCG is now quite widely used in applications, especially those that call for transparency between semantic and syntactic processing, and/or dislocation, such as machine translation (Birch and Osborne 2011; Mehay and Brew 2012), machine reading (Krishnamurthy and Mitchell 2014), incremental parsing (Pareschi and Steedman 1987; Niv 1994; Xu, Clark, and Zhang 2014; Ambati et al. 2015), human–robot interaction (Chai et al. 2014; Matuszek et al. 2013), and semantic parser induction (Zettlemoyer and Collins 2005; Kwiatkowski et al. 2010; Abend et al. 2017). Some of my own work with the same techniques has returned to their application in musical analysis (Granroth-Wilding and Clark 2014; McLeod and Steedman 2016), and shown that CCG grammars of the same SNCF class and parsing models of the same statistical kind are required there as well: It is only in the details of their compositional semantics that music and language differ very greatly. Rather than reflecting on this substantial body of work in detail, I’d like to conclude by examining two further more speculative questions. The first is an evolutionary question: Why sho"
J18-4001,D11-1140,1,0.878642,"Missing"
J18-4001,N16-1026,0,0.0529852,"Missing"
J18-4001,Q13-1015,1,0.839125,"ependent (and ultimately language-independent), we are using CCG parsers to machine-read the Web for relations between typed named entities. in order to detect consistent patterns of entailment between relations over named entities of the same types, using directional similarity over entity vectors representing relations. We then build an entailment graph (cleaning it up and closing it under relations such as transitivity [cf. Berant et al. 2015]). Cliques of mutually entailing relations in the entailment graph then constitute paraphrases that can be collapsed to a single relation identifier (Lewis and Steedman 2013a). (This can be done across text from multiple languages [Lewis and Steedman 2013b].) We can then replace the original naive semantics for relation expressions with the relevant paraphrase cluster identifiers, and reparse the entire corpus using this now both form-independent and language-independent semantic representation, building an enormous knowledge graph, with the entities as nodes, and the paraphrase identifiers as relations. To answer questions concerning the knowledge in this graph, we parse questions Q into the same form-independent semantics representation, which is now the langua"
J18-4001,D13-1064,1,0.849411,"ependent (and ultimately language-independent), we are using CCG parsers to machine-read the Web for relations between typed named entities. in order to detect consistent patterns of entailment between relations over named entities of the same types, using directional similarity over entity vectors representing relations. We then build an entailment graph (cleaning it up and closing it under relations such as transitivity [cf. Berant et al. 2015]). Cliques of mutually entailing relations in the entailment graph then constitute paraphrases that can be collapsed to a single relation identifier (Lewis and Steedman 2013a). (This can be done across text from multiple languages [Lewis and Steedman 2013b].) We can then replace the original naive semantics for relation expressions with the relevant paraphrase cluster identifiers, and reparse the entire corpus using this now both form-independent and language-independent semantic representation, building an enormous knowledge graph, with the entities as nodes, and the paraphrase identifiers as relations. To answer questions concerning the knowledge in this graph, we parse questions Q into the same form-independent semantics representation, which is now the langua"
J18-4001,D14-1107,1,0.908507,"Missing"
J18-4001,W12-3126,0,0.0191244,"dependency models pioneered by Don Hindle, Mats Rooth, Mike Collins, and Eugene Charniak are straightforwardly applicable (Hockenmaier and Steedman 2002b; Clark and Curran 2004). CCG is also particularly well-adapted to parsing with “supertagger” front ends, which can be optimized using embeddings and long short-term memory (LSTM) (Lewis and Steedman 2014; Lewis, Lee, and Zettlemoyer 2016). CCG is now quite widely used in applications, especially those that call for transparency between semantic and syntactic processing, and/or dislocation, such as machine translation (Birch and Osborne 2011; Mehay and Brew 2012), machine reading (Krishnamurthy and Mitchell 2014), incremental parsing (Pareschi and Steedman 1987; Niv 1994; Xu, Clark, and Zhang 2014; Ambati et al. 2015), human–robot interaction (Chai et al. 2014; Matuszek et al. 2013), and semantic parser induction (Zettlemoyer and Collins 2005; Kwiatkowski et al. 2010; Abend et al. 2017). Some of my own work with the same techniques has returned to their application in musical analysis (Granroth-Wilding and Clark 2014; McLeod and Steedman 2016), and shown that CCG grammars of the same SNCF class and parsing models of the same statistical kind are requi"
J18-4001,J88-2003,1,0.555089,"knowledge and entailment graphs. We shall then be able to discard hand-built knowledge graphs like Freebase in favor of a truly organic semantic net built in the language of mind, obviating the need to learn end-to-end transduction between semantic representations and the language of the knowledge graph. If this project is successful, the language-independent paraphrase cluster identifiers will perform the function of a “hidden” version of the decompositional semantic 624 Steedman The Lost Combinator features in semantic representations like those of Katz and Postal (1964), Jackendoff (1990), Moens and Steedman (1988), White (1994), and Pustejovsky (1998), while the entailment graph will form a similarly hidden version of the “meaning postulates” of Carnap (1952) and Fodor, Fodor, and Garrett (1975). Such semantic representations are essentially distributional, but with the advantage that they can be combined with traditional logical operators such as quantifiers and negation. This proposal for the discovery of hidden semantic primitives underlying natural language semantics stands in contrast to another quite different contemporary approach to distributional semantics that seeks to use dimensionally reduc"
J18-4001,P94-1018,0,0.340641,"(Hockenmaier and Steedman 2002b; Clark and Curran 2004). CCG is also particularly well-adapted to parsing with “supertagger” front ends, which can be optimized using embeddings and long short-term memory (LSTM) (Lewis and Steedman 2014; Lewis, Lee, and Zettlemoyer 2016). CCG is now quite widely used in applications, especially those that call for transparency between semantic and syntactic processing, and/or dislocation, such as machine translation (Birch and Osborne 2011; Mehay and Brew 2012), machine reading (Krishnamurthy and Mitchell 2014), incremental parsing (Pareschi and Steedman 1987; Niv 1994; Xu, Clark, and Zhang 2014; Ambati et al. 2015), human–robot interaction (Chai et al. 2014; Matuszek et al. 2013), and semantic parser induction (Zettlemoyer and Collins 2005; Kwiatkowski et al. 2010; Abend et al. 2017). Some of my own work with the same techniques has returned to their application in musical analysis (Granroth-Wilding and Clark 2014; McLeod and Steedman 2016), and shown that CCG grammars of the same SNCF class and parsing models of the same statistical kind are required there as well: It is only in the details of their compositional semantics that music and language differ v"
J18-4001,P87-1012,1,0.563602,"traightforwardly applicable (Hockenmaier and Steedman 2002b; Clark and Curran 2004). CCG is also particularly well-adapted to parsing with “supertagger” front ends, which can be optimized using embeddings and long short-term memory (LSTM) (Lewis and Steedman 2014; Lewis, Lee, and Zettlemoyer 2016). CCG is now quite widely used in applications, especially those that call for transparency between semantic and syntactic processing, and/or dislocation, such as machine translation (Birch and Osborne 2011; Mehay and Brew 2012), machine reading (Krishnamurthy and Mitchell 2014), incremental parsing (Pareschi and Steedman 1987; Niv 1994; Xu, Clark, and Zhang 2014; Ambati et al. 2015), human–robot interaction (Chai et al. 2014; Matuszek et al. 2013), and semantic parser induction (Zettlemoyer and Collins 2005; Kwiatkowski et al. 2010; Abend et al. 2017). Some of my own work with the same techniques has returned to their application in musical analysis (Granroth-Wilding and Clark 2014; McLeod and Steedman 2016), and shown that CCG grammars of the same SNCF class and parsing models of the same statistical kind are required there as well: It is only in the details of their compositional semantics that music and languag"
J18-4001,Q14-1030,1,0.907395,"Missing"
J18-4001,P87-1015,0,0.691346,"Missing"
J18-4001,H89-1033,0,0.731816,"oods 1970). In particular, unbounded wh-dependencies like the above were handled by: (a) putting a pointer into a * or HOLD register as soon as the “which” was encountered without regard to where it would end up; and (b) retrieving the pointer from HOLD when the verb needing an object “had” was encountered without regard to where it had started out. (It also included an ingenious mechanism for coordination called SYSCONJ, which one finds even now being reinvented on an almost yearly basis—cf. Woods [2010].) A * register was also used for wh-constructions within a systemic grammar framework by Winograd (1972, pages 52–53) in his inspiring conversational program SHRDLU. 615 Computational Linguistics Volume 44, Number 4 However, it was unclear how to generalize the HOLD register to handle the multiple long-range dependencies, including crossing dependencies, that are found in many other languages. In particular, if the HOLD register were assumed to be a stack, then the ATN becomes a two-stack machine (since we are already implicitly using one stack as a PDA to parse the context-free core grammar). On the computational side at least, the reaction to this impass took two distinct forms. Both reaction"
J18-4001,P14-1021,0,0.0742158,"Missing"
J18-4001,J91-3004,0,\N,Missing
J18-4001,J82-1001,0,\N,Missing
J18-4001,J15-2002,0,\N,Missing
J18-4001,P14-1112,0,\N,Missing
J18-4001,N15-1006,1,\N,Missing
J18-4001,P17-1044,0,\N,Missing
J88-2003,J88-2003,1,0.133543,"Missing"
J88-2003,P87-1001,1,0.212275,"Missing"
J88-2003,P87-1003,0,0.0543136,"Missing"
J88-2003,P87-1021,0,0.0791138,"rts the idea that a subsequent main clause identifies this next contingent event, then it will provide the temporal referent for that main clause. If the context does not support this interpretation, then the temporal referent will be unchanged, as in: 45. At five o&apos;clock, my car started and the rain stopped. In its ability to refer to temporal entities that have not been explicitly mentioned, but whose existence has merely been implied by the presence of an entity that has been mentioned, tense appears more like a definite NP (e.g., the music in the following example) than like a pronoun, as Webber 1987 points out. 46. I went to a party last night. The music was wonderful. 4.2 WHEN-CLAUSES The definite nature of tense together with the notion of the nucleus as the knowledge structure that tensed expressions conjure up explain the apparent ambiguity of when-clauses with which this paper began. A whenclause behaves rather like one of those phrases that are used to explicitly change topic, such as and your father in the following example (cf. Isard 1975): 47. And your father, how is he? Computational Linguistics, Volume 14, Number 2, June 1988 Temporal Ontology and Temporal Reference Marc Moens"
N03-1031,A00-2018,0,0.0290607,"s are iteratively re-trained on each other’s output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human"
N03-1031,P96-1042,0,0.012772,"Missing"
N03-1031,N01-1023,1,0.558581,"e. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a subset of their output, chosen by some selection mechanism, is used in order to minimize errors. The choice of selection method significantly affects the quality of the resulting parsers. We investigate a novel approach of selecting training examples for co-training parsers by incorporating the idea of maximizing training uti"
N03-1031,E03-1008,1,0.765705,"Missing"
N03-1031,W00-1306,1,0.314843,"ility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples inc"
N03-1031,P02-1016,0,0.00757357,"rion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a sub"
N03-1031,J93-2004,0,0.0255646,"h, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the"
N03-1031,P00-1016,0,0.0133509,"Missing"
N03-1031,W01-0501,0,0.0173865,"have different goals, their selection methods focus on different criteria: co-training typically favors selecting accurately labeled examples, while sample selection typically favors selecting examples with high training utility, which often are not sentences that the parsers already label accurately. In this work, we investigate selection methods for co-training that explore the trade-off between maximizing training utility and minimizing errors. Empirical studies were conducted to compare selection methods under both co-training and a semi-supervised framework called corrected co-training (Pierce and Cardie, 2001), in which the selected examples are manually checked and corrected before being added to the 1 In the context of training parsers, a labeled example is a sentence with its parse tree. Throughout this paper, we use the term “label” and “parse” interchangeably. training data. For co-training, we show that the benefit of selecting examples with high training utility can offset the additional errors they contain. For corrected co-training, we show that selecting examples with high training utility reduces the number of sentences the human annotator has to check. For both frameworks, we show that"
N03-1031,J03-4003,0,\N,Missing
N03-1031,P02-1046,0,\N,Missing
N15-1006,P13-2107,1,0.847568,"nologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 53–63, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics John likes mangoes NP (SNP)/NP NP from India madly (NPNP)/NP NP (SNP)(SNP) NPNP NP SNP > &lt; > SNP S &lt; &lt; Figure 1: Normal form CCG derivation. of incremental CCG derivations and can train on the dependencies in the existing treebank. Our approach can therefore be adapted to other languages with dependency treebanks, since CCG lexical categories can be easily extracted from dependency treebanks (Cakici, 2005; Ambati et al., 2013). The rest of the paper is arranged as follows. Section 2 gives a brief introduction to related work in the areas of CCG parsing and incremental parsing. In section 3, we describe our incremental shift-reduce parsing algorithm. Details about the experiments, evaluation metrices and analysis of the results are in section 4. We conclude with possible future directions in section 5. 2 Related Work In this section, we first give a brief introduction to various available CCG parsers. Then we describe approaches towards incremental and greedy parsing. 2.1 CCG Parsers There has been a significant amo"
N15-1006,P11-1048,0,0.652721,"we describe our incremental shift-reduce parsing algorithm. Details about the experiments, evaluation metrices and analysis of the results are in section 4. We conclude with possible future directions in section 5. 2 Related Work In this section, we first give a brief introduction to various available CCG parsers. Then we describe approaches towards incremental and greedy parsing. 2.1 CCG Parsers There has been a significant amount of work on developing chart-based parsers for CCG. Both generative (Hockenmaier and Steedman, 2002) and discriminative (Clark et al., 2002; Clark and Curran, 2007; Auli and Lopez, 2011; Lewis and Steedman, 2014) models have been developed. As these parsers employ a bottom-up chart-parsing strategy and use normal-form CCGbank derivations which are rightbranching, they are not incremental in nature. In an SVO (Subject-Verb-Object) language, these parsers first attach the object to the verb and then the subject. Two major works in shift-reduce CCG parsing with accuracies competitive with the widely used Clark and Curran (2007) parser (C&C) are Zhang and Clark (2011) and Xu et al. (2014). Zhang and Clark (2011) used a global linear model trained discriminatively with the averag"
N15-1006,P05-2013,0,0.0427919,"Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 53–63, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics John likes mangoes NP (SNP)/NP NP from India madly (NPNP)/NP NP (SNP)(SNP) NPNP NP SNP > &lt; > SNP S &lt; &lt; Figure 1: Normal form CCG derivation. of incremental CCG derivations and can train on the dependencies in the existing treebank. Our approach can therefore be adapted to other languages with dependency treebanks, since CCG lexical categories can be easily extracted from dependency treebanks (Cakici, 2005; Ambati et al., 2013). The rest of the paper is arranged as follows. Section 2 gives a brief introduction to related work in the areas of CCG parsing and incremental parsing. In section 3, we describe our incremental shift-reduce parsing algorithm. Details about the experiments, evaluation metrices and analysis of the results are in section 4. We conclude with possible future directions in section 5. 2 Related Work In this section, we first give a brief introduction to various available CCG parsers. Then we describe approaches towards incremental and greedy parsing. 2.1 CCG Parsers There has"
N15-1006,C04-1041,0,0.229815,"Missing"
N15-1006,J07-4004,0,0.836905,"achine translation (SMT) and automatic speech recognition (ASR) (Roark, 2001; Wang and Harper, 2003). Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2000) is an efficiently parseable, yet linguistically expressive grammar formalism. In addition to predicate-argument structure, CCG elegantly captures the unbounded dependencies found in grammatical constructions like relativization, coordination etc. Availability of the English CCGbank (Hockenmaier and Steedman, 2007) has enabled the creation of several robust and accurate wide-coverage CCG parsers (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Zhang and Clark, 2011). While the majority of CCG parsers use chart-based approaches (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), there has been some work on developing shift-reduce This paper develops a new incremental shiftreduce algorithm for parsing CCG by building a dependency graph in addition to the CCG derivation as a representation. The dependencies in the graph are extracted from the CCG derivation. A node can have multiple parents, and hence we construct a dependency graph rather than a tree. Two new actions are introduced in the shift-reduce paradigm for “revealing”"
N15-1006,P02-1042,1,0.911926,"sing and incremental parsing. In section 3, we describe our incremental shift-reduce parsing algorithm. Details about the experiments, evaluation metrices and analysis of the results are in section 4. We conclude with possible future directions in section 5. 2 Related Work In this section, we first give a brief introduction to various available CCG parsers. Then we describe approaches towards incremental and greedy parsing. 2.1 CCG Parsers There has been a significant amount of work on developing chart-based parsers for CCG. Both generative (Hockenmaier and Steedman, 2002) and discriminative (Clark et al., 2002; Clark and Curran, 2007; Auli and Lopez, 2011; Lewis and Steedman, 2014) models have been developed. As these parsers employ a bottom-up chart-parsing strategy and use normal-form CCGbank derivations which are rightbranching, they are not incremental in nature. In an SVO (Subject-Verb-Object) language, these parsers first attach the object to the verb and then the subject. Two major works in shift-reduce CCG parsing with accuracies competitive with the widely used Clark and Curran (2007) parser (C&C) are Zhang and Clark (2011) and Xu et al. (2014). Zhang and Clark (2011) used a global linear"
N15-1006,P04-1015,0,0.295872,"ither on CCGbank derivations (Zhang and Clark, 2011) which are non-incremental, or on dependencies (Xu et al., 2014) which could be incremental in simple cases, but do not guarantee incrementality. Hassan et al. (2009) developed a semi-incremental CCG parser by transforming the English CCGbank into left branching derivation trees. The strictly incremental version performed with very low accuracy but a semi-incremental version gave a balance between incrementality and accuracy. There is also some work on incremental parsing using grammar formalisms other than CCG like phrase structure grammar (Collins and Roark, 2004) and tree substitution grammar (Sangati and Keller, 2013). 2.3 Greedy Parsers There has been a significant amount of work on greedy shift-reduce dependency parsing. The Malt parser (Nivre et al., 2007) is one of the earliest parsers based on this paradigm. Goldberg and Nivre (2012) improved learning for greedy parsers by using dynamic oracles rather than a single static transition sequence as the oracle. In all the standard shift-reduce parsers, when two trees combine, only the top node (root) of each tree participates in the action. Sartorio et al. (2013) introduced a technique where in addit"
N15-1006,W02-1001,0,0.336296,"edman, 2014) models have been developed. As these parsers employ a bottom-up chart-parsing strategy and use normal-form CCGbank derivations which are rightbranching, they are not incremental in nature. In an SVO (Subject-Verb-Object) language, these parsers first attach the object to the verb and then the subject. Two major works in shift-reduce CCG parsing with accuracies competitive with the widely used Clark and Curran (2007) parser (C&C) are Zhang and Clark (2011) and Xu et al. (2014). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 2002) and beam search for their shiftreduce CCG parser. Xu et al. (2014) developed a 54 dependency model for shift-reduce CCG parsing using a dynamic oracle technique. Unlike the chart parsers, both these parsers can produce fragmentary analyses when a complete spanning analysis is not found. Both these shift-reduce parsers are more incremental than standard chart based parsers. But, as they employ an arc-standard (Yamada and Matsumoto, 2003) shift-reduce strategy on CCGbank, given an SVO language, these parsers are not guaranteed to attach the subject before the object. 2.2 Incremental Parsers A s"
N15-1006,P96-1011,0,0.41504,"Missing"
N15-1006,C12-1059,0,0.0497734,"glish CCGbank into left branching derivation trees. The strictly incremental version performed with very low accuracy but a semi-incremental version gave a balance between incrementality and accuracy. There is also some work on incremental parsing using grammar formalisms other than CCG like phrase structure grammar (Collins and Roark, 2004) and tree substitution grammar (Sangati and Keller, 2013). 2.3 Greedy Parsers There has been a significant amount of work on greedy shift-reduce dependency parsing. The Malt parser (Nivre et al., 2007) is one of the earliest parsers based on this paradigm. Goldberg and Nivre (2012) improved learning for greedy parsers by using dynamic oracles rather than a single static transition sequence as the oracle. In all the standard shift-reduce parsers, when two trees combine, only the top node (root) of each tree participates in the action. Sartorio et al. (2013) introduced a technique where in addition to the root node, nodes on the right and left periphery respectively are also available for attachment in the parsing process. A non-monotonic parsing strategy was introduced by Honnibal et al. (2013), where an action taken during the parsing process is revised based on future"
N15-1006,R09-1025,0,0.0425191,"Missing"
N15-1006,P02-1043,1,0.884997,"guage modeling for statistical machine translation (SMT) and automatic speech recognition (ASR) (Roark, 2001; Wang and Harper, 2003). Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2000) is an efficiently parseable, yet linguistically expressive grammar formalism. In addition to predicate-argument structure, CCG elegantly captures the unbounded dependencies found in grammatical constructions like relativization, coordination etc. Availability of the English CCGbank (Hockenmaier and Steedman, 2007) has enabled the creation of several robust and accurate wide-coverage CCG parsers (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Zhang and Clark, 2011). While the majority of CCG parsers use chart-based approaches (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), there has been some work on developing shift-reduce This paper develops a new incremental shiftreduce algorithm for parsing CCG by building a dependency graph in addition to the CCG derivation as a representation. The dependencies in the graph are extracted from the CCG derivation. A node can have multiple parents, and hence we construct a dependency graph rather than a tree. Two new actions are introduced in the shift-reduce p"
N15-1006,J07-3004,1,0.944505,"Missing"
N15-1006,W13-3518,1,0.854971,"Nivre et al., 2007) is one of the earliest parsers based on this paradigm. Goldberg and Nivre (2012) improved learning for greedy parsers by using dynamic oracles rather than a single static transition sequence as the oracle. In all the standard shift-reduce parsers, when two trees combine, only the top node (root) of each tree participates in the action. Sartorio et al. (2013) introduced a technique where in addition to the root node, nodes on the right and left periphery respectively are also available for attachment in the parsing process. A non-monotonic parsing strategy was introduced by Honnibal et al. (2013), where an action taken during the parsing process is revised based on future context. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) S S S S S RR RR RR S RR RL [ [ [ [ [ [ [ [ [ [ [ NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn Slikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes SNPlikes SNPlikes SNPlikes NPmangoes NPmangoes NPmangoes NPmangoes NPmangoes (NPNP)/NPf rom (NPNP)/NPf rom NPNPf rom (SNP)(SNP)madly NPIndia John likes (10) (8) mangoes madly (11) (7) from (6) India Figure 2: NonInc - Sequence of actions with"
N15-1006,D10-1119,1,0.856898,"gramming language, then while we usually think of CCG combinatory rules like the following as applying with the two categories on the left X/Y and Y as inputs, say instantiated as S /NP and NP , to define the category X on the right as S, in fact instantiating any two of those categories defines the third. X/Y Y =⇒ X 56 For example, if we define X and X/Y as S and S /NP , we clearly define Y as NP . They proposed to use unification-based revealing to recover unbuilt constituents in from the result of overlygreedy incremental parsing. A related secondorder matching-based mechanism was used by (Kwiatkowski et al., 2010) to decompose logical forms for semantic parser induction. The present incremental parser uses a related revealing technique confined to the right periphery. Using CCG combinators and rules like type-raising followed by forward composition, we combine nodes in the stack if there is a dependency between them. However, this can create problems for the newly shifted node as its dependent might already have been reduced. For instance, if the object ‘mangoes’ is reduced after it is shifted to the stack, then it won’t be available for the preposition phrase (PP) ‘from India’ (of course, this goes fo"
N15-1006,D14-1107,1,0.844119,"ental shift-reduce parsing algorithm. Details about the experiments, evaluation metrices and analysis of the results are in section 4. We conclude with possible future directions in section 5. 2 Related Work In this section, we first give a brief introduction to various available CCG parsers. Then we describe approaches towards incremental and greedy parsing. 2.1 CCG Parsers There has been a significant amount of work on developing chart-based parsers for CCG. Both generative (Hockenmaier and Steedman, 2002) and discriminative (Clark et al., 2002; Clark and Curran, 2007; Auli and Lopez, 2011; Lewis and Steedman, 2014) models have been developed. As these parsers employ a bottom-up chart-parsing strategy and use normal-form CCGbank derivations which are rightbranching, they are not incremental in nature. In an SVO (Subject-Verb-Object) language, these parsers first attach the object to the verb and then the subject. Two major works in shift-reduce CCG parsing with accuracies competitive with the widely used Clark and Curran (2007) parser (C&C) are Zhang and Clark (2011) and Xu et al. (2014). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 200"
N15-1006,P87-1012,1,0.722198,"ce. Figure 2 also shows the dependency graph generated and the arc labels give the step ID after which the dependency is generated. 3.2 Revealing based Incremental Algorithm (RevInc) The NonInc algorithm described above is not incremental because it relies purely on the mostly rightbranching CCG derivation. In our example sentence, the verb (likes) combines with the subject (John) only at the end (step ID = 11) after all the remaining words in the sentence are processed, making the parse non-incremental. In this section we describe a new incremental algorithm based on a ‘revealing’ technique (Pareschi and Steedman, 1987) which tries to build the most incremental derivation. 3.2.1 Revealing Pareschi and Steedman (1987)’s original version of revealing was defined in terms of (implicitly higher-order) unification. It was based on the following observation. If we think of categories as terms in a logic programming language, then while we usually think of CCG combinatory rules like the following as applying with the two categories on the left X/Y and Y as inputs, say instantiated as S /NP and NP , to define the category X on the right as S, in fact instantiating any two of those categories defines the third. X/Y Y"
N15-1006,J01-2004,0,0.585174,"Missing"
N15-1006,Q13-1010,0,0.026186,"ch are non-incremental, or on dependencies (Xu et al., 2014) which could be incremental in simple cases, but do not guarantee incrementality. Hassan et al. (2009) developed a semi-incremental CCG parser by transforming the English CCGbank into left branching derivation trees. The strictly incremental version performed with very low accuracy but a semi-incremental version gave a balance between incrementality and accuracy. There is also some work on incremental parsing using grammar formalisms other than CCG like phrase structure grammar (Collins and Roark, 2004) and tree substitution grammar (Sangati and Keller, 2013). 2.3 Greedy Parsers There has been a significant amount of work on greedy shift-reduce dependency parsing. The Malt parser (Nivre et al., 2007) is one of the earliest parsers based on this paradigm. Goldberg and Nivre (2012) improved learning for greedy parsers by using dynamic oracles rather than a single static transition sequence as the oracle. In all the standard shift-reduce parsers, when two trees combine, only the top node (root) of each tree participates in the action. Sartorio et al. (2013) introduced a technique where in addition to the root node, nodes on the right and left periphe"
N15-1006,P13-1014,0,0.0707343,"CCG like phrase structure grammar (Collins and Roark, 2004) and tree substitution grammar (Sangati and Keller, 2013). 2.3 Greedy Parsers There has been a significant amount of work on greedy shift-reduce dependency parsing. The Malt parser (Nivre et al., 2007) is one of the earliest parsers based on this paradigm. Goldberg and Nivre (2012) improved learning for greedy parsers by using dynamic oracles rather than a single static transition sequence as the oracle. In all the standard shift-reduce parsers, when two trees combine, only the top node (root) of each tree participates in the action. Sartorio et al. (2013) introduced a technique where in addition to the root node, nodes on the right and left periphery respectively are also available for attachment in the parsing process. A non-monotonic parsing strategy was introduced by Honnibal et al. (2013), where an action taken during the parsing process is revised based on future context. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) S S S S S RR RR RR S RR RL [ [ [ [ [ [ [ [ [ [ [ NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn Slikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes SNPlikes"
N15-1006,P14-1021,0,0.384084,"ier and Steedman, 2002) and discriminative (Clark et al., 2002; Clark and Curran, 2007; Auli and Lopez, 2011; Lewis and Steedman, 2014) models have been developed. As these parsers employ a bottom-up chart-parsing strategy and use normal-form CCGbank derivations which are rightbranching, they are not incremental in nature. In an SVO (Subject-Verb-Object) language, these parsers first attach the object to the verb and then the subject. Two major works in shift-reduce CCG parsing with accuracies competitive with the widely used Clark and Curran (2007) parser (C&C) are Zhang and Clark (2011) and Xu et al. (2014). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 2002) and beam search for their shiftreduce CCG parser. Xu et al. (2014) developed a 54 dependency model for shift-reduce CCG parsing using a dynamic oracle technique. Unlike the chart parsers, both these parsers can produce fragmentary analyses when a complete spanning analysis is not found. Both these shift-reduce parsers are more incremental than standard chart based parsers. But, as they employ an arc-standard (Yamada and Matsumoto, 2003) shift-reduce strategy on CCGbank, giv"
N15-1006,W03-3023,0,0.0828955,"007) parser (C&C) are Zhang and Clark (2011) and Xu et al. (2014). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 2002) and beam search for their shiftreduce CCG parser. Xu et al. (2014) developed a 54 dependency model for shift-reduce CCG parsing using a dynamic oracle technique. Unlike the chart parsers, both these parsers can produce fragmentary analyses when a complete spanning analysis is not found. Both these shift-reduce parsers are more incremental than standard chart based parsers. But, as they employ an arc-standard (Yamada and Matsumoto, 2003) shift-reduce strategy on CCGbank, given an SVO language, these parsers are not guaranteed to attach the subject before the object. 2.2 Incremental Parsers A strictly incremental parser is one which computes the relationship between words as soon as they are encountered in the input. Shift-reduce CCG parsers rely either on CCGbank derivations (Zhang and Clark, 2011) which are non-incremental, or on dependencies (Xu et al., 2014) which could be incremental in simple cases, but do not guarantee incrementality. Hassan et al. (2009) developed a semi-incremental CCG parser by transforming the Engli"
N15-1006,P11-1069,0,0.488319,"and automatic speech recognition (ASR) (Roark, 2001; Wang and Harper, 2003). Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2000) is an efficiently parseable, yet linguistically expressive grammar formalism. In addition to predicate-argument structure, CCG elegantly captures the unbounded dependencies found in grammatical constructions like relativization, coordination etc. Availability of the English CCGbank (Hockenmaier and Steedman, 2007) has enabled the creation of several robust and accurate wide-coverage CCG parsers (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Zhang and Clark, 2011). While the majority of CCG parsers use chart-based approaches (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), there has been some work on developing shift-reduce This paper develops a new incremental shiftreduce algorithm for parsing CCG by building a dependency graph in addition to the CCG derivation as a representation. The dependencies in the graph are extracted from the CCG derivation. A node can have multiple parents, and hence we construct a dependency graph rather than a tree. Two new actions are introduced in the shift-reduce paradigm for “revealing” (Pareschi and Steedman,"
N15-1006,P11-2033,0,0.0382044,"[ [ [ [ [ [ [ [ [ [ NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn Slikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes SNPlikes SNPlikes SNPlikes NPmangoes NPmangoes NPmangoes NPmangoes NPmangoes (NPNP)/NPf rom (NPNP)/NPf rom NPNPf rom (SNP)(SNP)madly NPIndia John likes (10) (8) mangoes madly (11) (7) from (6) India Figure 2: NonInc - Sequence of actions with parser configuration and the corresponding dependency graph. Though the performance of these greedy parsers is less accurate than related parsers using a beam (Zhang and Nivre, 2011), greedy parsers are interesting as they are very fast and are practically useful in large-scale applications such as parsing the web and online machine translation or speech recognition. In this work, we develop a new greedy transition-based algorithm for incremental CCG parsing, which is more incremental than Zhang and Clark (2011) and Xu et al. (2014) and more accurate than Hassan et al. (2009). Our algorithm is not strictly incremental as we only produce derivations which are compatible with the Strict Competence Hypothesis (Steedman, 2000) (details in §3.2.3). 3 Algorithms We first descri"
N15-1122,D13-1178,0,0.0871705,"Missing"
N15-1122,J08-1001,0,0.0391789,"ss the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predicting the likely order of event types is a step towards more intricate planning and reasoning scenarios (see §3), and is useful in itself for tasks such as conceptto-text generation (Reiter et al., 2000), or in validating the correctness of instruction sets. A related idea can be found in modeling sentence coherence (Lapata, 2003; Barzilay and Lapata, 2008, inter alia), although here we focus on lexical relations between events, rather than coherence relations between complete sentences. Compiling a resource of temporal tendencies between events can hardly be done manually, given the number and wealth of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the lates"
N15-1122,J92-4003,0,0.0957719,"rstorder Markovian and use the same (greedy) inference procedure. Lapata’s model differs from G REEDY-L OG L IN in being a generative model, where each event is a tuple of features, and the transition probability between events is defined as the product of transition probabilities between feature pairs. G REEDY-L OG L IN is discriminative, so to be maximally comparable to the presented model. 5 The Feature Set Table 1 presents the complete set of features. We consider three sets of features: Lexical encodes the written forms of the event pair predicates and objects; Brown uses Brown clusters (Brown et al., 1992) to encode similar information, but allows generalization between distributionally similar words; and Frequency encodes the empirical distribution of temporally-related phenomena. The feature definitions make use of several functions. For brevity, we sometimes say that an event e is (a, c1 ) if e’s predicate is a and its first argument is c1 , disregarding its other arguments. Let C be a reference corpus of recipes for collecting statistics. The function B(w) gives the Brown cluster of a word w, as determined by clustering C into 50 clusters {1, . . . , 50}. The function ORD(a, c) returns the"
N15-1122,P14-2082,0,0.0379789,"Missing"
N15-1122,D08-1073,0,0.203506,"h of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013). Previous work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012). However, despite their appeal, these methods have mostly fo1161 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics cused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions, and are hard to extend to include rich features (e.g., order-based and pattern-based features). Furthermore, measuring recall without annotated data is notoriously difficult, and evaluation is often"
N15-1122,P08-1090,0,0.373477,"h of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013). Previous work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012). However, despite their appeal, these methods have mostly fo1161 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics cused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions, and are hard to extend to include rich features (e.g., order-based and pattern-based features). Furthermore, measuring recall without annotated data is notoriously difficult, and evaluation is often"
N15-1122,P09-1068,0,0.176291,"Missing"
N15-1122,W04-3205,0,0.0471795,", given the number and wealth of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013). Previous work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012). However, despite their appeal, these methods have mostly fo1161 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics cused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions, and are hard to extend to include rich features (e.g., order-based and pattern-based features). Furthermore, measuring recall without annotated data is notoriously diffic"
N15-1122,W13-2102,0,0.0613741,"Missing"
N15-1122,W02-1001,0,0.0254305,"Missing"
N15-1122,P07-1030,0,0.0179481,"Missing"
N15-1122,W08-1301,0,0.0143165,"Missing"
N15-1122,N13-1112,0,0.0503299,"Missing"
N15-1122,E14-1006,0,0.211782,"Missing"
N15-1122,E12-1034,0,0.229035,"Missing"
N15-1122,P06-1095,0,0.280094,"s in a domain where this order is aligned with their temporal order, namely cooking recipes. 1 Introduction Temporal relations between events are often implicit, and inferring them relies on lexical and world knowledge about the likely order of events. For instance, to execute the instruction “fry the onion,” the hearer should probably obtain oil beforehand, even if not instructed so explicitly. Lexical knowledge about the likely order of events is therefore necessary for any semantic task that requires temporal reasoning or planning, such as classifying temporal relations (Mani et al., 2006; Lapata and Lascarides, 2006; Yoshikawa et al., 2009; D’Souza and Ng, 2013; Mirza and Tonelli, 2014, inter alia), textual entailment (Dagan et al., 2013) or temporal information extraction (Ling and Weld, 2010). Lexical temporal knowledge is further important for modeling grammatical phenomena such as tense and aspect (Steedman, 2002). In this paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predic"
N15-1122,P03-1069,0,0.236224,"paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predicting the likely order of event types is a step towards more intricate planning and reasoning scenarios (see §3), and is useful in itself for tasks such as conceptto-text generation (Reiter et al., 2000), or in validating the correctness of instruction sets. A related idea can be found in modeling sentence coherence (Lapata, 2003; Barzilay and Lapata, 2008, inter alia), although here we focus on lexical relations between events, rather than coherence relations between complete sentences. Compiling a resource of temporal tendencies between events can hardly be done manually, given the number and wealth of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annot"
N15-1122,J06-4002,0,0.0246016,".edu/software/corenlp.shtml Links to the original recipes, the preprocessed recipes and all extracted events can be found in http://homepages. inf.ed.ac.uk/oabend/event_order.html. 1166 This did not result from an extraction problem, but rather from the recipe text itself being too noisy to interpret. 7 Events are parsed manually so to avoid confounding the results with the parser’s performance. of recipes does not suggest that the textual order is the only order of events that would yield the same outcome. We compute the Kendall’s Tau correlation, a standard measure for information ordering (Lapata, 2006), between the temporal and linear orderings for each recipe. In cases of several events that happen simultaneously (including disjunctions), we take their ordinals to be equal. For instance, for three events where the last two happen at the same time, we take their ordering to be (1,2,2) in our analysis. We find that indeed temporal and textual orderings are in very high agreement, with 6 recipes of the 19 perfectly aligned. The average Kendall’s Tau between the temporal ordering and the linear one is 0.924. 7 Experimental Setup Evaluation. We compute the accuracy of our algorithms by comparin"
N15-1122,W14-2407,0,0.0803724,"Missing"
N15-1122,P05-1012,0,0.0307215,"Missing"
N15-1122,E14-1033,0,0.0122346,"cooking recipes. 1 Introduction Temporal relations between events are often implicit, and inferring them relies on lexical and world knowledge about the likely order of events. For instance, to execute the instruction “fry the onion,” the hearer should probably obtain oil beforehand, even if not instructed so explicitly. Lexical knowledge about the likely order of events is therefore necessary for any semantic task that requires temporal reasoning or planning, such as classifying temporal relations (Mani et al., 2006; Lapata and Lascarides, 2006; Yoshikawa et al., 2009; D’Souza and Ng, 2013; Mirza and Tonelli, 2014, inter alia), textual entailment (Dagan et al., 2013) or temporal information extraction (Ling and Weld, 2010). Lexical temporal knowledge is further important for modeling grammatical phenomena such as tense and aspect (Steedman, 2002). In this paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predicting the likely order of event types is a step towards more intricate p"
N15-1122,W14-1606,0,0.176512,"Missing"
N15-1122,E14-1024,0,0.163803,"Missing"
N15-1122,P09-2004,0,0.0384851,"Missing"
N15-1122,P10-1100,0,0.312406,"Missing"
N15-1122,Q13-1003,0,0.0758771,"Missing"
N15-1122,D13-1177,0,0.172065,"Missing"
N15-1122,D09-1105,0,0.0757134,"Missing"
N15-1122,S13-2001,0,0.0440954,"ons between events, rather than coherence relations between complete sentences. Compiling a resource of temporal tendencies between events can hardly be done manually, given the number and wealth of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013). Previous work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012). However, despite their appeal, these methods have mostly fo1161 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics cused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions,"
N15-1122,P09-1046,0,0.0272091,"r is aligned with their temporal order, namely cooking recipes. 1 Introduction Temporal relations between events are often implicit, and inferring them relies on lexical and world knowledge about the likely order of events. For instance, to execute the instruction “fry the onion,” the hearer should probably obtain oil beforehand, even if not instructed so explicitly. Lexical knowledge about the likely order of events is therefore necessary for any semantic task that requires temporal reasoning or planning, such as classifying temporal relations (Mani et al., 2006; Lapata and Lascarides, 2006; Yoshikawa et al., 2009; D’Souza and Ng, 2013; Mirza and Tonelli, 2014, inter alia), textual entailment (Dagan et al., 2013) or temporal information extraction (Ling and Weld, 2010). Lexical temporal knowledge is further important for modeling grammatical phenomena such as tense and aspect (Steedman, 2002). In this paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predicting the likely order of"
N16-1052,D15-1159,0,0.0376278,"Missing"
N16-1052,N15-1006,1,0.844612,"rs Due to the availability of English CCGbank (Hockenmaier and Steedman, 2007), several widecoverage CCG parsers have been developed (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Auli and Lopez, 2011; Zhang and Clark, 447 Proceedings of NAACL-HLT 2016, pages 447–453, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2011; Lewis and Steedman, 2014a). While the majority of CCG parsers are chart-based (Clark and Curran, 2007; Lewis and Steedman, 2014a), there has been some work on shift-reduce CCG parsing (Zhang and Clark, 2011; Xu et al., 2014; Ambati et al., 2015). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 2002) and beam search for their shift-reduce CCG parser. A dependency model for shift-reduce CCG parsing using a dynamic oracle technique (Goldberg and Nivre, 2012) was developed by Xu et al. (2014). Ambati et al. (2015) presented an incremental algorithm for transition based CCG parsing which introduced two novel revealing actions to overcome the consequences of the greedy nature of the previous parsers. 2.2 Neural Network Parsers Neural Network parsers are attracting interest f"
N16-1052,P11-1048,0,0.127314,"Missing"
N16-1052,D14-1082,0,0.837765,"the model settings in this section. We also discuss our structured neural network model. 3.1 Architecture Figure 1 shows the architecture of our neural network parser. There are three layers in the parser: input, hidden and output layers. We first extract discrete features like words, POS-tags and CCG su448 Figure 1: Our Neural Network Architecture (adapted from Chen and Manning (2014)). pertags from the parser configuration. For each of these discrete features we obtain a continuous vector representation in the form of their corresponding embeddings and use them in the input layer. Following Chen and Manning (2014), we use a cube activation function and softmax for output layer. 3.2 Feature and Model Settings We extract features from a) top four nodes in the stack, b) next four nodes in the input and c) left and right children of the top two nodes in the stack. We obtain words and POS-tags of all these 12 nodes. In case of CCG supertags, in addition to the CCG categories of the nodes in the stack (top four nodes, left and right children of top two nodes), we also obtain the lexical head categories for the top two nodes. We use a special token ‘NULL’ if a feature is not present in the parser configuratio"
N16-1052,C04-1041,0,0.127462,"es of 64 feature templates is used. For NNPar, the 34 feature templates described in section 3.2 are used. We employ an arc-standard style shift-reduce algorithm for CCG parsing, similar to Zhang and Clark (2011), for all our experiments. 4.1 Data and Settings We use the standard CCGbank training (sections 02 − 21), development (section 00) and testing (section 23) splits for our experiments. All the experiments are performed using automatic POS-tags and CCG supertags. We compare performance using two types of taggers: maximum entropy and neural network based taggers (NNT). The C&C taggers 3 (Clark and Curran, 2004) are used for maximum entropy taggers. For neural network taggers, SENNA tagger4 (version 3.0) (Collobert et al., 2011) is used 2 We used Chen and Manning (2014)’s classifier for implementing our NNPar 3 http://svn.ask.it.usyd.edu.au/trac/ candc/wiki 4 http://ronan.collobert.com/senna/ for POS-tagging and EasyCCG tagger5 (Lewis and Steedman, 2014a) is used for supertagging. Both these taggers use a feed-forward neural network architecture with a single hidden layer similar to our NNPar architecture. In the case of POS-tags, we consider the first best tag given by the POS tagger. For CCG supert"
N16-1052,P04-1015,0,0.145264,"ause there are many more CCG categories (∼ 500) compared to dependency labels, there are relatively more operations in a CCG parser. 4 Experiments and Results We first compare our neural network parser (NNPar)2 with a perceptron based parser in the greedy settings. Then we analyze the impact of beam using neural network (NNPar) and structured neural network (Structured NNPar) models. The perceptron based parser is a reimplementation of Zhang and Clark (2011)’s parser (Z&C*). A global linear model trained with the averaged perceptron (Collins, 2002) is used for this parser and an early-update (Collins and Roark, 2004) strategy is used during training. In the greedy setting (beam=1), when the predicted action differs from the gold action, decoding stops and weights are updated accordingly. When a beam is used (beam=16), weights are updated when the gold parse configuration falls out of the beam. For Z&C*, the feature set of Zhang and Clark (2011), which comprises of 64 feature templates is used. For NNPar, the 34 feature templates described in section 3.2 are used. We employ an arc-standard style shift-reduce algorithm for CCG parsing, similar to Zhang and Clark (2011), for all our experiments. 4.1 Data and"
N16-1052,W02-1001,0,0.633253,"loped (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Auli and Lopez, 2011; Zhang and Clark, 447 Proceedings of NAACL-HLT 2016, pages 447–453, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2011; Lewis and Steedman, 2014a). While the majority of CCG parsers are chart-based (Clark and Curran, 2007; Lewis and Steedman, 2014a), there has been some work on shift-reduce CCG parsing (Zhang and Clark, 2011; Xu et al., 2014; Ambati et al., 2015). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 2002) and beam search for their shift-reduce CCG parser. A dependency model for shift-reduce CCG parsing using a dynamic oracle technique (Goldberg and Nivre, 2012) was developed by Xu et al. (2014). Ambati et al. (2015) presented an incremental algorithm for transition based CCG parsing which introduced two novel revealing actions to overcome the consequences of the greedy nature of the previous parsers. 2.2 Neural Network Parsers Neural Network parsers are attracting interest for both speed and accuracy. There has been some work on neural networks for constituent based parsing (Collobert, 2011; S"
N16-1052,P15-1033,0,0.0957162,"Missing"
N16-1052,C12-1059,0,0.0456691,"53, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2011; Lewis and Steedman, 2014a). While the majority of CCG parsers are chart-based (Clark and Curran, 2007; Lewis and Steedman, 2014a), there has been some work on shift-reduce CCG parsing (Zhang and Clark, 2011; Xu et al., 2014; Ambati et al., 2015). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 2002) and beam search for their shift-reduce CCG parser. A dependency model for shift-reduce CCG parsing using a dynamic oracle technique (Goldberg and Nivre, 2012) was developed by Xu et al. (2014). Ambati et al. (2015) presented an incremental algorithm for transition based CCG parsing which introduced two novel revealing actions to overcome the consequences of the greedy nature of the previous parsers. 2.2 Neural Network Parsers Neural Network parsers are attracting interest for both speed and accuracy. There has been some work on neural networks for constituent based parsing (Collobert, 2011; Socher et al., 2013; Watanabe and Sumita, 2015). Chen and Manning (2014) developed a neural network architecture for dependency parsing. This parser was fast an"
N16-1052,P02-1043,1,0.728127,"Missing"
N16-1052,J07-3004,1,0.732134,"Missing"
N16-1052,D14-1107,1,0.956676,"F-scores respectively. To the best of our knowledge, ours is the first neural network based parser for CCG and also the first work on exploring neural network taggers for shift-reduce CCG parsing. 2 2.1 Related Work CCG Parsers Due to the availability of English CCGbank (Hockenmaier and Steedman, 2007), several widecoverage CCG parsers have been developed (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Auli and Lopez, 2011; Zhang and Clark, 447 Proceedings of NAACL-HLT 2016, pages 447–453, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2011; Lewis and Steedman, 2014a). While the majority of CCG parsers are chart-based (Clark and Curran, 2007; Lewis and Steedman, 2014a), there has been some work on shift-reduce CCG parsing (Zhang and Clark, 2011; Xu et al., 2014; Ambati et al., 2015). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 2002) and beam search for their shift-reduce CCG parser. A dependency model for shift-reduce CCG parsing using a dynamic oracle technique (Goldberg and Nivre, 2012) was developed by Xu et al. (2014). Ambati et al. (2015) presented an incremental algorithm for tra"
N16-1052,P13-1045,0,0.181847,"Missing"
N16-1052,P10-1040,0,0.396737,"OS-tag and 10 CCG supertag features. For each of these 34 features we obtain their corresponding embeddings. We use Turian embeddings of dimensionality 50 (Turian-50)1 . For the words which are not in the word embeddings dictionary, embeddings of ‘-UNKNOWN-’ token are used as a backoff. For POS-tags and CCG supertags, the parameters are randomly initialized with values between -0.01 and 0.01. Our input layer is a 34 (feature templates) X 50 (embedding size) dimensional vector. We use 200 1 Lewis and Steedman (2014b) explored different publicly available word embeddings (Mnih and Hinton, 2009; Turian et al., 2010; Collobert et al., 2011; Mikolov et al., 2013) for CCG supertagging and showed that Turian-50 gave best results. hidden units in the the hidden layer. For the output layer we compute softmax probabilities only for the actions which are possible in a particular parser configuration instead of all the actions. We use the training settings of Chen and Manning (2014) for our parser. The training objective is to minimize the cross-entropy loss with an l2 -regularization and the training error derivatives are backpropagated during training. For optimization we use AdaGrad (Duchi et al., 2011). 10−8"
N16-1052,P15-1113,0,0.0915838,"Missing"
N16-1052,P15-1032,0,0.155794,"based counterpart. 1 Introduction Shift-reduce parsing is interesting for practical realworld applications like parsing the web, since parsing can be achieved in linear time. Although greedy parsers are fast, accuracies of these parsers are typically much lower than graph-based parsers. Conversely, beam-search parsers achieve accuracies comparable to graph-based parsers (Zhang and Nivre, 2011) but are much slower than their greedy counterparts. Recently, Chen and Manning (2014) have showed that fast and accurate parsing can be achieved using neural network based parsers. Improving their work, Weiss et al. (2015) presented a structured neural network model which gave stateof-the-art results for English dependency parsing. There has been increasing interest in Combinatory Categorial Grammar (CCG) (Steedman, 2000) parsing due to the simplicity of its interface between syntax and semantics. In addition to predicateargument structure, CCG captures the unbounded dependencies found in grammatical constructions like relativization, coordination, etc. We present a neural network based shift-reduce CCG parser, the first neural network based parser for CCG. We first adapt Chen and Manning (2014)’s shift-reduce"
N16-1052,P14-1021,0,0.214407,"ed Work CCG Parsers Due to the availability of English CCGbank (Hockenmaier and Steedman, 2007), several widecoverage CCG parsers have been developed (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Auli and Lopez, 2011; Zhang and Clark, 447 Proceedings of NAACL-HLT 2016, pages 447–453, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2011; Lewis and Steedman, 2014a). While the majority of CCG parsers are chart-based (Clark and Curran, 2007; Lewis and Steedman, 2014a), there has been some work on shift-reduce CCG parsing (Zhang and Clark, 2011; Xu et al., 2014; Ambati et al., 2015). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 2002) and beam search for their shift-reduce CCG parser. A dependency model for shift-reduce CCG parsing using a dynamic oracle technique (Goldberg and Nivre, 2012) was developed by Xu et al. (2014). Ambati et al. (2015) presented an incremental algorithm for transition based CCG parsing which introduced two novel revealing actions to overcome the consequences of the greedy nature of the previous parsers. 2.2 Neural Network Parsers Neural Network parsers are"
N16-1052,P15-2041,0,0.291634,"Missing"
N16-1052,P11-1069,0,0.797945,"CG parsing. 2 2.1 Related Work CCG Parsers Due to the availability of English CCGbank (Hockenmaier and Steedman, 2007), several widecoverage CCG parsers have been developed (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Auli and Lopez, 2011; Zhang and Clark, 447 Proceedings of NAACL-HLT 2016, pages 447–453, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2011; Lewis and Steedman, 2014a). While the majority of CCG parsers are chart-based (Clark and Curran, 2007; Lewis and Steedman, 2014a), there has been some work on shift-reduce CCG parsing (Zhang and Clark, 2011; Xu et al., 2014; Ambati et al., 2015). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 2002) and beam search for their shift-reduce CCG parser. A dependency model for shift-reduce CCG parsing using a dynamic oracle technique (Goldberg and Nivre, 2012) was developed by Xu et al. (2014). Ambati et al. (2015) presented an incremental algorithm for transition based CCG parsing which introduced two novel revealing actions to overcome the consequences of the greedy nature of the previous parsers. 2.2 Neural Network Parsers Neural Ne"
N16-1052,P11-2033,0,0.0913244,"Missing"
N16-1052,P15-1117,0,0.0545237,"Missing"
N16-1052,J07-4004,0,\N,Missing
N16-1052,Q14-1026,1,\N,Missing
N16-1052,N16-1026,0,\N,Missing
N16-1120,N15-1006,1,0.900649,"Missing"
N16-1120,P11-2117,0,0.158616,"Missing"
N16-1120,C12-1065,0,0.0447967,"Missing"
N16-1120,N15-1022,0,0.31539,"Missing"
N16-1120,P03-1054,0,0.0326684,"Missing"
N16-1120,Q14-1026,1,0.87849,"Missing"
N16-1120,J01-2004,0,0.281102,"Missing"
N16-1120,P11-1063,0,0.0692792,"Missing"
N16-1120,E14-1076,0,0.0634645,"Missing"
N16-1120,E14-1031,0,0.267204,"Missing"
N16-1120,P12-1107,0,0.142533,"Missing"
N16-1120,P11-1069,0,0.0594387,"Missing"
N16-1120,N01-1021,0,\N,Missing
N16-1120,W04-0304,0,\N,Missing
N16-1120,C10-1152,0,\N,Missing
N19-1020,P96-1011,0,0.498469,"between parser states are: • shift(X) – moves the first word from the buffer to the stack and labels it with category X, • reduceUnary(C) – applies a unary combinatory rule C to the topmost constituent on the stack, • reduceBinary(C) – applies a binary combinatory rule C to the two topmost constituents on 1 This notation differs unimportantly from Steedman (2000) who uses a ternary coordination rule, and more recent work in which conjunctions are XX/X. 230 3.1 >B(x + y − 1) >Bx the stack. CCG shift-reduce parsers are often built over right-branching derivations that obey Eisner normal form (Eisner, 1996). Processing leftbranching derivations is not any different except that it requires an opposite normal form. Our revealing algorithm adds a couple of modifications to this default shift-reduce algorithm. First, it guarantees that all the trees stored on the stack are right-branching – this still allows leftbranching parsing and only adds the requirement of adjusting newly reduced trees on the stack to be right leaning. Second, it adds revealing transitions that exploit the right-branching guarantee to apply right adjunction. Both tree rotation and revealing are performed efficiently as describ"
N19-1020,N15-1006,1,0.346116,"ich are present in the majority of English sentences, as well as from optional constituents that adjoin on the right, such as right adjuncts and right conjuncts. In CCG, many right-branching derivations can be replaced by semantically equivalent leftbranching incremental derivations. The problem of right-adjunction is more resistant to solution, and has been tackled in the past using revealing-based approaches that often rely either on the higher-order unification over lambda terms (Pareschi and Steedman, 1987) or heuristics over dependency representations that do not cover the whole CCGbank (Ambati et al., 2015). We propose a new incremental parsing algorithm for CCG following the same revealing tradition of work but having a purely syntactic approach that does not depend on access to a distinct level of semantic representation. This algorithm can cover the whole CCGbank, with greater incrementality and accuracy than previous proposals. 1 Introduction Combinatory Categorial Grammar (CCG) (Ades and Steedman, 1982; Steedman, 2000) is a mildly context sensitive grammar formalism that is attractive both from a cognitive and an engineering perspective. Compared to other grammar formalisms, the aspect in w"
N19-1020,J07-4004,0,0.481663,"e recognise that right adjunction needs to take place because we have a category of shape XX (concretely (SNP)(SNP) but in the present CCG notation slashes “associate to the left”, so we drop the first pair of brackets). Thanks to the type of the adjunct we know that the 229 the right conjunct, with the result later combining with the left conjunct 1 : conj X ⇒ X[conj] (>Φ) X X[conj] ⇒ X (<Φ) Some additional unary and binary type-changing rules are also needed to process the derivations in CCGbank (Hockenmaier and Steedman, 2007). We use the same type-changing rules as those described in (Clark and Curran, 2007). Among the unary combinatory rules the most important one is type-raising. The first reason for that is that it allows CCG to handle constructions like argument cluster coordination in a straightforward way. Second, it allows CCG to be much more incremental as seen from the example in Figure 1b. Type-raising rules are expressed in the following way: X ⇒ Y /(Y X) (>T) X ⇒ Y (Y /X) (<T) Type-raising, is strictly limited to applying to category types that are arguments, such as NP, PP, etc., making it analogous to grammatical case in languages like Latin and Japanese, in spite of the lack of m"
N19-1020,C10-1053,0,0.0235106,"gewick, 1978; Okasaki, 1999). The main difference is that here we are trying to do the opposite of AVLs: instead of making the tree perfectly balanced we are trying to make it perfectly unbalanced, i.e. leaning to the right (or left). Also, our imperfections start at the top and are pushed to the bottom of the tree which is in contrast to AVLs trees where imperfections start at the bottom and get pushed to the top. The last important point about tree rotation concerns punctuation rules. All punctuation is attached to the left of the highest possible node in case of left-branching derivations (Hockenmaier and Bisk, 2010), while in the right-branching derivations we lower the punctuation to the bottom left neighbouring node. Punctuation has no influence on the predicate-argument structure so it is safe to apply this transformation. 232 3.2 4 Revealing transitions If the topmost element on the stack is of the form XX and the second topmost element on the stack has on its right edge one or more constituents of a type X|$ we allow reveal transition.4 This is a more general way of revealing than approaches of Pareschi and Steedman (1987) and Ambati et al. (2015) who attempt to reveal only constituents of type X w"
N19-1020,P02-1042,1,0.898464,"Which algorithm gives the best parsing results? We have tested on the development set which of the parsing algorithms gives best parsing accuracy. All the algorithms use the same neural architecture and training method except for the revealing operations that require additional mechanisms to choose the node for revealing. This allows us to isolate machine learning factors and see which of the parsing strategies works the best. There are two methods that are often used for evaluating CCG parsers. They are both based on “deep” dependencies extracted from the derivation trees. The first is from (Clark et al., 2002) and is closer to categorial grammar view of dependencies. The second is from (Clark and Curran, 2007) and is meant to be more formalism independent and closer to standard dependencies (Caroll et al., 1998). We opt for the first option for development as we find it more robust and reliable but we report both types on the test set. Table 2 shows the results on development set. The heads column shows if the head words representation is used for computing the representation of the nodes in the tree. The SMP column shows if Selective Modifier Placement is used: whether we choose where to attach ri"
N19-1020,J07-3004,1,0.893217,"ch but instead runs a single CCG combinatory rule backwards. In the example at hand, first we recognise that right adjunction needs to take place because we have a category of shape XX (concretely (SNP)(SNP) but in the present CCG notation slashes “associate to the left”, so we drop the first pair of brackets). Thanks to the type of the adjunct we know that the 229 the right conjunct, with the result later combining with the left conjunct 1 : conj X ⇒ X[conj] (>Φ) X X[conj] ⇒ X (<Φ) Some additional unary and binary type-changing rules are also needed to process the derivations in CCGbank (Hockenmaier and Steedman, 2007). We use the same type-changing rules as those described in (Clark and Curran, 2007). Among the unary combinatory rules the most important one is type-raising. The first reason for that is that it allows CCG to handle constructions like argument cluster coordination in a straightforward way. Second, it allows CCG to be much more incremental as seen from the example in Figure 1b. Type-raising rules are expressed in the following way: X ⇒ Y /(Y X) (>T) X ⇒ Y (Y /X) (<T) Type-raising, is strictly limited to applying to category types that are arguments, such as NP, PP, etc., making it analogous"
N19-1020,D16-1262,0,0.351986,"Missing"
N19-1020,N16-1026,0,0.0761134,"Missing"
N19-1020,P15-1033,0,0.0401678,"ings, • head words encoding – because each constituent can have a set of heads, for instance arising from coordination, we model representation of heads with DeepSet architecture (Zaheer et al., 2017) over representations of head terminals. We do not use recursive neural networks like TreeLSTM (Tai et al., 2015) to encode subtrees because of the frequency of tree rotation. These operations are fast, but they would trigger frequent recomputation of the neural tree representation, so we opted for a mechanism that is invariant to rebranching. The stack representation is encoded using Stack-LSTM (Dyer et al., 2015). The configuration representation is the concatenation of the stack representation and the representation of the rightmost terminal in the stack. The next nonrevealing transition is chosen by a two-layer feedforward network. If the reveal transition is triggered, the system needs to choose which among the candidate nodes X|$ to adjoin the right modifier XX to. The number of these modifiers can vary so we cannot use a simple feed-forward network to choose among them. Instead, we use the mechanism of Pointer networks (Vinyals et al., 2015), which works in a similar way to attention (Bahdanau e"
N19-1020,D14-1107,1,0.962215,"time and connectedness. Waiting time is the average number of nodes that need to be shifted before the dependency between two nodes is established. The minimal value for a fully incremental algorithm is 0 (the single shift that is always necessary is not counted). Connectedness is defined as the average stack size before a shift operation is performed (the initial two shifts are forced so they are not taken in the average). The minimal value for connectedness is 1. We have computed these measures on the training portion of the CCGbank for standard non-incremental right-branching deriva234 90 Lewis and Steedman (2014) Ambati et al. (2015) Hockenmaier (2003) Zhang and Clark (2011) Clark and Curran (2007) Revealing (beam=1) Revealing (beam=4) 89.83 Labelled F1 89.8 89.61 89.6 89.58 89.43 89.4 89.2 89 89.21 Revealing Left Right 89.19 1 2 4 8 Tag 93.0 91.2 92.2 93.1 94.3 95.2 95.4 UF 88.6 89.0 92.0 — 93.0 95.5 95.8 LF 81.3 81.4 84.4 85.5 87.6 89.8 90.2 Table 3: Test set F1 results for prediction of supertags (Tag), unlabelled (UF) and labelled (LF) CCG dependencies extracted using scripts from Hockenmaier (2003) parser. 16 Beam size Figure 5: Influence of beam size on the dev results. Clark and Curran (2007) L"
N19-1020,N16-1024,0,0.170999,"those using some criteria. Woods (1973) suggests using lexical semantic information for this selection, but in his ATN system only handwritten semantic selection rules were used. Here we will also use selection based on the lexical content but it will be broad coverage and learned from the data. This ability to semantically select the modifier’s attachment point is essential for good parsing results as will be shown. Neural Model The neural probabilistic model that chooses which transition should be taken next conditions on the whole state of the configuration in a similar way to RNNG parser (Dyer et al., 2016). The words in the sentence are first embedded using the concatenation of top layers of ELMo embeddings (Peters et al., 2018) that are normalised to L2 norm and then refined with two layers of bi-LSTM (Graves et al., 2005). The neural representation of the terminal is composed of concatenated ELMo embedding and supertag embedding. The representation of a subtree combines: • span representation – we subtract representation of the leftmost terminal from the representation of the rightmost terminal as done in LSTM-Minus architecture (Wang and Chang, 2016), • combinator and category embeddings, •"
N19-1020,N16-1027,0,0.196513,"Missing"
N19-1020,P94-1018,0,0.69226,"Missing"
N19-1020,P16-1218,0,0.0251136,"nfiguration in a similar way to RNNG parser (Dyer et al., 2016). The words in the sentence are first embedded using the concatenation of top layers of ELMo embeddings (Peters et al., 2018) that are normalised to L2 norm and then refined with two layers of bi-LSTM (Graves et al., 2005). The neural representation of the terminal is composed of concatenated ELMo embedding and supertag embedding. The representation of a subtree combines: • span representation – we subtract representation of the leftmost terminal from the representation of the rightmost terminal as done in LSTM-Minus architecture (Wang and Chang, 2016), • combinator and category embeddings, • head words encoding – because each constituent can have a set of heads, for instance arising from coordination, we model representation of heads with DeepSet architecture (Zaheer et al., 2017) over representations of head terminals. We do not use recursive neural networks like TreeLSTM (Tai et al., 2015) to encode subtrees because of the frequency of tree rotation. These operations are fast, but they would trigger frequent recomputation of the neural tree representation, so we opted for a mechanism that is invariant to rebranching. The stack representa"
N19-1020,P87-1012,1,0.294216,"to atThe main obstacle to incremental sentence processing arises from right-branching constituent structures, which are present in the majority of English sentences, as well as from optional constituents that adjoin on the right, such as right adjuncts and right conjuncts. In CCG, many right-branching derivations can be replaced by semantically equivalent leftbranching incremental derivations. The problem of right-adjunction is more resistant to solution, and has been tackled in the past using revealing-based approaches that often rely either on the higher-order unification over lambda terms (Pareschi and Steedman, 1987) or heuristics over dependency representations that do not cover the whole CCGbank (Ambati et al., 2015). We propose a new incremental parsing algorithm for CCG following the same revealing tradition of work but having a purely syntactic approach that does not depend on access to a distinct level of semantic representation. This algorithm can cover the whole CCGbank, with greater incrementality and accuracy than previous proposals. 1 Introduction Combinatory Categorial Grammar (CCG) (Ades and Steedman, 1982; Steedman, 2000) is a mildly context sensitive grammar formalism that is attractive bot"
N19-1020,N18-1202,0,0.0380306,"em only handwritten semantic selection rules were used. Here we will also use selection based on the lexical content but it will be broad coverage and learned from the data. This ability to semantically select the modifier’s attachment point is essential for good parsing results as will be shown. Neural Model The neural probabilistic model that chooses which transition should be taken next conditions on the whole state of the configuration in a similar way to RNNG parser (Dyer et al., 2016). The words in the sentence are first embedded using the concatenation of top layers of ELMo embeddings (Peters et al., 2018) that are normalised to L2 norm and then refined with two layers of bi-LSTM (Graves et al., 2005). The neural representation of the terminal is composed of concatenated ELMo embedding and supertag embedding. The representation of a subtree combines: • span representation – we subtract representation of the leftmost terminal from the representation of the rightmost terminal as done in LSTM-Minus architecture (Wang and Chang, 2016), • combinator and category embeddings, • head words encoding – because each constituent can have a set of heads, for instance arising from coordination, we model repr"
N19-1020,C92-1032,0,0.655201,"Missing"
N19-1020,N16-1025,0,0.167493,"d Clark (2011) Clark and Curran (2007) Revealing (beam=1) Revealing (beam=4) 89.83 Labelled F1 89.8 89.61 89.6 89.58 89.43 89.4 89.2 89 89.21 Revealing Left Right 89.19 1 2 4 8 Tag 93.0 91.2 92.2 93.1 94.3 95.2 95.4 UF 88.6 89.0 92.0 — 93.0 95.5 95.8 LF 81.3 81.4 84.4 85.5 87.6 89.8 90.2 Table 3: Test set F1 results for prediction of supertags (Tag), unlabelled (UF) and labelled (LF) CCG dependencies extracted using scripts from Hockenmaier (2003) parser. 16 Beam size Figure 5: Influence of beam size on the dev results. Clark and Curran (2007) Lewis and Steedman (2014) Yoshikawa et al. (2017) Xu et al. (2016) Lewis et al. (2016) tri-train Vaswani et al. (2016) Lee et al. (2016) tri-train Yoshikawa et al. (2017) tri-train Revealing (beam=1) on the position embeddings or also on the node’s lexical content. First we can see that Revealing approach that uses head representation and does selective modifier placement outperforms all the models both on labelled and unlabelled dependencies. Ablation experiments show that SMP was the crucial component: without it the Revealing model is much worse. This is a clear evidence that attachment heuristics are not enough and also that previous approaches that extr"
N19-1020,W18-2809,1,0.902379,"Missing"
N19-1020,P14-1021,0,0.15632,"Missing"
N19-1020,P17-1026,0,0.507081,"or resolving attachment ambiguities. In Table 4 we show results with the second type of dependencies used for CCG evaluation. All the models, except Clark and Curran (2007), are neural and use external embeddings. From the presented models only Revealing and Xu et al. (2016) are transition based. All other models have a global search either via CKY or A* search. Our revealing-based parser that does only greedy search is outperforming all of them including those trained on large amounts of unlabelled data using semi-supervised techniques like tri-training (Lewis et al., 2016; Lee et al., 2016; Yoshikawa et al., 2017). In some sense, all the neural models in Table 4 are implicitly trained in semi-supervised way because they use pretrained embeddings that are estimated on unlabelled data. The quality of ELMo embeddings is probably one of the reasons why our parser achieves such good results. However, another semi-supervised training method, namely tri-training, is particularly attractive because, unlike ELMo, it is trained on a CCG parsing objective which is more closely aligned to what we want to do. All tri-training models are trained on much larger dataset that in addition to CCGbank also includes 43 mil"
N19-1020,P15-1150,0,0.186352,"Missing"
nissim-etal-2004-annotation,P98-1013,0,\N,Missing
nissim-etal-2004-annotation,C98-1013,0,\N,Missing
nissim-etal-2004-annotation,J96-2004,1,\N,Missing
nissim-etal-2004-annotation,carletta-etal-2004-using,1,\N,Missing
nissim-etal-2004-annotation,salmon-alt-vieira-2002-nominal,0,\N,Missing
P02-1042,J99-2004,0,\N,Missing
P02-1042,A00-2018,0,\N,Missing
P02-1042,J97-4005,0,\N,Missing
P02-1042,C96-1058,0,\N,Missing
P02-1042,hockenmaier-steedman-2002-acquiring,1,\N,Missing
P02-1042,J03-4003,0,\N,Missing
P02-1042,P02-1043,1,\N,Missing
P02-1042,P96-1011,0,\N,Missing
P02-1042,P00-1058,0,\N,Missing
P02-1042,P96-1025,0,\N,Missing
P02-1042,1997.iwpt-1.17,0,\N,Missing
P02-1042,P99-1069,0,\N,Missing
P02-1043,J97-4005,0,0.14797,"nce our approach makes so much use of the POS-tag information for unknown words. However, a POS-tagger trained on CCGbank might yield slightly better results. 5.5 Limitations of the current model Unlike Clark et al. (2002), our parser does not always model the dependencies in the logical form. For example, in the interpretation of a coordinate structure like “buy and sell shares”, shares will head an object of both buy and sell. Similarly, in examples like “buy the company that wins”, the relative construction makes company depend upon both buy as object and wins as subject. As is well known (Abney, 1997), DAG-like dependencies cannot in general be modeled with a generative approach of the kind taken here3 . 5.6 Comparison with Clark et al. (2002) Clark et al. (2002) presents another statistical CCG parser, which is based on a conditional (rather than generative) model of the derived dependency structure, including non-surface dependencies. The following table compares the two parsers according to the evaluation of surface and deep dependencies given in Clark et al. (2002). We use Clark et al.’s parser to generate these dependencies from the output of our parser (see Clark and Hockenmaier (200"
P02-1043,P02-1042,1,0.374333,"ssary to acquire correct features on categories). It is reasonable to assume that this input is of higher quality than can be produced by a POS-tagger. We therefore ran the dependency model on a test corpus tagged with the POS-tagger of Ratnaparkhi (1996), which is trained on the original Penn Treebank (see HWDep (+ tagger) in Table 3). Performance degrades slightly, which is to be expected, since our approach makes so much use of the POS-tag information for unknown words. However, a POS-tagger trained on CCGbank might yield slightly better results. 5.5 Limitations of the current model Unlike Clark et al. (2002), our parser does not always model the dependencies in the logical form. For example, in the interpretation of a coordinate structure like “buy and sell shares”, shares will head an object of both buy and sell. Similarly, in examples like “buy the company that wins”, the relative construction makes company depend upon both buy as object and wins as subject. As is well known (Abney, 1997), DAG-like dependencies cannot in general be modeled with a generative approach of the kind taken here3 . 5.6 Comparison with Clark et al. (2002) Clark et al. (2002) presents another statistical CCG parser, whi"
P02-1043,P97-1003,0,0.320625,"the training corpus does not contain all the entries required to parse the test corpus. We discuss a simple, but imperfect, solution to this problem in section 7. 5 Extending the baseline model State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes. We too can extend the baseline model described in the previous section by including more features. Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly. In order to estimate the conditional probabilities of our model, we recursively smooth empirical estimates eˆi of specific conditional distributions with (possible smoothed) estimates of less specific distributions e˜i 1 , using linear interpolation: e˜i = λeˆi + (1 λ)e˜i 1 λ is a smoothing weight which depends on the particular distribution.2 When defining models, we will indicate a backoff level with a # sign between conditioning variables, eg. A; B # C # D means that we inter"
P02-1043,W01-0521,0,0.0598412,"Missing"
P02-1043,hockenmaier-steedman-2002-acquiring,1,0.563785,"Missing"
P02-1043,1997.iwpt-1.13,0,0.102221,"Missing"
P02-1043,J98-4004,0,0.0521291,"Missing"
P02-1043,A00-2018,0,\N,Missing
P02-1043,W96-0213,0,\N,Missing
P02-1043,J03-4003,0,\N,Missing
P02-1043,P00-1058,0,\N,Missing
P13-2107,C04-1041,0,0.517598,"Missing"
P13-2107,W10-1403,1,0.645131,"Missing"
P13-2107,J07-3004,1,0.118458,"free word order language. We first describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery. 1 2 A CCG Treebank from a Dependency Treebank There have been some efforts at automatically extracting treebanks of CCG derivations from phrase structure treebanks (Hockenmaier and Steedman, 2007; Hockenmaier, 2006; Tse and Curran, 2010), and CCG lexicons from dependency treebanks (C¸akıcı, 2005). Bos et al. (2009) created a CCGbank from an Italian dependency treebank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). In this work, following C¸akıcı (2005), we first extract a Hindi CCG lexicon from a dependency treebank. We then use a CKY parser based on the CCG formalism to automatically obtain a treebank of CCG derivations from this lexicon, a novel methodology that may be applicable to obtaining CCG"
P13-2107,P06-1064,0,0.0273563,"st describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery. 1 2 A CCG Treebank from a Dependency Treebank There have been some efforts at automatically extracting treebanks of CCG derivations from phrase structure treebanks (Hockenmaier and Steedman, 2007; Hockenmaier, 2006; Tse and Curran, 2010), and CCG lexicons from dependency treebanks (C¸akıcı, 2005). Bos et al. (2009) created a CCGbank from an Italian dependency treebank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). In this work, following C¸akıcı (2005), we first extract a Hindi CCG lexicon from a dependency treebank. We then use a CKY parser based on the CCG formalism to automatically obtain a treebank of CCG derivations from this lexicon, a novel methodology that may be applicable to obtaining CCG treebanks in other"
P13-2107,J95-3006,0,0.379588,"ion etc 604 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604–609, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics ModifyTree(DependencyTree tree); for (each node in tree): handlePostPositionMarkers(node); handleCoordination(node); handleRelativeClauses(node); if (node is an argument of parent): cat = node.chunkTag; else: prescat = parent.resultCategory; cat = prescat + getDir(node, parent) + prescat; for(each child of node): if (child is an argument of node): cat = cat + getDir(child, node) + child.chunkTag; (Bharati et al., 1995; Bharati et al., 2009). The treebank contains 12,041 training, 1,233 development and 1,828 testing sentences with an average of 22 words per sentence. We used the CoNLL format1 for our purposes, which contains word, lemma, pos-tag, and coarse pos-tag in the WORD , LEMMA , POS , and CPOS fields respectively and morphological features and chunk information in the FEATS column. 2.2 Algorithm We first made a list of argument and adjunct dependency labels in the treebank. For e.g., dependencies with the label k1 and k2 (corresponding to subject and object respectively) are considered to be argumen"
P13-2107,D07-1013,0,0.0264177,"CCG treebanks in other languages as well. Introduction As compared to English, many Indian languages including Hindi have a freer word order and are also morphologically richer. These characteristics pose challenges to statistical parsers. Today, the best dependency parsing accuracies for Hindi are obtained by the shift-reduce parser of Nivre et al. (2007) (Malt). It has been observed that Malt is relatively accurate at recovering short distance dependencies, like arguments of a verb, but is less accurate at recovering long distance dependencies like co-ordination, root of the sentence, etc (Mcdonald and Nivre, 2007; Ambati et al., 2010). In this work, we show that using CCG lexical categories (Steedman, 2000), which contain subcategorization information and capture long distance dependencies elegantly, can help Malt with those dependencies. Section 2 first shows how we extract a CCG lexicon from an existing Hindi dependency treebank (Bhatt et al., 2009) and then use it to create a Hindi CCGbank. In section 3, we develop a supertagger using the CCGbank and explore different ways of providing CCG categories 2.1 Hindi Dependency Treebank In this paper, we work with a subset of the Hindi Dependency Treebank"
P13-2107,W06-2932,0,0.0517525,"Missing"
P13-2107,W09-3036,0,0.0135214,"l. (2007) (Malt). It has been observed that Malt is relatively accurate at recovering short distance dependencies, like arguments of a verb, but is less accurate at recovering long distance dependencies like co-ordination, root of the sentence, etc (Mcdonald and Nivre, 2007; Ambati et al., 2010). In this work, we show that using CCG lexical categories (Steedman, 2000), which contain subcategorization information and capture long distance dependencies elegantly, can help Malt with those dependencies. Section 2 first shows how we extract a CCG lexicon from an existing Hindi dependency treebank (Bhatt et al., 2009) and then use it to create a Hindi CCGbank. In section 3, we develop a supertagger using the CCGbank and explore different ways of providing CCG categories 2.1 Hindi Dependency Treebank In this paper, we work with a subset of the Hindi Dependency Treebank (HDT ver-0.5) released as part of Coling 2012 Shared Task on parsing (Bharati et al., 2012). HDT is a multi-layered dependency treebank (Bhatt et al., 2009) annotated with morpho-syntactic (morphological, part-of-speech and chunk information) and syntactico-semantic (dependency) information (Bharati et al., 2006; Bharati et al., 2009). Depend"
P13-2107,C10-1122,0,0.0179716,"way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery. 1 2 A CCG Treebank from a Dependency Treebank There have been some efforts at automatically extracting treebanks of CCG derivations from phrase structure treebanks (Hockenmaier and Steedman, 2007; Hockenmaier, 2006; Tse and Curran, 2010), and CCG lexicons from dependency treebanks (C¸akıcı, 2005). Bos et al. (2009) created a CCGbank from an Italian dependency treebank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). In this work, following C¸akıcı (2005), we first extract a Hindi CCG lexicon from a dependency treebank. We then use a CKY parser based on the CCG formalism to automatically obtain a treebank of CCG derivations from this lexicon, a novel methodology that may be applicable to obtaining CCG treebanks in other languages as well. Intr"
P13-2107,P05-2013,0,0.449713,"Missing"
P13-2107,P11-1069,0,0.00568072,"e have presented an approach for automatically extracting a CCG lexicon from a dependency treebank for Hindi. We have also presented a novel way of creating a CCGbank from a dependency treebank using a CCG parser and the CCG lexicon. Unlike previous work, we obtained improvements in dependency recovery using automatic supertags, as well as gold information. We have shown that informative CCG categories improve the performance of a shift-reduce dependency parser (Malt) in recovering some long distance relations. In future work, we would like to directly train a CCG shift-reduce parser (such as Zhang and Clark (2011)’s English parser) on the Hindi CCGbank. We would also like to see the impact of generalisation of our lexicon using the free-word order formalism for CCG categories of Baldridge (2002). Table 2: Supertagger impact on Hindi dependency parsing (ST=Supertags). McNemar’s test, * = p < 0.01. It is interesting to notice the impact of using automatic CCG categories from a supertagger on long distance dependencies. It is known that Malt is weak at long-distance relations (Mcdonald and Nivre, 2007; Ambati et al., 2010). Providing CCG categories as features improved handling of long-distance dependenci"
P13-2108,N10-1095,0,0.0181504,"ke Petrov & Klein (2007a). 611 • Sibling head, modifier m, and m’s nearest inner sibling • Grandchild head, modifier m, and one of m’s modifiers • Sibling+Grandchild head, modifier m, m’s nearest inner sibling, and one of m’s modifiers • Grandchild+Grandsibling head, modifier m, one of m’s modifiers g, and g’s inner sibling These features are insensitive to arc labels in the present experiments, but future work will incorporate arc labels. Each feature class contains more and less lexicalized versions. Huang (2008). Some features are omitted, with choices made based on the ablation studies of Johnson & Ural (2010). This feature set, which we call Φphrase , contains the following, mostly nonlocal, features, which are described and depicted in Charniak & Johnson (2005), Huang (2008), and Johnson & Ural (2010): • CoPar The depth (number of levels) of parallelism between adjacent conjuncts • CoParLen The difference in length between adjacent conjuncts • Edges The words or (part-of-speech) tags on the outside and inside edges of a given XP6 • NGrams Sub-parts of a given rule production • NGramTree An n-gram of the input sentence, or the tags, along with the minimal tree containing that n-gram • HeadTree A s"
P13-2108,D12-1091,0,0.0144337,"extracted from full output trees using the algorithm of de Marneffe & Manning (2008). We chose this dependency extractor, firstly, because it is natively meant to be run on the output of phrase-structure parsers, rather than on gold trees with function tags and traces still present, as is, e.g., the Penn-Converter of Johansson & Nugues (2007). Also, this is the extractor that was used in a recent shared task (Petrov & McDonald, 2012). We use EVALB and eval.pl to calculate scores. For hypothesis testing, we used the paired bootstrap test recently empirically evaluated in the context of NLP by Berg-Kirkpatrick et al. (2012). This involves drawing b subsamples of size n with replacement from the test set in question, and checking relative performance of the models on the subsample (see the reference). We use b = 106 and n = 500 in all tests. 5.2 Results The performance of the models is shown in Table 1, and Table 2 depicts the results of significance tests of differences between key model pairs. We find that adding in the higher-order dependency feature set, Φdeps , makes a statistically significant improvement in accuracy on most metrics, in most conditions. On the in-domain WSJ test set, we find that Φphrase+de"
P13-2108,P10-1001,0,0.16365,"of standard metrics. What is especially interesting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overfitting the training material. 1 Introduction Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.1 The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy (McDonald & Pereira, 2006; Carreras, 2007; Koo & Collins, 2010; Zhang & Nivre, 2011; Zhang & McDonald, 2012). This finding suggests that the same benefits might be observed in phrase-structure parsing. But, this is not necessarily implied. Phrasestructure parsers are generally stronger than dependency parsers (Petrov et al., 2010; Petrov & McDonald, 2012), and make use of more kinds of information. So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case. 1 Examples of first-order and higher-order dependency features are given in §3.2. 610 Proceedings of the 51st Annual Meeting"
P13-2108,P89-1018,0,0.210576,"vy use of head-word dependency relations necessitates the use of non-local features. 2 2.3 2.2 While the use of non-local features destroys the ability to do exact search, we can still do inexact search using Huang’s (2008) cube decoding algorithm.4 A tractable first-stage parser prunes the space of possible parses, and outputs a forest, which is a set of rule production instances that can be used to make a parse for the given sentence, and which is significantly pruned compared to the entire space allowed by the grammar. The size of this forest is at most cubic in the length of the sentence (Billot & Lang, 1989), but implicitly represents exponentially many parses. To decode, we fix an beam width of k (an integer). Then, when parsing, we visit each node n in the same bottomup order we would use for Viterbi decoding, and compute a list of the top k parses to n, according to a global linear model (Collins, 2002), using the trees that have survived the beam at earlier nodes. Phrase-Structure Parsing with Non-Local Features 2.1 Cube Decoding The First-Stage Parser As noted, we require a first-stage parser to prune the search space.5 As a by-product of this pruning procedure, we are able to use the model"
P13-2108,J93-2004,0,0.0445184,"se+deps phrase phrase+deps deps phrase+gen phrase phrase+deps+gen phrase+gen phrase+deps+gen phrase+deps WSJ F1 .042 — .013 .030 .019 BROWN UAS LAS .029 &lt;.001 .003 .122 .020 .018 — &lt;.001 .151 &lt;.001 F1 .140 — .016 .059 .008 UAS LAS .022 .012 .090 .008 .040 .009 — &lt;.001 .020 &lt;.001 Table 2: Results of statistical significance evaluations of hypotheses of the form X’s accuracy is greater than Y’s on the various test sets and metrics. Bold face indicates p &lt; .05. optimization, as is standard. The test sets are WSJ 23 (in-domain test set), and BROWN 9 (out-ofdomain test set) from the Penn Treebank (Marcus et al., 1993).7 We evaluate using harmonic mean between labelled bracket recall and precision (EVALB F1 ), unlabelled dependency accuracy (UAS), and labelled dependency accuracy (LAS). Dependencies are extracted from full output trees using the algorithm of de Marneffe & Manning (2008). We chose this dependency extractor, firstly, because it is natively meant to be run on the output of phrase-structure parsers, rather than on gold trees with function tags and traces still present, as is, e.g., the Penn-Converter of Johansson & Nugues (2007). Also, this is the extractor that was used in a recent shared task"
P13-2108,D07-1101,0,0.130423,"acy on the range of standard metrics. What is especially interesting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overfitting the training material. 1 Introduction Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.1 The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy (McDonald & Pereira, 2006; Carreras, 2007; Koo & Collins, 2010; Zhang & Nivre, 2011; Zhang & McDonald, 2012). This finding suggests that the same benefits might be observed in phrase-structure parsing. But, this is not necessarily implied. Phrasestructure parsers are generally stronger than dependency parsers (Petrov et al., 2010; Petrov & McDonald, 2012), and make use of more kinds of information. So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case. 1 Examples of first-order and higher-order dependency features are given in §3.2. 610 Proceedings of the"
P13-2108,W08-1301,0,0.090172,"Missing"
P13-2108,P05-1022,0,0.662919,"d.ac.uk Abstract To investigate this issue, we experiment using Huang’s (2008) cube decoding algorithm. This algorithm allows structured prediction with nonlocal features, as discussed in §2. Collins’s (1997) strategy of expanding the phrase-structure parser’s dynamic program to incorporate head-modifier dependency information would not scale to the complex kinds of dependencies we will consider. Using Huang’s algorithm, we can indeed incorporate arbitrary types of dependency feature, using a single, simple dynamic program. Compared to the baseline, non-local feature set of Collins (2000) and Charniak & Johnson (2005), we find that higher-order dependencies do in fact tend to improve performance significantly on both dependency and constituency accuracy metrics. Our most interesting finding, though, is that higher-order dependency features show a consistent and unambiguous contribution to the dependency accuracy, both labelled and unlabelled, of our phrase-structure parsers on outof-domain tests (which means, here, trained on WSJ , but tested on BROWN ). In fact, the gains are even stronger on out-of-domain tests than on indomain tests. One might have thought that higherorder dependencies, being rather spe"
P13-2108,P97-1003,0,0.278116,"Missing"
P13-2108,W02-1001,0,0.756788,"parses, and outputs a forest, which is a set of rule production instances that can be used to make a parse for the given sentence, and which is significantly pruned compared to the entire space allowed by the grammar. The size of this forest is at most cubic in the length of the sentence (Billot & Lang, 1989), but implicitly represents exponentially many parses. To decode, we fix an beam width of k (an integer). Then, when parsing, we visit each node n in the same bottomup order we would use for Viterbi decoding, and compute a list of the top k parses to n, according to a global linear model (Collins, 2002), using the trees that have survived the beam at earlier nodes. Phrase-Structure Parsing with Non-Local Features 2.1 Cube Decoding The First-Stage Parser As noted, we require a first-stage parser to prune the search space.5 As a by-product of this pruning procedure, we are able to use the model score of the first-stage parser as a feature in our ultimate model at no additional cost. As a first-stage parser, we use Huang et al.’s (2010) implementation of the LA - PCFG parser of Petrov et al. (2006), which uses a generative, latent-variable model. Non-Local Features To decode using exact dynamic"
P13-2108,P06-1043,0,0.129914,"Missing"
P13-2108,E06-1011,0,0.176034,"onsiderable gains in accuracy on the range of standard metrics. What is especially interesting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overfitting the training material. 1 Introduction Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.1 The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy (McDonald & Pereira, 2006; Carreras, 2007; Koo & Collins, 2010; Zhang & Nivre, 2011; Zhang & McDonald, 2012). This finding suggests that the same benefits might be observed in phrase-structure parsing. But, this is not necessarily implied. Phrasestructure parsers are generally stronger than dependency parsers (Petrov et al., 2010; Petrov & McDonald, 2012), and make use of more kinds of information. So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case. 1 Examples of first-order and higher-order dependency features are given in §3.2. 610 Pr"
P13-2108,P05-1012,0,0.13762,"rase+gen , and Φphrase+deps+gen , are then model combinations of the first three. The combination weights for these combinations are obtained using Och’s (2003) Minimum Error-Rate Training (MERT). The MERT stage helps to avoid feature undertraining (Sutton et al., 2005), and avoids the problem of scaling involved in a model that contains mostly boolean features, but one, real-valued, logscale feature. Training is conducted in three stages (SVM, MERT, SVM), so that there is no influence of any data outside the given training set (WSJ2-21) on the combination weights. Dependency Parsing Features McDonald et al. (2005) showed that chart-based dependency parsing, based on Eisner’s (1996) algorithm, could be successfully approached in a discriminative framework. In this earliest work, each feature function could only refer to a single, bigram head-modifier relationship, e.g., Modifier, below. Subsequent work (McDonald & Pereira, 2006; Carreras, 2007; Koo & Collins, 2010) looked at allowing features to access more complex, higher-order relationships, including trigram and 4-gram relationships, e.g., all features apart from Modifier, below. With the ability to incorporate non-local phrase-structure parse featur"
P13-2108,C96-1058,0,0.211509,"Missing"
P13-2108,N10-1069,0,0.016868,"3 Generative Model Score Feature Finally, we have a feature set, Φgen , containing only one feature function. This feature maps a parse to the logarithm of the MAX - RULE PRODUCT score of that parse according to the LA PCFG parsing model, which is trained separately. This score has the character of a conditional likelihood for the parse (see Petrov & Klein (2007b)). 4 Training We have two feature sets Φphrase and Φdeps , for which we fix weights using parallel stochastic optimization of a structured SVM objective (Collins, 2002; Taskar et al., 2004; Crammer et al., 2006; Martins et al., 2010; McDonald et al., 2010). To the single feature in the set Φgen (i.e. the generative model score), we give the weight 1. The combined models, Φphrase+deps , Φphrase+gen , and Φphrase+deps+gen , are then model combinations of the first three. The combination weights for these combinations are obtained using Och’s (2003) Minimum Error-Rate Training (MERT). The MERT stage helps to avoid feature undertraining (Sutton et al., 2005), and avoids the problem of scaling involved in a model that contains mostly boolean features, but one, real-valued, logscale feature. Training is conducted in three stages (SVM, MERT, SVM), so"
P13-2108,P08-1109,0,0.0232444,"arser during the training stage of a semi-supervised parser training strategy. For example, Petrov et al. (2010) have shown that a fast parser (Nivre et al., 2007) can be profitably trained from the output of a slower but more accurate one (Petrov et al., 2006), in a strategy they call uptraining. We make the source code for these experiments available.2 head phrase of each local rule production, until we reach a terminal node (or tag node). This recursion would not be allowed in standard CKY. Many discriminative parsers have used only local features (Taskar et al., 2004; Turian et al., 2007; Finkel et al., 2008). However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. And, our desire to make heavy use of head-word dependency relations necessitates the use of non-local features. 2 2.3 2.2 While the use of non-local features destroys the ability to do exact search, we can still do inexact search using Huang’s (2008) cube decoding algorithm.4 A tractable first-stage parser prunes the space of possible parses, and outputs a forest, which is a set of rule production instances that can be used to make a parse for the given sentence, and wh"
P13-2108,W01-0521,0,0.21226,"cant improvement in accuracy on most metrics, in most conditions. On the in-domain WSJ test set, we find that Φphrase+deps is significantly better than either of its component parts on all metrics. But, Φphrase+deps+gen is significantly better than Φphrase+gen only on F1 , but not on UAS or LAS. However, on the out-of-domain BROWN tests, we find that adding Φdeps always adds considerably, and in a statistically significant way, to both LAS and UAS. That is, not only is Φphrase+deps better at dependency recovery than its component parts, but Φphrase+deps+gen is also considerably bet7 Following Gildea (2001), the BROWN test set is usually divided into 10 parts. If we start indexing at 0, then the last (test) section has index 9. We received the BROWN data splits from David McClosky, p.c. 613 Type G+D D G+D G+D ter on dependency recovery than Φphrase+gen , which represents the previous state-of-the-art in this vein of research (Huang, 2008). This result is perhaps counter-intuitive, in the sense that one might have supposed that higher-order dependency features, being highly specific by nature, might only have only served to over-fit the training material. However, this result shows otherwise. Not"
P13-2108,P08-1067,0,0.228539,"of a semi-supervised parser training strategy. For example, Petrov et al. (2010) have shown that a fast parser (Nivre et al., 2007) can be profitably trained from the output of a slower but more accurate one (Petrov et al., 2006), in a strategy they call uptraining. We make the source code for these experiments available.2 head phrase of each local rule production, until we reach a terminal node (or tag node). This recursion would not be allowed in standard CKY. Many discriminative parsers have used only local features (Taskar et al., 2004; Turian et al., 2007; Finkel et al., 2008). However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. And, our desire to make heavy use of head-word dependency relations necessitates the use of non-local features. 2 2.3 2.2 While the use of non-local features destroys the ability to do exact search, we can still do inexact search using Huang’s (2008) cube decoding algorithm.4 A tractable first-stage parser prunes the space of possible parses, and outputs a forest, which is a set of rule production instances that can be used to make a parse for the given sentence, and which is significantly pr"
P13-2108,P03-1021,0,0.0119678,"Missing"
P13-2108,P07-1019,0,0.151879,"d to know exactly what the generative model is contributing to the accuracy of a generative-discriminative model combination. Thus, we conduct experiments in sets: in some cases the generative model score is used, and in others it is not used. Compared to the faster and more psychologically plausible shift-reduce parsers (Zhang & Nivre, 2011; Zhang & Clark, 2011), cube decoding is a computationally expensive method. But, cube decoding provides a relatively exact environment with which to compare different feature sets, has close connections with modern phrasebased machine translation methods (Huang & Chiang, 2007), and produces very accurate parsers. In some cases, one might want to use a slower, but more accurate, parser during the training stage of a semi-supervised parser training strategy. For example, Petrov et al. (2010) have shown that a fast parser (Nivre et al., 2007) can be profitably trained from the output of a slower but more accurate one (Petrov et al., 2006), in a strategy they call uptraining. We make the source code for these experiments available.2 head phrase of each local rule production, until we reach a terminal node (or tag node). This recursion would not be allowed in standard C"
P13-2108,P06-1055,0,0.226979,"unlabelled, of our phrase-structure parsers on outof-domain tests (which means, here, trained on WSJ , but tested on BROWN ). In fact, the gains are even stronger on out-of-domain tests than on indomain tests. One might have thought that higherorder dependencies, being rather specific by nature, would tend to pick out only very rare events, and so only serve to over-fit the training material, but this is not what we find. We speculate as to what this might mean in §5.2. The cube decoding paradigm requires a firststage parser to prune the output space. For this, we use the generative parser of Petrov et al. (2006). We can use this parser’s model score as a feature in our discriminative model at no additional cost. However, doing so conflates the contribution to accuracy of the generative model, on the one hand, and the discriminatively trained, handHigher-order dependency features are known to improve dependency parser accuracy. We investigate the incorporation of such features into a cube decoding phrase-structure parser. We find considerable gains in accuracy on the range of standard metrics. What is especially interesting is that we find strong, statistically significant gains on dependency recovery"
P13-2108,D10-1002,0,0.0235349,"Missing"
P13-2108,D10-1069,0,0.255337,"ial. 1 Introduction Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.1 The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy (McDonald & Pereira, 2006; Carreras, 2007; Koo & Collins, 2010; Zhang & Nivre, 2011; Zhang & McDonald, 2012). This finding suggests that the same benefits might be observed in phrase-structure parsing. But, this is not necessarily implied. Phrasestructure parsers are generally stronger than dependency parsers (Petrov et al., 2010; Petrov & McDonald, 2012), and make use of more kinds of information. So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case. 1 Examples of first-order and higher-order dependency features are given in §3.2. 610 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 610–616, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics written, features, on the other. Future systems might use the same or a similar feature set to ours, but in an architectu"
P13-2108,W07-2416,0,0.0306812,"test set), and BROWN 9 (out-ofdomain test set) from the Penn Treebank (Marcus et al., 1993).7 We evaluate using harmonic mean between labelled bracket recall and precision (EVALB F1 ), unlabelled dependency accuracy (UAS), and labelled dependency accuracy (LAS). Dependencies are extracted from full output trees using the algorithm of de Marneffe & Manning (2008). We chose this dependency extractor, firstly, because it is natively meant to be run on the output of phrase-structure parsers, rather than on gold trees with function tags and traces still present, as is, e.g., the Penn-Converter of Johansson & Nugues (2007). Also, this is the extractor that was used in a recent shared task (Petrov & McDonald, 2012). We use EVALB and eval.pl to calculate scores. For hypothesis testing, we used the paired bootstrap test recently empirically evaluated in the context of NLP by Berg-Kirkpatrick et al. (2012). This involves drawing b subsamples of size n with replacement from the test set in question, and checking relative performance of the models on the subsample (see the reference). We use b = 106 and n = 500 in all tests. 5.2 Results The performance of the models is shown in Table 1, and Table 2 depicts the result"
P13-2108,N07-1051,0,0.16516,"g., the first word dominated by S is Pierre is also local, since the words of the sentence are constant across hypothesized parses, and words can be referred to by their position with respect to a given rule production. See Huang (2008) for more details. This algorithm is closely related to the algorithm for phrase-based machine translation using a language model (Huang & Chiang, 2007). 5 All work in this paradigm has used a generative parser as the first-stage parser. But, this is arguably a historical accident. We could just as well use a discriminative parser with only local features, like Petrov & Klein (2007a). 611 • Sibling head, modifier m, and m’s nearest inner sibling • Grandchild head, modifier m, and one of m’s modifiers • Sibling+Grandchild head, modifier m, m’s nearest inner sibling, and one of m’s modifiers • Grandchild+Grandsibling head, modifier m, one of m’s modifiers g, and g’s inner sibling These features are insensitive to arc labels in the present experiments, but future work will incorporate arc labels. Each feature class contains more and less lexicalized versions. Huang (2008). Some features are omitted, with choices made based on the ablation studies of Johnson & Ural (2010)."
P13-2108,W04-3201,0,0.266366,"want to use a slower, but more accurate, parser during the training stage of a semi-supervised parser training strategy. For example, Petrov et al. (2010) have shown that a fast parser (Nivre et al., 2007) can be profitably trained from the output of a slower but more accurate one (Petrov et al., 2006), in a strategy they call uptraining. We make the source code for these experiments available.2 head phrase of each local rule production, until we reach a terminal node (or tag node). This recursion would not be allowed in standard CKY. Many discriminative parsers have used only local features (Taskar et al., 2004; Turian et al., 2007; Finkel et al., 2008). However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. And, our desire to make heavy use of head-word dependency relations necessitates the use of non-local features. 2 2.3 2.2 While the use of non-local features destroys the ability to do exact search, we can still do inexact search using Huang’s (2008) cube decoding algorithm.4 A tractable first-stage parser prunes the space of possible parses, and outputs a forest, which is a set of rule production instances that can be used to"
P13-2108,D12-1030,0,0.0679997,"nteresting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overfitting the training material. 1 Introduction Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.1 The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy (McDonald & Pereira, 2006; Carreras, 2007; Koo & Collins, 2010; Zhang & Nivre, 2011; Zhang & McDonald, 2012). This finding suggests that the same benefits might be observed in phrase-structure parsing. But, this is not necessarily implied. Phrasestructure parsers are generally stronger than dependency parsers (Petrov et al., 2010; Petrov & McDonald, 2012), and make use of more kinds of information. So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case. 1 Examples of first-order and higher-order dependency features are given in §3.2. 610 Proceedings of the 51st Annual Meeting of the Association for Computational Linguisti"
P13-2108,P11-1069,0,0.0176563,"tures, on the other. Future systems might use the same or a similar feature set to ours, but in an architecture that does not include any generative parser. On the other hand, some systems might indeed incorporate this generative model’s score. So, we need to know exactly what the generative model is contributing to the accuracy of a generative-discriminative model combination. Thus, we conduct experiments in sets: in some cases the generative model score is used, and in others it is not used. Compared to the faster and more psychologically plausible shift-reduce parsers (Zhang & Nivre, 2011; Zhang & Clark, 2011), cube decoding is a computationally expensive method. But, cube decoding provides a relatively exact environment with which to compare different feature sets, has close connections with modern phrasebased machine translation methods (Huang & Chiang, 2007), and produces very accurate parsers. In some cases, one might want to use a slower, but more accurate, parser during the training stage of a semi-supervised parser training strategy. For example, Petrov et al. (2010) have shown that a fast parser (Nivre et al., 2007) can be profitably trained from the output of a slower but more accurate one"
P13-2108,P11-2033,0,0.23304,"What is especially interesting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overfitting the training material. 1 Introduction Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.1 The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy (McDonald & Pereira, 2006; Carreras, 2007; Koo & Collins, 2010; Zhang & Nivre, 2011; Zhang & McDonald, 2012). This finding suggests that the same benefits might be observed in phrase-structure parsing. But, this is not necessarily implied. Phrasestructure parsers are generally stronger than dependency parsers (Petrov et al., 2010; Petrov & McDonald, 2012), and make use of more kinds of information. So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case. 1 Examples of first-order and higher-order dependency features are given in §3.2. 610 Proceedings of the 51st Annual Meeting of the Association fo"
P14-1061,W03-1812,0,0.0281587,"to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs"
P14-1061,D10-1115,0,0.0291917,"nes of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 We propose a method for addressing MWPs of varying degrees of compositionality through the integration of the distributional representation of"
P14-1061,D13-1147,0,0.0157785,"ositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 We propose a method for addressing MWPs of varying degrees of compositionality through the integration of the distributional representation of multiple sub-sets of the predicate’s words (LCs). We use it to tackle a supervised prediction task that represents predicates distributionally. Our model assumes a latent distribution over the LCs, and estimates its parameters so to best conf"
P14-1061,P11-1062,0,0.424813,"difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of ar"
P14-1061,Q13-1015,1,0.929241,"between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 2008; Melamud et al., 2013b), or as part of a more comprehensive system (Berant et al., 2011; Lewis and Steedman, 2013). For example, consider the verb “take”. While the inference relation “have → take” does not generally hold, it does hold in the case of some light verbs, such as “have a look → take a look”, underscoring the importance of taking more inclusive LCs into account. On the other hand, the predicate “likely to give a green light” is unlikely to appear often even within a very large corpus, and could benefit from taking its lexical sub-units (e.g., “likely” or “give a green light”) into account. We present a novel approach to the task that models the selection and relative weighting of the predicate"
P14-1061,W11-1304,0,0.0207168,"fication of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositio"
P14-1061,P98-2127,0,0.0575403,"Missing"
P14-1061,P99-1041,0,0.0817136,"ultiple LCs to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly b"
P14-1061,W02-0109,0,0.0615685,"Missing"
P14-1061,W03-1810,0,0.0446691,"Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess"
P14-1061,P13-1131,0,0.299606,"applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the r"
P14-1061,C10-2029,0,0.0623497,"Missing"
P14-1061,P13-2051,0,0.0727332,"applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the r"
P14-1061,E09-1025,0,0.0154396,"icate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 200"
P14-1061,D11-1142,0,0.0351763,"This section lists the features used for our experiments. We intentionally select a feature set that relies on either completely unsupervised or shallow processing tools that are available for a wide variety of languages and domains. Given a predicate pair p(i) , a label y ∈ {1, −1} and a latent state h ∈ H (i) , we define their feature vector as Φ(p(i) , y, h) = y · Φ(p(i) , h). The computation of Φ(p(i) , h) requires a reference corpus R that contains triplets of the type (p, x, y) where p is a binary predicate and x and y are its arguments. We use the Reverb corpus as R in our experiments (Fader et al., 2011; see Section 4). We refrain from encoding features that directly reflect the vocabulary of the training set. Such features are not applicable beyond that set’s vocabulary, and as available datasets contain no more than a few thousand examples, these features are unlikely to generalize well. Table 1 presents the set of features we use in our experiments. The features can be divided into two main categories: similarity features between the LHS and the RHS predicates (table’s top), and features that reflect the individual properties of each ∇L = Eh [Φ(pi , yi , h)] − Eh,y [Φ(pi , y, h)] − λ · w"
P14-1061,P06-2075,0,0.0230173,"te inference rules with token-level information from their arguments in a specific context. Melamud et al. (2013b) used lexical expansion to improve the representation of infrequent predicates. Lewis and Steedman (2013) combined distributional and symbolic representations, evaluating on a Question Answering task, as well as on a quantification-focused entailment dataset. Several studies tackled the task using supervised systems. Weisman et al. (2012) used a set of linguistically motivated features, but evaluated their system on a corpus that consists almost entirely of single-word predicates. Mirkin et al. (2006) presented a system for learning inference rules between nouns, using distributional similarity and pattern-based features. Hagiwara et al. (2009) identified synonyms using a supervised approach relying on distributional and syntactic features. Berant et al. (2011) used distributional similarity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtained a more accurate set of inference rules. Previous work used simple methods to select the predicate’s LC. Some filtered out frequent highly ambiguous verbs (Lewis and"
P14-1061,E06-1043,0,0.0497327,"Missing"
P14-1061,D07-1110,0,0.0218242,"Missing"
P14-1061,D13-1060,0,0.0132378,"tt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWE"
P14-1061,P13-2046,0,0.0173758,"bject of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional"
P14-1061,P02-1006,0,0.00907062,"h addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it gener"
P14-1061,W03-1011,0,0.152252,"pic|hL ) for each of the induced topics. The entropy of the topic distribution P (topic|hL ) Table 1: The feature set used in our experiments. The top part presents the similarity measures based on the DIRT approach. The rest of the listed features apply to the LHS predicate (hL ), and to the first word in it (hA L ). Analogous features are A introduced for the second word, hB L , and for the RHS predicate. The upper-middle part presents the word features for hL . The lower-middle part presents features that apply where hL is of size 2. The bottom part lists the LDA-based features. (1998) and Weeds and Weir (2003) did not yield additional improvements. We encode the similarity of all measures for the A pair hL and hR as well as the pair hA L and hR . The latter feature is an approximation to the similarity between the heads of the predicates, as heads in English tend to be to the left of the predicates. These two features coincide for h values of size 1. Word and Pair Features. These features encode the basic properties of the LC. The motivation behind them is to allow a more accurate leveraging of the similarity features, as well as to better determine the relative weights of h ∈ H (i) . The feature s"
P14-1061,I11-1024,0,0.02046,"rm treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 We propose a method for addressing MWPs of varying degrees of compositionality through the integration of the distributional representation of multiple sub-sets of the predicate’s words (LCs). We use it to tackle a supervised prediction task that represents predicates distributionally. Our model assumes a latent distribution over the"
P14-1061,D12-1018,0,0.153469,"presented an unsupervised system for learning inference rules directly from open-domain web data. Melamud et al. (2013a) used topic models to combine typelevel predicate inference rules with token-level information from their arguments in a specific context. Melamud et al. (2013b) used lexical expansion to improve the representation of infrequent predicates. Lewis and Steedman (2013) combined distributional and symbolic representations, evaluating on a Question Answering task, as well as on a quantification-focused entailment dataset. Several studies tackled the task using supervised systems. Weisman et al. (2012) used a set of linguistically motivated features, but evaluated their system on a corpus that consists almost entirely of single-word predicates. Mirkin et al. (2006) presented a system for learning inference rules between nouns, using distributional similarity and pattern-based features. Hagiwara et al. (2009) identified synonyms using a supervised approach relying on distributional and syntactic features. Berant et al. (2011) used distributional similarity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtaine"
P14-1061,P10-1044,0,0.0605228,"Missing"
P14-1061,P06-1107,0,0.0264756,"to define and model, and consequently pose serious difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation"
P14-1061,D10-1106,0,0.510611,"nd consequently pose serious difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally hold"
P14-1061,P12-2031,0,0.683025,"selection and relative weighting of the predicate’s LCs using latent variables. This approach allows the classifier that uses the distributional representations to take into account the most relevant LCs in order to make the prediction. By doing so, we avoid the notoriously difficult problem of defining and identifying MWPs and account for predicates of various sizes and degrees of compositionality. To our knowledge, this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics. We conduct experiments on the dataset of Zeichner et al. (2012) and compare our methods with analogous ones that select a fixed LC, using stateof-the-art feature sets. Our method obtains substantial performance gains across all scenarios. Finally, we note that our approach is cognitively appealing. Significant cognitive findings support the claim that a speaker’s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (200"
P14-1061,I05-5011,0,0.0293266,"main difficult to define and model, and consequently pose serious difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said t"
P14-1061,N06-1039,0,0.0171201,"paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distri"
P14-1061,C08-1107,0,0.405935,"ms (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 2008; Melamud et al., 2013b), or as part of a more comprehensive system (Berant et al., 2011; Lewis and Steedman, 2013). For example, consider the verb “take”. While the inference relation “have → take” does not generally hold, it does hold in the case of some light verbs, such as “have a look → take a look”, underscoring the importance of taking more inclusive LCs into account. On the other hand, the predicate “likely to give a green light” is unlikely to appear often even within a very large corpus, and could benefit from taking its lexical sub-units (e.g., “likely” or “give a green light”) into"
P14-1061,W11-0807,0,0.0965207,"it, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used composi"
P14-1061,P06-4018,0,\N,Missing
P14-1061,P10-1045,0,\N,Missing
P14-1061,C98-2122,0,\N,Missing
P15-1126,D10-1056,1,0.866572,"action: smaller ≈ bigger − big + small. However, in the case of the CBSOW and CBSOWM models, we use the novel approach described in Section 3.5: vsmaller ≈ bsmall ⊕ sbigger . Similarity is then based on the cosine measure for all types of representation. The noun-verb similarity task is based on correlating the model’s predicted semantic similarity for words with human ratings gathered in an onThe syntactic component of the representations is evaluated by clustering the vectors and then comparing the induced classes to the POS classes found in the Penn Treebank. We use the manyto-one measure (Christodoulopoulos et al., 2010; Yatbaz et al., 2012) to determine the extent to which the clusters agree with the POS classes. Each cluster is mapped to its most frequent gold tag and the reported score is the proportion of word tokens correctly tagged using this mapping. The clustering itself is a form of k-means clustering, where similarity is measured in terms of the cosine measure. Each vector is assigned to a clus1306 0.7 0.6 ● ● ● ● ● ● ● ● ● CBOW CBSOW CBOM CBSOWM 0.5 ● 0.4 ● ● CBOW CBSOW CBOM CBSOWM 0.0 0.0 ● ● ● ● 0.1 ● ● ● ● ● 0.3 Average MTO 0.2 ● ● ● ● ● ● 0.2 0.4 0.3 ● ● 0.1 Average Correlation 0.5 0.6 ● ● 10"
P15-1126,E03-1009,0,0.109812,", then v would have a representation in terms of d = e + f components which would just be the concatenation of the two sets of components, which we will represent in terms of the operator ⊕. v =b⊕s (2) Achieving this differentiation within the representations requires that the model have a means of differentiating semantic and syntactic information in the raw text. We consider two very simple approaches for this purpose, based on morphological and word order features. Both these types of features have been previously employed in simple word co-occurrence models (e.g., McDonald and Lowe, 1998; Clark, 2003), with bag-of-words and 1302 b−1 b1 b2 Each word is given a Huffman code corresponding to a path through a binary tree, and the output predicts the binary choices on nodes of the tree as independent variables. In comparison to the computational cost of doing the full softmax over the whole vocabulary, this hierarchical approach is much more efficient. Each node is associated with a vector, n, and the output at that node, given a context vector, bcontext , is: wt−1 wt+1 wt+2 p = logistic(n · bcontext ) wt 6 bcontext b−2 k 3  A KQ Q     A Q   A QQ   A Q  wt−2 Figure 2: CBOW model pred"
P15-1126,Q13-1015,1,0.80721,"e related to orthogonality within the space. Explicitly designing such structure into a neural network model results in representations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within English Wikipedia text to enable this decomposition can produce substantial improvements on semantic-similarity, posinduction and word-analogy tasks. 1 Introduction Distributional methods have become widely used across computational linguistics. Recent applications include predicate clustering for question answering (Lewis and Steedman, 2013), bilingual embeddings for machine translation (Zou et al., 2013) and enhancing the coverage of POS tagging (Huang et al., 2013). The popularity of these methods, stemming from their conceptual simplicity and wide applicability, motivates a deeper analysis of the structure of the representations they produce. Commonly, these representations are made in a single vector space with similarity being the main structure of interest. However, recent work by Mikolov et al. (2013b) on a word-analogy task suggests that such spaces may have further useful internal regularities. They found that semantic d"
P15-1126,D14-1107,1,0.799635,"nondecomposed space. Future work ought to pursue models in which all morphemes contribute both semantic and syntactic content to the word representations. It would also be desirable to explore more practical applications of these representations than the limited evaluations presented here. It seems feasible that our decomposition of representations could benefit tasks that need to differentiate their treatment of semantic and syntactic content. In particular, applications of word embeddings that mainly involve syntax, such as POS tagging (e.g., Tsuboi, 2014) or supertagging for parsing (e.g., Lewis and Steedman, 2014), may be a reasonable starting point. Acknowledgements We would like to thank Stella Frank, Sharon Goldwater and other colleagues along with our reviewers for criticism, advice and discussion. This work was supported by ERC Advanced Fellowship 249520 GRAMPLUS and EU Cognitive Systems project FP7-ICT-270273 Xperience. References Our experiments demonstrate the utility of orthogonality within vector-space representations in a number of ways. In terms of existing models, we find that the cosines of vector-differences is a strong predictor of the performance of CBOW, Skip-gram and GloVe representa"
P15-1126,W13-3512,0,0.108747,"Missing"
P15-1126,D14-1101,0,0.0368363,"f all its morphemes, though only within a single nondecomposed space. Future work ought to pursue models in which all morphemes contribute both semantic and syntactic content to the word representations. It would also be desirable to explore more practical applications of these representations than the limited evaluations presented here. It seems feasible that our decomposition of representations could benefit tasks that need to differentiate their treatment of semantic and syntactic content. In particular, applications of word embeddings that mainly involve syntax, such as POS tagging (e.g., Tsuboi, 2014) or supertagging for parsing (e.g., Lewis and Steedman, 2014), may be a reasonable starting point. Acknowledgements We would like to thank Stella Frank, Sharon Goldwater and other colleagues along with our reviewers for criticism, advice and discussion. This work was supported by ERC Advanced Fellowship 249520 GRAMPLUS and EU Cognitive Systems project FP7-ICT-270273 Xperience. References Our experiments demonstrate the utility of orthogonality within vector-space representations in a number of ways. In terms of existing models, we find that the cosines of vector-differences is a strong predict"
P15-1126,D12-1086,0,0.0320897,"Missing"
P15-1126,D13-1141,0,0.0292535,"ructure into a neural network model results in representations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within English Wikipedia text to enable this decomposition can produce substantial improvements on semantic-similarity, posinduction and word-analogy tasks. 1 Introduction Distributional methods have become widely used across computational linguistics. Recent applications include predicate clustering for question answering (Lewis and Steedman, 2013), bilingual embeddings for machine translation (Zou et al., 2013) and enhancing the coverage of POS tagging (Huang et al., 2013). The popularity of these methods, stemming from their conceptual simplicity and wide applicability, motivates a deeper analysis of the structure of the representations they produce. Commonly, these representations are made in a single vector space with similarity being the main structure of interest. However, recent work by Mikolov et al. (2013b) on a word-analogy task suggests that such spaces may have further useful internal regularities. They found that semantic differences, such as between big and small, and also syntactic dif"
P15-1126,N13-1090,0,0.723674,"widely used across computational linguistics. Recent applications include predicate clustering for question answering (Lewis and Steedman, 2013), bilingual embeddings for machine translation (Zou et al., 2013) and enhancing the coverage of POS tagging (Huang et al., 2013). The popularity of these methods, stemming from their conceptual simplicity and wide applicability, motivates a deeper analysis of the structure of the representations they produce. Commonly, these representations are made in a single vector space with similarity being the main structure of interest. However, recent work by Mikolov et al. (2013b) on a word-analogy task suggests that such spaces may have further useful internal regularities. They found that semantic differences, such as between big and small, and also syntactic differences, as between big and bigger, were encoded consistently across their space. In particular, they solved the word-analogy problems by exploiting the fact that equivalent relations tended to correspond to parallel vectordifferences. In this paper, we investigate orthogonality between relations rather than parallelism. While parallelism serves to ensure that the same relation is encoded consistently, our"
P15-1126,W13-0210,1,0.843753,"son to the computational cost of doing the full softmax over the whole vocabulary, this hierarchical approach is much more efficient. Each node is associated with a vector, n, and the output at that node, given a context vector, bcontext , is: wt−1 wt+1 wt+2 p = logistic(n · bcontext ) wt 6 bcontext b−2 k 3  A KQ Q     A Q   A QQ   A Q  wt−2 Figure 2: CBOW model predicting wt from of a bag-of-words representation, bcontext , of a 4-word window around it. lemmatization being good for semantic applications, while sequential order and suffixes is more useful for syntax. More recently, Mitchell (2013) demonstrated that word order could be used to separate syntactic from semantic structure, but only within a simple bigram language model, rather than a neural network model, and without exploiting morphology. Our enhanced models are based on Mikolov et al.’s (2013a) CBOW architecture, which is described in Section 2. The novel extensions to it, employing a semantic-syntactic decomposition, are proposed in Section 3. We then describe our evaluation tasks and provide their results in Sections 5 and 6 respectively. These evaluations are based on the word-analogy dataset of Mikolov et al. (2013b)"
P15-1126,D14-1162,0,0.0938417,"Missing"
P15-1126,J14-1004,0,\N,Missing
P15-1141,P09-2085,1,0.884586,"Missing"
P15-1141,P13-1148,1,0.893639,"Missing"
P15-1141,N13-1019,1,0.941764,"e meaning of text by identifying a set of latent topics from a collection of documents and assigning each word in these documents to one of the latent topics. A document is modelled as a mixture of latent topics, and each topic is a distribution over a finite vocabulary of words. It is common for topic models to treat documents as bags-of-words, ignoring any internal structure. While this simplifies posterior inference, it also ignores the information encoded in, for example, syntactic relationships (Boyd-Graber and Blei, 2009), word order (Wallach, 2006) and the topic structure of documents (Du et al., 2013). Here we are interested in topic models that capture dependencies between adjacent words in a topic dependent way. For example, the phrase “white house” can be interpreted compositionally in a real-estate context, but not in a political context. Several extensions of LDA have been proposed that assign topics not only to individual words but also to multi-word phrases, which we call topical collocations. However, as we will discuss in section 2, most of those extensions either rely on a pre-processing step to identify potential collocations (e.g., bigrams and trigrams) or limit attention to bi"
P15-1141,P10-1117,1,0.84281,"Most existing topic models make the bagof-words assumption that words are generated independently, and so ignore potentially useful information about word order. Previous attempts to use collocations (short sequences of adjacent words) in topic models have either relied on a pipeline approach, restricted attention to bigrams, or resulted in models whose inference does not scale to large corpora. This paper studies how to simultaneously learn both collocations and their topic assignments. We present an efficient reformulation of the Adaptor Grammar-based topical collocation model (AG-colloc) (Johnson, 2010), and develop a point-wise sampling algorithm for posterior inference in this new formulation. We further improve the efficiency of the sampling algorithm by exploiting sparsity and parallelising inference. Experimental results derived in text classification, information retrieval and human evaluation tasks across a range of datasets show that this reformulation scales to hundreds of thousands of documents while maintaining the good performance of the AG-colloc model. 1 Introduction Probabilistic topic models like Latent Dirichlet Allocation (LDA) (Blei et al., 2003) are commonly used to study"
P15-1141,Q14-1036,0,0.0454692,"Missing"
P15-1141,P06-1124,0,0.0273164,"allowed TNG to outperform LDACOL on a standard information retrieval task. However, both LDACOL and TNG do not require words within a sequence to share the same topic, which can result in semantically incoherent collocations. Subsquent models have sought to encourage topically coherent collocations, including PhraseDiscovering LDA (Lindsey et al., 2012), the timebased topical n-gram model (Jameel and Lam, 2013a) and the n-gram Hierarchical Dirichlet Process (HDP) model (Jameel and Lam, 2013b). Phrase-Discovering LDA is a non-parametric ex1461 tension of TNG inspired by Bayesian N-gram models Teh (2006) that incorporate a Pitman-Yor Process prior. The n-gram HDP is a nonparametric extension of LDA-colloc, putting an HDP prior on the per-document topic distribution. Both of these non-parametric extensions use the Chinese Franchise representation for posterior inference. Our work here is based on the AG-colloc model proposed by Johnson (2010). He showed how Adaptor Grammars can generalise LDA to learn topical collocations of unbounded length while jointly identifying the topics that occur in each document. Unfortunately, because the Adaptor Grammar inference algorithm uses Probabilistic Contex"
P15-1141,E14-1056,0,0.0309651,"rd tokens which are grouped into D documents. We sample from the posterior distribution over segmentations of documents into collocations, and assignments of topics to collocations. Let each document d be a sequence of Nd words wd,1 , . . . , wd,Nd . We introduce a set of auxiliary random variables bd,1 , . . . , bd,Nd . The value 2 In the TCM, the vocabulary differs from topic to topic. Given a sequence of adjacent words, it is hard to tell if it is a collocation without knowing the topic of its context. Therefore, the Pointwise Mutual Information (PMI) (Newman et al., 2010) and its variant (Lau et al., 2014) are not applicable to our TCM in evaluation. of bd,j indicates whether there is a collocation boundary between wd,j and wd,j+1 , and, if there is, the topic of the collocation to the left of the boundary. If there is no boundary then bd,j = 0. Otherwise, there is a collocation to the left of the boundary consisting of the words wd,l+1 , . . . , wd,j where l = max {i |1 ≤ i ≤ j − 1 ∧ bd,i 6= 0}, and bd,j = k (1 ≤ k ≤ K) is the topic of the collocation. Note that bd,Nd must not be 0 as the end of a document is always a collocation boundary. For example, consider the document consisting of the w"
P15-1141,D12-1020,0,0.0195177,"lso the first word’s topic assignment, proposing the topical N-gram (TNG) model. In other words, whereas LDACOL only adds a distribution for every word-type to LDA, TNG adds a distribution for every possible word-topic pair. Wang et al. found that this modification allowed TNG to outperform LDACOL on a standard information retrieval task. However, both LDACOL and TNG do not require words within a sequence to share the same topic, which can result in semantically incoherent collocations. Subsquent models have sought to encourage topically coherent collocations, including PhraseDiscovering LDA (Lindsey et al., 2012), the timebased topical n-gram model (Jameel and Lam, 2013a) and the n-gram Hierarchical Dirichlet Process (HDP) model (Jameel and Lam, 2013b). Phrase-Discovering LDA is a non-parametric ex1461 tension of TNG inspired by Bayesian N-gram models Teh (2006) that incorporate a Pitman-Yor Process prior. The n-gram HDP is a nonparametric extension of LDA-colloc, putting an HDP prior on the per-document topic distribution. Both of these non-parametric extensions use the Chinese Franchise representation for posterior inference. Our work here is based on the AG-colloc model proposed by Johnson (2010)."
P18-1036,K17-3002,0,0.0157385,"ement, we combine character and morphology-level models and measure the performance of the ensemble to better understand how similar they are. We experiment with several languages with varying degrees of morphological richness and typology: Turkish, Finnish, Czech, German, Spanish, Catalan and English. Our experiments and analysis reveal insights such as: Character-level Models: Character-level models have proven themselves useful for many NLP tasks such as language modeling (Ling et al., 2015; Kim et al., 2016), POS tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Dozat et al., 2017) and machine translation (Lee et al., 2017). However the number of comparative studies that analyze their relation to morphology are rather limited. Recently, Vania and Lopez (2017) presented a unified framework, where they investigated the performances of different subword units, namely characters, morphemes and morphological analysis on language modeling task. They experimented with languages of varying morphological typologies and concluded that the performance of character models can not yet match the morphological models, albeit very close. Similarly, Belinkov et al. (2017) analyzed how d"
P18-1036,D15-1176,0,0.52325,"herefore, for a successful semantic application, the model should be able to capture both the regularities, i.e, morphological tags and the irregularities, i.e, lemmas of the word. Morphological analysis already provides the aforementioned information about the words. However access to useful morphological features may be problematic due to software licensing issues, lack of robust morphological analyzers and high ambiguity among analyses. Characterlevel models (CLM), being a cheaper and accessible alternative to morphology, have been reported as performing competitively on various NLP tasks (Ling et al., 2015; Plank et al., 2016; Lee et al., 2017). However the extent to which these tasks depend on morphology is small; and their relation to semantics is weak. Hence, little is known on their true ability to reveal the underlying morphological structure of a word and their semantic capabilities. Furthermore, their behaviour across languages from different families; and their limitations and strengths such as handling of longrange dependencies, reaction to model complexity or performance on out-of-domain data are unknown. Analyzing such issues is a key to fully Character-level models have become a pop"
P18-1036,D15-1112,0,0.0405881,"Missing"
P18-1036,K17-1041,0,0.0291754,"onvolutional network to perform various NLP tasks. Later, the combination of neural networks (LSTMs in particular) with traditional SRL features (categorical and binary) has been introduced (FitzGerald et al., 2015). Recently, it has been shown that careful design and tuning of deep models can achieve state-of-the-art with no or minimal syntactic knowledge for English and Chinese SRL. Although the architectures vary slightly, they are mostly based on a variation of bi-LSTMs. Zhou and Xu (2015); He et al. (2017) connect the layers of LSTM in an interleaving pattern where in (Wang et al., 2015; Marcheggiani et al., 2017) regular bi-LSTM layers are used. Commonly used features for the encoding layer are: pretrained word embeddings; distance from the predicate; predicate context; predicate region mark or flag; POS tag; and predicate lemma embedding. Only a few of the models (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017) perform dependency-based SRL. Furthermore, all methods focus on languages with rich resources and less morphological complexity like English and Chinese. [Villagers]comers came [to town]end point We use a simple method based on bidirectional LSTMs to train three types of base semantic"
P18-1036,D17-1159,0,0.015632,"-the-art with no or minimal syntactic knowledge for English and Chinese SRL. Although the architectures vary slightly, they are mostly based on a variation of bi-LSTMs. Zhou and Xu (2015); He et al. (2017) connect the layers of LSTM in an interleaving pattern where in (Wang et al., 2015; Marcheggiani et al., 2017) regular bi-LSTM layers are used. Commonly used features for the encoding layer are: pretrained word embeddings; distance from the predicate; predicate context; predicate region mark or flag; POS tag; and predicate lemma embedding. Only a few of the models (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017) perform dependency-based SRL. Furthermore, all methods focus on languages with rich resources and less morphological complexity like English and Chinese. [Villagers]comers came [to town]end point We use a simple method based on bidirectional LSTMs to train three types of base semantic role labelers that employ (1) words (2) characters and character sequences and (3) gold morphological analysis. The gold morphology serves as the upper bound for us to compare and analyze the performances of character-level models on languages of varying morphological typologies. We carry out an exhaustive error"
P18-1036,P16-2067,0,0.0850624,"cessful semantic application, the model should be able to capture both the regularities, i.e, morphological tags and the irregularities, i.e, lemmas of the word. Morphological analysis already provides the aforementioned information about the words. However access to useful morphological features may be problematic due to software licensing issues, lack of robust morphological analyzers and high ambiguity among analyses. Characterlevel models (CLM), being a cheaper and accessible alternative to morphology, have been reported as performing competitively on various NLP tasks (Ling et al., 2015; Plank et al., 2016; Lee et al., 2017). However the extent to which these tasks depend on morphology is small; and their relation to semantics is weak. Hence, little is known on their true ability to reveal the underlying morphological structure of a word and their semantic capabilities. Furthermore, their behaviour across languages from different families; and their limitations and strengths such as handling of longrange dependencies, reaction to model complexity or performance on out-of-domain data are unknown. Analyzing such issues is a key to fully Character-level models have become a popular approach specia"
P18-1036,P17-1044,0,0.0235758,"first introduced to the SRL scene by Collobert et al. (2011), where they use a unified end-to-end convolutional network to perform various NLP tasks. Later, the combination of neural networks (LSTMs in particular) with traditional SRL features (categorical and binary) has been introduced (FitzGerald et al., 2015). Recently, it has been shown that careful design and tuning of deep models can achieve state-of-the-art with no or minimal syntactic knowledge for English and Chinese SRL. Although the architectures vary slightly, they are mostly based on a variation of bi-LSTMs. Zhou and Xu (2015); He et al. (2017) connect the layers of LSTM in an interleaving pattern where in (Wang et al., 2015; Marcheggiani et al., 2017) regular bi-LSTM layers are used. Commonly used features for the encoding layer are: pretrained word embeddings; distance from the predicate; predicate context; predicate region mark or flag; POS tag; and predicate lemma embedding. Only a few of the models (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017) perform dependency-based SRL. Furthermore, all methods focus on languages with rich resources and less morphological complexity like English and Chinese. [Villagers]comers cam"
P18-1036,P17-1184,0,0.181493,"s with varying degrees of morphological richness and typology: Turkish, Finnish, Czech, German, Spanish, Catalan and English. Our experiments and analysis reveal insights such as: Character-level Models: Character-level models have proven themselves useful for many NLP tasks such as language modeling (Ling et al., 2015; Kim et al., 2016), POS tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Dozat et al., 2017) and machine translation (Lee et al., 2017). However the number of comparative studies that analyze their relation to morphology are rather limited. Recently, Vania and Lopez (2017) presented a unified framework, where they investigated the performances of different subword units, namely characters, morphemes and morphological analysis on language modeling task. They experimented with languages of varying morphological typologies and concluded that the performance of character models can not yet match the morphological models, albeit very close. Similarly, Belinkov et al. (2017) analyzed how different word representations help learn better morphology and model rare words on a neural MT task and concluded that characterbased representations are much better for learning •"
P18-1036,Q17-1026,0,0.249231,"lication, the model should be able to capture both the regularities, i.e, morphological tags and the irregularities, i.e, lemmas of the word. Morphological analysis already provides the aforementioned information about the words. However access to useful morphological features may be problematic due to software licensing issues, lack of robust morphological analyzers and high ambiguity among analyses. Characterlevel models (CLM), being a cheaper and accessible alternative to morphology, have been reported as performing competitively on various NLP tasks (Ling et al., 2015; Plank et al., 2016; Lee et al., 2017). However the extent to which these tasks depend on morphology is small; and their relation to semantics is weak. Hence, little is known on their true ability to reveal the underlying morphological structure of a word and their semantic capabilities. Furthermore, their behaviour across languages from different families; and their limitations and strengths such as handling of longrange dependencies, reaction to model complexity or performance on out-of-domain data are unknown. Analyzing such issues is a key to fully Character-level models have become a popular approach specially for their acces"
P18-1036,D15-1186,0,0.0222925,"nified end-to-end convolutional network to perform various NLP tasks. Later, the combination of neural networks (LSTMs in particular) with traditional SRL features (categorical and binary) has been introduced (FitzGerald et al., 2015). Recently, it has been shown that careful design and tuning of deep models can achieve state-of-the-art with no or minimal syntactic knowledge for English and Chinese SRL. Although the architectures vary slightly, they are mostly based on a variation of bi-LSTMs. Zhou and Xu (2015); He et al. (2017) connect the layers of LSTM in an interleaving pattern where in (Wang et al., 2015; Marcheggiani et al., 2017) regular bi-LSTM layers are used. Commonly used features for the encoding layer are: pretrained word embeddings; distance from the predicate; predicate context; predicate region mark or flag; POS tag; and predicate lemma embedding. Only a few of the models (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017) perform dependency-based SRL. Furthermore, all methods focus on languages with rich resources and less morphological complexity like English and Chinese. [Villagers]comers came [to town]end point We use a simple method based on bidirectional LSTMs to train"
P18-1036,P15-1109,0,0.0239997,"l networks have been first introduced to the SRL scene by Collobert et al. (2011), where they use a unified end-to-end convolutional network to perform various NLP tasks. Later, the combination of neural networks (LSTMs in particular) with traditional SRL features (categorical and binary) has been introduced (FitzGerald et al., 2015). Recently, it has been shown that careful design and tuning of deep models can achieve state-of-the-art with no or minimal syntactic knowledge for English and Chinese SRL. Although the architectures vary slightly, they are mostly based on a variation of bi-LSTMs. Zhou and Xu (2015); He et al. (2017) connect the layers of LSTM in an interleaving pattern where in (Wang et al., 2015; Marcheggiani et al., 2017) regular bi-LSTM layers are used. Commonly used features for the encoding layer are: pretrained word embeddings; distance from the predicate; predicate context; predicate region mark or flag; POS tag; and predicate lemma embedding. Only a few of the models (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017) perform dependency-based SRL. Furthermore, all methods focus on languages with rich resources and less morphological complexity like English and Chinese. [Vi"
P18-2072,D17-1156,0,0.0582927,"Missing"
P18-2072,H01-1052,0,0.176018,"ects in a document classification task. We believe this is an important first step in developing methods for estimating the resources required to meet specific engineering performance targets. 1 Introduction An engineering discipline should be able to predict the cost of a project before the project is started. Because training data is often the most expensive part of an NLP or ML project, it is important to estimate how much training data required for a system to achieve a target accuracy. Unfortunately our field only offers fairly impractical advice, e.g., that more data increases accuracy (Banko and Brill, 2001); we currently have no practical methods for estimating how much data or what quality of data is required to achieve a target accuracy goal. Imagine if bridge construction was planned the way we build our systems! 2 Related work Power analysis (Cohen, 1992) is widely-used statistical technique (e.g., in biomedical trials) for predicting the number of measurements required in an experimental design; we aim to develop sim450 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 450–455 c Melbourne, Australia, July 15 - 20, 2018. 2018 Associ"
P19-1238,P07-1032,0,0.03944,"ermine the subcategorization frame of the words they label. For example, the category for a transitive verb is (SNP)/NP, which says that this word must combine with an (object) NP on its right (indicated by the forward slash), which will yield a category which must combine with a second (subject) NP on its left (indicated by the backward slash). In place of movement, CCG uses type raising and function composition rules to capture unbounded long distance dependencies. CCG already has a very well-established research tradition in wide-coverage parsing (see, e.g., Hockenmaier and Steedman 2002; Clark and Curran 2007b; Lewis and Steedman 2014; Xu 2016; Lewis et al. 2016; Wu et al. 2017). A key advancement in CCG parsing that enabled it to become efficient enough to support large-scale NLP tasks was the introduction of Markovian supertagging techniques in Clark and Curran (2007b) that were borrowed from Lexicalised Tree Adjoining Grammar (LTAG; Bangalore and Joshi 1999). Supertagging is essentially just part-of-speech tagging for strongly lexicalised formalisms, which have much larger tagsets than the 50 or so tags used in the PTB. Because the supertags predetermine much of the combinatorics, this is somet"
P19-1238,J07-4004,0,0.0785911,"ermine the subcategorization frame of the words they label. For example, the category for a transitive verb is (SNP)/NP, which says that this word must combine with an (object) NP on its right (indicated by the forward slash), which will yield a category which must combine with a second (subject) NP on its left (indicated by the backward slash). In place of movement, CCG uses type raising and function composition rules to capture unbounded long distance dependencies. CCG already has a very well-established research tradition in wide-coverage parsing (see, e.g., Hockenmaier and Steedman 2002; Clark and Curran 2007b; Lewis and Steedman 2014; Xu 2016; Lewis et al. 2016; Wu et al. 2017). A key advancement in CCG parsing that enabled it to become efficient enough to support large-scale NLP tasks was the introduction of Markovian supertagging techniques in Clark and Curran (2007b) that were borrowed from Lexicalised Tree Adjoining Grammar (LTAG; Bangalore and Joshi 1999). Supertagging is essentially just part-of-speech tagging for strongly lexicalised formalisms, which have much larger tagsets than the 50 or so tags used in the PTB. Because the supertags predetermine much of the combinatorics, this is somet"
P19-1238,P02-1042,1,0.591538,"abelled bi-lexical dependencies for each binary non-terminal in the Xbar phrase structure trees transduced from the MG derivation trees.9 To make up for the short8 This number of tags is closer to the 4727 elementary trees of the TAG treebank of Chen (2001) than to CCGbank’s (Hockenmaier and Steedman, 2007) 1286 lexical categories. 9 As in Collins (1999), the labels are triples of the parent, non-head child and head child categories. The dependencies include both local dependencies and those created by movement, hence this evaluation is more akin to the deep dependency evaluation discussed in Clark et al. (2002) for CCG than to the more standard practice of evaluating parsers in terms of just local dependencies (e.g. Collins 1999). The semantic head of the clause is taken to be the main verb, while its syntactic head, if present, is the overt complemenULAB LAB ULAB LAB Model description syntax 5.1 Experiments semantics 5 model Abstract Reified Abstract Reified Abstract Reified Abstract Reified F1 79.33 80.10 84.57 85.19 74.90 75.47 83.69 84.11 P 81.87 83.43 87.15 88.63 77.17 78.53 86.16 87.47 R 76.94 77.02 82.14 82.02 72.75 72.64 81.36 81.01 E 21.01 21.61 29.59 30.49 20.96 21.56 33.30 34.50 Table 1:"
P19-1238,W04-3215,1,0.788792,"ence lengths and the average curve. The average curve is less informative in very long sentences due to the smaller number of parses, but in regions where there are more data points a clear pattern can be observed: a cubic polynomial curve approximates average time taken to parse sentences extremely well, which means that the expected time complexity of MG 2492 primary motivation for using linguistically expressive parsers in NLP. Wh-object questions themselves are extremely rare in the PTB, but object relative clauses, which also involve unbounded movement, are relatively frequent. Following Clark et al. (2004), we manually evaluated our parser on the free and non-free object (and embedded subject) relative clauses in section 00 of the PTB, as well as on the two examples of so-called tough movement. The MGbank analyses of these constructions are discussed in Appendix B. 10 average 0.00012 n3 minutes 8 6 4 2 0 0 10 20 words 30 40 Figure 2: Parsing speed for Abstract model on test set. parsing with our grammar and statistical model is O(n3 ). This is much better than the worst case analysis, although the variance is high, with some sentences still requiring a very long time to parse. Recently, Stanoje"
P19-1238,P02-1043,1,0.613775,". The functional categories determine the subcategorization frame of the words they label. For example, the category for a transitive verb is (SNP)/NP, which says that this word must combine with an (object) NP on its right (indicated by the forward slash), which will yield a category which must combine with a second (subject) NP on its left (indicated by the backward slash). In place of movement, CCG uses type raising and function composition rules to capture unbounded long distance dependencies. CCG already has a very well-established research tradition in wide-coverage parsing (see, e.g., Hockenmaier and Steedman 2002; Clark and Curran 2007b; Lewis and Steedman 2014; Xu 2016; Lewis et al. 2016; Wu et al. 2017). A key advancement in CCG parsing that enabled it to become efficient enough to support large-scale NLP tasks was the introduction of Markovian supertagging techniques in Clark and Curran (2007b) that were borrowed from Lexicalised Tree Adjoining Grammar (LTAG; Bangalore and Joshi 1999). Supertagging is essentially just part-of-speech tagging for strongly lexicalised formalisms, which have much larger tagsets than the 50 or so tags used in the PTB. Because the supertags predetermine much of the combi"
P19-1238,J07-3004,1,0.651329,"on of the supertags for each word. The parameters are trained using an Adam optimizer with a learning rate of 0.0002. 5.2 Recovering MGBank dependencies We first tested the parser on its ability to recover global syntactic and semantic (local and non-local) dependencies extracted from MGbank. We extracted labelled and unlabelled bi-lexical dependencies for each binary non-terminal in the Xbar phrase structure trees transduced from the MG derivation trees.9 To make up for the short8 This number of tags is closer to the 4727 elementary trees of the TAG treebank of Chen (2001) than to CCGbank’s (Hockenmaier and Steedman, 2007) 1286 lexical categories. 9 As in Collins (1999), the labels are triples of the parent, non-head child and head child categories. The dependencies include both local dependencies and those created by movement, hence this evaluation is more akin to the deep dependency evaluation discussed in Clark et al. (2002) for CCG than to the more standard practice of evaluating parsers in terms of just local dependencies (e.g. Collins 1999). The semantic head of the clause is taken to be the main verb, while its syntactic head, if present, is the overt complemenULAB LAB ULAB LAB Model description syntax 5"
P19-1238,W13-3001,0,0.0507713,"hese parsers unsuitable for implementing MP analyses involving remnant movement (see Stabler 1999). 2.1 MG parsers A number of parsers have been developed for Stablerian MGs, which do allow for actual movement, including remnant movement. What all working MG parsers (Harkema, 2001; Hale, 2003; Stabler, 2013; Stanojevi´c and Stabler, 2018) have until now shared in common is that they are smallscale theoretical implementations equipped only with toy lexicons/grammars. There has been a limited amount of research into probabilistic MGs, notably in generative locally normalised models (Hale, 2003; Hunter and Dyer, 2013). However, these works remain so far untested owing to the unavailability, until very recently, of any MG treebank for training and evaluating models. 2.2 MGbank MGbank (Torr, 2017, 2018) is a treebank of MG derivation trees constructed in part manually by hand-annotating a subset of PTB sentences and in part automatically using a parser equipped with the manually constructed grammar and guided by the corresponding PTB and CCGbank (Hockenmaier and Steedman, 2007) structures. The corpus was continuously machine tested for over- and undergeneration throughout its development. It currently covers"
P19-1238,P02-1018,0,0.102944,"ays the kicker, “both these candidates are named Rudolph Giuliani.”), the licensing of polarity items such as anything, anymore and much by interrogative and negation heads (you have *(not) eaten anything), and the distributional dependency between expletive there and its obligatorily indefinite DP associate (there seem to be some/several/*the/*those problems). All of these long distance dependencies, along with those involved in control, raising, topicalization and wh movement, are integrated into the grammar itself, obviating the need for separate post-processing techniques to recover them (Johnson, 2002; Cahill et al., 2004). The MG lexical categories have also been annotated with over 100 fine-grained selectional and agreement restriction features (e.g. +3SG, -NOM, +INF, MASC, +INDEF, +FOR, MNR, +LOC, etc) to avoid many instances of unwanted overgeneration. Movement is clearly a very powerful operation. However, it is constrained here using many of the locality constraints proposed in the TG literature. These include not only Stabler’s (1997) strict version of the Shortest Move Constraint, but also a partially derelativized version (DSMC) inspired by Rizzi (1990), along with versions of the"
P19-1238,N03-1016,0,0.0916756,"dvancement in CCG parsing that enabled it to become efficient enough to support large-scale NLP tasks was the introduction of Markovian supertagging techniques in Clark and Curran (2007b) that were borrowed from Lexicalised Tree Adjoining Grammar (LTAG; Bangalore and Joshi 1999). Supertagging is essentially just part-of-speech tagging for strongly lexicalised formalisms, which have much larger tagsets than the 50 or so tags used in the PTB. Because the supertags predetermine much of the combinatorics, this is sometimes referred to as ‘almost parsing’. Inspired by the A* algorithm for PCFGs of Klein and Manning (2003), L&S present a simple yet highly effective CCG parsing model which is factored over the probabilities assigned by the lexical supertagger alone, with no explicit model of the derivation at all. This approach is highly efficient and avoids the need for aggressively pruning the search space, which degraded the performance of earlier CKY CCG parsers. Instead, the parser considers the complete distribution of the 425 most commonly occurring CCG lexical categories for each word. The supertagger was originally a unigram log-linear classifier, but Lewis et al. (2016) greatly enhanced its accuracy by"
P19-1238,W08-2315,0,0.210434,"in Figure 1. Therefore, we must redefine ↵ in Equations 2 and 3 to be the set of word indices covered by all the spans contained within an MG expression. The second issue is that, following T&S, the MGbank grammar allows for so-called Acrossthe-Board (ATB) head and phrasal movements in order to capture adjunct control, parasitic gaps, and certain coordination structures. ATB phrasal movement is illustrated in 2 below. (2) Whoi did Jack say Mary likes ti and Pete hates ti ? In 2, who has moved from two separate base generated object positions in across-the-board fashion. T&S (adapting ideas in Kobele 2008) propose to account for this by initially generating 5 See Berwick and Epstein (1995) on the convergence of Minimalist syntax and Categorial Grammar. two instances of who in the two object positions and then later unifying them into a single item when the second conjunct is merged into the main structure. For A*, when two expressions containing unifiable movers are merged together, only one of those movers must contribute to the cost of the resulting expression in order to avoid excessive penalisation for what is now just a single instance of the moving item. We can achieve this for both ATB h"
P19-1238,C90-3084,0,0.700341,"terpretation of mainstream MP that is weakly equivalent to Multiple Context-Free Grammars (MCFG; Seki et al. 1991). The parser itself is an adaptation of a highly efficient A* CCG parsing algorithm (Lewis and Steedman, 2014) with a bi-LSTM model trained on MGbank, an MG version of the English Penn Treebank (PTB; Marcus et al. 1993) currently under development. 2 Background Beginning in the 1960s, a number of parsers were developed which implemented aspects of the various iterations of Chomskyan syntactic theory (e.g. Petrick 1965; Zwicky et al. 1965; Woods 1970, 1973; Plath 1973; Marcus 1980; Kuhns 1990; Fong 1991; Stabler 1992; Fong and Ginsburg 2012), but most of these systems operated over relatively closed domains and were never evaluated against wide-coverage treebank test data. Principar (Lin, 1993), and its descendant Minipar (Lin, 1998, 2001), are the only truly widecoverage parsers in the Chomskyan tradition of which we are aware. Minipar incorporates MP’s bare phrase structure and some of its economy principles. It is also statistical, having been selftrained on a 1GB corpus. However, while these parsers model the phrase structure and locality constraints of TG, they are not transf"
P19-1238,N16-1026,0,0.116046,"s the semantic AGENT of eat; in 1b, meanwhile, it moves from the deep object position and so is interpreted instead as the semantic PATIENT of eat. Minimalist Grammars (Stabler, 1997) are a computationally oriented, and rigorous formalisation of many aspects of Chomsky’s (1995) Minimalist Program. This paper presents the first ever application of this formalism to the task of realistic wide-coverage parsing. The parser uses a linguistically expressive yet highly constrained grammar, together with an adaptation of the A* search algorithm currently used in CCG parsing (Lewis and Steedman, 2014; Lewis et al., 2016), with supertag probabilities provided by a bi-LSTM neural network supertagger trained on MGbank, a corpus of MG derivation trees. We report on some promising initial experimental results for overall dependency recovery as well as on the recovery of certain unbounded long distance dependencies. Finally, although like other MG parsers, ours has a high order polynomial worst case time complexity, we show that in practice its expected time complexity is O(n3 ). The parser is publicly available.1 1 (1) Introduction a. Whati do you think ti eats mice? b. Whati do you think mice eat ti ? Parsers bas"
P19-1238,D14-1107,1,0.908946,"therefore be interpreted as the semantic AGENT of eat; in 1b, meanwhile, it moves from the deep object position and so is interpreted instead as the semantic PATIENT of eat. Minimalist Grammars (Stabler, 1997) are a computationally oriented, and rigorous formalisation of many aspects of Chomsky’s (1995) Minimalist Program. This paper presents the first ever application of this formalism to the task of realistic wide-coverage parsing. The parser uses a linguistically expressive yet highly constrained grammar, together with an adaptation of the A* search algorithm currently used in CCG parsing (Lewis and Steedman, 2014; Lewis et al., 2016), with supertag probabilities provided by a bi-LSTM neural network supertagger trained on MGbank, a corpus of MG derivation trees. We report on some promising initial experimental results for overall dependency recovery as well as on the recovery of certain unbounded long distance dependencies. Finally, although like other MG parsers, ours has a high order polynomial worst case time complexity, we show that in practice its expected time complexity is O(n3 ). The parser is publicly available.1 1 (1) Introduction a. Whati do you think ti eats mice? b. Whati do you think mice"
P19-1238,P93-1016,0,0.584734,"nd Steedman, 2014) with a bi-LSTM model trained on MGbank, an MG version of the English Penn Treebank (PTB; Marcus et al. 1993) currently under development. 2 Background Beginning in the 1960s, a number of parsers were developed which implemented aspects of the various iterations of Chomskyan syntactic theory (e.g. Petrick 1965; Zwicky et al. 1965; Woods 1970, 1973; Plath 1973; Marcus 1980; Kuhns 1990; Fong 1991; Stabler 1992; Fong and Ginsburg 2012), but most of these systems operated over relatively closed domains and were never evaluated against wide-coverage treebank test data. Principar (Lin, 1993), and its descendant Minipar (Lin, 1998, 2001), are the only truly widecoverage parsers in the Chomskyan tradition of which we are aware. Minipar incorporates MP’s bare phrase structure and some of its economy principles. It is also statistical, having been selftrained on a 1GB corpus. However, while these parsers model the phrase structure and locality constraints of TG, they are not transformational: movement is merely ‘simulat[ed]’ (Lin, 1993, page 116) by passing features up a precompiled network of nodes representing a tree, from the site of the trace to the site of the antecedent, with t"
P19-1238,H01-1046,0,0.0216294,"Missing"
P19-1238,W12-4615,0,0.0668432,"Missing"
P19-1238,W09-0104,0,0.0314966,"obele et al., 2013; Stabler, 2013; Graf and Marcinek, 2014; Graf et al., 2015; Gerth, 2015; Stanojevi´c and Stabler, 2018). On the other hand, TG has enjoyed far less popularity within computational linguistics more generally,2 which is unfortunate given that it is arguably the most extensively developed syntactic theory across the greatest number of languages, many of which are otherwise under-resourced. Conversely, the process of constructing large grammar fragments and 1 https://github.com/mgparsing/astar_ mg_parser 2 For an anti-Chomskyan perspective on why this disconnect came about, see Pullum (2009). 2486 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2486–2505 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics subjecting these to computational testing can have a salutary impact on the syntactic theory itself, forcing choices between competing analyses of the same construction, and exposing incompatibilities between analyses of different constructions, along with areas of over/undergeneration which may otherwise go unnoticed (Bierwisch 1963; Abney 1996; both cited in M¨uller 2016). The received wisdo"
P19-1238,1985.tmi-1.17,0,0.326254,". Whati do you think mice eat ti ? Parsers based on linguistically expressive formalisms, such as Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag 1994) and Combinatory Categorial Grammar (CCG; Steedman 1996), were shown in Rimell et al. (2009) and Nivre et al. (2010) to be more effective at recovering certain unbounded long-distance dependencies than those merely approximating human grammar with finite state or context-free covers. Such dependencies can be vital for tasks like open domain question answering, for example. Furthermore, as proven independently by Huybregts (1984) and Shieber (1985), some languages exhibit constructions which put them beyond even MP continues to dominate much of theoretical syntax, and Stabler’s (1997) rigorous formalisation of this framework has proven a popular choice for investigations into human sentence processing (Hale, 2003; Kobele et al., 2013; Stabler, 2013; Graf and Marcinek, 2014; Graf et al., 2015; Gerth, 2015; Stanojevi´c and Stabler, 2018). On the other hand, TG has enjoyed far less popularity within computational linguistics more generally,2 which is unfortunate given that it is arguably the most extensively developed syntactic theory acro"
P19-1238,D09-1085,1,0.746176,"or overall dependency recovery as well as on the recovery of certain unbounded long distance dependencies. Finally, although like other MG parsers, ours has a high order polynomial worst case time complexity, we show that in practice its expected time complexity is O(n3 ). The parser is publicly available.1 1 (1) Introduction a. Whati do you think ti eats mice? b. Whati do you think mice eat ti ? Parsers based on linguistically expressive formalisms, such as Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag 1994) and Combinatory Categorial Grammar (CCG; Steedman 1996), were shown in Rimell et al. (2009) and Nivre et al. (2010) to be more effective at recovering certain unbounded long-distance dependencies than those merely approximating human grammar with finite state or context-free covers. Such dependencies can be vital for tasks like open domain question answering, for example. Furthermore, as proven independently by Huybregts (1984) and Shieber (1985), some languages exhibit constructions which put them beyond even MP continues to dominate much of theoretical syntax, and Stabler’s (1997) rigorous formalisation of this framework has proven a popular choice for investigations into human se"
P19-1238,E17-3021,1,0.887977,"Missing"
P19-1468,J15-2003,0,0.306359,"taset. For the link prediction task, we compare the ConvE model with our proposed link prediction score. We test how MC and Aug MC entailment scores can improve the link prediction scores in both local and global settings. 5 Results and Discussion We first compare our proposed entailment score with the previous state-of-the-art results (§5.1) and then show that we can use entailment decisions to improve the link prediction task (§5.2). 9 Higher values of K was not feasible on our machines. We performed our experiments on a 32-core 2.3 GHz machine with 256GB of RAM. 10 The entailment graphs of Berant et al. (2015) yield similar results. 5.1 Entailment Scores based on Link Prediction In this section, we compare the variants of our method to the previous state-of-the-art results on the Levy/Holt’s dataset. We compute similarity scores and report precision-recall curve by changing the threshold for entailment between 0 and 1. In order to have a fair comparison with Berant’s ILP method, we first test a set of rule-based constraints proposed by them (Berant et al., 2011). We also apply the lemma baseline heuristic process of Levy and Dagan (2016) before testing the methods. Figure 3 shows the precision-reca"
P19-1468,P12-1013,0,0.0227334,"Missing"
P19-1468,P11-1062,0,0.162317,"how improvements over the raw link prediction scores. ele cte dp Abstract (B) run for presidency of be elected president of be nominated for presidency of Introduction Figure 1: A link prediction knowledge graph (A) and an Link prediction and entailment graph induction are often treated as different problems. The former (Figure 1A) is used to infer missing relations between entities in existing knowledge graphs (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013). The latter (Figure 1B) constructs entailment graphs with relations as nodes and entailment rules as edges between them (Berant et al., 2011, 2015; Hosseini et al., 2018) for the task of answering questions from text. In this paper, we show that these two problems are complementary by demonstrating how link prediction can help identify entailments and how discovered entailments can help predict missing links. Methods to learn entailment graphs (Berant et al., 2011, 2015; Hosseini et al., 2018) process large text corpora to find local entailment scores between relations based on the Distributional Inclusion Hypothesis which states that a word (relation) r entails another word (relation) q if and only if in any context that r can be"
P19-1468,D16-1146,0,0.0607028,"Missing"
P19-1468,P18-1011,0,0.112941,"ity scores for grounded logical rules as well as triples and learning entity and relation embeddings that score positive examples higher than negative ones. Guo et al. (2018) take an iterative approach where in each iteration a set of unseen triples are scored according to the current link prediction model and a small set of precomputed logical rules. The new triples and their scores are then used to update the current link prediction model. The above models need grounding of logical rules. A few recent works do not need grounding and are more space and time efficient (Demeester et al., 2016; Ding et al., 2018). They incorporate logical rules into distributed representations of relations. These models constrain entity or entity-pair vector representations to be nonnegative. They encourage partial ordering over relation embeddings based on implication rules; however, their methods can be only applied to (multi-)linear link prediction models such as ComplEx (Trouillon et al., 2016). In contrast, our method can be applied to any type of link prediction model. All these methods require entailment rules as their input. In most cases (Wang et al., 2015; Demeester et al., 2016; Guo et al., 2016), the entai"
P19-1468,P05-1014,0,0.431468,"g questions from text. In this paper, we show that these two problems are complementary by demonstrating how link prediction can help identify entailments and how discovered entailments can help predict missing links. Methods to learn entailment graphs (Berant et al., 2011, 2015; Hosseini et al., 2018) process large text corpora to find local entailment scores between relations based on the Distributional Inclusion Hypothesis which states that a word (relation) r entails another word (relation) q if and only if in any context that r can be used, q can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Kartsaklis and Sadrzadeh, 2016). They entailment graph (B) for entities of types politician,country. The solid lines are discovered correctly, but the dashed ones are missing. However, evidence from the link prediction model can be used to add the missing entailment rule in the entailment graph (B). Similarly, the entailment graph can be used to add the missing link in the knowledge graph (A). use types such as person, location and time, to disambiguate polysemous relations (e.g., person born in location and person born in time). Entailment graphs are then formed by imposing global constrain"
P19-1468,D16-1019,0,0.142714,"same patterns of entailments. Our method, in contrast, learns a new entailment score to improve local decisions, which in turn improves the entailment graphs. Entailment Rule Injection for link prediction. There are some attempts in recent years to improve link prediction by injecting entailment rules. Wang et al. (2015) incorporate various set of heuristic rules, including entailment rules, into embedding models for knowledge base completion. They formulate inference as an ILP problem, with the objective function generated from embeddings models and the constraints translated from the rules. Guo et al. (2016) extend the TransE model by defining plausibility scores for grounded logical rules as well as triples and learning entity and relation embeddings that score positive examples higher than negative ones. Guo et al. (2018) take an iterative approach where in each iteration a set of unseen triples are scored according to the current link prediction model and a small set of precomputed logical rules. The new triples and their scores are then used to update the current link prediction model. The above models need grounding of logical rules. A few recent works do not need grounding and are more spac"
P19-1468,C16-1268,0,0.26025,"Missing"
P19-1468,P16-2041,0,0.210562,"2.3 GHz machine with 256GB of RAM. 10 The entailment graphs of Berant et al. (2015) yield similar results. 5.1 Entailment Scores based on Link Prediction In this section, we compare the variants of our method to the previous state-of-the-art results on the Levy/Holt’s dataset. We compute similarity scores and report precision-recall curve by changing the threshold for entailment between 0 and 1. In order to have a fair comparison with Berant’s ILP method, we first test a set of rule-based constraints proposed by them (Berant et al., 2011). We also apply the lemma baseline heuristic process of Levy and Dagan (2016) before testing the methods. Figure 3 shows the precision-recall curve of all the methods in both local (A) and global (B) settings. From the SBOW methods, we only show the BInc score in the graphs as it got the best results on the development set. For Berant’s ILP method, we only have one point of precision and recall, as we had access to their entailment graphs for only one sparsity level. In both settings, Aug MC works better than all the other methods. This confirms that the link prediction method is indeed useful for finding entailment relations. Aug MC consistently outperforms MC suggest"
P19-1468,D14-1107,1,0.76789,"y of the triple being correct. We de2 note by S ∈ [0, 1]|R|×|E |the matrix containing triple probability scores. We define S(t1 , t2 ) ∈ 2 [0, 1]|R(t1 ,t2 )|×|E (t1 ,t2 ) |the submatrix of S with R(t1 , t2 ) as rows and E 2 (t1 , t2 ) as columns. We apply a link prediction model to a knowledge graph of predicate-argument structures extracted from text (§4.2). 2.2 Entailment Prediction The goal is to find entailment scores between all relations with the same types, where the 4737 3 For example by applying the Sigmoid function. entities can be in the same or opposite order (Berant et al., 2011; Lewis and Steedman, 2014b; Hosseini et al., 2018). We denote by W (t1 , t2 ) ∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2"
P19-1468,W14-2406,1,0.906373,"y of the triple being correct. We de2 note by S ∈ [0, 1]|R|×|E |the matrix containing triple probability scores. We define S(t1 , t2 ) ∈ 2 [0, 1]|R(t1 ,t2 )|×|E (t1 ,t2 ) |the submatrix of S with R(t1 , t2 ) as rows and E 2 (t1 , t2 ) as columns. We apply a link prediction model to a knowledge graph of predicate-argument structures extracted from text (§4.2). 2.2 Entailment Prediction The goal is to find entailment scores between all relations with the same types, where the 4737 3 For example by applying the Sigmoid function. entities can be in the same or opposite order (Berant et al., 2011; Lewis and Steedman, 2014b; Hosseini et al., 2018). We denote by W (t1 , t2 ) ∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2"
P19-1468,P98-2127,0,0.0642417,"et al., 2018). We denote by W (t1 , t2 ) ∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2003), Lin (Lin, 1998), and Balanced Inclusion (BInc; Szpektor and Dagan, 2008) are typically defined on feature vectors consisting of entitypairs (e.g., Obama-Hawaii), where the values are frequencies or pointwise mutual information (PMI) between the relations and the features (Berant et al., 2011, 2012, 2015). While these methods currently hold state-of-the-art results on relation entailment datasets (Hosseini et al., 2018), they suffer from low recall because the feature vectors are usually sparse and do not have high overlap with each other. The link prediction models, on the other hand, can predict the probabi"
P19-1468,N19-1226,0,0.0219182,"28.43 Table 2: Link prediction results on the test set of NewsSpike for all entities (top) and infrequent entities (below). We test the effect of refining ConvE scores with entailment relations. scores (>0.95). 6 Related Work Link Prediction. In recent years, many link prediction models have been proposed that learn vector or matrix representations for relations and entities (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013; Wang et al., 2014; Lin et al., 2015; Toutanova et al., 2016; Nguyen et al., 2016; Trouillon et al., 2016; Dettmers et al., 2018; Schlichtkrull et al., 2018; Nguyen et al., 2019). These models are trained by assigning higher plausibility scores to correct facts than incorrect ones. For example, the well-known TransE model (Bordes et al., 2013) captures relational similarity between entity pairs by considering a translation vector for the relations connecting them. In particular, it learns embeddings for entities and relations such that e~2 − e~1 ≈ ~r for any correct triple (r, e1 , e2 ). In our experiments we have used ConvE (Dettmers et al., 2018), however, our proposed score can be computed based on any link prediction model and the discovered entailment relations m"
P19-1468,N16-1054,1,0.846058,"9.26 46.10 1303.56 28.25 19.30 46.36 1154.06 28.33 19.29 46.60 1154.28 28.41 19.28 46.66 1118.09 28.43 Table 2: Link prediction results on the test set of NewsSpike for all entities (top) and infrequent entities (below). We test the effect of refining ConvE scores with entailment relations. scores (>0.95). 6 Related Work Link Prediction. In recent years, many link prediction models have been proposed that learn vector or matrix representations for relations and entities (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013; Wang et al., 2014; Lin et al., 2015; Toutanova et al., 2016; Nguyen et al., 2016; Trouillon et al., 2016; Dettmers et al., 2018; Schlichtkrull et al., 2018; Nguyen et al., 2019). These models are trained by assigning higher plausibility scores to correct facts than incorrect ones. For example, the well-known TransE model (Bordes et al., 2013) captures relational similarity between entity pairs by considering a translation vector for the relations connecting them. In particular, it learns embeddings for entities and relations such that e~2 − e~1 ≈ ~r for any correct triple (r, e1 , e2 ). In our experiments we have used ConvE (Dettmers et al., 2018), however, our proposed s"
P19-1468,Q14-1030,1,0.839153,"nek et al., 2007); however, we chose to experiment on assertions extracted from raw text. This is because we can then evaluate the predicted entailments on existing entailment datasets with examples stated in natural language (§4.3). We use the multiple-source NewsSpike corpus of Zhang and Weld (2013). The NewsSpike corpus includes 550K news articles and is well-suited for finding entailment and paraphrasing relations as it includes different articles from different sources describing identical news stories. We use the triples released by Hosseini et al. (2018)6 who run the semantic parser of Reddy et al. (2014), GraphParser, to extract binary relations between a predicate and its arguments. GraphParser uses Combinatorial Categorial Grammer (CCG) syntactic derivations by running EasyCCG (Lewis and Steedman, 2014a). The parser converts sentences to neo-Davisonian semantics, a first order logic that uses event identifiers and extracts one binary relation for each event and pair of arguments (Parsons, 1990). The entities are typed by first linking to Freebase (Bollacker et al., 2008) and then selecting the most notable type of the entity from Freebase and mapping it to FIGER types (Ling 4739 6 Accessed"
P19-1468,N13-1008,0,0.475509,"her. The new entailment score outperforms prior state-of-the-art results on a standard entialment dataset and the new link prediction scores show improvements over the raw link prediction scores. ele cte dp Abstract (B) run for presidency of be elected president of be nominated for presidency of Introduction Figure 1: A link prediction knowledge graph (A) and an Link prediction and entailment graph induction are often treated as different problems. The former (Figure 1A) is used to infer missing relations between entities in existing knowledge graphs (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013). The latter (Figure 1B) constructs entailment graphs with relations as nodes and entailment rules as edges between them (Berant et al., 2011, 2015; Hosseini et al., 2018) for the task of answering questions from text. In this paper, we show that these two problems are complementary by demonstrating how link prediction can help identify entailments and how discovered entailments can help predict missing links. Methods to learn entailment graphs (Berant et al., 2011, 2015; Hosseini et al., 2018) process large text corpora to find local entailment scores between relations based on the Distributi"
P19-1468,C08-1107,0,0.904373,"∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2003), Lin (Lin, 1998), and Balanced Inclusion (BInc; Szpektor and Dagan, 2008) are typically defined on feature vectors consisting of entitypairs (e.g., Obama-Hawaii), where the values are frequencies or pointwise mutual information (PMI) between the relations and the features (Berant et al., 2011, 2012, 2015). While these methods currently hold state-of-the-art results on relation entailment datasets (Hosseini et al., 2018), they suffer from low recall because the feature vectors are usually sparse and do not have high overlap with each other. The link prediction models, on the other hand, can predict the probability of any triple being in the knowledge graph. Using pr"
P19-1468,P16-1136,0,0.379252,"elations that entail each other in both directions are regarded as paraphrases. 4736 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4736–4746 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics near can be used to answer such questions. 2 On the other hand, link prediction (or knowledge base completion) models are based on distributional methods and directly predict the source data. These models have received much attention in the recent years (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013; Toutanova et al., 2016; Trouillon et al., 2016; Dettmers et al., 2018). The current methods learn embeddings for all entities and relations and a function to score any potential relation between the entities. One of the main capabilities of these models is that they implicitly exploit entailment relations such as person born in country entails person be from country (Riedel et al., 2013). However, entailment relations are not learned explicitly. For example, we cannot simply compute the cosine similarity of the vector representations of the two relations to detect the entailment between them, because cosine similar"
P19-1468,W03-1011,0,0.830383,"nd Steedman, 2014b; Hosseini et al., 2018). We denote by W (t1 , t2 ) ∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2003), Lin (Lin, 1998), and Balanced Inclusion (BInc; Szpektor and Dagan, 2008) are typically defined on feature vectors consisting of entitypairs (e.g., Obama-Hawaii), where the values are frequencies or pointwise mutual information (PMI) between the relations and the features (Berant et al., 2011, 2012, 2015). While these methods currently hold state-of-the-art results on relation entailment datasets (Hosseini et al., 2018), they suffer from low recall because the feature vectors are usually sparse and do not have high overlap with each other. The link prediction models, on the other hand, can pr"
P19-1468,D13-1183,0,0.342652,"We then describe the details of the link prediction model (§4.2), the datasets used to test the models (§4.3) and the baseline systems (§4.4). 4.1 Text Corpus Link prediction models are often applied to existing knowledge graphs such as Freebase (Bollacker et al., 2008), DBPedia (Lehmann et al., 2015) and Yago (Suchanek et al., 2007); however, we chose to experiment on assertions extracted from raw text. This is because we can then evaluate the predicted entailments on existing entailment datasets with examples stated in natural language (§4.3). We use the multiple-source NewsSpike corpus of Zhang and Weld (2013). The NewsSpike corpus includes 550K news articles and is well-suited for finding entailment and paraphrasing relations as it includes different articles from different sources describing identical news stories. We use the triples released by Hosseini et al. (2018)6 who run the semantic parser of Reddy et al. (2014), GraphParser, to extract binary relations between a predicate and its arguments. GraphParser uses Combinatorial Categorial Grammer (CCG) syntactic derivations by running EasyCCG (Lewis and Steedman, 2014a). The parser converts sentences to neo-Davisonian semantics, a first order lo"
P87-1001,P87-1003,0,\N,Missing
P87-1001,P87-1021,0,\N,Missing
P87-1012,P84-1027,0,0.0475015,"Missing"
P87-1012,C86-1045,0,0.0535296,"r papers (Ades and Steedman 1982, Pareschi 1986) that this property can be exploited for incremental semantic interpretation and evaluation, a suggestion which has been explored further by Haddock (1987) and Hinrichs and Polanyi (1986), two potentially serious problems arise from these spurious ambiguities. The fast is the possibility of producing a whole set of semantically equivalent analyses for each reading of a given siring. The second more serious problem is that of efficiently coping with nondeterminism in the face of such proliferating ambiguity in surface analyses. As Kamunen (1986), Uszkoreit (1986), Wittenburg (1986), and Zeevat et al. (1986) have noted, unification-based computational enviroments (Shieber 1986) offer a natural choice for implementing the categories and combination roles of CGs, because of their rigorously dermed declarative semantics. We describe below a unification-besed realisation of CCG which is both transparent to the linguistically motivated properties of the theory of granu&apos;nar and can be directly coupled to the parsing methodology we offer further on. (i) a constant The problem of avoiding equivalent derivations is common to parsers of all grammars, even contex"
P90-1002,P90-1001,0,0.0140365,"h subsume the intonational structures that are postulated by Pierrehumbert et al. to explain the possible intonation contours for sentences of English. More specifically, the claim is that that in spoken utterance, intonation helps to determine which of the many possible bracketings permitted by the combinatory syntax of English is intended, and that the interpretations of the constituents that arise from these derivations, far from being ""spurious"", are related to distinctions of discourse focus among the concepts and open propositions that the speaker has in mind. prefers Mary . . (See [9], [18] and [19] for a discussion of the obvious problems for parsing written text that the presence of such ""spurious"" (i.e. semantically equivalent) derivations engenders, and for some ways they might be overcome.) An entirely unconstrained combinatory grammar would in fact allow any bracketing on a sentence, although the grammars we actually write for configurational languages like English are heavily constrained by local conditions. (An example might be a condition on the composition rule that is tacitly assmned below, forbidding the variable Y in the composition rule to be instantiated as NP, th"
P90-1002,P87-1011,0,0.0135666,"the intonational structures that are postulated by Pierrehumbert et al. to explain the possible intonation contours for sentences of English. More specifically, the claim is that that in spoken utterance, intonation helps to determine which of the many possible bracketings permitted by the combinatory syntax of English is intended, and that the interpretations of the constituents that arise from these derivations, far from being ""spurious"", are related to distinctions of discourse focus among the concepts and open propositions that the speaker has in mind. prefers Mary . . (See [9], [18] and [19] for a discussion of the obvious problems for parsing written text that the presence of such ""spurious"" (i.e. semantically equivalent) derivations engenders, and for some ways they might be overcome.) An entirely unconstrained combinatory grammar would in fact allow any bracketing on a sentence, although the grammars we actually write for configurational languages like English are heavily constrained by local conditions. (An example might be a condition on the composition rule that is tacitly assmned below, forbidding the variable Y in the composition rule to be instantiated as NP, thus exclud"
P90-1002,P83-1007,0,0.0163427,"Missing"
P90-1002,P87-1012,1,0.901227,"nglish subsume the intonational structures that are postulated by Pierrehumbert et al. to explain the possible intonation contours for sentences of English. More specifically, the claim is that that in spoken utterance, intonation helps to determine which of the many possible bracketings permitted by the combinatory syntax of English is intended, and that the interpretations of the constituents that arise from these derivations, far from being ""spurious"", are related to distinctions of discourse focus among the concepts and open propositions that the speaker has in mind. prefers Mary . . (See [9], [18] and [19] for a discussion of the obvious problems for parsing written text that the presence of such ""spurious"" (i.e. semantically equivalent) derivations engenders, and for some ways they might be overcome.) An entirely unconstrained combinatory grammar would in fact allow any bracketing on a sentence, although the grammars we actually write for configurational languages like English are heavily constrained by local conditions. (An example might be a condition on the composition rule that is tacitly assmned below, forbidding the variable Y in the composition rule to be instantiated as"
P91-1010,C90-2025,0,0.149535,"Missing"
P91-1010,P90-1001,0,0.162399,"Missing"
P99-1039,T87-1035,0,0.0177248,"e paper will argue that arbitrary objects so interpreted are a necessary element of the ontology for natural language semantics, and that their involvement in CCG explains not only scope alternation (including occasions on which scope alternation is not available), but also certain cases of anomalous scopal binding which are unexplained under any of the alternatives discussed so far. 2 Donkeys as Skolem Terms One example of an indefinite that is probably better analyzed as an arbitrary object than as a quantified NP occurs in the following famous sentence, first brought to modern attention by Geach (1962): (6) Every farmer who owns a donkey/beats it/. The pronoun looks as though it might be a variable bound by an existential quantifier associated with a donkey. However, no purely combinatoric analysis in terms of the generalized quantifier categories offered earlier allows this, since the existential cannot both remain within the scope of the universal, and come to c-command the pronoun, as is required for true bound pronominal anaphora, as in: (7) Every farmer/in the room thinks that she/deserves a subsidy One popular reaction to this observation has been to try to generalize the notion of sc"
P99-1039,J87-1005,0,0.0783873,"trictions. However, the restriction that Geach noted seems intrinsically disjunctive, and hence appears to threaten efficiency in both parsing with, and disambiguation of, under-specified representations. The fact that relatively few readings are available and that they are so tightly related to surface structure and derivation means that the technique of incremental semantic or probabilistic disambiguation of fully specified partial logical forms mentioned earlier may be a more efficient technique for computing the contextually relevant readings. For example, in processing (22) (adapted from Hobbs and Shieber 1987), which Park 1995 claims to have only four readings, rather than the five predicted by their account, such a system can build both readings for the S/NP every representative of three companies saw and decide which is more likely, before building both compatible readings of the whole sentence and similarly resolving with respect to statistical or contextual support: (22) Every representative of three companies saw some sample. 5 Conclusion The above observations imply that only those socalled quantifiers in English which can engender dependency-inducing scope inversion have interpretations corr"
P99-1039,P95-1028,0,0.326603,"Alternating Quantifier Scope in CCG* Mark Steedman Division of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, U K steedman@cogsc i. ed. a c . uk Abstract The paper shows that movement or equivalent computational structure-changing operations of any kind at the level of logical form can be dispensed with entirely in capturing quantifer scope ambiguity. It offers a new semantics whereby the effects of quantifier scope alternation can be obtained by an entirely monotonic derivation, without typechanging rules. The paper follows Fodor (1982), Fodor and Sag (1982), and Park (1995, 1996) in viewing many apparent scope ambiguities as arising from referential categories rather than true generalized quantitiers. 1 Introduction It is standard to assume that the ambiguity of sentences like (1) is to be accounted for by assigning two logical forms which differ in the scopes assigned to these quantifiers, as in (2a,b): 1 (1) Every boy admires some saxophonist. (2) a. Vx.boy' x -+ 3y.saxophonis/ y A admires' yx b. 3y.saxophonis/ y A Vx.bo/x -+ admires'yx The question then arises of how a grammar/parser can assign all and only the correct interpretations to sentences with multi"
P99-1039,J90-1001,0,0.0410323,"Missing"
P99-1039,P99-1038,0,0.0244321,"2! • 1 = 2 readings. Bayer (1996) and Kayne (1998) have noted related 306 restrictions on scope alternation that would otherwise be allowed for arguments that are marooned in mid verb-group in German. Since such embeddings are crucial to obtaining proliferating readings, it is likely that in practice the number of available readings is usually quite small. It is interesting to speculate finally on the relation of the above account of the available scope readings with proposals to minimize search during processing by building &quot;underspecified&quot; logical forms by Reyle (1992), and others cited in Willis and Manandhar (1999). There is a sense in which arbitrary individuals are themselves under-specified quantitiers, which are disambiguated by Skolemization. However, under the present proposal, they are disambiguated during the derivation itself. The alternative of building a single underspecified logical form can under some circumstances dramatically reduce the search space and increase efficiency of parsing--for example with distributive expressions in sentences like Six girls ate .five pizzas, which are probably intrinsically unspecified. However, few studies of this kind have looked at the problems posed by th"
P99-1039,E95-1001,0,\N,Missing
Q13-1015,P98-1013,0,0.0212907,"Work Much work on semantics has taken place in a supervised setting—for example the GeoQuery (Zelle and Mooney, 1996) and ATIS (Dahl et al., 1994) semantic parsing tasks. This approach makes sense for generating queries for a specific database, but means the semantic representations do not generalize to other datasets. There have been several attempts to annotate larger corpora with semantics—such as Ontonotes (Hovy et al., 2006) or the Groningen Meaning Bank (Basile et al., 2012). These typically map words onto senses in ontologies such as WordNet, VerbNet (Kipper et al., 2000) and FrameNet (Baker et al., 1998). However, limitations of these ontologies mean that they do not support inferences such as X is the author of Y → X wrote Y. Given the difficulty of annotating large amounts of text with semantics, various approaches have attempted to learn meaning without annotated text. Distant Supervision approaches leverage existing knowledge bases, such as Freebase (Bollacker et al., 2008), to learn semantics (Mintz et al., 2009; Krishnamurthy and Mitchell, 2012). Dependency-based Compositional Semantics (Liang et al., 2011) learns the meaning of questions by using their answers as denotations—but this a"
Q13-1015,E12-1004,0,0.0440056,"Recent work has tackled the problem of composing these matrices to build up the semantics of phrases or sentences (Mitchell and Lapata, 2008). Another strand (Coecke et al., 2010; Grefenstette et al., 2011) has shown how to represent meanings as tensors, whose order depends on the syntactic category, allowing an elegant correspondence between syntactic and semantic types. Socher et al. (2012) train a composition function using a neural network—however their method requires annotated data. It is also not obvious how to represent logical relations such as quantification in vector-space models. Baroni et al. (2012) make progress towards this by learning a classifier that can recognise entailments such as all dogs =⇒ some dogs, but this remains some way from the power of first-order theorem proving of the kind required by the problem in Figure 5. An alternative strand of research has attempted to build computational models of linguistic theories based on formal compositional semantics, such as the CCG-based Boxer (Bos, 2008) and the LFGbased XLE (Bobrow et al., 2007). Such approaches convert parser output into formal semantic representations, and have demonstrated some ability to model complex phenomena"
Q13-1015,basile-etal-2012-developing,0,0.0147464,"on distributional semantics seem unable to tackle any of these problems, as they do not represent quantifiers or negation. 187 Related Work Much work on semantics has taken place in a supervised setting—for example the GeoQuery (Zelle and Mooney, 1996) and ATIS (Dahl et al., 1994) semantic parsing tasks. This approach makes sense for generating queries for a specific database, but means the semantic representations do not generalize to other datasets. There have been several attempts to annotate larger corpora with semantics—such as Ontonotes (Hovy et al., 2006) or the Groningen Meaning Bank (Basile et al., 2012). These typically map words onto senses in ontologies such as WordNet, VerbNet (Kipper et al., 2000) and FrameNet (Baker et al., 1998). However, limitations of these ontologies mean that they do not support inferences such as X is the author of Y → X wrote Y. Given the difficulty of annotating large amounts of text with semantics, various approaches have attempted to learn meaning without annotated text. Distant Supervision approaches leverage existing knowledge bases, such as Freebase (Bollacker et al., 2008), to learn semantics (Mintz et al., 2009; Krishnamurthy and Mitchell, 2012). Dependen"
Q13-1015,P11-1062,0,0.0719556,"o the standard CCG derivation is that the symbols used in the logical form are arbitrary relation identifiers. We learn these by first mapping to a deterministic logical form (using predicates such as author’ and write’), using a process similar to Boxer, and then clustering predicates based on their arguments. This lexicon can then be used to parse new sentences, and integrates seamlessly with CCG theories of formal semantics. Typing predicates—for example, determining that writing is a relation between people and books— has become standard in relation clustering (Schoenmackers et al., 2010; Berant et al., 2011; Yao et al., 2012). We demonstate how to build a typing model into the CCG derivation, by subcategorizing all terms representing entities in the logical form with a more detailed type. These types are also induced from text, as explained in Section 5, but for convenience we describe them with human-readable labels, such as PER, LOC and BOOK. A key advantage of typing is that it allows us to model ambiguous predicates. Following Berant et al. (2011), we assume that different type signatures of the same predicate have different meanings, but given a type signature a predicate is unambiguous. Fo"
Q13-1015,W06-3812,0,0.0322607,"C and bornarg0:PER,argIn:DAT are then incremented by these probabilities. 6.2 Clustering Many algorithms have been proposed for clustering predicates based on their arguments (Poon and Domingos, 2009; Yao et al., 2012). The number of relations in the corpus is unbounded, so the clustering algorithm should be non-parametric. It is also important that it remains tractable for very large numbers of predicates and arguments, in order to give us a greater coverage of language than can be achieved by hand-built ontologies. We cluster the typed predicate vectors using the Chinese Whispers algorithm (Biemann, 2006)— although somewhat ad-hoc, it is both non-parametric and highly scalable5. This has previously been used for noun-clustering by Fountain and Lapata (2011), who argue it is a cognitively plausible model for language acquisition. The collection of predicates and arguments is converted into a graph with one node per predicate, and edge weights representing the similarity between predicates. Predicates with different types have zero-similarity, and otherwise similarity is computed as the cosine-similarity of the tf-idf vectors of argument-pairs. We prune nodes occurring fewer than 20 times, edges"
Q13-1015,W07-1403,0,0.0605462,"Missing"
Q13-1015,H05-1079,0,0.0960971,"it is less clear that it can be applied to the meanings of function words. Semantic operators, such as determiners, negation, conjunctions, modals, tense, mood, aspect, and plurals are ubiquitous in natural language, and are crucial for high performance on many practical applications— but current distributional models struggle to capture even simple examples. Conversely, computational models of formal semantics have shown low recall on practical applications, stemming from their reliance on ontologies such as WordNet (Miller, 1995) to model the meanings of content words (Bobrow et al., 2007; Bos and Markert, 2005). For example, consider what is needed to answer a question like Did Google buy YouTube? from the following sentences: We introduce a new approach to semantics which combines the benefits of distributional and formal logical semantics. Distributional models have been successful in modelling the meanings of content words, but logical semantics is necessary to adequately represent many function words. We follow formal semantics in mapping language to logical representations, but differ in that the relational constants used are induced by offline distributional clustering at the level of predicat"
Q13-1015,W08-2222,0,0.422755,"bine with, and a semantic interpretation, which defines the compositional semantics. For example, the lexicon may contain the entry: write ` (SNP)/NP : λ yλ x.write0 (x, y) Crucially, there is a transparent interface between the syntactic category and the semantics. For example the transitive verb entry above defines the verb syntactically as a function mapping two nounphrases to a sentence, and semantically as a binary relation between its two argument entities. This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008). This 180 form of semantics captures the underlying predicateargument structure, but fails to license many important inferences—as, for example, write and author do not map to the same predicate. In addition to the lexicon, there is a small set of binary combinators and unary rules, which have a syntactic and semantic interpretation. Figure 1 gives an example CCG derivation. 3 Overview of Approach We attempt to learn a CCG lexicon which maps equivalent words onto the same logical form—for example learning entries such as: author ` N/PP[o f ] : λ xλ y.relation37(x, y) write ` (SNP)/NP : λ xλ"
Q13-1015,J92-4003,0,0.084625,"nt lexical entry for the verb born is used in the contexts Obama was born in Hawaii and Obama was born in 1961, reflecting a distinction in the semantics that is not obvious in the syntax1. Typing also greatly improves the efficiency of clustering, as we only need to compare predicates with the same type during clustering (for example, we do not have to consider clustering a predicate between people and places with predicates between people and dates). In this work, we focus on inducing binary relations. Many existing approaches have shown how to produce good clusterings of (non-event) nouns (Brown et al., 1992), any of which could be simply integrated into our semantics—but relation clustering remains an open problem (see Section 9). N-ary relations are binarized, by creating a binary relation between each pair of arguments. For example, for the sentence Russia sold Alaska to the United States, the system creates three binary relations— corresponding to sellToSomeone(Russia, Alaska), buyFromSomeone(US, Alaska), sellSomethingTo(Russia, US). This transformation does not 1 Whilst this assumption is very useful, it does not always hold— for example, the genitive in Shakespeare’s book is ambiguous betwee"
Q13-1015,P04-1014,0,0.108385,"s of the arguments. 181 exactly preserve meaning, but still captures the most important relations. Note that this allows us to compare semantic relations across different syntactic types—for example, both transitive verbs and argument-taking nouns can be seen as expressing binary semantic relations between entities. Figure 2 shows the layers used in our model. 4 Initial Semantic Analysis The initial semantic analysis maps parser output onto a logical form, in a similar way to Boxer. The semantic formalism is based on Steedman (2012). The first step is syntactic parsing. We use the C&C parser (Clark and Curran, 2004), trained on CCGBank (Hockenmaier and Steedman, 2007), using the refined version of Honnibal et al. (2010) which brings the syntax closer to the predicateargument structure. An automatic post-processing step makes a number of minor changes to the parser output, which converts the grammar into one more suitable for our semantics. PP (prepositional phrase) and PR (phrasal verb complement) categories are sub-categorised with the relevant preposition. Noun compounds with the same MUC named-entity type (Chinchor and Robinson, 1997) are merged into a single non-compositional node2 (we otherwise igno"
Q13-1015,H94-1010,0,0.100684,"predictions (with or without gold syntax). This suggests that making first-order logic inferences in applications will not harm precision. We are less robust than MacCartney and Manning (2007) to syntax errors but, conversely, we are able to attempt more of the problems (i.e. those with multi-sentence premises). Other approaches based on distributional semantics seem unable to tackle any of these problems, as they do not represent quantifiers or negation. 187 Related Work Much work on semantics has taken place in a supervised setting—for example the GeoQuery (Zelle and Mooney, 1996) and ATIS (Dahl et al., 1994) semantic parsing tasks. This approach makes sense for generating queries for a specific database, but means the semantic representations do not generalize to other datasets. There have been several attempts to annotate larger corpora with semantics—such as Ontonotes (Hovy et al., 2006) or the Groningen Meaning Bank (Basile et al., 2012). These typically map words onto senses in ontologies such as WordNet, VerbNet (Kipper et al., 2000) and FrameNet (Baker et al., 1998). However, limitations of these ontologies mean that they do not support inferences such as X is the author of Y → X wrote Y. G"
Q13-1015,D11-1142,0,0.00887541,"ating relations out of context. Our evaluation is based on that performed by Poon and Domingos (2009). We automatically construct a set of questions by sampling from text, and then evaluate how many answers can be found in a different corpus. From dependency-parsed • CCG-Distributional The logical form including the type model and clusters. • Relational LDA An LDA based model for clustering dependency paths (Yao et al., 2011). We train on New York Times subset of Gigaword9, using their setup of 50 iterations with 100 relation types. • Reverb A sophisticated Open Information Extraction system (Fader et al., 2011). verb → Y or X ← be → noun → Y patterns, where X and Y are proper nouns and the verb is not on a list of stop verbs, and deterministically convert these to questions. For example, from Google bought YouTube we create the questions What did Google buy? and What bought YouTube?. The task is to find proper-noun answers to these questions in a different corpus, which are then evaluated by human annotators based on the sentence the answer was retrieved from8. Systems can return multiple Unsupervised Semantic Parsing (USP; Poon and Domingos, 2009; USP; Poon and Domingos, 2010; USP; Titov and Klemen"
Q13-1015,W11-0112,0,0.344011,"e strand of research has attempted to build computational models of linguistic theories based on formal compositional semantics, such as the CCG-based Boxer (Bos, 2008) and the LFGbased XLE (Bobrow et al., 2007). Such approaches convert parser output into formal semantic representations, and have demonstrated some ability to model complex phenomena such as negation. For lexical semantics, they typically compile lexical resources such as VerbNet and WordNet into inference rules—but still achieve only low recall on opendomain tasks, such as RTE, mostly due to the low coverage of such resources. Garrette et al. (2011) 188 use distributional statistics to determine the probability that a WordNet-derived inference rule is valid in a given context. Our approach differs in that we learn inference rules not present in WordNet. Our lexical semantics is integrated into the lexicon, rather than being implemented as additional inference rules, meaning that inference is more efficient, as equivalent statements have the same logical form. Natural Logic (MacCartney and Manning, 2007) offers an interesting alternative to symbolic logics, and has been shown to be able to capture complex logical inferences by simply iden"
Q13-1015,W11-0114,0,0.0149981,"ough USP is an elegant model, it is too computationally expensive to run on large corpora. It is also based on frame semantics, so does not cluster equivalent predicates with different frames. To our knowledge, our work is the first such approach to be integrated within a linguistic theory supporting formal semantics for logical operators. Vector space models represent words by vectors based on co-occurrence counts. Recent work has tackled the problem of composing these matrices to build up the semantics of phrases or sentences (Mitchell and Lapata, 2008). Another strand (Coecke et al., 2010; Grefenstette et al., 2011) has shown how to represent meanings as tensors, whose order depends on the syntactic category, allowing an elegant correspondence between syntactic and semantic types. Socher et al. (2012) train a composition function using a neural network—however their method requires annotated data. It is also not obvious how to represent logical relations such as quantification in vector-space models. Baroni et al. (2012) make progress towards this by learning a classifier that can recognise entailments such as all dogs =⇒ some dogs, but this remains some way from the power of first-order theorem proving"
Q13-1015,J07-3004,1,0.073997,"ing, but still captures the most important relations. Note that this allows us to compare semantic relations across different syntactic types—for example, both transitive verbs and argument-taking nouns can be seen as expressing binary semantic relations between entities. Figure 2 shows the layers used in our model. 4 Initial Semantic Analysis The initial semantic analysis maps parser output onto a logical form, in a similar way to Boxer. The semantic formalism is based on Steedman (2012). The first step is syntactic parsing. We use the C&C parser (Clark and Curran, 2004), trained on CCGBank (Hockenmaier and Steedman, 2007), using the refined version of Honnibal et al. (2010) which brings the syntax closer to the predicateargument structure. An automatic post-processing step makes a number of minor changes to the parser output, which converts the grammar into one more suitable for our semantics. PP (prepositional phrase) and PR (phrasal verb complement) categories are sub-categorised with the relevant preposition. Noun compounds with the same MUC named-entity type (Chinchor and Robinson, 1997) are merged into a single non-compositional node2 (we otherwise ignore named-entity types). All argument NPs and PPs are"
Q13-1015,P10-1022,0,0.0843049,"t this allows us to compare semantic relations across different syntactic types—for example, both transitive verbs and argument-taking nouns can be seen as expressing binary semantic relations between entities. Figure 2 shows the layers used in our model. 4 Initial Semantic Analysis The initial semantic analysis maps parser output onto a logical form, in a similar way to Boxer. The semantic formalism is based on Steedman (2012). The first step is syntactic parsing. We use the C&C parser (Clark and Curran, 2004), trained on CCGBank (Hockenmaier and Steedman, 2007), using the refined version of Honnibal et al. (2010) which brings the syntax closer to the predicateargument structure. An automatic post-processing step makes a number of minor changes to the parser output, which converts the grammar into one more suitable for our semantics. PP (prepositional phrase) and PR (phrasal verb complement) categories are sub-categorised with the relevant preposition. Noun compounds with the same MUC named-entity type (Chinchor and Robinson, 1997) are merged into a single non-compositional node2 (we otherwise ignore named-entity types). All argument NPs and PPs are type-raised, allowing us to represent quantifiers. Al"
Q13-1015,N06-2015,0,0.0165752,"multi-sentence premises). Other approaches based on distributional semantics seem unable to tackle any of these problems, as they do not represent quantifiers or negation. 187 Related Work Much work on semantics has taken place in a supervised setting—for example the GeoQuery (Zelle and Mooney, 1996) and ATIS (Dahl et al., 1994) semantic parsing tasks. This approach makes sense for generating queries for a specific database, but means the semantic representations do not generalize to other datasets. There have been several attempts to annotate larger corpora with semantics—such as Ontonotes (Hovy et al., 2006) or the Groningen Meaning Bank (Basile et al., 2012). These typically map words onto senses in ontologies such as WordNet, VerbNet (Kipper et al., 2000) and FrameNet (Baker et al., 1998). However, limitations of these ontologies mean that they do not support inferences such as X is the author of Y → X wrote Y. Given the difficulty of annotating large amounts of text with semantics, various approaches have attempted to learn meaning without annotated text. Distant Supervision approaches leverage existing knowledge bases, such as Freebase (Bollacker et al., 2008), to learn semantics (Mintz et al"
Q13-1015,kingsbury-palmer-2002-treebank,0,0.0923645,"e (NN)/NP or ((SNP)(SNP))/NP), as it is difficult for the parser to distinguish arguments and adjuncts. Initial semantic lexical entries for almost all words can be generated automatically from the syntactic category and POS tag (obtained from the parser), as the syntactic category captures the underlying predicate-argument structure. We use a Davidsonian-style representation of arguments (Davidson, 1967), which we binarize by creating a separate predicate for each pair of arguments of a word. These predicates are labelled with the lemma of the head word and a Propbank-style argument key (Kingsbury and Palmer, 2002), e.g. arg0, argIn. We distinguish noun and verb predicates based on POS 2 For example, this allows us to give Barack Obama the semantics λ x.barack obama(x) instead of λ x.barack(x) ∧ obama(x), which is more convenient for collecting distributional statistics. Automatic Manual Word author write every not Category N/PP[o f ] (SNP)/NP NP↑ /N (SNP)/(SNP) Semantics λ xλ y.authorarg0,argOf (y,x) λ xλ y.writearg0,arg1 (y,x) λ pλ q.∀x[p(x) → q(x)] λ pλ x.¬p(x) Figure 3: Example initial lexical entries tag—so, for example, we have different predicates for effect as a noun or verb. This algorithm c"
Q13-1015,D12-1069,0,0.0815796,"Missing"
Q13-1015,P11-1060,0,0.133985,"enses in ontologies such as WordNet, VerbNet (Kipper et al., 2000) and FrameNet (Baker et al., 1998). However, limitations of these ontologies mean that they do not support inferences such as X is the author of Y → X wrote Y. Given the difficulty of annotating large amounts of text with semantics, various approaches have attempted to learn meaning without annotated text. Distant Supervision approaches leverage existing knowledge bases, such as Freebase (Bollacker et al., 2008), to learn semantics (Mintz et al., 2009; Krishnamurthy and Mitchell, 2012). Dependency-based Compositional Semantics (Liang et al., 2011) learns the meaning of questions by using their answers as denotations—but this appears to be specific to question parsing. Such approaches can only learn the pre-specified relations in the knowledge base. The approaches discussed so far in this section have all attempted to map language onto some prespecified set of relations. Various attempts have been made to instead induce relations from text by clustering predicates based on their arguments. For example, Yao et al. (2011) propose a series of LDAbased models which cluster relations between entities based on a variety of lexical, syntactic"
Q13-1015,W07-1431,0,0.633656,"rs. 186 The FraCaS suite (Cooper et al., 1996)11 contains a hand-built set of entailment problems designed to be challenging in terms of formal semantics. We use Section 1, which contains 74 problems requiring an understanding of quantifiers12. They do not require any knowledge of lexical semantics, meaning we can evaluate the formal component of our system in isolation. However, we use the same representations as in our previous experiment, even though the clusters provide no benefit on this task. Figure 5 gives an example problem. The only previous work we are aware of on this dataset is by MacCartney and Manning (2007). This approach learns the monotonicity properties of words from a hand-built training set, and uses this to transform a sentence into a polarity annotated string. The system then aims to transform the premise string into a hypothesis. Positively polarized words can be replaced with less specific ones (e.g. by deleting adjuncts), whereas negatively polarized words can be replaced with more specific ones (e.g. by adding adjuncts). Whilst this is highprecision and often useful, this logic is unable to perform inferences with multiple premise sentences (in contrast to our first-order logic). Deve"
Q13-1015,P09-1113,0,0.0083549,"al., 2006) or the Groningen Meaning Bank (Basile et al., 2012). These typically map words onto senses in ontologies such as WordNet, VerbNet (Kipper et al., 2000) and FrameNet (Baker et al., 1998). However, limitations of these ontologies mean that they do not support inferences such as X is the author of Y → X wrote Y. Given the difficulty of annotating large amounts of text with semantics, various approaches have attempted to learn meaning without annotated text. Distant Supervision approaches leverage existing knowledge bases, such as Freebase (Bollacker et al., 2008), to learn semantics (Mintz et al., 2009; Krishnamurthy and Mitchell, 2012). Dependency-based Compositional Semantics (Liang et al., 2011) learns the meaning of questions by using their answers as denotations—but this appears to be specific to question parsing. Such approaches can only learn the pre-specified relations in the knowledge base. The approaches discussed so far in this section have all attempted to map language onto some prespecified set of relations. Various attempts have been made to instead induce relations from text by clustering predicates based on their arguments. For example, Yao et al. (2011) propose a series of"
Q13-1015,P10-1045,0,0.0128013,"Missing"
Q13-1015,D09-1001,0,0.699096,"d types. We outperform a variety of existing approaches on a wide-coverage question answering task, and demonstrate the ability to make complex multi-sentence inferences involving quantifiers on the FraCaS suite. 1 Introduction Mapping natural language to meaning representations is a central challenge of NLP. There has been much recent progress in unsupervised distributional semantics, in which the meaning of a word is induced based on its usage in large corpora. This approach is useful for a range of key applications including question answering and relation extraction (Lin and Pantel, 2001; Poon and Domingos, 2009; Yao et al., 2011). Because such a semantics can be automically induced, it escapes the limitation of depending on relations from hand-built training data, knowledge bases or ontologies, which have proved 1. Google purchased YouTube 2. Google’s acquisition of YouTube 3. Google acquired every company 4. YouTube may be sold to Google 5. Google will buy YouTube or Microsoft 6. Google didn’t takeover YouTube All of these require knowledge of lexical semantics (e.g. that buy and purchase are synonyms), but some also need interpretation of quantifiers, negatives, modals and disjunction. It seems un"
Q13-1015,P10-1031,0,0.0184645,"rmation Extraction system (Fader et al., 2011). verb → Y or X ← be → noun → Y patterns, where X and Y are proper nouns and the verb is not on a list of stop verbs, and deterministically convert these to questions. For example, from Google bought YouTube we create the questions What did Google buy? and What bought YouTube?. The task is to find proper-noun answers to these questions in a different corpus, which are then evaluated by human annotators based on the sentence the answer was retrieved from8. Systems can return multiple Unsupervised Semantic Parsing (USP; Poon and Domingos, 2009; USP; Poon and Domingos, 2010; USP; Titov and Klementiev, 2011) would be another obvious baseline. However, memory requirements mean it is not possible to run at this scale (our system is trained on 4 orders of magnitude more data than the USP evaluation). Yao et al. (2011) found it had comparable performance to Relational LDA. For the CCG models, rather than performing full first-order inference on a large corpus, we simply test whether the question predicate subsumes a candidate answer predicate, and whether the arguments match10. In the case of CCG-Distributional, we calculate the probability that the two packed-predic"
Q13-1015,P10-1044,0,0.0421419,"Missing"
Q13-1015,D10-1106,0,0.0598576,"on37(x, y) The only change to the standard CCG derivation is that the symbols used in the logical form are arbitrary relation identifiers. We learn these by first mapping to a deterministic logical form (using predicates such as author’ and write’), using a process similar to Boxer, and then clustering predicates based on their arguments. This lexicon can then be used to parse new sentences, and integrates seamlessly with CCG theories of formal semantics. Typing predicates—for example, determining that writing is a relation between people and books— has become standard in relation clustering (Schoenmackers et al., 2010; Berant et al., 2011; Yao et al., 2012). We demonstate how to build a typing model into the CCG derivation, by subcategorizing all terms representing entities in the logical form with a more detailed type. These types are also induced from text, as explained in Section 5, but for convenience we describe them with human-readable labels, such as PER, LOC and BOOK. A key advantage of typing is that it allows us to model ambiguous predicates. Following Berant et al. (2011), we assume that different type signatures of the same predicate have different meanings, but given a type signature a predica"
Q13-1015,D12-1110,0,0.0494172,"Missing"
Q13-1015,P07-1058,0,0.0712528,"it combines with its arguments. However, all the possible logical forms are identical except for the symbols used, which means we can produce a packed logical form capturing the full distribution over logical forms. To do this, we make the predicate a function from argument types to relations. For each word, we first take the lexical semantic definition produced by the algorithm in Section 4. For binary predicates in this definition (which will 5 We also experimented with a Dirichlet Process Mixture Model (Neal, 2000), but even with the efficient A* search algorithms introduced by Daum´e III (2007), the cost of inference was found to be prohibitively high when run at large scale. 184 be untyped), we perform a deterministic lookup in the cluster model learned in Section 6, using all possible corresponding typed predicates. This allows us to represent the binary predicates as packed predicates: functions from argument types to relations. For example, if the clustering maps bornarg0:PER,argIn:LOC to rel49 (“birthplace”) and bornarg0:PER,argIn:DAT to rel53 (“birthdate”), our lexicon contains the following packed lexical entry (type-distributions on the variables are suppressed): born ` (SN"
Q13-1015,P11-1145,0,0.0251099,"er et al., 2011). verb → Y or X ← be → noun → Y patterns, where X and Y are proper nouns and the verb is not on a list of stop verbs, and deterministically convert these to questions. For example, from Google bought YouTube we create the questions What did Google buy? and What bought YouTube?. The task is to find proper-noun answers to these questions in a different corpus, which are then evaluated by human annotators based on the sentence the answer was retrieved from8. Systems can return multiple Unsupervised Semantic Parsing (USP; Poon and Domingos, 2009; USP; Poon and Domingos, 2010; USP; Titov and Klementiev, 2011) would be another obvious baseline. However, memory requirements mean it is not possible to run at this scale (our system is trained on 4 orders of magnitude more data than the USP evaluation). Yao et al. (2011) found it had comparable performance to Relational LDA. For the CCG models, rather than performing full first-order inference on a large corpus, we simply test whether the question predicate subsumes a candidate answer predicate, and whether the arguments match10. In the case of CCG-Distributional, we calculate the probability that the two packed-predicates 8 9 nsub j dob j nsub j newsw"
Q13-1015,D11-1135,0,0.167016,"variety of existing approaches on a wide-coverage question answering task, and demonstrate the ability to make complex multi-sentence inferences involving quantifiers on the FraCaS suite. 1 Introduction Mapping natural language to meaning representations is a central challenge of NLP. There has been much recent progress in unsupervised distributional semantics, in which the meaning of a word is induced based on its usage in large corpora. This approach is useful for a range of key applications including question answering and relation extraction (Lin and Pantel, 2001; Poon and Domingos, 2009; Yao et al., 2011). Because such a semantics can be automically induced, it escapes the limitation of depending on relations from hand-built training data, knowledge bases or ontologies, which have proved 1. Google purchased YouTube 2. Google’s acquisition of YouTube 3. Google acquired every company 4. YouTube may be sold to Google 5. Google will buy YouTube or Microsoft 6. Google didn’t takeover YouTube All of these require knowledge of lexical semantics (e.g. that buy and purchase are synonyms), but some also need interpretation of quantifiers, negatives, modals and disjunction. It seems unlikely that 179 Tra"
Q13-1015,P12-1075,0,0.070609,"rivation is that the symbols used in the logical form are arbitrary relation identifiers. We learn these by first mapping to a deterministic logical form (using predicates such as author’ and write’), using a process similar to Boxer, and then clustering predicates based on their arguments. This lexicon can then be used to parse new sentences, and integrates seamlessly with CCG theories of formal semantics. Typing predicates—for example, determining that writing is a relation between people and books— has become standard in relation clustering (Schoenmackers et al., 2010; Berant et al., 2011; Yao et al., 2012). We demonstate how to build a typing model into the CCG derivation, by subcategorizing all terms representing entities in the logical form with a more detailed type. These types are also induced from text, as explained in Section 5, but for convenience we describe them with human-readable labels, such as PER, LOC and BOOK. A key advantage of typing is that it allows us to model ambiguous predicates. Following Berant et al. (2011), we assume that different type signatures of the same predicate have different meanings, but given a type signature a predicate is unambiguous. For example a differe"
Q13-1015,C98-1013,0,\N,Missing
Q13-1015,P08-1028,0,\N,Missing
Q13-1015,D13-1064,1,\N,Missing
Q14-1026,P14-2133,0,0.0129844,"s to fully eliminate the current pipeline approach to CCG parsing. Our work also builds on approaches to semisupervised NLP using neural embeddings (Turian et 336 al., 2010; Collobert et al., 2011b). Existing work has mainly focused on ‘flat’ tagging problems, without hierarchical structure. Collobert (2011) gives a model for parsing using embeddings features, by treating each level of the parse tree as a sequence classification problem. Socher et al. (2013) introduce a model in which context-free grammar parses are reranked based on compositional distributional representations for each node. Andreas and Klein (2014) experiment with a number of approaches to improving the Berkeley parser with word embeddings. Such work has not improved over state-of-the-art existing feature sets for constituency parsing—although Bansal et al. (2014) achieve good results for dependency parsing using embeddings. CCG categories contain much of the hierarchical structure needed for parsing, giving a simpler way to improve a parser using embeddings. 6 Conclusions We have shown that CCG parsing can be significantly improved by predicting lexical categories based on unsupervised word embeddings. The resulting parsing pipeline is"
Q14-1026,P11-1048,0,0.0384094,"tate a small corpus of out-of-domain text. Rimell and Clark (2008) argue that this annotation is simpler for lexicalized formalisms such as CCG, as large improvements can be gained from annotating lexical categories, rather than full syntax trees. They achieve higher parsing accuracies than us on biomedical text, but our unsupervised method requires no annotation. It seems likely that our method could be further improved by incorporating out-ofdomain labelled data (where available). The best reported CCG parsing results have been achieved with a model that integrates supertagging and parsing (Auli and Lopez, 2011a; Auli and Lopez, 2011b). This work still uses the same feature set as the C&C parser, suggesting further improvements may be possible by using our embeddings features. Auli and Lopez POS-tag the sentence before parsing, but using our features would allow us to fully eliminate the current pipeline approach to CCG parsing. Our work also builds on approaches to semisupervised NLP using neural embeddings (Turian et 336 al., 2010; Collobert et al., 2011b). Existing work has mainly focused on ‘flat’ tagging problems, without hierarchical structure. Collobert (2011) gives a model for parsing using"
Q14-1026,D11-1031,0,0.103074,"tate a small corpus of out-of-domain text. Rimell and Clark (2008) argue that this annotation is simpler for lexicalized formalisms such as CCG, as large improvements can be gained from annotating lexical categories, rather than full syntax trees. They achieve higher parsing accuracies than us on biomedical text, but our unsupervised method requires no annotation. It seems likely that our method could be further improved by incorporating out-ofdomain labelled data (where available). The best reported CCG parsing results have been achieved with a model that integrates supertagging and parsing (Auli and Lopez, 2011a; Auli and Lopez, 2011b). This work still uses the same feature set as the C&C parser, suggesting further improvements may be possible by using our embeddings features. Auli and Lopez POS-tag the sentence before parsing, but using our features would allow us to fully eliminate the current pipeline approach to CCG parsing. Our work also builds on approaches to semisupervised NLP using neural embeddings (Turian et 336 al., 2010; Collobert et al., 2011b). Existing work has mainly focused on ‘flat’ tagging problems, without hierarchical structure. Collobert (2011) gives a model for parsing using"
Q14-1026,J99-2004,0,0.754398,"Missing"
Q14-1026,P14-2131,0,0.022516,"focused on ‘flat’ tagging problems, without hierarchical structure. Collobert (2011) gives a model for parsing using embeddings features, by treating each level of the parse tree as a sequence classification problem. Socher et al. (2013) introduce a model in which context-free grammar parses are reranked based on compositional distributional representations for each node. Andreas and Klein (2014) experiment with a number of approaches to improving the Berkeley parser with word embeddings. Such work has not improved over state-of-the-art existing feature sets for constituency parsing—although Bansal et al. (2014) achieve good results for dependency parsing using embeddings. CCG categories contain much of the hierarchical structure needed for parsing, giving a simpler way to improve a parser using embeddings. 6 Conclusions We have shown that CCG parsing can be significantly improved by predicting lexical categories based on unsupervised word embeddings. The resulting parsing pipeline is simpler, and has improved performance both in and out of domain. We expect further improvements to follow as better word embeddings are developed, without other changes to our model. Our approach reduces the problem of"
Q14-1026,W08-2222,0,0.024429,"Missing"
Q14-1026,J92-4003,0,0.227111,"ents out-of-domain. Our best model also outperforms Honnibal et al. (2009)’s self-training approach to domain adaptation on Wikipedia (which lowers performance on CCGBank). Our results show that word embeddings are an effective way of adding distributional information into CCG supertagging. A popular alternative approach 5 We briefly experimented setting the β parameters to match the ambiguity of the C&C supertagger on Section 00 of CCGBank, which caused the F1-score using the Turian-50 embeddings to drop slightly from 86.11 to 85.95. 333 for semi-supervised learning is to use Brown clusters (Brown et al., 1992). To ensure a fair comparison with the Turian embeddings, we use clusters trained on the same corpus, and use a comparable feature set (clusters, capitalization, and 2character suffixes—all implemented as sparse binary features). Brown clusters are hierarchical, and following Koo et al. (2008), we incorporate Brown clusters features at multiple levels of granularity— using 64 coarse clusters (loosely analogous to POStags) and 1000 fine-grained clusters. Results show slightly lower performance than C&C in domain, but higher performance out of domain. However, they are substantially lower than r"
Q14-1026,J07-4004,0,0.0599521,"uce a parse (F1-cov), and the parser’s coverage (COV). Where available we also report overall scores (F1-all), including parser failures, which we believe gives a more realistic assessment. used in the published results. Biomedical results use the publicly available parsing model, setting the ‘parser beam ratio’ parameter to 10−4 , which improved results on development data. To achieve full coverage on the Wikipedia corpus, we increased the ‘max supercats’ parameter to 107 . C&C accuracies differ very slightly from previously reported results, due to differences in the retrained models. As in Clark and Curran (2007), we use a variablewidth beam β that prunes categories whose probability is less than β times that of the most likely category. For simplicity, our supertaggers use the same β back-off parameters as are used by the C&C parser, though it is possible that further improvements could be gained by carefully tuning these parameters.5 In contrast to the C&C supertaggers, we do not make use a tag-dictionaries. Results are shown in Table 4, and our supertaggers consistently lead to improvements over the baseline parser across all domains, with larger improvements out-of-domain. Our best model also outp"
Q14-1026,W02-1001,0,0.140857,"Missing"
Q14-1026,P06-1088,0,0.339451,"supertags Embeddings Collobert&Weston Skip-Gram Turian HLBL Mikolov Model NNLM Skip Gram NNLM HLBL RNNLM Dimensionality 50 300 25, 50, 100, 200 50, 100 80, 640 Training Words 660M 100B 37M 37M 320M Training Domain Wikipedia Google News Newswire Newswire Broadcast News Table 1: Embeddings used in our experiments. Dimensionality is the set of dimensions of the word embedding space that we experimented with, and Training Words refers to the size of the unlabelled corpus the embeddings were trained on. may be very useful—for example, a noun is much more likely to follow an adjective than a verb. Curran et al. (2006) report a large improvement using a maximum-entropy Markov model for supertagging, conditioned on the surrounding supertags. We follow Turian et al. (2010) in using a linear chain CRF model for sequence classification using embeddings as features. This model does not allow supervised training to fine-tune the embeddings, though it would be possible to build a CRF/NN hybrid that enabled this. We use the same feature set as with the neural network model—so the probability of a category depends on embeddings, capitalization and suffix features—as well as the previous category. The model is traine"
Q14-1026,W11-2911,0,0.0428839,"Missing"
Q14-1026,E14-1014,1,0.853952,"k well for supertagging generalize to other tasks. 5 Related Work Many methods have recently been proposed for improving supervised parsers with unlabelled data. Most of these are orthogonal to our work, and larger improvements may be possible by combining them. Thomforde and Steedman (2011) extends a CCG lexicon by inferring categories for unseen words, based on the likely categories of surrounding words. Unlike our method, this approach is able to learn categories which were unseen in the labelled data, which is shown to be useful for parsing a corpus of questions. Deoskar et al. (2011) and Deoskar et al. (2014) use Viterbi-EM to learn new lexical entries by running a generative parser over a large unlabelled corpus. They show good improvements in accuracy on unseen words, but not overall parsing improvements in-domain. Their parsing model aims to capture non-local information about word usage, which would not be possible for the local context windows used to learn our embeddings. Self-training is another popular method for domain adaptation, and was used successfully by Honnibal et al. (2009) to improve CCG parser performance on Wikipedia. However, it caused a decrease in performance on the in-domai"
Q14-1026,J07-3004,1,0.205118,"Missing"
Q14-1026,W09-3306,0,0.639842,", which allows the model to both tune the embeddings and exploit sequence information. However, tagging with this model was considerably slower than the neural network (again, due to the cost of decoding), so we used the neural network architecture in the remaining experiments. 100 98 96 C&C Turian-50 94 1 2 3 Average Categories Per Word 4 Biomedical 100 98 96 94 92 C&C Turian-50 1 2 3 4 Average Categories Per Word 5 Figure 2: Ambiguity vs. Accuracy for different supertaggers across different domains. Datapoints for the C&C parser use its standard back-off parameters. Supertagger F1 (cov) C&C Honnibal et al. (2009) Brown Clusters Turian-50 Embeddings Turian-50 + POS tags Turian-50 + Frequent words 85.47 85.19 85.27 86.11 85.62 86.04 CCGBank COV F1 (all) 99.6 99.8 99.9 100.0 99.9 100.0 85.30 85.21 86.11 85.55 86.04 F1 (cov) 81.19 81.75 80.89 82.30 81.77 82.44 Wikipedia COV F1 (all) 99.0 99.4 100.0 100.0 100.0 100.0 80.64 80.89 82.30 81.77 82.44 F1 (cov) 76.08 76.06 78.41 77.05 78.10 Bioinfer COV F1 (all) 97.2 100.0 99.8 100.0 100.0 74.88 76.06 78.28 77.05 78.10 Table 4: Parsing F1-scores for labelled dependencies across a range of domains, using the C&C parser with different supertaggers. Embeddings mode"
Q14-1026,P10-1022,0,0.0372462,"ving a simpler way to improve a parser using embeddings. 6 Conclusions We have shown that CCG parsing can be significantly improved by predicting lexical categories based on unsupervised word embeddings. The resulting parsing pipeline is simpler, and has improved performance both in and out of domain. We expect further improvements to follow as better word embeddings are developed, without other changes to our model. Our approach reduces the problem of sparsity caused by the large number of CCG categories, suggesting that finer-grained categories could be created for CCGBank (in the spirit of Honnibal et al. (2010)), which lead to improved performance in downstream semantic parsers. Future work should also explore domain-adaptation, either using unsupervised embeddings trained on out-of-domain text, or using supervised training on out-of-domain corpora. Our results also have implications for other NLP tasks—suggesting that using word embeddings features may be particularly useful out-of-domain, in pipelines that currently rely on POS taggers, and in tasks which suffer from sparsity in the labelled data. Code for our supertagger is released as part of the E ASY CCG parser (Lewis and Steedman, 2014), avai"
Q14-1026,D12-1069,0,0.0104001,"Missing"
Q14-1026,D10-1119,1,0.882947,"Missing"
Q14-1026,D13-1161,0,0.0318493,"Missing"
Q14-1026,Q13-1015,1,0.861932,"Missing"
Q14-1026,D13-1064,1,0.757694,"Missing"
Q14-1026,D14-1107,1,0.173972,"Missing"
Q14-1026,J93-2004,0,0.0463529,"Missing"
Q14-1026,N06-1020,0,0.0182353,"rser over a large unlabelled corpus. They show good improvements in accuracy on unseen words, but not overall parsing improvements in-domain. Their parsing model aims to capture non-local information about word usage, which would not be possible for the local context windows used to learn our embeddings. Self-training is another popular method for domain adaptation, and was used successfully by Honnibal et al. (2009) to improve CCG parser performance on Wikipedia. However, it caused a decrease in performance on the in-domain data, and our method achieves better performance across all domains. McClosky et al. (2006) improve a Penn Treebank parser in-domain using self-training, but other work has failed to improve performance out-ofdomain using self training (Dredze et al., 2007). In a similar spirit to our work, Koo et al. (2008) improve parsing accuracy using unsupervised word cluster features—we have shown that word-embeddings outperform Brown clusters for CCG supertagging. An alternative approach to domain adaptation is to annotate a small corpus of out-of-domain text. Rimell and Clark (2008) argue that this annotation is simpler for lexicalized formalisms such as CCG, as large improvements can be gai"
Q14-1026,C10-1094,0,0.0184724,"Missing"
Q14-1026,D08-1050,0,0.232099,"a decrease in performance on the in-domain data, and our method achieves better performance across all domains. McClosky et al. (2006) improve a Penn Treebank parser in-domain using self-training, but other work has failed to improve performance out-ofdomain using self training (Dredze et al., 2007). In a similar spirit to our work, Koo et al. (2008) improve parsing accuracy using unsupervised word cluster features—we have shown that word-embeddings outperform Brown clusters for CCG supertagging. An alternative approach to domain adaptation is to annotate a small corpus of out-of-domain text. Rimell and Clark (2008) argue that this annotation is simpler for lexicalized formalisms such as CCG, as large improvements can be gained from annotating lexical categories, rather than full syntax trees. They achieve higher parsing accuracies than us on biomedical text, but our unsupervised method requires no annotation. It seems likely that our method could be further improved by incorporating out-ofdomain labelled data (where available). The best reported CCG parsing results have been achieved with a model that integrates supertagging and parsing (Auli and Lopez, 2011a; Auli and Lopez, 2011b). This work still use"
Q14-1026,D09-1085,1,0.873029,"Missing"
Q14-1026,P13-1045,0,0.132618,"occurs as an adjective, never a noun, meaning that the C&C parser is unable to analyse simple sentences like The director of the IMF is traditionally a European. These problems are particularly acute when parsing other domains (Rimell and Clark, 2009). 328 2.2 Semi Supervised NLP using Word Embeddings Recent work has explored using vector space embeddings for words as features in supervised models for a variety of tasks, such as POS-tagging, chunking, named-entity recognition, semantic role labelling, and phrase structure parsing (Turian et al., 2010; Collobert et al., 2011b; Collobert, 2011; Socher et al., 2013). The major motivation for using these techniques has been to minimize the level of task-specific feature engineering required, as the same feature set can lead to good results on a variety of tasks. Performance varies between tasks, but any gains over state-of-the-art traditional features have been small. A variety of techniques have been used for learning such embeddings from large unlabelled corpora, such as neural-network language models. 3 Models We introduce models for predicting CCG lexical categories based on vector-space embeddings. The models can then be used to replace the POStaggin"
Q14-1026,D11-1115,1,0.895691,"rom the labelled data. Interestingly, the best-performing Turian-50 embeddings were trained on just 37M words of text (compared to 100B words for the Skip-gram embeddings), suggesting that further improvements may well be possible using larger unlabelled corpora. Future work should investigate whether the models and embeddings that work well for supertagging generalize to other tasks. 5 Related Work Many methods have recently been proposed for improving supervised parsers with unlabelled data. Most of these are orthogonal to our work, and larger improvements may be possible by combining them. Thomforde and Steedman (2011) extends a CCG lexicon by inferring categories for unseen words, based on the likely categories of surrounding words. Unlike our method, this approach is able to learn categories which were unseen in the labelled data, which is shown to be useful for parsing a corpus of questions. Deoskar et al. (2011) and Deoskar et al. (2014) use Viterbi-EM to learn new lexical entries by running a generative parser over a large unlabelled corpus. They show good improvements in accuracy on unseen words, but not overall parsing improvements in-domain. Their parsing model aims to capture non-local information"
Q14-1026,P10-1040,0,0.723314,"galore and Joshi, 1999). A disadvantage of this approach is that larger sets of lexical categories mean increased sparsity, decreasing tagging accuracy. As large amounts of labelled data are unlikely to be made available, recent work has explored using unlabelled data to improve parser lexicons (Thomforde and Steedman, 2011; Deoskar et al., 2011; Deoskar et al., 2014). However, existing work has failed to improve the overall accuracy of state-of-the-art supervised parsers in-domain. Another strand of recent work has explored using unsupervised word embeddings as features in supervised models (Turian et al., 2010; Collobert et al., 2011b), largely motivated as a simpler and more general alternative to standard feature sets. We apply similar techniques to CCG supertagging, hypothesising that words which are close in the embedding space will have similar supertags. Most existing work has focused on flat tagging tasks, and has not produced state-of-the-art results on structured prediction tasks like parsing (Collobert, 2011; Andreas and Klein, 2014). CCG’s lexicalized nature provides a simple and elegant solution to treating parsing as a flat tagging task, as the lexical categories encode information abo"
Q14-1026,P08-1068,0,\N,Missing
Q14-1026,D07-1112,0,\N,Missing
Q14-1030,D11-1039,0,0.0484367,"interpretable formal meaning representations. Traditionally, sentences are converted into logical form grounded in the symbols of some fixed ontology or relational database. Approaches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). F"
Q14-1030,Q13-1005,0,0.640509,"ls of some fixed ontology or relational database. Approaches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a se"
Q14-1030,W13-2322,0,0.0261855,"hout using annotated question-answer pairs. We have shown how to obtain graph representations from the output of a CCG parser and subsequently learn their correspondence to Freebase using a rich feature set and their denotations as a form of weak supervision. Our parser yields state-of-the art performance on three large Freebase domains and is not limited to question answering. We can create semantic parses for any type of NL sentences. Our work brings together several strands of research. Graph-based representations of sentential meaning have recently gained some attention in the literature (Banarescu et al., 2013), and attempts to map sentences to semantic graphs have met with good inter-annotator agreement. Our work is also closely related to Kwiatkowski et al. (2013) and Berant and Liang (2014) who present open-domain se388 mantic parsers based on Freebase and trained on QA pairs. Despite differences in formulation and model structure, both approaches have explicit mechanisms for handling the mismatch between natural language and the KB (e.g., using logical-type equivalent operators or paraphrases). The mismatch is handled implicitly in our case via our graphical representation which allows for the i"
Q14-1030,D13-1160,0,0.919534,"e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowledge base that spans many sub-domains and stores real world facts in graphical format, and parsed sentences from a large corpus. We formulate semantic parsing as a graph matching problem. We convert t"
Q14-1030,P14-1133,0,0.753639,"m conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowledge base that spans many sub-domains and stores real world facts in graphical format, and parsed sentences from a large corpus. We formulate semantic parsing as a graph matching problem. We convert the output of an open-dom"
Q14-1030,C04-1180,1,0.417277,"tory Categorial Grammar The graph like structure of Freebase inspires us to create a graph like structure for natural language, and learn a mapping between them. To do this we take advantage of the representational power of Combinatory Categorial Grammar (Steedman, 2000). CCG is a linguistic formalism that tightly couples syntax and semantics, and can be used to model a wide range of language phenomCameron Titanic Cameron λyλx. directed.arg1(e, x) ∧ directed.arg2(e, y) NP Cameron e directed dir e cte d.a r g1 < e dir S directed.arg1(e, Cameron) ∧ directed.arg2(e, Titanic) ect 2 See Bos et al. (2004) for a detailed introduction to semantic representation using CCG. 3 Neo-Davidsonian semantics is a form of first-order logic that uses event identifiers (e) to connect verb predicates and their subcategorized arguments through conjunctions. 380 Cameron directed n 1997 in directed.arg1(e, Cameron) ∧ directed.arg2(e, Titanic) ∧ directed.in(e, 1997) ena. CCG is well known for capturing long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as standard local predicate-argument dependencies (Clark et al., 2002), thus supporting wide-coverag"
Q14-1030,P09-1010,0,0.0253326,"m each individual sentence taking into account all entities present, their relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi and Zettlemoyer, 2013; Liang et al., 2011; Clarke et al., 2010), feedback-based learning has been previously used for interpreting and following NL instructions (Branavan et al., 2009; Chen and Mooney, 2011), playing computer games (Branavan et al., 2012), and grounding language in the physical world (Krishnamurthy and Kollar, 2013; Matuszek et al., 2012). Lemma * * * * * be the * not, n’t no * * POS VB*, IN, TO, POS NN, NNS NNP*, PRP* RB* JJ* * * CD Semantic Class * NEGATION * WDT, WP*, WRB WDT, WP*, WRB COMPLEMENT EVENT TYPE ENTITY EVENTMOD TYPEMOD COPULA UNIQUE COUNT QUESTION CLOSED Semantic Category directed : (Se NPx <1&gt;)/NPy <2&gt; : λQλPλe.∃x∃y. directed.arg1(e, x) ∧ directed.arg2(e, y) ∧ P(x) ∧ Q(y) movie : NP : λx.movie(x) Obama : NP : λx.equal(x, Obama) annually :"
Q14-1030,P12-1014,0,0.0161775,"eir relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi and Zettlemoyer, 2013; Liang et al., 2011; Clarke et al., 2010), feedback-based learning has been previously used for interpreting and following NL instructions (Branavan et al., 2009; Chen and Mooney, 2011), playing computer games (Branavan et al., 2012), and grounding language in the physical world (Krishnamurthy and Kollar, 2013; Matuszek et al., 2012). Lemma * * * * * be the * not, n’t no * * POS VB*, IN, TO, POS NN, NNS NNP*, PRP* RB* JJ* * * CD Semantic Class * NEGATION * WDT, WP*, WRB WDT, WP*, WRB COMPLEMENT EVENT TYPE ENTITY EVENTMOD TYPEMOD COPULA UNIQUE COUNT QUESTION CLOSED Semantic Category directed : (Se NPx <1&gt;)/NPy <2&gt; : λQλPλe.∃x∃y. directed.arg1(e, x) ∧ directed.arg2(e, y) ∧ P(x) ∧ Q(y) movie : NP : λx.movie(x) Obama : NP : λx.equal(x, Obama) annually : Se Se : λPλe.lexe .annually(e) ∧ P(e) state : NPx /NPx : λPλx.lexx .sta"
Q14-1030,P13-1042,0,0.850757,"s have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs"
Q14-1030,P04-1014,0,0.0112942,"nation, extraction, raising and control, as well as standard local predicate-argument dependencies (Clark et al., 2002), thus supporting wide-coverage semantic analysis. Moreover, due to the transparent interface between syntax and semantics, it is relatively straightforward to built a semantic parse for a sentence from its corresponding syntactic derivation tree (Bos et al., 2004). In our case, the choice of syntactic parser is motivated by the scale of our problem; the parser must be broad-coverage and robust enough to handle a web-sized corpus. For these reasons, we rely on the C&C parser (Clark and Curran, 2004), a generalpurpose CCG parser, to obtain syntactic derivations. To our knowledge, we present the first attempt to use a CCG parser trained on treebanks for grounded semantic parsing. Most previous work has induced task-specific CCG grammars (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010). An example CCG derivation is shown in Figure 4. Semantic parses are constructed from syntactic CCG parses, with semantic composition being guided by the CCG syntactic derivation.2 We use a neo-Davidsonian (Parsons, 1990) semantics to represent semantic parses.3 Each word has a semantic categor"
Q14-1030,J07-4004,0,0.0398868,"mple of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowledge base that spans many sub-domains and stores real world facts in graphical format, and parsed sentences from a large corpus. We formulate semantic parsing as a graph matching problem. We convert the output of an open-domain combinatory categorial grammar (CCG) parser (Clark and Curran, 2007) into a graphical representation and subsequently map it onto Freebase. The parser’s graphs (also called ungrounded graphs) are mapped to all possible Freebase subgraphs (also called grounded graphs) by replacing edges and nodes with relations and types in Freebase. Each grounded graph corresponds to a unique grounded logical query. During learning, our semantic parser is trained to identify which KB subgraph best corresponds to the NL graph. Problem377 Transactions of the Association for Computational Linguistics, 2 (2014) 377–392. Action Editor: Noah Smith. c Submitted 3/2014; Revised 6/2014"
Q14-1030,P02-1042,1,0.698829,"cted.arg2(e, Titanic) ect 2 See Bos et al. (2004) for a detailed introduction to semantic representation using CCG. 3 Neo-Davidsonian semantics is a form of first-order logic that uses event identifiers (e) to connect verb predicates and their subcategorized arguments through conjunctions. 380 Cameron directed n 1997 in directed.arg1(e, Cameron) ∧ directed.arg2(e, Titanic) ∧ directed.in(e, 1997) ena. CCG is well known for capturing long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as standard local predicate-argument dependencies (Clark et al., 2002), thus supporting wide-coverage semantic analysis. Moreover, due to the transparent interface between syntax and semantics, it is relatively straightforward to built a semantic parse for a sentence from its corresponding syntactic derivation tree (Bos et al., 2004). In our case, the choice of syntactic parser is motivated by the scale of our problem; the parser must be broad-coverage and robust enough to handle a web-sized corpus. For these reasons, we rely on the C&C parser (Clark and Curran, 2004), a generalpurpose CCG parser, to obtain syntactic derivations. To our knowledge, we present the"
Q14-1030,W10-2903,0,0.627118,"gical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large com"
Q14-1030,W02-1001,0,0.0343689,"ven a NL sentence s, we construct from its CCG syntactic derivation all corresponding ungrounded graphs u. Using a beam search procedure (described in Section 4.2), we find the best scoring graphs (g, ˆ u), ˆ maximizing over different graph configurations (g, u) of s: (g, ˆ u) ˆ = arg max Φ(g, u, s, K B ) · θ g,u (1) We define the score of (g, ˆ u) ˆ as the dot product between a high dimensional feature representation Φ = (Φ1 , . . . Φm ) and a weight vector θ (see Section 3.3 for details on the features we employ). We estimate the weights θ using the averaged structured perceptron algorithm (Collins, 2002). As shown in Algorithm 1, the perceptron makes several passes over sentences, and in each iteration it computes the best scoring (g, ˆ u) ˆ among the candidate graphs for a given sentence. In line 6, the algorithm updates θ with the difference (if any) be383 Algorithm 1: Averaged Structured Perceptron 1 2 3 4 Input: Training sentences: {si }Ni=1 θ←0 for t ← 1 . . . T do for i ← 1 . . . N do (gˆi , uˆi ) = arg max Φ(gi , ui , si , K B ) · θ gi ,ui 5 6 7 + if (u+ i , gi ) 6= (uˆi , gˆi ) then + θ ← θ + Φ(g+ i , ui , si , K B )−Φ(gˆi , uˆi , si , K B ) return 1 T T 1 N i ∑t=i N ∑i=1 θ t tween th"
Q14-1030,P13-1158,0,0.286302,"sisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora."
Q14-1030,P11-1149,0,0.0128605,"ed, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge"
Q14-1030,P11-1060,0,0.8618,"d Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowl"
Q14-1030,P11-1055,0,0.0290585,"ails when questions are too short without any lexical clues (e.g., What did Charles Darwin do? ). Supervision from annotated data or paraphrasing could improve performance in such cases. In the future, we plan to explore cluster-based semantics (Lewis and Steedman, 2013) to increase the robustness on unseen NL predicates. Our work joins others in exploiting the connections between natural language and open-domain knowledge bases. Recent approaches in relation extraction use distant supervision from a knowledge base to predict grounded relations between two target entities (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). During learning, they aggregate sentences containing the target entities, ignoring richer contextual information. In contrast, we learn from each individual sentence taking into account all entities present, their relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi"
Q14-1030,Q13-1016,0,0.0259582,") formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi and Zettlemoyer, 2013; Liang et al., 2011; Clarke et al., 2010), feedback-based learning has been previously used for interpreting and following NL instructions (Branavan et al., 2009; Chen and Mooney, 2011), playing computer games (Branavan et al., 2012), and grounding language in the physical world (Krishnamurthy and Kollar, 2013; Matuszek et al., 2012). Lemma * * * * * be the * not, n’t no * * POS VB*, IN, TO, POS NN, NNS NNP*, PRP* RB* JJ* * * CD Semantic Class * NEGATION * WDT, WP*, WRB WDT, WP*, WRB COMPLEMENT EVENT TYPE ENTITY EVENTMOD TYPEMOD COPULA UNIQUE COUNT QUESTION CLOSED Semantic Category directed : (Se NPx <1&gt;)/NPy <2&gt; : λQλPλe.∃x∃y. directed.arg1(e, x) ∧ directed.arg2(e, y) ∧ P(x) ∧ Q(y) movie : NP : λx.movie(x) Obama : NP : λx.equal(x, Obama) annually : Se Se : λPλe.lexe .annually(e) ∧ P(e) state : NPx /NPx : λPλx.lexx .state(x) ∧ P(x) be: (Sy NPx )/NPy : λQλPλy.∃x.lexy (x) ∧ P(x) ∧ Q(y) the : NPx /"
Q14-1030,D12-1069,0,0.773171,"aches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or"
Q14-1030,D13-1161,0,0.680647,"te the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowledge base that spans many sub-domains and stores real world facts in graphical format, and parsed sentences from a large corpus. We formulate semantic parsing as a graph matching"
Q14-1030,D10-1119,1,0.94973,"s requiring communication with machines in a language interpretable by them. Semantic parsing addresses the specific task of learning to map natural language (NL) to machine interpretable formal meaning representations. Traditionally, sentences are converted into logical form grounded in the symbols of some fixed ontology or relational database. Approaches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining m"
Q14-1030,Q13-1015,1,0.822387,"s for the incorporation of all manner of powerful features. More generally, our method is based on the assumption that linguistic structure has a correspondence to Freebase structure which does not always hold (e.g., in Who is the grandmother of Prince William?, grandmother is not directly expressed as a relation in Freebase). Additionally, our model fails when questions are too short without any lexical clues (e.g., What did Charles Darwin do? ). Supervision from annotated data or paraphrasing could improve performance in such cases. In the future, we plan to explore cluster-based semantics (Lewis and Steedman, 2013) to increase the robustness on unseen NL predicates. Our work joins others in exploiting the connections between natural language and open-domain knowledge bases. Recent approaches in relation extraction use distant supervision from a knowledge base to predict grounded relations between two target entities (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). During learning, they aggregate sentences containing the target entities, ignoring richer contextual information. In contrast, we learn from each individual sentence taking into account all entities present, their relations, a"
Q14-1030,P09-1113,0,0.0420296,"ionally, our model fails when questions are too short without any lexical clues (e.g., What did Charles Darwin do? ). Supervision from annotated data or paraphrasing could improve performance in such cases. In the future, we plan to explore cluster-based semantics (Lewis and Steedman, 2013) to increase the robustness on unseen NL predicates. Our work joins others in exploiting the connections between natural language and open-domain knowledge bases. Recent approaches in relation extraction use distant supervision from a knowledge base to predict grounded relations between two target entities (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). During learning, they aggregate sentences containing the target entities, ignoring richer contextual information. In contrast, we learn from each individual sentence taking into account all entities present, their relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond"
Q14-1030,P13-1092,0,0.16245,"ing data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) a"
Q14-1030,N13-1008,0,0.0175128,"too short without any lexical clues (e.g., What did Charles Darwin do? ). Supervision from annotated data or paraphrasing could improve performance in such cases. In the future, we plan to explore cluster-based semantics (Lewis and Steedman, 2013) to increase the robustness on unseen NL predicates. Our work joins others in exploiting the connections between natural language and open-domain knowledge bases. Recent approaches in relation extraction use distant supervision from a knowledge base to predict grounded relations between two target entities (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). During learning, they aggregate sentences containing the target entities, ignoring richer contextual information. In contrast, we learn from each individual sentence taking into account all entities present, their relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi and Zettlemoyer, 2013"
Q14-1030,P07-1121,0,0.589876,"to play a game are tasks requiring communication with machines in a language interpretable by them. Semantic parsing addresses the specific task of learning to map natural language (NL) to machine interpretable formal meaning representations. Traditionally, sentences are converted into logical form grounded in the symbols of some fixed ontology or relational database. Approaches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answe"
Q14-1030,P14-1090,0,0.512733,"Missing"
Q14-1030,D07-1071,0,0.68019,"Missing"
Q16-1010,D15-1138,0,0.00679455,"atkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vecto"
Q16-1010,P02-1041,0,0.0927099,"(Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on te"
Q16-1010,P14-1091,0,0.0366925,"Missing"
Q16-1010,P14-1133,0,0.232833,"Missing"
Q16-1010,Q15-1039,0,0.656552,"Missing"
Q16-1010,D13-1160,0,0.418417,"za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-arg"
Q16-1010,D14-1067,0,0.304718,"in and obtains the best result to date. Interestingly, D EP T REE outperforms S IMPLE G RAPH in this case. We attribute this to the small training set and larger lexical variation of Free917. The structural features of the graph-based representations seem highly beneficial in this case. 6.3 Error Analysis We categorized 100 errors made by D EP L AMBDA (+C +E) on the WebQuestions development set. In 43 cases the correct answer is present in the beam, 136 Method Cai and Yates (2013) Berant et al. (2013) Kwiatkowski et al. (2013) Yao and Van Durme (2014) Berant and Liang (2014) Bao et al. (2014) Bordes et al. (2014) Yao (2015) Yih et al. (2015) (FB API) Bast and Haussmann (2015) Berant and Liang (2015) Yih et al. (2015) (Y&C) Free917 Accuracy WebQuestions Average F1 59.0 62.0 68.0 – 68.5 – – – – 76.4 – – – 35.7 – 33.0 39.9 37.5 39.2 44.3 48.4 49.4 49.7 52.5 This Work D EP T REE S IMPLE G RAPH CCGG RAPH (+ C + E) D EP L AMBDA (+ C + E) 53.2 43.7 73.3 78.0 40.4 48.5 48.6 50.3 Table 3: Question-answering results on the WebQuestions and Free917 test sets. but ranked below an incorrect answer (e.g., for where does volga river start, the annotated gold answer is Valdai Hills, which is ranked second, with Russi"
Q16-1010,C04-1180,1,0.158689,"There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment."
Q16-1010,P13-1042,0,0.356088,"Missing"
Q16-1010,P15-1127,1,0.568655,"was sworn into office when john f kennedy was assassinated ), we do not have a special treatment for them in the semantic representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from"
Q16-1010,W09-3726,0,0.0740413,"it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment. Our work, in contrast, assumes access only to dependency trees and offers an alternative method based on the lambda calculus, mimicking the stru"
Q16-1010,W02-1001,1,0.101306,"∈ &lt;n denotes the features for the pair of ungrounded and grounded graphs. Note that for a given query there may be multiple ungrounded graphs, primarily due to the optional use of the CON TRACT operation.3 The feature function has access to the ungrounded and grounded graphs, to the question, as well as to the content of the knowledge base and the denotation |g|K (the denotation of a grounded graph is defined as the set of entities or attributes reachable at its TARGET node). See Section 5.3 for the features employed. The model parameters are estimated with the averaged structured perceptron (Collins, 2002; Fre3 Another source of ambiguity may be a lexical item having multiple lambda-calculus entries; in our rules this only arises when analyzing count expressions such as how many. und and Schapire, 1999). Given a training questionanswer pair (q, A), the update is: θt+1 ← θt + Φ(u+ , g + , q, K) − Φ(ˆ u, gˆ, q, K) , where (u+ , g + ) denotes the pair of gold ungrounded and grounded graphs for q. Since we do not have direct access to these gold graphs, we instead rely on the set of oracle graphs, OK,A (q), as a proxy: (u+ , g + ) = arg max θt · Φ(u, g, q, K) , (u,g)∈OK,A (q) where OK,A (q) is def"
Q16-1010,P01-1019,0,0.126066,"nd Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph"
Q16-1010,C04-1026,0,0.10619,"tics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment. Our work, in contrast, assumes access only to dependency trees and offers an alternative method based on the lambda calculus,"
Q16-1010,P15-1026,0,0.238247,"et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting representation is subsequently grounded to Freebase by learning from question-answer pairs. E"
Q16-1010,P14-1134,0,0.0159776,"gh an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting represent"
Q16-1010,E03-1030,0,0.118015,"araphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to"
Q16-1010,P09-1069,0,0.0392655,"entation is an inherent problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to ra"
Q16-1010,P15-1143,0,0.0199683,"ntic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting representation is subsequently grou"
Q16-1010,N10-1145,0,0.0444525,"ource semantic representation and the target application’s representation is an inherent problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion"
Q16-1010,D12-1069,0,0.104733,"cquired Pixar nnp vbd nnp (nsubj (dobj acquired Pixar) Disney) (b) The s-expression for the dependency tree. λx. ∃yz. acquired(xe ) ∧ Disney(ya ) ∧ Pixar(za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances ove"
Q16-1010,Q15-1019,0,0.0603458,"s-expression is beta-reduced to give the logical form in Figure 1(c). Since dependency syntax does not have an associated type theory, we introduce a type system that assigns a single type to all constituents, thus avoiding the need for type checking (Section 2). D EP L AMBDA uses this system to generate robust logical forms, even when the dependency structure does not mirror predicate-argument relationships in constructions such as conjunctions, prepositional phrases, relative clauses, and wh-questions (Section 3). These ungrounded logical forms (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015) are used for question answering against Freebase, by passing them as input to G RAPH PARSER (Reddy et al., 2014), a system that learns to map logical predicates to Freebase, resulting in grounded Freebase queries (Section 4). We show that our approach achieves state-of-the-art performance on the Free917 dataset and competitive performance on the WebQuestions dataset, whereas building the Freebase queries directly from dependency trees gives significantly lower performance. Finally, we show that our approach outperforms a directly comparable method that generates ungrounded logical forms using"
Q16-1010,D10-1119,1,0.364959,"tructures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of"
Q16-1010,D13-1161,1,0.949182,"lambda-calculus expression and the relabeled s-expression is beta-reduced to give the logical form in Figure 1(c). Since dependency syntax does not have an associated type theory, we introduce a type system that assigns a single type to all constituents, thus avoiding the need for type checking (Section 2). D EP L AMBDA uses this system to generate robust logical forms, even when the dependency structure does not mirror predicate-argument relationships in constructions such as conjunctions, prepositional phrases, relative clauses, and wh-questions (Section 3). These ungrounded logical forms (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015) are used for question answering against Freebase, by passing them as input to G RAPH PARSER (Reddy et al., 2014), a system that learns to map logical predicates to Freebase, resulting in grounded Freebase queries (Section 4). We show that our approach achieves state-of-the-art performance on the Free917 dataset and competitive performance on the WebQuestions dataset, whereas building the Freebase queries directly from dependency trees gives significantly lower performance. Finally, we show that our approach outperforms a directly comparab"
Q16-1010,D14-1107,1,0.663912,"XPAND. development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. Syntactic Parsing. We recase the resolved entity mentions and run a case-sensitive second-order conditional random field part-of-speech tagger (Lafferty et al., 2001). The hypergraph parser of Zhang and McDonald (2014) is used for dependency parsing. The tagger and parser are both trained on the OntoNotes 5.0 corpus (Weischedel et al., 2011), with constituency trees converted to Stanford-style dependencies (De Marneffe and Manning, 2008). To derive the CCG-based representation, we use the output of the EasyCCG parser (Lewis and Steedman, 2014). edge, we can ground the edge to a Freebase relation, contract the edge in either direction, or skip the edge. For an entity type node, we can ground the node to a Freebase type, or skip the node. The order of traversal is based on the number of named entities connected to an edge. After an edge is grounded, the entity type nodes connected to it are grounded in turn, before the next edge is processed. To restrict the search, if two beam items correspond to the same grounded graph, the one with the lower score is discarded. A beam size of 100 was used in all experiments. Features. We use the f"
Q16-1010,P11-1060,0,0.141153,"plication’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally der"
Q16-1010,N15-1114,0,0.0108635,"representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TA"
Q16-1010,P14-5010,0,0.00310002,"presentation of Reddy et al. (2014), adding the CONTRACT and EXPAND operations to increase its expressivity. 5.3 Implementation Details Below are more details of our entity resolution model, the syntactic parser used, features in the grounding model and the beam search procedure. Entity Resolution. For Free917, we follow prior work and resolve entities by string match against the entity lexicon provided with the dataset. For WebQuestions, we use eight handcrafted part-of-speech patterns to identify entity span candidates. We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014). For each candidate mention span, we retrieve the top 10 entities according to the Freebase API.5 We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. Finally, we generate ungrounded graphs for the top 10 paths through the lattice and treat the final entity disambiguation as part of the semantic parsing problem. 4 5 http://github.com/sivareddyg/graph-parser http://developers.google.com/freebase/ Representation -C -E -C +E"
Q16-1010,P13-2109,0,0.018844,"robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of these logical forms for question answering from databases. Since our approach uses dependency trees as input, we hypothesize that it will generalize better to domains that are well covered by dependency parsers than methods that induce semantic grammars from scratch. The system that maps a dependency tree to its logical form (hencefo"
Q16-1010,H05-1066,0,0.0191937,"Missing"
Q16-1010,P14-1041,0,0.019399,"special treatment for them in the semantic representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al.,"
Q16-1010,N15-1077,0,0.0234129,"ith approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given q"
Q16-1010,P15-1146,0,0.0242314,"Missing"
Q16-1010,P13-1092,0,0.00889818,"nt problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential"
Q16-1010,Q14-1030,1,0.452199,"mbda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of t"
Q16-1010,N15-1118,0,0.0159445,"Missing"
Q16-1010,N15-1040,0,0.0128228,"epresentations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and a"
Q16-1010,N06-1056,0,0.0904852,"trongest result to date on Free917 and competitive results on WebQuestions. 1 Disney acquired Pixar nnp vbd nnp (nsubj (dobj acquired Pixar) Disney) (b) The s-expression for the dependency tree. λx. ∃yz. acquired(xe ) ∧ Disney(ya ) ∧ Pixar(za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et"
Q16-1010,P07-1121,0,0.0158316,"converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provid"
Q16-1010,P15-1049,0,0.0484602,"Missing"
Q16-1010,P14-1090,0,0.408064,"Missing"
Q16-1010,N13-1106,0,0.0699006,"Missing"
Q16-1010,N15-3014,0,0.0687989,"iginal dependency tree. An event is created for each parent and its dependents in the tree. Each dependent is linked to this event with an edge labeled with its dependency relation, while the parent is linked to the event with an edge labeled arg0 . If a word is a question word, an additional TARGET predicate is attached to its entity node. S IMPLE G RAPH. This representation has a single event to which all entities in the question are connected by the predicate arg1 . An additional TARGET node is connected to the event by the predicate arg0 . This is similar to the template representation of Yao (2015) and Bast and Haussmann (2015). Note that this cannot represent any compositional structure. CCGG RAPH. Finally, we compare to the CCGbased semantic representation of Reddy et al. (2014), adding the CONTRACT and EXPAND operations to increase its expressivity. 5.3 Implementation Details Below are more details of our entity resolution model, the syntactic parser used, features in the grounding model and the beam search procedure. Entity Resolution. For Free917, we follow prior work and resolve entities by string match against the entity lexicon provided with the dataset. For WebQuestions, we use"
Q16-1010,P15-1128,0,0.133944,"into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of these logical forms for question answering from databases. Since ou"
Q16-1010,P14-2107,0,0.0173573,".0 73.4 (c) Accuracy 42.6 48.2 46.5 48.8 42.6 48.2 48.9 50.4 D EP T REE S IMPLE G RAPH CCGG RAPH D EP L AMBDA Table 1: Oracle statistics and accuracies on the Web21.3 40.9 68.3 69.3 21.3 40.9 69.4 71.3 Table 2: Oracle statistics and accuracies on the Free917 Questions development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. Syntactic Parsing. We recase the resolved entity mentions and run a case-sensitive second-order conditional random field part-of-speech tagger (Lafferty et al., 2001). The hypergraph parser of Zhang and McDonald (2014) is used for dependency parsing. The tagger and parser are both trained on the OntoNotes 5.0 corpus (Weischedel et al., 2011), with constituency trees converted to Stanford-style dependencies (De Marneffe and Manning, 2008). To derive the CCG-based representation, we use the output of the EasyCCG parser (Lewis and Steedman, 2014). edge, we can ground the edge to a Freebase relation, contract the edge in either direction, or skip the edge. For an entity type node, we can ground the node to a Freebase type, or skip the node. The order of traversal is based on the number of named entities connect"
Q16-1010,J90-1001,0,\N,Missing
Q16-1010,D15-1198,0,\N,Missing
Q18-1048,P11-1062,0,0.135964,"t and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although they showed transitivity constraints to be effective in learning entailment graphs, the Integer Linear Programming (ILP) solution of Berant et al. is not scalable beyond a few hundred nodes. In fact, the problem of finding a maximally weighted transitive subgraph of a graph with arbitrary edge weights is NP-hard (Berant et al., 2011). This paper instead proposes a scalable solution that does not rely on transitivity closure, but 703 Transactions of the Association fo"
Q18-1048,D15-1075,0,0.125403,"Missing"
Q18-1048,D17-1070,0,0.0422865,"Missing"
Q18-1048,P14-1061,1,0.833691,"better by additionally assuming the entailment graphs are “forest reducible,” where a predicate cannot entail two (or more) predicates j and k such that neither j→k nor k→j (FRG assumption). However, the FRG assumption is not correct for many real-world domains. For example, a person visiting a place entails both arriving at that place and leaving that place, although the latter do not necessarily entail each other. Our work injects two other types of prior knowledge about the structure of the graph that are less expensive to incorporate and yield better results on entailment rule data sets. Abend et al. (2014) learn entailment relations over multi-word predicates with different levels of compositionality. Pavlick et al. (2015) add variety of relations, including entailment, to phrase pairs in PPDB. This includes a broader range of entailment relations such as lexical entailment. In contrast to our method, these works rely on supervised data and take a local learning approach. Another related strand of research is link prediction (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Yang et al., 2015; Trouillon et al., 2016; Dettmers et al., 2018), where the source data are extractions fro"
Q18-1048,P15-1034,0,0.0372871,"n C1 =3 unique predicates; (2) remove any predicate that is observed with fewer than C2 =3 unique argument-pairs. This leaves us with |P |=101K unique predicates in 346 entailment graphs. The maximum graph size is 53K nodes,8 and the total number of non-zero local scores in all graphs is 66M. In the future, we plan to test our method on an even larger corpus, but preliminary experiments suggest that data sparsity will persist regardless of the corpus size, because of the power law distribution of the terms. We compared our extractions qualitatively with Stanford Open IE (Etzioni et al., 2011; Angeli et al., 2015). Our CCG-based extraction generated noticeably 7 In our experiments, the total number of edges is ≈ .01|V |2 and most of predicate pairs are seen in less than 20 subgraphs, rather than |T |2 . 8 There are 4 graphs with more than 20K nodes, 3 graphs with 10K to 20K nodes, and 16 graphs with 1K to 10K nodes. 708 better relations for longer sentences with longrange dependencies such as those involving coordination. 5.2 ment graphs based on the predicates in their corpus. The data set contains 3,427 edges (positive), and 35,585 non-edges (negative). We evaluate our method on all the examples of B"
Q18-1048,J15-2003,0,0.30255,"ing the relation name with the object. For example, for the sentence China has a border with India, we extract a relation have border1,with between China and India. We perform a similar process for prepositional phrases attached to verb phrases. Most of the light verbs and multiword predicates will be extracted by the above post-processing (e.g., take care1,of ), which will recover many salient ternary relations. Although entailments and paraphrasing can benefit from n-ary relations—for example, person visits a location in a time—we currently follow previous work (Lewis and Steedman, 2013a); (Berant et al., 2015) in confining our attention to binary relations, leaving the construction of n-ary graphs to future work. wisdom is that entailment relations are a byproduct of these methods (Riedel et al., 2013). However, this assumption has not usually been explicitly evaluated. Explicit entailment rules provide explainable resources that can be used in downstream tasks. Our experiments show that our method significantly outperforms a state-of-the-art link prediction method. 3 Computing Local Similarity Scores We first extract binary relations as predicateargument pairs using a combinatory categorial gramma"
Q18-1048,D17-1091,1,0.841356,"only a few hours to apply to more than 100K predicates.1,2 Our experiments (§6) show that the global scores improve significantly over local scores and outperform state-of-the-art entailment graphs on two standard entailment rule data sets (Berant et al., 2011; Holt, 2018). We ultimately intend the typed entailment graphs to provide a resource for entailment and paraphrase rules for use in semantic parsing and open domain question answering, as has been done for similar resources such as the Paraphrase Database (PPDB; Ganitkevitch et al., 2013; Pavlick et al., 2015) in Wang et al. (2015) and Dong et al. (2017).3 With that end in view, we have included a comparison with PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within-graph (light blue) con"
Q18-1048,P12-1013,0,0.0150022,"ith PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within-graph (light blue) connections. on assumptions concerning the graph structure. Berant et al. (2012, 2015) propose Tree-Node-Fix (TNF), an approximation method that scales better by additionally assuming the entailment graphs are “forest reducible,” where a predicate cannot entail two (or more) predicates j and k such that neither j→k nor k→j (FRG assumption). However, the FRG assumption is not correct for many real-world domains. For example, a person visiting a place entails both arriving at that place and leaving that place, although the latter do not necessarily entail each other. Our work injects two other types of prior knowledge about the structure of the graph that are less expensiv"
Q18-1048,P16-2041,0,0.530878,"han 20K nodes, 3 graphs with 10K to 20K nodes, and 16 graphs with 1K to 10K nodes. 708 better relations for longer sentences with longrange dependencies such as those involving coordination. 5.2 ment graphs based on the predicates in their corpus. The data set contains 3,427 edges (positive), and 35,585 non-edges (negative). We evaluate our method on all the examples of Berant’s entailment data set. The types of this data set do not match with FIGER types, but we perform a simple handmapping between their types and FIGER types.10 Evaluation Entailment Data Sets Levy/Holt’s Entailment Data Set Levy and Dagan (2016) proposed a new annotation method (and a new data set) for collecting relational inference data in context. Their method removes a major bias in other inference data sets such as Zeichner’s (Zeichner et al., 2012), where candidate entailments were selected using a directional similarity measure. Levy and Dagan form questions of the type which city (qtype ), is located near (qrel ), mountains (qarg )? and provide possible answers of the form Kyoto (aanswer ), is surrounded by (arel ), mountains (aarg ). Annotators are shown a question with multiple possible answers, where aanswer is masked by q"
Q18-1048,S17-1026,0,0.22754,"Missing"
Q18-1048,N13-1092,0,0.0938356,"Missing"
Q18-1048,D13-1064,1,0.942149,"third argument by concatenating the relation name with the object. For example, for the sentence China has a border with India, we extract a relation have border1,with between China and India. We perform a similar process for prepositional phrases attached to verb phrases. Most of the light verbs and multiword predicates will be extracted by the above post-processing (e.g., take care1,of ), which will recover many salient ternary relations. Although entailments and paraphrasing can benefit from n-ary relations—for example, person visits a location in a time—we currently follow previous work (Lewis and Steedman, 2013a); (Berant et al., 2015) in confining our attention to binary relations, leaving the construction of n-ary graphs to future work. wisdom is that entailment relations are a byproduct of these methods (Riedel et al., 2013). However, this assumption has not usually been explicitly evaluated. Explicit entailment rules provide explainable resources that can be used in downstream tasks. Our experiments show that our method significantly outperforms a state-of-the-art link prediction method. 3 Computing Local Similarity Scores We first extract binary relations as predicateargument pairs using a comb"
Q18-1048,W14-2406,1,0.894326,"Missing"
Q18-1048,P05-1014,0,0.315327,"o arguments, where the type of each predicate is determined by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions signif"
Q18-1048,P98-2127,0,0.422662,"es as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies"
Q18-1048,P13-2078,0,0.0219818,"ype of each predicate is determined by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imp"
Q18-1048,C16-1268,0,0.106009,"ed by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on"
Q18-1048,P18-1188,1,0.78834,"Missing"
Q18-1048,W04-3250,0,0.0561007,"Missing"
Q18-1048,W17-2623,0,0.121698,"ign workers . . . . . . Barnes & Noble CEO William Lynch said as he unveiled his company’s Nook Tablet on Monday. The report said opium has accounted for more than half of Afghanistan’s gross domestic product in 2007. Who praised Mitt Romney’s credentials? Which gene did the ALS association discover ? How many Americans suffer from food allergies? What law might the deal break? Who launched the Nook Tablet? What makes up half of Afghanistans GDP ? Table 3: Examples where explicit entailment relations improve the rankings. The related words are boldfaced. contains questions about CNN articles (Trischler et al., 2017). Machine reading comprehension is usually evaluated by posing questions about a text passage and then assessing the answers of a system (Trischler et al., 2017). The data sets that are used for this task are often in the form of (document,question,answer) triples, where answer is a short span of the document. Answer selection is an important task, where the goal is to select the sentence(s) that contain the answer. We show improvements by adding knowledge from our learned entailments without changing the graphs or tuning them to this task in any way. Inverse sentence frequency (ISF) is a stro"
Q18-1048,P15-2070,0,0.0778647,"Missing"
Q18-1048,D14-1162,0,0.0911904,"Missing"
Q18-1048,P15-1129,0,0.0166683,"arallelizable and takes only a few hours to apply to more than 100K predicates.1,2 Our experiments (§6) show that the global scores improve significantly over local scores and outperform state-of-the-art entailment graphs on two standard entailment rule data sets (Berant et al., 2011; Holt, 2018). We ultimately intend the typed entailment graphs to provide a resource for entailment and paraphrase rules for use in semantic parsing and open domain question answering, as has been done for similar resources such as the Paraphrase Database (PPDB; Ganitkevitch et al., 2013; Pavlick et al., 2015) in Wang et al. (2015) and Dong et al. (2017).3 With that end in view, we have included a comparison with PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within"
Q18-1048,Q14-1030,1,0.832844,"(if any). We thus type all entities that can be grounded in Wikipedia. We first map the Wikipedia URL of the entities to Freebase (Bollacker et al., 2008). We select the most notable type of the entity from Freebase and map it to FIGER types (Ling and Weld, 2012) such as building, disease, person, and location, using only the first level of the FIGER type hierarchy.4 For example, instead of event/sports_event, we use event as type. If an entity cannot be grounded in Wikipedia or its Freebase type does not have a mapping to FIGER, we assign the default type thing to it. The semantic parser of Reddy et al. (2014), GraphParser, is run on the NewsSpike corpus (Zhang and Weld, 2013) to extract binary relations between a predicate and its arguments from sentences. GraphParser uses CCG syntactic derivations and λ-calculus to convert sentences to neo-Davisonian semantics, a first-order logic that uses event identifiers (Parsons, 1990). For example, for the sentence, Obama visited Hawaii in 2012, GraphParser produces the logical form ∃e.visit1 (e, Obama) ∧ visit2 (e, Hawaii)∧ visitin (e, 2012), where e denotes an event. We will consider a relation for each pair of arguments, hence, there will be three relati"
Q18-1048,W03-1011,0,0.834164,"and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although"
Q18-1048,N13-1008,0,0.245356,"dge about the structure of the graph that are less expensive to incorporate and yield better results on entailment rule data sets. Abend et al. (2014) learn entailment relations over multi-word predicates with different levels of compositionality. Pavlick et al. (2015) add variety of relations, including entailment, to phrase pairs in PPDB. This includes a broader range of entailment relations such as lexical entailment. In contrast to our method, these works rely on supervised data and take a local learning approach. Another related strand of research is link prediction (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Yang et al., 2015; Trouillon et al., 2016; Dettmers et al., 2018), where the source data are extractions from text, facts in knowledge bases, or both. Unlike our work, which directly learns entailment relations between predicates, these methods aim at predicting the source data—that is, whether two entities have a particular relationship. The common Related Work Our work is closely related to Berant et al. (2011), where entailment graphs are learned by imposing transitivity constraints on the entailment relations. However, the exact solution to the problem is not scalabl"
Q18-1048,D10-1106,0,0.090144,"Missing"
Q18-1048,P12-2031,0,0.170474,"Missing"
Q18-1048,D13-1183,0,0.540702,"edia. We first map the Wikipedia URL of the entities to Freebase (Bollacker et al., 2008). We select the most notable type of the entity from Freebase and map it to FIGER types (Ling and Weld, 2012) such as building, disease, person, and location, using only the first level of the FIGER type hierarchy.4 For example, instead of event/sports_event, we use event as type. If an entity cannot be grounded in Wikipedia or its Freebase type does not have a mapping to FIGER, we assign the default type thing to it. The semantic parser of Reddy et al. (2014), GraphParser, is run on the NewsSpike corpus (Zhang and Weld, 2013) to extract binary relations between a predicate and its arguments from sentences. GraphParser uses CCG syntactic derivations and λ-calculus to convert sentences to neo-Davisonian semantics, a first-order logic that uses event identifiers (Parsons, 1990). For example, for the sentence, Obama visited Hawaii in 2012, GraphParser produces the logical form ∃e.visit1 (e, Obama) ∧ visit2 (e, Hawaii)∧ visitin (e, 2012), where e denotes an event. We will consider a relation for each pair of arguments, hence, there will be three relations for the given sentence: visit1,2 with arguments (Obama, Hawaii),"
Q18-1048,C08-1107,0,0.828734,"as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although they showed transitivity"
U13-1001,U13-1001,1,0.0513221,"Missing"
W04-3215,J99-2004,0,\N,Missing
W04-3215,A00-2018,0,\N,Missing
W04-3215,C04-1180,1,\N,Missing
W04-3215,C04-1041,1,\N,Missing
W04-3215,J03-4003,0,\N,Missing
W04-3215,P02-1043,1,\N,Missing
W04-3215,P02-1018,0,\N,Missing
W04-3215,briscoe-carroll-2002-robust,0,\N,Missing
W04-3215,P02-1042,1,\N,Missing
W04-3215,P03-1055,0,\N,Missing
W04-3215,P03-1046,0,\N,Missing
W04-3215,P04-1014,1,\N,Missing
W04-3215,W97-1505,0,\N,Missing
W05-0307,P98-1013,0,0.023939,"ve and are linked back to their head. The guidelines contain a decision tree the annotators use to establish priority in case more than one class is appropriate for a given entity. For example, if a mediated/general entity is also old/identity the latter is to be preferred to the former. Similar precedence relations hold among subtypes. To provide more robust and reliable clues in annotating bridging types (e.g. for distinguishing between poss and part), we provided replacement tests and referred to relations encoded in knowledge bases such as WordNet (Fellbaum, 1998) (for part) and FrameNet (Baker et al., 1998) (for situation). 3.2 Validation of the Scheme Three Switchboard dialogues (for a total of 1738 markables) were marked up by two different annotators for assessing the validity of the scheme. We evaluated annotation reliability by using the Kappa statistic (Carletta, 1996). Good quality annotation of discourse phenomena normally yields a kappa ( ) of about .80. We assessed the validity of the scheme on the four-way classification into the three main categories (old, mediated and new) and the nonapplicable category. We also evaluated the annotation including the subtypes. All cases where at lea"
W05-0307,W04-2707,0,0.0598777,"Missing"
W05-0307,carletta-etal-2004-using,1,0.897851,"versations (average six minutes), between speakers of American English, for three million words. The corpus is distributed as stereo speech signals with an orthographic transcription per channel time-stamped at the word level. A third of this is syntactically parsed as part of the Penn Treebank (Marcus et al., 1993) and has dialog act annotation (Shriberg et al., 1998). We used a subset of this. In adherence with current standards, we converted all the existing annotations, and are producing the new discourse annotations in a coherent multi-layered XML-conformant schema, using NXT technology (Carletta et al., 2004). 1 This allows us to search over and integrate information from the many layers of annotation, including the 1 Beside the NXT tools, we also used the TIGER Switchboard filter (Mengel and Lezius, 2000) for the XMLconversion. Using existing markup we automatically selected and filtered NPs to be annotated, excluding locative, directional, and adverbial NPs and disfluencies, and adding possessive pronouns. See (Nissim et al., 2004) for technical details. 45 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 45–52, c Ann Arbor, June 2005. 2005 Association for"
W05-0307,J93-2004,0,0.0243998,"Missing"
W05-0307,mengel-lezius-2000-xml,0,0.0399157,"-stamped at the word level. A third of this is syntactically parsed as part of the Penn Treebank (Marcus et al., 1993) and has dialog act annotation (Shriberg et al., 1998). We used a subset of this. In adherence with current standards, we converted all the existing annotations, and are producing the new discourse annotations in a coherent multi-layered XML-conformant schema, using NXT technology (Carletta et al., 2004). 1 This allows us to search over and integrate information from the many layers of annotation, including the 1 Beside the NXT tools, we also used the TIGER Switchboard filter (Mengel and Lezius, 2000) for the XMLconversion. Using existing markup we automatically selected and filtered NPs to be annotated, excluding locative, directional, and adverbial NPs and disfluencies, and adding possessive pronouns. See (Nissim et al., 2004) for technical details. 45 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 45–52, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics sound files. NXT tools can be easily customised to accommodate different layers of annotation users want to add, including data sets that have low-level annotations time-stamp"
W05-0307,nissim-etal-2004-annotation,1,0.715633,"we converted all the existing annotations, and are producing the new discourse annotations in a coherent multi-layered XML-conformant schema, using NXT technology (Carletta et al., 2004). 1 This allows us to search over and integrate information from the many layers of annotation, including the 1 Beside the NXT tools, we also used the TIGER Switchboard filter (Mengel and Lezius, 2000) for the XMLconversion. Using existing markup we automatically selected and filtered NPs to be annotated, excluding locative, directional, and adverbial NPs and disfluencies, and adding possessive pronouns. See (Nissim et al., 2004) for technical details. 45 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 45–52, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics sound files. NXT tools can be easily customised to accommodate different layers of annotation users want to add, including data sets that have low-level annotations time-stamped against a set of synchronized signals, multiple, crossing tree structures, and connection to external corpus resources such as gesture ontologies and lexicons (Carletta et al., 2004). 3 Information Status Information Status descr"
W05-0307,C98-1013,0,\N,Missing
W05-0307,J96-2004,0,\N,Missing
W05-0307,W99-0309,0,\N,Missing
W11-2916,J07-4002,0,0.0303435,"Missing"
W11-2916,D07-1101,0,0.0197279,"on some data sets, but not on others. A look at the dev. sets show that second-order features were active in 9 of the first 60 decisions in the WSJ, but in 0 of the first 60 in Brown. We speculate that use of word senses as features, taking after Stetina and Nagao (1997), might result in better generality across domains, but leave this to future work. [NP salad [PP with [NP dressing]]] The active features are hsalad, withi and hsalad, with, dressingi. This latter type of feature represents straightforward specialization of the concept used by higher-order dependencies for parsing, especially Carreras (2007), to the problem of prepositional phrases. Our estimates of θD1 and θD2 are arrived at using structured perceptron training (Collins, 2002). The models trained on all examples in our training section (i.e. those of type NP and of type VP). We pick the number of perceptron training iterations for each model by maximizing performance on a held-out set, using our parameter tuning split described in §3.3. We only use feature instances that have occurred at least twice in training. If Φ(x, d) is a feature vector characterizing (x, d), the perceptron algorithm will output a parameter vector θDi , an"
W11-2916,W95-0103,0,0.10579,"h is better than either is alone, and which outperforms the modern parser of Petrov and Klein (2007) by a significant margin. We show that, when learning from unlabelled data, it can be beneficial to model the generation of modifiers of a head collectively, rather than individually. Finally, we suggest that our pair of models will be interesting to combine using new techniques for discriminatively constraining EM. 1 He ate a salad [PP with a fork] [PP of plastic] Prepositional phrase attachment is an important sub-problem of parsing in and of itself. Structural heuristics perform poorly (cf., Collins and Brooks, 1995), and so lexical knowledge is crucial. Moreover, the highly lexicalized nature of prepositional phrase attachment makes it a kind of microcosm of the general problem of learning dependency structure, and so acts as a computationally less-demanding testing ground on which to try out learning techniques. We have endeavoured to approach the problem with a strategy that might be likely to generalize: a mix of generative and discriminative lexical models, trained using techniques that have worked for parsers. The main contributions of this paper are: • We compare the performance on the prepositiona"
W11-2916,W02-1001,0,0.0694339,"the WSJ, but in 0 of the first 60 in Brown. We speculate that use of word senses as features, taking after Stetina and Nagao (1997), might result in better generality across domains, but leave this to future work. [NP salad [PP with [NP dressing]]] The active features are hsalad, withi and hsalad, with, dressingi. This latter type of feature represents straightforward specialization of the concept used by higher-order dependencies for parsing, especially Carreras (2007), to the problem of prepositional phrases. Our estimates of θD1 and θD2 are arrived at using structured perceptron training (Collins, 2002). The models trained on all examples in our training section (i.e. those of type NP and of type VP). We pick the number of perceptron training iterations for each model by maximizing performance on a held-out set, using our parameter tuning split described in §3.3. We only use feature instances that have occurred at least twice in training. If Φ(x, d) is a feature vector characterizing (x, d), the perceptron algorithm will output a parameter vector θDi , and the “score” assigned to a pair (x, d) under this interpretation will be θDi · Φ(x, d), with the predicted derivation being the d with the"
W11-2916,H94-1048,0,0.198696,"e derivations. In terms of size, our WSJ2-21 set has 29, 750 examples. The GigaWord set has 8, 038, 001 examples. 2 However, we do appeal to the labels of a portion of the out-of-domain Brown data once, in order to fix a single experimental parameter, which is the number of iterations of EM to run on the unlabelled data, cf. note 6. 1 Our data sets extracted from the Penn Treebank will be available on request to those with the relevant license(s) to use Penn Treebank data. 131 3.4 Baseline Parser Berkeley 5 SM Berkeley 6 SM Much past work has tested on the 4-tuple, binary decision data set of Ratnaparkhi et al. (1994). This data does not have all of the information required by our approach, and is based on a preliminary version of the Penn Treebank (version 0.75), which is incomplete and difficult to work with. Thus, we could not compare our work directly with past work. WSJ Dev. Test 85.3 83.0 84.6 83.0 Brown Dev. Test 82.4 81.1 83.3 82.7 Table 1: Performance of the Berkeley parser on the prepositional phrase attachment task. The best scores on each data set will be our baseline. on each data set as the baseline on that data set.3 In order to evaluate our performance, then, we will compare our model again"
W11-2916,P05-1045,0,0.00463106,"formance on prepositional phrase attachment of the Berkeley parser (Petrov and Klein, 2007), which is popular, readily available, and essentially state-of-art among supervised parsing methods. And, as we said, this is precisely the technology that we use to process unlabelled data, so it makes sense that our model should improve upon this in order to be of any use. 3.5 Reduction of Open-Class Words In all experiments, all nouns and verbs were replaced by more general forms. If applicable, nouns were replaced by their NER label, either person, place or organization, using the NER classifier of Finkel et al. (2005). All numeral strings of two or four digits were replaced with a symbol representing year, and all other numeral strings were replaced with a symbol representing numeric value. A word not reduced in either of these ways was replaced by its stem using the stemmer designed by Minnen et al. (2001).4 Finally, this reduced form is paired with the category of the word c ∈ {NOUN, VERB} to distinguish uses of words that can either be nouns or verbs. We find that these reductions improve performance slightly and also reduce the size of the generative probability table. So, we need to evaluate the prepo"
W11-2916,P98-2177,0,0.103701,"Missing"
W11-2916,P05-1003,0,0.0286159,"the performance of the model that combines θDi 0 with θGCol 0 on our WSJ sections 2-4, with a simple search over values [0, .01, · · · , .99, 1]. The optimal values for the k i were k 1 = .70 and k 2 = .71. A Combination of Models The Combination We now have two models of prepositional phrase attachment. One is estimated from labelled data, and one from unlabelled data. Each performs well in isolation. But, we find that the combination of the two in a logarithmic opinion pool framework works better than either does alone. With roots in Bordley (1982), a logarithmic opinion pool, as defined in Smith et al. (2005), has the form plop (d|x) = 1 Y Z lop 6.2 pα (d|x)w a α In related work, Hinton (1999) describes a product of experts, i.e. multiple models trained to work together, so that an unweighted product, will be sensible. Petrov (2010) has success with the unweighted product of the scores of several similar parsing models. Smith et al. (2005) adjust weights by maximizing the likelihood of a labelled dataset. Here, we weight the component models so as to maximize performance on a held out set. Where i is either 1 or 2, let plop,i (d |x; θDi , θGCol ) = 1 · pDi (d |x; θDi )k i · pGCol (d |x; θGCol )1 −"
W11-2916,W01-0521,0,0.0145759,"ter tuning. Scoring Performance When evaluating a prediction procedure, we will give it a series of attachment problems and ask for the derivations. In most cases, the score we will focus on is what we can call the binary decision score, i.e., the percentage of the time in which the first prepositional phrase following a verb–directobject pair is attached correctly. In this case, we are reporting the same score as is typically reported on this task, so as to avoid introducing a new metric. To be clear, then, when scoring, in this way, We split up the Brown portion of the treebank similarly to Gildea (2001)—i.e. we split it into 10 sections such that the s’th sentence is assigned to section s mod 10. We then use sections 0-2 development test set, 4-6 for tuning, and sections 7-9 are left for final evaluation. Our divisions of the Penn Treebank are chosen to resemble the canonical training-test split for parsing, but we use more sections for testing, to obtain more reliable test scores, as there are far fewer decisions to test on in each section in our task, when compared to parsing. [VP set [NP rate [PP on [NP refund]] ] [PP at [NP 5 percent]]]] we only ask where [PP on [NP refund]] attaches, an"
W11-2916,W97-0109,0,0.0526708,"hould be of interest to those interested the use of lexical features for parsing. The models that use lexical features outperform the semi-lexical model of Petrov and Klein (2007). Prepositional phrase attachment may be one area where lexicalized models are especially important. We also see that the use of second-order features buys extra performance on some data sets, but not on others. A look at the dev. sets show that second-order features were active in 9 of the first 60 decisions in the WSJ, but in 0 of the first 60 in Brown. We speculate that use of word senses as features, taking after Stetina and Nagao (1997), might result in better generality across domains, but leave this to future work. [NP salad [PP with [NP dressing]]] The active features are hsalad, withi and hsalad, with, dressingi. This latter type of feature represents straightforward specialization of the concept used by higher-order dependencies for parsing, especially Carreras (2007), to the problem of prepositional phrases. Our estimates of θD1 and θD2 are arrived at using structured perceptron training (Collins, 2002). The models trained on all examples in our training section (i.e. those of type NP and of type VP). We pick the numbe"
W11-2916,C02-1004,0,0.0663202,"Missing"
W11-2916,P91-1030,0,0.356516,"Missing"
W11-2916,P00-1014,0,0.0716085,"Missing"
W11-2916,N10-1003,0,0.0208612,"Combination We now have two models of prepositional phrase attachment. One is estimated from labelled data, and one from unlabelled data. Each performs well in isolation. But, we find that the combination of the two in a logarithmic opinion pool framework works better than either does alone. With roots in Bordley (1982), a logarithmic opinion pool, as defined in Smith et al. (2005), has the form plop (d|x) = 1 Y Z lop 6.2 pα (d|x)w a α In related work, Hinton (1999) describes a product of experts, i.e. multiple models trained to work together, so that an unweighted product, will be sensible. Petrov (2010) has success with the unweighted product of the scores of several similar parsing models. Smith et al. (2005) adjust weights by maximizing the likelihood of a labelled dataset. Here, we weight the component models so as to maximize performance on a held out set. Where i is either 1 or 2, let plop,i (d |x; θDi , θGCol ) = 1 · pDi (d |x; θDi )k i · pGCol (d |x; θGCol )1 −k i Zi Results Table 4 compares our two combined models against our individuals models, and our Berkeley parser baseline. We see that the combined models outperform their component models on all tasks. That is, the LOP1st-order"
W11-2916,N07-1051,0,\N,Missing
W11-2916,J93-2004,0,\N,Missing
W11-2916,J93-1005,0,\N,Missing
W11-2916,C94-2195,0,\N,Missing
W11-2916,J03-4003,0,\N,Missing
W11-2916,C98-2172,0,\N,Missing
W11-2916,N09-1069,0,\N,Missing
W12-0903,W04-1301,0,0.0218099,"Missing"
W12-0903,E12-1024,1,0.879481,"Missing"
W12-1913,P11-1048,0,0.0470754,"todoulopoulos† , Sharon Goldwater‡ , Mark Steedman‡ School of Informatics University of Edinburgh † christos.c@ed.ac.uk ‡ {steedman,sgwater}@inf.ed.ac.uk 1 Motivation Most unsupervised dependency systems rely on gold-standard Part-of-Speech (PoS) tags, either directly, using the PoS tags instead of words, or indirectly in the back-off mechanism of fully lexicalized models (Headden et al., 2009). It has been shown in supervised systems that using a hierarchical syntactic structure model can produce competitive sequence models; in other words that a parser can be a good tagger (Li et al., 2011; Auli and Lopez, 2011; Cohen et al., 2011). This is unsurprising, as the parser uses a rich set of hierarchical features that enable it to look at a less localized environment than a PoS tagger which in most cases relies solely on local contextual features. However this interaction has not been shown for the unsupervised setting. To our knowledge, this work is the first to show that using dependencies for unsupervised PoS induction is indeed useful. 2 Iterated learning Although most unsupervised systems depend on gold-standard PoS information, they can also be used in a fully unsupervised pipeline. One reason for"
W12-1913,D11-1059,1,0.837648,"rvised PoS induction (Headden et al., 2008). As discussed in that paper (and also by Klein and Manning (2004)) the quality of the dependencies drops with the use of induced tags. One way of producing better PoS tags is to use the dependency parser’s output to influence the PoS inducer, thus turning the pipeline into a loop. The main difficulty of this approach is to find a way of incorporating dependency information into a PoS induction system. In previous work 96 BMMM BMMM DMV Gen. 0 BMMM DMV Gen. 1 Gen. 2 Figure 1: The iterated learning paradigm for inducing both PoS tags and dependencies. (Christodoulopoulos et al., 2011) we have described BMMM: a PoS induction system that makes it is easy to incorporate multiple features either at the type or token level. For the dependency induction system we chose the DMV model of Klein and Manning (2004) because of its simplicity and its popularity. Both systems are described briefly in section 3. Using these two systems we performed an iterated learning experiment. The term is borrowed from the language evolution literature meaning “the process by which the output of one individual’s learning becomes the input to other individuals’ learning” (Smith et al., 2003). Here we"
W12-1913,D11-1005,0,0.0742278,"n Goldwater‡ , Mark Steedman‡ School of Informatics University of Edinburgh † christos.c@ed.ac.uk ‡ {steedman,sgwater}@inf.ed.ac.uk 1 Motivation Most unsupervised dependency systems rely on gold-standard Part-of-Speech (PoS) tags, either directly, using the PoS tags instead of words, or indirectly in the back-off mechanism of fully lexicalized models (Headden et al., 2009). It has been shown in supervised systems that using a hierarchical syntactic structure model can produce competitive sequence models; in other words that a parser can be a good tagger (Li et al., 2011; Auli and Lopez, 2011; Cohen et al., 2011). This is unsurprising, as the parser uses a rich set of hierarchical features that enable it to look at a less localized environment than a PoS tagger which in most cases relies solely on local contextual features. However this interaction has not been shown for the unsupervised setting. To our knowledge, this work is the first to show that using dependencies for unsupervised PoS induction is indeed useful. 2 Iterated learning Although most unsupervised systems depend on gold-standard PoS information, they can also be used in a fully unsupervised pipeline. One reason for doing so is to use de"
W12-1913,N06-1041,0,0.224034,"Missing"
W12-1913,C08-1042,0,0.139855,"es that enable it to look at a less localized environment than a PoS tagger which in most cases relies solely on local contextual features. However this interaction has not been shown for the unsupervised setting. To our knowledge, this work is the first to show that using dependencies for unsupervised PoS induction is indeed useful. 2 Iterated learning Although most unsupervised systems depend on gold-standard PoS information, they can also be used in a fully unsupervised pipeline. One reason for doing so is to use dependency parsing as an extrinsic evaluation for unsupervised PoS induction (Headden et al., 2008). As discussed in that paper (and also by Klein and Manning (2004)) the quality of the dependencies drops with the use of induced tags. One way of producing better PoS tags is to use the dependency parser’s output to influence the PoS inducer, thus turning the pipeline into a loop. The main difficulty of this approach is to find a way of incorporating dependency information into a PoS induction system. In previous work 96 BMMM BMMM DMV Gen. 0 BMMM DMV Gen. 1 Gen. 2 Figure 1: The iterated learning paradigm for inducing both PoS tags and dependencies. (Christodoulopoulos et al., 2011) we have de"
W12-1913,N09-1012,0,0.304428,"Missing"
W12-1913,P04-1061,0,0.726608,"a PoS tagger which in most cases relies solely on local contextual features. However this interaction has not been shown for the unsupervised setting. To our knowledge, this work is the first to show that using dependencies for unsupervised PoS induction is indeed useful. 2 Iterated learning Although most unsupervised systems depend on gold-standard PoS information, they can also be used in a fully unsupervised pipeline. One reason for doing so is to use dependency parsing as an extrinsic evaluation for unsupervised PoS induction (Headden et al., 2008). As discussed in that paper (and also by Klein and Manning (2004)) the quality of the dependencies drops with the use of induced tags. One way of producing better PoS tags is to use the dependency parser’s output to influence the PoS inducer, thus turning the pipeline into a loop. The main difficulty of this approach is to find a way of incorporating dependency information into a PoS induction system. In previous work 96 BMMM BMMM DMV Gen. 0 BMMM DMV Gen. 1 Gen. 2 Figure 1: The iterated learning paradigm for inducing both PoS tags and dependencies. (Christodoulopoulos et al., 2011) we have described BMMM: a PoS induction system that makes it is easy to inco"
W12-1913,D11-1109,0,0.042809,"on Christos Christodoulopoulos† , Sharon Goldwater‡ , Mark Steedman‡ School of Informatics University of Edinburgh † christos.c@ed.ac.uk ‡ {steedman,sgwater}@inf.ed.ac.uk 1 Motivation Most unsupervised dependency systems rely on gold-standard Part-of-Speech (PoS) tags, either directly, using the PoS tags instead of words, or indirectly in the back-off mechanism of fully lexicalized models (Headden et al., 2009). It has been shown in supervised systems that using a hierarchical syntactic structure model can produce competitive sequence models; in other words that a parser can be a good tagger (Li et al., 2011; Auli and Lopez, 2011; Cohen et al., 2011). This is unsurprising, as the parser uses a rich set of hierarchical features that enable it to look at a less localized environment than a PoS tagger which in most cases relies solely on local contextual features. However this interaction has not been shown for the unsupervised setting. To our knowledge, this work is the first to show that using dependencies for unsupervised PoS induction is indeed useful. 2 Iterated learning Although most unsupervised systems depend on gold-standard PoS information, they can also be used in a fully unsupervised pip"
W14-2406,S13-1002,0,0.0340827,"beth). Their method has shown good performance on a dataset of multi-sentence textual inference problems involving quantifiers, by using first-order the1 The e variables are Davidsonian event variables. 28 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 28–32, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics indicate entailment relations. Exactly the same methods can be used to build entailment graphs over the predicates derived from a CCG parse: orem proving. Ambiguity is handled by a probabilistic model, based on the types of the nouns. Beltagy et al. (2013) use an alternative approach with similar goals, in which every word instance expresses a unique semantic primitive, but is connected to the meanings of other word instances using distributionally-derived probabilistic inference rules. This approach risks requiring very large number of inference rules, which may make inference inefficient. Our approach avoid this problem by attempting to fully represent lexical semantics in the lexicon. 2 1 4 2 bombarg0,arg1 invadearg0,arg1 invasionposs,of 3 Proposal conquerarg0,arg1 annexarg0,arg1 The graph can then be converted to a CCG lexicon by making the"
W14-2406,P11-1062,0,0.131019,"tics could be solved with a system based on three components: • A CCG syntactic parse for modelling composition. Using CCG allows us to handle interesting forms of composition, such as coordination, extraction, questions, right node raising, etc. CCG also has both a developed theory of operator semantics and a transparent interface to the underlying predicate argument structure. • A small hand built lexicon for words with complex semantics—such as negatives, quantifiers, modals, and implicative verbs. • A rich model of lexical semantics derived from distributionally-induced entailment graphs (Berant et al., 2011), extended with subcategories of entailment relations in a similar way to Scaria et al. (2013). We show how such graphs can be converted into a CCG lexicon. 2.1 attackarg0,arg1 2.2 Temporal Semantics One case where combining formal and distributional semantics may be particularly helpful is in giving a detailed model of temporal semantics. A rich understanding of time would allow us to understand when events took place, or when states were true. Most existing work ignores tense, and would treat the expressions used to be president and is president either as equivalent or completely unrelated."
W14-2406,W08-2222,0,0.0195325,"vectors based on collocations in large corpora, and then these vectors a composed into vectors representing longer utterances. However, so far there is relatively limited empirical evidence that composed vectors provide useful representations for whole sentences, and it is unclear how to represent logical operators (such as universal quantifiers) in a vector space. While future breakthroughs may overcome these limitations, there are already well developed solutions in the formal semantics literature using logical representations. On the other hand, standard formal semantic approaches such as Bos (2008) have Shakespeare wrote Macbeth NP (S NP )/NP NP shakespeare λyλxλe.rel43(x, y, e) macbeth &gt; S NP λxλe.rel43(x, macbeth, e) &lt; S λe.rel43(shakespeare, macbeth, e) This approach interacts seamlessly with standard formal semantics—for example modelling negation by mapping Francis Bacon didn’t write Macbeth to ¬rel43(f rancis bacon, macbeth). Their method has shown good performance on a dataset of multi-sentence textual inference problems involving quantifiers, by using first-order the1 The e variables are Davidsonian event variables. 28 Proceedings of the ACL 2014 Workshop on Semantic Parsing,"
W14-2406,J88-2003,1,0.489408,"as 1 2.3 initiated by will is used Here, r is the reference time (e.g. the time that the news article was written). It is easy to verify that such a lexicon supports inferences such as is visiting→will leave, has visited→has arrived in, or used to be president→is not president. The model described here only discusses tense, not aspect—so does not distinguish between John arrived in Baltimore and John has arrived in Baltimore (the latter says that the consequences of his arrival still hold—i.e. that he is still in Baltimore). Going further, we could implement the much more detailed proposal of Moens and Steedman (1988). Building this model would require distinguishing states from events—for example, the semantics of arrive, visit and leave could all be expressed in terms of the times that an is in state holds. visitarg0,arg1 3 arrivearg0,in reacharg0,arg1 leavearg0,arg1 exitarg0,arg1 departarg0,f rom Intensional Semantics Similar work could be done by subcategorizing edges in the graph with other lexical relations. For example, we could extend the graph with goal relations between words, such as between set out for and arrive in, search and find, or invade and conquer: terminated by 2 ` (SNP)/(Sb NP) : λp"
W14-2406,D13-1177,0,0.350102,"λyλxλe.rel43(x, y, e) author ` N/PPof : λyλxλe.rel43(x, y, e) Equivalent sentences like Shakespeare wrote Macbeth and Shakespeare is the author of Macbeth can then both be mapped to a rel43(shakespeare, macbeth) logical form, using derivations such as: We outline a vision for computational semantics in which formal compositional semantics is combined with a powerful, structured lexical semantics derived from distributional statistics. We consider how existing work (Lewis and Steedman, 2013) could be extended with a much richer lexical semantics using recent techniques for modelling processes (Scaria et al., 2013)—for example, learning that visiting events start with arriving and end with leaving. We show how to closely integrate this information with theories of formal semantics, allowing complex compositional inferences such as is visiting→has arrived in but will leave, which requires interpreting both the function and content words. This will allow machine reading systems to understand not just what has happened, but when. 1 Combined Distributional and Logical Semantics Distributional semantics aims to induce the meaning of language from unlabelled text. Traditional approaches to distributional sema"
W14-2406,D13-1183,0,0.109523,"ich sentences describe the same Directional Inference A major limitation of our previous model is that it uses a flat clustering to model the meaning of content words. This method enables them to model synonymy relations between words, but not relations where the entailment only holds in one direction—for example, conquers→invades, but not vice-versa. This problem can be addressed using the entailment graph framework introduced by Berant et al. (2011), which learns globally consistent graphs over predicates in which directed edges 29 events using date-stamped text and simple tense heuristics (Zhang and Weld, 2013). Such methods escape common problems with traditional approaches to distributional similarity, such as conflating causes with effects, and may prove very useful for building entailment graphs. Temporal information is conveyed by both by auxiliary verbs such as will or used to, and in the semantics of content words. For example, the statement John is visiting Baltimore licences entailments such as John has arrived in Baltimore and John will leave Baltimore, which can only be understood through both knowledge of tense and lexical semantic relations. The requisite information about lexical seman"
W14-2406,Q13-1015,1,\N,Missing
W14-2406,2014.lilt-9.5,0,\N,Missing
W15-2610,E14-1014,1,0.793867,"rams centered on it, along with the corresponding ngrams for the existing lexicon. A reasonable direction for future work would be to develop the way we select the contexts on which our distributional representations are based. In particular, it would make sense to exploit the approach of Brent (1991) and Manning (1993) in which these contexts have an a priori linguistic association with particular syntactic frames, as opposed to a merely empirical association deriving from a k-nearest-neighbour model. existing model parameters to the statistics of the new domain, such as self-training (e.g., Deoskar et al., 2014), we expect further improvements to be achievable. Nonetheless, there were substantial variations in the strength of the improvement attained, with the weak performance of the Berkeley Parser being a notable disappointment. Several differences could be invoked to explain this shortfall. Firstly, the Berkeley Parser has a strong OOV process, and it may just be difficult to beat the estimates it produces, without seeing gold standard data. Secondly, it is a generative rather than a discriminative model, and this complicates the process of modifying the lexicon with questions of how much probabil"
W15-2610,P91-1027,0,0.647721,"am, 3gram and 4gram contexts that are intended to emphasise syntactic - as opposed to semantic - characteristics, following the structure of templates and frames proposed by e.g. Cartwright and Brent (1997), Mintz (2003) and Redington et al. (1998). Thus our 2gram contexts have two forms that distinguish occurrence on the left from occurrence on the right: hlef t token XXXi and hXXX right tokeni. The 3gram contexts are equivalent to Mintz’s (2003) frequent frames: hlef t token XXX right tokeni. And the 4gram contexts extend this frame to the right, mimicking the form of templates described by Brent (1991) and Cartwright and Brent (1997): hlef t token XXX right token1 right token2 i. The BOW approach ignores the sequential information contained in the ngram contexts and relies instead on counts of individual words that occur anywhere in 5 word-windows each side of a target word. In each case, we built distributional vectors using the most common of these contexts, with vector components based on a ratio of probabilities. vi = f reqi,t · f reqtotal p(ci |wt ) = p(ci ) f reqi · f reqt Collobert et al. (2011) trained a neural net language model on a snapshot of the English Wikipedia (≈ 631M words)"
W15-2610,1993.eamt-1.1,0,0.555675,"Missing"
W15-2610,J07-3004,1,0.671979,"s count according to the induced tag and sub-tag probabilities. In fact, our attempts to use KNN to induce probabilities over the sub-categories below the level of POS tags were fruitless, producing worse results than the original model in all experiments. Thus, we resorted to using the KNN approach to induce POS level probabilities and then basing the lower level probabilities on a 50-50 interpolation of a general profile for each POS tag and the probabilities assigned by the OOV process. 3.2 The C&C parser6 (Curran et al., 2007) is a discriminative parser, which has been trained on CCGbank (Hockenmaier and Steedman, 2007), a translation of the Penn Treebank into the CCG formalism. Roughly, the parser can be split into three modules: a POS-tagger, a super-tagger and the parser itself. The POS-tagger assigns fixed POS tags to the text to be parsed, based on a window of five words centred on the word to be tagged. The super-tagger takes these POS tags and words as input and, using the same five token window, passes CCG tags to the parser. The parser in turn tries to build a derivation from the CCG tags it has been given, but can request a re-analysis from the super-tagger if this fails. Each module uses a log-lin"
W15-2610,P01-1017,0,0.0493346,"015 Association for Computational Linguistics. tional level of detail. The first approach simply makes use of finer-grained syntactic categories, either instead of or in addition to POS tags (Steedman, 2000; Klein and Manning, 2003b; Petrov et al., 2006). These categories can then determine the missing information about the dependencies a word will take part in, such as whether a verb is intransitive or whether it takes prepositional arguments. The second approach instead increases the granularity of the production rules, by conditioning the probabilities on the heads of the phrases involved (Charniak, 2001; Collins, 2003). In this way, words are associated with probabilities for the structure of phrases that they head, determining, for example, the types of object that a verb phrase expands into. Although the two approaches are compatible, a significant difference makes the former more conducive to our purposes. Enhancing the granularity of the syntactic categories results in a much richer lexicon containing more information about how words behave syntactically. In principle, this should lead to an enlargement of the lexicon having a greater impact on performance by itself. In the latter approa"
W15-2610,N03-1016,0,0.671007,"er them, and this in turn requires knowing what sort of relations each word is likely to enter into. Unfortunately, gold standard training data, annotated with these syntactic relations, is generally in short supply. The vocabulary 79 Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis (Louhi), pages 79–89, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. tional level of detail. The first approach simply makes use of finer-grained syntactic categories, either instead of or in addition to POS tags (Steedman, 2000; Klein and Manning, 2003b; Petrov et al., 2006). These categories can then determine the missing information about the dependencies a word will take part in, such as whether a verb is intransitive or whether it takes prepositional arguments. The second approach instead increases the granularity of the production rules, by conditioning the probabilities on the heads of the phrases involved (Charniak, 2001; Collins, 2003). In this way, words are associated with probabilities for the structure of phrases that they head, determining, for example, the types of object that a verb phrase expands into. Although the two appro"
W15-2610,J03-4003,0,0.0445563,"for Computational Linguistics. tional level of detail. The first approach simply makes use of finer-grained syntactic categories, either instead of or in addition to POS tags (Steedman, 2000; Klein and Manning, 2003b; Petrov et al., 2006). These categories can then determine the missing information about the dependencies a word will take part in, such as whether a verb is intransitive or whether it takes prepositional arguments. The second approach instead increases the granularity of the production rules, by conditioning the probabilities on the heads of the phrases involved (Charniak, 2001; Collins, 2003). In this way, words are associated with probabilities for the structure of phrases that they head, determining, for example, the types of object that a verb phrase expands into. Although the two approaches are compatible, a significant difference makes the former more conducive to our purposes. Enhancing the granularity of the syntactic categories results in a much richer lexicon containing more information about how words behave syntactically. In principle, this should lead to an enlargement of the lexicon having a greater impact on performance by itself. In the latter approach, of lexicalis"
W15-2610,P03-1054,0,0.261113,"er them, and this in turn requires knowing what sort of relations each word is likely to enter into. Unfortunately, gold standard training data, annotated with these syntactic relations, is generally in short supply. The vocabulary 79 Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis (Louhi), pages 79–89, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. tional level of detail. The first approach simply makes use of finer-grained syntactic categories, either instead of or in addition to POS tags (Steedman, 2000; Klein and Manning, 2003b; Petrov et al., 2006). These categories can then determine the missing information about the dependencies a word will take part in, such as whether a verb is intransitive or whether it takes prepositional arguments. The second approach instead increases the granularity of the production rules, by conditioning the probabilities on the heads of the phrases involved (Charniak, 2001; Collins, 2003). In this way, words are associated with probabilities for the structure of phrases that they head, determining, for example, the types of object that a verb phrase expands into. Although the two appro"
W15-2610,P08-1068,0,0.0763769,"Missing"
W15-2610,P07-2009,0,0.0215809,"ies are smoothed, we assigned all new vocabulary a count of 101, and partitioned this count according to the induced tag and sub-tag probabilities. In fact, our attempts to use KNN to induce probabilities over the sub-categories below the level of POS tags were fruitless, producing worse results than the original model in all experiments. Thus, we resorted to using the KNN approach to induce POS level probabilities and then basing the lower level probabilities on a 50-50 interpolation of a general profile for each POS tag and the probabilities assigned by the OOV process. 3.2 The C&C parser6 (Curran et al., 2007) is a discriminative parser, which has been trained on CCGbank (Hockenmaier and Steedman, 2007), a translation of the Penn Treebank into the CCG formalism. Roughly, the parser can be split into three modules: a POS-tagger, a super-tagger and the parser itself. The POS-tagger assigns fixed POS tags to the text to be parsed, based on a window of five words centred on the word to be tagged. The super-tagger takes these POS tags and words as input and, using the same five token window, passes CCG tags to the parser. The parser in turn tries to build a derivation from the CCG tags it has been given"
W15-2610,D14-1107,1,0.918205,"Missing"
W15-2610,P11-1061,0,0.0376523,"Missing"
W15-2610,de-marneffe-etal-2006-generating,0,0.0777287,"Missing"
W15-2610,W07-1004,0,0.0275335,"Missing"
W15-2610,E06-1011,0,0.14579,"Missing"
W15-2610,N07-1051,0,0.150654,"Missing"
W15-2610,P06-1055,0,0.19934,"requires knowing what sort of relations each word is likely to enter into. Unfortunately, gold standard training data, annotated with these syntactic relations, is generally in short supply. The vocabulary 79 Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis (Louhi), pages 79–89, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. tional level of detail. The first approach simply makes use of finer-grained syntactic categories, either instead of or in addition to POS tags (Steedman, 2000; Klein and Manning, 2003b; Petrov et al., 2006). These categories can then determine the missing information about the dependencies a word will take part in, such as whether a verb is intransitive or whether it takes prepositional arguments. The second approach instead increases the granularity of the production rules, by conditioning the probabilities on the heads of the phrases involved (Charniak, 2001; Collins, 2003). In this way, words are associated with probabilities for the structure of phrases that they head, determining, for example, the types of object that a verb phrase expands into. Although the two approaches are compatible, a"
W15-2610,D10-1017,0,0.0526619,"Missing"
W15-2610,D14-1101,0,0.0133374,"oncrete occurrences in specific contexts, neural networks have recently become popular as a means to create more abstract representations. In this case, as the network adapts to the data, representations are learned that embed discrete inputs in a continuous space defined by its internal states. Researchers have been interested in the nature of such internal representations for some time (e.g., Small et al., 1995; Joanisse and Seidenberg, 1999). However, it has now become practical to induce such embeddings from large quantities of text and employ them in linguistic applications. For example, Tsuboi (2014) and Collobert et al. (2011) apply neural representations to POS tagging, and this suggests that at least some useful information about the syntax of unseen words might be gained from this source. 2 Approach Our approach is based on the assumption that words with similar syntactic properties should have similar distributional characteristics. We evaluate both neural embeddings and also raw context frequencies as the basis for measuring distributional similarity. These context vectors have components which correspond to occurrences within a corpus of raw biomedical text and we employ both SENNA"
W15-2610,N10-1123,0,0.0205992,"Missing"
W15-2610,P10-1040,0,0.155977,"Missing"
W15-2610,H91-1067,0,\N,Missing
W15-2610,P93-1032,0,\N,Missing
W15-2610,P14-2133,0,\N,Missing
W15-2610,P14-2131,0,\N,Missing
W19-0409,W14-0702,0,0.0191426,"nce is Pavlick and Callison-Burch (2016), who leverage implicative verbs to determine that managed to solve X |= X is solved. Our proposed dataset TEA fills a gap in the natural language inference evaluation repertoire by focusing on temporal and aspectual entailment. Recent years saw the release of a number of large-scale datasets, such as SNLI (Bowman et al., 2015), MNLI (Williams et al., 2017) or DNC (Poliak et al., 2018), but neither of these datasets focuses on, or includes a substantial proportion of, inference examples between temporal predications. TEA is related to work on causality (Mirza et al., 2014; Mirza and Tonelli, 2014), however our dataset has been created from scratch rather than derived from TimeBank (Pustejovsky et al., 2003), as for example explicit buys |= owns relations are rarely encountered in the same paragraph or connected by explicit causal links. Therefore, TEA captures many consequent state inferences that are missing from previous datasets. The most closely related task to TEA is the relation inference dataset of Levy and Dagan (2016), which however, contains only very few examples where temporality is a governing factor. 7 Future Work In future work we plan to levera"
W19-0409,C14-1198,0,0.0279371,"allison-Burch (2016), who leverage implicative verbs to determine that managed to solve X |= X is solved. Our proposed dataset TEA fills a gap in the natural language inference evaluation repertoire by focusing on temporal and aspectual entailment. Recent years saw the release of a number of large-scale datasets, such as SNLI (Bowman et al., 2015), MNLI (Williams et al., 2017) or DNC (Poliak et al., 2018), but neither of these datasets focuses on, or includes a substantial proportion of, inference examples between temporal predications. TEA is related to work on causality (Mirza et al., 2014; Mirza and Tonelli, 2014), however our dataset has been created from scratch rather than derived from TimeBank (Pustejovsky et al., 2003), as for example explicit buys |= owns relations are rarely encountered in the same paragraph or connected by explicit causal links. Therefore, TEA captures many consequent state inferences that are missing from previous datasets. The most closely related task to TEA is the relation inference dataset of Levy and Dagan (2016), which however, contains only very few examples where temporality is a governing factor. 7 Future Work In future work we plan to leverage tense- and aspect-based"
W19-0409,P15-1126,1,0.805738,"vs. non-past loves). It establishes a point of reference that allows the temporal organisation of events in a discourse. In English, tense interacts with aspectual auxiliaries such as the verbs be or have that influence the internal constituency of a predication, and determine whether an event is completed or ongoing. Tense and aspect therefore control the internal and external temporal structure of an event and govern the inferences that a predication licenses (Reichenbach, 1947; Dahl, 1985; Steedman, 1997). There is evidence that such morphology is represented in distributional embeddings (Mitchell and Steedman, 2015; Vylomova et al., 2016). In this paper we are concerned with perfect and progressive aspect, but do not focus on any other types of aspect such as the Aktionsart of a predication (Vendler, 1957), which we leave to future work. 2.1 The Interaction between Temporality and Entailment Perfect aspect (typically) describes events as a completed whole, and licenses inferences regarding the consequences of that event. The use of different tenses and aspects for past events influences their relevance to the present moment and thereby their entailment behaviour. For example, the consequences of an even"
W19-0409,J88-2003,1,0.664634,"aspect such as the Aktionsart of a predication (Vendler, 1957), which we leave to future work. 2.1 The Interaction between Temporality and Entailment Perfect aspect (typically) describes events as a completed whole, and licenses inferences regarding the consequences of that event. The use of different tenses and aspects for past events influences their relevance to the present moment and thereby their entailment behaviour. For example, the consequences of an event in the present perfect hold at the time of utterance, whereas events in the simple past or the past perfect do not (Comrie, 1985; Moens and Steedman, 1988; Depraetere, 1998; Katz, 2003). This is shown in sentences (3) and (4), where only sentence (3) licenses the inference of Elizabeth being in Meryton now. (3) Elizabeth has gone to Meryton. |= Elizabeth is in Meryton now. (4) Elizabeth went / had gone to Meryton. 6|= Elizabeth is in Meryton now. This property can be explained through a Reichenbachian view of the present perfect, where the point of reference coincides with the point of speech, thereby indicating its current relevance (Reichenbach, 1947). On the other hand, events in the past simple or the past perfect license inferences for con"
W19-0409,C18-1252,0,0.0417727,"Missing"
W19-0409,D16-1240,0,0.0295234,"ing primarily governed by distributional similarity. 6 Related Work Most previous work on inference between verbs was concerned with extracting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004, 2007; Hashimoto et al., 2009; Melamud et al., 2013). As a next step, Berant et al. (2010) and Hosseini et al. (2018) leverage these rules to build entailment graphs for modelling natural language inference. However in both cases the entailment graphs are built on the basis of verb lemmas and do not take tense and aspect into account. One example of using tense for inference is Pavlick and Callison-Burch (2016), who leverage implicative verbs to determine that managed to solve X |= X is solved. Our proposed dataset TEA fills a gap in the natural language inference evaluation repertoire by focusing on temporal and aspectual entailment. Recent years saw the release of a number of large-scale datasets, such as SNLI (Bowman et al., 2015), MNLI (Williams et al., 2017) or DNC (Poliak et al., 2018), but neither of these datasets focuses on, or includes a substantial proportion of, inference examples between temporal predications. TEA is related to work on causality (Mirza et al., 2014; Mirza and Tonelli, 2"
W19-0409,N18-1202,0,0.297145,"temporal structure of predications when performing natural language inference. To the best of our knowledge, this is the first dataset that is primarily focused on assessing natural language inference between temporally and aspectually modified predications. 1 For brevity we will refer to predications with different tenses and aspectual auxiliaries as temporal predications. As a first evaluation on our new dataset we compare to what extent five distributional embedding models, word2vec (Mikolov et al., 2013), Anchored Packed Trees (Weir et al., 2016), fastText (Bojanowski et al., 2017), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2018), and two bi-directional LSTM (biLSTM) encoders, pre-trained on SNLI (Bowman et al., 2015) and DNC (Poliak et al., 2018), respectively, are able to perform natural language inference on temporal predications. In our evaluation, we refrain from fine-tuning any of the models as our goal is to assess to what extent tense and aspect are captured in these models per se. As a pre-requisite diagnostic task for natural language inference between temporal predications we analysed whether the models encode the morphosyntax of tense and aspect and found that they capture a"
W19-0409,W18-5441,0,0.0930582,"Missing"
W19-0409,W17-4908,0,0.0140724,"tion 2 shows the setup where we use a neural network to learn a translation matrix from infinitive forms to inflected forms, where f is a tense-specific neural network with a single hidden layer, that takes an unseen lemma representation x0` as input and generates an inflected form x0t , and where Θt represent the learnable parameters of the network. ot = 1X xt − x` n x0t = f (x0` ; Θt ) (1) (2) x∈S We subsequently evaluate whether the correctly inflected verb is in the nearest neighbour list of the generated verb. The inflected verb generation setup is inspired by Bolukbasi et al. (2016) and Shoemark et al. (2017), who used a similar method in their respective works. For the dataset, we extracted verbs from the OBWB corpus where each inflected verb form occurred at least 50 times, resulting in ≈2.8k verbs per tense. 4.3 Entailment with Temporal Predications Lastly, we propose TEA — the Temporal Entailment Assessment dataset. TEA contains pairs of short sentences with the same argument structure that differ in tense and aspect of the main verb, and follows a binary label annotation scheme (entailment vs. non-entailment). Example sentences from TEA are shown in Table 1. The absence and infeasibility of c"
W19-0409,C08-1107,0,0.178484,"l information in their respective test sets. We therefore additionally cast TEA as a binary classification task, and report accuracy and macro-averaged F1-score for the two pre-trained biLSTM models. Table 3 shows the average precision scores for the models and the accuracy and F1-scores for the two pre-trained biLSTMs in comparison to a majority class baseline and a baseline predicting the majority class per tense pair. We used cosine as similarity measure for the embedding models and the softmax prediction scores for the biLSTMs. For A PTs, we also tried the asymmetric inclusion score BInc (Szpektor and Dagan, 2008), however found cosine working better. We furthermore experimented with distributional inference (Kober et al., 2016), and found a small positive impact on recall but a slightly larger negative dip in precision, which overall led to slightly lower average precision scores. The results show Model word2vec A PT fastText ELMo BERT biLSTM-DNC biLSTM-SNLI Maj. class Maj. class / tense pair Avg. Precision 0.31 0.28 0.30 0.21 0.27 0.22 0.21 0.22 0.35 Accuracy 0.58 0.51 0.78 0.80 F1-Score 0.49 0.47 0.44 0.66 Table 3: TEA results. All model results are significantly worse at the p < 0.01 level w.r.t. t"
W19-0409,P07-1058,0,0.123595,"Missing"
W19-0409,W04-3206,0,0.0583762,"onal similarity overwhelms any finer distinction that the models might have extracted. While our analysis indicates that the embedding models are able to extract knowledge about tense and aspect, the signal is not strong enough to reliably perform inference. A potential avenue for future work would therefore be the development of models that are able to better represent tense and aspect, while not being primarily governed by distributional similarity. 6 Related Work Most previous work on inference between verbs was concerned with extracting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004, 2007; Hashimoto et al., 2009; Melamud et al., 2013). As a next step, Berant et al. (2010) and Hosseini et al. (2018) leverage these rules to build entailment graphs for modelling natural language inference. However in both cases the entailment graphs are built on the basis of verb lemmas and do not take tense and aspect into account. One example of using tense for inference is Pavlick and Callison-Burch (2016), who leverage implicative verbs to determine that managed to solve X |= X is solved. Our proposed dataset TEA fills a gap in the natural language inference evaluation repertoire by foc"
W19-0409,J17-4004,0,0.0665975,"Missing"
W19-0409,N18-1103,0,0.055889,"Missing"
W19-0409,P16-1158,0,0.0192832,"blishes a point of reference that allows the temporal organisation of events in a discourse. In English, tense interacts with aspectual auxiliaries such as the verbs be or have that influence the internal constituency of a predication, and determine whether an event is completed or ongoing. Tense and aspect therefore control the internal and external temporal structure of an event and govern the inferences that a predication licenses (Reichenbach, 1947; Dahl, 1985; Steedman, 1997). There is evidence that such morphology is represented in distributional embeddings (Mitchell and Steedman, 2015; Vylomova et al., 2016). In this paper we are concerned with perfect and progressive aspect, but do not focus on any other types of aspect such as the Aktionsart of a predication (Vendler, 1957), which we leave to future work. 2.1 The Interaction between Temporality and Entailment Perfect aspect (typically) describes events as a completed whole, and licenses inferences regarding the consequences of that event. The use of different tenses and aspects for past events influences their relevance to the present moment and thereby their entailment behaviour. For example, the consequences of an event in the present perfect"
W19-0409,J16-4006,1,0.834733,"uires models to correctly determine the internal and external temporal structure of predications when performing natural language inference. To the best of our knowledge, this is the first dataset that is primarily focused on assessing natural language inference between temporally and aspectually modified predications. 1 For brevity we will refer to predications with different tenses and aspectual auxiliaries as temporal predications. As a first evaluation on our new dataset we compare to what extent five distributional embedding models, word2vec (Mikolov et al., 2013), Anchored Packed Trees (Weir et al., 2016), fastText (Bojanowski et al., 2017), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2018), and two bi-directional LSTM (biLSTM) encoders, pre-trained on SNLI (Bowman et al., 2015) and DNC (Poliak et al., 2018), respectively, are able to perform natural language inference on temporal predications. In our evaluation, we refrain from fine-tuning any of the models as our goal is to assess to what extent tense and aspect are captured in these models per se. As a pre-requisite diagnostic task for natural language inference between temporal predications we analysed whether the models encode th"
W19-0409,D12-1018,0,0.0588485,"Missing"
W19-0409,P13-1131,0,\N,Missing
W19-0409,W14-2406,1,\N,Missing
W19-0409,P15-1162,0,\N,Missing
W19-0409,Q17-1010,0,\N,Missing
W19-0409,Q16-1037,0,\N,Missing
W19-0409,W17-1910,1,\N,Missing
W19-0409,W18-2501,0,\N,Missing
W19-0409,D18-1007,0,\N,Missing
W19-0409,Q18-1048,1,\N,Missing
W19-0409,N18-1101,0,\N,Missing
W19-0409,W04-3205,0,\N,Missing
W19-0409,P10-1124,0,\N,Missing
W89-0217,E89-1002,0,0.357278,"Missing"
W89-0217,P83-1020,0,0.0534407,"Missing"
W89-0217,P87-1012,1,0.863487,") 3 / (3IP ) -------------------------------->B -------------------------------->B 3 / IP 3/IP ----------------------------------------------------------------- 1 3/IP Intonation in a CCG. Inspection of the above examples shows that Combinatory grammars embody an unusual view of surface structure, according to which strings like Betty might eat are constituents. In fact, according to this view, surface structure is a much more ambiguous affair than is generally realised, for they must also be possible constituents of non-coordinate sentences like Betty might eat the mushrooms, as well. (See [11] and [19] for a discussion of the obvious problems that this fact engenders for parsing written text.) An entirely unconstrained combinatory grammar would in fact allow more or less any bracketing on a sentence. However, the actual grammars we write for configurational languages like English are heavily constrained by local conditions. (An example would be a condition on the composition rule that is tacitly assumed here, forbidding the variable Y in the composition rule to be instantiated as NP, thus excluding constituents like *[eat th e]y p /^). The claim of the present paper is simply that"
W89-0217,P87-1011,0,0.591858,"IP ) -------------------------------->B -------------------------------->B 3 / IP 3/IP ----------------------------------------------------------------- 1 3/IP Intonation in a CCG. Inspection of the above examples shows that Combinatory grammars embody an unusual view of surface structure, according to which strings like Betty might eat are constituents. In fact, according to this view, surface structure is a much more ambiguous affair than is generally realised, for they must also be possible constituents of non-coordinate sentences like Betty might eat the mushrooms, as well. (See [11] and [19] for a discussion of the obvious problems that this fact engenders for parsing written text.) An entirely unconstrained combinatory grammar would in fact allow more or less any bracketing on a sentence. However, the actual grammars we write for configurational languages like English are heavily constrained by local conditions. (An example would be a condition on the composition rule that is tacitly assumed here, forbidding the variable Y in the composition rule to be instantiated as NP, thus excluding constituents like *[eat th e]y p /^). The claim of the present paper is simply that particula"
W90-0125,C90-2068,1,\N,Missing
Y07-1038,Y00-1022,0,0.05453,"Missing"
Y14-1001,E12-1024,1,\N,Missing
Y14-1001,W10-2903,0,\N,Missing
Y14-1001,N13-1008,0,\N,Missing
Y14-1001,W02-1001,0,\N,Missing
Y14-1001,D07-1071,0,\N,Missing
Y14-1001,S13-1002,0,\N,Missing
Y14-1001,W14-2406,1,\N,Missing
Y14-1001,P07-1121,0,\N,Missing
Y14-1001,Q13-1015,1,\N,Missing
Y14-1001,Q14-1030,1,\N,Missing
Y14-1001,D08-1082,0,\N,Missing
Y14-1001,W11-0112,0,\N,Missing
Y14-1001,P11-1060,0,\N,Missing
Y14-1001,P13-1042,0,\N,Missing
Y14-1001,P12-1013,0,\N,Missing
Y14-1001,D11-1131,0,\N,Missing
Y14-1001,D11-1140,1,\N,Missing
Y14-1001,P14-1133,0,\N,Missing
Y14-1001,S13-1045,0,\N,Missing
Y14-1001,D13-1161,0,\N,Missing
Y14-1001,D13-1064,1,\N,Missing
Y14-1001,D13-1160,0,\N,Missing
Y14-1001,W12-3016,0,\N,Missing
Y14-1001,P13-3000,0,\N,Missing
Y14-1001,P13-4000,0,\N,Missing
Y14-1001,P13-5000,0,\N,Missing
Y14-1001,P13-1000,0,\N,Missing
