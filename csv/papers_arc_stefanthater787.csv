S18-1119,{S}em{E}val-2018 Task 11: Machine Comprehension Using Commonsense Knowledge,2018,0,45,4,1,25223,simon ostermann,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This report summarizes the results of the SemEval 2018 task on machine comprehension using commonsense knowledge. For this machine comprehension task, we created a new corpus, MCScript. It contains a high number of questions that require commonsense knowledge for finding the correct answer. 11 teams from 4 different countries participated in this shared task, most of them used neural approaches. The best performing system achieves an accuracy of 83.95{\%}, outperforming the baselines by a large margin, but still far from the human upper bound, which was found to be at 98{\%}."
L18-1512,Mapping Texts to Scripts: An Entailment Study,2018,0,1,3,1,25223,simon ostermann,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1564,{MCS}cript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge,2018,21,11,4,1,25223,simon ostermann,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We introduce a large dataset of narrative texts and questions about these texts, intended to be used in a machine comprehension task that requires reasoning using commonsense knowledge. Our dataset complements similar datasets in that we focus on stories about everyday activities, such as going to the movies or working in the garden, and that the questions require commonsense knowledge, or more specifically, script knowledge, to be answered. We show that our mode of data collection via crowdsourcing results in a substantial amount of such inference questions. The dataset forms the basis of a shared task on commonsense and script knowledge organized at SemEval 2018 and provides challenging test cases for the broader natural language understanding community."
W17-0901,Inducing Script Structure from Crowdsourced Event Descriptions via Semi-Supervised Clustering,2017,22,4,3,1,24460,lilian wanzare,"Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics",0,"We present a semi-supervised clustering approach to induce script structure from crowdsourced descriptions of event sequences by grouping event descriptions into paraphrase sets (representing event types) and inducing their temporal order. Our approach exploits semantic and positional similarity and allows for flexible event order, thus overcoming the rigidity of previous approaches. We incorporate crowdsourced alignments as prior knowledge and show that exploiting a small number of alignments results in a substantial improvement in cluster quality over state-of-the-art models and provides an appropriate basis for the induction of temporal order. We also show a coverage study to demonstrate the scalability of our approach."
S17-1015,A Mixture Model for Learning Multi-Sense Word Embeddings,2017,40,10,4,0,22809,dai nguyen,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"Word embeddings are now a standard technique for inducing meaning representations for words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks."
S17-1016,Aligning Script Events with Narrative Texts,2017,24,0,3,1,25223,simon ostermann,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"Script knowledge plays a central role in text understanding and is relevant for a variety of downstream tasks. In this paper, we consider two recent datasets which provide a rich and general representation of script events in terms of paraphrase sets. We introduce the task of mapping event mentions in narrative texts to such script event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible."
I17-2007,Sequence to Sequence Learning for Event Prediction,2017,17,1,4,0,22809,dai nguyen,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"This paper presents an approach to the task of predicting an event description from a preceding sentence in a text. Our approach explores sequence-to-sequence learning using a bidirectional multi-layer recurrent neural network. Our approach substantially outperforms previous work in terms of the BLEU score on two datasets derived from WikiHow and DeScript respectively. Since the BLEU score is not easy to interpret as a measure of event prediction, we complement our study with a second evaluation that exploits the rich linguistic annotation of gold paraphrase sets of events."
W16-2608,{U}d{S}-(retrain|distributional|surface): Improving {POS} Tagging for {OOV} Words in {G}erman {CMC} and Web Data,2016,7,1,3,0,2245,jakob prange,Proceedings of the 10th Web as Corpus Workshop,0,We present in this paper our three system submissions for the POS tagging subtask of the Empirist Shared Task: Our baseline systemUdS-retrain extends a standard training dataset with in-domain training data; UdSdistributional and UdS-surface add two different ways of handling OOV words on top of the baseline system by using either distributional information or a combination of surface similarity and language model information. We reach the best performance using the distributional model.
L16-1030,Improving {POS} Tagging of {G}erman Learner Language in a Reading Comprehension Scenario,2016,0,0,3,0,34758,lena keiper,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a novel method to automatically improve the accurracy of part-of-speech taggers on learner language. The key idea underlying our approach is to exploit the structure of a typical language learner task and automatically induce POS information for out-of-vocabulary (OOV) words. To evaluate the effectiveness of our approach, we add manual POS and normalization information to an existing language learner corpus. Our evaluation shows an increase in accurracy from 72.4{\%} to 81.5{\%} on OOV words."
L16-1135,A Corpus of Literal and Idiomatic Uses of {G}erman Infinitive-Verb Compounds,2016,0,1,7,0,657,andrea horbach,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present an annotation study on a representative dataset of literal and idiomatic uses of German infinitive-verb compounds in newspaper and journal texts. Infinitive-verb compounds form a challenge for writers of German, because spelling regulations are different for literal and idiomatic uses. Through the participation of expert lexicographers we were able to obtain a high-quality corpus resource which offers itself as a testbed for automatic idiomaticity detection and coarse-grained word-sense disambiguation. We trained a classifier on the corpus which was able to distinguish literal and idiomatic uses with an accuracy of 85 {\%}."
L16-1270,Unsupervised Ranked Cross-Lingual Lexical Substitution for Low-Resource Languages,2016,0,0,3,0,34999,stefan ecker,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We propose an unsupervised system for a variant of cross-lingual lexical substitution (CLLS) to be used in a reading scenario in computer-assisted language learning (CALL), in which single-word translations provided by a dictionary are ranked according to their appropriateness in context. In contrast to most alternative systems, ours does not rely on either parallel corpora or machine translation systems, making it suitable for low-resource languages as the language to be learned. This is achieved by a graph-based scoring mechanism which can deal with ambiguous translations of context words provided by a dictionary. Due to this decoupling from the source language, we need monolingual corpus resources only for the target language, i.e. the language of the translation candidates. We evaluate our approach for the language pair Norwegian Nynorsk-English on an exploratory manually annotated gold standard and report promising results. When running our system on the original SemEval CLLS task, we rank 6th out of 18 (including 2 baselines and our 2 system variants) in the best evaluation."
L16-1556,A Crowdsourced Database of Event Sequence Descriptions for the Acquisition of High-quality Script Knowledge,2016,0,10,3,1,24460,lilian wanzare,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Scripts are standardized event sequences describing typical everyday activities, which play an important role in the computational modeling of cognitive abilities (in particular for natural language processing). We present a large-scale crowdsourced collection of explicit linguistic descriptions of script-specific event sequences (40 scenarios with 100 sequences each). The corpus is enriched with crowdsourced alignment annotation on a subset of the event descriptions, to be used in future work as seed data for automatic alignment of event descriptions (for example via clustering). The event descriptions to be aligned were chosen among those expected to have the strongest corrective effect on the clustering algorithm. The alignment annotation was evaluated against a gold standard of expert annotators. The resulting database of partially-aligned script-event descriptions provides a sound empirical basis for inducing high-quality script knowledge, as well as for any task involving alignment and paraphrase detection of events."
D16-1017,Event participant modelling with neural networks,2016,29,9,5,0,31667,ottokar tilk,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
E14-1057,What Substitutes Tell Us - Analysis of an {``}All-Words{''} Lexical Substitution Corpus,2014,33,23,4,0,40088,gerhard kremer,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present the first large-scale English xe2x80x9callwords lexical substitutionxe2x80x9d corpus. The size of the corpus provides a rich resource for investigations into word meaning. We investigate the nature of lexical substitute sets, comparing them to WordNet synsets. We find them to be consistent with, but more fine-grained than, synsets. We also identify significant differences to results for paraphrase ranking in context reported for the SEMEVAL lexical substitution data. This highlights the influence of corpus construction approaches on evaluation results."
Q13-1003,Grounding Action Descriptions in Videos,2013,29,137,4,0.416667,21823,michaela regneri,Transactions of the Association for Computational Linguistics,0,"Recent work has shown that the integration of visual information into text-based models can substantially improve model predictions, but so far only visual information extracted from static images has been used. In this paper, we consider the problem of grounding sentences describing actions in visual information extracted from videos. We present a general purpose corpus that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos, together with an annotation of how similar the action descriptions are to each other. Experimental results demonstrate that a text-based model of similarity between actions improves substantially when combined with visual information from videos depicting the described actions."
S12-1089,{S}aarland: Vector-based models of semantic textual similarity,2012,8,6,2,0,8881,georgiana dinu,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"This paper describes our system for the Semeval 2012 Sentence Textual Similarity task. The system is based on a combination of few simple vector space-based methods for word meaning similarity. Evaluation results show that a simple combination of these unsupervised data-driven methods can be quite successful. The simple vector space components achieve high performance on short sentences; on longer, more complex sentences, they are outperformed by a surprisingly competitive word overlap baseline, but they still bring improvements over this baseline when incorporated into a mixture model."
N12-1076,A comparison of models of word meaning in context,2012,12,15,2,0,8881,georgiana dinu,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This paper compares a number of recently proposed models for computing context sensitive word similarity. We clarify the connections between these models, simplify their formulation and evaluate them in a unified setting. We show that the models are essentially equivalent if syntactic information is ignored, and that the substantial performance differences previously reported disappear to a large extent when these simplified variants are evaluated under identical conditions. Furthermore, our reformulation allows for the design of a straightforward and fast implementation."
C12-2033,A Comparison of Knowledge-based Algorithms for Graded Word Sense Assignment,2012,20,0,3,0,785,annemarie friedrich,Proceedings of {COLING} 2012: Posters,0,"Standard word sense disambiguation (WSD) data sets annotate each word instance in context with exactly one sense of a predefined inventory, and WSD systems are traditionally evaluated with regard to how good they are at picking this sense. Recently, the notion of graded word sense assignment (GWSA) has gained attention as a more natural view of the contextual specification of word meaning; multiple senses may apply simultaneously to one instance of a word, and they may be applicable to different degrees. In this paper, we apply three different WSD algorithms to the task of GWSA. The three models belong to the class of knowledge-based models in the WSD terminology; they are unsupervised in the sense that they do not depend on annotated training material. We evaluate the models on two recently published GWSA data sets. We find positive correlations with the human judgments for all models, and develop a metric based on the notion of accuracy that highlights differences in the behaviors of the models."
I11-1127,Word Meaning in Context: A Simple and Effective Vector Model,2011,28,56,1,1,28852,stefan thater,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We present a model that represents word meaning in context by vectors which are modified according to the words in the targetxe2x80x99s syntactic context. Contextualization of a vector is realized by reweighting its components, based on distributional information about the context words. Evaluation on a paraphrase ranking task derived from the SemEval 2007 Lexical Substitution Task shows that our model outperforms all previous models on this task. We show that our model supports a wider range of applications by evaluating it on a word sense disambiguation task. Results show that our model achieves state-of-the-art performance."
D11-1072,Robust Disambiguation of Named Entities in Text,2011,27,571,8,0,7615,johannes hoffart,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Disambiguating named entities in natural-language text maps mentions of ambiguous names onto canonical entities like people or places, registered in a knowledge base such as DBpedia or YAGO. This paper presents a robust method for collective disambiguation, by harnessing context from knowledge bases and using a new form of coherence graph. It unifies prior approaches into a comprehensive framework that combines three measures: the prior probability of an entity being mentioned, the similarity between the contexts of a mention and a candidate entity, as well as the coherence among candidate entities for all mentions together. The method builds a weighted graph of mentions and candidate entities, and computes a dense subgraph that approximates the best joint mention-entity mapping. Experiments show that the new method significantly outperforms prior methods in terms of accuracy, with robust behavior across a variety of inputs."
P10-1004,Computing Weakest Readings,2010,29,11,2,0.337386,987,alexander koller,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We present an efficient algorithm for computing the weakest readings of semantically ambiguous sentences. A corpus-based evaluation with a large-scale grammar shows that our algorithm reduces over 80% of sentences to one or two readings, in negligible runtime, and thus makes it possible to work with semantic representations derived by deep large-scale grammars."
P10-1097,Contextualizing Semantic Representations Using Syntactically Enriched Vector Models,2010,22,83,1,1,28852,stefan thater,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of first- and second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task."
W09-2506,Ranking Paraphrases in Context,2009,14,24,1,1,28852,stefan thater,Proceedings of the 2009 Workshop on Applied Textual Inference ({T}ext{I}nfer),0,"We present a vector space model that supports the computation of appropriate vector representations for words in context, and apply it to a paraphrase ranking task. An evaluation on the SemEval 2007 lexical substitution task data shows promising results: the model significantly outperforms a current state of the art model, and our treatment of context is effective."
P08-1026,Regular Tree Grammars as a Formalism for Scope Underspecification,2008,29,17,3,0.569673,987,alexander koller,Proceedings of ACL-08: HLT,1,"We propose the use of regular tree grammars (RTGs) as a formalism for the underspecified processing of scope ambiguities. By applying standard results on RTGs, we obtain a novel algorithm for eliminating equivalent readings and the first efficient algorithm for computing the best reading of a scope ambiguity. We also show how to derive RTGs from more traditional underspecified descriptions."
W07-1402,A Semantic Approach To Textual Entailment: System Evaluation and Task Analysis,2007,9,35,3,0,13863,aljoscha burchardt,Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing,0,"This paper discusses our contribution to the third RTE Challenge -- the SALSA RTE system. It builds on an earlier system based on a relatively deep linguistic analysis, which we complement with a shallow component based on word overlap. We evaluate their (combined) performance on various data sets. However, earlier observations that the combination of features improves the overall accuracy could be replicated only partly."
W06-3904,Towards a redundancy elimination algorithm for underspecified descriptions,2006,13,6,2,0.6,987,alexander koller,Proceedings of the Fifth International Workshop on Inference in Computational Semantics ({IC}o{S}-5),0,"This paper proposes an efficient algorithm for the redundancy elimination problem: Given an underspecified semantic representation (USR), compute an USR which has fewer readings, but still describes at least one representative of each semantic equivalence class of the original readings. The algorithm operates on underspecified chart representations which are derived from dominance graphs; it can be applied to the USRs computed by largescale grammars. To our knowledge, it is the first redundancy elimination algorithm which maintains underspecification, rather than just enumerating non-redundant readings."
P06-1052,An Improved Redundancy Elimination Algorithm for Underspecified Representations,2006,15,10,2,0.6,987,alexander koller,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We present an efficient algorithm for the redundancy elimination problem: Given an underspecified semantic representation (USR) of a scope ambiguity, compute an USR with fewer mutually equivalent readings. The algorithm operates on underspecified chart representations which are derived from dominance graphs; it can be applied to the USRs computed by large-scale grammars. We evaluate the algorithm on a corpus, and show that it reduces the degree of ambiguity significantly while taking negligible runtime."
W05-1105,The Evolution of Dominance Constraint Solvers,2005,11,9,2,1,987,alexander koller,Proceedings of Workshop on Software,0,"We describe the evolution of solvers for dominance constraints, a formalism used in underspecified semantics, and present a new graph-based solver using charts. An evaluation on real-world data shows that each solver (including the new one) is significantly faster than its predecessors. We believe that our strategy of successively tailoring a powerful formalism to the actual inputs is more generally applicable."
P05-3003,Efficient Solving and Exploration of Scope Ambiguities,2005,12,13,2,1,987,alexander koller,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"We present the currently most efficient solver for scope underspecification; it also converts between different underspecification formalisms and counts readings. Our tool makes the practical use of large-scale grammars with (underspecified) semantic output more feasible, and can be used in grammar debugging."
W04-3320,{TAG} Parsing as Model Enumeration,2004,-1,-1,4,0,51326,ralph debusmann,Proceedings of the 7th International Workshop on Tree Adjoining Grammar and Related Formalisms,0,None
P04-1032,"{M}inimal {R}ecursion {S}emantics as Dominance Constraints: Translation, Evaluation, and Analysis",2004,13,26,4,0,51759,ruth fuchss,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,We show that a practical translation of MRS descriptions into normal dominance constraints is feasible. We start from a recent theoretical translation and verify its assumptions on the outputs of the English Resource Grammar (ERG) on the Redwoods corpus. The main assumption of the translation---that all relevant underspecified descriptions are nets---is validated for a large majority of cases; all non-nets computed by the ERG seem to be systematically incomplete.
C04-1026,A Relational Syntax-Semantics Interface Based on Dependency Grammar,2004,17,32,6,0,51326,ralph debusmann,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We propose a syntax-semantics interface that realises the mapping between syntax and semantics as a relation and does not make functionality assumptions in either direction. This interface is stated in terms of Extensible Dependency Grammar (XDG), a grammar formalism we newly specify. XDG's constraint-based parser supports the concurrent flow of information between any two levels of linguistic representation, even when only partial analyses are available. This generalises the concept of underspecification."
P03-1047,Bridging the Gap Between Underspecification Formalisms: {M}inimal {R}ecursion {S}emantics as Dominance Constraints,2003,11,21,2,1,51760,joachim niehren,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"Minimal Recursion Semantics (MRS) is the standard formalism used in large-scale HPSG grammars to model underspecified semantics. We present the first provably efficient algorithm to enumerate the readings of MRS structures, by translating them into normal dominance constraints."
E03-1024,Underspecification formalisms: Hole semantics as dominance constraints,2003,0,0,3,1,987,alexander koller,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,None
P01-1028,Generating with a Grammar Based on Tree Descriptions: a Constraint-Based Approach,2001,19,5,2,0,850,claire gardent,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"While the generative view of language processing builds bigger units out of smaller ones by means of rewriting steps, the axiomatic view eliminates invalid linguistic structures out of a set of possible structures by means of well formedness principles. We present a generator based on the axiomatic view and argue that when combined with a TAG-like grammar and a flat semantics, this axiomatic view permits avoiding drawbacks known to hold either of top-down or of bottom-up generators."
