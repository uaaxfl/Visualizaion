S15-1009,A New Dataset and Evaluation for Belief/Factuality,2015,15,5,17,0,90,vinodkumar prabhakaran,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"The terms xe2x80x9cbeliefxe2x80x9d and xe2x80x9cfactualityxe2x80x9d both refer to the intention of the writer to present the propositional content of an utterance as firmly believed by the writer, not firmly believed, or having some other status. This paper presents an ongoing annotation effort and an associated evaluation."
W13-0905,Automatic Metaphor Detection using Large-Scale Lexical Resources and Conventional Metaphor Extraction,2013,21,20,1,1,37311,yorick wilks,Proceedings of the First Workshop on Metaphor in {NLP},0,"The paper presents an experimental algorithm to detect conventionalized metaphors implicit in the lexical data in a resource like WordNet, where metaphors are coded into the senses and so would never be detected by any algorithm based on the violation of preferences, since there would always be a constraint satisfied by such senses. We report an implementation of this algorithm, which was implemented first the preference constraints in VerbNet. We then derived in a systematic way a far more extensive set of constraints based on WordNet glosses, and with this data we reimplemented the detection algorithm and got a substantial improvement in recall. We suggest that this technique could contribute to improve the performance of existing metaphor detection strategies that do not attempt to detect conventionalized metaphors. The new WordNet-derived data is of wider significance because it also contains adjective constraints, unlike any existing lexical resource, and can be applied to any language with a semantic parser (and WN) for it."
catizone-etal-2012-lie,"{LIE}: Leadership, Influence and Expertise",2012,11,3,4,1,42896,roberta catizone,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes our research into methods for inferring social and instrumental roles and relationships from document and discourse corpora. The goal is to identify the roles of initial authors and participants in internet discussions with respect to leadership, influence and expertise. Web documents, forums and blogs provide data from which the relationships between these concepts are empirically derived and compared. Using techniques from Natural Language Processing (NLP), characterizations of authority and expertise are hypothesized and then tested to see if these pick out the same or different participants as may be chosen by techniques based on social network analysis (Huffaker 2010) see if they pick out the same discourse participants for any given level of these qualities (i.e. leadership, expertise and influence). Our methods could be applied, in principle, to any domain topic, but this paper will describe an initial investigation into two subject areas where a range of differing opinions are available and which differ in the nature of their appeals to authority and truth: Âgenetic engineering' and a ÂMuslim Forum'. The available online corpora for these topics contain discussions from a variety of users with different levels of expertise, backgrounds and personalities."
W10-2703,Is a Companion a Distinctive Kind of Relationship with a Machine?,2010,-1,-1,1,1,37311,yorick wilks,Proceedings of the 2010 Workshop on Companionable Dialogue Systems,0,None
P10-4013,Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images,2010,2,1,1,1,37311,yorick wilks,Proceedings of the {ACL} 2010 System Demonstrations,0,"This paper describes an initial prototype demonstrator of a Companion, designed as a platform for novel approaches to the following: 1) The use of Information Extraction (IE) techniques to extract the content of incoming dialogue utterances after an Automatic Speech Recognition (ASR) phase, 2) The conversion of the input to Resource Descriptor Format (RDF) to allow the generation of new facts from existing ones, under the control of a Dialogue Manger (DM), that also has access to stored knowledge and to open knowledge accessed in real time from the web, all in RDF form, 3) A DM implemented as a stack and network virtual machine that models mixed initiative in dialogue control, and 4) A tuned dialogue act detector based on corpus evidence. The prototype platform was evaluated, and we describe this briefly; it is also designed to support more extensive forms of emotion detection carried by both speech and lexical content, as well as extended forms of machine learning."
W09-3931,Artificial {C}ompanions as Dialogue Agents,2009,0,3,1,1,37311,yorick wilks,Proceedings of the {SIGDIAL} 2009 Conference,0,"COMPANIONS is an EU project that aims to change the way we think about the relationships of people to computers and the Internet by developing a virtual conversational 'Companion'. This is intended as an agent or 'presence' that stays with the user for long periods of time, developing a relationship and 'knowing' its owners preferences and wishes. The Companion communicates with the user primarily through speech. This paper describes the functionality and system modules of the Senior Companion, one of two initial prototypes built in the first two years of the project. The Senior Companion provides a multimodal interface for eliciting and retrieving personal information from the elderly user through a conversation about their photographs. The Companion will, through conversation, elicit their life memories, often prompted by discussion of their photographs; the aim is that the Companion should come to know a great deal about its user, their tastes, likes, dislikes, emotional reactions etc, through long periods of conversation. It is a further assumption that most life information will be stored on the internet (as in the Memories for Life project: http://www.memoriesforlife.org/) and the SC is linked directly to photo inventories in Facebook, to gain initial information about people and relationships, as well as to Wikipedia to enable it to respond about places mentioned in conversations about images. The overall aim of the SC, not yet achieved, is to produce a coherent life narrative for its user from these materials, although its short term goals are to assist, amuse, entertain and gain the trust of the user. The Senior Companion uses Information Extraction to get content from the speech input, rather than conventional parsing, and retains utterance content, extracted internet information and ontologies all in RDF formalism over which it does primitive reasoning about people. It has a dialogue manager virtual machine intended to capture mixed initiative, between Companion and user, and which can be a basis for later replacement by learned components."
guthrie-etal-2008-unsupervised,An Unsupervised Probabilistic Approach for the Detection of Outliers in Corpora,2008,0,3,3,1,46329,david guthrie,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Many applications of computational linguistics are greatly influenced by the quality of corpora available and as automatically generated corpora continue to play an increasingly common role, it is essential that we not overlook the importance of well-constructed and homogeneous corpora. This paper describes an automatic approach to improving the homogeneity of corpora using an unsupervised method of statistical outlier detection to find documents and segments that do not belong in a corpus. We consider collections of corpora that are homogeneous with respect to topic (i.e. about the same subject), or genre (written for the same audience or from the same source) and use a combination of stylistic and lexical features of the texts to automatically identify pieces of text in these collections that break the homogeneity. These pieces of text that are significantly different from the rest of the corpus are likely to be errors that are out of place and should be removed from the corpus before it is used for other tasks. We evaluate our techniques by running extensive experiments over large artificially constructed corpora that each contain single pieces of text from a different topic, author, or genre than the rest of the collection and measure the accuracy of identifying these pieces of text without the use of training data. We show that when these pieces of text are reasonably large (1,000 words) we can reliably identify them in a corpus."
wilks-etal-2008-dialogue,"Dialogue, Speech and Images: the Companions Project Data Set",2008,6,4,1,1,37311,yorick wilks,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper describes part of the corpus collection efforts underway in the EC funded Companions project. The Companions project is collecting substantial quantities of dialogue a large part of which focus on reminiscing about photographs. The texts are in English and Czech. We describe the context and objectives for which this dialogue corpus is being collected, the methodology being used and make observations on the resulting data. The corpora will be made available to the wider research community through the Companions Project web site."
catizone-etal-2008-information,Information Extraction Tools and Methods for Understanding Dialogue in a Companion,2008,4,8,4,1,42896,roberta catizone,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper discusses how Information Extraction is used to understand and manage Dialogue in the EU-funded Companions project. This will be discussed with respect to the Senior Companion, one of two applications under development in the EU-funded Companions project. Over the last few years, research in human-computer dialogue systems has increased and much attention has focused on applying learning methods to improving a key part of any dialogue system, namely the dialogue manager. Since the dialogue manager in all dialogue systems relies heavily on the quality of the semantic interpretation of the userÂs utterance, our research in the Companions project, focuses on how to improve the semantic interpretation and combine it with knowledge from the Knowledge Base to increase the performance of the Dialogue Manager. Traditionally the semantic interpretation of a user utterance is handled by a natural language understanding module which embodies a variety of natural language processing techniques, from sentence splitting, to full parsing. In this paper we discuss the use of a variety of NLU processes and in particular Information Extraction as a key part of the NLU module in order to improve performance of the dialogue manager and hence the overall dialogue system."
webb-etal-2008-cross,Cross-Domain Dialogue Act Tagging,2008,22,7,4,0,27626,nick webb,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We present recent work in the area of Cross-Domain Dialogue Act (DA) tagging. We have previously reported on the use of a simple dialogue act classifier based on purely intra-utterance features - principally involving word n-gram cue phrases automatically generated from a training corpus. Such a classifier performs surprisingly well, rivalling scores obtained using far more sophisticated language modelling techniques. In this paper, we apply these automatically extracted cues to a new annotated corpus, to determine the portability and generality of the cues we learn."
J08-4001,{ACL} Lifetime Achievement Award: On Whose Shoulders?,2008,-1,-1,1,1,37311,yorick wilks,Computational Linguistics,0,None
N07-2018,Clustered Sub-Matrix Singular Value Decomposition,2007,4,0,2,0,38123,fang huang,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,This paper presents an alternative algorithm based on the singular value decomposition (SVD) that creates vector representation for linguistic units with reduced dimensionality. The work was motivated by an application aimed to represent text segments for further processing in a multi-document summarization system. The algorithm tries to compensate for SVD's bias towards dominant-topic documents. Our experiments on measuring document similarities have shown that the algorithm achieves higher average precision with lower number of dimensions than the baseline algorithms - the SVD and the vector space model.
W06-0903,Automatic Dating of Documents and Temporal Text Classification,2006,12,2,2,1,49824,angelo dalli,Proceedings of the Workshop on Annotating and Reasoning about Time and Events,0,"Temporal information is presently underutilised for document and text processing purposes. This work presents an unsupervised method of extracting periodicity information from text, enabling time series creation and filtering to be used in the creation of sophisticated language models that can discern between repetitive trends and non-repetitive writing pat-terns. The algorithm performs in O(n log n) time for input of length n. The temporal language model is used to create rules based on temporal-word associations inferred from the time series. The rules are used to automatically guess at likely document creation dates, based on the assumption that natural languages have unique signatures of changing word distributions over time. Experimental results on news items spanning a nine year period show that the proposed method and algorithms are accurate in discovering periodicity patterns and in dating documents automatically solely from their content."
guthrie-etal-2006-closer,A Closer Look at Skip-gram Modelling,2006,5,146,5,1,46329,david guthrie,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Data sparsity is a large problem in natural language processing that refers to the fact that language is a system of rare events, so varied and complex, that even using an extremely large corpus, we can never accurately model all possible strings of words. This paper examines the use of skip-grams (a technique where by n-grams are still stored to model language, but they allow for tokens to be skipped) to overcome the data sparsity problem. We analyze this by computing all possible skip-grams in a training corpus and measure how many adjacent (standard) n-grams these cover in test documents. We examine skip-gram modelling using one to four skips with various amount of training data and test against similar documents as well as documents generated from a machine translation system. In this paper we also determine the amount of extra training data required to achieve skip-gram coverage using standard adjacent tri-grams."
iria-etal-2006-incremental,An Incremental Tri-Partite Approach To Ontology Learning,2006,13,10,4,0,45937,jose iria,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper we present a new approach to ontology learning. Its basis lies in a dynamic and iterative view of knowledge acquisition for ontologies. The Abraxas approach is founded on three resources, a set of texts, a set of learning patterns and a set of ontological triples, each of which must remain in equilibrium. As events occur which disturb this equilibrium various actions are triggered to re- establish a balance between the resources. Such events include acquisition of a further text from external resources such as the Web or the addition of ontological triples to the ontology. We develop the concept of a knowledge gap between the coverage of an ontology and the corpus of texts as a measure triggering actions. We present an overview of the algorithm and its functionalities."
catizone-etal-2006-evaluating,Evaluating Automatically Generated Timelines from the Web,2006,6,9,3,1,42896,roberta catizone,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"As web searches increase, there is a need to represent the search results in the most comprehensible way possible. In particular, we focus on search results from queries about people and places. The standard method for presentation of search results is an ordered list determined by the Web search engine. Although this is satisfactory in some cases, when searching for people and places, presenting the information indexed by time may be more desirable. We are developing a system called Cronopath, which generates a timeline of web search engine results by determining the time frame of each document in the collection and linking elements in the timeline to the relevant articles. In this paper, we propose evaluation guidelines for judging the quality of automatically generated timelines based on a set of common features."
wilks-etal-2004-human,Human Dialogue Modelling Using Annotated Corpora,2004,7,0,1,1,37311,yorick wilks,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We describe two major dialogue system segments: first we describe a Dialogue Manager which uses a representation of stereotypical dialogue patterns that we call Dialogue Action Frames and which, we believe, generate strong and novel constraints on later access to incomplete dialogue topics. Secondly, an analysis module that learns to assign dialogue acts from corpora, but on the basis of limited quantities of data, and up to what seems to be some kind of limit on this task, a fact we also discuss."
dalli-etal-2004-web,Web Services Architecture for Language Resources,2004,7,5,4,1,49824,angelo dalli,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Abstract A web services based architecture for Language Resources utilizing existing technology such as XML, SOAP, WSDL and UDDI is presented. The web services architecture creates a pervasive information infrastructure that enables straightforward access to two kinds of Language Resources: traditional information sources and language processing resources. Details bout two practical aimplementations of this web services architecture are given. Web Services and databases with minimal means, if any, of The concept of web services as being lightweight components that offer an elegant means of integrating different information repositories and services across the Internet has always been a main objective in developing a standard, interoperable system of web services. Industrial and academic support for web services is increasingly gaining strength and the future looks promising for their widespread adoption (Narsu and Murphy, 2002; Conner, 2001; Gates, 2003). The idea of using web services for Computational Linguistics is also gaining acceptance with the increasing availability of various useful services permitting researchers unprecented access to huge amounts of information and advanced search services like Google (Google, 2002). Linguistic resources are prime candidates for web services applications to enable increased collaboration between research groups and avoid reduplication of resources and effort. Fortunately, current web services technology can be used to provide effective solutions to common problems faced by researchers (Dalli, 2001; Dalli, 2002). We propose a web services architecture for Language Resources that uses a combination of Extensible Markup Language (XML), Simple Object Access Protocol (SOAP), Web Services Description Language (WSDL) and Universal Discovery Description Integration (UDDI) to achieve maximum benefit from these technologies in a Computational Linguistics context (Box et al., 2000; Christensen, et al., 2001; UDDI, 2001). The web services architecture creates a pervasive information infrastructure that enables straightforward access to two kinds of Language Resources: traditional resources such as lexicons, corpora, semantic networks, etc. and language processing resources. The use of standard technology ensures that there is wide support for developers working with minimal knowledge of web services, and also guarantees compatibility with legacy applications, while keeping compatibility with major development frameworks such as Sunxe2x80x99s Java, IBMxe2x80x99s WebSphere, and Microsoftxe2x80x99s .NET."
pastra-wilks-2004-image,"Image-Language Multimodal Corpora: Needs, Lacunae and an {AI} Synergy for Annotation",2004,13,7,2,1,46284,katerina pastra,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The growing demand for intelligent multimedia systems has led to the development of various multimodal resources and corresponding annotation schemes and processing tools. In this paper, we argue that there is a striking lack of multimodal corpora capturing the association and interaction of visual and linguistic data. We relate this research lacuna to vision-language integration prototypes developed within Artificial Intelligence (AI) and show how the needs of the latter dictate the development of such resources for a wide variety of applications. We identify the annotation requirements imposed on image-language corpora by these needs and the nature of the modalities involved and suggest a semi-automatic way of meeting them."
brewster-etal-2004-data,Data Driven Ontology Evaluation,2004,12,291,4,1,48122,christopher brewster,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The evaluation of ontologies is vital for the growth of the Semantic Web. We consider a number of problems in evaluating a knowledge artifact like an ontology. We propose in this paper that one approach to ontology evaluation should be corpus or data driven. A corpus is the most accessible form of knowledge and its use allows a measure to be derived of the xe2x80x98fitxe2x80x99 between an ontology and a domain of knowledge. We consider a number of methods for measuring this xe2x80x98fitxe2x80x99 and propose a measure to evaluate structural fit, and a probabilistic approach to identifying the best ontology."
C04-1143,{FASIL} Email Summarisation System,2004,32,11,3,1,49824,angelo dalli,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,Email summarisation presents a unique set of requirements that are different from general text summarisation. This work describes the implementation of an email summarisation system for use in a voice-based Virtual Personal Assistant developed for the EU FASiL Project. Evaluation results from the first integrated version of the project are presented.
2004.tmi-1.10,Cross-language algorithms: the progressive conflation of the {MT} and {IR} paradigms,2004,-1,-1,1,1,37311,yorick wilks,Proceedings of the 10th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
W03-2701,"{I}ntroduction: Dialogue Systems: Interaction, Adaptation and Styles of Management",2003,0,2,5,0,15952,kristiina jokinen,"Proceedings of the 2003 {EACL} Workshop on Dialogue Systems: interaction, adaptation and styes of management",0,None
W03-2705,Multimodal Dialogue Management in the {COMIC} Project,2003,8,24,3,1,42896,roberta catizone,"Proceedings of the 2003 {EACL} Workshop on Dialogue Systems: interaction, adaptation and styes of management",0,"The next generation internet applications will feature not only the ability to understand spoken and written natural language text, (pen) gestures and body postures, they will also and importantly be able to engage with the user in a natural dialogue about the application. In this paper we will describe the design of a multimodal dialogue and action management module, part of the COMIC demonstrator, which is aimed at these next generation applications. The design uses well understood structures like stacks and augmented transition networks in a novel way to obtain the flexibility needed for mixed-initiative dialogue. We also show how this is applied to the application of the COMIC demonstrator bathroom design."
E03-2014,"Event-Coreference across Multiple, Multi-lingual Sources in the Mumis Project",2003,5,4,9,0.612245,5986,horacio saggion,Demonstrations,0,"We present our work on information extraction from multiple, multi-lingual sources for the Multimedia Indexing and Searching Environment (MUMIS), a project aiming at developing technology to produce formal annotations about essential events in multimedia programme material. The novelty of our approach consists on the use of a merging or cross-document coreference algorithm that aims at combining the output delivered by the information extraction systems."
E03-1011,Mining Web Sites Using Unsupervised Adaptive Information Extraction,2003,10,16,4,1,45648,alexiei dingli,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Adaptive Information Extraction systems (IES) are currently used by some Semantic Web (SW) annotation tools as support to annotation (Handschuh et al., 2002; Vargas-Vera et al., 2002). They are generally based on fully supervised methodologies requiring fairly intense domain-specific annotation. Unfortunately, selecting representative examples may be difficult and annotations can be incorrect and require time. In this paper we present a methodology that drastically reduce (or even remove) the amount of manual annotation required when annotating consistent sets of pages. A very limited number of user-defined examples are used to bootstrap learning. Simple, high precision (and possibly high recall) IE patterns are induced using such examples, these patterns will then discover more examples which will in turn discover more patterns, etc. The key feature that enables such bootstrapping is the Redundancy on the Web. Redundancy is given by the presence of multiple citations of the same facts in different superficial formats and is currently used for several tasks such as improving question answering systems (Dumais et al., 2002) and performing information extraction using machine learning (Mitchell, 2001). When known information is presented in different sources, it is possible to use its multiple occurrences to bootstrap recognisers that when generalised will retrieve other pieces of information, producing in turn more (generic) recognisers. In our model redundancy of information is increased by using preexisting services (e.g. search engines, digital libraries). This improves the effectiveness of bootstrapping. Another typical feature of Web pages that we exploit for learning is document formatting: HTML and XML pages often contain formatting directives (tables, lists, etc.) that group identical or related information. Identifying such formatted areas and their content can be very useful. For example, a structure listing some known names can be used to discover other names if it is possible to generalise over the regularity of the list. In the rest of the paper we will present the details of our methodology as implemented in the Armadillo System, using an application of IE from Computer Science Web sites as a matter of exemplification."
E03-1065,{NLP} for Indexing and Retrieval of Captioned Photographs,2003,8,5,3,0.612245,5986,horacio saggion,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present a text-based approach for the automatic indexing and retrieval of digital photographs taken at crime scenes. Our research prototype, SOCIS, goes beyond keyword-based approaches and methods that extract syntactic relations from captions; it relies on advanced Natural Language Processing techniques in order to extract relational facts. These relational facts consist of a pragmatic relation and the entities this relation connects (triples of the form: ARG1-REL- ARG2). In SOCIS, the triples are used as complex image indexing terms; however, the extraction mechanism is used not only for indexing purposes but also for image retrieval using free text queries. The retrieval mechanism computes similarity scores between query-triples and indexing-triples making use of a domain-specific ontology."
P02-1020,Measuring Text Reuse,2002,13,117,4,0,40907,paul clough,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we present results from the METER (MEasuring TExt Reuse) project whose aim is to explore issues pertaining to text reuse and derivation, especially in the context of newspapers using newswire sources. Although the reuse of text by journalists has been studied in linguistics, we are not aware of any investigation using existing computational methods for this particular task. We investigate the classification of newspaper articles according to their degree of dependence upon, or derivation from, a newswire source using a simple 3-level scheme designed by journalists. Three approaches to measuring text similarity are considered: n-gram overlap, Greedy String Tiling, and sentence alignment. Measured against a manually annotated corpus of source and derived news text, we show that a combined classifier with features automatically selected performs best overall for the ternary classification achieving an average F1-measure score of 0.664 across all three categories."
saggion-etal-2002-extracting,Extracting Information for Automatic Indexing of Multimedia Material,2002,12,5,7,0.612245,5986,horacio saggion,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper discusses our work on information extraction (IE) from multi-lingual, multi-media, multi-genre Language Resources, in a domain where there are many different event types. This work is being carried out in the context of MUMIS, an EU-funded project that aims at the development of basic technology for the creation of a composite index from multiple and multi-lingual sources. Our approach to IE relies on a finite state machinery provided by GATE, a General Architecture for Text Engineering, pipelined with full syntactic analysis and discourse interpretation implemented in Prolog."
pastra-etal-2002-feasible,How feasible is the reuse of grammars for Named Entity Recognition?,2002,9,26,5,1,46284,katerina pastra,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"In this paper, we investigate whether reusing existing grammars for NE recognition instead of creating them from scratch is a viable solution to time constraints in developing grammars. We discuss three possible factors that hinder grammar reuse and we present our corresponding empirical results, that encourage more widespread use of valuable existing resources."
W01-1004,"Using {HLT} for Acquiring, Retrieving and Publishing Knowledge in {AKT}",2001,0,3,7,1,11076,kalina bontcheva,Proceedings of the {ACL} 2001 Workshop on Human Language Technology and Knowledge Management,0,None
W01-1013,Multilingual Authoring: the {NAMIC} Approach,2001,11,7,7,0,12620,roberto basili,Proceedings of the {ACL} 2001 Workshop on Human Language Technology and Knowledge Management,0,"With increasing amounts of electronic information available, and the increase in the variety of languages used to produce documents of the same type, the problem of how to manage similar documents in different languages arises. This paper proposes an approach to processing/structuring text so that Multilingual Authoring (creating hypertext links) can be effectively carried out. This work, funded by the European Union, is applied to the Multilingual Authoring of news agency text. We have applied methods from Natural Language Processing, especially Information Extraction technology, to both monolingual and Multilingual Authoring."
J01-3001,The Interaction of Knowledge Sources in Word Sense Disambiguation,2001,54,154,2,0.454545,2873,mark stevenson,Computational Linguistics,0,Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial in telligence research. An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results. We present a sense tagger which uses several knowledge sources. Tested accuracy exceeds 94% on our evaluation corpus.Our system attempts to disambiguate all content words in running text rather than limiting itself to treating a restricted vocabulary of words. It is argued that this approach is more likely to assist the creation of practical systems.
W00-1501,Experience using {GATE} for {NLP} {R}{\\&}{D},2000,11,23,5,1,41365,hamish cunningham,Proceedings of the {COLING}-2000 Workshop on Using Toolsets and Architectures To Build {NLP} Systems,0,"GATE, a General Architecture for Text Engineering, aims to provide a software infrastructure for researchers and developers working in NLP. GATE has now been widely available for four years. In this paper we review the objectives which motivated the creation of GATE and the functionality and design of the current system. We discuss the strengths and weaknesses of the current system, identify areas for improvement."
cunningham-etal-2000-software,Software Infrastructure for Language Resources: a Taxonomy of Previous Work and a Requirements Analysis,2000,60,24,4,1,41365,hamish cunningham,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"This paper presents a taxonomy of previous work on infrastructures, architectures and development environments for representing and processing Language Resources (LRs), corpora, and annotations. This classification is then used to derive a set of requirements for a Software Architecture for Language Engineering (SALE). The analysis shows that a SALE should address common problems and support typical activities in the development, deployment, and maintenance of LE software. The results will be used in the next phase of construction of an infrastructure for LR production, distribution, and access."
W99-0701,Unsupervised Learning of Word Boundary with Description Length Gain,1999,15,67,2,0,21686,chunyu kit,{EACL} 1999: {C}o{NLL}-99 Computational Natural Language Learning,0,None
W98-1208,Implementing a Sense Tagger in a General Architecture for Text Engineering,1998,28,0,3,1,41365,hamish cunningham,New Methods in Language Processing and Computational Natural Language Learning,0,"We describe two systems: GATE (General Architecture for Text Engineering), an architecture to aid in the production and delivery of language engineering systems which significantly reduces development time and ease of reuse in such systems. We also describe a sense tagger which we implemented within the GATE architecture, and which achieves high accuracy (92% of all words in text to a broad semantic level). We used the implementation of the sense tagger as a real-world task on which to evaluate the usefulness of the GATE architecture and identified strengths and weaknesses in the architecture."
P98-2228,Word Sense Disambiguation using Optimised Combinations of Knowledge Sources,1998,21,54,1,1,37311,yorick wilks,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"Word sense disambiguation algorithms, with few exceptions, have made use of only one lexical knowledge source. We describe a system which performs word sense disambiguation on all content words in free text by combining different knowledge sources: semantic preferences, dictionary definitions and subject/domain codes along with part-of-speech tags, optimised by means of a learning algorithm. We also describe the creation of a new sense tagged corpus by combining existing resources. Tested accuracy of our approach on this corpus exceeds 92%, demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample."
P98-1115,Compacting the {P}enn {T}reebank Grammar,1998,5,38,4,0,55364,alexander krotov,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"Treebanks, such as the Penn Treebank (PTB), offer a simple approach to obtaining a broad coverage grammar: one can simply read the grammar off the parse trees in the treebank. While such a grammar is easy to obtain, a square-root rate of growth of the rule set with corpus size suggests that the derived grammar is far from complete and that much more treebanked text would be required to obtain a complete grammar, if one exists at some limit. However, we offer an alternative explanation in terms of the underspecification of structures within the treebank. This hypothesis is explored by applying an algorithm to compact the derived grammar by eliminating redundant rules - rules whose right hand sides can be parsed by other rules. The size of the resulting compacted grammar, which is significantly less than that of the full treebank grammar, is shown to approach a limit. However, such a compacted grammar does not yield very good performance figures. A version of the compaction algorithm taking rule probabilities into account is proposed, which is argued to be more linguistically motivated. Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall, but a loss in precision."
O98-4001,Senses and Texts,1998,-1,-1,1,1,37311,yorick wilks,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 3, Number 2, August 1998",0,None
O98-4002,Information Extraction: Beyond Document Retrieval,1998,42,180,2,1,33330,robert gaizauskas,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 3, Number 2, August 1998",0,"In this paper we give a synoptic view of the growth of the text processing technology of information extraction (IE) whose function is to extract information about a prexe2x80x90specified set of entities, relations or events from natural language texts and to record this information in structured representations called templates. Here we describe the nature of the IE task, review the history of the area from its origins in AI work in the 1960s and 70s till the present, discuss the techniques being used to carry out the task, describe application areas where IE systems are or are about to be at work, and conclude with a discussion of the challenges facing the area. What emerges is a picture of an exciting new text processing technology with a host of new applications, both on its own and in conjunction with other technologies, such as information retrieval, machine translation and data mining."
C98-2223,Word Sense Disambiguation using Optimised Combinations of Knowledge Sources,1998,21,54,1,1,37311,yorick wilks,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"Word sense disambiguation algorithms, with few exceptions, have made use of only one lexical knowledge source. We describe a system which performs word sense disambiguation on all content words in free text by combining different knowledge sources: semantic preferences, dictionary definitions and subject/domain codes along with part-of-speech tags, optimised by means of a learning algorithm. We also describe the creation of a new sense tagged corpus by combining existing resources. Tested accuracy of our approach on this corpus exceeds 92%, demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample."
C98-1111,Compacting the {P}enn {T}reebank Grammar,1998,5,38,4,0,55364,alexander krotov,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"Treebanks, such as the Penn Treebank (PTB), offer a simple approach to obtaining a broad coverage grammar: one can simply read the grammar off the parse trees in the treebank. While such a grammar is easy to obtain, a square-root rate of growth of the rule set with corpus size suggests that the derived grammar is far from complete and that much more treebanked text would be required to obtain a complete grammar, if one exists at some limit. However, we offer an alternative explanation in terms of the underspecification of structures within the treebank. This hypothesis is explored by applying an algorithm to compact the derived grammar by eliminating redundant rules - rules whose right hand sides can be parsed by other rules. The size of the resulting compacted grammar, which is significantly less than that of the full treebank grammar, is shown to approach a limit. However, such a compacted grammar does not yield very good performance figures. A version of the compaction algorithm taking rule probabilities into account is proposed, which is argued to be more linguistically motivated. Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall, but a loss in precision."
W97-0208,Sense Tagging: Semantic Tagging with a Lexicon,1997,16,41,1,1,37311,yorick wilks,"Tagging Text with Lexical Semantics: Why, What, and How?",0,"Sense tagging, the automatic assignment of the appropriate sense from some lexicon to each of the words in a text, is a specialised instance of the general problem of semantic tagging by category or type. We discuss which recent word sense disambignation algorithms are appropriate for sense tagging. It is our belief that sense tagging can be carried out effectively by combining several simple, independent, methods and we include the design of such a tagger. A prototype of this system has been implemented, correctly tagging 86% of polysemous word tokens in a small test set, providing evidence that our hypothesis is correct."
A97-2017,{GATE} - a General Architecture for Text Engineering,1997,34,126,4,1,41365,hamish cunningham,Fifth Conference on Applied Natural Language Processing: Descriptions of System Demonstrations and Videos,0,"Much progress has been made in the provision of reusable data resources for Natural Language Engineering, such as grammars, lexicons, thesauruscs. Although a number of projects have addressed the provision of reusable algorithmic resources (or 'tools'), takeup of these resources has been relatively slow. This paper describes GATE, a General Architecture for Text Engineering, which is a freely-available system designed to help alleviate the problem."
A97-1035,Software Infrastructure for Natural Language Processing,1997,9,62,4,1,41365,hamish cunningham,Fifth Conference on Applied Natural Language Processing,0,"We classify and review current approaches to software infrastructure for research, development and delivery of NLP systems. The task is motivated by a discussion of current trends in the field of NLP and Language Engineering. We describe a system called GATE (a General Architecture for Text Engineering) that provides a software infrastructure on top of which heterogeneous NLP processing modules may be evaluated and refined individually, or may be combined into larger application systems. GATE aims to support both researchers and developers working on component technologies (e.g. parsing, tagging, morphological analysis) and those working on developing end-user applications (e.g. information extraction, text summarisation, document generation, machine translation, and second language learning). GATE promotes reuse of component technology, permits specialisation and collaboration in large-scale projects, and allows for the comparison and evaluation of alternative technologies. The first release of GATE is now available."
X96-1027,{TIPSTER}-Compatible Projects at {S}heffield,1996,-1,-1,4,1,41365,hamish cunningham,"TIPSTER TEXT PROGRAM PHASE II: Proceedings of a Workshop held at Vienna, Virginia, May 6-8, 1996",0,None
X96-1059,{NEC} Corporation and {U}niversity of {S}heffield: {``}Description of {NEC}/{S}heffleld System Used For {MET} {J}apanese{''},1996,2,1,5,0,55880,yoshikazu takemoto,"TIPSTER TEXT PROGRAM PHASE II: Proceedings of a Workshop held at Vienna, Virginia, May 6-8, 1996",0,Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing ([1] [2]). It has also been studied in the framework of Japanese information extraction ([3]) in recent years.
C96-2118,An ascription-based approach to Speech Acts,1996,7,15,2,0,503,mark lee,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"The two principal areas of natural language processing research in pragmantics are belief modelling and speech act processing. Belief modelling is the development of techniques to represent the mental attitudes of a dialogue participant. The latter approach, speech act processing, based on speech act theory, involves viewing dialogue in planning terms. Utterances in a dialogue are modelled as steps in a plan where understanding an utterance involves deriving the complete plan a speaker is attempting to achieve. However, previous speech act based approaches have been limited by a reliance upon relatively simplistic belief modelling techniques and their relationship to planning and plan recognition. In particular, such techniques assume precomputed nested belief structures. In this paper, we will present an approach to speech act processing based on novel belief modelling techniques where nested beliefs are propagated on demand."
C96-2187,{GATE}-a General Architecture for Text Engineering,1996,-1,-1,2,1,41365,hamish cunningham,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,None
C96-1071,Evaluation of an Algorithm for the Recognition and Classification of Proper Names,1996,4,67,3,0,55307,takahiro wakao,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"We describe an information extraction system in which four classes of naming expressions - organisation, person, location and time names - are recognised and classified with nearly 92% combined precision and recall. The system applies a mixture of techniques to perform this task and these are described in detail. We have quantitatively evaluated the system against a blind test set of Wall Street Journal business articles and report results not only for the system as a whole, but for each component technique and for each class of name. These results show that in order to have high recall, the system needs to make use not only of information internal to the naming expression but also information from outside the name. They also show that the contribution of each system component varies from one class of name expression to another."
1996.amta-1.24,Panel: The limits of automation: optimists vs skeptics.,1996,-1,-1,7,0,1043,eduard hovy,Conference of the Association for Machine Translation in the Americas,0,None
C94-1019,Two Types of Adaptive {MT} Environments,1994,7,7,4,0,32552,sergei nirenburg,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"A number of proposal have come up in recent years for hybridization of MT. Current MT projects --- both pure and hybrid, both predominantly technology-oriented and scientific (including those currently funded by NSF) are single-engine projects, capable of one particular type of source text analysis, one particular method of finding target language correspondences for source language elements and one prescribed method of generating the target language text. While such projects can be quite useful, we believe that it is time to make the next step in the design of machine translation systems and to move toward adaptive, multiple-engine systems. We describe the architecture of an adaptive multi-engine MT system which uses each of the engines under the circumstances which are most favorable for its success."
1994.bcs-1.2,Some notes on the state of the art: Where are we now in {MT}: what works and what doesn{'}t?,1994,-1,-1,1,1,37311,yorick wilks,Proceedings of the Second International Conference on Machine Translation: Ten years on,0,"The paper examines briefly the impact of the {``}statistical turn{''} in machine translation (MT) R{\&}D in the last decade, and particularly the way in which it has made large scale language resources (lexicons, text corpora etc.) more important than ever before and reinforced the role of evaluation in the development of the field. But resources mean, almost by definition, co-operation between groups and, in the case of MT, specifically co-operation between language groups and states. The paper then considers what alternatives there are now for MT R{\&}D. One is to continue with interlingual methods of translation, even though those are not normally thought of as close to statistical methods. The reason is that statistical methods, taken alone, have almost certainly reached a ceiling in terms of the proportion of sentences and linguistic phenomena they can translate successfully. Interlingual methods remain popular within large electronics companies in Japan, and in a large US Government funded project (PANGLOSS). The question then discussed is what role there can be for interlinguas and interlingual methods in co-operation in MT across linguistic and national boundaries. The paper then turns to evaluation and asks whether, across national and continental boundaries, it can become a co-operative or a {``}hegemonic{''} enterprise. Finally the paper turns to resources themselves and asks why co-operation on resources is proving so hard, even though there are bright spots of real co-operation."
X93-1021,{CRL}/{B}randeis: The {D}iderot System,1993,8,7,9,0.833333,53660,jim cowie,"TIPSTER TEXT PROGRAM: PHASE {I}: Proceedings of a Workshop held at Fredricksburg, Virginia, September 19-23, 1993",0,Diderot is an information extraction system built at CRL and Brandeis University over the past two years. It was produced as part of our efforts in the Tipster project. The same overall system architecture has been used for English and Japanese and for the micro-electronics and joint venture domains.
1993.tc-1.1,Developments in machine translation research in the {US},1993,-1,-1,1,1,37311,yorick wilks,Proceedings of Translating and the Computer 15,0,None
1993.mtsummit-1.12,Corpora and Machine Translation,1993,-1,-1,1,1,37311,yorick wilks,Proceedings of Machine Translation Summit IV,0,None
M92-1014,{CRL}/{NMSU} and {B}randeis {M}uc{B}ruce: {MUC}-4 Test Results and Analysis,1992,-1,-1,3,0.833333,53660,jim cowie,"{F}ourth {M}essage {U}understanding {C}onference ({MUC}-4): Proceedings of a Conference Held in {M}c{L}ean, {V}irginia, {J}une 16-18, 1992",0,None
M92-1031,{CRL}/{NMSU} and {B}randeis: Description of the {M}uc{B}ruce System as Used for {MUC}-4,1992,5,6,3,0.833333,53660,jim cowie,"{F}ourth {M}essage {U}understanding {C}onference ({MUC}-4): Proceedings of a Conference Held in {M}c{L}ean, {V}irginia, {J}une 16-18, 1992",0,"Through their involvement in the Tipster project the Computing Research Laboratory at New Mexico State University and the Computer Science Department at Brandeis University are developing a method for identifying articles of interest and extracting and storing specific kinds of information from large volumes of Japanese and English texts. We intend that the method be general and extensible. The techniques involved are not explicitly tied to these two languages nor to a particular subject area. Development for Tipster has been going on since September, 1992."
J92-1007,Book Reviews: Semantic Structures,1992,-1,-1,1,1,37311,yorick wilks,Computational Linguistics,0,None
C92-2081,The Automatic Creation of Lexical Entries for a Multilingual {MT} System,1992,12,15,3,1,48134,david farwell,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper, we describe a method of extracting information from an on-line resource for the construction of lexical entries for a multi-lingual, interlingual MT system (ULTRA). We have been able to automatically generate lexical entries for interlingual concepts corresponding to nouns, verbs, adjectives and adverbs. Although several features of these entries continue to be supplied manually we have greatly decreased the time required to generate each entry and see this as a promising method for the creation of large-scale lexicons."
P91-1019,Subject-Dependent Co-Occurrence and Word Sense Disambiguation,1991,10,85,4,0,57277,joe guthriee,29th Annual Meeting of the Association for Computational Linguistics,1,"We describe a method for obtaining subject-dependent word sets relative to some (subject) domain. Using the subject classifications given in the machine-redable version of Longman's Dictionary of Contemporary English, we established subject-dependent co-occurrence links between words of the defining vocabulary to construct these neighborhoods. Here, we describe the application of these neighborhoods to information retrieval, and present a method of word sense disambiguation based on these co-occurrences, an extension of previous work."
H91-1094,Active Knowledge Structures in Natural Language Understanding,1991,0,0,1,1,37311,yorick wilks,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"To investigate a theory of message and discourse understanding based on the building of explanatory causal models and the nested beliefs of discourse agents, so as to construct robust systems for message understanding with wider application to machine translation and text retrieval."
1991.mtsummit-papers.3,{ULTRA}: A Multi-lingual Machine Translator,1991,-1,-1,2,1,48134,david farwell,Proceedings of Machine Translation Summit III: Papers,0,"ULTRA (Universal Language TRAnslator) is a multilingual, interlingual machine translation system currently under development at the Computing Research Laboratory at New Mexico State University. It translates between five languages (Chinese, English, German, Japanese, Spanish) with vocabularies in each language based on approximately 10,000 word senses. The major design criteria are that the system be robust and general purpose with simple to use utilities for customization to suit the needs of particular users. This paper describes the central characteristics of the system: the intermediate representation, the language components, semantic and pragmatic processes, and supporting lexical entry tools."
1991.mtsummit-panels.5,Evaluation of {MT} Systems,1991,1,0,2,0,50182,margaret king,Proceedings of Machine Translation Summit III: Panels,0,None
1991.iwpt-1.1,Proceedings of the Second International Workshop on Parsing Technologies ({IWPT} {'}91),1991,-1,-1,8,0,56874,masaru tomita,Proceedings of the Second International Workshop on Parsing Technologies,0,"February 13-25, 1991"
H90-1072,Machine Translation Again?,1990,0,2,1,1,37311,yorick wilks,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"Machine translation (MT) remains the paradigm task for natural language processing (NLP) since its inception in the 1950s. Unless NLP can succeed with the central task of machine translation, it cannot be considered successful as a field. We maintain that the most profitable approach to MT at the present time is an interlingual and modular one. MT is one the precious few computational tasks falling broadly within artificial intelligence (AI) that combine a fundamental intellectual research challenge with enormous proven need. To establish the latter, one only has to note that in Japan alone the current MT requirement is for 20 billion pages a year (a market of some $66 billion a year)."
H90-1093,PROGRESS REPORT: Active Knowledge Structures in Natural Language Understanding,1990,0,0,1,1,37311,yorick wilks,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,Work on the project has concentrated in the last six months on (a) reimplementing our two semantics-based message parsers and (b) integrating them with the KR formalism we use: a form of conceptual graphs (CG) embedded in the MGR (Model Generative Reasoning) framework.
C90-3025,Is there content in empty heads?,1990,6,47,3,1,37309,louise guthrie,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"We describe a technique for automatically constructing a taxonomy of word senses from a machine readable dictionary. Previous taxonomies developed from dictionaries have two properties in common. First, they are based on a somewhat loosely defined notion of the IS-A relation. Second, they require human intervention to identify the sense of the genus term being used. We believe that for taxonomies of this type to serve a useful role in subsequent natural language processing tasks, the taxonomy must be based on a consistent use of the IS-A relation which allows inheritance and transitivity. We show that hierarchies of this type can be automatically constructed, by using the semantic category codes and the subject codes of the Longman Dictionary of Contemporary English (LDOCE) to disambiguate the genus terms in noun definitions. In addition, we discuss how certain genus terms give rise to other semantic relations between definitions."
W89-0242,{PREMO}: Parsing by Conspicuous Lexical Consumption,1989,-1,-1,2,1,57321,brian slator,Proceedings of the First International Workshop on Parsing Technologies,0,"PREMO is a knowledge-based Preference Semantics parser with access to a large, lexical semantic knowledge base and organized along the lines of an operating system. The state of every partial parse is captured in a structure called a language object, and the control structure of the preference machine is a priority queue of these language objects. The language object at the front of the queue has the highest score as computed by a preference metric that weighs grammatical predictions, semantic type matching, and pragmatic coherence. The highest priority language object is the intermediate reading that is currently most preferred (the others are still {``}alive,{''} but not actively pursued); in this way the preference machine avoids combinatorial explosion by following a {``}best-first{''} strategy for parsing. The system has clear extensions into parallel processing."
H89-2030,Belief Ascription and Model Generative Reasoning: joining two paradigms to a robust parser of messages.,1989,40,3,1,1,37311,yorick wilks,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"This paper discusses the extension of ViewGen, a program for belief ascription, to the area of intensional object identification with applications to battle environments, and its combination in a overall system with MGR, a Model-Generative Reasoning system, and PREMO a semantics-based parser for robust parsing of noisy message data.ViewGen represents the beliefs of agents as explicit, partitioned proposition-sets known as environments. Environments are convenient, even essential, for addressing important pragmatic issues of reasoning. The paper concentrates on showing that the transfer of information in intensional object identification and belief ascription itself can both be seen as different manifestations of a single environment-amalgamation process. The entities we shall be concerned with will be ones, for example, the system itself believes to be separate entities while it is computing the beliefs and reasoning of a hostile agent that believes them to be the same entity (e.g. we believe enemy radar shows two of our ships to be the same ship, or vice-versa. The KAL disaster should bring the right kind of scenario to mind). The representational issue we address is how to represent that fictional single entity in the belief space of the other agent, and what content it should have given that it is an amalgamation of two real entities.A major feature of the paper is our work on embedding within the ViewGen belief-and-point-of-view system the knowledge representation system of our MGR reasoner, and then bringing together the multiple viewpoints offered by ViewGen with the multiple representations of MGR. The fusing of these techniques, we believe, offers a very strong system for extracting message gists from texts and reasoning about them."
H89-1029,{N}ew {M}exico {S}tate {U}niversity {C}omputing {R}esearch {L}aboratory,1989,47,0,1,1,37311,yorick wilks,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,CRL's contribution to DARPA's program is to bring to bear on natural language understanding two closely-related belief and context mechanisms: dynamic generation of nested belief structures (ViewGen) and hypotheses for reasoning and problem-solving (MGR).
C88-2153,Machine Tractable Dictionaries as Tools and Resources for Natural Language Processing,1988,29,29,1,1,37311,yorick wilks,{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper discusses three different but related large-scale computational methods for the transformation of machine readable dictionaries (MRDs) into machine tractable dictionaries, i.e., MRDs converted into a format usable for natural language processing tasks. The MRD used is The Longman Dictionary of Contemporary English."
1988.tc-1.17,Themes in the work of Margaret Masterman,1988,-1,-1,1,1,37311,yorick wilks,Proceedings of Translating and the Computer 10: The translation environment 10 years on,0,None
T87-1026,On keeping logic in its place,1987,4,4,1,1,37311,yorick wilks,Theoretical Issues in Natural Language Processing 3,0,"There need be no real dispute on this panel about what is meant , in the broades t terms, by formal semant ics (FS) when opposed to c o m m o n s e n s e semant ics (CSS): after registering his complaints and worries, the opposit ion David Israel opts for in his paper is broadly the one adopted here, and the model theore t ic semanticis ts he mentions will do just fine for me, and I suspect for Karen Sparck-Jones in her characterisation of a  logicis t approach to natural language processing. As will appear below, though, I want to list a range of s t rengths of FS c o m m i t m e n t , no t all of which are modeltheoret ic . So, whatever we find to argue about on this panel, it n e e d n ' t be those two terms."
E85-1013,Right Attachment and Preference Semantics .,1985,11,7,1,1,37311,yorick wilks,Second Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The paper claims that the right attachment rules for phrases originally suggested by Frazier and Fodor are wrong, and that none of the subsequent patchings of the rules by syntactic methods have improved the situation. For each rule there are perfectly straightforward and indefinitely large classes of simple counter-examples. We then examine suggestions by Ford et al., Schubert and Hirst which are quasi-semantic in nature and which we consider ingenious but unsatisfactory. We point towards a straightforward solution within the framework of preference semantics, set out in detail elsewhere, and argue that the principal issue is not the type and nature of information required to get appropriate phrase attachments, but the issue of where to store the information and with what processes to apply it."
1985.tmi-1.25,"Relevance, Points of View and Dialogue Modelling",1985,-1,-1,1,1,37311,yorick wilks,Proceedings of the first Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
J83-3004,"Preference Semantics, Ill-Formedness, and Metaphor",1983,10,75,2,0,54230,dan fass,American Journal of Computational Linguistics,0,"This paper is about the relationships between Preference Semantics (PS) and ill-formedness, and between Preference Semantics and metaphor. Two types of preference, declarative and procedural, are distinguished. The PS framework is examined with respect to notions of well- and ill-formedness, and two criteria for ill-formedness are distinguished, both of which are possessed by PS: an absolute criterion that corresponds to conventional notions of well- and ill-formedness, and a relative criterion that does not.Four possible strategies are described for representing ill-formed input in general, and metaphors in particular. The strategies and the semantic representations produced by them are compared regarding their correspondence to human understanding (admittedly superficial given the shallowness of the PS representation) and their ability to produce correct sentence translations. We conclude that, because of the ambiguity of many individual and extended metaphors, two broad types of metaphor representation strategy are needed. A control mechanism is described that uses both these major types of strategy and that permits the temporary semantic representation of metaphorical ambiguity."
1981.tc-1.23,Concluding remarks,1981,0,3,1,1,37311,yorick wilks,Translating and the Computer: Practical experience of machine translation,0,"The intention of this book is to examine and compare the possibilities for using solar energy in dwellings located in the various countries of the EC. Therefore, solar system hardware, design principles and the performance of various solar system types have been discussed in broad terms taking into account climatic, economic and energy policy aspects. In this discussion many of the accomplishments and difficulties of the emerging solar industry have been highlighted. There are presently several basic designs for solar hardware and solar energy systems, such as selective and non-selective collectors, passive and active solar systems, air-type and liquid-type system designs. The advantages and disadvantages of the various types of components and systems have been discussed at length. Further development, combined with increasing practical experience, will contribute to improving the technical performance and reliability of all types of solar components and systems. In turn this will lead to more economically optimum solutions. Thus it is very likely that domestic use of solar energy in the years ahead will be characterised by a variety of technical solutions, as have today's conventional space and water heating systems."
T78-1025,Semantic Primitives in Language and Vision,1978,7,3,1,1,37311,yorick wilks,Theoretical Issues in Natural Language Processing-2,0,None
J78-3032,Semantic Primitives in Language and Vision,1978,7,3,1,1,37311,yorick wilks,American Journal of Computational Linguistics,0,None
1978.tc-1.2,Machine translation and artificial intelligence Implementing machine aids to translation,1978,59,12,1,1,37311,yorick wilks,Translating and the Computer,0,None
J76-4007,Processing Case,1976,-1,-1,1,1,37311,yorick wilks,American Journal of Computational Linguistics,0,None
J76-1007,Natural Language Understanding Systems within the {A}. {I}. Paradigm: A Survey and Some Comparisons,1976,-1,-1,1,1,37311,yorick wilks,American Journal of Computational Linguistics,0,None
T75-2009,Primitives and Words,1975,3,19,1,1,37311,yorick wilks,Theoretical Issues in Natural Language Processing,0,"We may usefully distinguish between internal and external questions when discussing the use of primitives for representing natural language content and doing related semantic computations. Here I shall give a few examples of internal questions; go on to explain why I shall turn immediately to external questions; and finally discuss two of the latter: the justification of primitives in general, and the distinction, if any, between primitives and words."
T75-2026,Methodology in {AI} and Natural Language Understanding,1975,7,3,1,1,37311,yorick wilks,Theoretical Issues in Natural Language Processing,0,None
C69-0801,Interactive Semantic Analysis of {E}nglish Paragraphs,1969,6,1,1,1,37311,yorick wilks,{I}nternational {C}onference on {C}omputational {L}inguistics {COLING} 1969: Preprint No. 8,0,"This paper describes the use of an on-line system to do word-sense ambiguity resolution and content analysis of English text paragraphs, using a system of semantic analysis programmed in Q32 LISP 1.5. The system of semantic analysis comprised dictionary codings for the text words, coded forms of permitted message, and rules producing message forms in combination on the basis of a criterion of semantic closeness. All these can be expressed within a single system of rules of phrase-structure form. In certain circumstances the system is able to enlarge its own dictionary in a real-time mode on the basis of information gained from the actual texts analysed. An interpretation of the system in terms of meaningfulness is suggested."
