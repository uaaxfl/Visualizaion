2003.mtsummit-papers.6,C00-1078,0,0.0349095,"Missing"
2003.mtsummit-papers.6,A00-2018,1,0.515738,"Missing"
2003.mtsummit-papers.6,P01-1017,1,0.892719,"Missing"
2003.mtsummit-papers.6,P97-1003,0,0.237838,"Missing"
2003.mtsummit-papers.6,P01-1030,1,0.879232,"Missing"
2003.mtsummit-papers.6,W01-1406,0,0.0372217,"Missing"
2003.mtsummit-papers.6,P02-1038,0,0.279021,"Missing"
2003.mtsummit-papers.6,P02-1025,0,0.0737252,"Missing"
2003.mtsummit-papers.6,P01-1067,1,0.865271,"Missing"
2003.mtsummit-papers.6,P02-1039,1,0.891478,"Missing"
A00-2018,W98-1115,1,0.118561,"s required by Equation 7 cannot be used without smoothing. In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17]. While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation. (Actually, we use a minor variant described in [4].) 4 The E x p e r i m e n t We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model. For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard P C F G information. This allows the second pass to see expansions not present in the training corpus. We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words. We g"
A00-2018,P97-1003,0,0.405873,"thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. = 1-[ v(t I l,z).v(h I t,l,H).v(¢ I cE;¢ 132 Next we describe how we assign a probability to the expansion e of a constituent. In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank g r a m m a r [3] from the training corpus. The method that gives the best results, however, uses a Markov g r a m m a r - - a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15]. The method we use follows that of [10]. In this scheme a traditional probabilistic context-free g r a m m a r (PCFG) rule can be thought of as consisting of a left-hand side with a label l(e) drawn from the non-terminal symbols of our grammar, and a right-hand side t h a t is a sequence of one or more such symbols. (We assume t h a t all terminal symbols are generated by rules of the form ""preterm -+ word' and we treat these as a special case.) For us the non-terminal symbols are those of the tree-bank, augmented by the symbols aux and auxg, which have been assigned deterministically to cert"
A00-2018,W96-0212,1,0.125344,"s required by Equation 7 cannot be used without smoothing. In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17]. While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation. (Actually, we use a minor variant described in [4].) 4 The E x p e r i m e n t We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model. For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard P C F G information. This allows the second pass to see expansions not present in the training corpus. We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words. We g"
A00-2018,P80-1024,0,0.194654,"Missing"
A00-2018,J98-2004,1,\N,Missing
A00-2018,J98-4004,0,\N,Missing
A00-2018,J93-2004,0,\N,Missing
A00-2018,W99-0623,0,\N,Missing
A00-2018,J03-4003,0,\N,Missing
A00-2018,J96-1002,0,\N,Missing
A00-2018,P95-1037,0,\N,Missing
A00-2031,H90-1053,0,0.0394123,"Missing"
A00-2031,P97-1003,0,0.0404864,"Missing"
A00-2031,W97-0301,0,0.0118511,"Missing"
A00-2031,A00-2018,1,\N,Missing
A00-2031,J96-1002,0,\N,Missing
C08-1042,P06-1109,0,0.267974,"ced tags. We find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance. 2 Part-of-speech Tag Induction 1 Introduction There has been a great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech (Johnson, 2007; Goldwater and Griffiths, 2007; Biemann, 2006; Dasgupta and Ng, 2007) and deeper grammatical structure like constituency and dependency trees (Klein and Manning, 2004; Smith, 2006; Bod, 2006; Seginer, 2007; Van Zaanen, 2001). While some grammar induction systems operate on raw text, many of the most successful ones presume prior part-of-speech tagging. Meanwhile, most recent work in part-of-speech induction focuses on increasing the degree to which their tags match hand-annotated ones such as those in the Penn Treebank. In this work our goal is to evaluate how improvements in part-of-speech tag induction affects grammar induction. Using several different unsupervised taggers, we induce tags and train three grammar induction systems on the results. c 2008. Licensed under the Creat"
C08-1042,A00-2018,1,0.248661,"this by experimenting with a supervised parser, training off trees where the gold parts-of-speech have been removed and replaced with induced tags. Our expectation was that the brackets, the head propagation paths, and the phrasal categories in the training trees would be sufficient to overcome any loss in information that the gold tags might provide. Additionally it was possible the induced tags would ignore rare parts-of-speech such as FW, and make better use of the available tags, perhaps using new distributional clues not in the original tags. To this end we modified the Charniak Parser (Charniak, 2000) to train off induced parts-ofspeech. The Charniak parser is a lexicalized PCFG parser for which the part-of-speech of a head word is a key aspect of its model. During training, the head-paths from the gold part-of-speech tags are retained, but we replace the tags themselves. We ran experiments using the bitag HMM from Section 2.2 trained using EM, as well as with the Sch¨utze SVD tagger from Section 2.1. The parser was trained on sections 2-21 of the Penn Treebank for training and section 24 was used for evaluation. As before we calculated τ scores between each tagging metric and supervised f"
C08-1042,E03-1009,0,0.786416,"g supported by many different parameter settings. Following the setup in Johnson (2007), we initialize the transition and emission distributions to be uniform with a small amount of noise, and run EM and VB for 1000 iterations. We label these systems as HMM-EM and HMM-VB respectively in our experiments. In our VB experiments we set αi = βj = 0.1, ∀i ∈ {1, ..., |T |} , j ∈ {1, ..., |V |}, which yielded the best performance on most reported metrics in Johnson (2007). We use maximum marginal decoding, which Johnson (2007) reports performs better than Viterbi decoding. 2.3 Systems with Morphology Clark (2003) presents several part-of-speech induction systems which incorporate morphological as well as distributional information. We use the 330 implementation found on his website.1 2.3.1 Ney-Essen with Morphology The simplest model is based on work by (Ney et al., 1994). It uses a bitag HMM, with the restriction that each word type in the vocabulary can only be generated by a single part-of-speech. Thus the tag induction task here reduces to finding a multiway partition of the vocabulary. The learning algorithm greedily reassigns each word type to the part-of-speech that results in the greatest incr"
C08-1042,P07-3008,0,0.124333,"induction. Smith and Eisner (2004) present an alternative estimation technique for CCM which uses annealing to try to escape local maxima. Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing framework. Several approaches try to learn structure directly from raw text. Seginer (2007) has an incremental parsing approach using a novel representation called common-cover-links, which can be converted to constituent brackets. Van Zaanen (2001)’s ABL attempts to align sentences to determine what sequences of words are substitutable. The work closest in spirit to this paper is Cramer (2007), who evaluates several grammar induction systems on the Eindhoven corpus (Dutch). One of his experiments compares the grammar induction performance of these systems starting with tags induced using the system described by Biemann (2006), to the performance of the systems on manually-marked tags. However he does not evaluate to what degree better tagging performance leads to improvement in these systems. 3.2 Dependency Grammar Induction A dependency tree is a directed graph whose nodes are words in the sentence. A directed edge exists between two words if the target word (argument) is a depend"
C08-1042,D07-1023,0,0.124027,"es of unsupervised part-of-speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags. We find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance. 2 Part-of-speech Tag Induction 1 Introduction There has been a great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech (Johnson, 2007; Goldwater and Griffiths, 2007; Biemann, 2006; Dasgupta and Ng, 2007) and deeper grammatical structure like constituency and dependency trees (Klein and Manning, 2004; Smith, 2006; Bod, 2006; Seginer, 2007; Van Zaanen, 2001). While some grammar induction systems operate on raw text, many of the most successful ones presume prior part-of-speech tagging. Meanwhile, most recent work in part-of-speech induction focuses on increasing the degree to which their tags match hand-annotated ones such as those in the Penn Treebank. In this work our goal is to evaluate how improvements in part-of-speech tag induction affects grammar induction. Using several different unsupe"
C08-1042,C04-1052,0,0.0864307,"on, while morphological refers to modeling the internal structure of a word. All the systems below make use of distributional information, whereas only two use morphological features. We primarily focus on the metrics used to evaluate induced taggings. The catalogue of recent partof-speech systems is large, and we can only test 329 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 329–336 Manchester, August 2008 the tagging metrics using a few systems. Recent work that we do not explore explicitly includes (Biemann, 2006; Dasgupta and Ng, 2007; Freitag, 2004; Smith and Eisner, 2005). We have selected a few systems, described below, that represent a broad range of features and techniques to make our evaluation of the metrics as broad as possible. 2.1 (EM) algorithm, which searches for a local maximum in the likelihood of the observed words. Other methods approach the problem from a Bayesian perspective. These methods place Dirichlet priors over the parameters of each transition and emission multinomial. For an HMM with a set of states T and a set of output symbols V : Clustering using SVD and K-means ∀t ∈ T θt ∼ Dir(α1 , ...α|T |) (1) Sch¨utze (19"
C08-1042,H94-1020,0,0.107668,"bracketing F-score decreases from 77.6 to 72.9 and directed dependency accuracy measures decreases from 47.5 to 42.3 when switching to induced tags from gold. However for each metric, the systems still do quite well with induced tags. As in the constituency case, Smith (2006) presents several alternative estimation procedures for DMV, which try to minimize the local maximum problems inherent in EM. It is thus possible these methods might yield better performance for the models when run off of induced tags. 4 Experiments We induce tags with each system on the Penn Treebank Wall Street Journal (Marcus et al., 1994), sections 0-10, which contain 20,260 sentences. We vary the number of tags (10, 20, 50) and run each system 10 times for a given setting. The result of each run is used as the input to the CCM, DMV, and CCM+DMV systems. While the tags are induced from all sentences in the section, following the practice in (Klein and Manning, 2002; Klein and Manning, 2004), we remove punctuation, and consider only sentences of length not greater than 10 in our grammar induction experiments. Taggings are evaluated after punctuation is removed, but before filtering for length. To explore the relationship betwee"
C08-1042,J94-2001,0,0.351468,"in three grammar induction systems on the results. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Part-of-speech tag induction can be thought of as a clustering problem where, given a corpus of words, we aim to group word tokens into syntactic classes. Two tasks are commonly labeled unsupervised part-of-speech induction. In the first, tag induction systems are allowed the use of a tagging dictionary, which specifies for each word a set of possible parts-of-speech (Merialdo, 1994; Smith and Eisner, 2005; Goldwater and Griffiths, 2007). In the second, only the word tokens and sentence boundaries are given. In this work we focus on this latter task to explore grammar induction in a maximally unsupervised context. Tag induction systems typically focus on two sorts of features: distributional and morphological. Distributional refers to what sorts of words appear in close proximity to the word in question, while morphological refers to modeling the internal structure of a word. All the systems below make use of distributional information, whereas only two use morphological"
C08-1042,E95-1020,0,0.580591,"Missing"
C08-1042,P07-1049,0,0.40585,"e find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance. 2 Part-of-speech Tag Induction 1 Introduction There has been a great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech (Johnson, 2007; Goldwater and Griffiths, 2007; Biemann, 2006; Dasgupta and Ng, 2007) and deeper grammatical structure like constituency and dependency trees (Klein and Manning, 2004; Smith, 2006; Bod, 2006; Seginer, 2007; Van Zaanen, 2001). While some grammar induction systems operate on raw text, many of the most successful ones presume prior part-of-speech tagging. Meanwhile, most recent work in part-of-speech induction focuses on increasing the degree to which their tags match hand-annotated ones such as those in the Penn Treebank. In this work our goal is to evaluate how improvements in part-of-speech tag induction affects grammar induction. Using several different unsupervised taggers, we induce tags and train three grammar induction systems on the results. c 2008. Licensed under the Creative Commons Att"
C08-1042,P04-1062,0,0.0900445,"onstituents spanning the same bracket. They evaluate their CCM system on the Penn Treebank WSJ sentences of length 10 or less, using part-of-speech tags induced by the baseline system of Sch¨utze (1995). They report that switching to induced tags decreases the overall bracketing F-score from 71.1 to 63.2, although the recall of VP and S constituents actually improves. Additionally, they find that NP and PP recall decreases substantially with induced tags. They attribute this to the fact that nouns end up in many induced tags. There has been quite a bit of other work on constituency induction. Smith and Eisner (2004) present an alternative estimation technique for CCM which uses annealing to try to escape local maxima. Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing framework. Several approaches try to learn structure directly from raw text. Seginer (2007) has an incremental parsing approach using a novel representation called common-cover-links, which can be converted to constituent brackets. Van Zaanen (2001)’s ABL attempts to align sentences to determine what sequences of words are substitutable. The work closest in spirit to this paper is Cramer (2007), who evaluates sever"
C08-1042,P05-1044,0,0.17648,"induction systems on the results. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Part-of-speech tag induction can be thought of as a clustering problem where, given a corpus of words, we aim to group word tokens into syntactic classes. Two tasks are commonly labeled unsupervised part-of-speech induction. In the first, tag induction systems are allowed the use of a tagging dictionary, which specifies for each word a set of possible parts-of-speech (Merialdo, 1994; Smith and Eisner, 2005; Goldwater and Griffiths, 2007). In the second, only the word tokens and sentence boundaries are given. In this work we focus on this latter task to explore grammar induction in a maximally unsupervised context. Tag induction systems typically focus on two sorts of features: distributional and morphological. Distributional refers to what sorts of words appear in close proximity to the word in question, while morphological refers to modeling the internal structure of a word. All the systems below make use of distributional information, whereas only two use morphological features. We primarily"
C08-1042,P07-1094,0,0.726759,"plores the relationship between various measures of unsupervised part-of-speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags. We find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance. 2 Part-of-speech Tag Induction 1 Introduction There has been a great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech (Johnson, 2007; Goldwater and Griffiths, 2007; Biemann, 2006; Dasgupta and Ng, 2007) and deeper grammatical structure like constituency and dependency trees (Klein and Manning, 2004; Smith, 2006; Bod, 2006; Seginer, 2007; Van Zaanen, 2001). While some grammar induction systems operate on raw text, many of the most successful ones presume prior part-of-speech tagging. Meanwhile, most recent work in part-of-speech induction focuses on increasing the degree to which their tags match hand-annotated ones such as those in the Penn Treebank. In this work our goal is to evaluate how improvements in part-of-speech tag induction affects grammar in"
C08-1042,N06-1041,0,0.200006,"d part-of-speech class to a gold tag. One option is what Johnson (2007) calls “many-to-one” (M-to-1) accuracy, in which each induced tag is labeled with its most frequent gold tag. Although this results in a situation where multiple induced tags may share a single gold tag, it does not punish a system for providing tags of a finer granularity than the gold standard. In contrast, “one-to-one” (1-to-1) accuracy restricts each gold tag to having a single induced tag. The mapping typically is made to try to give the most favorable mapping in terms of accuracy, typically using a greedy assignment (Haghighi and Klein, 2006). In cases where the number of gold tags is different than the number of induced tags, some must necessarily remain unassigned (Johnson, 2007). In addition to accuracy, there are several information theoretic criteria presented in the literature. These escape the problem of trying to find an appropriate mapping between induced and gold tags, at the expense of perhaps being less intuitive. Let TI be the tag assignments to the words in the corpus created by an unsupervised tagger, and let TG be the gold standard tag assignments. Clark (2003) uses Shannon’s conditional entropy of the gold tagging"
C08-1042,D07-1031,0,0.36844,"Missing"
C08-1042,P02-1017,0,0.539013,"niques to make our evaluation of the metrics as broad as possible. 2.1 (EM) algorithm, which searches for a local maximum in the likelihood of the observed words. Other methods approach the problem from a Bayesian perspective. These methods place Dirichlet priors over the parameters of each transition and emission multinomial. For an HMM with a set of states T and a set of output symbols V : Clustering using SVD and K-means ∀t ∈ T θt ∼ Dir(α1 , ...α|T |) (1) Sch¨utze (1995) presents a series of part-of-speech inducers based on distributional clustering. We implement the baseline system, which Klein and Manning (2002) use for their grammar induction experiments with induced part-of-speech tags. For each word type w in the vocabulary V , the system forms a feature row vector consisting of the number of times each of the F most frequent words occur to the left of w and to the right of w. It normalizes these row vectors and assembles them into a |V |× 2F matrix. It then performs a Singular Value Decomposition on the matrix and rank reduces it to decrease its dimensionality to d principle components (d < 2F ). This results in a representation of each word as a point in a d dimensional space. We follow Klein an"
C08-1042,P04-1061,0,0.567593,"ervised parsing models trained on induced tags. We find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance. 2 Part-of-speech Tag Induction 1 Introduction There has been a great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech (Johnson, 2007; Goldwater and Griffiths, 2007; Biemann, 2006; Dasgupta and Ng, 2007) and deeper grammatical structure like constituency and dependency trees (Klein and Manning, 2004; Smith, 2006; Bod, 2006; Seginer, 2007; Van Zaanen, 2001). While some grammar induction systems operate on raw text, many of the most successful ones presume prior part-of-speech tagging. Meanwhile, most recent work in part-of-speech induction focuses on increasing the degree to which their tags match hand-annotated ones such as those in the Penn Treebank. In this work our goal is to evaluate how improvements in part-of-speech tag induction affects grammar induction. Using several different unsupervised taggers, we induce tags and train three grammar induction systems on the results. c 2008."
C08-1042,J06-4002,0,0.0295399,"G |TI ) (Figure 1), and CCM f-score vs. variation of information (Figure 2). These were selected because they have relatively high magnitude τ s. From these plots it is clear that although there 333 may be a slight correspondence, the relationships are weak at best. Each tagging and grammar induction metric gives us a ranking over the set of taggings of the data generated over the course of our experiments. These are ordered from best to worst according to the metric, so for instance H(TG |TI ) would give highest rank to its lowest value. We can compare the two rankings using Kendall’s τ (see Lapata (2006) for an overview), a nonparametric measure of correspondence for rankings. τ measures the difference between the number of concordant pairs (items the two rankings place in the same order) and discordant pairs (those the rankings place in opposite order), divided by the total number of pairs. A value of 1 indicates the rankings have perfect correspondence, -1 indicates they are in the opposite order, and 0 indicates they are independent. The τ values are shown in Table 2. The scatter-plot in Figure 1 shows the τ with the greatest magnitude. However, we can see that even these rankings have bar"
C08-1042,P06-3002,0,\N,Missing
C08-1071,P05-1022,1,0.871349,"act of search errors, value of non-generative reranker features, and effects of unknown words). From these experiments, we gain a better understanding of why self-training works for parsing. Since improvements from selftraining are correlated with unknown bigrams and biheads but not unknown words, the benefit of self-training appears most influenced by seeing known words in new combinations. 1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure (Bod, 2003; Charniak and Johnson, 2005; Collins and Koo, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These annotations can be used by various higher-level applications such as semantic role labeling (Pradhan et al., 2007) and machine translation (Yamada and Knight, 2001). c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. However, labeled training data is expensive to annotate. Given the large amount of unlabeled text available for many domains and languages, techniques which allow us to use bot"
C08-1071,N06-1020,1,0.72165,"pe of semi-supervised learning. In self-training, first we train a model on the labeled data and use that model to label the unlabeled data. From the combination of our original labeled data and the newly labeled data, we train a second model – our self-trained model. The process can be iterated, where the self-trained model is used to label new data in the next iteration. One can think of self-training as a simple case of cotraining (Blum and Mitchell, 1998) using a single learner instead of several. Alternatively, one can think of it as one step of the Viterbi EM algorithm. Studies prior to McClosky et al. (2006) failed to show a benefit to parsing from self-training (Charniak, 1997; Steedman et al., 2003). While the recent success of self-training has demonstrated its merit, it remains unclear why self-training helps in some cases but not others. Our goal is to better understand when and why self-training is beneficial. In Section 2, we discuss the previous applications of self-training to parsing. Section 3 describes our experimental setup. We present and test four hypotheses of why self-training helps in Section 4 and conclude with discussion and future work in Section 5. 2 Previous Work To our kno"
C08-1071,J93-2004,0,0.0369226,"Missing"
C08-1071,P06-1055,0,0.0243637,"ker features, and effects of unknown words). From these experiments, we gain a better understanding of why self-training works for parsing. Since improvements from selftraining are correlated with unknown bigrams and biheads but not unknown words, the benefit of self-training appears most influenced by seeing known words in new combinations. 1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure (Bod, 2003; Charniak and Johnson, 2005; Collins and Koo, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These annotations can be used by various higher-level applications such as semantic role labeling (Pradhan et al., 2007) and machine translation (Yamada and Knight, 2001). c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. However, labeled training data is expensive to annotate. Given the large amount of unlabeled text available for many domains and languages, techniques which allow us to use both labeled and unlabeled text to train our mo"
C08-1071,N07-1070,0,0.00990236,"provements from selftraining are correlated with unknown bigrams and biheads but not unknown words, the benefit of self-training appears most influenced by seeing known words in new combinations. 1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure (Bod, 2003; Charniak and Johnson, 2005; Collins and Koo, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These annotations can be used by various higher-level applications such as semantic role labeling (Pradhan et al., 2007) and machine translation (Yamada and Knight, 2001). c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. However, labeled training data is expensive to annotate. Given the large amount of unlabeled text available for many domains and languages, techniques which allow us to use both labeled and unlabeled text to train our models are desirable. These methods are called semi-supervised. Selftraining is a specific type of semi-supervised learning. In self-training, first we"
C08-1071,P07-1080,0,0.0118957,"ects of unknown words). From these experiments, we gain a better understanding of why self-training works for parsing. Since improvements from selftraining are correlated with unknown bigrams and biheads but not unknown words, the benefit of self-training appears most influenced by seeing known words in new combinations. 1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure (Bod, 2003; Charniak and Johnson, 2005; Collins and Koo, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These annotations can be used by various higher-level applications such as semantic role labeling (Pradhan et al., 2007) and machine translation (Yamada and Knight, 2001). c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. However, labeled training data is expensive to annotate. Given the large amount of unlabeled text available for many domains and languages, techniques which allow us to use both labeled and unlabeled text to train our models are desirable. These me"
C08-1071,P01-1067,0,0.00770831,"h unknown bigrams and biheads but not unknown words, the benefit of self-training appears most influenced by seeing known words in new combinations. 1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure (Bod, 2003; Charniak and Johnson, 2005; Collins and Koo, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These annotations can be used by various higher-level applications such as semantic role labeling (Pradhan et al., 2007) and machine translation (Yamada and Knight, 2001). c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. However, labeled training data is expensive to annotate. Given the large amount of unlabeled text available for many domains and languages, techniques which allow us to use both labeled and unlabeled text to train our models are desirable. These methods are called semi-supervised. Selftraining is a specific type of semi-supervised learning. In self-training, first we train a model on the labeled data and use that mod"
C08-1071,J05-1003,0,\N,Missing
C08-1071,P07-1078,0,\N,Missing
C08-1071,E03-1005,0,\N,Missing
C98-2177,J93-1003,0,0.0101698,"common nouns may not occur ill the corpus more than a handful of times in such a context. The two figures of merit that we employ, one to select and one to produce a final rank, use the following two counts for each noun: 1. a n o u n ' s c o - o c c t l r r e n c e 8 w i t h seed words 2. a. noun's co-occurrences with any word To select new seed words, we take the ratio of count 1. to count 2 for tile noun in question. This is similar to tile figure of merit used in R&S, and also tends to promote low fl:equency norms, f o r the final ranking, we chose the log likelihood statistic outlined in Dunning (1993), which is based upon the co-occurrence counts of all nouns (see Dunning tbr details). This statistic essentially measures how surprising the given pattern of co-occurrence would be if the distributions were completely random, l.br instance, suppose that two words occur forty times ea,ch, and they co-occur twenty times in a millionword corpus. This would be more surprising for two completely random distributions than if they had each occurred twice and had always co-occurred. A simple probability does not capture this fact. The rationale for using two different statistics for this task is t h"
C98-2177,W97-0313,0,0.51128,"ple, both Escort and Chinook are (among other things) types of vehicles (a car and a helicopter, respectively), but neither are cited as so in Wordnet. Manually building domain-specific lexicons can be a costly, time-consuming affair. Utilizing existing resources, such as on-line corpora, to aid in this task could improve performance both by decreasing the time to construct the lexicon and by improving its quality. Extracting semantic information from word co-occurrence statistics has been effective, particularly for sense disambiguation (Schiitze, 1992; Gale et al., 1992; Yarowsky, 1995). In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal cate1110 gory membership, for the purpose of aiding in the construction of semantic lexicons. Generically, their algorithm can be outlined as follows: 1. For a given category, choose a small set of exemplars (or 'seed words') 2. Count co-occurrence of words and seed words within a corpus 3. Use a figure of merit based upon these counts to select new seed words 4. Return to step 2 and iterate n times 5. Use a figure of merit to rank words for category membership and output a ranked list Our algorithm uses roughly this same generic st"
C98-2177,P95-1007,0,0.00989848,"or if the resulting compound has already been output, tile entry is skipped. Each noun is evaluated as follows: First, the head of that noun is determined. To get a sense of what is meant here, consider the following compound: nuclear-powered aircraft carrier. In evaluating tile word nuclearpowered, it is unclear if this word is attached to aircraft or to carrier. While we know that lhe head of the entire compound is carrier, in order to properly evaluate the word in question, we must determine which of the words following it is its head. This is done, in the spirit of the Dependency Model of Lauer (1995), by selecting the noun to its right in tile compound with the highest probability of occuring with the word in question when occurring ill a nouu compound. (In the case that two nouns have the same probability, the rightmost noun is chosen.) Once the head of the word is determined, the ratio of count 1 (with the head noun chosen) to count 2 is compared to an empirically set cutoff. If it falls below that cutoff, it is omitted. If it does not fall below the cutoff, then it is kept (provided its head noun is not later omitted). 6 O u t l i n e of t h e a l g o r i t h m The input to tlle algori"
C98-2177,P95-1026,0,0.00635419,"inadequate. For example, both Escort and Chinook are (among other things) types of vehicles (a car and a helicopter, respectively), but neither are cited as so in Wordnet. Manually building domain-specific lexicons can be a costly, time-consuming affair. Utilizing existing resources, such as on-line corpora, to aid in this task could improve performance both by decreasing the time to construct the lexicon and by improving its quality. Extracting semantic information from word co-occurrence statistics has been effective, particularly for sense disambiguation (Schiitze, 1992; Gale et al., 1992; Yarowsky, 1995). In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal cate1110 gory membership, for the purpose of aiding in the construction of semantic lexicons. Generically, their algorithm can be outlined as follows: 1. For a given category, choose a small set of exemplars (or 'seed words') 2. Count co-occurrence of words and seed words within a corpus 3. Use a figure of merit based upon these counts to select new seed words 4. Return to step 2 and iterate n times 5. Use a figure of merit to rank words for category membership and output a ranked list Our algorithm us"
C98-2177,J93-2004,0,0.0334145,"Missing"
D10-1066,P05-1022,1,0.81895,"Missing"
D10-1066,A00-2018,1,0.604134,"Missing"
D10-1066,P96-1041,0,0.0194948,"rest size we picked is 4400 nodes. For the look-ahead probability, LAP, we use a single decision tree with greedy optimal questions and 1600 nodes. We smooth our random forest probabilities by successively backing off to distributions three earlier in the decision tree. We use linear interpolation so pl (x |t) = λ(cl )∗ pˆl (x |t)+(1−λ(cl ))∗pl−3 (x |t) Here pl is the smoothed distribution for level l of the tree and pˆl is the maximum likelihood (unsmoothed) distribution. We use Chen smoothing so the linear interpolation parameters λ are functions of the Chen number of the level l node. See (Chen and Goodman, 1996). We could back off to l − 1, but this would slow the algorithm, and seemed unnecessary. 679 Following (Klein and Manning, 2003) we handle unknown and rare words by replacing them with one of about twenty unknown word types. For example, “barricading” would be replaced by UNK-ING, denoting an unknown word ending in “ing.” Any word that occurs less than twenty times in the training corpus is considered rare. The only information that is retained about it is the parts of speech with which it has appeared. Future uses are restricted to these pre-terminals. Because random forests have so much lati"
D10-1066,P04-1015,0,0.0474606,"Missing"
D10-1066,J03-4003,0,0.125175,"Missing"
D10-1066,P81-1022,0,0.791923,"does so. We are using the terms “top-down” and “leftto-right” following e.g., (Abney and Johnson, 1991; Roark, 2001). In particular In top-down strategies a node is enumerated before any of its descendents.(Abney and Johnson, 1991) In this era of statistical parsers it is useful to think in terms of possible conditioning information. In typical bottom up CKY parsing when creating, say, a constituent X from positions i to j we may not condition on its parent. That the grammar is “context-free” means that this constituent may be used anywhere. Using our definition, the Earley parsing algorithm(Earley, 1970), which is often cited as “top-down,” is no such thing. In fact, it is long been noted that the Earley algorithm is “almost identical”(Graham et al., 1980) to CKY. Again, when Earley posits an X it may not condition on the parent. Similarly, consider the more recent work of Nivre(Nivre, 2003) and Henderson(Henderson, 675 parse (w0,n−1 ) 1 C[0](= h =< q, r, t >) ←< 1, 1, ROOT > 2 for i = 0, n 3 do while ABOVE-THRESHOLD (h, C, N ) 4 remove h from C 5 for all x such that p(x |t) > 0 6 let h′ =< q ′ , r′ , t′ > 7 where q ′ = q ∗ p(x |t), r′ = LAP(t′ , w′ ), and t′ = t ◦ x 8 if(x = w) then w′ = wi+"
D10-1066,N03-1014,0,0.697975,"Missing"
D10-1066,C00-1052,0,0.0301291,"at it can overfit the training data, although we have not done an explicit study to confirm this. As a mild corrective we only allow verbs appearing 75 times or more, and all other words appearing 250 times or more, to be conditioned upon in question types 16, 18, 20, 22, and 27. Because the inner loop of random-forest training involves moving a conditioning event to the other decedent node to see if this raises training data probability, this also substantially speeds up training time. Lastly Roark obtained the results we quote here with selective use of left-corner transforms (Demers, 1977; Johnson and Roark, 2000). We also use this technique but the details differ. Roark uses left-corner transforms only for immediately recursive NP’s, the most common situation by far. As it was less trouble to do so, we use them for any immediately recursive constituent. However, we are also aware that in some respects left-corner transforms work against the fully-connected tree rule as operationalizing the “understand as you go along” cognitive constraint. For example, the normal sentence initial NP serves as the subject of the sentence. However in Penn-treebank grammar style an initial NP could also be a possessive N"
D10-1066,P03-1054,0,0.0429272,"ions and 1600 nodes. We smooth our random forest probabilities by successively backing off to distributions three earlier in the decision tree. We use linear interpolation so pl (x |t) = λ(cl )∗ pˆl (x |t)+(1−λ(cl ))∗pl−3 (x |t) Here pl is the smoothed distribution for level l of the tree and pˆl is the maximum likelihood (unsmoothed) distribution. We use Chen smoothing so the linear interpolation parameters λ are functions of the Chen number of the level l node. See (Chen and Goodman, 1996). We could back off to l − 1, but this would slow the algorithm, and seemed unnecessary. 679 Following (Klein and Manning, 2003) we handle unknown and rare words by replacing them with one of about twenty unknown word types. For example, “barricading” would be replaced by UNK-ING, denoting an unknown word ending in “ing.” Any word that occurs less than twenty times in the training corpus is considered rare. The only information that is retained about it is the parts of speech with which it has appeared. Future uses are restricted to these pre-terminals. Because random forests have so much latitude in picking combinations of words for specific situations we have the impression that it can overfit the training data, alth"
D10-1066,J93-2004,0,0.0346462,"Missing"
D10-1066,W03-3017,0,0.0407352,"erms of possible conditioning information. In typical bottom up CKY parsing when creating, say, a constituent X from positions i to j we may not condition on its parent. That the grammar is “context-free” means that this constituent may be used anywhere. Using our definition, the Earley parsing algorithm(Earley, 1970), which is often cited as “top-down,” is no such thing. In fact, it is long been noted that the Earley algorithm is “almost identical”(Graham et al., 1980) to CKY. Again, when Earley posits an X it may not condition on the parent. Similarly, consider the more recent work of Nivre(Nivre, 2003) and Henderson(Henderson, 675 parse (w0,n−1 ) 1 C[0](= h =< q, r, t >) ←< 1, 1, ROOT > 2 for i = 0, n 3 do while ABOVE-THRESHOLD (h, C, N ) 4 remove h from C 5 for all x such that p(x |t) > 0 6 let h′ =< q ′ , r′ , t′ > 7 where q ′ = q ∗ p(x |t), r′ = LAP(t′ , w′ ), and t′ = t ◦ x 8 if(x = w) then w′ = wi+1 insert h′ in N ′ 9 else w = w 10 insert h′ in C 11 empty C 12 exchange C and N 13 output t(C[0]). Figure 1: Roark’s Fully-Connected Parsing Algorithm 2003). The reason these are not fully-connected is the same. While they are incremental parsers, they are not top down — both are shift-reduc"
D10-1066,P06-1055,0,0.0456788,"Missing"
D10-1066,J01-2004,0,0.884919,"because this parser type is comparatively little explored one may hope for further substantial improvements, and proposes avenues to be explored. 2 Previous Work on Top-Down Parsing and the Roark Model We care about top-down incremental parsing because it automatically satisfies the criteria we have established for cognitive plausibility. Before looking at previous work on this type model we briefly discuss work that does not meet the criteria we have set out, but which people often assume does so. We are using the terms “top-down” and “leftto-right” following e.g., (Abney and Johnson, 1991; Roark, 2001). In particular In top-down strategies a node is enumerated before any of its descendents.(Abney and Johnson, 1991) In this era of statistical parsers it is useful to think in terms of possible conditioning information. In typical bottom up CKY parsing when creating, say, a constituent X from positions i to j we may not condition on its parent. That the grammar is “context-free” means that this constituent may be used anywhere. Using our definition, the Earley parsing algorithm(Earley, 1970), which is often cited as “top-down,” is no such thing. In fact, it is long been noted that the Earley a"
D10-1066,W04-3242,0,\N,Missing
D13-1148,S07-1002,0,0.135116,"tries many different seeds for bootstrapping and chooses the “best” classifier at the end. Eisner and Karakos’s algorithm is limited in that their system is designed for disambiguating words that have only 2 senses. Bayesian WSI systems have been developed by several authors. Brody and Lapata (2009) apply Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to WSI. They run a topic modeling algorithm on texts with some fixed number of topics that correspond to senses and induce a cluster by finding target words assigned to the same topic. Their system is evaluated on SemEval-2007 noun data (Agirre and Soroa, 2007). Lau et al. (2012) apply a nonparametric model, Hierarchical Dirichlet Processes (HDP), to SemEval-2010 data (Manandhar et al., 2010). 3 Model Following Yarowsky (1995), we assume that a word in a document has one sense. Multiple occurrences of a word in a document refer to the same object or concept. The naive Bayes model is well suited for this one-sense-per-document assumption. Each document has one topic corresponding to the sense of the target word that needs disambiguation. Context words in a document are drawn from the conditional distribution of words given the sense. Context words ar"
D13-1148,E09-1013,0,0.088607,"ation and one-sense-per-discourse. But this algorithm cannot easily be scaled up because for any new ambiguous word humans need to pick a few seed words, which initialize the algorithm. In order to automate the semi-supervised system, Eisner and Karakos (2005) propose an unsupervised bootstrapping algorithm. Their system tries many different seeds for bootstrapping and chooses the “best” classifier at the end. Eisner and Karakos’s algorithm is limited in that their system is designed for disambiguating words that have only 2 senses. Bayesian WSI systems have been developed by several authors. Brody and Lapata (2009) apply Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to WSI. They run a topic modeling algorithm on texts with some fixed number of topics that correspond to senses and induce a cluster by finding target words assigned to the same topic. Their system is evaluated on SemEval-2007 noun data (Agirre and Soroa, 2007). Lau et al. (2012) apply a nonparametric model, Hierarchical Dirichlet Processes (HDP), to SemEval-2010 data (Manandhar et al., 2010). 3 Model Following Yarowsky (1995), we assume that a word in a document has one sense. Multiple occurrences of a word in a document refer to th"
D13-1148,H05-1050,0,0.0279334,"e relevant previous work. In Section 3 and 4 we introduce the naive Bayes model for WSI and inference schemes for the model. In Section 5 we evaluate the model on SemEval-2010 data. In Section 6 we conclude. Related Work Yarowsky (1995) introduces a semi-supervised bootstrapping algorithm with two assumptions that rivals supervised algorithms: one-sense-percollocation and one-sense-per-discourse. But this algorithm cannot easily be scaled up because for any new ambiguous word humans need to pick a few seed words, which initialize the algorithm. In order to automate the semi-supervised system, Eisner and Karakos (2005) propose an unsupervised bootstrapping algorithm. Their system tries many different seeds for bootstrapping and chooses the “best” classifier at the end. Eisner and Karakos’s algorithm is limited in that their system is designed for disambiguating words that have only 2 senses. Bayesian WSI systems have been developed by several authors. Brody and Lapata (2009) apply Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to WSI. They run a topic modeling algorithm on texts with some fixed number of topics that correspond to senses and induce a cluster by finding target words assigned to the sam"
D13-1148,S10-1080,0,0.0195806,"ced group of tokens as a cluster. We refer to the model learned with the sampler and EM as NB, and to the model learned with EM only as NB0. 5.4.1 Short Descriptions of Other WSI Systems Evaluated on SemEval-2010 The baseline assigns every instance of a target word with the most frequent sense (MFS). UoY runs a clustering algorithm on a graph with words as nodes and co-occurrences between words as edges (Korkontzelos and Manandhar, 2010). Hermit approximates co-occurrence space with Random Indexing and applies a hybrid of k-means and Hierarchical Agglomerate Clustering to co-occurrence space (Jurgens and Stevens, 2010). NMFlib factors a matrix using nonnegative matrix factorization and runs a clustering algorithm on test instances represented by factors (Van de Cruys et al., 2011). 5.4.2 V-Measure V-Measure computes the quality of induced clusters as the harmonic mean of two values, homogeneity and completeness. Homogeneity measures whether instances of a cluster belong to a single gold class. Completeness measures whether instances of a gold class belong to a cluster. V-Measure is between 0 and 1; higher is better. See Table 3 for details of V-Measure evaluation (#cl is the number of induced clusters). Wit"
D13-1148,S10-1079,0,0.456987,"metrics for unsupervised evaluation (VMeasure, paired F-Score) and one metric for supervised evaluation (supervised recall). We refer to the true group of tokens as a gold class and to an induced group of tokens as a cluster. We refer to the model learned with the sampler and EM as NB, and to the model learned with EM only as NB0. 5.4.1 Short Descriptions of Other WSI Systems Evaluated on SemEval-2010 The baseline assigns every instance of a target word with the most frequent sense (MFS). UoY runs a clustering algorithm on a graph with words as nodes and co-occurrences between words as edges (Korkontzelos and Manandhar, 2010). Hermit approximates co-occurrence space with Random Indexing and applies a hybrid of k-means and Hierarchical Agglomerate Clustering to co-occurrence space (Jurgens and Stevens, 2010). NMFlib factors a matrix using nonnegative matrix factorization and runs a clustering algorithm on test instances represented by factors (Van de Cruys et al., 2011). 5.4.2 V-Measure V-Measure computes the quality of induced clusters as the harmonic mean of two values, homogeneity and completeness. Homogeneity measures whether instances of a cluster belong to a single gold class. Completeness measures whether in"
D13-1148,E12-1060,0,0.101693,"ds for bootstrapping and chooses the “best” classifier at the end. Eisner and Karakos’s algorithm is limited in that their system is designed for disambiguating words that have only 2 senses. Bayesian WSI systems have been developed by several authors. Brody and Lapata (2009) apply Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to WSI. They run a topic modeling algorithm on texts with some fixed number of topics that correspond to senses and induce a cluster by finding target words assigned to the same topic. Their system is evaluated on SemEval-2007 noun data (Agirre and Soroa, 2007). Lau et al. (2012) apply a nonparametric model, Hierarchical Dirichlet Processes (HDP), to SemEval-2010 data (Manandhar et al., 2010). 3 Model Following Yarowsky (1995), we assume that a word in a document has one sense. Multiple occurrences of a word in a document refer to the same object or concept. The naive Bayes model is well suited for this one-sense-per-document assumption. Each document has one topic corresponding to the sense of the target word that needs disambiguation. Context words in a document are drawn from the conditional distribution of words given the sense. Context words are assumed to be ind"
D13-1148,S10-1011,0,0.505079,"in that their system is designed for disambiguating words that have only 2 senses. Bayesian WSI systems have been developed by several authors. Brody and Lapata (2009) apply Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to WSI. They run a topic modeling algorithm on texts with some fixed number of topics that correspond to senses and induce a cluster by finding target words assigned to the same topic. Their system is evaluated on SemEval-2007 noun data (Agirre and Soroa, 2007). Lau et al. (2012) apply a nonparametric model, Hierarchical Dirichlet Processes (HDP), to SemEval-2010 data (Manandhar et al., 2010). 3 Model Following Yarowsky (1995), we assume that a word in a document has one sense. Multiple occurrences of a word in a document refer to the same object or concept. The naive Bayes model is well suited for this one-sense-per-document assumption. Each document has one topic corresponding to the sense of the target word that needs disambiguation. Context words in a document are drawn from the conditional distribution of words given the sense. Context words are assumed to be independent from each other given 1433 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pro"
D13-1148,S10-1081,0,0.5197,"Missing"
D13-1148,P11-1148,0,0.0855099,"Missing"
D13-1148,P95-1026,0,0.854859,"orm better than WSI systems, but building labeled data can be prohibitively expensive. In addition, WSD systems are not suitable for newly created words, new senses of existing words, or domainspecific words. On the other hand, WSI systems can learn new senses of words directly from texts because these programs do not rely on a predefined set of senses. In Section 2 we describe relevant previous work. In Section 3 and 4 we introduce the naive Bayes model for WSI and inference schemes for the model. In Section 5 we evaluate the model on SemEval-2010 data. In Section 6 we conclude. Related Work Yarowsky (1995) introduces a semi-supervised bootstrapping algorithm with two assumptions that rivals supervised algorithms: one-sense-percollocation and one-sense-per-discourse. But this algorithm cannot easily be scaled up because for any new ambiguous word humans need to pick a few seed words, which initialize the algorithm. In order to automate the semi-supervised system, Eisner and Karakos (2005) propose an unsupervised bootstrapping algorithm. Their system tries many different seeds for bootstrapping and chooses the “best” classifier at the end. Eisner and Karakos’s algorithm is limited in that their s"
D13-1182,W04-2328,0,0.0320294,"dstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently gained traction (Eisenstein et al., 2011; Paul,"
D13-1182,W04-3240,0,0.0354424,"lly, (Cretchley et al., 2010) leveraged concept maps to explore conversations between people with schizophrenia and their carers. Briefly, this approach allowed them to (qualitatively) identify two distinct conversational strategies used by care-takers and their patients. Angus et al. (Angus et al., 2012) presented a similar approach in which they used text visualization software to explore patterns of (inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2"
D13-1182,D08-1035,0,0.387184,"ffer substantially. The model we develop in this work assumes that transcripts have been manually segmented. While this comes at some cost, segmenting is still much cheaper than annotating transcripts. Manually annotating a single visit with GMIAS codes takes 24 hours and must be performed by someone with substantive domain expertise. By contrast, segmenting transcripts into utterances takes at most 1/4th of the time as annotation and can be done by a less highly-skilled individual. That said, in future work we hope to explore incorporating automatic segmentation methods (Galley et al., 2003; Eisenstein and Barzilay, 2008) into our approach. Each utterance is assigned a single topic code and a single speech act code. Inter-rater agreement has been observed to be relatively high for this task: Kappa between three trained annotators and a reference annotation ranged from 0.89 to 1.0 for topics and 0.81 to 0.95 for speech acts. We next deCount (prevalence) 2939 (0.013) 245 (0.001) 328 (0.001) 4298 (0.018) 1650 (0.007) 111 (0.000) 12796 (0.055) 46 (0.000) 977 (0.004) 15 (0.000) 13753 (0.059) 1049 (0.005) 1005 (0.004) 17611 (0.076) 4617 (0.020) 423 (0.002) 54231 (0.233) 255 (0.001) 4426 (0.019) 119 (0.001) 5517 (0.0"
D13-1182,E12-1079,0,0.018561,"ion software to explore patterns of (inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporat"
D13-1182,P03-1071,0,0.294606,"are not likely to differ substantially. The model we develop in this work assumes that transcripts have been manually segmented. While this comes at some cost, segmenting is still much cheaper than annotating transcripts. Manually annotating a single visit with GMIAS codes takes 24 hours and must be performed by someone with substantive domain expertise. By contrast, segmenting transcripts into utterances takes at most 1/4th of the time as annotation and can be done by a less highly-skilled individual. That said, in future work we hope to explore incorporating automatic segmentation methods (Galley et al., 2003; Eisenstein and Barzilay, 2008) into our approach. Each utterance is assigned a single topic code and a single speech act code. Inter-rater agreement has been observed to be relatively high for this task: Kappa between three trained annotators and a reference annotation ranged from 0.89 to 1.0 for topics and 0.81 to 0.95 for speech acts. We next deCount (prevalence) 2939 (0.013) 245 (0.001) 328 (0.001) 4298 (0.018) 1650 (0.007) 111 (0.000) 12796 (0.055) 46 (0.000) 977 (0.004) 15 (0.000) 13753 (0.059) 1049 (0.005) 1005 (0.004) 17611 (0.076) 4617 (0.020) 423 (0.002) 54231 (0.233) 255 (0.001) 44"
D13-1182,D10-1084,0,0.262144,"onv. Mgmt. Biomedical Ask Q. Biomedical Ask Q. Biomedical Conv. Mgmt. Biomedical Give Info. Table 1: An excerpt from a patient-doctor interaction, annotated with topic and speech act codes. The D and P roles denote doctor and patient, respectively. Conv. Mgmt. abbreviates conversation management; Ask Q. abbreviates ask question. to the same topic but is a question. Both aspects are necessary to understand conversation. Previous computational work on speech acts – which we review in Section 6 – has modeled them in isolation (Perrault and Allen, 1980; Stolcke et al., 1998; Stolcke et al., 2000; Kim et al., 2010), i.e., independent of topical content. But a richer model would account for both speech acts and the contextualizing topic of each utterance. To this end, we develop a novel joint, generative model of topics and speech acts. We focus on physician-patient communication as a motivating domain. This is of interest because 1765 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1765–1775, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics it is widely appreciated that effective communication is an integral part"
D13-1182,Y12-1050,0,0.0144149,"et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently gained traction"
D13-1182,D12-1009,0,0.0977969,"2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently gained traction (Eisenstein et al., 2011; Paul, 2012; Paul and Dredze, 2012; Paul et al., 2013). To our knowledge, this is the first extension of supervised additive component models to a sequential task.6 7 Conclusions and Future Directions We have proposed a novel Joint, Additive, Sequential (JAS) model of conversational topics and speech acts. In contrast to previous approaches to modeling conversational exchanges, this model factors both the current topic and the current speech act into token emission and state transition probabilities. We demonstrated that this model consistently outperforms a univariate generative baseline that treats spe"
D13-1182,D11-1069,0,0.0225729,"lar approach in which they used text visualization software to explore patterns of (inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) an"
D13-1182,J00-3003,0,0.534962,"Missing"
D13-1182,W12-0603,0,0.0118348,"(inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods int"
D13-1182,J80-3003,0,\N,Missing
D15-1160,P05-1022,1,0.7322,"chboard (SB) and Wall Street Journal (WSJ) (Foster and van Genabith, 2008; Francis and Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4 , Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accuracies on the WSJ test set is reported in Table 1. We convert to Stanford 1 For parsers that return log probabilities, we turn these into probabilities first. 2 http://nlp.stanford.edu/˜mcclosky/ biomedical.html 3 http://nlp.stanford.edu/data/ QuestionBank-Stanford.shtml 4 Using the ‘WSJ+Gigaword-v2’ BLLIP model. 1361 Parser Stanford Stanford RNN5 Berkeley Charniak BLLIP Self-trained BLLIP F1 85.4 89.6 90.0 89.7 91.5 92.2 UAS 90.0 92.9 93.5 9"
D15-1160,A00-2018,1,0.572145,"ational Corpus (BNC), Brown, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) (Foster and van Genabith, 2008; Francis and Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4 , Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accuracies on the WSJ test set is reported in Table 1. We convert to Stanford 1 For parsers that return log probabilities, we turn these into probabilities first. 2 http://nlp.stanford.edu/˜mcclosky/ biomedical.html 3 http://nlp.stanford.edu/data/ QuestionBank-Stanford.shtml 4 Using the ‘WSJ+Gigaword-v2’ BLLIP model. 1361 Parser Stanford Stanford RNN5 Berkeley Charniak BLLIP"
D15-1160,N09-2064,0,0.0780314,"e on Empirical Methods in Natural Language Processing, pages 1360–1366, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. constructs a tree with constituents from the majority of the trees, which boosts precision significantly. Henderson and Brill (1999) show that this process is guaranteed to produce a valid tree. Sagae and Lavie (2006) generalize this work by reparsing the chart populated with constituents whose counts are above a certain threshold. By adjusting the threshold on development data, their generalized method balances precision and recall. Fossum and Knight (2009) further extend this line of work by using n-best lists from multiple parsers and combining productions in addition to constituents. Their model assigns sums of joint probabilities of constituents and parsers to constituents. Surprisingly, exploiting n-best trees does not lead to large improvement over combining 1-best trees in their experiments. Our extension takes the n-best trees from a parser as if they are 1-best parses from n parsers, then follows Sagae and Lavie (2006). Parses are weighted by the estimated probabilities from the parser. Given n trees and their weights, the model compute"
D15-1160,J93-2004,0,0.0510678,"ent parsers. On the other hand, having β > 1 skews the distribution toward parses with higher scores and helps under-confident parsers. Note that setting β = 0 weights all parses equally and results in majority voting at the constituent level. We leave developing other nonlinear functions for fusion as future work. 3 Experiments Corpora: Parse fusion is evaluated on British National Corpus (BNC), Brown, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) (Foster and van Genabith, 2008; Francis and Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4 , Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accur"
D15-1160,C10-1045,0,0.0349034,"Missing"
D15-1160,W99-0623,0,0.67839,"ombine trees from m parsers in three steps: populate a chart with constituents along with the number of times they appear in the trees; remove any constituent with count less than m/2 from the chart; and finally create a final tree with all the remaining constituents. Intuitively their method 1360 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1360–1366, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. constructs a tree with constituents from the majority of the trees, which boosts precision significantly. Henderson and Brill (1999) show that this process is guaranteed to produce a valid tree. Sagae and Lavie (2006) generalize this work by reparsing the chart populated with constituents whose counts are above a certain threshold. By adjusting the threshold on development data, their generalized method balances precision and recall. Fossum and Knight (2009) further extend this line of work by using n-best lists from multiple parsers and combining productions in addition to constituents. Their model assigns sums of joint probabilities of constituents and parsers to constituents. Surprisingly, exploiting n-best trees does n"
D15-1160,D10-1002,0,0.182998,"Missing"
D15-1160,P08-1067,0,0.0758877,"Missing"
D15-1160,N10-1095,0,0.0384717,"Missing"
D15-1160,P06-1063,0,0.0767775,"Missing"
D15-1160,N06-1020,1,0.820813,"r and van Genabith, 2008; Francis and Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4 , Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accuracies on the WSJ test set is reported in Table 1. We convert to Stanford 1 For parsers that return log probabilities, we turn these into probabilities first. 2 http://nlp.stanford.edu/˜mcclosky/ biomedical.html 3 http://nlp.stanford.edu/data/ QuestionBank-Stanford.shtml 4 Using the ‘WSJ+Gigaword-v2’ BLLIP model. 1361 Parser Stanford Stanford RNN5 Berkeley Charniak BLLIP Self-trained BLLIP F1 85.4 89.6 90.0 89.7 91.5 92.2 UAS 90.0 92.9 93.5 93.2 94.4 94.7 LAS 87.3 90.4 91.2 90.8 92.0 9"
D15-1160,J11-1007,0,0.0241918,"Missing"
D15-1160,D15-1214,0,0.0213258,"Missing"
D15-1160,U07-1012,0,0.0558067,"Missing"
D15-1160,P06-1055,0,0.126876,"d Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4 , Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accuracies on the WSJ test set is reported in Table 1. We convert to Stanford 1 For parsers that return log probabilities, we turn these into probabilities first. 2 http://nlp.stanford.edu/˜mcclosky/ biomedical.html 3 http://nlp.stanford.edu/data/ QuestionBank-Stanford.shtml 4 Using the ‘WSJ+Gigaword-v2’ BLLIP model. 1361 Parser Stanford Stanford RNN5 Berkeley Charniak BLLIP Self-trained BLLIP F1 85.4 89.6 90.0 89.7 91.5 92.2 UAS 90.0 92.9 93.5 93.2 94.4 94.7 LAS 87.3 90.4 91.2 90.8 92.0 92.2 Parser BLLIP + Fusion + Majori"
D15-1160,N10-1003,0,0.386211,"Missing"
D15-1160,N06-2033,0,0.0791767,"the number of times they appear in the trees; remove any constituent with count less than m/2 from the chart; and finally create a final tree with all the remaining constituents. Intuitively their method 1360 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1360–1366, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. constructs a tree with constituents from the majority of the trees, which boosts precision significantly. Henderson and Brill (1999) show that this process is guaranteed to produce a valid tree. Sagae and Lavie (2006) generalize this work by reparsing the chart populated with constituents whose counts are above a certain threshold. By adjusting the threshold on development data, their generalized method balances precision and recall. Fossum and Knight (2009) further extend this line of work by using n-best lists from multiple parsers and combining productions in addition to constituents. Their model assigns sums of joint probabilities of constituents and parsers to constituents. Surprisingly, exploiting n-best trees does not lead to large improvement over combining 1-best trees in their experiments. Our ex"
D15-1160,W14-6111,0,0.0473306,"Missing"
D15-1160,P12-1046,0,0.315797,"Missing"
D15-1160,A97-1014,0,0.0600109,"Missing"
D15-1160,P13-1045,0,0.0178208,"et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4 , Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accuracies on the WSJ test set is reported in Table 1. We convert to Stanford 1 For parsers that return log probabilities, we turn these into probabilities first. 2 http://nlp.stanford.edu/˜mcclosky/ biomedical.html 3 http://nlp.stanford.edu/data/ QuestionBank-Stanford.shtml 4 Using the ‘WSJ+Gigaword-v2’ BLLIP model. 1361 Parser Stanford Stanford RNN5 Berkeley Charniak BLLIP Self-trained BLLIP F1 85.4 89.6 90.0 89.7 91.5 92.2 UAS 90.0 92.9 93.5 93.2 94.4 94.7 LAS 87.3 90.4 91.2 90.8 92.0 92.2 Parser BLLIP + Fusion + Majority voting (β = 0) + Rank-based weighting"
D15-1160,W05-1518,0,0.0741966,"Missing"
D15-1160,D09-1161,0,0.199464,"Missing"
D15-1160,foster-van-genabith-2008-parser,0,\N,Missing
D15-1160,P03-1054,0,\N,Missing
D16-1257,P16-1231,0,0.125255,"Missing"
D16-1257,A00-2018,1,0.619107,"al. (2010). 2333 Figure 2: Perplexity and F1 on the development set at each epoch during training. n 10 50 51o 100 500 Oracle 94.0 96.7 100 96.3 97.0 Final 91.2 91.7 93.9 91.7 91.8 Exact 39.8 40.0 49.7 39.9 40.0 Table 1: The performance of LSTM-LM (G) with varying n-best parses on the dev set. Oracle refers to Charniak parser’s oracle F1 . Final and Exact report LSTM-LM (G)’s F1 and exact match percentage respectively. To simulate an optimal scenario, we include gold trees to 50-best trees and rerank them with LSTM-LM (G) (51o ). al., 2015) because in preliminary experiments Charniak parser (Charniak, 2000) performed better when trained on all of 24 million trees than when trained on resampled two million trees. Given x, we produce Y 0 (x), 50-best trees, with Charniak parser and find y with LSTM-LM as Dyer et al. (2016) do with their discriminative and generative models.3 4.2 Training and Development 4.2.1 Supervision We unk words that appear fewer than 10 times in the WSJ training (6,922 types) and drop activations with probability 0.7. At the beginning of each epoch, we shuffle the order of trees in the training data. Both perplexity and F1 of LSTM-LM (G) improve and then plateau (Figure 2)."
D16-1257,D15-1160,1,0.895424,"Missing"
D16-1257,N16-1024,0,0.571289,"illustrated in Figure 1, we can define a probability distribution over (x, y) as follows: P (x, y) = P (z) = P (z1 , · · · , zm ) m Y = P (zt |z1 , · · · , zt−1 ), (2) which is equivalent to Equation (1). We have reduced parsing to language modeling and can use language modeling techniques of estimating P (zt |z1 , · · · , zt−1 ) for parsing. Previous Work We look here at three neural net (NN) models closest to our research along various dimensions. The first (Zaremba et al., 2014) gives the basic language modeling architecture that we have adopted, while the other two (Vinyals et al., 2015; Dyer et al., 2016) are parsing models that have the current best results in NN parsing. 2.1 P (y|x) = P (y1 , · · · , yl |x) = t=1 2 conditional probability: l Y t=1 P (yt |x, y1 , · · · , yt−1 ), where the conditioning event (x, y1 , · · · , yt−1 ) is modeled by an LSTM encoder and an LSTM decoder. The encoder maps x into he , a set of vectors that represents x, and the decoder obtains a summary vector (h0t ) which is concatenation of the ded coder’s hidden state weighted sum of word Pn(ht ) and representations ( i=1 αi hei ) with an alignment vector (α). Finally the decoder predicts yt given h0t . Inspired by"
D16-1257,D10-1002,0,0.0770225,"Missing"
D16-1257,P14-1043,0,0.0181537,"use for evaluation, detail training and development processes.1 4.1 Data We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed “silver” trees (McClosky et al., 2006; Huang et al., 2010; Vinyals et al., 2015) for tritraining. To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011) with a product of eight Berkeley parsers (Petrov, 2010)2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014). We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained models used for experiments are available at github.com/cdg720/emnlp2016. 2 We use the reimplementation by Huang et al. (2010). 2333 Figure 2: Perplexity and F1 on the development set at each epoch during training. n 10 50 51o 100 500 Oracle 94.0 96.7 100 96.3 97.0 Final 91.2 91.7 93.9 91.7 91.8 Exact 39.8 40.0 49.7 39.9 40.0 Table 1: The performance of LSTM-LM (G) with varying n-best parses on the dev set. Oracle refers to Charniak parser’s oracle F1 . Final an"
D16-1257,J93-2004,0,0.0660716,"U(−0.05, 0.05). Dropout is applied to non-recurrent connections (Pham et al., 2014) and gradients are clipped when their norm is bigger than 20 (Pascanu et al., 2013). The learning rate is 0.25 · 0.85max(−15, 0) where  is an epoch number. For simplicity, we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax (Morin and Bengio, 2005) or noise contrastive estimation (Gutmann and Hyv¨arinen, 2012). 4 Experiments We describe datasets we use for evaluation, detail training and development processes.1 4.1 Data We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed “silver” trees (McClosky et al., 2006; Huang et al., 2010; Vinyals et al., 2015) for tritraining. To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011) with a product of eight Berkeley parsers (Petrov, 2010)2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014). We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained mo"
D16-1257,N06-1020,1,0.682875,"norm is bigger than 20 (Pascanu et al., 2013). The learning rate is 0.25 · 0.85max(−15, 0) where  is an epoch number. For simplicity, we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax (Morin and Bengio, 2005) or noise contrastive estimation (Gutmann and Hyv¨arinen, 2012). 4 Experiments We describe datasets we use for evaluation, detail training and development processes.1 4.1 Data We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed “silver” trees (McClosky et al., 2006; Huang et al., 2010; Vinyals et al., 2015) for tritraining. To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011) with a product of eight Berkeley parsers (Petrov, 2010)2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014). We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained models used for experiments are available at github.com/cdg720/emnlp2016. 2 We use the reimplementation by Huang et al. (20"
D16-1257,N10-1003,0,0.00919653,"noise contrastive estimation (Gutmann and Hyv¨arinen, 2012). 4 Experiments We describe datasets we use for evaluation, detail training and development processes.1 4.1 Data We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed “silver” trees (McClosky et al., 2006; Huang et al., 2010; Vinyals et al., 2015) for tritraining. To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011) with a product of eight Berkeley parsers (Petrov, 2010)2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014). We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained models used for experiments are available at github.com/cdg720/emnlp2016. 2 We use the reimplementation by Huang et al. (2010). 2333 Figure 2: Perplexity and F1 on the development set at each epoch during training. n 10 50 51o 100 500 Oracle 94.0 96.7 100 96.3 97.0 Final 91.2 91.7 93.9 91.7 91.8 Exact 39.8 40.0 49.7 39.9 40.0 Table 1: The performance of LSTM-LM"
D16-1257,P12-1046,0,0.240904,"Missing"
D16-1257,P13-1043,0,0.0673434,"tion (Gutmann and Hyv¨arinen, 2012). 4 Experiments We describe datasets we use for evaluation, detail training and development processes.1 4.1 Data We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed “silver” trees (McClosky et al., 2006; Huang et al., 2010; Vinyals et al., 2015) for tritraining. To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011) with a product of eight Berkeley parsers (Petrov, 2010)2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014). We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained models used for experiments are available at github.com/cdg720/emnlp2016. 2 We use the reimplementation by Huang et al. (2010). 2333 Figure 2: Perplexity and F1 on the development set at each epoch during training. n 10 50 51o 100 500 Oracle 94.0 96.7 100 96.3 97.0 Final 91.2 91.7 93.9 91.7 91.8 Exact 39.8 40.0 49.7 39.9 40.0 Table 1: The performance of LSTM-LM (G) with varying n-best pars"
E09-1018,W99-0613,0,0.0972512,"Missing"
E09-1018,N04-4009,0,0.107532,"ion.) The test set has 1119 personal pronouns of which 246 are non-anaphoric. Our selection of this dataset, rather than the widely used MUC-6 corpus, is motivated by this large number of pronouns. We compared our results to four currentlyavailable anaphora programs from the web. These four were selected by sending a request to a commonly used mailing list (the “corpora-list”) asking for such programs. We received four leads: JavaRAP, Open-NLP, BART and GuiTAR. Of course, these systems represent the best available work, not the state of the art. We presume that more recent supervised systems (Kehler et al., 2004b; Yang et al., 2004; Yang et al., 2006) perDefinition of Correctness We evaluate all programs according to Mitkov’s “resolution etiquette” scoring metric (also used in Cherry and Bergsma (2005)), which is defined as follows: if N is the number of non-anaphoric pronouns correctly identified, A the number of anaphoric pronouns correctly linked to their antecedent, and P the total number of pronouns, then a pronoun-anaphora program’s percentage correct is NP+A . Most papers dealing with pronoun coreference use this simple ratio, or the variant that ignores non-anaphoric pronouns. It has appeared"
E09-1018,N04-1037,0,0.0285598,"ion.) The test set has 1119 personal pronouns of which 246 are non-anaphoric. Our selection of this dataset, rather than the widely used MUC-6 corpus, is motivated by this large number of pronouns. We compared our results to four currentlyavailable anaphora programs from the web. These four were selected by sending a request to a commonly used mailing list (the “corpora-list”) asking for such programs. We received four leads: JavaRAP, Open-NLP, BART and GuiTAR. Of course, these systems represent the best available work, not the state of the art. We presume that more recent supervised systems (Kehler et al., 2004b; Yang et al., 2004; Yang et al., 2006) perDefinition of Correctness We evaluate all programs according to Mitkov’s “resolution etiquette” scoring metric (also used in Cherry and Bergsma (2005)), which is defined as follows: if N is the number of non-anaphoric pronouns correctly identified, A the number of anaphoric pronouns correctly linked to their antecedent, and P the total number of pronouns, then a pronoun-anaphora program’s percentage correct is NP+A . Most papers dealing with pronoun coreference use this simple ratio, or the variant that ignores non-anaphoric pronouns. It has appeared"
E09-1018,J94-4002,0,0.0490232,"led error chaining (Walker, 1989)), both x and y are to be scored as wrong, as they both end up in the wrong coreferential chain. We believe this is, in fact, the standard (Mitkov, personal communication), although 3 Of course our system does not attempt NP coreference resolution, nor does JavaRAP. The other three comparison systems do. 153 System GuiTAR JavaRap Open-NLP Our System form better. Unfortunately, we were unable to obtain a comparison unsupervised learning system at all. Only one of the four is explicitly aimed at personal-pronoun anaphora — RAP (Resolution of Anaphora Procedure) (Lappin and Leass, 1994). It is a non-statistical system originally implemented in Prolog. The version we used is JavaRAP, a later reimplementation in Java (Long Qiu and Chua, 2004). It only handles third person pronouns. The other three are more general in that they handle all NP anaphora. The GuiTAR system (Poesio and Kabadjov, 2004) is designed to work in an “off the shelf” fashion on general text GUITAR resolves pronouns using the algorithm of (Mitkov et al., 2002), which filters candidate antecedents and then ranks them using morphosyntactic features. Due to a bug in version 3, GUITAR does not currently handle p"
E09-1018,J93-2003,0,0.0192564,"Missing"
E09-1018,qiu-etal-2004-public,0,0.088315,"e standard (Mitkov, personal communication), although 3 Of course our system does not attempt NP coreference resolution, nor does JavaRAP. The other three comparison systems do. 153 System GuiTAR JavaRap Open-NLP Our System form better. Unfortunately, we were unable to obtain a comparison unsupervised learning system at all. Only one of the four is explicitly aimed at personal-pronoun anaphora — RAP (Resolution of Anaphora Procedure) (Lappin and Leass, 1994). It is a non-statistical system originally implemented in Prolog. The version we used is JavaRAP, a later reimplementation in Java (Long Qiu and Chua, 2004). It only handles third person pronouns. The other three are more general in that they handle all NP anaphora. The GuiTAR system (Poesio and Kabadjov, 2004) is designed to work in an “off the shelf” fashion on general text GUITAR resolves pronouns using the algorithm of (Mitkov et al., 2002), which filters candidate antecedents and then ranks them using morphosyntactic features. Due to a bug in version 3, GUITAR does not currently handle possessive pronouns.GUITAR also has an optional discoursenew classification step, which cannot be used as it requires a discontinued Google search API. OpenNL"
E09-1018,P02-1011,0,0.0180899,"pon a distribution p(anaphora|context). The nature of the “context” is discussed below. Then given the antecedent we generative the pronoun’s person according to p(person|antecedent), the pronoun’s gender according to p(gender|antecedent), number, p(number|antecedent) and governor/relationto-governor from p(governor/relation|antecedent). To generate a non-anaphoric third person singular “it” we first guess that the non-anaphoric pronouns is “it” according to p(“it”|non-anaphoric). 2 Actually, as in most previous work, we only consider referents realized by NPs. For more general approaches see Byron (2002). 150 Word paul paula pig piggy wal-mart waist probability mass, so each one will only get an increase of 0.05. Thus on the first iteration only the first two sentences have the power to move the distributions, but they do, and they make NPs in the current sentence very slightly more likely to generate the pronoun than the sentence one back, which in turn is more likely than the ones two back. This slight imbalance is reflected when EM readjusts the probability distribution at the end of the first iteration. Thus for the second iteration everyone contributes to subsequent imbalances, because i"
E09-1018,W99-0611,0,0.380923,"es. First we only concern ourselves with “personal” pronouns: “I”, “you”, “he”, “she”, “it”, and their variants. We ignore, e.g., relative pronouns (“who”, “which”, etc.), deictic pronouns (“this”, “that”) and others. Personal pronouns come in four basic types: subject “I”, “she”, etc. Used in subject position. object “me”, “her” etc. Used in non-subject position. possessive “my” “her”, and reflexive “myself”, “herself” etc. Required by English grammar in certain constructions — e.g., “I kicked myself.” There are also several papers which treat coference as an unsupervised clustering problem (Cardie and Wagstaff, 1999; Angheluta et al., 2004). In this literature there is no generative model at all, and thus this work is only loosely connected to the above models. The system described here handles all of these cases. Note that the type of a pronoun is not connected with its antecedent, but rather is completely determined by the role it plays in it’s sentence. Personal pronouns are either anaphoric or nonanaphoric. We say that a pronoun is anaphoric when it is coreferent with another piece of text in the same discourse. As is standard in the field we distinguish between a referent and an antecedent. The refe"
E09-1018,P05-1022,1,0.0355759,"ing a non-anaphoric “it”. Lastly we have a probability of generating each of the other nonmonotonic pronouns along with (the nth root of) their governor. These parameters are 6, 0.1, and 0.0004 respectively. 6 7 Evaluation To develop and test our program we use the dataset annotated by Niyu Ge (Ge et al., 1998). This consists of sections 0 and 1 of the Penn treebank. Ge marked every personal pronoun and all noun phrases that were coreferent with these pronouns. We used section 0 as our development set, and section 1 for testing. We reparsed the sentences using the Charniak and Johnson parser (Charniak and Johnson, 2005) rather than using the gold-parses that Ge marked up. We hope thereby to make the results closer to those a user will experience. (Generally the gold trees perform about 0.005 higher than the machine parsed version.) The test set has 1119 personal pronouns of which 246 are non-anaphoric. Our selection of this dataset, rather than the widely used MUC-6 corpus, is motivated by this large number of pronouns. We compared our results to four currentlyavailable anaphora programs from the web. These four were selected by sending a request to a commonly used mailing list (the “corpora-list”) asking fo"
E09-1018,W05-0612,0,0.544311,"to find good models for the tasks to which it is set, in this case it works quite well. We have compared it to several systems available on the web (all we have found so far). Our program significantly outperforms all of them. The algorithm is fast and robust, and has been made publically available for downloading. 1 2 Previous Work The literature on pronominal anaphora is quite large, and we cannot hope to do justice to it here. Rather we limit ourselves to particular papers and systems that have had the greatest impact on, and similarity to, ours. Probably the closest approach to our own is Cherry and Bergsma (2005), which also presents an EM approach to pronoun resolution, and obtains quite successful results. Our work improves upon theirs in several dimensions. Firstly, they do not distinguish antecedents of non-reflexive pronouns based on syntax (for instance, subjects and objects). Both previous work (cf. Tetreault (2001) discussed below) and our present results find these distinctions extremely helpful. Secondly, their system relies on a separate preprocessing stage to classify non-anaphoric pronouns, and mark the gender of certain NPs (Mr., Mrs. and some first names). This allows the incorporation"
E09-1018,poesio-kabadjov-2004-general,0,0.427236,"Missing"
E09-1018,J98-2001,0,0.0144778,"Missing"
E09-1018,J01-4004,0,0.102185,"edents and then ranks them using morphosyntactic features. Due to a bug in version 3, GUITAR does not currently handle possessive pronouns.GUITAR also has an optional discoursenew classification step, which cannot be used as it requires a discontinued Google search API. OpenNLP (Morton et al., 2005) uses a maximum-entropy classifier to rank potential antecedents for pronouns. However despite being the best-performing (on pronouns) of the existing systems, there is a remarkable lack of published information on its innards. BART (Versley et al., 2008) also uses a maximum-entropy model, based on Soon et al. (2001). The BART system also provides a more sophisticated feature set than is available in the basic model, including tree-kernel features and a variety of web-based knowledge sources. Unfortunately we were not able to get the basic version working. More precisely we were able to run the program, but the results we got were substantially lower than any of the other models and we believe that the program as shipped is not working properly. Some of these systems provide their own preprocessing tools. However, these were bypassed, so that all systems ran on the Charniak parse trees (with gold sentence"
E09-1018,J01-4003,0,0.264839,"es it is measure forward from the start of the sentence. • syntactic positions — generally we expect NPs in subject position to be more likely antecedents than those in object position, and those more likely than other positions (e.g., object of a preposition). • position of the pronoun — for example the subject of the previous sentence is very likely to be the antecedent if the pronoun is very early in the sentence, much less likely if it is at the end. • type of pronoun — reflexives can only be bound within the same sentence, while sub152 5.3 Parameters Not Set by EM there are a few papers (Tetreault, 2001; Yang et al., 2006) which do the opposite and many which simply do not discuss this case. One more issue arises in the case of a system attempting to perform complete NP anaphora3 . In these cases the coreferential chains they create may not correspond to any of the original chains. In these cases, we call a pronoun correctly resolved if it is put in a chain including at least one correct non-pronominal antecedent. This definition cannot be used in general, as putting all NPs into the same set would give a perfect score. Fortunately, the systems we compare against do not do this – they seem m"
E09-1018,P08-4003,0,0.0144991,"algorithm of (Mitkov et al., 2002), which filters candidate antecedents and then ranks them using morphosyntactic features. Due to a bug in version 3, GUITAR does not currently handle possessive pronouns.GUITAR also has an optional discoursenew classification step, which cannot be used as it requires a discontinued Google search API. OpenNLP (Morton et al., 2005) uses a maximum-entropy classifier to rank potential antecedents for pronouns. However despite being the best-performing (on pronouns) of the existing systems, there is a remarkable lack of published information on its innards. BART (Versley et al., 2008) also uses a maximum-entropy model, based on Soon et al. (2001). The BART system also provides a more sophisticated feature set than is available in the basic model, including tree-kernel features and a variety of web-based knowledge sources. Unfortunately we were not able to get the basic version working. More precisely we were able to run the program, but the results we got were substantially lower than any of the other models and we believe that the program as shipped is not working properly. Some of these systems provide their own preprocessing tools. However, these were bypassed, so that"
E09-1018,P89-1031,0,0.305948,"c pronouns. It has appeared under a number of names: success (Yang et al., 2006), accuracy (Kehler et al., 2004a; Angheluta et al., 2004) and success rate (Tetreault, 2001). The other occasionally-used metric is the MUC score restricted to pronouns, but this has well-known problems (Bagga and Baldwin, 1998). To make the definition perfectly concrete, however, we must resolve a few special cases. One is the case in which a pronoun x correctly says that it is coreferent with another pronoun y. However, the program misidentifies the antecedent of y. In this case (sometimes called error chaining (Walker, 1989)), both x and y are to be scored as wrong, as they both end up in the wrong coreferential chain. We believe this is, in fact, the standard (Mitkov, personal communication), although 3 Of course our system does not attempt NP coreference resolution, nor does JavaRAP. The other three comparison systems do. 153 System GuiTAR JavaRap Open-NLP Our System form better. Unfortunately, we were unable to obtain a comparison unsupervised learning system at all. Only one of the four is explicitly aimed at personal-pronoun anaphora — RAP (Resolution of Anaphora Procedure) (Lappin and Leass, 1994). It is a"
E09-1018,P04-1017,0,0.0266244,"1119 personal pronouns of which 246 are non-anaphoric. Our selection of this dataset, rather than the widely used MUC-6 corpus, is motivated by this large number of pronouns. We compared our results to four currentlyavailable anaphora programs from the web. These four were selected by sending a request to a commonly used mailing list (the “corpora-list”) asking for such programs. We received four leads: JavaRAP, Open-NLP, BART and GuiTAR. Of course, these systems represent the best available work, not the state of the art. We presume that more recent supervised systems (Kehler et al., 2004b; Yang et al., 2004; Yang et al., 2006) perDefinition of Correctness We evaluate all programs according to Mitkov’s “resolution etiquette” scoring metric (also used in Cherry and Bergsma (2005)), which is defined as follows: if N is the number of non-anaphoric pronouns correctly identified, A the number of anaphoric pronouns correctly linked to their antecedent, and P the total number of pronouns, then a pronoun-anaphora program’s percentage correct is NP+A . Most papers dealing with pronoun coreference use this simple ratio, or the variant that ignores non-anaphoric pronouns. It has appeared under a number of n"
E09-1018,P06-1006,0,0.0654905,"forward from the start of the sentence. • syntactic positions — generally we expect NPs in subject position to be more likely antecedents than those in object position, and those more likely than other positions (e.g., object of a preposition). • position of the pronoun — for example the subject of the previous sentence is very likely to be the antecedent if the pronoun is very early in the sentence, much less likely if it is at the end. • type of pronoun — reflexives can only be bound within the same sentence, while sub152 5.3 Parameters Not Set by EM there are a few papers (Tetreault, 2001; Yang et al., 2006) which do the opposite and many which simply do not discuss this case. One more issue arises in the case of a system attempting to perform complete NP anaphora3 . In these cases the coreferential chains they create may not correspond to any of the original chains. In these cases, we call a pronoun correctly resolved if it is put in a chain including at least one correct non-pronominal antecedent. This definition cannot be used in general, as putting all NPs into the same set would give a perfect score. Fortunately, the systems we compare against do not do this – they seem more likely to over-s"
E09-1018,W98-1119,1,\N,Missing
E09-1018,P07-1107,0,\N,Missing
E14-4033,W13-1706,0,0.374006,"We define a computational methodology that produces a concise list of lexicalized syntactic patterns that are controlled for redundancy and ranked by relevancy to language transfer. We demonstrate the ability of our methodology to detect hundreds of such candidate patterns from currently available data sources, and validate the quality of the proposed patterns through classification experiments. 1 Introduction The fact that students with different native language backgrounds express themselves differently in second language writing samples has been established experimentally many times over (Tetreault et al., 2013), and is intuitive to most people with experience learning a new language. The exposure and understanding of this process could potentially enable the creation of second language (L2) instruction that is tailored to the native language (L1) of students. The detectable connection between L1 and L2 text comes from a range of sources. On one end of the spectrum are factors such as geographic or cultural preference in word choice, which are a powerful L1 indicator. On the other end lie linguistic phenomena such as language transfer, in which the preferential over-use or under-use of structures in"
E14-4033,brooke-hirst-2012-measuring,0,0.0643834,"primarily on nouns while we focus on stopwords. We also transform the UTB into constituent format, in a manner inspired by Carroll and Charniak (1992). There is a large amount of related research in Native Language Identification (NLI), the task of predicting L1 given L2 text. This work has culminated in a well attended shared task (Tetreault et al., 2013), whose cited report contains an excellent survey of the history of this task. In NLI, however, L1 data is not traditionally used, and patterns are learned directly from L2 text that has been annotated with L1 labels. One notable outlier is Brooke and Hirst (2012), which attempts NLI using only L1 data for training using large online dictionaries to tie L2 English bigrams and collocations to possible direct translations from native languages. Jarvis and Crossley (2012) presents another set of studies that use NLI as a method to form language transfer hypotheses. 3 root det ROOT DT The nsubj NN poodle dobj VBZ chews PRP it Under our transformation, the above dependency parse becomes ROOT root VBZ-L VBZ VBZ-R nsubj chews dobj NN-L NN PRP det poodle it DT the We also require a multilingual lexicon in the form of a function ML (w) for each language L that"
E14-4033,P10-2042,0,0.012451,"features whose usage frequency can be determined for each L1 background in both L1 and L2 text (e.g. both German and English written by Germans). We claim that a feature exhibiting a sufficiently non-uniform usage histogram in L1 that is mirrored in L2 data is a strong language transfer candidate, and provide a quantified measure of this property. We represent both L1 and L2 sentences in a universal constituent-style syntactic format and model language transfer hypotheses with contiguous syntax sub-structures commonly known as Tree Substitution Grammar (TSG) fragments (Post and Gildea, 2009)(Cohn and Blunsom, 2010). With these features we produce a concise ranked list of candidate language transfer hypotheses and their usage statistics that can be automatically augmented as increasing amounts of data become available. Language transfer, the preferential second language behavior caused by similarities to the speaker’s native language, requires considerable expertise to be detected by humans alone. Our goal in this work is to replace expert intervention by data-driven methods wherever possible. We define a computational methodology that produces a concise list of lexicalized syntactic patterns that are co"
E14-4033,P08-1088,0,0.00463524,"ing dependency tree to illustrate our transformation. This work leverages several recently released data sets and analysis techniques, with the primary contribution being the transformations necessary to combine these disparate efforts. Our analysis methods are closely tied to those described in Swanson and Charniak (2013), which contrasts techniques for the discovery of discriminative TSG fragments in L2 text. We modify and extend these methods to apply to the universal dependency treebanks of McDonald et al. (2013), which we will refer to below to as the UTB. Bilingual lexicon construction (Haghighi et al., 2008) is also a key component, although previous work has focused primarily on nouns while we focus on stopwords. We also transform the UTB into constituent format, in a manner inspired by Carroll and Charniak (1992). There is a large amount of related research in Native Language Identification (NLI), the task of predicting L1 given L2 text. This work has culminated in a well attended shared task (Tetreault et al., 2013), whose cited report contains an excellent survey of the history of this task. In NLI, however, L1 data is not traditionally used, and patterns are learned directly from L2 text tha"
E14-4033,2005.mtsummit-papers.11,0,0.00396548,"Missing"
E14-4033,J03-1002,0,0.0104346,"Missing"
E14-4033,P09-2012,0,0.0348171,"f a class of linguistic features whose usage frequency can be determined for each L1 background in both L1 and L2 text (e.g. both German and English written by Germans). We claim that a feature exhibiting a sufficiently non-uniform usage histogram in L1 that is mirrored in L2 data is a strong language transfer candidate, and provide a quantified measure of this property. We represent both L1 and L2 sentences in a universal constituent-style syntactic format and model language transfer hypotheses with contiguous syntax sub-structures commonly known as Tree Substitution Grammar (TSG) fragments (Post and Gildea, 2009)(Cohn and Blunsom, 2010). With these features we produce a concise ranked list of candidate language transfer hypotheses and their usage statistics that can be automatically augmented as increasing amounts of data become available. Language transfer, the preferential second language behavior caused by similarities to the speaker’s native language, requires considerable expertise to be detected by humans alone. Our goal in this work is to replace expert intervention by data-driven methods wherever possible. We define a computational methodology that produces a concise list of lexicalized syntac"
E14-4033,N13-1009,1,0.822522,"the way to our goal, defining a dependency grammar with a universal set of part of speech (POS) tags and dependency arc labels. Two barriers remain to the use of standard TSG induction algorithms. The first is to define a mapping from the dependency tree format to constituency format. We use the following dependency tree to illustrate our transformation. This work leverages several recently released data sets and analysis techniques, with the primary contribution being the transformations necessary to combine these disparate efforts. Our analysis methods are closely tied to those described in Swanson and Charniak (2013), which contrasts techniques for the discovery of discriminative TSG fragments in L2 text. We modify and extend these methods to apply to the universal dependency treebanks of McDonald et al. (2013), which we will refer to below to as the UTB. Bilingual lexicon construction (Haghighi et al., 2008) is also a key component, although previous work has focused primarily on nouns while we focus on stopwords. We also transform the UTB into constituent format, in a manner inspired by Carroll and Charniak (1992). There is a large amount of related research in Native Language Identification (NLI), the"
E14-4033,P13-2017,0,\N,Missing
H05-1030,W02-1007,1,0.908554,"Missing"
H05-1030,graff-bird-2000-many,0,0.311885,"et of opportunities and challenges. While new obstacles arise from the presence of speech repairs, the possibility of word errors, and the absence of punctuation and sentence boundaries, speech also presents a tremendous opportunity to leverage multi-modal input, in the form of acoustic or even visual cues. As a step in this direction, this paper identifies a set of useful prosodic features and describes how they can be effectively incorporated into a statistical parsing model, ignoring for now the problem of word errors. Evaluated on the Switchboard corpus of conversational telephone speech (Graff and Bird, 2000), our prosody-aware parser out-performs a state-of-the-art system that uses lexical and syntactic features only. While we are not the first to employ prosodic cues in a statistical parsing model, previous efforts (Gregory et al., 2004; Kahn et al., 2004) incorporated these features as word tokens and thereby suffered from the side-effect of displacing words in the n-gram models by the parser. To avoid this problem, we generate a set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses. Our system architecture combines"
H05-1030,N04-1011,1,0.863186,"leverage multi-modal input, in the form of acoustic or even visual cues. As a step in this direction, this paper identifies a set of useful prosodic features and describes how they can be effectively incorporated into a statistical parsing model, ignoring for now the problem of word errors. Evaluated on the Switchboard corpus of conversational telephone speech (Graff and Bird, 2000), our prosody-aware parser out-performs a state-of-the-art system that uses lexical and syntactic features only. While we are not the first to employ prosodic cues in a statistical parsing model, previous efforts (Gregory et al., 2004; Kahn et al., 2004) incorporated these features as word tokens and thereby suffered from the side-effect of displacing words in the n-gram models by the parser. To avoid this problem, we generate a set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses. Our system architecture combines earlier models proposed for parse reranking (Collins, 2000) and filtering out edit regions (Charniak and Johnson, 2001). Detecting and removing edits prior to parsing is motivated by the claim that probabilistic contextfree grammars ("
H05-1030,P83-1019,0,0.0846047,"Missing"
H05-1030,N04-4032,1,0.919297,"input, in the form of acoustic or even visual cues. As a step in this direction, this paper identifies a set of useful prosodic features and describes how they can be effectively incorporated into a statistical parsing model, ignoring for now the problem of word errors. Evaluated on the Switchboard corpus of conversational telephone speech (Graff and Bird, 2000), our prosody-aware parser out-performs a state-of-the-art system that uses lexical and syntactic features only. While we are not the first to employ prosodic cues in a statistical parsing model, previous efforts (Gregory et al., 2004; Kahn et al., 2004) incorporated these features as word tokens and thereby suffered from the side-effect of displacing words in the n-gram models by the parser. To avoid this problem, we generate a set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses. Our system architecture combines earlier models proposed for parse reranking (Collins, 2000) and filtering out edit regions (Charniak and Johnson, 2001). Detecting and removing edits prior to parsing is motivated by the claim that probabilistic contextfree grammars (PCFGs) perform poorl"
H05-1030,P90-1003,0,0.0944145,"is also perceptual evidence that prosody provides cues to human listeners that aid in syntactic disambiguation (Price et al., 1991), and the most important of these cues seems to be the prosodic phrases (perceived groupings of words) or the boundary events marking them. However, the utility of sentence-internal prosody in parsing conversational speech is not well established. Most early work on integrating prosody in parsing was in the context of human-computer dialog systems, where parsers typically operated on isolated utterances. The primary use of prosody was to rule out candidate parses (Bear and Price, 1990; Batliner et al., 1996). Since then, parsing has advanced considerably, and the use of statistical parsers makes the candidate pruning benefits of prosody less important. This raises the question of whether prosody is useful for improving parsing accuracy for conversational speech, apart from its use in sentence a new part-of-speech tag EW. Consecutive sequences of edit words are inserted as single, flat EDITED constituents. 4. Features (syntactic and/or prosodic) are extracted for each candidate, i.e. candidates are converted to feature vector representation. 5. The candidates are rescored b"
H05-1030,N01-1016,1,0.85709,"actic features only. While we are not the first to employ prosodic cues in a statistical parsing model, previous efforts (Gregory et al., 2004; Kahn et al., 2004) incorporated these features as word tokens and thereby suffered from the side-effect of displacing words in the n-gram models by the parser. To avoid this problem, we generate a set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses. Our system architecture combines earlier models proposed for parse reranking (Collins, 2000) and filtering out edit regions (Charniak and Johnson, 2001). Detecting and removing edits prior to parsing is motivated by the claim that probabilistic contextfree grammars (PCFGs) perform poorly at detecting edit regions. We validate this claim empirically: two state-of-the-art PCFGs (Bikel, 2004; Charniak and Johnson, 2005) are both shown to perform significantly below a state-of-the-art edit detection system (Johnson et al., 2004). 2 Previous Work As mentioned earlier, conversational speech presents a different set of challenges and opportunities than encountered in parsing text. This paper focuses on the challenges associated with disfluencies (Se"
H05-1030,P05-1022,1,0.900216,"porated into a statistical parsing model. On the Switchboard corpus of conversational speech, the system achieves improved parse accuracy over a state-of-the-art system which uses only lexical and syntactic features. Since removal of edit regions is known to improve downstream parse accuracy, we explore alternatives for edit detection and show that PCFGs are not competitive with more specialized techniques. 1 Introduction For more than a decade, the Penn Treebank’s Wall Street Journal corpus has served as a benchmark for developing and evaluating statistical parsing techniques (Collins, 2000; Charniak and Johnson, 2005). While this common benchmark has served as a valuable shared task for focusing community effort, it has unfortunately led to the relative neglect of other genres, particularly speech. Parsed speech stands to benefit from practically every application envisioned for parsed text, including machine translation, information extraction, and language modeling. In contrast to text, however, speech (in particular, conversational speech) presents a distinct set of opportunities and challenges. While new obstacles arise from the presence of speech repairs, the possibility of word errors, and the absenc"
H05-1030,P99-1053,0,0.322839,"Missing"
H05-1030,H91-1073,1,0.912342,"Missing"
H05-1030,J99-4003,0,\N,Missing
I05-1006,A00-2018,1,0.622668,"pear to have been limited by the parsing technologies employed. Reported diﬃculties include poor coverage, inability to resolve syntactic ambiguity, unacceptable memory and speed, and diﬃculty in hand-crafting rules of grammar [12,13]. Perhaps the most telling indicator of community perspective came in a recent survey’s bleak observation that eﬃcient and accurate parsing of unrestricted text appears to be out of reach of current techniques [14]. In this paper, we show that broad, accurate parsing of biomedical literature is indeed possible. Using an oﬀ-the-shelf WSJ-trained statistical parser [3] as our baseline, we provide the ﬁrst full-coverage parse accuracy results for biomedical literature, as measured on the GENIA corpus of MEDLINE abstracts [1,2]. Furthermore, after showing that PTB is lexically impoverished when measured on various genres of scientiﬁc and technical writing, we describe three methods for improving parse accuracy by leveraging lexical resources from the domain: part-of-speech (POS) tags, dictionary collocations, and named-entities. Our general hope is that lexically-based techniques such as these can provide alternative and complementary value to treebank-based"
I05-1006,J93-2004,0,0.0335982,"Missing"
I05-1006,W01-0521,0,0.0708368,"owever, while PTB’s Wall Street Journal (WSJ) corpus has historically served as the canonical benchmark for evaluating statistical parsing, the need for broader evaluation has been increasingly recognized in recent years. Furthermore, since it is impractical to create a large treebank like PTB for every genre of interest, signiﬁcant attention has been directed towards maximally reusing existing training data in order to mitigate the need for domain-speciﬁc training examples. These issues have been most notably explored in parser adaptation studies conducted between PTB’s WSJ and Brown corpora [6,7,8,9]. As part of our own exploration of these issues, we have been investigating statistical parser adaptation to a novel domain: biomedical literature. This literature presents a stark contrast to WSJ and Brown: it is suﬀused with domainspeciﬁc vocabulary, has markedly diﬀerent stylistic constraints, and is often written by non-native speakers. Moreover, broader consideration of technical literature shows this challenge and opportunity is not conﬁned to biomedical literature  We would like to thank the National Science Foundation for their support of this work (IIS-0112432, LIS-9721276, and DMS-"
I05-1006,N03-1027,0,0.0421222,"owever, while PTB’s Wall Street Journal (WSJ) corpus has historically served as the canonical benchmark for evaluating statistical parsing, the need for broader evaluation has been increasingly recognized in recent years. Furthermore, since it is impractical to create a large treebank like PTB for every genre of interest, signiﬁcant attention has been directed towards maximally reusing existing training data in order to mitigate the need for domain-speciﬁc training examples. These issues have been most notably explored in parser adaptation studies conducted between PTB’s WSJ and Brown corpora [6,7,8,9]. As part of our own exploration of these issues, we have been investigating statistical parser adaptation to a novel domain: biomedical literature. This literature presents a stark contrast to WSJ and Brown: it is suﬀused with domainspeciﬁc vocabulary, has markedly diﬀerent stylistic constraints, and is often written by non-native speakers. Moreover, broader consideration of technical literature shows this challenge and opportunity is not conﬁned to biomedical literature  We would like to thank the National Science Foundation for their support of this work (IIS-0112432, LIS-9721276, and DMS-"
I05-1006,N03-1031,0,0.0137318,"owever, while PTB’s Wall Street Journal (WSJ) corpus has historically served as the canonical benchmark for evaluating statistical parsing, the need for broader evaluation has been increasingly recognized in recent years. Furthermore, since it is impractical to create a large treebank like PTB for every genre of interest, signiﬁcant attention has been directed towards maximally reusing existing training data in order to mitigate the need for domain-speciﬁc training examples. These issues have been most notably explored in parser adaptation studies conducted between PTB’s WSJ and Brown corpora [6,7,8,9]. As part of our own exploration of these issues, we have been investigating statistical parser adaptation to a novel domain: biomedical literature. This literature presents a stark contrast to WSJ and Brown: it is suﬀused with domainspeciﬁc vocabulary, has markedly diﬀerent stylistic constraints, and is often written by non-native speakers. Moreover, broader consideration of technical literature shows this challenge and opportunity is not conﬁned to biomedical literature  We would like to thank the National Science Foundation for their support of this work (IIS-0112432, LIS-9721276, and DMS-"
I05-1006,P03-1002,0,0.0142271,"Missing"
I05-1006,W04-1219,0,0.0512483,"Missing"
I05-1006,J05-1003,0,\N,Missing
I11-1034,P10-3015,0,0.713071,"that the task of Sam . dhiSplitting is ambiguous. For instance, any of {(ca,api), (c¯a,api), (ca,¯api), (c¯a,¯api)} could have combined to form c¯api. Contextual knowledge is necessary for perfect analysis of Sanskrit text (Hellwig, 2009), and current methods have not evolved enough to be able to supply context. However the field of Statistical Machine Learning, by making reasonable assumptions, allows us to provide approximations of these processes. Previous work (Beesley, 1998; Hyman, 2007) indicates that Finite State Transducers (FST) could be used to generate morphologically valid splits. Mittal (2010) considers the FST approach as well as an approach based on Optimality Theory (Prince and Smolensky, 1993), by defining a posterior probability function to choose among all morphologically valid splits of a chunk (OT1). More recently, Kumar et al. (2010) report findings using a different posterior probability function with the same Optimality Theory approach (OT2). Firstly, we derive our own posterior probability function, which is different from OT1 and OT2. Introduction Sanskrit, considered to be among the oldest languages in the world, is elaborate in its oral specifications. It possesses a"
I11-1034,W98-1007,0,0.0622899,"d to as Analysis or Sam . dhi-Splitting , and the text formed as a result will be referred to as Analysed text. It is easy to see that the task of Sam . dhiSplitting is ambiguous. For instance, any of {(ca,api), (c¯a,api), (ca,¯api), (c¯a,¯api)} could have combined to form c¯api. Contextual knowledge is necessary for perfect analysis of Sanskrit text (Hellwig, 2009), and current methods have not evolved enough to be able to supply context. However the field of Statistical Machine Learning, by making reasonable assumptions, allows us to provide approximations of these processes. Previous work (Beesley, 1998; Hyman, 2007) indicates that Finite State Transducers (FST) could be used to generate morphologically valid splits. Mittal (2010) considers the FST approach as well as an approach based on Optimality Theory (Prince and Smolensky, 1993), by defining a posterior probability function to choose among all morphologically valid splits of a chunk (OT1). More recently, Kumar et al. (2010) report findings using a different posterior probability function with the same Optimality Theory approach (OT2). Firstly, we derive our own posterior probability function, which is different from OT1 and OT2. Introd"
J10-3004,W09-1803,1,0.889183,"004). Finally, we brieﬂy mention some work which appeared after we developed the system described here. Wang and Oard (2009) is another system that uses TF–IDF unigrams, but augments these feature vectors using the information retrieval technique of message expansion. They report results on our corpus which improve on our own. Adams (2008) attempts to use not only WordNet but Latent Dirichlet Allocation to describe semantic relatedness. He ﬁnds both techniques ineffective. In addition, he annotates a large corpus of Internet Relay Chat and similarly ﬁnds that annotators have trouble agreeing. Elsner and Schudy (2009) explore different partitioning strategies, improving on the greedy algorithm that we present. 3. Data Set Our data set is recorded from IRC channel ## LINUX at free–node.net, using the freely available gaim client. ## LINUX is an unofﬁcial tech support line for the Linux operating system, selected because it is one of the most active chat rooms on freenode, leading to many simultaneous conversations, and because its content is typically inoffensive. Although it is notionally intended only for tech support, it includes large amounts of social chat as well, such as the conversation about factor"
J10-3004,N06-1041,0,0.027446,"l task, and metrics for agreement on supervised classiﬁcation, such as the κ statistic, are not applicable. To measure global similarity between annotations, we use one-to-one accuracy. This measure describes how well we can extract whole conversations intact, as required for summarization or information extraction. To compute it, we pair up conversations from the two annotations to maximize the total overlap by computing an optimal maxweight bipartite matching, then report the percentage of overlap found.3 One-to-one accuracy is a standard metric in unsupervised part-of-speech tagging (e.g., Haghighi and Klein 2006), and is equivalent to mention-based CEAF (Luo 2005) for coreference resolution. If we intend to monitor or participate in the conversation as it occurs, we will care more about local judgments. The local agreement metric is a constrained form of the Rand index for clusterings (Rand 1971) which counts agreements and disagreements for pairs within a context k. We consider a particular utterance: The previous k utterances are each in either the same or a different conversation. The lock score between two annotators is their average agreement on these k same/different judgments, averaged over all"
J10-3004,W04-2317,0,0.15917,"Missing"
J10-3004,H05-1004,0,0.0140731,"as the κ statistic, are not applicable. To measure global similarity between annotations, we use one-to-one accuracy. This measure describes how well we can extract whole conversations intact, as required for summarization or information extraction. To compute it, we pair up conversations from the two annotations to maximize the total overlap by computing an optimal maxweight bipartite matching, then report the percentage of overlap found.3 One-to-one accuracy is a standard metric in unsupervised part-of-speech tagging (e.g., Haghighi and Klein 2006), and is equivalent to mention-based CEAF (Luo 2005) for coreference resolution. If we intend to monitor or participate in the conversation as it occurs, we will care more about local judgments. The local agreement metric is a constrained form of the Rand index for clusterings (Rand 1971) which counts agreements and disagreements for pairs within a context k. We consider a particular utterance: The previous k utterances are each in either the same or a different conversation. The lock score between two annotators is their average agreement on these k same/different judgments, averaged over all utterances. For example, loc1 counts pairs of adjac"
J10-3004,P06-1004,0,0.0145708,"peaker takes part in about 3.3 conversations (the actual number varies for each annotator). The more talkative a speaker is, the more conversations they participate in, as shown by a plot of conversations versus utterances (Figure 4). The assumption is not very accurate, especially for speakers with more than 10 utterances. 4. Model Our model for disentanglement ﬁts into the general class of graph partitioning algorithms (Roth and Yih 2004) that have been used for a variety of tasks in NLP, including coreference resolution (Soon, Ng, and Lim 2001) and the related task of meeting segmentation (Malioutov and Barzilay 2006). These algorithms operate in two stages: First, 397 Computational Linguistics Volume 36, Number 3 Table 2 Feature functions with performance on development data. Chat-speciﬁc (Acc: 73 Prec: 73 Rec: 61 F: 66) Time Speaker Mention x-y Mention same Mention other The time between x and y in seconds, discretized into logarithmically sized bins. x and y have the same speaker. x mentions the speaker of y (or vice versa). For example, this feature is true for a pair such as: Felicia “Gale: ... and any utterance spoken by Gale. Both x and y mention the same name. either x or y mentions a third person’"
J10-3004,C02-1139,0,0.0261248,"Missing"
J10-3004,W04-2401,0,0.00842338,"on about the data set, we test the appropriateness of the assumption (used in previous work) that each speaker takes part in only one conversation. In our data, the average speaker takes part in about 3.3 conversations (the actual number varies for each annotator). The more talkative a speaker is, the more conversations they participate in, as shown by a plot of conversations versus utterances (Figure 4). The assumption is not very accurate, especially for speakers with more than 10 utterances. 4. Model Our model for disentanglement ﬁts into the general class of graph partitioning algorithms (Roth and Yih 2004) that have been used for a variety of tasks in NLP, including coreference resolution (Soon, Ng, and Lim 2001) and the related task of meeting segmentation (Malioutov and Barzilay 2006). These algorithms operate in two stages: First, 397 Computational Linguistics Volume 36, Number 3 Table 2 Feature functions with performance on development data. Chat-speciﬁc (Acc: 73 Prec: 73 Rec: 61 F: 66) Time Speaker Mention x-y Mention same Mention other The time between x and y in seconds, discretized into logarithmically sized bins. x and y have the same speaker. x mentions the speaker of y (or vice versa"
J10-3004,J01-4004,0,0.034712,"Missing"
J10-3004,traum-etal-2004-evaluation,0,0.0634965,"Missing"
J10-3004,N09-1023,0,0.426937,"ell (2008) is based, and uses very similar methodology based on TF-IDF. Our two-stage classiﬁcation and partitioning algorithm draws on work on coreference resolution. Many approaches to coreference (starting with Soon, Ng, and Lim 2001) use such an approach, building global clusters based on pairwise decisions made by a 392 Elsner and Charniak Disentangling Chat classiﬁer. The global partitioning problem was identiﬁed as correlation clustering, an NP-hard problem, by McCallum and Wellner (2004). Finally, we brieﬂy mention some work which appeared after we developed the system described here. Wang and Oard (2009) is another system that uses TF–IDF unigrams, but augments these feature vectors using the information retrieval technique of message expansion. They report results on our corpus which improve on our own. Adams (2008) attempts to use not only WordNet but Latent Dirichlet Allocation to describe semantic relatedness. He ﬁnds both techniques ineffective. In addition, he annotates a large corpus of Internet Relay Chat and similarly ﬁnds that annotators have trouble agreeing. Elsner and Schudy (2009) explore different partitioning strategies, improving on the greedy algorithm that we present. 3. Da"
J10-3004,E06-1022,0,\N,Missing
J11-4001,E03-1009,0,0.0621654,"actic. For example, a good way to raise probability is to split determiners into two categories, one including a, the second an. Similarly, for nouns create two classes, “starting with vowel” and “starting with consonant.” This means that other less-used categories (foreign word or symbol) get omitted entirely. This is the sort of thing EM does. Words are assigned many tags (typically on the order of 10 to 20) and it evens out the tag assignments so most tags have the same number of words assigned to them. There is, however, prior knowledge that is useful here. I am following here the work of Clark (2003). Clark ﬁrst notes that, for the languages tried to date, most words have either just one tag or one tag that dominates all the others. As just remarked, this is a far cry from what EM does. So the ﬁrst thing Clark (2003) does is to impose a one-tag-perword-type constraint. This more closely models the true distribution than does the EM outcome. In the paper he notes that giving this up inevitably yields poorer results. Secondly, he notes that, again contrary to EM’s proclivities, tags have very skewed word counts. Some, typically content-word types like nouns, have very large numbers of word"
J11-4001,P07-1094,0,0.136707,"odels coming out on top. 3. Informative Priors Bayes’ Law shows us how priors on possible world models should be combined with the evidence of our senses to guide our search for the correct one. In this section I give three examples of such priors, two in language, one in vision. 3.1 Priors in Word Segmentation In section 1 we looked at the work in Saffran, Aslin, and Newport (1996) on infants’ ability to divide a speech stream into individual “words.” This problem has also been attacked using computational approaches, albeit with simplifying assumptions. We base our discussion on the work of Goldwater and Grifﬁths (2007), as it dramatically shows the need for some informative prior. This, like most other computational linguistic work on word segmentation, considers an abstract model of the problem. A corpus of child-directed speech is translated into a simpliﬁed phoneme string by ﬁrst transcribing the words, then for each word writing down its most common pronunciation. All spaces between sounds inside an utterance are removed, but the boundaries between utterances are kept. For example, you want to see the book would come out as yuwanttusiD6bUk. The output is to be the phoneme sequence with word boundaries r"
J11-4001,J94-2001,0,0.0349866,"ptures the syntactic properties of the word. The problem is not completely well deﬁned insofar as there is no single “correct” set—part-of-speech sets for English range in size between 12 and 20 · 12. Nevertheless, words do fall into such categories and even as rough approximations they are useful as a ﬁrst step in parsing. In unsupervised part-of-speech induction we want our program to infer a set of parts of speech. Because of their arbitrary properties, the size of this set is typically set in advance—for English typically 45, corresponding to the Penn Treebank set. Since the early work of Merialdo (1994) we have known that this is a difﬁcult problem, and in retrospect it is easy to see why. Most generative models work from the following generative story: For each word wi Generate ti from ti−1 according to P(ti |ti−1 ) Generate wi from its tag according to P(wi |ti ). 650 Charniak The Brain as a Statistical Inference Engine where ti is the ith (current) tag, ti−1 the previous, and wi is the current word. With no prior over models, any such story is simply going to divide words into some number of classes to best raise the probability of the words. Merialdo uses expectation maximization (EM) to"
J11-4001,D10-1066,1,\N,Missing
J11-4001,N10-1061,0,\N,Missing
J98-2004,W96-0212,1,0.427523,"Missing"
J98-2004,H90-1053,0,0.216003,"equation, we can see that oL(Xj,k) and p(to,n) represent the influence of the surrounding words. Thus using fl alone assumes that o~ and p(to,n) can be ignored. We will refer to this figure of merit as straight ft. 3.2 Normalized fl One side effect of omitting the c~ and p(to,,) terms in the straight fl figure above is that inside probability alone tends to prefer shorter constituents to longer ones, as the 279 Computational Linguistics Volume 24, Number 2 inside probability of a longer constituent involves the product of more probabilities. This can result in a &quot;thrashing&quot; effect as noted in Chitrao and Grishman (1990), where the system parses short constituents, even very low-probability ones, while avoiding combining them into longer constituents. To avoid thrashing, some technique is used to normalize the inside probability for use as a figure of merit. One approach is to take the geometric m e a n of the inside probability, to obtain a per-word inside probability. (In the &quot;ideal&quot; model, the p(to,n) term acts as a normalizing factor.) The per-word inside probability of the constituent N~,k is calculated as: We will refer to this figure as normalized ft. 3.3 Trigram Estimate An alternative w a y to rewrit"
J98-2004,J91-3004,0,0.0345559,"rule, the probability of the rule itself, and fl of each nonterminal N~,s appearing to the left of Nj,k in the rule. Then aL(N~,k) is the sum of these products: O~L( N~,k ) -~ ~ lhs(e) aL(N~tart(e),end(e))p(rule(e) ) Hfl(N~,s ). eEq,, N~q,~ Given a complete parse of the sentence, the formula above gives an exact value for aL. During parsing, the set gjik is not complete, and so the formula gives an approximation of aL. This formula can be infinitely recursive, depending on the properties of the grammar. A method for calculating O~Lmore efficiently can be derived from the calculations given in Jelinek and Lafferty (1991). A simple extension to the normalized fl model allows us to estimate the perword probability of all tags in the sentence through the end of the constituent under consideration. This allows us to take advantage of information already obtained in a left-right parse. We calculate this quantity as follows: k Ni i We are again taking the geometric mean to avoid thrashing by compensating for the aLfl quantity&apos;s preference for shorter constituents, as explained in the previous section. We refer to this figure of merit as normalized OlLfl. 4.2 Prefix Estimate We also derived an estimate of the ideal"
J98-2004,H91-1045,0,0.0783045,"and Chitrao and Grishman (1990) introduced statistical agendabased parsing techniques. Chitrao and Grishman implemented a best-first probabilistic parser and noted the parser&apos;s tendency to prefer shorter constituents. They proposed a heuristic solution of penalizing shorter constituents by a fixed amount per word. Miller and Fox (1994) compare the performance of parsers using three different types of grammars, and show that a probabilistic context-free grammar using inside probability (unnormalized) as a figure of merit outperforms both a context-free grammar and a context-dependent grammar. Kochman and Kupin (1991) propose a figure of merit closely related to our prefix estimate. They do not actually incorporate this figure into a best-first parser. 292 Caraballo and Charniak Figures of Merit Magerman and Marcus (1991) use the geometric mean to compute a figure of merit that is independent of constituent length. Magerman and Weir (1992) use a similar model with a different parsing algorithm. 9. Conclusions We have presented and evaluated several figures of merit for best-first parsing. The best performer according to all of our measures was the parser using the boundary trigram estimate as a figure of m"
J98-2004,H91-1044,0,0.0512391,"They proposed a heuristic solution of penalizing shorter constituents by a fixed amount per word. Miller and Fox (1994) compare the performance of parsers using three different types of grammars, and show that a probabilistic context-free grammar using inside probability (unnormalized) as a figure of merit outperforms both a context-free grammar and a context-dependent grammar. Kochman and Kupin (1991) propose a figure of merit closely related to our prefix estimate. They do not actually incorporate this figure into a best-first parser. 292 Caraballo and Charniak Figures of Merit Magerman and Marcus (1991) use the geometric mean to compute a figure of merit that is independent of constituent length. Magerman and Weir (1992) use a similar model with a different parsing algorithm. 9. Conclusions We have presented and evaluated several figures of merit for best-first parsing. The best performer according to all of our measures was the parser using the boundary trigram estimate as a figure of merit, and this result holds for two different grammars. This figure has the additional advantage that it can be easily incorporated into existing best-first parsers using a figure of merit based on inside pro"
J98-2004,J93-2004,0,0.0535138,"Missing"
J98-2004,H94-1051,0,0.0442635,"in Charniak (1996), and the parser in. that paper is a best-first parser using the boundary trigram figure of merit. The literature shows many implementations of best-first parsing, but none of the previous work shares our goal of explicitly comparing figures of merit. Bobrow (1990) and Chitrao and Grishman (1990) introduced statistical agendabased parsing techniques. Chitrao and Grishman implemented a best-first probabilistic parser and noted the parser&apos;s tendency to prefer shorter constituents. They proposed a heuristic solution of penalizing shorter constituents by a fixed amount per word. Miller and Fox (1994) compare the performance of parsers using three different types of grammars, and show that a probabilistic context-free grammar using inside probability (unnormalized) as a figure of merit outperforms both a context-free grammar and a context-dependent grammar. Kochman and Kupin (1991) propose a figure of merit closely related to our prefix estimate. They do not actually incorporate this figure into a best-first parser. 292 Caraballo and Charniak Figures of Merit Magerman and Marcus (1991) use the geometric mean to compute a figure of merit that is independent of constituent length. Magerman a"
J98-2004,H91-1042,0,\N,Missing
J98-2004,P92-1006,0,\N,Missing
N01-1007,P99-1008,1,0.797257,"Missing"
N01-1007,W95-0101,0,0.127551,"Missing"
N01-1007,P99-1016,0,0.0326923,"Missing"
N01-1007,W99-0613,0,0.143343,"Missing"
N01-1007,W98-1119,1,0.849493,"Missing"
N01-1007,J94-2001,0,0.0139139,"referent with “Yotaro Suzuki” by labeling “Mikio” descriptor. As noted briefly in section 3, we have considered more complicated probabilities models to replace equation 2. The most obvious of these is to allow the distribution over numbers of words for each label to be conditioned on the previous label — e.g., a bi-label model. This model generally performed poorly, although the coreference versions often performed as well as the coreference model reported here. Our hypothesis is that we are seeing problems similar to those that have bedeviled applying EM to tasks like part-of-speech tagging [7]. In such cases EM typically tries to lower probabilities of the corpus by using the tags to encode common wordword combinations. As the models corresponding to equations 2 and 8 do not include any label-label probabilities, this problem does not appear in these models. 6 Other Applications It is probably clear to most readers that the structure and probabilities learned by these models, particularly the coreferent model, could be used for tasks other than assigning structure to names. For starters, we would imagine that a named entity recognition program that used information about name struc"
N01-1007,W97-0313,0,0.0350131,"Missing"
N01-1007,P98-2182,1,0.856715,"Missing"
N01-1007,P95-1026,0,0.0592379,"Missing"
N01-1007,C98-2177,1,\N,Missing
N01-1016,P92-1008,0,0.0358054,"Missing"
N01-1016,A00-2018,1,0.155269,"r would add, e.g., the node dominating the EDITED node, seems much less critical. Note that of the major problems associated with transcribed speech, we choose to deal with only one of them, speech repairs, in a special fashion. Our reasoning here is based upon what one might and might not expect from a secondpass statistical parser. For example, ungrammaticality in some sense is relative, so if the training corpus contains the same kind of ungrammatical examples as the testing corpus, one would not expect ungrammaticality itself to be a show stopper. Furthermore, the best statistical parsers [3,5] do not use grammatical rules, but rather define probability distributions over all possible rules. Similarly, parentheticals and filled pauses exist in the newspaper text these parsers currently handle, albeit at a much lower rate. Thus there is no particular reason to expect these constructions to have a major impact.1 This leaves speech repairs as the one major phenomenon not present in written text that might pose a major problem for our parser. It is for that reason that we have chosen to handle it separately. The organization of this paper follows the architecture just described. Section"
N01-1016,P96-1025,0,0.100431,"Missing"
N01-1016,P97-1003,0,0.0749165,"r would add, e.g., the node dominating the EDITED node, seems much less critical. Note that of the major problems associated with transcribed speech, we choose to deal with only one of them, speech repairs, in a special fashion. Our reasoning here is based upon what one might and might not expect from a secondpass statistical parser. For example, ungrammaticality in some sense is relative, so if the training corpus contains the same kind of ungrammatical examples as the testing corpus, one would not expect ungrammaticality itself to be a show stopper. Furthermore, the best statistical parsers [3,5] do not use grammatical rules, but rather define probability distributions over all possible rules. Similarly, parentheticals and filled pauses exist in the newspaper text these parsers currently handle, albeit at a much lower rate. Thus there is no particular reason to expect these constructions to have a major impact.1 This leaves speech repairs as the one major phenomenon not present in written text that might pose a major problem for our parser. It is for that reason that we have chosen to handle it separately. The organization of this paper follows the architecture just described. Section"
N01-1016,J03-4003,0,0.0505572,"bitag tagger). We compare these results to a baseline “null” classifier, which never identifies a word as EDITED. Our basic measure of performance is the word misclassification rate (see Section 2.1). However, we also report precision and recall scores for EDITED words alone. All words are assigned one of the two possible labels, EDITED or not. However, in our evaluation we report the accuracy of only words other than punctuation and filled pauses. Our logic here is much the same as that in the statistical parsing community which ignores the location of punctuation for purposes of evaluation [3,5, 6] on the grounds that its placement is entirely conventional. The same can be said for filled pauses in the switchboard corpus. Our results are given in Table 2. They show that our classifier makes only approximately 1/3 W0 P0 , P1 , P2 , Pf T−1 , T0 , T1 , T2 , Tf Nm Nu Ni Nl Nr Ct Cw Ti Orthographic word Partial word flags POS tags Number of words in common in source and copy Number of words in source that do not appear in copy Number of words in interregnum Number of words to left edge of source Number of words to right edge of source Followed by identical tag flag Followed by identical word"
N01-1016,P99-1053,0,0.445837,"ring given to the second pass, an already existing statistical parser trained on a transcribed speech ∗ This research was supported in part by NSF grant LIS SBR 9720368 and by NSF ITR grant 20100203. corpus. (In particular, all of the research in this paper was performed on the parsed “Switchboard” corpus as provided by the Linguistic Data Consortium.) This architecture is based upon a fundamental assumption: that the semantic and pragmatic content of an utterance is based solely on the unedited words in the word sequence. This assumption is not completely true. For example, Core and Schubert [8] point to counterexamples such as “have the engine take the oranges to Elmira, um, I mean, take them to Corning” where the antecedent of “them” is found in the EDITED words. However, we believe that the assumption is so close to true that the number of errors introduced by this assumption is small compared to the total number of errors made by the system. In order to evaluate the parser’s output we compare it with the gold-standard parse trees. For this purpose a very simple third pass is added to the architecture: the hypothesized edited words are inserted into the parser output (see Section"
N01-1016,P97-1033,0,0.0433446,"Missing"
N01-1016,J99-4003,0,0.217249,"Missing"
N01-1016,P83-1019,0,0.0541695,"Missing"
N01-1016,P95-1037,0,0.117166,"Missing"
N04-1011,N01-1016,1,0.951414,"eriments below differ from the experiments of N¨oth and Kompe in many ways. First, we used speech transcripts rather than speech recognizer lattices. Second, we used a general-purpose broadcoverage statistical parser rather than a unification grammar parser with a hand-constructed grammar. 2 Method The data used for this study is the transcribed version of the Switchboard Corpus as released by the Linguistic Data Consortium. The Switchboard Corpus is a corpus of telephone conversations between adult speakers of varying dialects. The corpus was split into training and test data as described in Charniak and Johnson (2001). The training data consisted of all files in sections 2 and 3 of the Switchboard treebank. The testing corpus consists of files sw4004.mrg to sw4153.mrg, while files sw4519.mrg to sw4936.mrg were used as development corpus. 2.1 Prosodic variables Prosodic information for the corpus was obtained from forced alignments provided by Hamaker et al. (2003) and Ferrer et al. (2002). Hamaker et al. (2003) provided word alignments between the LDC parsed corpus and new alignments of the Switchboard Coprus. Most of the differences between the two alignments were individual lexical items. In cases of dif"
N04-1011,A00-2018,1,0.333122,"sodic cues improve parsing accuracy in the same way that punctuation does. Punctuation is represented in the various Penn treebank corpora as independent word-like tokens, with corresponding terminal and preterminal nodes, as shown in Figure 1 (Bies et al., 1995). Even though this seems linguistically highly unnatural (e.g., punctuation might indicate suprasegmental prosodic properties), statistical parsers generally perform significantly better when their training and test data contains punctuation represented in this way than if the punctuation is stripped out of the training and test data (Charniak, 2000; Engel et al., 2002; Johnson, 1998). On the Switchboard treebank data set using the experimental setup described below we obtained an F-score of 0.882 when using punctuation and 0.869 when punctuation was stripped out, replicating previous experiments demonstrating the importance of punctuation. (F-score is a standard measure of parse accuracy, see e.g., Manning and Sch¨utze (1999) for details). This paper investigates how prosodic cues, when encoded in the parser’s input in a manner similar to the way the Penn treebanks encode punctuation, affect parser accuracy. Our starting point is the ob"
N04-1011,W02-1007,1,0.912989,"ve parsing accuracy in the same way that punctuation does. Punctuation is represented in the various Penn treebank corpora as independent word-like tokens, with corresponding terminal and preterminal nodes, as shown in Figure 1 (Bies et al., 1995). Even though this seems linguistically highly unnatural (e.g., punctuation might indicate suprasegmental prosodic properties), statistical parsers generally perform significantly better when their training and test data contains punctuation represented in this way than if the punctuation is stripped out of the training and test data (Charniak, 2000; Engel et al., 2002; Johnson, 1998). On the Switchboard treebank data set using the experimental setup described below we obtained an F-score of 0.882 when using punctuation and 0.869 when punctuation was stripped out, replicating previous experiments demonstrating the importance of punctuation. (F-score is a standard measure of parse accuracy, see e.g., Manning and Sch¨utze (1999) for details). This paper investigates how prosodic cues, when encoded in the parser’s input in a manner similar to the way the Penn treebanks encode punctuation, affect parser accuracy. Our starting point is the observation that the P"
N04-1011,J98-4004,1,0.757271,"in the same way that punctuation does. Punctuation is represented in the various Penn treebank corpora as independent word-like tokens, with corresponding terminal and preterminal nodes, as shown in Figure 1 (Bies et al., 1995). Even though this seems linguistically highly unnatural (e.g., punctuation might indicate suprasegmental prosodic properties), statistical parsers generally perform significantly better when their training and test data contains punctuation represented in this way than if the punctuation is stripped out of the training and test data (Charniak, 2000; Engel et al., 2002; Johnson, 1998). On the Switchboard treebank data set using the experimental setup described below we obtained an F-score of 0.882 when using punctuation and 0.869 when punctuation was stripped out, replicating previous experiments demonstrating the importance of punctuation. (F-score is a standard measure of parse accuracy, see e.g., Manning and Sch¨utze (1999) for details). This paper investigates how prosodic cues, when encoded in the parser’s input in a manner similar to the way the Penn treebanks encode punctuation, affect parser accuracy. Our starting point is the observation that the Penn treebank ann"
N06-1020,P05-1022,1,0.841488,"native reranker reorders the n-best list. These components constitute two views of the data, though the reranker’s view is restricted to the parses suggested by the first-stage parser. The reranker is not able to suggest new parses and, moreover, uses the probability of each parse tree according to the parser as a feature to perform the reranking. Nevertheless, the reranker’s value comes from its ability to make use of more powerful features. 3.1 The first-stage 50-best parser The first stage of our parser is the lexicalized probabilistic context-free parser described in (Charniak, 2000) and (Charniak and Johnson, 2005). The parser’s grammar is a smoothed third-order Markov grammar, enhanced with lexical heads, their parts of speech, and parent and grandparent information. The parser uses five probability distributions, 153 one each for heads, their parts-of-speech, headconstituent, left-of-head constituents, and right-ofhead constituents. As all distributions are conditioned with five or more features, they are all heavily backed off using Chen back-off (the average-count method from Chen and Goodman (1996)). Also, the statistics are lightly pruned to remove those that are statistically less reliable/useful"
N06-1020,A00-2018,1,0.383878,"ses. Next, a discriminative reranker reorders the n-best list. These components constitute two views of the data, though the reranker’s view is restricted to the parses suggested by the first-stage parser. The reranker is not able to suggest new parses and, moreover, uses the probability of each parse tree according to the parser as a feature to perform the reranking. Nevertheless, the reranker’s value comes from its ability to make use of more powerful features. 3.1 The first-stage 50-best parser The first stage of our parser is the lexicalized probabilistic context-free parser described in (Charniak, 2000) and (Charniak and Johnson, 2005). The parser’s grammar is a smoothed third-order Markov grammar, enhanced with lexical heads, their parts of speech, and parent and grandparent information. The parser uses five probability distributions, 153 one each for heads, their parts-of-speech, headconstituent, left-of-head constituents, and right-ofhead constituents. As all distributions are conditioned with five or more features, they are all heavily backed off using Chen back-off (the average-count method from Chen and Goodman (1996)). Also, the statistics are lightly pruned to remove those that are s"
N06-1020,P96-1041,0,0.00644101,"ur parser is the lexicalized probabilistic context-free parser described in (Charniak, 2000) and (Charniak and Johnson, 2005). The parser’s grammar is a smoothed third-order Markov grammar, enhanced with lexical heads, their parts of speech, and parent and grandparent information. The parser uses five probability distributions, 153 one each for heads, their parts-of-speech, headconstituent, left-of-head constituents, and right-ofhead constituents. As all distributions are conditioned with five or more features, they are all heavily backed off using Chen back-off (the average-count method from Chen and Goodman (1996)). Also, the statistics are lightly pruned to remove those that are statistically less reliable/useful. As in (Charniak and Johnson, 2005) the parser has been modified to produce n-best parses. However, the n-best parsing algorithm described in that paper has been replaced by the much more efficient algorithm described in (Jimenez and Marzal, 2000; Huang and Chang, 2005). 3.2 The MaxEnt Reranker The second stage of our parser is a Maximum Entropy reranker, as described in (Charniak and Johnson, 2005). The reranker takes the 50-best parses for each sentence produced by the first-stage 50best pa"
N06-1020,W03-0407,0,0.52591,"Missing"
N06-1020,W01-0521,0,0.494912,"es self-training to POS-tagging and reports the same outcomes. One would assume that errors in the original model would be amplified in the new model. Parser adaptation can be framed as a semisupervised or unsupervised learning problem. In parser adaptation, one is given annotated training data from a source domain and unannotated data from a target. In some cases, some annotated data from the target domain is available as well. The goal is to use the various data sets to produce a model that accurately parses the target domain data despite seeing little or no annotated data from that domain. Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing ac152 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 152–159, c New York, June 2006. 2006 Association for Computational Linguistics curacy. The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found. Our work differs in that all our data is in-domain while Bacchiani et al. uses the Brown corpus as labelled data. These correspond to different scenarios. Additionally, we e"
N06-1020,P04-1013,0,0.0092553,"Missing"
N06-1020,J93-1005,0,0.073668,"Conjunctions are about the hardest things in parsing, and we have no grip on exactly what it takes to help parse them. Conversely, everyone expected improvements on unknown words, as the self-training should dras2000 0 Number of sentences 500 1000 1500 Better No change Worse 0 1 2 3 Number of CCs 4 5 Figure 5: How self-training improves performance as a function of number of conjunctions tically reduce the number of them. It is also the case that we thought PP attachment might be improved because of the increased coverage of prepositionnoun and preposition-verb combinations that work such as (Hindle and Rooth, 1993) show to be so important. Currently, our best conjecture is that unknowns are not improved because the words that are unknown in the WSJ are not significantly represented in the LA Times we used for self-training. CCs are difficult for parsers because each conjunct has only one secure boundary. This is particularly the case with longer conjunctions, those of VPs and Ss. One thing we know is that self-training always improves performance of the parsing model when used as a language model. We think CC improvement is connected with this fact and our earlier point that the probabilities of the 50-"
N06-1020,W05-1506,0,0.198641,"h, headconstituent, left-of-head constituents, and right-ofhead constituents. As all distributions are conditioned with five or more features, they are all heavily backed off using Chen back-off (the average-count method from Chen and Goodman (1996)). Also, the statistics are lightly pruned to remove those that are statistically less reliable/useful. As in (Charniak and Johnson, 2005) the parser has been modified to produce n-best parses. However, the n-best parsing algorithm described in that paper has been replaced by the much more efficient algorithm described in (Jimenez and Marzal, 2000; Huang and Chang, 2005). 3.2 The MaxEnt Reranker The second stage of our parser is a Maximum Entropy reranker, as described in (Charniak and Johnson, 2005). The reranker takes the 50-best parses for each sentence produced by the first-stage 50best parser and selects the best parse from those 50 parses. It does this using the reranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al. (1999). Our reranker classifies each parse with respect to 1,333,519 features (most of which only occur on few parses). The features consist of those descr"
N06-1020,P99-1069,1,0.452655,"Missing"
N06-1020,P02-1017,0,0.0932785,"tracting the appropriate parsing decisions from textual examples. Given sufficient labelled data, there are several “supervised” techniques of training high-performance parsers (Charniak and Johnson, 2005; Collins, 2000; Henderson, 2004). Other methods are “semi-supervised” where they use some labelled data to annotate unlabeled data. Examples of this include self-training (Charniak, 1997) and co-training (Blum and Mitchell, 1998; Steedman et al., 2003). Finally, there are “unsupervised” strategies where no data is labeled and all annotations (including the grammar itself) must be discovered (Klein and Manning, 2002). Semi-supervised and unsupervised methods are important because good labeled data is expensive, A simple method of incorporating unlabeled data into a new model is self-training. In self-training, the existing model first labels unlabeled data. The newly labeled data is then treated as truth and combined with the actual labeled data to train a new model. This process can be iterated over different sets of unlabeled data if desired. It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al. (2003) report either minor improvements or significant damag"
N06-1020,J93-2004,0,0.0474116,"Missing"
N06-1020,N01-1023,0,0.274276,"nker. Co-training is another way to train models from unlabeled data (Blum and Mitchell, 1998). Unlike self-training, co-training requires multiple learners, each with a different “view” of the data. When one learner is confident of its predictions about the data, we apply the predicted label of the data to the training set of the other learners. A variation suggested by Dasgupta et al. (2001) is to add data to the training set when multiple learners agree on the label. If this is the case, we can be more confident that the data was labelled correctly than if only one learner had labelled it. Sarkar (2001) and Steedman et al. (2003) investigated using co-training for parsing. These studies suggest that this type of co-training is most effective when small amounts of labelled training data is available. Additionally, co-training for parsing can be helpful for parser adaptation. 3 Experimental Setup Our parsing model consists of two phases. First, we use a generative parser to produce a list of the top n parses. Next, a discriminative reranker reorders the n-best list. These components constitute two views of the data, though the reranker’s view is restricted to the parses suggested by the first-"
N06-1020,E03-1008,0,0.896701,"phenomenon. 2 Previous work 1 Introduction In parsing, we attempt to uncover the syntactic structure from a string of words. Much of the challenge of this lies in extracting the appropriate parsing decisions from textual examples. Given sufficient labelled data, there are several “supervised” techniques of training high-performance parsers (Charniak and Johnson, 2005; Collins, 2000; Henderson, 2004). Other methods are “semi-supervised” where they use some labelled data to annotate unlabeled data. Examples of this include self-training (Charniak, 1997) and co-training (Blum and Mitchell, 1998; Steedman et al., 2003). Finally, there are “unsupervised” strategies where no data is labeled and all annotations (including the grammar itself) must be discovered (Klein and Manning, 2002). Semi-supervised and unsupervised methods are important because good labeled data is expensive, A simple method of incorporating unlabeled data into a new model is self-training. In self-training, the existing model first labels unlabeled data. The newly labeled data is then treated as truth and combined with the actual labeled data to train a new model. This process can be iterated over different sets of unlabeled data if desir"
N06-1022,P04-1006,1,0.79928,"ll too slow for many applications. In some cases researchers need large quantities of parsed data and do not have the hundreds of machines necessary to parse gigaword corpora in a week or two. More pressingly, in real-time applications such as speech recognition, a parser would be only a part of a much larger system, and the system builders are not keen on giving the parser one of the ten seconds available to process, say, a thirty-word sentence. Even worse, some applications require the parsing of multiple candidate strings per sentence (Johnson and Charniak, 2004) or parsing from a lattice (Hall and Johnson, 2004), and in these applications parsing efficiency is even more important. We present here a multilevel coarse-to-fine (mlctf) PCFG parsing algorithm that reduces the complexity of the search involved in finding the best parse. It defines a sequence of increasingly more complex PCFGs, and uses the parse forest produced by one PCFG to prune the search of the next more complex PCFG. We currently use four levels of grammars in our mlctf algorithm. The simplest PCFG, which we call the level-0 grammar, contains only one nontrivial nonterminal and is so simple that minimal time is needed to parse a sent"
N06-1022,P04-1005,1,0.389081,"about a second per sentence. Unfortunately, this is still too slow for many applications. In some cases researchers need large quantities of parsed data and do not have the hundreds of machines necessary to parse gigaword corpora in a week or two. More pressingly, in real-time applications such as speech recognition, a parser would be only a part of a much larger system, and the system builders are not keen on giving the parser one of the ten seconds available to process, say, a thirty-word sentence. Even worse, some applications require the parsing of multiple candidate strings per sentence (Johnson and Charniak, 2004) or parsing from a lattice (Hall and Johnson, 2004), and in these applications parsing efficiency is even more important. We present here a multilevel coarse-to-fine (mlctf) PCFG parsing algorithm that reduces the complexity of the search involved in finding the best parse. It defines a sequence of increasingly more complex PCFGs, and uses the parse forest produced by one PCFG to prune the search of the next more complex PCFG. We currently use four levels of grammars in our mlctf algorithm. The simplest PCFG, which we call the level-0 grammar, contains only one nontrivial nonterminal and is so"
N06-1022,J98-4004,1,0.862318,"stituent type found in “S →NP VP .” is mapped into its generalization at level 1. The probabilities of all rules are computed using maximum likelihood for constituents at that level. The grammar used by the parser can best be described as being influenced by four components: 1. the nonterminals defined at that level of parsing, 2. the binarization scheme, 3. the generalizations defined over the binarization, and 171 Grammars induced in this way tend to be too specific, as the binarization introduce a very large number of very specialized phrasal categories (the Ai ). Following common practice Johnson (1998; Klein and Manning (2003b) we Markovize by replacing these nonterminals with ones that remember less of the immediate rule context. In our version we keep track of only the parent, the head constituent and the constituent immediately to the right or left, depending on which side of the constituent we are processing. With this scheme the above rules now look like this: A →Ad,c e Ad,c →Aa,c d Aa,c →a Ab,c Ab,c →b c So, for example, the rule “A →Ad,c e” would have a high probability if constituents of type A, with c as their head, often have d followed by e at their end. Lastly, we add parent an"
N06-1022,N03-1016,0,0.854396,"f the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: p(nki,j |s) = α(nki,j )β(nki,j ) . p(s) (1) It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compared directly to ours, but are roughly in the same equivalence class. Those presented in Charniak et al. (1998) are superior, but in Section 5 below we suggest that a combination of the techniques could yield better results still. Klein and Manning (2003a) describe efficient A∗ for the most likely parse, where pruning is accomplished by using Equation 1 and a true upper bound on the outside probability. While their maximum is a looser estimate of the outside probability, it is an admissible heuristic and together with an A∗ search is guaranteed to find the best parse first. One question is if the guarantee is worth the extra search required by the looser estimate of the true outside probability. Tsuruoka and Tsujii (2004) explore the framework developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap mani"
N06-1022,P05-1022,1,0.233451,"parser while the first stage is a regular-expression approximation of the grammar. Again, the second stage is constrained by the parses found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsing algorithm and pulling constituents off the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: p(nki,j |s) = α(nki,j )β(nki,j ) . p(s) (1) It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compa"
N06-1022,P03-1054,0,0.677716,"f the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: p(nki,j |s) = α(nki,j )β(nki,j ) . p(s) (1) It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compared directly to ours, but are roughly in the same equivalence class. Those presented in Charniak et al. (1998) are superior, but in Section 5 below we suggest that a combination of the techniques could yield better results still. Klein and Manning (2003a) describe efficient A∗ for the most likely parse, where pruning is accomplished by using Equation 1 and a true upper bound on the outside probability. While their maximum is a looser estimate of the outside probability, it is an admissible heuristic and together with an A∗ search is guaranteed to find the best parse first. One question is if the guarantee is worth the extra search required by the looser estimate of the true outside probability. Tsuruoka and Tsujii (2004) explore the framework developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap mani"
N06-1022,W98-1115,1,0.878022,"found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsing algorithm and pulling constituents off the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: p(nki,j |s) = α(nki,j )β(nki,j ) . p(s) (1) It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compared directly to ours, but are roughly in the same equivalence class. Those presented in Charniak et al. (1998) are superior, but in"
N06-1022,A00-2018,1,0.744452,"the multilevel dynamic programming algorithm needed for coarse-to-fine analysis (which they apply to decoding rather than parsing), and show how to perform exact coarse-to-fine computation, rather than the heuristic search we perform here. A paper closely related to ours is Goodman (1997). In our terminology, Goodman’s parser is a two-stage ctf parser. The second stage is a standard tree-bank parser while the first stage is a regular-expression approximation of the grammar. Again, the second stage is constrained by the parses found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsin"
N06-1022,P97-1003,0,0.216398,"equired by the heap manipulations necessary in this scheme. They describe an iterative deepening algorithm that does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bilexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing — after all, there are no phrasal constituents to consider. The current paper can be thought of as a way to take the sting out of the grammar constant for PCFGs by parsing first with very few phrasal constituents and adding them only 170 Level: 0 1 © S1 S1                       HP"
N06-1022,P99-1059,0,0.0120516,"t does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bilexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing — after all, there are no phrasal constituents to consider. The current paper can be thought of as a way to take the sting out of the grammar constant for PCFGs by parsing first with very few phrasal constituents and adding them only 170 Level: 0 1 © S1 S1                       HP                     P                       MP                    2 ©  S1           S"
N06-1022,J93-2004,0,0.0285293,"S1 S VP UCP SQ SBAR SBARQ SINV NP NAC NX LST X UCP FRAG ADJP QP CONJP ADVP INTJ PRN PRT PP PRT RRC WHADJP WHADVP WHNP WHPP Figure 1: The levels of nonterminal labels after most constituents have been pruned away. 3 Multilevel Course-to-fine Parsing We use as the underlying parsing algorithm a reasonably standard CKY parser, modified to allow unary branching rules. The complete nonterminal clustering is given in Figure 1. We do not cluster preterminals. These remain fixed at all levels to the standard Penn-tree-bank set Marcus et al. (1993). Level-0 makes two distinctions, the root node and everybody else. At level 1 we make one further distinction, between phrases that tend to be heads of constituents (NPs, VPs, and Ss) and those that tend to be modifiers (ADJPs, PPs, etc.). Level-2 has a total of five categories: root, things that are typically headed by nouns, those headed by verbs, things headed by prepositions, and things headed by classical modifiers (adjectives, adverbs, etc.). Finally, level 3 is the S1 S1 P HP P P PRP VBD He P . ate IN P . at DT 4. extra annotation to improve parsing accuracy. HP HP PRP VBD MP He ate IN"
N06-1022,J93-4001,0,0.0119203,"measured by the total number of constituents processed) is decreased by a factor of ten over standard CKY parsing at the final level. We also discuss some fine points of the results therein. Finally in section 5 we suggest that because the search space of mlctf algorithms is, at this point, almost totally unexplored, future work should be able to improve significantly on these results. 2 Previous Research Coarse-to-fine search is an idea that has appeared several times in the literature of computational linguistics and related areas. The 169 first appearance of this idea we are aware of is in Maxwell and Kaplan (1993), where a covering CFG is automatically extracted from a more detailed unification grammar and used to identify the possible locations of constituents in the more detailed parses of the sentence. Maxwell and Kaplan use their covering CFG to prune the search of their unification grammar parser in essentially the same manner as we do here, and demonstrate significant performance improvements by using their coarse-to-fine approach. The basic theory of coarse-to-fine approximations and dynamic programming in a stochastic framework is laid out in Geman and Kochanek (2001). This paper describes the"
N06-1022,P05-1012,0,0.0309759,"extra search required by the looser estimate of the true outside probability. Tsuruoka and Tsujii (2004) explore the framework developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap manipulations necessary in this scheme. They describe an iterative deepening algorithm that does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bilexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing — after all, there are no phrasal constituents to consider. The current paper can be thought of as a way to take the sting out of the grammar cons"
N06-1022,J98-2004,1,\N,Missing
N06-1022,W97-0302,0,\N,Missing
N07-1055,P04-1051,0,0.0201021,"have little paragraph structure, so τ is an effective metric. Sentence Ordering In the sentence ordering task, (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Soricut and Marcu, 2006), we view a document as an unordered bag of sentences and try to find the ordering of the sentences which maximizes coherence according to our model. This type of ordering process has applications in natural language generation and multi-document summarization. Unfortunately, finding the optimal ordering according to a probabilistic model with local features is NP-complete and non-approximable (Althaus et al., 2004). Moreover, since our model is not Markovian, the relaxation used as a heuristic for A∗ search by Soricut and Marcu (2006) is ineffective. We therefore use simulated annealing to find a high-probability ordering, starting from a random permutation of the sentences. Our search system has few Estimated Search Errors as defined by Soricut and Marcu (2006); it rarely proposes an ordering which has lower proba441 5.2 Discrimination Our second task is the discriminative test used by (Barzilay and Lapata, 2005). In this task we generate random permutations of a test document, and measure how often th"
N07-1055,P05-1018,0,0.148365,"r, Joseph Austerweil, and Eugene Charniak Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University Providence, RI 02912 {melsner,ec}@cs.brown.edu, joseph.austerweil@gmail.com Abstract We present a model for discourse coherence which combines the local entitybased approach of (Barzilay and Lapata, 2005) and the HMM-based content model of (Barzilay and Lee, 2004). Unlike the mixture model of (Soricut and Marcu, 2006), we learn local and global features jointly, providing a better theoretical explanation of how they are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about documen"
N07-1055,N04-1015,0,0.723731,"cal models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong near one another in the document. However, they have trouble finding the beginning or end of the document, or recovering from sudden shifts in topic (such as occur at paragraph boundaries). Some local models also have trouble deciding which of a pair of related sentences ought to come first. In contrast, the global HMM model of Barzilay and Lee (2004) tries to track the predictable changes in topic between sentences. This gives it a pronounced advantage in ordering sentences, since it can learn to represent beginnings, ends and boundaries as separate states. However, it has no local features; the particular words in each sentence are generated based only on the current state of the document. Since information can pass from sentence to sentence only in this restricted manner, the model sometimes fails to place sentences next to the correct neighbors. We attempt here to unify the two approaches by constructing a model with both sentence-to-s"
N07-1055,P05-1022,1,0.165188,"actic roles: subject if possible, then object, or finally other. An example text is figure 1, whose grid is figure 2. Nouns are also treated as salient or non-salient, another important concern of Centering Theory. We condition events involving a noun on the frequency of that noun. Unfortunately, this way of representing salience makes our model slightly deficient, since the model conditions on a particular noun occurring e.g. 2 times, but assigns nonzero probabilities to documents where it occurs 3 times. This is theo1 Roles are determined heuristically using trees produced by the parser of (Charniak and Johnson, 2005). Following previous work, we slightly conflate thematic and syntactic roles, marking the subject of a passive verb as O. 2 The numeric token “1300” is removed in preprocessing, and “Nuevo Laredo” is marked as “PROPER”. 437 0 [The commercial pilot]O , [sole occupant of [the airplane]X ]X , was not injured . 1 [The airplane]O was owned and operated by [a private owner]X . 2 [Visual meteorological conditions]S prevailed for [the personal cross country flight for which [a VFR flight plan]O was filed]X . 3 [The flight]S originated at [Nuevo Laredo , Mexico]X , at [approximately 1300]X . Figure 1:"
N07-1055,J95-2003,0,0.894935,". However, mixture models lack explanatory power; since each of the individual component models is known to be flawed, it is difficult to say that the combination is theoretically more sound than the parts, even if it usually works better. Moreover, since the model we describe uses a strict subset of the features used in the component models of (Soricut and Marcu, 2006), we suspect that adding it to the mixture would lead to still further improved results. 2 Naive Entity Grids Entity grids, first described in (Lapata and Barzilay, 2005), are designed to capture some ideas of Centering Theory (Grosz et al., 1995), namely that adjacent utterances in a locally coherent discourses are likely to contain the same nouns, and that important nouns often appear in syntactically important roles such as subject or object. An entity grid represents a document as a matrix with a column for each entity, and a row for each sentence. The entry ri,j describes the syntactic role of entity j in sentence i: these roles are subject (S), object (O), or some other role (X)1 . In addition there is a special marker (-) for nouns which do not appear at all in a given sentence. Each noun appears only once in a given row of the"
N07-1055,N04-1024,0,0.0358159,"ey are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong near one another in the document. However, they have trouble finding the beginning or end of the document, or recovering"
N07-1055,J04-4001,0,0.0248343,"ricut and Marcu, 2006), we learn local and global features jointly, providing a better theoretical explanation of how they are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong ne"
N07-1055,P03-1069,0,0.759769,"and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong near one another in the document. However, they have trouble finding the beginning or end of the document, or recovering from sudden shifts in topic (such as occur at paragraph boundaries). Some local models also have trouble deciding which of a pair of related sentences ought to come first. In contrast, the global HMM model of Barzilay and Lee (2004)"
N07-1055,J06-4002,0,0.0549184,"Missing"
N07-1055,C04-1129,0,0.0185847,"t. 7 Future Work Ordering in the AIRPLANE corpus and similar constrained sets of short documents is by no means a solved problem, but the results so far show a good deal of promise. Unfortunately, in longer and less formulaic corpora, the models, inference algorithms and even evaluation metrics used thus far may prove extremely difficult to scale up. Domains with more natural writing styles will make lexical prediction a much more difficult problem. On the other hand, the wider variety of grammatical constructions used may motivate more complex syntactic features, for instance as proposed by (Siddharthan et al., 2004) in sentence clustering. Finding optimal orderings is a difficult task even for short documents, and will become exponentially more challenging in longer ones. For multiparagraph documents, it is probably impractical to use full-scale coherence models to find optimal orderings directly. A better approach may be a coarseto-fine or hierarchical system which cuts up longer documents into more manageable chunks that can be ordered as a unit. Multi-paragraph documents also pose a problem for the τ metric itself. In documents with clear thematic divisions between their different sections, a good ord"
N07-1055,P06-2103,0,0.426319,"ph.austerweil@gmail.com Abstract We present a model for discourse coherence which combines the local entitybased approach of (Barzilay and Lapata, 2005) and the HMM-based content model of (Barzilay and Lee, 2004). Unlike the mixture model of (Soricut and Marcu, 2006), we learn local and global features jointly, providing a better theoretical explanation of how they are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Ba"
N07-1055,J08-1001,0,\N,Missing
N07-2045,1991.mtsummit-papers.16,0,0.0350311,"Missing"
N07-2045,C94-1002,0,0.162645,"Missing"
N07-2045,N04-2006,0,0.781289,"Missing"
N07-2045,P01-1017,1,0.833304,"e automatic, but there may have been errors in determiner selection due to parsing error. Table 2 gives “error” examples. Some errors are wrong (either grammatically or yielding a significantly different interpretation), but some “incorrect” answers are reasonable possibilities. Furthermore, even all the text of the article is not enough for classification at times. In particular note Example 5, where unless you know whether IBM was the world leader or simply one of the world leaders at the time of the article, no additional context would help. 4 Conclusions and Future Work With the Charniak (Charniak, 2001) language model, our results exceed those of the previous best (Minnen et al., 2000) on the determiner selection task. This shows the benefits of the language model features in determining the most grammatical determiner to use in a noun phrase. Such a language model looks at much of the structure in individual sentences, but there may be additional features that could improve performance. There is a high rate of ambiguity for many of the misclassified sentences. The success of using a state-of-the-art language Guess the null Correct null the a/an the the a/an a/an null Sentence (1) The comput"
N07-2045,J93-2004,0,0.0279808,"Missing"
N07-2045,C00-1027,0,0.0897374,"Missing"
N07-2045,N06-1020,1,0.828561,"Missing"
N07-2045,W00-0708,0,0.800444,"ing error. Table 2 gives “error” examples. Some errors are wrong (either grammatically or yielding a significantly different interpretation), but some “incorrect” answers are reasonable possibilities. Furthermore, even all the text of the article is not enough for classification at times. In particular note Example 5, where unless you know whether IBM was the world leader or simply one of the world leaders at the time of the article, no additional context would help. 4 Conclusions and Future Work With the Charniak (Charniak, 2001) language model, our results exceed those of the previous best (Minnen et al., 2000) on the determiner selection task. This shows the benefits of the language model features in determining the most grammatical determiner to use in a noun phrase. Such a language model looks at much of the structure in individual sentences, but there may be additional features that could improve performance. There is a high rate of ambiguity for many of the misclassified sentences. The success of using a state-of-the-art language Guess the null Correct null the a/an the the a/an a/an null Sentence (1) The computers were crude by today’s standards. (2) In addition, the Apple II was an affordable"
N07-2045,1993.tmi-1.18,0,0.501305,"Missing"
N07-2045,C90-2023,0,\N,Missing
N07-2045,P98-1085,0,\N,Missing
N07-2045,C98-1082,0,\N,Missing
N09-1019,M98-1014,0,0.012749,"Missing"
N09-1019,E09-1018,1,0.512187,"e 3). Our pronoun information is derived from an unsupervised coreference algorithm which does not use named entity informa1 We stem modifiers with the Porter stemmer. ROOT →Modifiers 0 # NE 0 # Prepositions 0 # Pronouns 0 # ... Pronouns 0 →Pronoun 0 Pronouns 0 Pronouns 0 → Pronoun 0 →pers|loc|org|any pers →i |he|she|who|me . . . loc →where|which|it|its org →which|it|they|we . . . Figure 3: A fragment of the full grammar. The symbol # represents punctuation between different feature types. The prior for class 0 is concentrated around personal pronouns, although other types are possible. tion (Charniak and Elsner, 2009). This algorithm uses EM to learn a generative model with syntactic, number and gender parameters. Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. By setting up the base grammar so that each named-entity class prefers to associate to a single type of pronoun, we can also determine the correspondence between our named-entity symbols and the actual named-entity labels– for the models without pronoun information, this matching is arbitrary and must be inferred during the evaluation process. 3.4 Data Preparation To prepare data for cluste"
N09-1019,P05-1022,1,0.178168,"yntactic, number and gender parameters. Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. By setting up the base grammar so that each named-entity class prefers to associate to a single type of pronoun, we can also determine the correspondence between our named-entity symbols and the actual named-entity labels– for the models without pronoun information, this matching is arbitrary and must be inferred during the evaluation process. 3.4 Data Preparation To prepare data for clustering with our system, we first parse it with the parser of Charniak and Johnson (2005). We then annotate pronouns with Charniak and Elsner (2009). For the evaluation set, we use the named entity data from MUC-7. Here, we extract all strings in <ne> tags and determine their cores, plus any relevant modifiers, governing prepositions and pronouns, by examining the parse trees. In addition, we supply the system with additional data from the North American News Corpus (NANC). Here we extract all NPs headed by proper nouns. We then process our data by merging all examples with the same core; some merged examples from our dataset are shown in Figure 4. When two examples are merged, we"
N09-1019,N01-1007,1,0.896329,"the use of extremely large corpora which allow very precise, but sparse features. For instance Etzioni et al. (2005) and Pasca (2004) use web queries to count occurrences of “cities such as X” and similar phrases. Although our research makes use of a fairly large amount of data, our method is designed to make better use of relatively common contextual features, rather than searching for high-quality semantic features elsewhere. Models of the internal structure of names have been used for cross-document coreference (Li et al., 2004; Bhattacharya and Getoor, 2006) and a goal in their own right (Charniak, 2001). Li et al. (2004) take named entity classes as a given, and develops both generative and discriminative models to detect coreference between members of each class. Their generative model designates a particular mention of a name as a “representative” and generates all other mentions from it according to an editing process. Bhattacharya and Getoor (2006) operates only on authors of scientific papers. Their model accounts for a wider variety of name variants than ours, including misspellings and initials. In addition, they confirm our intuition that Gibbs sampling for inference has insufficient"
N09-1019,W99-0613,0,0.529894,"he i prior parameters for the entity-specific symbols Exk are fixed so that, with overwhelming probability, only one expansion occurs. We can represent any fixed number of entities Ek with a standard adaptor grammar, but since we do not know the correct number, we must extend the adaptor model slightly to allow for an unbounded number. We generate the Ek from a Chinese Restaurant process prior. (General grammars with infinite numbers of nonterminals were studied by (Liang et al., 2007b)). 3.3 Modifiers, Prepositions and Pronouns Next, we introduce two types of context information derived from Collins and Singer (1999): nominal modifiers and prepositional information. A nominal modifier is either the head of an appositive phrase (“Maury Cooper, a vice president”) or a non-proper prenominal (“spokesman John Smith”)1 . If the entity is the complement of a preposition, we extract the preposition and the head of the governing NP (“a federally funded sewage plant in Georgia”). These are added to the grammar at the named-entity class level (separated from the core by a special punctuation symbol). Finally, we add information about pronouns and wh-complementizers (Figure 3). Our pronoun information is derived from"
N09-1019,D07-1074,0,0.0335999,"Missing"
N09-1019,P07-1107,0,0.5112,"m (using no “seed rules” or initial heuristics); to our knowledge this is the best such system reported on the MUC-7 dataset. In addition, the model clusters the words which appear in named entities, discovering groups of words with similar roles such as first names and types of organization. Finally, the model defines a notion of consistency between different references to the same entity; this component of the model yields a significant increase in performance. The main motivation for our system is the recent success of unsupervised generative models for coreference resolution. The model of Haghighi and Klein (2007) incorporated a latent variable for named entity class. They report a named entity score 164 of 61.2 percent, well above the baseline of 46.4, but still far behind existing named-entity systems. We suspect that better models for named entities could aid in the coreference task. The easiest way to incorporate a better model is simply to run a supervised or semi-supervised system as a preprocess. To perform joint inference, however, requires an unsupervised generative model for named entities. As far as we know, this work is the best such model. Named entities also pose another problem with the"
N09-1019,N07-1018,1,0.413381,"ers) observed throughout a large input corpus while keeping the size of our input file small. To create an input file, we first add all the MUC7 examples. We then draw additional examples from NANC, ranking them by how many features they have, until we reach a specified number (larger datasets take longer, but without enough data, results tend to be poor). 3.5 Inference Our implementation of adaptor grammars is a modified version of the Pitman-Yor adaptor grammar sampler2 , altered to deal with the infinite number of entities. It carries out inference using a Metropoliswithin-Gibbs algorithm (Johnson et al., 2007), in which it repeatedly parses each input line using the CYK algorithm, samples a parse, and proposes this as the new tree. To do Gibbs sampling for our consistencyenforcing model, we would need to sample a parse for an example from the posterior over every possible entity. However, since there are thousands of entities (the number grows roughly linearly with the number of merged examples in the data file), this is not tractable. Instead, we perform a restricted Gibbs sampling search, where we enumerate the posterior only for entities which share a word in their core with the example in quest"
N09-1019,D07-1072,0,0.0410452,"ER,Clinton ∼ EP0 ER , which we intend to be a distribution over titles in general. The resulting grammar is shown in Figure 2; the i prior parameters for the entity-specific symbols Exk are fixed so that, with overwhelming probability, only one expansion occurs. We can represent any fixed number of entities Ek with a standard adaptor grammar, but since we do not know the correct number, we must extend the adaptor model slightly to allow for an unbounded number. We generate the Ek from a Chinese Restaurant process prior. (General grammars with infinite numbers of nonterminals were studied by (Liang et al., 2007b)). 3.3 Modifiers, Prepositions and Pronouns Next, we introduce two types of context information derived from Collins and Singer (1999): nominal modifiers and prepositional information. A nominal modifier is either the head of an appositive phrase (“Maury Cooper, a vice president”) or a non-proper prenominal (“spokesman John Smith”)1 . If the entity is the complement of a preposition, we extract the preposition and the head of the governing NP (“a federally funded sewage plant in Georgia”). These are added to the grammar at the named-entity class level (separated from the core by a special pu"
N09-1019,M98-1021,0,0.0138538,"Missing"
N09-1019,D08-1067,0,0.0360479,"d aid in the coreference task. The easiest way to incorporate a better model is simply to run a supervised or semi-supervised system as a preprocess. To perform joint inference, however, requires an unsupervised generative model for named entities. As far as we know, this work is the best such model. Named entities also pose another problem with the Haghighi and Klein (2007) coreference model; since it models only the heads of NPs, it will fail to resolve some references to named entities: (“Ford Motor Co.”, “Ford”), while erroneously merging others: (“Ford Motor Co.”, “Lockheed Martin Co.”). Ng (2008) showed that better features for matching named entities– exact string match and an “alias detector” looking for acronyms, abbreviations and name variants– improve the model’s performance substantially. Yet building an alias detector is nontrivial (Uryupina, 2004). English speakers know that “President Clinton” is the same person as “Bill Clinton” , not “President Bush”. But this cannot be implemented by simple substring matching. It requires some concept of the role of each word in the string. Our model attempts to learn this role information by clustering the words within named entities. 2 R"
N09-1019,uryupina-2004-evaluating,0,0.0143615,"we know, this work is the best such model. Named entities also pose another problem with the Haghighi and Klein (2007) coreference model; since it models only the heads of NPs, it will fail to resolve some references to named entities: (“Ford Motor Co.”, “Ford”), while erroneously merging others: (“Ford Motor Co.”, “Lockheed Martin Co.”). Ng (2008) showed that better features for matching named entities– exact string match and an “alias detector” looking for acronyms, abbreviations and name variants– improve the model’s performance substantially. Yet building an alias detector is nontrivial (Uryupina, 2004). English speakers know that “President Clinton” is the same person as “Bill Clinton” , not “President Bush”. But this cannot be implemented by simple substring matching. It requires some concept of the role of each word in the string. Our model attempts to learn this role information by clustering the words within named entities. 2 Related Work Supervised named entity recognition now performs almost as well as human annotation in English (Chinchor, 1998) and has excellent performance on other languages (Tjong Kim Sang and De Meulder, 2003). For a survey of the state of the art, Human Language"
N09-1019,M98-1004,0,\N,Missing
N09-1019,M98-1012,0,\N,Missing
N09-1019,W03-0419,0,\N,Missing
N10-1004,P07-1056,0,0.0568525,"ll non-oracle baselines. We conclude with a discussion and future work (Section 7). 2 Related work The closest work to ours is Plank and Sima’an (2008), where unlabeled text is used to group sentences from WSJ into subdomains. The authors create a model for each subdomain which weights trees from its subdomain more highly than others. Given the domain specific models, they consider different parse combination strategies. Unfortunately, these methods do not yield a statistically significant improvement. 29 Multiple source domain adaptation has been done for other tasks (e.g. classification in (Blitzer et al., 2007; Daum´e III, 2007; Dredze and Crammer, 2008)) and is related to multitask learning. Daum´e III (2007) shows that an extremely simple method delivers solid performance on a number of domain adaptation classification tasks. This is achieved by making a copy of each feature for each source domain plus the “general” pseudodomain (for capturing domain independent features). This allows the classifier to directly model which features are domain-specific. Finkel and Manning (2009) demonstrate the hierarchical Bayesian extension of this where domain-specific models draw from a general base distributi"
N10-1004,W08-2102,0,0.0245555,"Missing"
N10-1004,P05-1022,1,0.862789,"tistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations of parsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition"
N10-1004,A00-2018,1,0.822038,"et text) but not the identity of its domain. The challenge is determining how to best use the available 28 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 28–36, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics Test Train BNC GENIA BROWN SWBD ETT WSJ GENIA 66.3 81.0 70.8 72.7 82.5 83.6 71.5 62.9 65.3 74.9 64.6 86.3 75.5 75.4 83.8 51.6 79.0 89.0 75.2 78.5 69.0 80.9 75.9 81.9 83.4 66.6 80.6 69.1 73.2 89.0 BROWN SWBD ETT WSJ Average 67.0 79.9 73.9 73.9 82.0 Table 1: Cross-domain f-score performance of the Charniak (2000) parser. Averages are macro-averages. Performance drops as training and test domains diverge. On average, the WSJ model is the most accurate. resources from training to maximize accuracy across multiple target texts. Broadly put, we model how domain differences influence parsing accuracy. This is done by taking several computational measures of domain differences between the target text and each source domain. We use these features in a simple linear regression model which is trained to predict the accuracy of a parsing model (or, more generally, a mixture of parsing models) on a target text."
N10-1004,P07-1033,0,0.082164,"Missing"
N10-1004,D08-1072,0,0.0166981,"h a discussion and future work (Section 7). 2 Related work The closest work to ours is Plank and Sima’an (2008), where unlabeled text is used to group sentences from WSJ into subdomains. The authors create a model for each subdomain which weights trees from its subdomain more highly than others. Given the domain specific models, they consider different parse combination strategies. Unfortunately, these methods do not yield a statistically significant improvement. 29 Multiple source domain adaptation has been done for other tasks (e.g. classification in (Blitzer et al., 2007; Daum´e III, 2007; Dredze and Crammer, 2008)) and is related to multitask learning. Daum´e III (2007) shows that an extremely simple method delivers solid performance on a number of domain adaptation classification tasks. This is achieved by making a copy of each feature for each source domain plus the “general” pseudodomain (for capturing domain independent features). This allows the classifier to directly model which features are domain-specific. Finkel and Manning (2009) demonstrate the hierarchical Bayesian extension of this where domain-specific models draw from a general base distribution. This is applied to classification (named"
N10-1004,N09-1068,0,0.155731,"Missing"
N10-1004,foster-van-genabith-2008-parser,0,0.021802,"Missing"
N10-1004,W01-0521,0,0.460508,"non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language’s syntax. Recently, Daum´e III (2007) and Finkel and Manning (2009) showed techniques for training models that attempt to separate domainspecific and general properties. However, even when given models for multiple training domains, it is not stra"
N10-1004,I08-2097,0,0.0846298,"domain independent features). This allows the classifier to directly model which features are domain-specific. Finkel and Manning (2009) demonstrate the hierarchical Bayesian extension of this where domain-specific models draw from a general base distribution. This is applied to classification (named entity recognition) as well as dependency parsing. These works describe how to train models in many different domains but sidestep the problem of domain detection. Thus, our work is orthogonal to theirs. Our domain detection strategy draws on work in parser accuracy prediction (Ravi et al., 2008; Kawahara and Uchimoto, 2008). These works aim to predict the parser performance on a given target sentence. Ravi et al. (2008) frame this as a regression problem. Kawahara and Uchimoto (2008) treat it as a binary classification task and predict whether a specific parse is at a certain level of accuracy or higher. Ravi et al. (2008) show that their system can be used to return a ranking over different parsing models which we extend to the multiple domain setting. They also demonstrate that training their model on WSJ allows them to accurately predict parsing accuracy on the BROWN corpus. In contrast, our models are traine"
N10-1004,J93-2004,0,0.0529407,"-trained corpora as in McClosky et al. (2006a) which have been shown to work well across domains). Our final set includes text from news (WSJ, NANC), broadcast news (ETT), literature (BROWN, GUTENBERG), biomedical (GENIA, MEDLINE), spontaneous speech (SWBD), and the British National Corpus (BNC). In our experiments, self-trained corpora cannot be used as target domains since we lack gold annotations and BNC is not used as a source domain due to its size. An overview of our corpora is shown in Table 3. We use news articles portion of the Wall Street Journal corpus (WSJ) from the Penn Treebank (Marcus et al., 1993) in conjunction with the self-trained North American News Text Corpus (NANC, Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation6 of broadcast news in Arabic. For literature, we use the BROWN corpus (Francis and Kuˇcera, 1979) and the same division as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sentences which we downloaded from Project Gutenberg7 as a self-trained corpus. The Switchboard corpus (SWBD) consists of transcribed telephone conversations. While the original trees include disfluency information, we assume our spee"
N10-1004,N06-1020,1,0.915962,"quantitative measures of domain differences and how those differences affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations of parsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general pro"
N10-1004,P06-1043,1,0.961271,"quantitative measures of domain differences and how those differences affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations of parsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general pro"
N10-1004,plank-simaan-2008-subdomain,0,0.0501567,"Missing"
N10-1004,D08-1093,0,0.244766,"ain (for capturing domain independent features). This allows the classifier to directly model which features are domain-specific. Finkel and Manning (2009) demonstrate the hierarchical Bayesian extension of this where domain-specific models draw from a general base distribution. This is applied to classification (named entity recognition) as well as dependency parsing. These works describe how to train models in many different domains but sidestep the problem of domain detection. Thus, our work is orthogonal to theirs. Our domain detection strategy draws on work in parser accuracy prediction (Ravi et al., 2008; Kawahara and Uchimoto, 2008). These works aim to predict the parser performance on a given target sentence. Ravi et al. (2008) frame this as a regression problem. Kawahara and Uchimoto (2008) treat it as a binary classification task and predict whether a specific parse is at a certain level of accuracy or higher. Ravi et al. (2008) show that their system can be used to return a ranking over different parsing models which we extend to the multiple domain setting. They also demonstrate that training their model on WSJ allows them to accurately predict parsing accuracy on the BROWN corpus. In c"
N10-1004,E95-1020,0,0.0753869,"Missing"
N10-1004,A97-1015,0,0.185502,"utperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language’s syntax. Recently, Daum´e III (2007) and Finkel and Manning (2009) showed techniques for training models that attempt to separate domainspecific and general properties. However, even when given models for multiple training domains,"
N10-1004,D09-1058,0,0.0117461,"affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations of parsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language’s syntax. Recently, Daum´e III (2007) and Finke"
N10-1004,I05-2038,0,0.0361261,"he translation6 of broadcast news in Arabic. For literature, we use the BROWN corpus (Francis and Kuˇcera, 1979) and the same division as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sentences which we downloaded from Project Gutenberg7 as a self-trained corpus. The Switchboard corpus (SWBD) consists of transcribed telephone conversations. While the original trees include disfluency information, we assume our speech corpora have had speech repairs excised (e.g. using a system such as Johnson et al. (2004)). Our biomedical data comes from the GENIA treebank8 (Tateisi et al., 2005), a corpus of abstracts from the Medline database.9 We downloaded additional sentences 6 The transcription and translation were done by humans. http://gutenberg.org/ 8 http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/ 9 http://www.ncbi.nlm.nih.gov/PubMed/ 7 from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE. These differ on whether they were parsed using GENIA or WSJ as a base model to study the effect on cross-domain performance. Finally, we use a small number of sentences from the British National Corpus (BNC) (Foster an"
N10-1004,N07-1051,0,\N,Missing
N10-1004,J03-4003,0,\N,Missing
N12-1018,P07-1126,0,0.195483,"people, scenes, and events. Text processing is computationally less expensive than image processing and easily provides information that is difficult to learn visually. For this reason, most commerical image search websites identify the semantic content of images using co-occurring text exclusively. But co-occurring text is also a noisy source for candidate annotations, since not all of the text is visually relevant. Techniques from Natural Language Processing help align descriptive words and images. Some examples of previous research use named-entity recognition to identify people in images (Deschacht and Moens, 2007); term association to estimate the “visualness” of candidate annotations (Boiy et al., 2008; Leong et al., 2010); and topic models to annotate images given both visual and textual features (Feng and Lapata, 2010b). Image annotation using NLP is still an emerging area with many different tasks, datasets, and evaluation methods, making it impossible to compare many recent systems to each other. Although there is some effort being made towards establishing shared tasks1 , it is not yet clear which kinds of tasks and datasets will provide interesting research questions and practical applications i"
N12-1018,P08-1032,0,0.454159,"image search engines is a common practice in CV (Fei-Fei et al., 2004; Berg and Forsyth, 2006) and can also be used to provide more challenging and diverse instances of images and co-occurring text. The additional challenge for NLP is that text content on many websites is written to improve their rank in search engines, using techniques such as listing dozens of popular keywords. Co-occurring text for retrieved images on popular queries may not be representative of the task to be performed. 3 Datasets In this paper, we examine two established image annotation datasets: the BBC News Dataset of Feng and Lapata (2008) (henceforth referred to as BBC), and the general web dataset of Leong et al. (2010) (henceforth referred to as UNT). These datasets were both built to evaluate image annotation systems that use longer co-occurring text such as a news article or a webpage, but they use data from differDataset data instances source of data candidate keywords or collocations for annotation gold annotations evaluation metric number of train instances number of test instances preprocessing procedure average number of text tokens after preprocessing average document title length total vocabulary after preprocessing"
N12-1018,P10-1126,0,0.549578,"ebsites identify the semantic content of images using co-occurring text exclusively. But co-occurring text is also a noisy source for candidate annotations, since not all of the text is visually relevant. Techniques from Natural Language Processing help align descriptive words and images. Some examples of previous research use named-entity recognition to identify people in images (Deschacht and Moens, 2007); term association to estimate the “visualness” of candidate annotations (Boiy et al., 2008; Leong et al., 2010); and topic models to annotate images given both visual and textual features (Feng and Lapata, 2010b). Image annotation using NLP is still an emerging area with many different tasks, datasets, and evaluation methods, making it impossible to compare many recent systems to each other. Although there is some effort being made towards establishing shared tasks1 , it is not yet clear which kinds of tasks and datasets will provide interesting research questions and practical applications in the long term. Until then, establishing general “best practices” for NLP image annotation will help advance and legitimitize this work. In this paper, we propose some good practices and demonstrate why they ar"
N12-1018,N10-1125,0,0.559146,"ebsites identify the semantic content of images using co-occurring text exclusively. But co-occurring text is also a noisy source for candidate annotations, since not all of the text is visually relevant. Techniques from Natural Language Processing help align descriptive words and images. Some examples of previous research use named-entity recognition to identify people in images (Deschacht and Moens, 2007); term association to estimate the “visualness” of candidate annotations (Boiy et al., 2008; Leong et al., 2010); and topic models to annotate images given both visual and textual features (Feng and Lapata, 2010b). Image annotation using NLP is still an emerging area with many different tasks, datasets, and evaluation methods, making it impossible to compare many recent systems to each other. Although there is some effort being made towards establishing shared tasks1 , it is not yet clear which kinds of tasks and datasets will provide interesting research questions and practical applications in the long term. Until then, establishing general “best practices” for NLP image annotation will help advance and legitimitize this work. In this paper, we propose some good practices and demonstrate why they ar"
N12-1018,N09-1041,0,0.0288886,"l features (SIFT features quantized to discrete “image words” using k-means). A Latent Dirichlet Allocation topic model is trained on articles, images, and captions from the training set. Keywords are generated for an unseen image and article pair by estimating the distribution of topics that generates the test instance, then multiplying them with the word distributions in each topic to find the probability of textual keywords for the image. Text LDA is is the same model but only using words and not image features. 8 One could also think of this as a version of the KLSum summarization system (Haghighi and Vanderwende, 2009) that stops after one sentence. 176 5.3 Results The evaluation results for the BBC Dataset are shown in Table 2. Clearly, term frequency is a stronger baseline than tf*idf by a large margin. The reason for this is simple: since nearly all of BBC’s function words are removed during preprocessing, the only words downweighted by the idf score are common – but meaningful – words such as police or government. This is worth pointing out because, in many cases, the choice of using a term frequency or tf*idf baseline is made based on what was used in previous work. As we show here and in Section 6.3,"
N12-1018,C10-2074,0,0.168762,"information that is difficult to learn visually. For this reason, most commerical image search websites identify the semantic content of images using co-occurring text exclusively. But co-occurring text is also a noisy source for candidate annotations, since not all of the text is visually relevant. Techniques from Natural Language Processing help align descriptive words and images. Some examples of previous research use named-entity recognition to identify people in images (Deschacht and Moens, 2007); term association to estimate the “visualness” of candidate annotations (Boiy et al., 2008; Leong et al., 2010); and topic models to annotate images given both visual and textual features (Feng and Lapata, 2010b). Image annotation using NLP is still an emerging area with many different tasks, datasets, and evaluation methods, making it impossible to compare many recent systems to each other. Although there is some effort being made towards establishing shared tasks1 , it is not yet clear which kinds of tasks and datasets will provide interesting research questions and practical applications in the long term. Until then, establishing general “best practices” for NLP image annotation will help advance an"
N12-1018,S07-1009,0,0.0207608,"frequency in the text. Wikipedia Salience assigns scores to words based on a graph-based measure of importance that considers each term’s document frequency in Wikipedia. Pachinko Allocation Model is a topic model that captures correlations between topics (Li and McCallum, 2006). PAM infers subtopics and supertopics for the text, then retrieves top words from the top topics as annotations. There is also a combined model of these features using an SVM with 10-fold cross-validation. 6.2 Evaluation Evaluation on UNT uses a framework originally developed for the SemEval lexical substitution task (McCarthy and Navigli, 2007). This framework accounts for disagreement between annotators by weighting each generated keyword by the number of human annotators who also selected that keyword. The scoring framework consists of four evaluation measures: best normal, best mode, oot (out-of-ten) normal, and oot mode.11 The two best evaluations find the accuracy of a single “best” keyword generated by the system12 . 10 And as we stated earlier, the relative performance of term frequency vs tf*idf is different from dataset to dataset. 11 Both the original framework and its adaptation by Leong et al. (2010) give precision and r"
N12-1018,D11-1041,0,\N,Missing
N13-1009,brooke-hirst-2012-measuring,0,0.0810071,"roposed different feature sets to be used as predictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach, as it is crucial to any claims of feature relevance. Feature selection itself is a well studied problem, and the most thorough systems address both relevancy and redundancy. While some work tackles these problems by optimizing a metric over both simultaneously (Peng et al, 2005), we decouple the notions of relevancy and redundancy to allow ad-hoc metrics for either, similar to the method of Yu and Liu (2004). The measurement of feature relevancy in NLI has to this point been handled primarily with Informati"
N13-1009,C12-1027,0,0.261461,"edictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach, as it is crucial to any claims of feature relevance. Feature selection itself is a well studied problem, and the most thorough systems address both relevancy and redundancy. While some work tackles these problems by optimizing a metric over both simultaneously (Peng et al, 2005), we decouple the notions of relevancy and redundancy to allow ad-hoc metrics for either, similar to the method of Yu and Liu (2004). The measurement of feature relevancy in NLI has to this point been handled primarily with Information Gain, and elimination of feature redundancy h"
N13-1009,N09-1062,0,0.0668341,"Missing"
N13-1009,W07-0602,0,0.129934,"Missing"
N13-1009,P12-1046,0,0.0233789,"can express the generative model formally by defining the probability of a rule r expanding a symbol s in a sentence labeled η as D1 D2 L1 L2 1000 100 500 50 D1 D2 L1 L2 1000 750 500 750 Figure 2: Two hypothetical feature profiles that illustrate the problems with filtering only on data set independence, which prefers the right profile over the left. Our method has the opposite preference. θη ∼ Dir(νη ) ziη ∼ M ult(θη ) Hs ∼ DP (γ, P0 (•|s)) Gks ∼ DP (αs , Hs ) riηs ∼ Gziη s This is closely related to the application of the Hierarchical Pitman Yor Process used in (Blunsom and Cohn, 2010) and (Shindo et al, 2012), which interpolates between multiple coarse and fine mappings of the data items being clustered to deal with sparse data. While the underlying Chinese Restaurant Process sampling algorithm is quite similar, our approach differs in that it models several different distributions with the same support that share a common prior. By careful choice of the number of grammars K, the Dirichlet priors ν, and the backoff concentration parameter γ, a variety of interesting models can easily be defined, as demonstrated in our experiments. 5 5.1 Feature Selection Dataset Independence The first step in our"
N13-1009,P12-2038,1,0.884081,"s, avoid85 Proceedings of NAACL-HLT 2013, pages 85–94, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics ing redundancy and artifacts from the corpus creation process. We release this list for use by the linguistics and SLA research communities, and plan to expand it with upcoming releases of L1 labeled corpora1 . 2 Related Work Our work is closely related to the recent surge of research in NLI. Beginning with Koppel et al (2005), several papers have proposed different feature sets to be used as predictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach, as it is crucial to any clai"
N13-1009,C12-1158,0,0.150066,"e sets to be used as predictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach, as it is crucial to any claims of feature relevance. Feature selection itself is a well studied problem, and the most thorough systems address both relevancy and redundancy. While some work tackles these problems by optimizing a metric over both simultaneously (Peng et al, 2005), we decouple the notions of relevancy and redundancy to allow ad-hoc metrics for either, similar to the method of Yu and Liu (2004). The measurement of feature relevancy in NLI has to this point been handled primarily with Information Gain, and eliminatio"
N13-1009,U09-1008,0,0.0989314,"Missing"
N13-1009,D11-1148,0,0.450985,"native language labels, avoid85 Proceedings of NAACL-HLT 2013, pages 85–94, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics ing redundancy and artifacts from the corpus creation process. We release this list for use by the linguistics and SLA research communities, and plan to expand it with upcoming releases of L1 labeled corpora1 . 2 Related Work Our work is closely related to the recent surge of research in NLI. Beginning with Koppel et al (2005), several papers have proposed different feature sets to be used as predictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach"
N13-1009,U11-1015,0,0.0630047,"native language labels, avoid85 Proceedings of NAACL-HLT 2013, pages 85–94, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics ing redundancy and artifacts from the corpus creation process. We release this list for use by the linguistics and SLA research communities, and plan to expand it with upcoming releases of L1 labeled corpora1 . 2 Related Work Our work is closely related to the recent surge of research in NLI. Beginning with Koppel et al (2005), several papers have proposed different feature sets to be used as predictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach"
N13-1009,D12-1064,0,0.098465,"Missing"
N13-1009,P10-2042,0,\N,Missing
N13-1009,D10-1117,0,\N,Missing
N13-1009,P06-1055,0,\N,Missing
N13-1009,P09-2012,0,\N,Missing
N13-1009,C12-1025,0,\N,Missing
N15-1008,D11-1024,0,0.0496823,"ing citations. In turn, these topics are arguably the most cohesive; thus, our system, as a byproduct, provides a metric for measuring the “quality” of each topic. Namely, the weight associated with each topic feature indicates the topic’s importance – the lower the weight the better. Table 6 shows our system’s ranking of the most important topics, signified by “Logit-weight.” We did not prompt humans to evaluate the quality of the topics, so in attempt to offer a comparison, we also rank each topic according to two popular metrics: Pointwise Mutual Information (PMI) and Topic Coherence (TC) (Mimno et al., 2011). For a topic k, let V (k) represent the top M words for K; where (k) (k) V (k) = (vi , ..., vM ) and D(v) represents the document frequency of word type v. Then, P M I(k) is defined by Equation 4 and T C(k) is defined by Equation 5. In Table 6, we see that our most useful topic (Topic 49) concerns vision research, and since our corpus is heavily filled with research concerning (non-vision-related) natural language processing, it makes sense for this topic to be highly important for predicting citations. Similarly, we see the other topranking topics all represent a well-defined, subfield of na"
P01-1017,P01-1010,0,0.0252467,"Missing"
P01-1017,P95-1037,0,0.31156,"Missing"
P01-1017,J93-2004,0,0.0601359,"ion of conditioning events2 As with [3], a subset of parses is computed with a non-lexicalized PCFG, and the most probable edges (using an empirically established threshold) have their probabilities recomputed according to the complete probability model of Equation 5. Both searches are conducted using dynamic programming. 4 Experiments 4.1 The Immediate-Bihead Language Model The parser as described in the previous section was trained and tested on the data used in the previously described grammar-based language modeling research [4,15]. This data is from the Penn Wall Street Journal tree-bank [13], but modified to make the text more “speech-like”. In particular: 1. all punctuation is removed, 2. no capitalization is used, 3. all symbols and digits are replaced by the symbol N, and 2 They should sum to one. We are just checking that there are no bugs in the code. Model C&J Roark Bihead Trigram 167.14 167.02 167.89 Perplexity Grammar Interpolation 158.28 148.90 152.26 137.26 144.98 133.15 Table 2: Perplexity results for the immediatebihead model 4. all words except for the 10,000 most common are replaced by the symbol UNK. As in previous work, files F0 to F20 are used for training, F21-F"
P01-1017,A00-2018,1,\N,Missing
P01-1017,J98-2005,0,\N,Missing
P01-1017,J95-2002,0,\N,Missing
P01-1017,P97-1003,0,\N,Missing
P01-1017,J03-4003,0,\N,Missing
P01-1017,P94-1011,0,\N,Missing
P01-1017,P98-1035,0,\N,Missing
P01-1017,C98-1035,0,\N,Missing
P01-1017,J01-2004,0,\N,Missing
P01-1017,P95-1007,0,\N,Missing
P02-1026,A00-2018,1,\N,Missing
P02-1026,J93-2004,0,\N,Missing
P04-1005,N01-1016,1,0.447866,"antage over other kinds of models that they can in principle be integrated with other probabilistic models to produce a combined model that uses all available evidence to select the globally optimal analysis. Shriberg and Stolcke (1998) studied the location and distribution of repairs in the Switchboard corpus, but did not propose an actual model of repairs. Heeman and Allen (1999) describe a noisy channel model of speech repairs, but leave “extending the model to incorporate higher level syntactic . . . processing” to future work. The previous work most closely related to the current work is Charniak and Johnson (2001), who used a boosted decision stub classifier to classify words as edited or not on a word Eugene Charniak Brown University Providence, RI 02912 ec@cs.brown.edu by word basis, but do not identify or assign a probability to a repair as a whole. There are two innovations in this paper. First, we demonstrate that using a syntactic parser-based language model Charniak (2001) instead of bi/trigram language models significantly improves the accuracy of repair detection and correction. Second, we show how Tree Adjoining Grammars (TAGs) can be used to provide a precise formal description and probabili"
P04-1005,P01-1017,1,0.898728,"describe a noisy channel model of speech repairs, but leave “extending the model to incorporate higher level syntactic . . . processing” to future work. The previous work most closely related to the current work is Charniak and Johnson (2001), who used a boosted decision stub classifier to classify words as edited or not on a word Eugene Charniak Brown University Providence, RI 02912 ec@cs.brown.edu by word basis, but do not identify or assign a probability to a repair as a whole. There are two innovations in this paper. First, we demonstrate that using a syntactic parser-based language model Charniak (2001) instead of bi/trigram language models significantly improves the accuracy of repair detection and correction. Second, we show how Tree Adjoining Grammars (TAGs) can be used to provide a precise formal description and probabilistic model of the crossed dependencies occurring in speech repairs. The rest of this paper is structured as follows. The next section describes the noisy channel model of speech repairs and the section after that explains how it can be applied to detect and repair speech repairs. Section 4 evaluates this model on the Penn 3 disfluency-tagged Switchboard corpus, and secti"
P04-1005,C90-3045,0,0.235708,"age model. On the other hand, it seems desirable to use a language model that is sensitive to more global properties of the sentence, and we do this by reranking the initial analysis, replacing the bigram language model with a syntactic parser based model. We do not need to intersect this parser based language model with our TAG channel model since we evaluate each analysis separately. 2.2 The TAG channel model The TAG channel model defines a stochastic mapping of source sentences X into observed sentences Y . There are several ways to define transducers using TAGs such as Shieber and Schabes (1990), but the following simple method, inspired by finite-state transducers, suffices for the application here. The TAG defines a language whose vocabulary is the set of pairs (Σ∪{∅})×(Σ∪{∅}), where Σ is the vocabulary of the observed sentences Y . A string Z in this language can be interpreted as a pair of strings (Y, X), where Y is the concatenation of the projection of the first components of Z and X is the concatenation of the projection of the second components. For example, the string Z = a:a flight:flight to:∅ Boston:∅ uh:∅ I:∅ mean:∅ to:to Denver:Denver on:on Friday:Friday corresponds to t"
P04-1005,J99-4003,0,\N,Missing
P04-1005,W90-0102,0,\N,Missing
P05-1022,W97-0302,0,0.0593829,"ming states according to) features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more. Thus nobody has implemented this version. There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a “coarse-to-fine” parser. By “coarse-to-fine” we mean that it first produces a crude version of the parse using coarse-grained dynamic programming states, and then builds fine-grained analyses by splitting the most promising of coarse-grained states. A prime example of this idea is from Goodman (1997), who describes a method for producing a simple but crude approximate grammar of a standard context-free grammar. He parses a sentence using the approximate grammar, and the results are used to constrain the search for a parse with the full CFG. He finds that total parsing time is greatly reduced. A somewhat different take on this paradigm is seen in the parser we use in this paper. Here the parser first creates a parse forest based upon a much less complex version of the complete grammar. In particular, it only looks at standard CFG features, the parent and neighbor labels. Because this gramm"
P05-1022,W05-1506,0,0.736476,"mal decision must be the second-best choice at that choice point. Further, the nth-best parse can only involve at most n suboptimal parsing decisions, and all but one of these must be involved in one of the second through the n − 1th-best parses. Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on. The algorithm was originally described for hidden Markov models. Since this first draft of this paper we have become aware of two PCFG implementations of this algorithm (Jimenez and Marzal, 2000; Huang and Chang, 2005). The first was tried on relatively small grammars, while the second was implemented on top of the Bikel re-implementation of the Collins parser (Bikel, 2004) and achieved oracle results for 50-best parses similar to those we report below. Here, however, we describe how to find n-best parses in a more straight-forward fashion. Rather than storing a single best parse of each edge, one stores n of them. That is, when using dynamic programming, rather than throwing away a candidate if it scores less than the best, one keeps it if it is one of the top n analyses for this edge discovered so far. Th"
P05-1022,J04-4004,0,0.0724418,"f these must be involved in one of the second through the n − 1th-best parses. Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on. The algorithm was originally described for hidden Markov models. Since this first draft of this paper we have become aware of two PCFG implementations of this algorithm (Jimenez and Marzal, 2000; Huang and Chang, 2005). The first was tried on relatively small grammars, while the second was implemented on top of the Bikel re-implementation of the Collins parser (Bikel, 2004) and achieved oracle results for 50-best parses similar to those we report below. Here, however, we describe how to find n-best parses in a more straight-forward fashion. Rather than storing a single best parse of each edge, one stores n of them. That is, when using dynamic programming, rather than throwing away a candidate if it scores less than the best, one keeps it if it is one of the top n analyses for this edge discovered so far. This is really very straight-forward. The problem is space. Dynamic programming parsing algorithms for PCFGs require O(m2 ) dynamic programming states, where m"
P05-1022,E03-1005,0,0.0193579,"Missing"
P05-1022,A00-2018,1,0.285717,"of the top n analyses for this edge discovered so far. This is really very straight-forward. The problem is space. Dynamic programming parsing algorithms for PCFGs require O(m2 ) dynamic programming states, where m is the length of the sentence, so an n-best parsing algorithm requires O(nm2 ). However things get much worse when the grammar is bilexicalized. As shown by Eisner (Eisner and Satta, 1999) the dynamic programming algorithms for bilexicalized PCFGs require O(m3 ) states, so a n-best parser would require O(nm3 ) states. Things become worse still in a parser like the one described in Charniak (2000) because it conditions on (and hence splits the dynamic programming states according to) features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more. Thus nobody has implemented this version. There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a “coarse-to-fine” parser. By “coarse-to-fine” we mean that it first produces a crude version of the parse using coarse-grained dynamic programming states, and then builds fine-grained analyses by splitting the most promising"
P05-1022,P97-1003,0,0.194461,"-best parsing is straight-forward in best-first search or beam search approaches that do not use dynamic programming: to generate more than one parse, one simply allows the search mechanism to create successive versions to one’s heart’s content. A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning. At the end one has a beam-width’s number of best parses (Roark, 2001). The Collins parser (Collins, 1997) does use dynamic programming in its search. That is, whenever a constituent with the same history is generated a second time, it is discarded if its probability is lower than the original version. If the opposite is true, then the original is discarded. This is fine if one only 174 wants the first-best, but obviously it does not directly enumerate the n-best parses. However, Collins (Collins, 2000; Collins and Koo, in submission) has created an nbest version of his parser by turning off dynamic programming (see the user’s guide to Bikel’s re-implementation of Collins’ parser, http://www.cis.u"
P05-1022,P99-1059,0,0.620531,"hion. Rather than storing a single best parse of each edge, one stores n of them. That is, when using dynamic programming, rather than throwing away a candidate if it scores less than the best, one keeps it if it is one of the top n analyses for this edge discovered so far. This is really very straight-forward. The problem is space. Dynamic programming parsing algorithms for PCFGs require O(m2 ) dynamic programming states, where m is the length of the sentence, so an n-best parsing algorithm requires O(nm2 ). However things get much worse when the grammar is bilexicalized. As shown by Eisner (Eisner and Satta, 1999) the dynamic programming algorithms for bilexicalized PCFGs require O(m3 ) states, so a n-best parser would require O(nm3 ) states. Things become worse still in a parser like the one described in Charniak (2000) because it conditions on (and hence splits the dynamic programming states according to) features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more. Thus nobody has implemented this version. There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a “coarse-to-fi"
P05-1022,P99-1069,1,0.0726976,"Missing"
P05-1022,P03-1054,0,0.178434,"s of speech of the lexical head and the functional head of nodes in parse trees. WProj (158,771) The instances of this schema are preterminals together with the categories of ` of their closest maximal projection ancestors. The parameters of this schema control the number ` of maximal projections, and whether the preterminals and the ancestors are lexicalized. 178 Word (49,097) The instances of this schema are lexical items together with the categories of ` of their immediate ancestor nodes, where ` is a schema parameter (` = 2 or ` = 3 here). This feature was inspired by a similar feature in Klein and Manning (2003). HeadTree (72,171) The instances of this schema are tree fragments consisting of the local trees consisting of the projections of a preterminal node and the siblings of such projections. This schema is parameterized by the head type (lexical or functional) used to determine the projections of a preterminal, and whether the head preterminal is lexicalized. NGramTree (291,909) The instances of this schema are subtrees rooted in the least common ancestor of ` contiguous preterminal nodes. This schema is parameterized by the number ` of contiguous preterminals (` = 2 or ` = 3 here) and whether th"
P05-1022,W02-2018,0,0.0557284,"he-art quality, but the reranker further improves the f -score. f (y)Pθ (y|Y) y∈Y 6 In the experiments reported here, we used a GausP 2 sian or quadratic regularizer R(w) = c m j=1 wj , where c is an adjustable parameter that controls the amount of regularization, chosen to optimize the reranker’s f -score on the development set (section 24 of the treebank). We used the Limited Memory Variable Metric optimization algorithm from the PETSc/TAO optimization toolkit (Benson et al., 2004) to find the optimal feature weights θˆ because this method seems substantially faster than comparable methods (Malouf, 2002). The PETSc/TAO toolkit provides a variety of other optimization algorithms and flags for controlling convergence, but preliminary experiments on the Collins’ trees with different algorithms and early stopping did not show any performance improvements, so we used the default PETSc/TAO setting for our experiments here. 5 f -score 0.9102 0.9037 Experimental results We evaluated the performance of our reranking parser using the standard PARSEVAL metrics. We 179 Conclusion This paper has described a dynamic programming n-best parsing algorithm that utilizes a heuristic coarse-to-fine refinement of"
P05-1022,P02-1035,1,0.123862,"heir correct parses y? (s1 ), . . . , y? (sn ). We used the 20-fold crossvalidation technique described in Collins (2000) to compute the n-best parses Y(s) for each sentence s in D. In general the correct parse y? (s) is not a member of Y(s), so instead we train the reranker to identify one of the best parses Y+ (s) = arg maxy∈Y(s) Fy? (s) (y) in the n-best parser’s output, where Fy? (y) is the Parseval f -score of y evaluated with respect to y? . Because there may not be a unique best parse for each sentence (i.e., |Y+ (s) |> 1 for some sentences s) we used the variant of MaxEnt described in Riezler et al. (2002) for partially labelled training data. n-best trees New Collins Recall the standard MaxEnt conditional probability model for a parse y ∈ Y: exp vθ (y) , where 0 y 0 ∈Y exp vθ (y ) P Pθ (y|Y) = vθ (y) = θ · f (y) = m X θj fj (y). j=1 The loss function LD proposed in Riezler et al. (2002) is just the negative log conditional likelihood of the best parses Y+ (s) relative to the n-best parser output Y(s): n X 0 LD (θ) = − log Pθ (Y+ (si )|Y(si )), where i=1 Pθ (Y+ |Y) = X Pθ (y|Y) y∈Y+ The partial derivatives of this loss function, which are required by the numerical estimation procedure, are: ∂LD"
P05-1022,J01-2004,0,0.463729,"θ that maximizes the score vθ (y) of the parses y ∈ Y+ (s) relative to the scores of the other parses in Y(s), for each s in the training data. 2 Recovering the n-best parses using coarse-to-fine parsing The major difficulty in n-best parsing, compared to 1-best parsing, is dynamic programming. For example, n-best parsing is straight-forward in best-first search or beam search approaches that do not use dynamic programming: to generate more than one parse, one simply allows the search mechanism to create successive versions to one’s heart’s content. A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning. At the end one has a beam-width’s number of best parses (Roark, 2001). The Collins parser (Collins, 1997) does use dynamic programming in its search. That is, whenever a constituent with the same history is generated a second time, it is discarded if its probability is lower than the original version. If the opposite is true, then the original is discarded. This is fine if one only 174 want"
P05-1036,A00-2031,1,0.441403,"DT JJ NN) = 1. To determine this, you divide the count of NP → DT JJ NN = 3 by all the possible long versions of NP → DT JJ NN = 3. P(NP → DT JJ NN |NP → DT NN) = 3/7. The count of NP → DT JJ NN = 3, and the possible long 292 Even with the unsupervised constraint from section 3, the fact that we have artificially created our joint rules gives us some fairly ungrammatical compressions. Adding extra constraints improves our unsupervised compressions, and gives us better performance on the supervised version as well. We use a program to label syntactic arguments with the roles they are playing (Blaheta and Charniak, 2000), and the rules for complement/adjunct distinction given by (Collins, 1997) to never allow deletion of the complement. Since many nodes that should not be deleted are not labeled with their syntactic role, we add another constraint that disallows deletion of NPs. 6 Evaluation As with Knight and Marcu’s (2000) original work, we use the same 32 sentence pairs as our Test Corpus, leaving us with 1035 training pairs. After adjusting the supervised weighting parameter, we fold the development set back into the training data. We presented four judges with nine compressed versions of each of the 32 l"
P05-1036,P01-1017,1,0.685556,"o expansion probability according to the training data is instead assigned a very small probability. A tree extractor (Langkilde, 2000) collects the short sentences with the highest score for P (s |l). 2.2 Our Noisy-Channel Model Our starting implementation is intended to follow the K&M model fairly closely. We use the same 1067 pairs of sentences from the Ziff-Davis corpus, with 32 used as testing and the rest as training. The main difference between their model and ours is that instead of using the rather ad-hoc K&M language model, we substitute the syntax-based language model described in (Charniak, 2001). We slightly modify the channel model equation to be P (l |s) = Pexpand (l |s)Pdeleted , where Pdeleted is the probability of adding the deleted subtrees back into s to get l. We determine this probability also using the Charniak language model. We require an extra parameter to encourage compression. We create a development corpus of 25 sentences from the training data in order to adjust this parameter. That we require a parameter to encourage compression is odd as K&M required a parameter to discourage compression, but we address this point in the penultimate section. Another difference is t"
P05-1036,P97-1003,0,0.041768,"possible long versions of NP → DT JJ NN = 3. P(NP → DT JJ NN |NP → DT NN) = 3/7. The count of NP → DT JJ NN = 3, and the possible long 292 Even with the unsupervised constraint from section 3, the fact that we have artificially created our joint rules gives us some fairly ungrammatical compressions. Adding extra constraints improves our unsupervised compressions, and gives us better performance on the supervised version as well. We use a program to label syntactic arguments with the roles they are playing (Blaheta and Charniak, 2000), and the rules for complement/adjunct distinction given by (Collins, 1997) to never allow deletion of the complement. Since many nodes that should not be deleted are not labeled with their syntactic role, we add another constraint that disallows deletion of NPs. 6 Evaluation As with Knight and Marcu’s (2000) original work, we use the same 32 sentence pairs as our Test Corpus, leaving us with 1035 training pairs. After adjusting the supervised weighting parameter, we fold the development set back into the training data. We presented four judges with nine compressed versions of each of the 32 long sentences: A humangenerated short version, the K&M version, our first s"
P05-1036,A00-2023,0,0.0158278,"ount(joint(l, s)) is the count of alignments of the long rule and the short. Many compressions do not align exactly. Sometimes the parses do not match, and sometimes there are deletions that are too complex to be modeled in this way. In these cases sentence pairs, or sections of them, are ignored. The K&M model creates a packed parse forest of all possible compressions that are grammatical with respect to the Penn Treebank (Marcus et al., 1993). 291 Any compression given a zero expansion probability according to the training data is instead assigned a very small probability. A tree extractor (Langkilde, 2000) collects the short sentences with the highest score for P (s |l). 2.2 Our Noisy-Channel Model Our starting implementation is intended to follow the K&M model fairly closely. We use the same 1067 pairs of sentences from the Ziff-Davis corpus, with 32 used as testing and the rest as training. The main difference between their model and ours is that instead of using the rather ad-hoc K&M language model, we substitute the syntax-based language model described in (Charniak, 2001). We slightly modify the channel model equation to be P (l |s) = Pexpand (l |s)Pdeleted , where Pdeleted is the probabil"
P05-1036,J93-2004,0,0.0255481,"on is the same as the short, are also counted. The expansion probability, as used in the channel model, is given by Pexpand (l |s) = count(joint(l, s)) count(s) (2) where count(joint(l, s)) is the count of alignments of the long rule and the short. Many compressions do not align exactly. Sometimes the parses do not match, and sometimes there are deletions that are too complex to be modeled in this way. In these cases sentence pairs, or sections of them, are ignored. The K&M model creates a packed parse forest of all possible compressions that are grammatical with respect to the Penn Treebank (Marcus et al., 1993). 291 Any compression given a zero expansion probability according to the training data is instead assigned a very small probability. A tree extractor (Langkilde, 2000) collects the short sentences with the highest score for P (s |l). 2.2 Our Noisy-Channel Model Our starting implementation is intended to follow the K&M model fairly closely. We use the same 1067 pairs of sentences from the Ziff-Davis corpus, with 32 used as testing and the rest as training. The main difference between their model and ours is that instead of using the rather ad-hoc K&M language model, we substitute the syntax-ba"
P05-1036,P99-1072,0,\N,Missing
P06-1043,W01-0521,0,0.742981,"lone WSJ+2,500k NANC B ROWN alone B ROWN+50k NANC B ROWN+250k NANC B ROWN+500k NANC WSJ+B ROWN WSJ+B ROWN +50k NANC WSJ+B ROWN +250k NANC WSJ+B ROWN +500k NANC Parser alone 83.9 86.4 86.3 86.8 86.8 86.7 86.5 86.8 86.8 86.6 Reranking parser 85.8 87.7 87.4 88.0 88.1 87.8 88.1 88.1 88.1 87.7 Table 3: f -scores from various combinations of WSJ, NANC, and B ROWN corpora on B ROWN development. The reranking parser used the WSJ-trained reranker model. The B ROWN parsing model is naturally better than the WSJ model for this task, but combining the two training corpora results in a better model (as in Gildea (2001)). Adding small amounts of NANC further improves the models. Parser model WSJ WSJ+NANC B ROWN Parser alone 82.9 87.1 86.7 WSJ-reranker 85.2 87.8 88.2 B ROWN-reranker 85.2 87.9 88.4 Table 5: Performance of various combinations of parser and reranker models when evaluated on B ROWN test. The WSJ+NANC parser with the WSJ reranker comes close to the B ROWN-trained reranking parser. The B ROWN reranker provides only a small improvement over its WSJ counterpart, which is not statistically significant. 342 Bracketing agreement f -score Complete match Average crossing brackets POS Tagging agreement 88"
P06-1043,P99-1010,0,0.0140006,"Missing"
P06-1043,I05-1006,1,0.53592,"a different class, and data from a different domain. He also notes that different domains have very different structures by looking at frequent grammar productions. For these reasons he takes the position that we should, instead, simply create treebanks for a large number of domains. While this is a coherent position, it is far from the majority view. There are many different approaches to parser adaptation. Steedman et al. (2003) apply cotraining to parser adaptation and find that cotraining can work across domains. The need to parse biomedical literature inspires (Clegg and Shepherd, 2005; Lease and Charniak, 2005). Clegg and Shepherd (2005) provide an extensive side-by-side performance analysis of several modern statistical parsers when faced with such data. They find that techniques which combine differ338 3 Corpora cleanups on NANC to ease parsing. NANC contains news articles from various news sources including the Wall Street Journal, though for this paper, we only use articles from the LA Times portion. To use the data from NANC, we use self-training (McClosky et al., 2006). First, we take a WSJ trained reranking parser (i.e. both the parser and reranker are built from WSJ training data) and parse"
P06-1043,J93-2004,0,0.0335602,"Missing"
P06-1043,P05-1022,1,0.281883,"s treebank. Furthermore, the treebanked Brown data is mostly general non-fiction and much closer to WSJ than, e.g., medical corpora would be. Thus, most work on parser adaptation resorts to using some labeled in-domain data to fortify the larger quantity of outof-domain data. In this paper, we present some encouraging results on parser adaptation without any in-domain data. (Though we also present results with indomain data as a reference point.) In particular we note the effects of two comparatively recent techniques for parser improvement. The first of these, parse-reranking (Collins, 2000; Charniak and Johnson, 2005) starts with a “standard” generative parser, but uses it to generate the n-best parses rather than a single parse. Then a reranking phase uses more detailed features, features which would (mostly) be impossible to incorporate in the initial phase, to reorder Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years. Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typically) the WSJ treebank data. This has led to concern that such parsers may be too finely tun"
P06-1043,W05-1102,0,0.0458518,"the same class, data from a different class, and data from a different domain. He also notes that different domains have very different structures by looking at frequent grammar productions. For these reasons he takes the position that we should, instead, simply create treebanks for a large number of domains. While this is a coherent position, it is far from the majority view. There are many different approaches to parser adaptation. Steedman et al. (2003) apply cotraining to parser adaptation and find that cotraining can work across domains. The need to parse biomedical literature inspires (Clegg and Shepherd, 2005; Lease and Charniak, 2005). Clegg and Shepherd (2005) provide an extensive side-by-side performance analysis of several modern statistical parsers when faced with such data. They find that techniques which combine differ338 3 Corpora cleanups on NANC to ease parsing. NANC contains news articles from various news sources including the Wall Street Journal, though for this paper, we only use articles from the LA Times portion. To use the data from NANC, we use self-training (McClosky et al., 2006). First, we take a WSJ trained reranking parser (i.e. both the parser and reranker are built from WS"
P06-1043,N06-1020,1,0.773237,"d find that cotraining can work across domains. The need to parse biomedical literature inspires (Clegg and Shepherd, 2005; Lease and Charniak, 2005). Clegg and Shepherd (2005) provide an extensive side-by-side performance analysis of several modern statistical parsers when faced with such data. They find that techniques which combine differ338 3 Corpora cleanups on NANC to ease parsing. NANC contains news articles from various news sources including the Wall Street Journal, though for this paper, we only use articles from the LA Times portion. To use the data from NANC, we use self-training (McClosky et al., 2006). First, we take a WSJ trained reranking parser (i.e. both the parser and reranker are built from WSJ training data) and parse the sentences from NANC with the 50-best (Charniak and Johnson, 2005) parser. Next, the 50-best parses are reordered by the reranker. Finally, the 1-best parses after reranking are combined with the WSJ training set to retrain the firststage parser.1 McClosky et al. (2006) find that the self-trained models help considerably when parsing WSJ. We primarily use three corpora in this paper. Selftraining requires labeled and unlabeled data. We assume that these sets of data"
P06-1043,A97-1015,0,0.114939,"se a “model-merging” (Bacchiani et al., 2006) approach. The different corpora are, in effect, concatenated together. However, (Bacchiani et al., 2006) achieve a larger gain by weighting the in-domain (Brown) data more heavily than the out-of-domain WSJ data. One can imagine, for instance, five copies of the Brown data concatenated with just one copy of WSJ data. 2 Related Work Work in parser adaptation is premised on the assumption that one wants a single parser that can handle a wide variety of domains. While this is the goal of the majority of parsing researchers, it is not quite universal. Sekine (1997) observes that for parsing a specific domain, data from that domain is most beneficial, followed by data from the same class, data from a different class, and data from a different domain. He also notes that different domains have very different structures by looking at frequent grammar productions. For these reasons he takes the position that we should, instead, simply create treebanks for a large number of domains. While this is a coherent position, it is far from the majority view. There are many different approaches to parser adaptation. Steedman et al. (2003) apply cotraining to parser ad"
P06-1043,E03-1008,0,0.736667,"esearchers, it is not quite universal. Sekine (1997) observes that for parsing a specific domain, data from that domain is most beneficial, followed by data from the same class, data from a different class, and data from a different domain. He also notes that different domains have very different structures by looking at frequent grammar productions. For these reasons he takes the position that we should, instead, simply create treebanks for a large number of domains. While this is a coherent position, it is far from the majority view. There are many different approaches to parser adaptation. Steedman et al. (2003) apply cotraining to parser adaptation and find that cotraining can work across domains. The need to parse biomedical literature inspires (Clegg and Shepherd, 2005; Lease and Charniak, 2005). Clegg and Shepherd (2005) provide an extensive side-by-side performance analysis of several modern statistical parsers when faced with such data. They find that techniques which combine differ338 3 Corpora cleanups on NANC to ease parsing. NANC contains news articles from various news sources including the Wall Street Journal, though for this paper, we only use articles from the LA Times portion. To use t"
P08-1095,P03-1071,0,0.0175328,"Missing"
P08-1095,P06-1004,0,0.0308436,"Missing"
P08-1095,W04-2401,0,0.013538,"g makes at least a reasonable starting point for further efforts, since it is a natural online algorithm– it assigns each utterance as it arrives, without reference to the future. At any rate, we should not take our objective function too seriously. Although it is roughly correlated with performance, the high error rate of the classifier makes it unlikely that small changes in objective will mean much. In fact, the objective value of our output solutions are generally higher than those for true so8 We set up the problem by taking the weight of edge i, j as the classifier’s decision pi,j − .5. Roth and Yih (2004) use log probabilities as weights. Bansal et al. (2004) propose the log odds ratio log(p/(1 − p)). We are unsure of the relative merit of these approaches. 840 lutions, which implies we have already reached the limits of what our classifier can tell us. 5 Experiments We annotate the 800 line test transcript using our system. The annotation obtained has 63 conversations, with mean length 12.70. The average density of conversations is 2.9, and the entropy is 3.79. This places it within the bounds of our human annotations (see table 1), toward the more general end of the spectrum. As a standard o"
P08-2011,P05-1018,0,0.477531,"ble, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which probabilistically attempts to describe these preferences (Ge et al., 1998). These two models can be combined with the entity grid described by Lapata and Barzilay (2005) for significant improvement. The magnitude of the improvement is particularly interesting given that Barzilay and Lapata (2005) do use a coreference system but are unable to derive much advantage from it. 2 Discourse-new Model In the task of discourse-new classification, the model is given a referring expression (as in previous work, we consider only NPs) from a document and must 41 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics determine whether it is a first mention (discoursenew) or a subsequent mention (discourse-old). Features such as full names, appositives, and restrictive relative clauses are associated"
P08-2011,N04-1015,0,0.16387,"Missing"
P08-2011,P99-1048,0,0.0174872,"Missing"
P08-2011,D07-1009,0,0.2172,"un coreference algorithm to the information ordering task, and show significant improvements in performance over the entity grid, a popular model of local coherence. 1 Introduction Models of discourse coherence describe the relationships between nearby sentences, in which previous sentences help make their successors easier to understand. Models of coherence have been used to impose an order on sentences for multidocument summarization (Barzilay et al., 2002), to evaluate the quality of human-authored essays (Miltsakaki and Kukich, 2004), and to insert new information into existing documents (Chen et al., 2007). These models typically view a sentence either as a bag of words (Foltz et al., 1998) or as a bag of entities associated with various syntactic roles (Lapata and Barzilay, 2005). However, a mention of an entity contains more information than just its head and syntactic role. The referring expression itself contains discourse-motivated information distinguishing familiar entities from unfamiliar and salient from non-salient. These patterns have been studied extensively, by linguists (Prince, 1981; Fraurud, 1990) and in the field of coreference resolution. We draw on the coreference work, takin"
P08-2011,W98-1119,1,0.640917,"been previously mentioned in the discourse. As noted by studies since Hawkins (1978), there are marked syntactic differences between the two classes. Our second model describes pronoun coreference. To be intelligible, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which probabilistically attempts to describe these preferences (Ge et al., 1998). These two models can be combined with the entity grid described by Lapata and Barzilay (2005) for significant improvement. The magnitude of the improvement is particularly interesting given that Barzilay and Lapata (2005) do use a coreference system but are unable to derive much advantage from it. 2 Discourse-new Model In the task of discourse-new classification, the model is given a referring expression (as in previous work, we consider only NPs) from a document and must 41 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, c Columbus, Ohio, USA, June 2008. 2008 Assoc"
P08-2011,J95-2003,0,0.82439,"ference work, taking two standard models from the literature and applying them to coherence modeling. Our first model distinguishes discourse-new from discourse-old noun phrases, using features based on Uryupina (2003). Discourse-new NPs are those whose referents have not been previously mentioned in the discourse. As noted by studies since Hawkins (1978), there are marked syntactic differences between the two classes. Our second model describes pronoun coreference. To be intelligible, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which probabilistically attempts to describe these preferences (Ge et al., 1998). These two models can be combined with the entity grid described by Lapata and Barzilay (2005) for significant improvement. The magnitude of the improvement is particularly interesting given that Barzilay and Lapata (2005) do use a coreference system but are unable to derive much advantage from it. 2 Discourse-new Model In the"
P08-2011,J06-4002,0,0.0598224,"cu (2006). The entity grid represents each entity by tracking the syntactic roles in which it appears throughout the document. The internal syntax of the various referring expressions is ignored. Since it also uses the “same head” coreference heuristic, it also disregards pronouns. Since the three models use very different feature sets, we combine them by assuming independence and multiplying the probabilities. 5 Experiments We evaluate our models using two tasks, both based on the assumption that a human-authored document is coherent, and uses the best possible ordering of its sentences (see Lapata (2006)). In the discrimination task (Barzilay and Lapata, 2005), a document is compared with a random permutation of its sentences, and we score the system correct if it indicates the original as more coherent4 . 4 Since the model might refuse to make a decision by scoring a permutation the same as the original, we also report F-score, where precision is correct/decisions and recall is correct/total. 43 Discrimination becomes easier for longer documents, since a random permutation is likely to be much less similar to the original. Therefore we also test our systems on the task of insertion (Chen et"
P08-2011,N03-2024,0,0.306405,"Missing"
P08-2011,C02-1139,0,0.0558178,"Missing"
P08-2011,J98-2001,0,0.0370675,"Missing"
P08-2011,P06-2103,0,0.705054,"004 WSJ documents. Finding the probability of a document using this model requires us to sum out the antecedents a. Unfortunately, because each ai is conditioned on the previous ones, this cannot be done efficiently. Instead, we use a greedy search, assigning each pronoun left to right. Finally we report the probability of the resulting sequence of pronoun assignments. 4 Baseline Model As a baseline, we adopt the entity grid (Lapata and Barzilay, 2005). This model outperforms a variety of word overlap and semantic similarity models, and is used as a component in the state-of-the-art system of Soricut and Marcu (2006). The entity grid represents each entity by tracking the syntactic roles in which it appears throughout the document. The internal syntax of the various referring expressions is ignored. Since it also uses the “same head” coreference heuristic, it also disregards pronouns. Since the three models use very different feature sets, we combine them by assuming independence and multiplying the probabilities. 5 Experiments We evaluate our models using two tasks, both based on the assumption that a human-authored document is coherent, and uses the best possible ordering of its sentences (see Lapata (2"
P08-2011,P03-2012,0,0.0165502,"ver, a mention of an entity contains more information than just its head and syntactic role. The referring expression itself contains discourse-motivated information distinguishing familiar entities from unfamiliar and salient from non-salient. These patterns have been studied extensively, by linguists (Prince, 1981; Fraurud, 1990) and in the field of coreference resolution. We draw on the coreference work, taking two standard models from the literature and applying them to coherence modeling. Our first model distinguishes discourse-new from discourse-old noun phrases, using features based on Uryupina (2003). Discourse-new NPs are those whose referents have not been previously mentioned in the discourse. As noted by studies since Hawkins (1978), there are marked syntactic differences between the two classes. Our second model describes pronoun coreference. To be intelligible, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which pro"
P08-2011,J00-4003,0,0.0267774,"Missing"
P08-2026,J04-4004,0,0.172218,"Missing"
P08-2026,W05-1102,0,0.0647427,"Missing"
P08-2026,I05-1006,1,0.842357,"Missing"
P08-2026,J93-2004,0,0.0297328,"Missing"
P08-2026,N06-1020,1,0.42174,"ly the case that they would not significantly alter the landscape. 3 Central Experimental Result We used as the base parser the standardly available C/J parser. We then self-trained the parser on approximately 270,000 sentences — a random selection of abstracts from Medline.3 Medline is a large database of abstracts and citations from a wide variety of biomedical literature. As we note in the next section, the number 270,000 was selected by observing performance on a development set. We weighted the original WSJ hand annotated sentences equally with self-trained Medline data. So, for example, McClosky et al. (2006a) found that the data from the handannotated WSJ data should be considered at least five times more important than NANC data on an event by event level. We did no tuning to find out if there is some better weighting for our domain than one-to-one. The resulting parser was tested on a test corpus of hand-parsed sentences from the Genia Treebank (Tateisi et al., 2005). These are exactly the same sentences as used in the comparisons of the last section. Genia is a corpus of abstracts from the Medline database selected from a search with the keywords Human, Blood Cells, and Transcription Factors."
P08-2026,P06-1043,1,0.480377,"ly the case that they would not significantly alter the landscape. 3 Central Experimental Result We used as the base parser the standardly available C/J parser. We then self-trained the parser on approximately 270,000 sentences — a random selection of abstracts from Medline.3 Medline is a large database of abstracts and citations from a wide variety of biomedical literature. As we note in the next section, the number 270,000 was selected by observing performance on a development set. We weighted the original WSJ hand annotated sentences equally with self-trained Medline data. So, for example, McClosky et al. (2006a) found that the data from the handannotated WSJ data should be considered at least five times more important than NANC data on an event by event level. We did no tuning to find out if there is some better weighting for our domain than one-to-one. The resulting parser was tested on a test corpus of hand-parsed sentences from the Genia Treebank (Tateisi et al., 2005). These are exactly the same sentences as used in the comparisons of the last section. Genia is a corpus of abstracts from the Medline database selected from a search with the keywords Human, Blood Cells, and Transcription Factors."
P08-2026,P06-1055,0,0.0192458,"Missing"
P08-2026,J01-2004,0,0.0948807,"Missing"
P08-2026,E03-1008,0,0.150568,"Missing"
P08-2026,I05-2038,0,0.0179927,"erature. As we note in the next section, the number 270,000 was selected by observing performance on a development set. We weighted the original WSJ hand annotated sentences equally with self-trained Medline data. So, for example, McClosky et al. (2006a) found that the data from the handannotated WSJ data should be considered at least five times more important than NANC data on an event by event level. We did no tuning to find out if there is some better weighting for our domain than one-to-one. The resulting parser was tested on a test corpus of hand-parsed sentences from the Genia Treebank (Tateisi et al., 2005). These are exactly the same sentences as used in the comparisons of the last section. Genia is a corpus of abstracts from the Medline database selected from a search with the keywords Human, Blood Cells, and Transcription Factors. Thus the Genia treebank data are all from a small domain within Biology. As already noted, the Medline abstracts used for self-training were chosen randomly and thus span a large number of biomedical sub-domains. The results, the central results of this paper, are shown in Figure 1. Clegg and Shepherd (2005) do not provide separate precision and recall numbers. Howe"
P10-2007,E09-1018,1,0.946536,"ked as mentions. In this paper, we count how often same-head pairs fail to corefer in the MUC-6 corpus, showing that gold mention detection hides most such pairs, but more realistic detection finds large numbers. We also present an unsupervised generative model which learns to make certain samehead pairs non-coreferent. The model is based on the idea that pronoun referents are likely to be salient noun phrases in the discourse, so we can learn about NP antecedents using pronominal antecedents as a starting point. Pronoun anaphora, in turn, is learnable from raw data (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). Since our model links fewer NPs than the baseline, it improves precision but decreases recall. This tradeoff is favorable for CEAF, but not for b3 . We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving prec"
P10-2007,W05-0612,0,0.0315234,"nal assumption are not marked as mentions. In this paper, we count how often same-head pairs fail to corefer in the MUC-6 corpus, showing that gold mention detection hides most such pairs, but more realistic detection finds large numbers. We also present an unsupervised generative model which learns to make certain samehead pairs non-coreferent. The model is based on the idea that pronoun referents are likely to be salient noun phrases in the discourse, so we can learn about NP antecedents using pronominal antecedents as a starting point. Pronoun anaphora, in turn, is learnable from raw data (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). Since our model links fewer NPs than the baseline, it improves precision but decreases recall. This tradeoff is favorable for CEAF, but not for b3 . We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntac"
P10-2007,C00-1027,0,0.0139205,"as a starting point. For translation, we use a trivial model, p(ni |gai ) = 1 if the two have the same head, and 0 otherwise, except for the null antecedent, which draws heads from a multinomial distribution over words. While we could learn an alignment and then treat all generators as antecedents, so that only NPs aligned to the null antecedent were not labeled coreferent, in practice this model would align nearly all the same-head pairs. This is true because many words are “bursty”; the probability of a second occurrence given the first is higher than the a priori probability of occurrence (Church, 2000). Therefore, our model is actually a mixture of two IBM models, pC and pN , where pC produces NPs with antecedents and pN produces pairs that share a head, but are not coreferent. To break the symmetry, we allow pC to use any parameters w, while pN uses a uniform alignment, w ≡ ~0. We interpolate between these two models with a constant λ, the single manually set parameter of our system, which we fixed at .9. The full model, therefore, is: ator (the largest term in either of the sums) is from pT and is not the null antecedent are marked as coreferent to the generator. Other NPs are marked not"
P10-2007,P07-1107,0,0.084103,"arlier in the document are the easiest to 2 Related work Unsupervised systems specify the assumption of same-head coreference in several ways: by as1 Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause (Poon and Domingos, 2008), and using a sparse Dirichlet prior on word emissions (Haghighi and Klein, 2007). (These three systems, perhaps not coincidentally, use gold mentions.) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs. This work uses mentions extracted with an NP chunker. No specific results are reported for same-head NPs. However, while using exact string match raises precision, many non-matching phrases are still coreferent, so this approach cannot be considered a full solution to the problem. Supervised systems do better on the"
P10-2007,D09-1120,0,0.0593013,"ask in a notoriously difficult area; Stoyanov et al. (2009) shows that, among NPs headed by common nouns, those which have an exact match earlier in the document are the easiest to 2 Related work Unsupervised systems specify the assumption of same-head coreference in several ways: by as1 Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause (Poon and Domingos, 2008), and using a sparse Dirichlet prior on word emissions (Haghighi and Klein, 2007). (These three systems, perhaps not coincidentally, use gold mentions.) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs. This work uses mentions extracted with an NP chunker. No specific results are reported for same-head NPs. However, while using exact string match raises precision, many non-matchi"
P10-2007,N09-1069,0,0.0271473,"process fills in all the NP nodes in order, from left to right. This process ensures that, when generating node ni , we have already filled in all the NPs in the set G (since these all precede ni ). When deciding on a generator for NP ni , we can extract features characterizing its p(ai = j|G, D) ∝ exp(f (ni , gj , D) • w) The weights w are learned by gradient descent on the log-likelihood. To use this model within EM, we alternate an E-step where we calculate the expected alignments E[ai = j], then an Mstep where we run gradient descent. (We have also had some success with stepwise EM as in (Liang and Klein, 2009), but this requires some tuning to work properly.) 4 35 Downloaded from http://bllip.cs.brown.edu. As features, we take the same features as Charniak and Elsner (2009): sentence and word-count distance between ni and gj , sentence position of each, syntactic role of each, and head type of gj (proper, common or pronoun). We add binary features for the nonterminal directly over gj (NP, VP, PP, any S type, or other), the type of phrases modifying gj (proper nouns, phrasals (except QP and PP), QP, PP-of, PP-other, other modifiers, or nothing), and the type of determiner of gj (possessive, definite"
P10-2007,H05-1004,0,0.266784,"Missing"
P10-2007,N06-1020,1,0.855374,"Missing"
P10-2007,D08-1067,0,0.0667463,"coreference in several ways: by as1 Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause (Poon and Domingos, 2008), and using a sparse Dirichlet prior on word emissions (Haghighi and Klein, 2007). (These three systems, perhaps not coincidentally, use gold mentions.) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs. This work uses mentions extracted with an NP chunker. No specific results are reported for same-head NPs. However, while using exact string match raises precision, many non-matching phrases are still coreferent, so this approach cannot be considered a full solution to the problem. Supervised systems do better on the task, but not perfectly. Recent work (Stoyanov et al., 2009) attempts to determine the contributi"
P10-2007,D08-1068,0,0.0797683,"shows that, among NPs headed by common nouns, those which have an exact match earlier in the document are the easiest to 2 Related work Unsupervised systems specify the assumption of same-head coreference in several ways: by as1 Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause (Poon and Domingos, 2008), and using a sparse Dirichlet prior on word emissions (Haghighi and Klein, 2007). (These three systems, perhaps not coincidentally, use gold mentions.) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs. This work uses mentions extracted with an NP chunker. No specific results are reported for same-head NPs. However, while using exact string match raises precision, many non-matching phrases are still coreferent, so this approach cannot b"
P10-2007,P09-1074,0,0.187105,"ve not attracted nearly as much attention, and many systems, especially unsupervised ones, operate under the assumption that all same-head pairs corefer. This is by no means always the case– there are several systematic exceptions to the rule. In this paper, we show that these exceptions are fairly common, and describe an unsupervised system which learns to distinguish them from coreferent same-head pairs. There are several reasons why relatively little attention has been paid to same-head pairs. Primarily, this is because they are a comparatively easy subtask in a notoriously difficult area; Stoyanov et al. (2009) shows that, among NPs headed by common nouns, those which have an exact match earlier in the document are the easiest to 2 Related work Unsupervised systems specify the assumption of same-head coreference in several ways: by as1 Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause ("
P10-2007,P07-1031,0,0.0773972,"Missing"
P10-2007,nissim-etal-2004-annotation,0,\N,Missing
P10-2007,W06-1612,0,\N,Missing
P10-2007,N04-4009,0,\N,Missing
P11-1118,P05-1018,0,0.282907,"rk; one, the topical entity grid, is a novel extension of the entity grid. For the experiments below, we train the models on SWBD, sometimes augmented with a larger set of automatically parsed conversations from the F ISHER corpus. Since the two corpora are quite similar, F ISHER is a useful source for extra data; McClosky et al. (2010) uses it for this purpose in parsing experiments. (We continue to use SWBD/F ISHER even for experiments on IRC, because we do not have enough disentangled training data to learn lexical relationships.) 3.1 Entity grid The entity grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005) is an attempt to model some principles of Centering Theory (Grosz et al., 1995) in a statistical manner. It represents a document in terms of entities and their syntactic roles: subject (S), object (O), other (X) and not present (-). In each new 2 cs.brown.edu/melsner utterance, the grid predicts the role in which each entity will appear, given its history of roles in the previous sentences, plus a salience feature counting the total number of times the entity occurs. For instance, for an entity which is the subject of sentence 1, the object of sentence 2, and occurs four times in total, the"
P11-1118,N04-1015,0,0.128303,"apply these improvements to the full task. We suspect that, with better low-level annotation tools for the chat domain and a good way of integrating prior information, our improvements on SWBD could transfer fully to IRC chat. 2 Related work There is extensive previous work on coherence models for text ordering; we describe several specic models below, in section 2. This study focuses on models of local coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the uency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the ta"
P11-1118,E09-1018,1,0.736844,"proposed earlier (Lapata, 2003). This model attempts to generate the content words of the next sentence by translating them from the words of the previous sentence, plus a null word; thus, it will learn alignments between pairs of words that tend to occur in adjacent sentences. We learn parameters on the F ISHER corpus, and on NANC for news. 3.4 Pronouns The use of a generative pronoun resolver for coherence modeling originates in Elsner and Charniak (2008a). That paper used a supervised model (Ge et al., 1998), but we adapt a newer, unsupervised model which they also make publicly available (Charniak and Elsner, 2009)4 . They model each pronoun as generated by an antecedent somewhere in the previous two sentences. If a good antecedent is found, the probability of the pronoun's occurrence will be high; otherwise, the probability is low, signaling that the text is less coherent because the pronoun is hard to interpret correctly. We use the model as distributed for news text. For conversation, we adapt it by running a few iterations of their EM training algorithm on the F ISHER data. 4 bllip.cs.brown.edu/resources.shtml #software 3 1181 3.5 Discourse-newness Building on work from summarization (Nenkova and M"
P11-1118,N09-1042,0,0.0112794,"entanglement task, though so far, we are unable to apply these improvements to the full task. We suspect that, with better low-level annotation tools for the chat domain and a good way of integrating prior information, our improvements on SWBD could transfer fully to IRC chat. 2 Related work There is extensive previous work on coherence models for text ordering; we describe several specic models below, in section 2. This study focuses on models of local coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the uency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate"
P11-1118,D08-1035,0,0.0574798,"though so far, we are unable to apply these improvements to the full task. We suspect that, with better low-level annotation tools for the chat domain and a good way of integrating prior information, our improvements on SWBD could transfer fully to IRC chat. 2 Related work There is extensive previous work on coherence models for text ordering; we describe several specic models below, in section 2. This study focuses on models of local coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the uency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses i"
P11-1118,P08-2011,1,0.548915,"adopt the S WITCHBOARD (SWBD) corpus. SWBD contains recorded telephone conversations with known topics and hand-annotated parse trees; this allows us to control for the performance of our parser and other informational resources. To compare the two algorithmic settings, we use SWBD for ordering experiments, and also articially entangle pairs of telephone dialogues to create synthetic transcripts which we can disentangle. Finally, we present results on actual internet chat corpora. On synthetic SWBD transcripts, local coherence models improve performance considerably over our baseline model, Elsner and Charniak (2008b). On 1179 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1179–1189, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics internet chat, we continue to do better on a constrained disentanglement task, though so far, we are unable to apply these improvements to the full task. We suspect that, with better low-level annotation tools for the chat domain and a good way of integrating prior information, our improvements on SWBD could transfer fully to IRC chat. 2 Related work There is extensive previous work on coherenc"
P11-1118,P08-1095,1,0.367747,"Missing"
P11-1118,N10-1060,0,0.01319,"uns +EGrid/Topic/IBM-1 E+C `08b 74.0 79.3 76.8 76.3 73.9 78.3 76.4 +EGrid E+C `08b # IPHONE 92.3 89.0 # PHYSICS 96.6 90.2 # PYTHON 91.1 88.4 Table 5: Average accuracy for disentanglement of a single utterance for 19581 total lines from Adams (2008). Table 4: Accuracy for single utterance disentanglement, averaged over annotations of 800 lines of # LINUX data. In order to use syntactic models like the entity grid, we parse the transcripts using (McClosky et al., 2006). Performance is bad, although the parser does identify most of the NPs; poor results are typical for a standard parser on chat (Foster, 2010). We postprocess the parse trees to retag lol, haha and yes as UH (rather than NN, NNP and JJ). In this section, we use all three of our chatspecic models (sec. 2.0.6; time, speaker and mention) as a baseline. This baseline is relatively strong, so we evaluate our other models in combination with it. 6.1 Disentangling a single sentence As before, we show results on correctly disentangling a single sentence, given the correct structure of the rest of the transcript. We average performance on each transcript over the different annotations, then average the transcripts, weighing them by le"
P11-1118,W98-1119,1,0.516387,"ed for coherence by Soricut and Marcu (2006), although a less probabilistically elegant version was proposed earlier (Lapata, 2003). This model attempts to generate the content words of the next sentence by translating them from the words of the previous sentence, plus a null word; thus, it will learn alignments between pairs of words that tend to occur in adjacent sentences. We learn parameters on the F ISHER corpus, and on NANC for news. 3.4 Pronouns The use of a generative pronoun resolver for coherence modeling originates in Elsner and Charniak (2008a). That paper used a supervised model (Ge et al., 1998), but we adapt a newer, unsupervised model which they also make publicly available (Charniak and Elsner, 2009)4 . They model each pronoun as generated by an antecedent somewhere in the previous two sentences. If a good antecedent is found, the probability of the pronoun's occurrence will be high; otherwise, the probability is low, signaling that the text is less coherent because the pronoun is hard to interpret correctly. We use the model as distributed for news text. For conversation, we adapt it by running a few iterations of their EM training algorithm on the F ISHER data. 4 bllip.cs.brown."
P11-1118,J95-2003,0,0.584922,"riments below, we train the models on SWBD, sometimes augmented with a larger set of automatically parsed conversations from the F ISHER corpus. Since the two corpora are quite similar, F ISHER is a useful source for extra data; McClosky et al. (2010) uses it for this purpose in parsing experiments. (We continue to use SWBD/F ISHER even for experiments on IRC, because we do not have enough disentangled training data to learn lexical relationships.) 3.1 Entity grid The entity grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005) is an attempt to model some principles of Centering Theory (Grosz et al., 1995) in a statistical manner. It represents a document in terms of entities and their syntactic roles: subject (S), object (O), other (X) and not present (-). In each new 2 cs.brown.edu/melsner utterance, the grid predicts the role in which each entity will appear, given its history of roles in the previous sentences, plus a salience feature counting the total number of times the entity occurs. For instance, for an entity which is the subject of sentence 1, the object of sentence 2, and occurs four times in total, the grid predicts its role in sentence 3 according to the conditional P (jS; O; sa"
P11-1118,P04-1050,0,0.0208496,"es probabilities for three outcomes: no name mention, a mention of someone who has previously spoken 1182 The weights  can be learned discriminatively, maximizing the probability of d relative to a taskspecic contrast set. For ordering experiments, the contrast set is a single random permutation of d; we explain the training regime for disentanglement below, in subsection 4.1. 4 Comparing orderings of SWBD To measure the differences in performance caused by moving from news to a conversational domain, we rst compare our models on an ordering task, discrimination (Barzilay and Lapata, 2005; Karamanis et al., 2004). In this task, we take an original document and randomly permute its sentences, creating an articial incoherent document. We then test to see if our model prefers the coherent original. For SWBD, rather than compare permutations of the individual utterances, we permute conversational turns (sets of consecutive utterances by each speaker), since turns are natural discourse units in conversation. We take documents numbered 2000 3999 as training/development and the remainder as test, yielding 505 training and 153 test documents; we evaluate 20 permutations per document. As a comparison, we als"
P11-1118,P03-1069,0,0.139893,"omain exibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available # LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measure similarity between two sentences; Wang and Oard (2009) use a message expansion technique to incorporate context beyond a single sentence. Unigram overlaps are used to model coherence, but more sophisticated methods using syntax (Lapata and Barzilay, 2005) or lexical features (Lapata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and Charniak (2008b), which we use as a baseline because there is a publicly available imple1180 mentation2 . Adams (2008) also created and released a disentanglement corpus. They use LDA (Blei et al., 2001) to discover latent topics in their corpus, then measuring similarity by looking for shared topics. These features fail to improve their performance, which is puzzling in light of the success of topic modeling for other coherence and segmentation problems (Eisenstein and Barzilay, 2008; Foltz"
P11-1118,J06-4002,0,0.00561801,"l coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the uency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the target genre is still informative writing, so they reveal little about cross-domain exibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available # LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measu"
P11-1118,N06-1020,1,0.245318,"esting; together, they consist of 19581 lines of chat, with each section containing 500 to 1000 lines. Chat-specic +EGrid +Topical EGrid +IBM-1 +Pronouns +EGrid/Topic/IBM-1 E+C `08b 74.0 79.3 76.8 76.3 73.9 78.3 76.4 +EGrid E+C `08b # IPHONE 92.3 89.0 # PHYSICS 96.6 90.2 # PYTHON 91.1 88.4 Table 5: Average accuracy for disentanglement of a single utterance for 19581 total lines from Adams (2008). Table 4: Accuracy for single utterance disentanglement, averaged over annotations of 800 lines of # LINUX data. In order to use syntactic models like the entity grid, we parse the transcripts using (McClosky et al., 2006). Performance is bad, although the parser does identify most of the NPs; poor results are typical for a standard parser on chat (Foster, 2010). We postprocess the parse trees to retag lol, haha and yes as UH (rather than NN, NNP and JJ). In this section, we use all three of our chatspecic models (sec. 2.0.6; time, speaker and mention) as a baseline. This baseline is relatively strong, so we evaluate our other models in combination with it. 6.1 Disentangling a single sentence As before, we show results on correctly disentangling a single sentence, given the correct structure of the rest"
P11-1118,N03-2024,0,0.0151163,"Elsner, 2009)4 . They model each pronoun as generated by an antecedent somewhere in the previous two sentences. If a good antecedent is found, the probability of the pronoun's occurrence will be high; otherwise, the probability is low, signaling that the text is less coherent because the pronoun is hard to interpret correctly. We use the model as distributed for news text. For conversation, we adapt it by running a few iterations of their EM training algorithm on the F ISHER data. 4 bllip.cs.brown.edu/resources.shtml #software 3 1181 3.5 Discourse-newness Building on work from summarization (Nenkova and McKeown, 2003) and coreference resolution (Poesio et al., 2005), Elsner and Charniak (2008a) use a model which recognizes discourse-new versus old NPs as a coherence model. For instance, the model can learn that President Barack Obama is a more likely rst reference than Obama. Following their work, we score discourse-newness with a maximum-entropy classier using syntactic features counting different types of NP modiers, and we use NP head identity as a proxy for coreference. in the conversation, or a mention of someone else. (The third option is extremely rare; this accounts for most of the model's p"
P11-1118,W06-1612,0,0.0201703,"single model, showing the information provided by the weaker models is not completely redundant. Overall, these results suggest that most previously proposed local coherence models are domaingeneral; they work on conversation as well as news. The exception is the discourse-newness model, which benets most from the specic conventions of a written style. Full names with titles (like President Barack Obama) are more common in news, while conversation tends to involve fewer completely unfamiliar entities and more cases of bridging reference, in which grounding information is given implicitly (Nissim, 2006). Due to its poor performance, we omit the discourse-newness model in our remaining experiments. 5 Disentangling SWBD We now turn to the task of disentanglement, testing whether models that are good at ordering also do well in this new setting. We would like to hold the domain constant, but we do not have any disentanglement data recorded from naturally occurring speech, so we create synthetic instances by merging pairs of SWBD dialogues. Doing so creates an articial transcript in which two pairs of people appear to be talking simultaneously over a shared channel. The situation is somewhat co"
P11-1118,D08-1020,0,0.0317348,"focuses on models of local coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the uency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the target genre is still informative writing, so they reveal little about cross-domain exibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available # LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping uni"
P11-1118,P06-2103,0,0.649207,"f the previous sentence. These features can detect a transition like: The House voted yesterday. The Senate will consider the bill today.. If House and Senate have a high similarity, then the feature will have a high value, predicting that Senate is a good subject for the current sentence. As in the previous section, we learn the conditional probabilities with logistic regression; we train in parallel by splitting the data and averaging (Mann et al., 2009). The topics are trained on F ISHER, and on NANC for news. 3.3 IBM-1 The IBM translation model was rst considered for coherence by Soricut and Marcu (2006), although a less probabilistically elegant version was proposed earlier (Lapata, 2003). This model attempts to generate the content words of the next sentence by translating them from the words of the previous sentence, plus a null word; thus, it will learn alignments between pairs of words that tend to occur in adjacent sentences. We learn parameters on the F ISHER corpus, and on NANC for news. 3.4 Pronouns The use of a generative pronoun resolver for coherence modeling originates in Elsner and Charniak (2008a). That paper used a supervised model (Ge et al., 1998), but we adapt a newer, unsu"
P11-1118,N09-1023,0,0.061662,"by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the target genre is still informative writing, so they reveal little about cross-domain exibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available # LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measure similarity between two sentences; Wang and Oard (2009) use a message expansion technique to incorporate context beyond a single sentence. Unigram overlaps are used to model coherence, but more sophisticated methods using syntax (Lapata and Barzilay, 2005) or lexical features (Lapata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and Charniak (2008b), which we use as a baseline because there is a publicly available imple1180 mentation2 . Adams (2008) also created and released a dise"
P11-1118,N10-1004,1,\N,Missing
P11-2022,J95-2003,0,0.975848,"Missing"
P11-2022,N10-1061,0,0.0124322,"entation is available via https:// bitbucket.org/melsner/browncoherence. (and what syntactic role it will have) given its history of occurrences in the previous sentences. For instance, it estimates the probability that Clinton will be the subject of sentence 2, given that it was the subject of sentence 1. The standard grid model uses no information about the entity itself the probability is the same whether the entity under discussion is Hillary Clinton or wheat. Plainly, this assumption is too strong. Distinguishing important from unimportant entity types is important in coreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lap"
P11-2022,P05-1018,0,0.687763,"ce 1. The standard grid model uses no information about the entity itself the probability is the same whether the entity under discussion is Hillary Clinton or wheat. Plainly, this assumption is too strong. Distinguishing important from unimportant entity types is important in coreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the ent"
P11-2022,J08-1001,0,0.766763,"and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but nd no signicant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, c Portland, Oregon, June 19-24, 2011"
P11-2022,N10-1099,0,0.16164,"Missing"
P11-2022,E09-1018,1,0.418722,"target document itself. This avoids the problem that coreference resolvers do not work well for disordered or automatically produced text such as multidocument summary sentences, and also avoids the computational cost associated with coreference resolution. Linkable Was the head word of the entity ever marked as coreferring in MUC6? Unlinkable Did the head word of the entity occur 5 times in MUC6 and never corefer? Has pronouns Were there 5 or more pronouns coreferent with the head word of the entity in the NANC corpus? (Pronouns in NANC are automatically resolved using an unsupervised model (Charniak and Elsner, 2009).) No pronouns Did the head word of the entity occur over 50 times in NANC, and have fewer than 5 coreferent pronouns? To learn probabilities based on these features, we model the conditional probability p(r jF ) using multilabel logistic regression. Our model has a parameter for each combination of syntactic role r , entity-specic feature h and feature vector F : r  h  F . This allows the old and new features to interact while keeping the parameter space tractable7 . In Table 2, we examine the changes in our estimated probability in one particular context: an entity with salience 3 which a"
P11-2022,P05-1022,1,0.353675,"this often fails to improve results; in Barzilay and Lapata (2005), coreference improves results in only one of their target domains, and actually hurts for readability prediction. Their results, moreover, rely on running coreference on the document in its original order; in a summarization task, the correct order is not known, which will cause even more resolver errors. To build a model based on the grid, we treat the columns (entities) as independent, and look at local transitions between sentences. We model the i;j 2 Roles are determined heuristically using trees produced by the parser of (Charniak and Johnson, 2005). 126 ;j i ;j i;j ;f light 4 Experimental design We test our model on two experimental tasks, both testing its ability to distinguish between correct and incorrect orderings for WSJ articles. In document discrimination (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentences, scoring the system correct if it prefers the original ordering4 . We also evaluate on the more difcult task of sentence insertion (Chen et al., 2007; Elsner and Charniak, 2008). In this task, we remove each sentence from the article and test whether the model prefers to re-insert it at"
P11-2022,D07-1009,0,0.0177826,"ocal transitions between sentences. We model the i;j 2 Roles are determined heuristically using trees produced by the parser of (Charniak and Johnson, 2005). 126 ;j i ;j i;j ;f light 4 Experimental design We test our model on two experimental tasks, both testing its ability to distinguish between correct and incorrect orderings for WSJ articles. In document discrimination (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentences, scoring the system correct if it prefers the original ordering4 . We also evaluate on the more difcult task of sentence insertion (Chen et al., 2007; Elsner and Charniak, 2008). In this task, we remove each sentence from the article and test whether the model prefers to re-insert it at its original location. We report the average proportion of correct insertions per document. As in Elsner and Charniak (2008), we test on sections 14-24 of the Penn Treebank, for 1004 test documents. We test signicance using the Wilcoxon Sign-rank test, which detects signicant differences in the medians of two distributions5 . 5 Mention detection Our main contribution is to extend the entity grid by adding a large number of entity-specic features. Before"
P11-2022,P10-1020,0,0.0674261,"ity is mentioned in the document. We denote this feature vector F . For example, the vector for ight after the last sentence of the example would be F3 = hX; S; sal = 2i. Using two sentences of context and capping salience at 4, there are only 64 possible vectors, so we can learn an independent multinomial distribution for each F . However, the number of vectors grows exponentially as we add features. i;j i laredo X Figure 1: A short text (using NP-only mention detection), and its corresponding entity grid. The numeric token 1300 is removed in preprocessing. ment over the original model. Cheung and Penn (2010) adapt the grid to German, where focused constituents are indicated by sentence position rather than syntactic role. The best entity grid for English text, however, is still the original. 3 Entity grids The entity grid represents a document as a matrix (Figure 1) with a row for each sentence and a column for each entity. The entry for (sentence i, entity j ), which we write r , represents the syntactic role that entity takes on in that sentence: subject (S), object (O), or some other role (X)2 . In addition, there is a special marker (-) for entities which do not appear at all in a given sente"
P11-2022,P08-2011,1,0.760466,"tistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but nd no signicant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 1 [Visual meteorological conditions]S prevailed for [the personal cross country ight for which [a VFR ight plan]O was led]X ."
P11-2022,P10-2007,1,0.882727,"Missing"
P11-2022,W07-2321,0,0.101782,"tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but nd no signicant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 1 [Visual meteorological conditions]S prevailed for [the personal cross country ight for which [a VFR ight plan]O was led]X . 2 [The ight]S originated at [Nuevo Laredo , Mexico]X , at [approximately 1300]X . s 1 2 conditions S - plan O - flight X S transitions using the generative approach given in Lapata and Barzilay (2005)3"
P11-2022,P03-1069,0,0.080079,"Missing"
P11-2022,P10-1158,0,0.0282456,"tant from unimportant entity types is important in coreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but nd no signicant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Lin"
P11-2022,H05-1031,0,0.0190441,"org/melsner/browncoherence. (and what syntactic role it will have) given its history of occurrences in the previous sentences. For instance, it estimates the probability that Clinton will be the subject of sentence 2, given that it was the subject of sentence 1. The standard grid model uses no information about the entity itself the probability is the same whether the entity under discussion is Hillary Clinton or wheat. Plainly, this assumption is too strong. Distinguishing important from unimportant entity types is important in coreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et"
P11-2022,P10-1056,0,0.0216028,"oreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but nd no signicant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, c Portla"
P11-2022,P06-2103,0,0.170713,"med-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but nd no signicant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 1 [Visual meteorological conditions]S prevailed for [the personal cross country ight for which [a VF"
P12-2038,P10-1112,0,0.0430461,"Missing"
P12-2038,N09-1062,0,0.0218752,"Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics 2.2 TSG induction One inherent difficulty in the use of TSGs is controlling the size of grammars automatically induced from data, which with any reasonable corpus quickly becomes too large for modern workstations to handle. When automatically induced TSGs were first proposed by Bod (1991), the problem of grammar induction was tackled with random selection of fragments or weak constraints that led to massive grammars. A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). They provide a local Gibbs sampling algorithm, and Cohn and Blunsom (2010) later developed a block sampling algorithm with better convergence behavior. While this Bayesian method has yet to produce state of the art parsing results, it has achieved state of the art results for unsupervised grammar induction (Blunsom and Cohn, 2010) and has been extended to synchronous grammars for use in sentence compression (Yamangil and Shieber, 2010). More recently, (Sangati and Zuidema, 2011) presented an elegantly simple heuristic inspired by tree kernels that they call DoubleD"
P12-2038,P11-2038,0,0.114265,"anker. Since then, such tree kernels have been used to perform a variety of text classification tasks, such as semantic role labeling (Moschitti et al, 2008), authorship attribution (Kim et al, 2010), or the work of Suzuki and Isozaki (2006) that performs question classification, subjectivity detection, and polarity identification. Syntactic features have also been used in non194 kernelized classifiers, such as in the work of Wong and Dras (2011a) mentioned in Section 2.1. Additional examples include Raghavan et al (2010), which uses a CFG language model to perform authorship attribution, and Post (2011), which uses TSG features in a logistic regression model to perform grammaticality detection. 3 Tree Substitution Grammars Tree Substitution Grammars are similar to Context Free Grammars, differing in that they allow rewrite rules of arbitrary parse tree structure with any number of nonterminal or terminal leaves. We adopt the common term fragment2 to refer to these rules, as they are easily visualised as fragments of a complete parse tree. S NP VP VBZ NP NP NP NP NNP NN NNS George broccoli shoes hates Figure 1: Fragments from a Tree Substitution Grammar capable of deriving the sentences “Geor"
P12-2038,D11-1008,0,0.266293,"A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). They provide a local Gibbs sampling algorithm, and Cohn and Blunsom (2010) later developed a block sampling algorithm with better convergence behavior. While this Bayesian method has yet to produce state of the art parsing results, it has achieved state of the art results for unsupervised grammar induction (Blunsom and Cohn, 2010) and has been extended to synchronous grammars for use in sentence compression (Yamangil and Shieber, 2010). More recently, (Sangati and Zuidema, 2011) presented an elegantly simple heuristic inspired by tree kernels that they call DoubleDOP. They showed that manageable grammar sizes can be obtained from a corpus the size of the Penn Treebank by recording all fragments that occur at least twice, subject to a pairwise constraint of maximality. Using an additional heuristic to provide a distribution over fragments, DoubleDOP achieved the current state of the art for TSG parsing, competing closely with the absolute best results set by refinement based parsers. 2.3 Fragment Based Classification The use of parse tree fragments for classification"
P12-2038,D11-1148,0,0.763621,"of possible TSG rules given a corpus. This is an active area of research with two distinct effective solutions. The first uses a nonparametric Bayesian model to handle the large number 1 a.k.a. Maximum Entropy Model Related Work Native Language Detection Work in automatic native language detection has been mainly associated with the ICLE, published in 2002. Koppel et al (2005) first constructed such a system with a feature set consisting of function words, POS bi-grams, and character n-grams. These features provide a strong baseline but cannot capture many linguistic phenomena. More recently, Wong and Dras (2011a) considered syntactic features for this task, using logistic regression with features extracted from parse trees produced by a state of the art statistical parser. They investigated two classes of features: reranking features from the Charniak parser and CFG features. They showed that while reranking features capture long range dependencies in parse trees that CFG rules cannot, they do not produce classification performance superior to simple CFG rules. Their CFG feature approach represents the best performing model to date for the task of native language detection. Wong and Dras (2011b) als"
P12-2038,U11-1015,0,0.288493,"of possible TSG rules given a corpus. This is an active area of research with two distinct effective solutions. The first uses a nonparametric Bayesian model to handle the large number 1 a.k.a. Maximum Entropy Model Related Work Native Language Detection Work in automatic native language detection has been mainly associated with the ICLE, published in 2002. Koppel et al (2005) first constructed such a system with a feature set consisting of function words, POS bi-grams, and character n-grams. These features provide a strong baseline but cannot capture many linguistic phenomena. More recently, Wong and Dras (2011a) considered syntactic features for this task, using logistic regression with features extracted from parse trees produced by a state of the art statistical parser. They investigated two classes of features: reranking features from the Charniak parser and CFG features. They showed that while reranking features capture long range dependencies in parse trees that CFG rules cannot, they do not produce classification performance superior to simple CFG rules. Their CFG feature approach represents the best performing model to date for the task of native language detection. Wong and Dras (2011b) als"
P12-2038,P10-1096,0,0.0219097,"Missing"
P12-2038,J08-2003,0,\N,Missing
P12-2038,P10-2042,0,\N,Missing
P12-2038,D10-1117,0,\N,Missing
P12-2038,P06-1055,0,\N,Missing
P12-2038,P09-2012,0,\N,Missing
P12-2038,P10-2008,0,\N,Missing
P12-2038,C92-3126,0,\N,Missing
P13-1030,P03-1054,0,0.00786232,"If we decide on adjunction, one of the available type (3) rules is chosen from a multinomial. By conditioning the probability of adjunction on varying amounts of information about the node, alternative models can easily be defined. Experiments As a proof of concept, we investigate OS TAG in the context of the classic Penn Treebank statistical parsing setup; training on section 2-21 and testing on section 23. For preprocessing, words that occur only once in the training data are mapped to the unknown categories employed in the parser of Petrov et al. (2006). We also applied the annotation from Klein and Manning (2003) that appends “-U” to each nonterminal node with a single child, drastically reducing the presence of looping unary chains. This allows the use of a coarse to fine parsing strategy (Charniak et al., 2006) in which a sentence is first parsed with the Maximum Likelihood PCFG and only constituents whose probability exceeds a cutoff of 10−4 are allowed in the OS TAG chart. Designed to facilitate sister adjunction, we define our binarization scheme by example in which the added nodes, indicated by @, record both the parent and head child of the rule. NP @NN-NP @NN-NP DT SBAR @NN-NP JJ NN A compact"
P13-1030,P06-1055,0,0.42664,"sirable for systems that seek to extract coherent and concise information from text. Introduction While it is widely accepted that natural language is not context-free, practical limitations of existing algorithms motivate Context-Free Grammars (CFGs) as a good balance between modeling power and asymptotic performance (Charniak, 1996). In constituent-based parsing work, the prevailing technique to combat this divide between efficient models and real world data has been to selectively strengthen the dependencies in a CFG by increasing the grammar size through methods such as symbol refinement (Petrov et al., 2006). Another approach is to employ a more powerful grammatical formalism and devise constraints and transformations that allow use of essential efficient algorithms such as the Inside-Outside algorithm (Lari and Young, 1990) and CYK parsing. Tree-Adjoining Grammar (TAG) is a natural TAG allows a linguistically motivated treatment of the example sentences above by generating the last two sentences through modification of the first, applying operations corresponding to negation and the use of a subordinate clause. Unfortunately, the added expressive power of TAG comes with O(n6 ) time complexity fo"
P13-1030,P00-1058,0,0.0849415,"ter than or equal to 1, we 303 VP + VP always VP* S NP quickly S ⇒ NP VP VP VP always runs VP quickly runs Figure 2: The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle) to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is denoted with an asterisk. The OS TAG constraint would disallow further adjunction at the bold VP node only, as it is along the spine of the auxiliary tree. complexity of TIG. Several probabilistic models have been proposed for TIG. While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. Later approaches (Shindo et al., 2011; Yamangil and Shieber, 2012) were able to extend the non-parametric modeling of TSGs to TIG, providing methods for both modeling and grammar induction. set of auxiliary trees A and the adjunction operation that governs their use. An auxiliary tree α is an elementary tree containing a single distinguished nonterminal leaf, the foot node, with the same symbol as the root of α. An auxiliary tree with root and foot node X can be adjoined into an internal node of an elementary"
P13-1030,P10-2042,0,0.103092,"re of the original elementary trees. A more compact CFG representation can be obtained by marking each node in each elementary tree with a signature of its subtree. This transform, presented by Goodman (2003), can rein in the grammar constant G, as the crucial CFG algorithms for a sentence of length n have complexity O(Gn3 ). A simple probabilistic model for a TSG is a set of multinomials, one for each nonterminal in N corresponding to its possible substitutions in R. A more flexible model allows a potentially infinite number of substitution rules using a Dirichlet Process (Cohn et al., 2009; Cohn and Blunsom, 2010). This model has proven effective for grammar induction via Markov Chain Monte Carlo (MCMC), in which TSG derivations of the training set are repeatedly sampled to find frequently occurring elementary trees. A straightforward technique for induction of a finite TSG is to perform this nonparametric induction and select the set of rules that appear in at least one sampled derivation at one or several of the final iterations. that it can be reduced to a CFG, allowing the use of traditional cubic-time algorithms. The reduction is reversible, so that the original TAG derivation can be recovered exa"
P13-1030,J95-4002,0,0.535819,"bidden in TIG. This targeted blocking of recursion has similar motivations and benefits to the approximation of CFGs with regular languages (Mohri and jan Nederhof, 2000). The following sections discuss in detail the context-free nature of OS TAG and alternative probabilistic models for its equivalent CFG form. We propose a simple but empirically effective heuristic for grammar induction for our experiments on Penn Treebank data. Tree Insertion Grammar Tree Insertion Grammars (TIGs) are a longstanding compromise between the intuitive expressivity of TAG and the algorithmic simplicity of CFGs. Schabes and Waters (1995) showed that by restricting the form of the auxiliary trees in A and the set of auxiliary trees that may adjoin at particular nodes, a TAG generates only context-free languages. The TIG restriction on auxiliary trees states that the foot node must occur as either the leftmost or rightmost leaf node. This introduces an important distinction between left, right, and wrapping auxiliary trees, of which only the first two are allowed in TIG. Furthermore, TIG disallows adjunction of left auxiliary trees on the spines of right auxiliary trees, and vice versa. This is to prevent the construction of wr"
P13-1030,N09-1062,0,0.0639577,"record the structure of the original elementary trees. A more compact CFG representation can be obtained by marking each node in each elementary tree with a signature of its subtree. This transform, presented by Goodman (2003), can rein in the grammar constant G, as the crucial CFG algorithms for a sentence of length n have complexity O(Gn3 ). A simple probabilistic model for a TSG is a set of multinomials, one for each nonterminal in N corresponding to its possible substitutions in R. A more flexible model allows a potentially infinite number of substitution rules using a Dirichlet Process (Cohn et al., 2009; Cohn and Blunsom, 2010). This model has proven effective for grammar induction via Markov Chain Monte Carlo (MCMC), in which TSG derivations of the training set are repeatedly sampled to find frequently occurring elementary trees. A straightforward technique for induction of a finite TSG is to perform this nonparametric induction and select the set of rules that appear in at least one sampled derivation at one or several of the final iterations. that it can be reduced to a CFG, allowing the use of traditional cubic-time algorithms. The reduction is reversible, so that the original TAG deriva"
P13-1030,P11-2036,0,0.0683419,"The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle) to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is denoted with an asterisk. The OS TAG constraint would disallow further adjunction at the bold VP node only, as it is along the spine of the auxiliary tree. complexity of TIG. Several probabilistic models have been proposed for TIG. While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. Later approaches (Shindo et al., 2011; Yamangil and Shieber, 2012) were able to extend the non-parametric modeling of TSGs to TIG, providing methods for both modeling and grammar induction. set of auxiliary trees A and the adjunction operation that governs their use. An auxiliary tree α is an elementary tree containing a single distinguished nonterminal leaf, the foot node, with the same symbol as the root of α. An auxiliary tree with root and foot node X can be adjoined into an internal node of an elementary tree labeled with X by splicing the auxiliary tree in at that internal node, as pictured in Figure 2. We refer to the path"
P13-1030,P12-2022,1,0.643971,"tion combines the auxiliary tree (left) with the elementary tree (middle) to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is denoted with an asterisk. The OS TAG constraint would disallow further adjunction at the bold VP node only, as it is along the spine of the auxiliary tree. complexity of TIG. Several probabilistic models have been proposed for TIG. While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. Later approaches (Shindo et al., 2011; Yamangil and Shieber, 2012) were able to extend the non-parametric modeling of TSGs to TIG, providing methods for both modeling and grammar induction. set of auxiliary trees A and the adjunction operation that governs their use. An auxiliary tree α is an elementary tree containing a single distinguished nonterminal leaf, the foot node, with the same symbol as the root of α. An auxiliary tree with root and foot node X can be adjoined into an internal node of an elementary tree labeled with X by splicing the auxiliary tree in at that internal node, as pictured in Figure 2. We refer to the path between the root and foot no"
P13-1030,P98-1091,0,0.0874012,"e of depth greater than or equal to 1, we 303 VP + VP always VP* S NP quickly S ⇒ NP VP VP VP always runs VP quickly runs Figure 2: The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle) to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is denoted with an asterisk. The OS TAG constraint would disallow further adjunction at the bold VP node only, as it is along the spine of the auxiliary tree. complexity of TIG. Several probabilistic models have been proposed for TIG. While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. Later approaches (Shindo et al., 2011; Yamangil and Shieber, 2012) were able to extend the non-parametric modeling of TSGs to TIG, providing methods for both modeling and grammar induction. set of auxiliary trees A and the adjunction operation that governs their use. An auxiliary tree α is an elementary tree containing a single distinguished nonterminal leaf, the foot node, with the same symbol as the root of α. An auxiliary tree with root and foot node X can be adjoined into an internal node"
P13-1030,C94-2149,0,\N,Missing
P13-1030,C98-1088,0,\N,Missing
P13-1030,N06-1022,1,\N,Missing
P14-2084,W13-1104,0,0.25049,"ve new research on methods for verbal irony detection. 1 https://github.com/bwallace/ ACL-2014-irony 512 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 512–516, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Previous Work There has recently been a flurry of interesting work on automatic irony detection (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). In these works, verbal irony detection has mostly been treated as a standard text classification task, though with some innovative approaches specific to detecting irony. 1 2 4 3 Figure 1: The web-based tool used by our annotators to label reddit comments. Enumerated interface elements are described as follows: 1 the text of the comment to be annotated – sentences marked as ironic are highlighted; 2 buttons to label sentences as ironic or unironic; 3 buttons to request additional context (the embedding discussion thread or associated webpage – see Section 3.2); 4 radio"
P14-2084,D13-1066,0,0.658212,"ds for verbal irony detection. 1 https://github.com/bwallace/ ACL-2014-irony 512 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 512–516, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Previous Work There has recently been a flurry of interesting work on automatic irony detection (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). In these works, verbal irony detection has mostly been treated as a standard text classification task, though with some innovative approaches specific to detecting irony. 1 2 4 3 Figure 1: The web-based tool used by our annotators to label reddit comments. Enumerated interface elements are described as follows: 1 the text of the comment to be annotated – sentences marked as ironic are highlighted; 2 buttons to label sentences as ironic or unironic; 3 buttons to request additional context (the embedding discussion thread or associated webpage – see Section 3.2); 4 radio button to provide conf"
P14-2084,P09-2041,0,0.0486233,"dgements for this task, so too do computers. Our hope is that these observations and this dataset will spur innovative new research on methods for verbal irony detection. 1 https://github.com/bwallace/ ACL-2014-irony 512 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 512–516, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Previous Work There has recently been a flurry of interesting work on automatic irony detection (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). In these works, verbal irony detection has mostly been treated as a standard text classification task, though with some innovative approaches specific to detecting irony. 1 2 4 3 Figure 1: The web-based tool used by our annotators to label reddit comments. Enumerated interface elements are described as follows: 1 the text of the comment to be annotated – sentences marked as ironic are highlighted; 2 buttons to label sentences as ironic or unironic; 3 buttons"
P14-2084,walker-etal-2012-corpus,0,0.0599253,"es marked as ironic are highlighted; 2 buttons to label sentences as ironic or unironic; 3 buttons to request additional context (the embedding discussion thread or associated webpage – see Section 3.2); 4 radio button to provide confidence in comment labels (low, medium or high). The most common data source used to experiment with irony detection systems has been Twitter (Reyes et al., 2012; Gonz´alez-Ib´an˜ ez et al., 2011; Davidov et al., 2010), though Amazon product reviews have been used experimentally as well (Tsur et al., 2010; Davidov et al., 2010; Reyes et al., 2012; Filatova, 2012). Walker et al. (2012) also recently introduced the Internet Argument Corpus (IAC), which includes a sarcasm label (among others). 3 Introducing the reddit Irony Dataset Here we introduce the first version (β 1.0) of our irony corpus. Reddit (http://reddit. com) is a social-news website to which news stories (and other links) are posted, voted on and commented upon. The forum component of reddit is extremely active: popular posts often have well into 1000’s of user comments. Reddit comprises ‘sub-reddits’, which focus on specific topics. For example, http:// reddit.com/r/politics features articles (and hence commen"
P14-2084,filatova-2012-irony,0,\N,Missing
P14-2084,P11-2102,0,\N,Missing
P14-2097,P10-1127,0,0.0983512,"retrieved for each visual entity, which are then fused into a single output caption. Like I M 2T EXT, their approach uses visual similarity as a proxy for textual relevance. Other related work models the text more directly, but is more restrictive about the source and quality of the human-written training data. Farhadi et al. (2010) and Hodosh et al. (2013) learn joint representations for images and captions, but can only be trained on data with very strong alignment between images and descriptions (i.e. captions written by Mechanical Turkers). Another line of related work (Fan et al., 2010; Aker and Gaizauskas, 2010; Feng and Lapata, 2010) generates captions by extracting sentences from documents which are related to the query image. These approaches are tailored toward specific domains, such as travel and news, where images tend to appear with corresponding text. 4.2 Measuring Visual Similarity Data-driven matching methods have shown to be very effective for a variety of challenging problems (Hays and Efros, 2008; Makadia et al., 2008; Tighe and Lazebnik, 2010). Typically these methods compute global (scene-based) descriptors rather than object and entity detections. Scenebased techniques in CV are gene"
P14-2097,N12-1094,0,0.022994,"t and entity detections. Scenebased techniques in CV are generally more robust, and can be computed more efficiently on large datasets. The basic I M 2T EXT model uses an equally weighted average of GIST (Oliva and Torralba, 2001) and TinyImage (Torralba et al., 2008) features, which coarsely localize low-level features in scenes. The output is a multi-dimensional image space where semantically similar scenes (e.g. streets, beaches, highways) are projected near each other. 2 http://tamaraberg.com/CLSP11/ In particular, papers stemming from the 2011 JHU-CLSP Summer Workshop (Berg et al., 2012; Dodge et al., 2012; Mitchell et al., 2012) and more recently, the best paper award winner at ICCV (Ordonez et al., 2013). 3 1 See http://bllip.cs.brown.edu/ download/captioning_resources.zip or ACL Anthology. 593 Patterson and Hays (2012) present “scene attribute” representations which are characterized using low-level perceptual attributes as used by GIST (e.g. openness, ruggedness, naturalness), as well as high-level attributes informed by openended crowd-sourced image descriptions (e.g., indoor lighting, running water, places for learning). Follow-up work (Patterson et al., 2014) shows that their attributes"
P14-2097,E12-1076,0,0.167781,"Missing"
P14-2097,P10-1126,0,0.0141557,"entity, which are then fused into a single output caption. Like I M 2T EXT, their approach uses visual similarity as a proxy for textual relevance. Other related work models the text more directly, but is more restrictive about the source and quality of the human-written training data. Farhadi et al. (2010) and Hodosh et al. (2013) learn joint representations for images and captions, but can only be trained on data with very strong alignment between images and descriptions (i.e. captions written by Mechanical Turkers). Another line of related work (Fan et al., 2010; Aker and Gaizauskas, 2010; Feng and Lapata, 2010) generates captions by extracting sentences from documents which are related to the query image. These approaches are tailored toward specific domains, such as travel and news, where images tend to appear with corresponding text. 4.2 Measuring Visual Similarity Data-driven matching methods have shown to be very effective for a variety of challenging problems (Hays and Efros, 2008; Makadia et al., 2008; Tighe and Lazebnik, 2010). Typically these methods compute global (scene-based) descriptors rather than object and entity detections. Scenebased techniques in CV are generally more robust, and c"
P14-2097,N09-1041,0,0.0695238,"Missing"
P14-2097,P02-1040,0,0.102666,"Missing"
P14-2097,P12-1038,0,0.33239,"Missing"
P14-2097,P13-2138,0,0.0640034,"images and generated captions. ment over the C OLLECTIVE system captions. Although our system captions score lower than the human captions on average, there are some instances of our system captions being judged as more relevant than the human-written captions. 6 consider combined approaches which incorporate more regional content structures. For example, previous work in nonparametric hierarchical topic modeling (Blei et al., 2010) and scene labeling (Liu et al., 2011) may provide avenues for further improvement of this model. Compression methods for removing visually irrelevant information (Kuznetsova et al., 2013) may also help increase the relevance of extracted captions. We leave these ideas for future work. Discussion and Examples Example captions are shown in Table 3. In many instances, scene-based image descriptors provide enough information to generate a complete description of the image, or at least a sufficiently good one. However, there are some kinds of images for which scene-based features alone are insufficient. For example, the last example describes the small pink flowers in the background, but misses the bear. Image captioning is a relatively novel task for which the most compelling appl"
P14-2097,P13-1006,0,0.0198518,"Missing"
P14-2097,D11-1041,0,\N,Missing
P15-1100,P09-2041,0,0.0373675,"uz does not find general support even in this community. Example comments include: “Stay classy Ted Cruz” and “Great idea on the talkathon Cruz”. The ‘mr’ and ‘king’ terms are almost exclusively references to Obama in the conservative subreddit. In any case, because these are three-way interaction terms, they are all relatively rare: therefore we would caution against over interpretation here. 6 Related Work The task of automated irony detection has recently received a great deal of attention from the NLP and ML communities (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). This work has mostly focussed on exploiting token4 ‘Ollie’ is a conservative political commentator. based indicators of verbal irony. For example, it is clear that gratuitous punctuation (e.g. “oh really??!!!”) signals irony (Carvalho et al., 2009). Davidov et al. (2010) proposed a semisupervised approach in which they look for sentence templates indicative of irony. Elsewhere, Riloff et al. (2013) proposed a method that exploits apparently contrasting sentim"
P15-1100,D13-1066,0,0.443222,"terpretation of verbal irony supports this supposition (Grice, 1975; Clark and Gerrig, 1984; Wallace, 2013; Wallace et al., 2014). Individuals will be more likely, in general, to use sarcasm when discussing specific entities. Which entities will depend in part on the community to which the individual belongs. As a proxy for user community, here we leverage the subreddits to which comments were posted. Sentiment may also play an important role. In general, verbal irony is almost always used to convey negative views via ostensibly positive utterances (Sperber and Wilson, 1981). And recent work (Riloff et al., 2013) has exploited features based on sentiment to improve irony detection. To summarize: when assuming an ironic voice we expect that individuals will convey ostensibly positive sentiment about entities, and that these entities will depend on the type of individual in question. We propose capitalizing on such information by introducing features that encode subreddits, sentiment and noun phrases (NNPs), as we describe next. 2.2 Features We leverage the feature sets enumerated in Table 1. Subreddits are observed variables. Noun phrase (NNP) extraction and sentiment inference are performed automatica"
P15-1100,D13-1170,0,0.0025008,"ng an ironic voice we expect that individuals will convey ostensibly positive sentiment about entities, and that these entities will depend on the type of individual in question. We propose capitalizing on such information by introducing features that encode subreddits, sentiment and noun phrases (NNPs), as we describe next. 2.2 Features We leverage the feature sets enumerated in Table 1. Subreddits are observed variables. Noun phrase (NNP) extraction and sentiment inference are performed automatically via state of the art NLP tools. In particular, we use the Stanford Sentiment Analysis tool (Socher et al., 2013) to infer sentiment. To extract NNPs we use the Stanford 1036 Feature Sentiment Subreddit NNP NNP+ Description The inferred sentiment (negative/neutral or positive) for a given comment. the subreddit (e.g., progressive or conservative; atheism or Christianity) to which a comment was posted. Noun phrases (e.g., proper nouns) extracted from comment texts. Noun phrases extracted from comment texts and the thread to which they belong (for example, ‘Obamacare’ from the title in Figure 1). 3 Enforcing sparsity 3.1 Preliminaries In this work we consider linear models with binary outputs (y ∈ {−1, +1}"
P15-1100,N03-1033,0,0.039938,"ample, ‘Obamacare’ from the title in Figure 1). 3 Enforcing sparsity 3.1 Preliminaries In this work we consider linear models with binary outputs (y ∈ {−1, +1}). We will assume we have access to a training dataset comprising n instances, x = {x1 , ..., xn } and associated labels y = {y1 , ..., yn }. We then aim to find a weightvector w that optimizes the following objective. Table 1: Feature types that we exploit. We view the (observed) subreddit as a proxy for user type. We combine this with sentiment and extracted noun phrases (NNPs) to improve classifier performance. Part of Speech tagger (Toutanova et al., 2003). We then introduce ‘bag-of-NNP’ features and features that indicate whether the sentiment inferred for a given sentence was positive or not. Additionally, we introduce ‘interaction’ features that capture combinations of these. For example, a feature that indicates whether a given sentence mentions Obamacare (which will be one of many NNPs automatically extracted) and was posted in the conservative subreddit. This is an example of a two-way interaction. We also experiment with three-way interactions, crossing sentiment with NNPs and subreddits. An example is a feature that indicates if a sente"
P15-1100,P11-1137,0,0.0109233,"utterance to detect irony. While innovative, these approaches still rely on features intrinsic to comments; i.e., they do not attempt to capitalize on contextualizing features external to the comment text. This means that there will necessarily be certain (subtle) ironies that escape detection by such approaches. For example, without any additional information about the speaker, it would be impossible to deduce whether the comment “Obamacare is a great program” is intended sarcastically. Other related recent work has shown the promise of sparse models, both for prediction and interpretation (Eisenstein et al., 2011a; Eisenstein et al., 2011b; Yogatama and Smith, 2014a). Yogatama (2014a; 2014b), e.g., has leveraged the group lasso approach to impose ‘structured’ sparsity on feature weights. Our work here may similarly be viewed as assuming a specific sparsity pattern (specifically that feature weights for ‘interaction features’ will be sparse) and expressing this via regularization. 7 Conclusions and Future Directions We have shown that we can leverage contextualizing information to improve identification of verbal irony in online comments. This is in contrast to previous models, which have relied predom"
P15-1100,filatova-2012-irony,0,0.0232235,"e: “Stay classy Ted Cruz” and “Great idea on the talkathon Cruz”. The ‘mr’ and ‘king’ terms are almost exclusively references to Obama in the conservative subreddit. In any case, because these are three-way interaction terms, they are all relatively rare: therefore we would caution against over interpretation here. 6 Related Work The task of automated irony detection has recently received a great deal of attention from the NLP and ML communities (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). This work has mostly focussed on exploiting token4 ‘Ollie’ is a conservative political commentator. based indicators of verbal irony. For example, it is clear that gratuitous punctuation (e.g. “oh really??!!!”) signals irony (Carvalho et al., 2009). Davidov et al. (2010) proposed a semisupervised approach in which they look for sentence templates indicative of irony. Elsewhere, Riloff et al. (2013) proposed a method that exploits apparently contrasting sentiment in the same utterance to detect irony. While innovative, these ap"
P15-1100,P11-2102,0,0.411338,"e comments include: “Stay classy Ted Cruz” and “Great idea on the talkathon Cruz”. The ‘mr’ and ‘king’ terms are almost exclusively references to Obama in the conservative subreddit. In any case, because these are three-way interaction terms, they are all relatively rare: therefore we would caution against over interpretation here. 6 Related Work The task of automated irony detection has recently received a great deal of attention from the NLP and ML communities (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). This work has mostly focussed on exploiting token4 ‘Ollie’ is a conservative political commentator. based indicators of verbal irony. For example, it is clear that gratuitous punctuation (e.g. “oh really??!!!”) signals irony (Carvalho et al., 2009). Davidov et al. (2010) proposed a semisupervised approach in which they look for sentence templates indicative of irony. Elsewhere, Riloff et al. (2013) proposed a method that exploits apparently contrasting sentiment in the same utterance to detect irony. While inno"
P15-1100,W13-1104,0,0.195272,"ities (external the comment text itself) provide contextual signals indicating that the shown comment was intended ironically. As we shall see, Obamacare is in general a strong indicator of irony when present in posts to the conservative subreddit, but less so in posts to the progressive subreddit. Introduction and Motivation Automated verbal irony detection is a challenging problem.1 But recognizing when an author has intended a statement ironically is practically important for many text classification tasks (e.g., sentiment detection). Previous models for irony detection (Tsur et al., 2010; Lukin and Walker, 2013; Riloff et al., 1 In this paper we will be a bit cavalier in using the terms ‘verbal irony’ and ‘sarcasm’ interchangeably. We recognize that the latter is a special type of the former, the definition of which is difficult to pin down precisely. 2013) have relied predominantly on features intrinsic to the texts to be classified. By contrast, here we propose exploiting contextualizing information, which is often available for web-based classification tasks. More specifically, we exploit signal gleaned from the conversational threads to which comments belong. Our approach capitalizes on the intu"
P15-1100,P09-1054,0,0.0243295,"feature weights, which is in contrast to the elastic net, which imposes the composite penalty to all feature weights. One can view this as using the regularizer to encourage a sparsity pattern specific to the task at hand. 2 Note that we apply both `1 and `2 penalties to the features in I and T . 3.3 Inference We fit this model via Stochastic Gradient Descent (SGD).3 During each update, we impose both the squared `2 and `1 penalties; the latter is applied only to the contextual/interaction features in I and T . For the `1 penalty, we adopt the cumulative truncated gradient method proposed by Tsuruoka et al. (2009). 4 4.1 Experimental Setup Datasets For our development dataset, we used a subset of the reddit irony corpus (Wallace et al., 2014) comprising annotated comments from the progressive and conservative subreddits. We also report results from experiments performed using a separate, held-out portion of this data, which we did not use during model refinement. Furthermore, we later present results on comments from the atheism and Christianity subreddits (we did not use this data during model development, either). The development dataset includes 1,825 annotated comments (876 and 949 from the progres"
P15-1100,P14-2084,1,0.700577,"(or can be derived) for comments posted to different mediums on the web: for example on Twitter we know who a user follows; and on YouTube we know the channels to which videos belong. 2 2.1 Exploiting context Communities and sentiment As discussed above, a shortcoming with existing models for detecting sarcasm/verbal irony on the web is their failure to capitalize on contextualizing information. But such information is critical to discerning irony. A large body of work on the use and interpretation of verbal irony supports this supposition (Grice, 1975; Clark and Gerrig, 1984; Wallace, 2013; Wallace et al., 2014). Individuals will be more likely, in general, to use sarcasm when discussing specific entities. Which entities will depend in part on the community to which the individual belongs. As a proxy for user community, here we leverage the subreddits to which comments were posted. Sentiment may also play an important role. In general, verbal irony is almost always used to convey negative views via ostensibly positive utterances (Sperber and Wilson, 1981). And recent work (Riloff et al., 2013) has exploited features based on sentiment to improve irony detection. To summarize: when assuming an ironic"
P15-1100,P14-1074,0,0.0119202,"approaches still rely on features intrinsic to comments; i.e., they do not attempt to capitalize on contextualizing features external to the comment text. This means that there will necessarily be certain (subtle) ironies that escape detection by such approaches. For example, without any additional information about the speaker, it would be impossible to deduce whether the comment “Obamacare is a great program” is intended sarcastically. Other related recent work has shown the promise of sparse models, both for prediction and interpretation (Eisenstein et al., 2011a; Eisenstein et al., 2011b; Yogatama and Smith, 2014a). Yogatama (2014a; 2014b), e.g., has leveraged the group lasso approach to impose ‘structured’ sparsity on feature weights. Our work here may similarly be viewed as assuming a specific sparsity pattern (specifically that feature weights for ‘interaction features’ will be sparse) and expressing this via regularization. 7 Conclusions and Future Directions We have shown that we can leverage contextualizing information to improve identification of verbal irony in online comments. This is in contrast to previous models, which have relied predominantly on features that are intrinsic to the texts t"
P86-1003,P85-1003,0,0.175425,"lap. 3. allow the exact time of event to be unfixed until it is pinpointed based on contextual information or adverbial modification. 4. allow reference to points and intervals of time (eg. precisely at 3 PM VS. for 5 hours). This work has been supported in part by the National Science Foundation under grants IST 8416034 and IST 8515005, and Office of Naval Research under grant N00014-79-C-0529. 5. allow parsing of temporal information in sentences to be simple and compositional. These criteria were used to judge previous temporal representation research (Bruce (1972), Hornstein (1977, 1981), Yip (1985)). None fulfilled all five criteria. The criteria will also be used to judge the representations developed here. Tense The representations for tense, adverbs, and temporal connectives developed here is based on McDermott's (1982) temporal logic. McDermott's ""point-based"" temporal logic was chosen because it is not unusual to talk about the beginning and end points of a period of time or an event. In fact, the semantics of tense developed here relate the endpoints o f events in sentences. This representation of tense provides a flexibility not found in many other representations of tense (eg. ("
P98-2182,J93-1003,0,0.0114492,"structure; even relatively common nouns m~v not occur in the corpus more than a handful of times in such a context. The two figures of merit t h a t we employ, one to select and one to produce a final rank, use the following two counts for each noun: 1. a noun's co-occurrences with seed words 2. a noun's co-occurrences with any word To select new seed words, we take the ratio of count 1 to count 2 for the noun in question. This is similar to the figure of merit used in R&:S, and also tends to promote low frequency nouns. For the final ranking, we chose the log likelihood statistic outlined in Dunning (1993), which is based upon the co-occurrence counts of all nouns (see Dunning for details). This statistic essentially measures how surprising the given pattern of co-occurrence would be if the distributions were completely random. For instance, suppose t h a t two words occur forty times each, and they co-occur twenty times in a millionword corpus. This would be more surprising for two completely random distributions than if they had each occurred twice and had always co-occurred. A simple probability does not capture this fact. The rationale for using two different statistics for this task is t h"
P98-2182,P95-1007,0,0.011228,"f the resulting compound has already been output, the entry is skipped. Each noun is evaluated as follows: First, the head of t h a t noun is determined. To get a sense of what is meant here, consider the following compound: nuclear-powered aircraft carrier. In evaluating the word nuclearpowered, it is unclear if this word is attached to aircraft or to carrier. While we know t h a t the head of the entire compound is carrier, in order to properly evaluate the word in question, we must determine which of the words following it is its head. This is done, in the spirit of the Dependency Model of Lauer (1995), by selecting the noun to its right in the compound with the highest probability of occuring with the word in question when occurring in a noun compound. (In the case t h a t two nouns have the same probability, the rightmost noun is chosen.) Once the head of the word is determined, the ratio of count 1 (with the head noun chosen) to count 2 is compared to an empirically set cutoff. If it falls below t h a t cutoff, it is omitted. If it does not fall below the cutoff, then it is kept (provided its head noun is not later omitted). 6 Outline of the algorithm The input to the algorithm is a pars"
P98-2182,J93-2004,0,0.0327198,"onents: one to deal with conjunctions, lists, and appositives; and the other to deal with noun compounds. All compound nouns in the former constructions are represented by the head of the compound. We made the simplifying assumptions t h a t a compound noun is a string of consecutive nouns (or, in certain cases, adjectives - see discussion below), and t h a t the head of the compound is the rightmost noun. To identify conjunctions, lists, and appositives, we first parsed the corpus, using an efficient statistical parser (Charniak et al., 1998), trMned on the Penn Wall Street Journal Treebank (Marcus et al., 1993). We defined cooccurrence in these constructions using the standard definitions of dominance and precedence. The relation is stipulated to be transitive, so t h a t all head nouns in a list co-occur with each other (e.g. in the phrase planes, trains, and automobiles all three nouns are counted as co-occuring with each other). Two head nouns co-occur in this algorithm if they meet the following four conditions: 1. they are both dominated by a common NP node 2. no dominating S or VP nodes are dominated by t h a t same NP node 3. all head nouns t h a t precede one, precede the other 4. there is a"
P98-2182,W97-0313,0,0.511932,"ple, both Escort and Chinook are (among other things) types of vehicles (a car and a helicopter, respectively), but neither are cited as so in Wordnet. Manually building domain-specific lexicons can be a costly, time-consuming affair. Utilizing existing resources, such as on-line corpora, to aid in this task could improve performance both by decreasing the time to construct the lexicon and by improving its quality. Extracting semantic information from word co-occurrence statistics has been effective, particularly for sense disambiguation (Schiitze, 1992; Gale et al., 1992; Yarowsky, 1995). In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal category membership, for the purpose of aiding in the construction of semantic lexicons. Generically, their algorithm can be outlined as follows: 1. For a given category, choose a small set of exemplars (or 'seed words') 2. Count co-occurrence of words and seed words within a corpus 3. Use a figure of merit based upon these counts to select new seed words 4. Return to step 2 and iterate n times 5. Use a figure of merit to rank words for category membership and o u t p u t a ranked list Our algorithm uses roughly this same generic st"
P98-2182,P95-1026,0,0.00787085,"inadequate. For example, both Escort and Chinook are (among other things) types of vehicles (a car and a helicopter, respectively), but neither are cited as so in Wordnet. Manually building domain-specific lexicons can be a costly, time-consuming affair. Utilizing existing resources, such as on-line corpora, to aid in this task could improve performance both by decreasing the time to construct the lexicon and by improving its quality. Extracting semantic information from word co-occurrence statistics has been effective, particularly for sense disambiguation (Schiitze, 1992; Gale et al., 1992; Yarowsky, 1995). In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal category membership, for the purpose of aiding in the construction of semantic lexicons. Generically, their algorithm can be outlined as follows: 1. For a given category, choose a small set of exemplars (or 'seed words') 2. Count co-occurrence of words and seed words within a corpus 3. Use a figure of merit based upon these counts to select new seed words 4. Return to step 2 and iterate n times 5. Use a figure of merit to rank words for category membership and o u t p u t a ranked list Our algorithm us"
P99-1008,C92-2082,0,0.563178,"ed as divided and which touser and added to an existing ontology (e.g., Word- gether constitute the whole."" The vagueness of this Net), or used as a part of a rough semantic lexicon. definition translates into a lack of guidance on exactly To the best of our knowledge, there is no published what constitutes a part, which in turn translates into work on automatically finding parts from unlabeled some doubts about evaluating the results of any procorpora. Casting our nets wider, the work most sim- cedure that claims to find them. More specifically, ilar to what we present here is that by Hearst [2] on note that the definition does not claim that parts acquisition of hyponyms (""isa"" relations). In that pa- must be physical objects. Thus, say, ""novel"" might per Hearst (a) finds lexical correlates to the hyponym have ""plot"" as a part. relations by looking in text for cases where known hyIn this study we handle this problem by asking inponyms appear in proximity (e.g., in the construction formants which words in a list are parts of some target (NP, NP and (NP other NN)) as in ""boats, cars, and word, and then declaring majority opinion to be corother vehicles""), (b) tests the proposed patter"
P99-1008,W97-0313,0,0.370235,"parts according to some appropriate metric. We took some care in the selection of this metric. At an intuitive level the metric should be something like p(w [ p). (Here and in what follows w denotes the outcome of the random variable generating wholes, and p the outcome for parts. W(w) states that w appears in the patterns AB as a whole, while P(p) states that p appears as a part.) Metrics of the form p(w I P) have the desirable property that they are invariant over p with radically different base frequencies, and for this reason have been widely used in corpus-based lexical semantic research [3,6,9]. However, in making this intuitive idea someone more precise we found two closely related versions: p(w, W(w) IP) p(w, w(~,) I p, e(p)) We call metrics based on the first of these ""loosely conditioned"" and those based on the second ""strongly conditioned"". While invariance with respect to frequency is generally a good property, such invariant metrics can lead to bad results when used with sparse data. In particular, if a part word p has occurred only once in the data in the AB patterns, then perforce p(w [ P) = 1 for the entity w with which it is paired. Thus this metric must be tempered to ta"
P99-1008,P98-2127,0,0.290468,"Missing"
P99-1008,P98-2182,1,0.515418,"parts according to some appropriate metric. We took some care in the selection of this metric. At an intuitive level the metric should be something like p(w [ p). (Here and in what follows w denotes the outcome of the random variable generating wholes, and p the outcome for parts. W(w) states that w appears in the patterns AB as a whole, while P(p) states that p appears as a part.) Metrics of the form p(w I P) have the desirable property that they are invariant over p with radically different base frequencies, and for this reason have been widely used in corpus-based lexical semantic research [3,6,9]. However, in making this intuitive idea someone more precise we found two closely related versions: p(w, W(w) IP) p(w, w(~,) I p, e(p)) We call metrics based on the first of these ""loosely conditioned"" and those based on the second ""strongly conditioned"". While invariance with respect to frequency is generally a good property, such invariant metrics can lead to bad results when used with sparse data. In particular, if a part word p has occurred only once in the data in the AB patterns, then perforce p(w [ P) = 1 for the entity w with which it is paired. Thus this metric must be tempered to ta"
P99-1008,P97-1023,0,0.0330245,"Missing"
P99-1008,P98-2180,0,0.0608368,"Missing"
P99-1008,C98-2175,0,\N,Missing
P99-1008,C98-2177,1,\N,Missing
P99-1008,C98-2122,0,\N,Missing
P99-1066,J98-2004,1,0.834269,"1970) suggests that by ordering the agenda one can find a parse without resorting to an exhaustive search. The introduction of statistical parsing brought with an obvious tactic for ranking the agenda: (Bobrow, 1990) and (Chitrao and Grishman, 1990) first used probabilistic context free grammars (PCFGs) to generate probabilities for use in a figure of merit (FOM). Later work introduced other FOMs formed from PCFG data (Kochman and Kupin, 1991); (Magerman and Marcus, 1991); and (Miller and Fox, 1994). More recently, we have seen parse times lowered by several orders of magnitude. The (Caraballo and Charniak, 1998) article considers a number of different figures of merit for ordering the agenda, and ultimately recommends one that reduces the number of edges required for a full parse into the thousands. (Goldwater et al., 1998) (henceforth [Gold98]) introduces an edge-based technique, (instead of constituent-based), which drops the average edge count into the hundreds. However, if we establish &quot;perfection&quot; as the minimum number of edges needed to generate the correct parse 47.5 edges on average in our corpus--we can hope for still more improvement. This paper looks at two new figures of merit, both of wh"
P99-1066,W98-1115,1,0.765203,"90) and (Chitrao and Grishman, 1990) first used probabilistic context free grammars (PCFGs) to generate probabilities for use in a figure of merit (FOM). Later work introduced other FOMs formed from PCFG data (Kochman and Kupin, 1991); (Magerman and Marcus, 1991); and (Miller and Fox, 1994). More recently, we have seen parse times lowered by several orders of magnitude. The (Caraballo and Charniak, 1998) article considers a number of different figures of merit for ordering the agenda, and ultimately recommends one that reduces the number of edges required for a full parse into the thousands. (Goldwater et al., 1998) (henceforth [Gold98]) introduces an edge-based technique, (instead of constituent-based), which drops the average edge count into the hundreds. However, if we establish &quot;perfection&quot; as the minimum number of edges needed to generate the correct parse 47.5 edges on average in our corpus--we can hope for still more improvement. This paper looks at two new figures of merit, both of which take the [Gold98] figure (of &quot;independent&quot; merit) as a starting point in cMculating a new figure of merit for each edge, taking into account some additional information. Our work further lowers the average edge c"
P99-1066,H91-1045,0,0.0223268,"The exhaustion of the agenda definitively marks the completion of the parsing algorithm, but the parse needn&apos;t take that long; Mready in the early work on chart parsing, (Kay, 1970) suggests that by ordering the agenda one can find a parse without resorting to an exhaustive search. The introduction of statistical parsing brought with an obvious tactic for ranking the agenda: (Bobrow, 1990) and (Chitrao and Grishman, 1990) first used probabilistic context free grammars (PCFGs) to generate probabilities for use in a figure of merit (FOM). Later work introduced other FOMs formed from PCFG data (Kochman and Kupin, 1991); (Magerman and Marcus, 1991); and (Miller and Fox, 1994). More recently, we have seen parse times lowered by several orders of magnitude. The (Caraballo and Charniak, 1998) article considers a number of different figures of merit for ordering the agenda, and ultimately recommends one that reduces the number of edges required for a full parse into the thousands. (Goldwater et al., 1998) (henceforth [Gold98]) introduces an edge-based technique, (instead of constituent-based), which drops the average edge count into the hundreds. However, if we establish &quot;perfection&quot; as the minimum number of edg"
P99-1066,H91-1044,0,0.0180477,"da definitively marks the completion of the parsing algorithm, but the parse needn&apos;t take that long; Mready in the early work on chart parsing, (Kay, 1970) suggests that by ordering the agenda one can find a parse without resorting to an exhaustive search. The introduction of statistical parsing brought with an obvious tactic for ranking the agenda: (Bobrow, 1990) and (Chitrao and Grishman, 1990) first used probabilistic context free grammars (PCFGs) to generate probabilities for use in a figure of merit (FOM). Later work introduced other FOMs formed from PCFG data (Kochman and Kupin, 1991); (Magerman and Marcus, 1991); and (Miller and Fox, 1994). More recently, we have seen parse times lowered by several orders of magnitude. The (Caraballo and Charniak, 1998) article considers a number of different figures of merit for ordering the agenda, and ultimately recommends one that reduces the number of edges required for a full parse into the thousands. (Goldwater et al., 1998) (henceforth [Gold98]) introduces an edge-based technique, (instead of constituent-based), which drops the average edge count into the hundreds. However, if we establish &quot;perfection&quot; as the minimum number of edges needed to generate the cor"
P99-1066,H94-1051,0,0.0185023,"ion of the parsing algorithm, but the parse needn&apos;t take that long; Mready in the early work on chart parsing, (Kay, 1970) suggests that by ordering the agenda one can find a parse without resorting to an exhaustive search. The introduction of statistical parsing brought with an obvious tactic for ranking the agenda: (Bobrow, 1990) and (Chitrao and Grishman, 1990) first used probabilistic context free grammars (PCFGs) to generate probabilities for use in a figure of merit (FOM). Later work introduced other FOMs formed from PCFG data (Kochman and Kupin, 1991); (Magerman and Marcus, 1991); and (Miller and Fox, 1994). More recently, we have seen parse times lowered by several orders of magnitude. The (Caraballo and Charniak, 1998) article considers a number of different figures of merit for ordering the agenda, and ultimately recommends one that reduces the number of edges required for a full parse into the thousands. (Goldwater et al., 1998) (henceforth [Gold98]) introduces an edge-based technique, (instead of constituent-based), which drops the average edge count into the hundreds. However, if we establish &quot;perfection&quot; as the minimum number of edges needed to generate the correct parse 47.5 edges on ave"
P99-1066,H91-1042,0,\N,Missing
P99-1066,H90-1053,0,\N,Missing
ringger-etal-2004-using,A00-2018,1,\N,Missing
ringger-etal-2004-using,J93-2004,0,\N,Missing
ringger-etal-2004-using,J03-4003,0,\N,Missing
ringger-etal-2004-using,P02-1034,0,\N,Missing
ringger-etal-2004-using,P02-1035,0,\N,Missing
ringger-etal-2004-using,P95-1037,0,\N,Missing
roark-etal-2006-sparseval,A00-2018,1,\N,Missing
roark-etal-2006-sparseval,J93-2004,0,\N,Missing
roark-etal-2006-sparseval,P97-1003,0,\N,Missing
roark-etal-2006-sparseval,N01-1016,1,\N,Missing
roark-etal-2006-sparseval,N04-4032,1,\N,Missing
roark-etal-2006-sparseval,J01-2004,1,\N,Missing
W00-0601,A00-2018,1,0.202241,"Missing"
W00-0601,W98-1119,1,0.83041,"Missing"
W00-0601,P99-1042,0,0.652177,"described herein). We discuss a variety of techniques that tend to give small improvements, ranging from the fairly simple (give verbs more weight in answer selection) to the fairly complex (use specific techniques for answering specific kinds of questions). 1 Introduction CS241, the graduate course in statistical language processing at Brown University, had as its class project the creation of programs to answer reading-comprehension tests. In particular, we used the RemediaT M reading comprehension test data as annotated by a group at MITRE Corporation, henceforth called the Deep Read group [3]. The class divided itself into four groups with sizes ranging from two to four students. In the first half of the semester the goal was to reproduce the results of Deep Read and of one aother. After this learning and debugging period the groups were encouraged to think of and implement new ideas. The Deep Read group provided us with an on-line version of the Remedia material along with several marked up versions of * This research was supported in part by NSF grant LIS SBR 9720368. Thanks to Marc Light and the group at MITRE Corporation for providing the online versions of the reading compreh"
W00-0601,C96-1047,0,0.0605083,"Missing"
W00-1604,J98-2004,1,\N,Missing
W00-1604,A00-2018,1,\N,Missing
W00-1604,J98-4004,0,\N,Missing
W00-1604,J93-2004,0,\N,Missing
W00-1604,W97-0301,0,\N,Missing
W00-1604,P97-1003,0,\N,Missing
W00-1604,J01-2004,1,\N,Missing
W00-1604,W00-1603,0,\N,Missing
W00-1604,P99-1066,1,\N,Missing
W02-1007,A00-2018,1,0.809791,"9 5 0.429 4 0.343 4 0.343 3 0.257 3 0.257 3 0.257 3 0.257 3 0.257 3 0.257 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 2 1 1 0.171 0.171 0.085 0.085 1 0.085 1 0.085 Table 2: The 40 Most Common Parentheticals erty accounts for their observation that removing these dis uencies does not help in language modeling perplexity results. This strongly suggests that INTJ/PRN location information in speech text might in fact, improve parsing performance by helping the parser locate constituent boundaries with high accuracy. That is, a statistic parser such as [1] or [3] when trained on parsed Switchboard text with these phenomena left in, might learn the statistical correlations between them and phrase boundaries just as they are obviously learning the correlations between punctuation and phrase boundaries in written text. In this paper then we wish to determine if the presence of INTJs and PRNs do help parsing, at least for one state-of-the-art statistical parser [1]. 2 Experimental Design The experimental design used was more complicated than we initially expected. We had anticipated that the experiments would be conducted analogously to the 
o pun"
W02-1007,N01-1016,1,0.909019,"Missing"
W02-1007,P97-1003,0,0.197375,"29 4 0.343 4 0.343 3 0.257 3 0.257 3 0.257 3 0.257 3 0.257 3 0.257 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 2 1 1 0.171 0.171 0.085 0.085 1 0.085 1 0.085 Table 2: The 40 Most Common Parentheticals erty accounts for their observation that removing these dis uencies does not help in language modeling perplexity results. This strongly suggests that INTJ/PRN location information in speech text might in fact, improve parsing performance by helping the parser locate constituent boundaries with high accuracy. That is, a statistic parser such as [1] or [3] when trained on parsed Switchboard text with these phenomena left in, might learn the statistical correlations between them and phrase boundaries just as they are obviously learning the correlations between punctuation and phrase boundaries in written text. In this paper then we wish to determine if the presence of INTJs and PRNs do help parsing, at least for one state-of-the-art statistical parser [1]. 2 Experimental Design The experimental design used was more complicated than we initially expected. We had anticipated that the experiments would be conducted analogously to the 
o punctuatio"
W03-1009,P02-1026,1,0.873864,"e sentences the discourse context is often very important. The later sentences in the discourse contain references to the entities in the preceding sentences, and this fact is often useful, e.g., in caching for language modeling (Goodman, 2001). The indirect influence of the context, however, can be observed even when a sentence is taken as a stand-alone unit, i.e., without its context. It is possible to distinguish between a set of earlier sentences and a set of later sentences without any direct comparison by computing certain local statistics of individual sentences, such as their entropy (Genzel and Charniak, 2002). In this work we provide additional evidence for this hypothesis and investigate other sentence statistics. 1.1 Entropy Rate Constancy Entropy, as a measure of information, is often used in the communication theory. If humans have evolved to communicate in the most efficient way (some evidence for that is provided by Plotkin and Nowak (2000)), then they would communicate in such a way that the entropy rate would be constant, namely, equal to the channel capacity (Shannon, 1948). In our previous work (Genzel and Charniak, 2002) we propose that entropy rate is indeed constant in human communica"
W03-1009,J93-2004,0,0.0267524,"Corr. coef. −0.342 ± 0.014 0.073 ± 0.016 0.175 ± 0.015 0.261 ± 0.013 0.328 ± 0.012 0.365 ± 0.013 0.371 ± 0.012 0.391 ± 0.012 0.409 ± 0.011 0.411 ± 0.013 0.433 ± 0.047 0.445 ± 0.010 0.478 ± 0.011 0.512 ± 0.004 0.517 ± 0.007 0.521 ± 0.010 0.541 ± 0.009 0.541 ± 0.008 0.598 ± 0.007 0.619 ± 0.006 0.622 ± 0.009 0.676 ± 0.007 0.678 ± 0.007 0.688 ± 0.004 0.774 ± 0.002 0.850 ± 0.002 0.894 ± 0.001 Table 1: Correlation coefficient for different genres trees and investigate if any statistics show a significant change with the sentence number. 4.1 Experimental Setup We use the whole Penn Treebank corpus (Marcus et al., 1993) as our data set. This corpus contains about 50000 parsed sentences. Many of the statistics we wish to compute are very sensitive to the length of the sentence. For example, the depth of the tree is almost linearly related to the sentence length. This is important because the average length of the sentence varies with the sentence number. To make sure we exclude the effect of the sentence length, we need to normalize for it. We proceed in the following way. Let T be the set of trees, and f : T → R be some statistic of a tree. Let l(t) be the length of the underlying sentence for 1.015 1.01 Adj"
W06-1636,N03-1014,0,0.0661366,"on automatically. Our results are not quite as good as those carefully created by hand, but they are close (84.8 vs 85.7). 1 Introduction and Previous Research It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank (Marcus et al., 1993) labels (e.g., S, N P , etc.) (Charniak, 1996). Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003; Klein and Manning, 2003; Matsuzaki et al., 2005) (and others). One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003). Rather than imagining the parser roaming around the tree for picking up the infor301 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301–307, c Sydney, July 2006. 2006 Association for Computational Linguistics splitting them. We also consider a much wider range of contextual information than just parent phrase-markers."
W06-1636,J98-4004,1,0.908409,"to learn the “right” combination automatically. Our results are not quite as good as those carefully created by hand, but they are close (84.8 vs 85.7). 1 Introduction and Previous Research It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank (Marcus et al., 1993) labels (e.g., S, N P , etc.) (Charniak, 1996). Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003; Klein and Manning, 2003; Matsuzaki et al., 2005) (and others). One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003). Rather than imagining the parser roaming around the tree for picking up the infor301 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301–307, c Sydney, July 2006. 2006 Association for Computational Linguistics splitting them. We also consider a much wider range of contextual information"
W06-1636,P03-1054,0,0.168315,"r relabel the nodes to directly encode this information. Thus rather than have the parser “look” to find out that, say, the parent of some N P is an S, we simply relabel the N P as an N P [S]. This viewpoint is even more compelling if one does not intend to smooth the probabilities. For example, consider p(N P → P RN |N P [S]) If we have no intention of backing off this probability to p(N P → P RN |N P ) we can treat N P [S] as an uninterpreted phrasal category and run all of the standard PCFG algorithms without change. The result is a vastly simplified parser. This is exactly what is done by Klein and Manning (2003). Thus the “phrasal categories” of our title refer to these new, hybrid categories, such as N P [S]. We hope to learn which of these categories work best given that they cannot be made too specific because that would create sparse data problems. The Klein and Manning (2003) parser is an unlexicalized PCFG with various carefully selected context annotations. Their model uses some parent annotations, and marks nodes which initiate or in certain cases conclude unary productions. They also propose linguistically motivated annotations for several tags, including V P , IN , CC,N P and S. This result"
W06-1636,P95-1037,0,0.414689,"by hand. Here we try to automate the process — to learn the “right” combination automatically. Our results are not quite as good as those carefully created by hand, but they are close (84.8 vs 85.7). 1 Introduction and Previous Research It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank (Marcus et al., 1993) labels (e.g., S, N P , etc.) (Charniak, 1996). Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003; Klein and Manning, 2003; Matsuzaki et al., 2005) (and others). One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003). Rather than imagining the parser roaming around the tree for picking up the infor301 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301–307, c Sydney, July 2006. 2006 Association for Computational Linguistics splitting them. We also consider"
W06-1636,J93-2004,0,0.0264133,"create various grammars with different contextual annotations on the non-terminals. These grammars were then used in conjunction with a CKY parser. The authors explored the space of different annotation combinations by hand. Here we try to automate the process — to learn the “right” combination automatically. Our results are not quite as good as those carefully created by hand, but they are close (84.8 vs 85.7). 1 Introduction and Previous Research It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank (Marcus et al., 1993) labels (e.g., S, N P , etc.) (Charniak, 1996). Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003; Klein and Manning, 2003; Matsuzaki et al., 2005) (and others). One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003). Rather than imagining the parser roaming around the tree for picking up the infor301 Proce"
W06-1636,C02-1068,0,0.018351,"n talking about clustering nodes, as we do, Magerman creates a decision tree, but the differences between clustering and decision trees are small. Perhaps a more substantial difference is that by not casting his problem as one of learning phrasal categories Magerman loses all of the free PCFG technology that we can leverage. For instance, Magerman must use heuristic search to find his parses and incurs search errors because of it. We use an efficient CKY algorithm to do exhaustive search in reasonable time. where φs ∈ Φ are clusters of annotated nonterminals and where: C(φi → φj φk . . .) = P Belz (2002) considers the problem in a manner more similar to our approach. Beginning with both a non-annotated grammar and a parent annotated grammar, using a beam search they search the space of grammars which can be attained via merging nonterminals. They guide the search using the performance on parsing (and several other tasks) of the grammar at each stage in the search. In contrast, our approach explores the space of grammars by starting with few nonterminals and (λi ,λj ,λk ...)∈φi ×φj ×φk ... C(λi → λj λk . . .) We refer to the PCFG of some clustering as the clustered grammar. 2.1 Features Most o"
W06-1636,P05-1010,0,0.160362,"e as good as those carefully created by hand, but they are close (84.8 vs 85.7). 1 Introduction and Previous Research It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank (Marcus et al., 1993) labels (e.g., S, N P , etc.) (Charniak, 1996). Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003; Klein and Manning, 2003; Matsuzaki et al., 2005) (and others). One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003). Rather than imagining the parser roaming around the tree for picking up the infor301 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301–307, c Sydney, July 2006. 2006 Association for Computational Linguistics splitting them. We also consider a much wider range of contextual information than just parent phrase-markers. ters of these annotations. Mohri and Roark (2006)"
W06-1636,N06-1020,1,0.792088,"atsuzaki et al., 2005) (and others). One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003). Rather than imagining the parser roaming around the tree for picking up the infor301 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301–307, c Sydney, July 2006. 2006 Association for Computational Linguistics splitting them. We also consider a much wider range of contextual information than just parent phrase-markers. ters of these annotations. Mohri and Roark (2006) tackle this problem by searching for what they call “structural zeros”or sets of events which are individually very likely, but are unlikely to coincide. This is to be contrasted with sets of events that do not appear together simply because of sparse data. They consider a variety of statistical tests to decide whether a joint event is a structural zero. They mark the highest scoring nonterminals that are part of these joint events in the treebank, and use the resulting PCFG. 2 Background A PCFG is a tuple (V, M, µ0 , R, q : R → [0, 1]), where V is a set of terminal symbols; M = {µi } is a se"
W06-1636,A00-2018,1,\N,Missing
W06-1636,W04-3242,0,\N,Missing
W06-1636,P97-1003,0,\N,Missing
W06-1636,J03-4003,0,\N,Missing
W06-1636,P96-1025,0,\N,Missing
W11-0507,P10-1084,0,0.108953,"ds R-1 R-2 R-SU4 34.6 7.3 10.4 35.6 9.9 12.8 35.7 8.9 12.1 35.1 8.3 11.8 ROUGE w/ stopwords R-1 R-2 R-SU4 43.1 9.7 15.3 43.2 11.6 16.6 42.6 11.9 16.8 45.6 11.4 17.2 Table 1: ROUGE scores on the DUC 2007 document sets. The first two rows compare the results of the unigram H IER S UM system with its original and our improved selection metrics. Bolded scores represent where our system has a significant improvement over the orignal H IER S UM. For further comparison, the last two rows show the ROUGE scores of two other state-of-the-art multi-document summarization systems (Toutanova et al., 2007; Celikyilmaz & Hakkani-Tur, 2010). See section 6.2 for more details. marization systems. Both of these systems select sentences discriminatively on many features in order to maximize ROUGE scores. The first, P YTHY (Toutanova et al., 2007), trains on dozens of sentence-level features, such as n-gram and skipgram frequency, named entities, sentence length and position, and also utilizes sentence compression. The second, H YB HS UM (Celikyilmaz & HakkaniTur, 2010), uses a nested Chinese restaurant process (Blei et al., 2004) to model a hierarchical content distribution with more complexity than H IER S UM, and uses a regression"
W11-0507,P06-1039,0,0.422299,"Missing"
W11-0507,W10-0722,0,0.0226759,"Missing"
W11-0507,N09-1041,0,0.412831,"interviewed in one article. Most approaches to this problem generate summaries extractively, selecting whole or partial sentences from the original text, then attempting to piece them together in a coherent manner. Extracted text is selected based on its relevance to the main ideas of the document set. Summaries can be evaluated manually, or with automatic metrics such as ROUGE (Lin, 2004). The use of structured probabilistic topic models has made it possible to represent document set content with increasing complexity (Daum´e & Marcu, 2006; Tang et al., 2009; Celikyilmaz & HakkaniTur, 2010). Haghighi and Vanderwende (2009) demonstrated that these models can improve the quality of generic multi-document summaries over simpler surface models. Their most complex hierarchial model improves summary content by teasing out the words that are not general enough to represent the document set as a whole. Once those words are no longer included in the content word distribution, they are implicitly less likely to appear in the extracted summary as well. But this objective does not sufficiently keep document-specific content from appearing in multi-document summaries. In this paper, we present a selection objective that exp"
W11-0507,N09-2029,0,0.108774,"Missing"
W11-0507,W04-1013,0,0.060902,"mation that is too specific to any one document. For example, a summary of multiple news articles about the Star Wars movies could contain the words “Lucas ”and “Jedi”, but should not contain the name of a fan who was interviewed in one article. Most approaches to this problem generate summaries extractively, selecting whole or partial sentences from the original text, then attempting to piece them together in a coherent manner. Extracted text is selected based on its relevance to the main ideas of the document set. Summaries can be evaluated manually, or with automatic metrics such as ROUGE (Lin, 2004). The use of structured probabilistic topic models has made it possible to represent document set content with increasing complexity (Daum´e & Marcu, 2006; Tang et al., 2009; Celikyilmaz & HakkaniTur, 2010). Haghighi and Vanderwende (2009) demonstrated that these models can improve the quality of generic multi-document summaries over simpler surface models. Their most complex hierarchial model improves summary content by teasing out the words that are not general enough to represent the document set as a whole. Once those words are no longer included in the content word distribution, they are"
W13-1301,P10-1126,0,0.623831,"and-annotated examples for each of the labels. However, the amount of information available on the web continues to grow, the task of organizing and describing visual data becomes increasingly complex. For example, a shopping website might arrange products into broad categories such as “shoes” and “handbags” with each category containing tens of thousands of products that are difficult for users A secondary motivation for this work is to use the image annotations as a component in language generation systems such as for automatic image captioning. We point to examples of previous work such as Feng and Lapata (2010a) where image annotations generated from a topic model are used to help generate full sentences to describe images. Much of the current research in image captioning is limited by the current technology for object recognition in Computer Vision. For example, SBU-Flickr dataset (Ordonez et al., 2011) with 1 million images and captions, is considered to be general-domain but is actually built by querying Flickr using a pre-defined term list related to visual attributes that there are trained recognition systems for. While these systems can accurately generate descriptions for common visual objec"
W13-1301,N10-1125,0,0.288914,"and-annotated examples for each of the labels. However, the amount of information available on the web continues to grow, the task of organizing and describing visual data becomes increasingly complex. For example, a shopping website might arrange products into broad categories such as “shoes” and “handbags” with each category containing tens of thousands of products that are difficult for users A secondary motivation for this work is to use the image annotations as a component in language generation systems such as for automatic image captioning. We point to examples of previous work such as Feng and Lapata (2010a) where image annotations generated from a topic model are used to help generate full sentences to describe images. Much of the current research in image captioning is limited by the current technology for object recognition in Computer Vision. For example, SBU-Flickr dataset (Ordonez et al., 2011) with 1 million images and captions, is considered to be general-domain but is actually built by querying Flickr using a pre-defined term list related to visual attributes that there are trained recognition systems for. While these systems can accurately generate descriptions for common visual objec"
W13-1301,D09-1092,0,0.171199,"Missing"
W13-1301,A97-1004,0,0.179118,"te SIFT features at points of interest and to cluster the SIFT features into discrete “visual terms” using the k-means algorithm. There are 750 visual terms for SIFT features. 4 Color: We use two representations for color, RGB (red, green, blue) and HSV (hue, saturation, value). 25 pixels are sampled from the center 100x100 pixels of the image (to avoid sampling from the background of the image). Those pixel values are also clustered to visual terms using k-means, with 100 visual terms each. Feature Representation 4.1 Text Features The bag-of-words model is used for text. We use Mxterminator (Reynar and Ratnaparkhi, 1997) to split sentences in the captions (in many instances, nothing is done in this step becuase there are no full sentences in the caption), Stanford POS Tagger(Toutanova et al., 2003) to tag words, then include adjectives, adverbs, verbs, and nouns in the topic model (except for proper nouns and common background English words from a stoplist). However, these tags are really more a rough estimate of parts of speech due to the number of incomplete sentences and phrases, and the fact that many of the words used to describe styles or attributes of clothing have different meanings in colloqueal Engl"
W13-1301,N03-1033,0,0.00549827,"use two representations for color, RGB (red, green, blue) and HSV (hue, saturation, value). 25 pixels are sampled from the center 100x100 pixels of the image (to avoid sampling from the background of the image). Those pixel values are also clustered to visual terms using k-means, with 100 visual terms each. Feature Representation 4.1 Text Features The bag-of-words model is used for text. We use Mxterminator (Reynar and Ratnaparkhi, 1997) to split sentences in the captions (in many instances, nothing is done in this step becuase there are no full sentences in the caption), Stanford POS Tagger(Toutanova et al., 2003) to tag words, then include adjectives, adverbs, verbs, and nouns in the topic model (except for proper nouns and common background English words from a stoplist). However, these tags are really more a rough estimate of parts of speech due to the number of incomplete sentences and phrases, and the fact that many of the words used to describe styles or attributes of clothing have different meanings in colloqueal English.3 All tokens are converted to lower case, but there is no stemming or lemmatization. After preprocessing, the size of the shoes text vocabulary is 9578 words, with an average of"
W13-1301,N12-1018,1,\N,Missing
W14-1602,W04-1013,0,0.0114485,"le in our setup. 16 KL (E XTRACTION ) GIST (E XTRACTION ) LM-O NLY (C OMPRESSION ) S YSTEM (C OMPRESSION ) BLEU@1 .2098 .4259 .4780 .4841 Query Image Table 5: BLEU@1 scores of generated captions against human authored captions. Our model (bolded) has the highest BLEU@1 score with significance. Extraction: Shimmering snake-embossed leather upper in a slingback evening dress sandal style with a round open toe. Compression: Shimmering upper in a slingback evening dress sandal style with a round open toe. held-out captions in order to increase the amount of text we have to compare against. ROUGE (Lin, 2004) is a summarization evaluation metric which has also been used to evaluate image captions (Yang et al., 2011). It is usually a recall-oriented measure, but we also report precision and f-measure because our sentence compressions do not improve recall. Table 4 shows ROUGE-2 (bigram) scores computed without stopwords. We observe that our system very significantly improves ROUGE-2 precision of the GIST extracted caption, without significantly reducing recall. While LM-Only also improves precision against GIST extraction, it indiscriminately removes some words which are relevant to the query image"
W14-1602,W09-1801,0,0.0206491,"t|q img , z
 , Φimg , αm) (Nt )
 + αmt ∝ φimg img P qn |t t Nt − 1 + α 4.3 (6) Sentence Compression Let w = w1 , w2 , ..., wn be the words in the extracted caption for q img . For each word, we define a binary decision variable δ, such that δi = 1 if wi is included in the output compression, and δi = 0 otherwise. Our objective is to find values of δ which generate a caption for q img which is both semantically and grammatically correct. We cast this problem as an Integer Linear Program (ILP), which has previously been used for the standard sentence compression task (Clarke and Lapata, 2008; Martins and Smith, 2009). ILP is a mathematical optimization method for determining the optimal values of integer variables in order to maximize an objective given a set of constraints. (1) θznimg (3) θzntxt Observed words are generated according to their probabilities in the modality-specific topics: Modality-specific latent topic assignments z img and z txt are drawn for each of the text words and codewords: zimg ∼ P (zimg |θ) = Y (2) 6 While space limits a more detailed explanation of visual bag-of-word features, Section 5.2 provides a brief overview of the specific visual attributes used in this model. 14 4.3.1 O"
W14-1602,P10-1126,0,0.229879,"understanding images and text. Typically, image understanding systems use supervised algorithms to detect visual entities and concepts in images. However, these typically require accurate hand-labeled training data, which is not available in most specific domains. Ideally, 11 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11–20, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics ate extractive captions. Other models learn correspondences between domain-specific images and natural language captions (Berg et al., 2010; Feng and Lapata, 2010b) but cannot generate descriptions for new images without the use of auxiliary text. Kuznetsova et al. (2013) propose a sentence compression model for editing image captions, but their compression objective is not conditioned on a query image, and their system also requires general-domain visual detections. This paper proposes an image captioning framework which extends these ideas and culminates in the first domain-specific image caption generation system. More broadly, our goal for image caption generation is to work toward less supervised captioning methods which could be used to generate"
W14-1602,W13-1301,1,0.910716,"al., 2010; Ordonez et al., 2011; Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Yu and Siskind, 2013). However, these systems typically rely on a small number of detection types, e.g. the twenty object categories from the PASCAL VOC challenge.2 These object categories include entities which are commonly described in general domain images (people, cars, cats, etc) but these require labeled training data which is not typically available for the visually relevant entities in specific domains. Our caption generation system employs a multimodal topic model from our previous work (Mason and Charniak, 2013) which generates descriptive words, but lacks the spatial structure needed to generate a full sentence caption. Other previous work uses topic models to learn the semantic correspondence between images and labels (e.g. Blei and Jordan (2003)), but learning from natural language descriptions is considerably more difficult because of polysemy, hypernymy, and misalginment between the visual content of an image and the content humans choose to describe. The MixLDA model (Feng and Lapata, 2010b; Feng and Lapata, 2010a) learns from news images and natural language descriptions, but to generate words"
W14-1602,N10-1125,0,0.253197,"understanding images and text. Typically, image understanding systems use supervised algorithms to detect visual entities and concepts in images. However, these typically require accurate hand-labeled training data, which is not available in most specific domains. Ideally, 11 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11–20, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics ate extractive captions. Other models learn correspondences between domain-specific images and natural language captions (Berg et al., 2010; Feng and Lapata, 2010b) but cannot generate descriptions for new images without the use of auxiliary text. Kuznetsova et al. (2013) propose a sentence compression model for editing image captions, but their compression objective is not conditioned on a query image, and their system also requires general-domain visual detections. This paper proposes an image captioning framework which extends these ideas and culminates in the first domain-specific image caption generation system. More broadly, our goal for image caption generation is to work toward less supervised captioning methods which could be used to generate"
W14-1602,N13-2010,1,0.59052,"ive to the query image determine the length of the output. Our objective differs from that of Kuznetsova et al. (2013), who compress image caption sentences with the objective of creating a corpus of generally transferrable image captions. Their compression objective is to maximize the probability of a caption conditioned on the source Image Understanding Recent improvements in state-of-the-art visual object class detections (Felzenszwalb et al., 2010) 1 A research proposal for this framework and other image captioning ideas was previously presented at NAACL Student Research Workshop in 2013 (Mason, 2013). This paper presents a completed project including implementation details and experimental results. 2 http://pascallin.ecs.soton.ac.uk/ challenges/VOC/ 12 Two adjustable buckle straps top a classic rubber rain boot grounded by a thick lug sole for excellent wet-weather traction. Available in Plus Size. Faux snake skin flats with a large crossover buckle at the toe. Padded insole for a comfortable all day fit. Glitter-covered elastic upper in a two-piece dress sandal style with round open toe. Single vamp strap with contrasting trim matching elasticized heel strap crisscrosses at instep. Explo"
W14-1602,D09-1092,0,0.0498781,"Missing"
W14-1602,P03-1054,0,0.00657378,"e cannot compare our system against prior work in general-domain image captioning, because those models use visual detection systems which train on labeled data that is not available in our domain. Compression The sentence compression ILP is implemented using the CPLEX optimization toolkit9 . The language model weighting factor in the objective is λ = 10−3 , which was hand-tuned according to observed output. The trigram language model is trained on training set captions using BerkeleyLM (Pauls and Klein, 2011) with Kneser-Ney smoothing. For the constraints, we use parses from Stanford Parser (Klein and Manning, 2003) and the “semantic head” variation of the Collins headfinder Collins (1999). 6 Evaluation 6.1 Setup We compare the following systems and baselines: KL (E XTRACTION ): The top performing extractive model from Feng and Lapata (2010a), and the second-best captioning model overall. Using estimated topic distributions from our joint model, we extract the source with minimum KL Divergence from q img . 6.2 Automatic Evaluation We perform automatic evaluation using similarity measures between automatically generated and human-authored captions. Note that currently our system and baselines only generat"
W14-1602,E12-1076,0,0.0585342,"Missing"
W14-1602,P12-1038,0,0.047385,"Missing"
W14-1602,P02-1040,0,0.0937532,", but we also report precision and f-measure because our sentence compressions do not improve recall. Table 4 shows ROUGE-2 (bigram) scores computed without stopwords. We observe that our system very significantly improves ROUGE-2 precision of the GIST extracted caption, without significantly reducing recall. While LM-Only also improves precision against GIST extraction, it indiscriminately removes some words which are relevant to the query image. We also observe that GIST extraction strongly outperforms the KL model, which demonstrates the importance of visual structure. We also report BLEU (Papineni et al., 2002) scores, which are the most popularly accepted automatic metric for captioning evaluation (Farhadi et al., 2010; Kulkarni et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Kuznetsova et al., 2013). Results are very similar to the ROUGE-2 precision scores, except the difference between our system and LM-Only is less pronounced because BLEU counts function words, while ROUGE does not. 6.3 GIST Nearest-Neighbor Query Image GIST Nearest-Neighbor Extraction: This sporty sneaker clog keeps foot cool and comfortable and fully supported. Compression: This clog keeps foot comfortable and sup"
W14-1602,P13-2138,0,0.584091,"visual entities and concepts in images. However, these typically require accurate hand-labeled training data, which is not available in most specific domains. Ideally, 11 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11–20, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics ate extractive captions. Other models learn correspondences between domain-specific images and natural language captions (Berg et al., 2010; Feng and Lapata, 2010b) but cannot generate descriptions for new images without the use of auxiliary text. Kuznetsova et al. (2013) propose a sentence compression model for editing image captions, but their compression objective is not conditioned on a query image, and their system also requires general-domain visual detections. This paper proposes an image captioning framework which extends these ideas and culminates in the first domain-specific image caption generation system. More broadly, our goal for image caption generation is to work toward less supervised captioning methods which could be used to generate detailed and accurate descriptions for a variety of long-tail domains of captioned image data, such as in natu"
W14-1602,P11-1027,0,0.0299681,"nguage model (still subject to the constraints). S YSTEM (C OMPRESSION ): Our full system. Unfortunately, we cannot compare our system against prior work in general-domain image captioning, because those models use visual detection systems which train on labeled data that is not available in our domain. Compression The sentence compression ILP is implemented using the CPLEX optimization toolkit9 . The language model weighting factor in the objective is λ = 10−3 , which was hand-tuned according to observed output. The trigram language model is trained on training set captions using BerkeleyLM (Pauls and Klein, 2011) with Kneser-Ney smoothing. For the constraints, we use parses from Stanford Parser (Klein and Manning, 2003) and the “semantic head” variation of the Collins headfinder Collins (1999). 6 Evaluation 6.1 Setup We compare the following systems and baselines: KL (E XTRACTION ): The top performing extractive model from Feng and Lapata (2010a), and the second-best captioning model overall. Using estimated topic distributions from our joint model, we extract the source with minimum KL Divergence from q img . 6.2 Automatic Evaluation We perform automatic evaluation using similarity measures between a"
W14-1602,P05-1036,1,0.748612,"on, captions are generated by retrieving human-authored descriptions from visually similar images. Farhadi et al. (2010) and Ordonez et al. (2011) retrieve whole captions to apply to a query image, while Kuznetsova et al. (2012) generate captions using text retrieved from multiple sources. The descriptions are related to visual concepts in the query image, but these models use visual similarity to approximate textual relevance; they do not model image and textual features jointly. 2.2 2.3 Sentence Compression Typical models for sentence compression (Knight and Marcu, 2002; Furui et al., 2004; Turner and Charniak, 2005; Clarke and Lapata, 2008) have a summarization objective: reduce the length of a source sentence without changing its meaning. In contrast, our objective is to change the meaning of the source sentence, letting its overall correctness relative to the query image determine the length of the output. Our objective differs from that of Kuznetsova et al. (2013), who compress image caption sentences with the objective of creating a corpus of generally transferrable image captions. Their compression objective is to maximize the probability of a caption conditioned on the source Image Understanding R"
W14-1602,P13-1006,0,0.0233006,"detections. This paper proposes an image captioning framework which extends these ideas and culminates in the first domain-specific image caption generation system. More broadly, our goal for image caption generation is to work toward less supervised captioning methods which could be used to generate detailed and accurate descriptions for a variety of long-tail domains of captioned image data, such as in nature and medicine. 2 have enabled much recent work in image caption generation (Farhadi et al., 2010; Ordonez et al., 2011; Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Yu and Siskind, 2013). However, these systems typically rely on a small number of detection types, e.g. the twenty object categories from the PASCAL VOC challenge.2 These object categories include entities which are commonly described in general domain images (people, cars, cats, etc) but these require labeled training data which is not typically available for the visually relevant entities in specific domains. Our caption generation system employs a multimodal topic model from our previous work (Mason and Charniak, 2013) which generates descriptive words, but lacks the spatial structure needed to generate a full"
W14-1602,D11-1041,0,\N,Missing
W14-1602,J03-4003,0,\N,Missing
W14-1815,P96-1041,0,0.371942,"Missing"
W14-1815,W09-0613,0,0.0283489,"any sentence that did not parse to a top level S containing at least one NP and one VP child. Even with such strong filters, we retained over 140K sentences for use as training data, and provide this exact set of parse trees for use in future work.2 Inspired by the application in language education, for our vocabulary list we use the English Vocabulary Profile (Capel, 2012), which predicts student vocabulary at different stages of learning English as a second language. We take the most basic American English vocabulary (the A1 list), and retrieve all inflections for each word using SimpleNLG (Gatt and Reiter, 2009), yielding a vocabulary of 1226 simple words and punctuation. To mitigate noise in the data, we discard any pair of context and outcome that appears only once in the training data, and estimate the parameters of the unconstrained model using EM. 6.1 S PINE D EP unsmoothed S PINE D EP WordNet tences where all words are in-vocab are allowed. The second demands not only that all words are S PINE D EP word2vec 5000 in-vocab, but specifies the inclusion of a single arbitrary word somewhere in the sentence. These Scontraints PINEareDmost EPnatural word2vec 500 in the case of language education, wher"
W14-1815,P13-1045,0,0.0353016,"Missing"
W14-1815,D10-1051,0,0.0248178,"amount of information. This leads to sparse estimates even on large amounts of training data, a problem that can be addressed by smoothing. We identify two complementary types of smoothing, and illustrate them with the following sentences. Related Work The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry (Greene et al., 2010) (Colton et al., 2012) (Jiang and Zhou, 2008) or song lyrics (Wu et al., 2013) (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric. Computational creativity is another subfield of NLG that often does not fix an a priori meaning in ¨ its output. Examples such as Ozbal et al. (2013) and Valitutti et al. (2013) use template filling techniques guided by quantified notions of humor or how catchy a phrase is. Our motivation for generation of mater"
W14-1815,W05-0210,0,0.0245883,"ong lyrics (Wu et al., 2013) (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric. Computational creativity is another subfield of NLG that often does not fix an a priori meaning in ¨ its output. Examples such as Ozbal et al. (2013) and Valitutti et al. (2013) use template filling techniques guided by quantified notions of humor or how catchy a phrase is. Our motivation for generation of material for language education exists in work such as Sumita et al. (2005) and Mostow and Jang (2012), which deal with automatic generation of classic fill in the blank questions. Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences. 3 the set C of contexts c the set O of outcomes o the “Imply” function I(c, o) → List[c ∈ C] M : derivation tree  sentence The furry dog bit me. The cute cat licked me. An unsmoothed bigram model trained on this data can only generate the two sentences verbatim. If, however, we know that the tokens “dog” and “cat” are semantically similar, we can smooth by a"
W14-1815,C08-1048,0,0.0143307,"estimates even on large amounts of training data, a problem that can be addressed by smoothing. We identify two complementary types of smoothing, and illustrate them with the following sentences. Related Work The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry (Greene et al., 2010) (Colton et al., 2012) (Jiang and Zhou, 2008) or song lyrics (Wu et al., 2013) (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric. Computational creativity is another subfield of NLG that often does not fix an a priori meaning in ¨ its output. Examples such as Ozbal et al. (2013) and Valitutti et al. (2013) use template filling techniques guided by quantified notions of humor or how catchy a phrase is. Our motivation for generation of material for language education exists in work suc"
W14-1815,P13-2044,0,0.0260386,"ptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry (Greene et al., 2010) (Colton et al., 2012) (Jiang and Zhou, 2008) or song lyrics (Wu et al., 2013) (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric. Computational creativity is another subfield of NLG that often does not fix an a priori meaning in ¨ its output. Examples such as Ozbal et al. (2013) and Valitutti et al. (2013) use template filling techniques guided by quantified notions of humor or how catchy a phrase is. Our motivation for generation of material for language education exists in work such as Sumita et al. (2005) and Mostow and Jang (2012), which deal with automatic generation of classic fill in the blank questions. Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences. 3 the set C of contexts c the set O of outcomes o the “Imply” function I(c, o) → List[c ∈ C] M : derivation tree  sentence The furry dog bit me. The cute"
W14-1815,P04-1061,0,0.0715579,"Missing"
W14-1815,D13-1011,0,0.05026,"aining data, a problem that can be addressed by smoothing. We identify two complementary types of smoothing, and illustrate them with the following sentences. Related Work The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry (Greene et al., 2010) (Colton et al., 2012) (Jiang and Zhou, 2008) or song lyrics (Wu et al., 2013) (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric. Computational creativity is another subfield of NLG that often does not fix an a priori meaning in ¨ its output. Examples such as Ozbal et al. (2013) and Valitutti et al. (2013) use template filling techniques guided by quantified notions of humor or how catchy a phrase is. Our motivation for generation of material for language education exists in work such as Sumita et al. (2005) and Mos"
W14-1815,N13-1137,0,0.0613691,"Missing"
W14-1815,W12-2016,0,0.0221466,"13) (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric. Computational creativity is another subfield of NLG that often does not fix an a priori meaning in ¨ its output. Examples such as Ozbal et al. (2013) and Valitutti et al. (2013) use template filling techniques guided by quantified notions of humor or how catchy a phrase is. Our motivation for generation of material for language education exists in work such as Sumita et al. (2005) and Mostow and Jang (2012), which deal with automatic generation of classic fill in the blank questions. Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences. 3 the set C of contexts c the set O of outcomes o the “Imply” function I(c, o) → List[c ∈ C] M : derivation tree  sentence The furry dog bit me. The cute cat licked me. An unsmoothed bigram model trained on this data can only generate the two sentences verbatim. If, however, we know that the tokens “dog” and “cat” are semantically similar, we can smooth by assuming the words that foll"
W14-1815,P13-1142,0,0.020784,") which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry (Greene et al., 2010) (Colton et al., 2012) (Jiang and Zhou, 2008) or song lyrics (Wu et al., 2013) (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric. Computational creativity is another subfield of NLG that often does not fix an a priori meaning in ¨ its output. Examples such as Ozbal et al. (2013) and Valitutti et al. (2013) use template filling techniques guided by quantified notions of humor or how catchy a phrase is. Our motivation for generation of material for language education exists in work such as Sumita et al. (2005) and Mostow and Jang (2012), which deal with automatic generation of classic fill in the blank questions. Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences. 3 the set C of contexts c the set O of outcomes o the “Imply” function I(c, o) → List[c ∈ C] M : derivation tree  sentence The"
W14-1815,W09-2006,0,\N,Missing
W96-0212,H90-1053,0,0.114788,"Missing"
W96-0212,J91-3004,0,0.230527,"Missing"
W96-0212,H91-1045,0,0.433656,"Missing"
W96-0212,H91-1044,0,0.568764,"Missing"
W96-0212,P92-1006,0,0.166215,"Missing"
W98-1115,H90-1053,0,0.0422779,"have found that exhaustively parsing maximum-40-word sentences from the Penn II treebank requires an average of about 1.2 million edges per sentence. Numbers like this suggest that any approach that offers the possibility of reducing the work load is well worth pursuing, a fact that has been noted by several researchers. Early on, Kay (1980) suggested the use of the chart agenda for this purpose. More recently, the statistical approach to language processing and the use of probabilistic context-free grammars (PCFGs) has suggested using the PCFG probabilities to create a FOM. Bobrow (1990) and Chitrao and Grishman (1990) introduced best-first PCFG parsing, the approach taken here. Subsequent work has suggested different FOMs built from PCFG probabilities (Miller and Fox. 1994: Kochman and Kupin. 1991: Magerman and Marcus, 1991). Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&C) (Forthcoming). They consider a large number of FOMs, and view them as approximations of some ""ideal"" (but only computable after the fact) FOM. Of these they recommend one as the best of the lot. In this paper we basically adopt both their framework"
W98-1115,P97-1003,0,0.104889,"y more plausible, smaller ones. 6 Conclusion It is worth noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather pure form of P C F G parsing, there is no particular reason to assume that the technique is so limited in its domain of applicability. One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized P C F G s (Charniak, 1997), or even grammars not based upon literal rules, b u t probability distributions that describe how rules are built up from smaller components (Magerman, 1995; Collins, 1997). Clearly further research is warranted. Be this as it may, the take-home lesson from this paper is simple: combining an edge-based agenda with the figure of merit from C & C • is easy to do by simply binarizing the grammar • provides a factor of 20 or so reduction in the number of edges required to find a first parse, and • improves parsing precision and recall over exhaustive parsing. To the best of our knowledge this is currently the most effecient parsing technique for P C F G grammars induced from large tree-banks. As such we strongly recommend this technique to others interested in PCFG"
W98-1115,P96-1024,0,0.142388,"Missing"
W98-1115,W97-0302,0,0.0885422,"1994: Kochman and Kupin. 1991: Magerman and Marcus, 1991). Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&C) (Forthcoming). They consider a large number of FOMs, and view them as approximations of some ""ideal"" (but only computable after the fact) FOM. Of these they recommend one as the best of the lot. In this paper we basically adopt both their framework and their recommended FOM. The next section describes their work in more detail, Besides C&C the work that is most directly comparable to ours is that of Goodman (1997) and Ratnaparki (1997). G o o d m a n uses an FOM that is similar to that of C&C but one that should, in general, be somewhat more accurate. However, both Goodman's and Ratnaparki's work assumes that one is doing a beam search of some sort, rather than a best-first search, and their FOM are unfortunately tied to their frameworks and thus cannot be adopted here. We briefly compare our results to theirs in Section 5. As noted, our paper takes off from that of C&C and uses the same FOM. The major difference is simply that our parser uses the FOM to rank edges (including incomplete edges), rather"
W98-1115,H91-1045,0,0.196176,"Missing"
W98-1115,H91-1044,0,0.0428787,"ssibility of reducing the work load is well worth pursuing, a fact that has been noted by several researchers. Early on, Kay (1980) suggested the use of the chart agenda for this purpose. More recently, the statistical approach to language processing and the use of probabilistic context-free grammars (PCFGs) has suggested using the PCFG probabilities to create a FOM. Bobrow (1990) and Chitrao and Grishman (1990) introduced best-first PCFG parsing, the approach taken here. Subsequent work has suggested different FOMs built from PCFG probabilities (Miller and Fox. 1994: Kochman and Kupin. 1991: Magerman and Marcus, 1991). Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&C) (Forthcoming). They consider a large number of FOMs, and view them as approximations of some ""ideal"" (but only computable after the fact) FOM. Of these they recommend one as the best of the lot. In this paper we basically adopt both their framework and their recommended FOM. The next section describes their work in more detail, Besides C&C the work that is most directly comparable to ours is that of Goodman (1997) and Ratnaparki (1997). G o o d m a n uses"
W98-1115,P95-1037,0,0.0839334,"g other, strictly more plausible, smaller ones. 6 Conclusion It is worth noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather pure form of P C F G parsing, there is no particular reason to assume that the technique is so limited in its domain of applicability. One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized P C F G s (Charniak, 1997), or even grammars not based upon literal rules, b u t probability distributions that describe how rules are built up from smaller components (Magerman, 1995; Collins, 1997). Clearly further research is warranted. Be this as it may, the take-home lesson from this paper is simple: combining an edge-based agenda with the figure of merit from C & C • is easy to do by simply binarizing the grammar • provides a factor of 20 or so reduction in the number of edges required to find a first parse, and • improves parsing precision and recall over exhaustive parsing. To the best of our knowledge this is currently the most effecient parsing technique for P C F G grammars induced from large tree-banks. As such we strongly recommend this technique to others int"
W98-1115,J93-2004,0,0.0255481,"edge B ~ '7- to yield the edge A ~ a B . fl, corresponds to the left-factored productions ' a B ' ~ a B if fl is non-empty or A ~ ' a ' B if fl is empty. Thus in general a single 'new' non-terminal in a C K Y parse using the left-factored grammar abbreviates several incomplete edges in the Earley algorithm. 4 Figure h r/vs. P o p p e d Edges 4o0 3oo 2oo 1.0 1.5 ! 2.0 N o r m a l i z a t i o n constant Figure 2: r] vs. Precision and Recall 76 ~ 74 130 , ! . . . . lO0 The Experiment For our experiment, we used a tree-bank grammar induced from sections 2-21 of the Penn Wall Street Journal text (Marcus et al., 1993), with section 22 reserved for testing. All sentences of length greater than 40 were ignored for testing purposes as done in b o t h C&C and Goodman (1997). We applied the binarization technique described above to the grammar. We chose to measure the amount of work done by the parser in terms of the average number of edges popped off the agenda before finding a parse. This method has the advantage of being platform independent, as well as providing a measure of ""perfection"". Here, perfection is the minimum number of edges we would need to pop off the agenda in order to create the correct parse"
W98-1115,H94-1051,0,0.135544,"Missing"
W98-1115,W97-0301,0,0.025136,"upin. 1991: Magerman and Marcus, 1991). Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&C) (Forthcoming). They consider a large number of FOMs, and view them as approximations of some ""ideal"" (but only computable after the fact) FOM. Of these they recommend one as the best of the lot. In this paper we basically adopt both their framework and their recommended FOM. The next section describes their work in more detail, Besides C&C the work that is most directly comparable to ours is that of Goodman (1997) and Ratnaparki (1997). G o o d m a n uses an FOM that is similar to that of C&C but one that should, in general, be somewhat more accurate. However, both Goodman's and Ratnaparki's work assumes that one is doing a beam search of some sort, rather than a best-first search, and their FOM are unfortunately tied to their frameworks and thus cannot be adopted here. We briefly compare our results to theirs in Section 5. As noted, our paper takes off from that of C&C and uses the same FOM. The major difference is simply that our parser uses the FOM to rank edges (including incomplete edges), rather than simply completed"
W98-1115,H91-1042,0,\N,Missing
W98-1119,P87-1022,0,0.538663,"The last factor we consider is referents&apos; mention count. Noun phrases that are mentioned repeatedly are preferred. The training corpus is marked with the number of times a referent has been mentioned up to that point in the story. Here we are concerned with the probability that a proposed antecedent is correct given that it has been repeated a certain number of times. 162 In effect, we use this probability information to identify the topic of the segment with the belief that the topic is more likely to be referred to by a pronoun. The idea is similar to t h a t used in the centering approach (Brennan et al., 1987) where a continued topic is the highest-ranked candidate for pronominalization. Given the above possible sources of informar tion, we arrive at the following equation, where F(p) denotes a function from pronouns to their antecedents: F(p) = argmaxP( A(p) = alp, h, l~&apos;, t, l, so, d~ A~&apos;) where A(p) is a random variable denoting the referent of the pronoun p and a is a proposed antecedent. In the conditioning events, h is the head constituent above p, l~r is the list of candidate antecedents to be considered, t is the type of phrase of the proposed antecedent (always a noun-phrase in this study)"
W98-1119,J93-1003,0,0.0467707,"o for a pronoun p that maximizes this probability. More formally, we want the program to return our antecedent function F(p), where F(p) = We use [ z [ to denote the number of times z is observed in our training set. 3.1 The gender/animaticity statistics After we have identified the correct antecedents it is a simple counting procedure to compute P(p[wa) where wa is in the correct antecedent for the pronoun p (Note the pronouns are grouped by their gender): P(pl o) = [ wain the antecedent for p [ When there are multiple relevant words in the antecedent we apply the likelihood test designed by Dunning (1993) on all the words in the candidate NP. Given our limited data, the Dunning test tells which word is the most informative, call it w i, and we then use P(p[wi). arg maax P(A(p) = alp, h, 1~, t, l, sp, d: M) = argmaxP(dH[a)P(plwa) 112a e(walh, t,t) e(almo ) P(wolt, t) 3 I correct antecedent at Hobbs distance i i [ correct antecedents 1 The Implementation We use a small portion of the Penn Wall Street Journal Tree-bank as our training corpus. From this data, we collect the three statistics detailed ha the following subsections. 3.0.1 T h e H o b b s a l g o r i t h m The Hobbs algorithm makes a f"
W98-1119,P97-1023,0,0.0109961,"gher the success rate we expect an anaphora resolution system can achieve. 5 Unsupervised Information Learning of Gender The importance of gender information as revealed in the previous experiments caused us to consider automatic methods for estimating the probability that nouns occurring in a large corpus of English text deonote inanimate, masculine or feminine things. The method described here is based on simply counting co-occurrences of pronouns and noun phrases, and thus can employ any method of analysis of the text stream that results in referent/pronoun pairs (cf. (Hatzivassiloglou and McKeown, 1997) for another application in which no explicit indicators are available in the stream). We present two very simple methods for finding referent/pronoun pairs, and also give an application of a salience statistic t h a t can indicate how confident we should be about the predictions the method makes. Following this, we show the results of applying this method to the 21-million-word 1987 Wall Street Journal corpus using two different pronoun reference strategies of varying sophistication, and evaluate their performance using honorifics as reliable gender indicators. The method is a very simple mec"
W98-1119,J93-2004,0,0.0552509,"Missing"
W98-1119,W97-1303,0,0.193208,"er describes a method for using (portions of) t~e aforementioned program to learn automatically the typical gender of English words, information that is itself used in the pronoun resolution program. In particular, the scheme infers the gender of a referent from the gender of the pronouns that 161 refer to it and selects referents using the pronoun anaphora program. We present some typical results as well as the more rigorous results of a blind evaluation of its output. 2 A Probabilistic Model There are many factors, both syntactic and semantic, upon which a pronoun resolution system relies. (Mitkov (1997) does a detailed study on factors in anaphora resolution.) We first discuss the training features we use and then derive the probability equations from them. The first piece of useful information we consider is the distance between the pronoun and the candidate antecedent. Obviously the greater the distance the lower the probability. Secondly, we look at the syntactic situation in which the pronoun finds itself. The most well studied constraints are those involving reflexive pronouns. One classical approach to resolving pronouns in text that takes some syntactic factors into consideration is t"
W98-1119,J94-4002,0,\N,Missing
W99-0609,C92-2082,0,\N,Missing
W99-0609,P99-1008,1,\N,Missing
W99-0609,P98-2182,1,\N,Missing
W99-0609,C98-2177,1,\N,Missing
