C88-1057,W91-0306,0,0.0356632,"Missing"
C88-1057,T75-2013,0,\N,Missing
C90-2035,T75-2013,0,0.176043,"Missing"
C90-2035,P88-1011,0,0.0676156,"Missing"
C90-2035,P88-1012,0,0.312546,"Missing"
C90-2035,C88-1057,1,0.889144,"Missing"
C90-2035,C90-2071,0,0.0109196,"or a specification of the means or plan by which some goal is accomplished. We can ignore for Full I n t e g r a t i o n A l g o r i t h m : Integrate the set of constituent relations with the relations present in the candidate by finding an appropriate gap (variable) in one of the two structures to integrate with the other. Integration is an augmentation of unification. In order to handle more complex constructions, the operation would need to be augmented further, adding more inferential power. For example, a certain class of inference is required to integrate constructions like DoubleNoun (Wu 1990), where the relation between the constituents can be quite indirect and contextually-influenced. Such an integration algorithm might also need to make the kind 4This occurs in such common phenomena as WHmovement, Y-Movement, Topicalization, and other phenomenon classically analyzed as movement rules, where there is some long-distance link between some element and the valence-bearing predicate into which it integrates. 202 4 ol! metaphoric inferences studied by Martin (1988), ok: the abductive inferences of Charniak & Goldman (11988) or Hobbs et al. (1988). But rather than making these inferenc"
C98-2179,A97-1052,0,0.0971389,"Missing"
C98-2179,P96-1025,0,0.0751001,"Missing"
C98-2179,P97-1003,0,0.0913407,"Missing"
C98-2179,C94-2123,0,0.0476071,"Missing"
C98-2179,P98-1071,0,0.0382711,"Missing"
C98-2179,C94-1024,0,0.0602316,"Missing"
C98-2179,J93-2004,0,0.0523819,"Missing"
C98-2179,H94-1020,0,0.0336472,"Missing"
C98-2179,W93-0109,0,0.115022,"Missing"
C98-2179,P93-1032,0,\N,Missing
C98-2179,J93-2001,0,\N,Missing
C98-2179,C98-1068,0,\N,Missing
C98-2179,H92-1045,0,\N,Missing
D07-1107,W02-0805,0,0.426344,"Missing"
D07-1107,W04-0827,0,0.0376346,"Missing"
D07-1107,C94-2113,0,0.117186,"ed evaluation data ing, and the gold standard datasets that we use in our work. In Section 3 we introduce our battery of features; in Section 4 we show how to extend our sensemerging model to cluster full taxonomies like WordNet. In Section 5 we evaluate our classifier against thirteen previously proposed methods. 2 Background A wide number of manual and automatic techniques have been proposed for clustering sense inventories and mapping between sense inventories of different granularities. Much work has gone into methods for measuring synset similarity; early work in this direction includes (Dolan, 1994), which attempted to discover sense similarities between dictionary senses. A variety of synset similarity measures based on properties of WordNet itself have been proposed; nine such measures are discussed in (Pedersen et al., 2004), including gloss-based heuristics (Lesk, 1986; Banerjee and Pedersen, 2003), information-content based measures (Resnik, 1995; Lin, 1998; Jiang and Conrath, 1997), and others. Other approaches have used specific cues from WordNet structure to inform the construction of semantic rules; for example, (Peters et al., 1998) suggest clustering two senses based on a wide"
D07-1107,W98-0705,0,0.20834,"of our classifier, and we make available several automatically sense-clustered WordNets of various sense granularities. 1 Introduction Defining a discrete inventory of senses for a word is extremely difficult (Kilgarriff, 1997; Hanks, 2000; Palmer et al., 2005). Perhaps the greatest obstacle is the dynamic nature of sense definition: the correct granularity for word senses depends on the application. For language learners, a fine-grained set of word senses may help in learning subtle distinctions, while coarsely-defined senses are probably more useful in NLP tasks like information retrieval (Gonzalo et al., 1998), query expansion (Moldovan and Mihalcea, 2000), and WSD (Resnik and Yarowsky, 1999; Palmer et al., 2005). Lexical resources such as WordNet (Fellbaum, 1998) use extremely fine-grained notions of word sense, which carefully capture even minor distinctions between different possible word senses (e.g., In response to these challenges, we propose a new algorithm for clustering large-scale sense hierarchies like WordNet. Our algorithm is based on a supervised classifier that learns to make graduated judgments corresponding to the estimated probability that each particular sense pair should be merg"
D07-1107,N06-2015,0,0.0233271,"o et al., 1998; Chugur et al., 2002). 2.1 Gold standard sense clustering data Our approach for learning how to merge senses relies upon the availability of labeled judgments of sense relatedness. In this work we focus on two datasets of hand-labeled sense groupings for WordNet: first, a dataset of sense groupings over nouns, verbs, and adjectives provided as part of the S ENSEVAL -2 English lexical sample WSD task (Kilgarriff, 2001), and second, a corpus-driven mapping of nouns and verbs in WordNet 2.1 to the Omega Ontology (Philpot et al., 2005), produced as part of the O NTO N OTES project (Hovy et al., 2006). A wide variety of semantic and syntactic criteria were used to produce the S ENSEVAL -2 groupings (Palmer et al., 2004; Palmer et al., 2005); this data covers all senses of 411 nouns, 519 verbs, and 257 adjectives, and has been used as gold standard sense clustering data in previous work (Agirre and Lopez, 2003; McCarthy, 2006)2 . The number of judgments within this data (after mapping to WordNet 2.1) is displayed in Table 1. Due to a lack of interannotator agreement data for this dataset, (McCarthy, 2006) performed an annotation study using three labelers on a 20-noun subset of the S ENSEVA"
D07-1107,O97-1002,0,0.0761328,"een proposed for clustering sense inventories and mapping between sense inventories of different granularities. Much work has gone into methods for measuring synset similarity; early work in this direction includes (Dolan, 1994), which attempted to discover sense similarities between dictionary senses. A variety of synset similarity measures based on properties of WordNet itself have been proposed; nine such measures are discussed in (Pedersen et al., 2004), including gloss-based heuristics (Lesk, 1986; Banerjee and Pedersen, 2003), information-content based measures (Resnik, 1995; Lin, 1998; Jiang and Conrath, 1997), and others. Other approaches have used specific cues from WordNet structure to inform the construction of semantic rules; for example, (Peters et al., 1998) suggest clustering two senses based on a wide variety of structural cues from WordNet, including if they are twins (if two synsets share more than one word in their synonym list) or if they represent an example of autohyponymy (if one sense is the direct descendant of the other). (Mihalcea and Moldovan, 2001) implements six semantic rules, using twin and autohyponym features, in addition to other WordNet-structure-based rules such as whe"
D07-1107,S01-1004,0,0.0322912,"tionary. Other work has attempted to exploit translational equivalences of WordNet senses in other languages, for example using foreign language WordNet interlingual indexes (Gonzalo et al., 1998; Chugur et al., 2002). 2.1 Gold standard sense clustering data Our approach for learning how to merge senses relies upon the availability of labeled judgments of sense relatedness. In this work we focus on two datasets of hand-labeled sense groupings for WordNet: first, a dataset of sense groupings over nouns, verbs, and adjectives provided as part of the S ENSEVAL -2 English lexical sample WSD task (Kilgarriff, 2001), and second, a corpus-driven mapping of nouns and verbs in WordNet 2.1 to the Omega Ontology (Philpot et al., 2005), produced as part of the O NTO N OTES project (Hovy et al., 2006). A wide variety of semantic and syntactic criteria were used to produce the S ENSEVAL -2 groupings (Palmer et al., 2004; Palmer et al., 2005); this data covers all senses of 411 nouns, 519 verbs, and 257 adjectives, and has been used as gold standard sense clustering data in previous work (Agirre and Lopez, 2003; McCarthy, 2006)2 . The number of judgments within this data (after mapping to WordNet 2.1) is displaye"
D07-1107,P98-2127,0,0.146154,"ques have been proposed for clustering sense inventories and mapping between sense inventories of different granularities. Much work has gone into methods for measuring synset similarity; early work in this direction includes (Dolan, 1994), which attempted to discover sense similarities between dictionary senses. A variety of synset similarity measures based on properties of WordNet itself have been proposed; nine such measures are discussed in (Pedersen et al., 2004), including gloss-based heuristics (Lesk, 1986; Banerjee and Pedersen, 2003), information-content based measures (Resnik, 1995; Lin, 1998; Jiang and Conrath, 1997), and others. Other approaches have used specific cues from WordNet structure to inform the construction of semantic rules; for example, (Peters et al., 1998) suggest clustering two senses based on a wide variety of structural cues from WordNet, including if they are twins (if two synsets share more than one word in their synonym list) or if they represent an example of autohyponymy (if one sense is the direct descendant of the other). (Mihalcea and Moldovan, 2001) implements six semantic rules, using twin and autohyponym features, in addition to other WordNet-structu"
D07-1107,magnini-cavaglia-2000-integrating,0,0.04026,"data7 described in (Agirre and Lopez, 2004), yielding representative contexts for all nominal synsets from WordNet 1.6. These topic signatures were obtained by weighting the contexts of monosemous relatives of each noun synset (i.e., single-sense synsets related by hypernym, hyponym, or other relations); the text for these contexts were extracted from snippets using the Google search engine. We then create a sense similarity feature by taking a thresholded cosine similarity between pairs of topic signatures for these noun synsets. Additionally, we use the WordNet domain dataset described in (Magnini and Cavaglia, 2000; Bentivogli et al., 2004). This dataset contains one or more labels indicating of 164 hierarchically organized “domains” or “subject fields” for each noun, verb, and adjective synset in WordNet; we derive a set of binary features from this data, with a single feature indicating whether or not two synsets share a domain, and one indicator feature per pair of domains indicating respective membership of the sense pair within those domains. Finally, we use as a feature the mappings produced in (Navigli, 2006) of WordNet senses to Oxford English Dictionary senses. This OED dataset was used as the"
D07-1107,W06-2503,0,0.345128,"niques from being used effectively to compare different senses of the same word. Some corpus-based attempts that are capable of estimating similarity between word senses include the topic signatures method; here, (Agirre and Lopez, 2003) collect contexts for a polysemous word based either on sensetagged corpora or by using a weighted agglomeration of contexts of a polysemous word’s monosemous relatives (i.e., single-sense synsets related by hypernym, hyponym, or other relations) from some large untagged corpus. Other corpus-based techniques developed specifically for sense clustering include (McCarthy, 2006), which uses a combination of word-to-word distributional similarity combined with the JCN WordNet-based similarity measure, and work by (Chugur et al., 2002) in finding co-occurrences of senses within documents in sense-tagged corpora. Other attempts have exploited disagreements between WSD systems (Agirre and Lopez, 2003) or between human labelers (Chklovski and Mihalcea, 2003) to create synset similarity measures; while promising, these techniques are severely limited by the performance of the WSD systems or the amount of available labeled data. Some approaches for clustering have made use"
D07-1107,W04-0838,0,0.0141492,"ed by our sense-clustered taxonomy. We may then compare the scores of each system on the coarse-grained task against their scores given a random clustering at the same resolution. Our expectation is that, if our sense clustering is much better than a random sense clustering (and, of course, that the WSD algorithms perform better than random guessing), we will see a marked improvement in the performance of WSD algorithms using our coarse-grained sense hierarchy. We consider the outputs of the top 3 allwords WSD systems that participated in Senseval-3: Gambl (Decadt et al., 2004), SenseLearner (Mihalcea and Faruque, 2004), and KOC University (Yuret, 2004). A guess by a system is given full credit if it was either the correct answer or if it was in the same cluster as the correct answer. Clearly any amount of clustering will only increase WSD performance. Therefore, to account for this natural improvement and consider only the effect of our particular clustering, we also calculate the expected score for a random clustering of the same granularity, as follows: Let C represent the set of clusters over the possible N synsets containing a given word; we then calculate the expectation that an incorrectly-chosen sens"
D07-1107,P06-1014,0,0.78068,"omatic attempts to extract patterns of systematic polysemy based on minimal description length principles (Tomuro, 2001). Another family of approaches has been to use either manually-annotated or automaticallyconstructed mappings to coarser-grained sense inventories; an attempt at providing coarse-grained sense distinctions for the S ENSEVAL -1 exercise included a mapping between WordNet and the Hector lexicon (Palmer et al., 2005). Other attempts in this vein include mappings between WordNet and PropBank (Palmer et al., 2004) and mappings to Levin classes (Levin, 1993; Palmer et al., 2005). (Navigli, 2006) presents an automatic approach for mapping between sense inventories; here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary. Other work has attempted to exploit translational equivalences of WordNet senses in other languages, for example using foreign language WordNet interlingual indexes (Gonzalo et al., 1998; Chugur et al., 2002). 2.1 Gold standard sense clustering data Our approach for learning how to merge senses relies"
D07-1107,N04-3012,0,0.0235097,"rdNet. In Section 5 we evaluate our classifier against thirteen previously proposed methods. 2 Background A wide number of manual and automatic techniques have been proposed for clustering sense inventories and mapping between sense inventories of different granularities. Much work has gone into methods for measuring synset similarity; early work in this direction includes (Dolan, 1994), which attempted to discover sense similarities between dictionary senses. A variety of synset similarity measures based on properties of WordNet itself have been proposed; nine such measures are discussed in (Pedersen et al., 2004), including gloss-based heuristics (Lesk, 1986; Banerjee and Pedersen, 2003), information-content based measures (Resnik, 1995; Lin, 1998; Jiang and Conrath, 1997), and others. Other approaches have used specific cues from WordNet structure to inform the construction of semantic rules; for example, (Peters et al., 1998) suggest clustering two senses based on a wide variety of structural cues from WordNet, including if they are twins (if two synsets share more than one word in their synonym list) or if they represent an example of autohyponymy (if one sense is the direct descendant of the other"
D07-1107,P93-1024,0,0.296413,"Missing"
D07-1107,I05-7009,0,0.0219284,"Missing"
D07-1107,N01-1010,0,0.0476848,"g have made use of regular patterns of polysemy among words. (Peters et al., 1998) uses the C OUSIN relation defined in WordNet 1.5 to cluster hyponyms of categorically related noun synsets, e.g., “container/quantity” (e.g., for clustering senses of “cup” or “barrel”) or ”organization/construction” (e.g., for the building and institution senses of “hospital” or “school”); other approaches based on systematic polysemy include the hand-constructed CORELEX database (Buitelaar, 1998), and automatic attempts to extract patterns of systematic polysemy based on minimal description length principles (Tomuro, 2001). Another family of approaches has been to use either manually-annotated or automaticallyconstructed mappings to coarser-grained sense inventories; an attempt at providing coarse-grained sense distinctions for the S ENSEVAL -1 exercise included a mapping between WordNet and the Hector lexicon (Palmer et al., 2005). Other attempts in this vein include mappings between WordNet and PropBank (Palmer et al., 2004) and mappings to Levin classes (Levin, 1993; Palmer et al., 2005). (Navigli, 2006) presents an automatic approach for mapping between sense inventories; here similarities in gloss definiti"
D07-1107,P94-1019,0,0.0367324,"range as those observed in (McCarthy, 2006). 3 Learning to merge word senses 3.1 WordNet-based features Here we describe the feature space we construct for classifying whether or not a pair of synsets should be merged; first, we employ a wide variety of linguistic features based on information derived from WordNet. We use eight similarity measures implemented within the WordNet::Similarity package5 , described in (Pedersen et al., 2004); these include three measures derived from the paths between the synsets in WordNet: HSO (Hirst and St-Onge, 1998), LCH (Leacock and Chodorow, 1998), and WUP (Wu and Palmer, 1994); three measures based on information content: RES (Resnik, 1995), LIN (Lin, 1998), and JCN (Jiang and Conrath, 1997); the gloss-based Extended Lesk Measure LESK, (Banerjee and Pedersen, 2003), and finally the gloss vector similarity measure VECTOR (Patwardan, 2003). We implement the TWIN feature (Peters et al., 1998), which counts the number of shared synonyms between the two synsets. Additionally we produce pairwise features indicating whether two senses share an ANTONYM , PERTAINYM , or derivationally-related forms (DERIV). We also create the verb-specific features of whether two verb synse"
D07-1107,W04-0864,0,0.0349071,"ompare the scores of each system on the coarse-grained task against their scores given a random clustering at the same resolution. Our expectation is that, if our sense clustering is much better than a random sense clustering (and, of course, that the WSD algorithms perform better than random guessing), we will see a marked improvement in the performance of WSD algorithms using our coarse-grained sense hierarchy. We consider the outputs of the top 3 allwords WSD systems that participated in Senseval-3: Gambl (Decadt et al., 2004), SenseLearner (Mihalcea and Faruque, 2004), and KOC University (Yuret, 2004). A guess by a system is given full credit if it was either the correct answer or if it was in the same cluster as the correct answer. Clearly any amount of clustering will only increase WSD performance. Therefore, to account for this natural improvement and consider only the effect of our particular clustering, we also calculate the expected score for a random clustering of the same granularity, as follows: Let C represent the set of clusters over the possible N synsets containing a given word; we then calculate the expectation that an incorrectly-chosen sense and the actual correct sense wou"
D07-1107,W04-2807,0,\N,Missing
D07-1107,agirre-de-lacalle-2004-publicly,0,\N,Missing
D07-1107,W04-2214,0,\N,Missing
D07-1107,C98-2122,0,\N,Missing
D08-1027,P98-1013,0,0.097975,"rs. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense. 1 Introduction Large scale annotation projects such as TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), TimeBank (Pustejovsky et al., 2003), FrameNet (Baker et al., 1998), SemCor (Miller et al., 1993), and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms. The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost. Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them (Banko and Brill, 2001), one promising alternative for some tasks is the collection of non-expert annotations. In this work we explore the use of Amazon Mechanical Turk1 (AMT) to determin"
D08-1027,P01-1005,0,0.142298,"1 Introduction Large scale annotation projects such as TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), TimeBank (Pustejovsky et al., 2003), FrameNet (Baker et al., 1998), SemCor (Miller et al., 1993), and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms. The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost. Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them (Banko and Brill, 2001), one promising alternative for some tasks is the collection of non-expert annotations. In this work we explore the use of Amazon Mechanical Turk1 (AMT) to determine whether nonexpert labelers can provide reliable natural language annotations. We chose five natural language understanding tasks that we felt would be sufficiently natural and learnable for non-experts, and for which we had gold standard labels from expert labelers, as well as (in some cases) expert labeler agreement information. The tasks are: affect recognition, word similarity, recognizing textual entailment, event temporal ord"
D08-1027,W02-0817,0,0.0605218,"hods in Natural Language Processing, pages 254–263, c Honolulu, October 2008. 2008 Association for Computational Linguistics 2 Related Work The idea of collecting annotations from volunteer contributors has been used for a variety of tasks. Luis von Ahn pioneered the collection of data via online annotation tasks in the form of games, including the ESPGame for labeling images (von Ahn and Dabbish, 2004) and Verbosity for annotating word relations (von Ahn et al., 2006). The Open Mind Initiative (Stork, 1999) has taken a similar approach, attempting to make such tasks as annotating word sense (Chklovski and Mihalcea, 2002) and commonsense word relations (Singh, 2002) sufficiently “easy and fun” to entice users into freely labeling data. There have been an increasing number of experiments using Mechanical Turk for annotation. In (Su et al., 2007) workers provided annotations for the tasks of hotel name entity resolution and attribute extraction of age, product brand, and product model, and were found to have high accuracy compared to gold-standard labels. Kittur et al. (2008) compared AMT evaluations of Wikipedia article quality against experts, finding validation tests were important to ensure good results. Zae"
D08-1027,J93-2004,0,0.1036,"en Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense. 1 Introduction Large scale annotation projects such as TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), TimeBank (Pustejovsky et al., 2003), FrameNet (Baker et al., 1998), SemCor (Miller et al., 1993), and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms. The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost. Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them (Banko and Brill, 2001), one promising alternative for some tasks is the collection of"
D08-1027,H93-1061,0,0.133687,"cognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense. 1 Introduction Large scale annotation projects such as TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), TimeBank (Pustejovsky et al., 2003), FrameNet (Baker et al., 1998), SemCor (Miller et al., 1993), and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms. The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost. Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them (Banko and Brill, 2001), one promising alternative for some tasks is the collection of non-expert annotations. In this work we explore the use of Amazon Mechanical Turk1 (AMT) to determine whether nonexpert labelers c"
D08-1027,J05-1004,0,0.112806,"notations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense. 1 Introduction Large scale annotation projects such as TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), TimeBank (Pustejovsky et al., 2003), FrameNet (Baker et al., 1998), SemCor (Miller et al., 1993), and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms. The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost. Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them (Banko and Brill, 2001), one promising alternative for some tasks is the collection of non-expert annotations. In this"
D08-1027,P99-1032,0,0.388035,"Missing"
D08-1027,D07-1108,0,\N,Missing
D08-1027,S07-1016,0,\N,Missing
D08-1027,S07-1067,0,\N,Missing
D08-1027,S07-1013,0,\N,Missing
D08-1027,C98-1013,0,\N,Missing
D08-1027,P08-1080,0,\N,Missing
D08-1027,W07-1401,0,\N,Missing
D08-1027,kaisser-lowe-2008-creating,0,\N,Missing
D08-1038,A00-2035,0,0.0330014,"Missing"
D08-1038,W93-0231,0,0.289836,"Missing"
D08-1073,P07-2044,1,0.35018,"e individual events are further tagged for temporal information such as tense, modality and grammatical aspect. Time expressions use the TimeML (Ingria and Pustejovsky, 2002) markup language. There are 6 main relations and their inverses in Timebank: before, ibefore, includes, begins, ends and simultaneous. This paper describes work that classifies the relations between events, making use of relations between events and times, and between the times themselves to help inform the decisions. 4 dence scores is important for the global maximization step that is described in the next section. As in Chambers et al. (2007), we build support vector machine (SVM) classifiers and use the probabilities from pairwise SVM decisions as our confidence scores. These scores are then used to choose an optimal global ordering. Following our previous work, we use the set of features summarized in figure 1. They vary from POS tags and lexical features surrounding the event, to syntactic dominance, to whether or not the events share the same tense, grammatical aspect, or aspectual class. These features are the highest performing set on the basic 6-way classification of Timebank. Feature Word* Lemma* Synset* POS* POS bigram* P"
D08-1073,P06-1095,0,0.316594,"ot the events share the same tense, grammatical aspect, or aspectual class. These features are the highest performing set on the basic 6-way classification of Timebank. Feature Word* Lemma* Synset* POS* POS bigram* Prep* Tense* Aspect* Modal* Polarity* Class* Tense Pair Aspect Pair Class Pair POS Pair Tense Match Aspect Match Class Match Dominates The Global Model Text Order Our initial model has two components: (1) a pairwise classifier between events, and (2) a global constraint satisfaction layer that maximizes the confidence scores from the classifier. The first is based on previous work (Mani et al., 2006; Chambers et al., 2007) and the second is a novel contribution to event-event classification. 4.1 Pairwise Classification Classifying the relation between two events is the basis of our model. A soft classification with confi699 Entity Match Same Sent Description The text of the event The lemmatized head word The WordNet synset of head word 4 POS tags, 3 before, and 1 event The POS bigram of the event and its preceding tag Preposition lexeme, if in a prepositional phrase The event’s tense The event’s grammatical aspect The modality of the event Positive or negative The aspecual class of the e"
D08-1073,D08-1027,1,0.0281711,"Missing"
D08-1073,S07-1014,0,0.44595,"Work Recent work on classifying temporal relations within the Timebank Corpus built 6-way relation classifiers over 6 of the corpus’ 13 relations (Mani et al., 2006; Mani et al., 2007; Chambers et al., 2007). A wide range of features are used, ranging from surface indicators to semantic classes. Classifiers make 698 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 698–706, c Honolulu, October 2008. 2008 Association for Computational Linguistics local pairwise decisions and do not consider global implications between the relations. The TempEval-07 (Verhagen et al., 2007) contest recently used two relations, before and after, in a semi-complete textual classification task with a new third relation to distinguish relations that can be labeled with high confidence from those that are uncertain, called vague. The task was a simplified classification task from Timebank in that only one verb, the main verb, of each sentence was used. Thus, the task can be viewed as ordering the main events in pairwise sentences rather than the entire document. This paper uses the core relations of TempEval (before,after,vague) and applies them to a full document ordering task that"
D08-1073,W06-1623,0,\N,Missing
D11-1117,N10-1083,0,0.0757698,"Missing"
D11-1117,D10-1117,0,0.610987,"Missing"
D11-1117,J93-2003,0,0.0353667,"Missing"
D11-1117,W06-2920,0,0.297863,"y, relative to standard EM training. We controlled for important dimensions of variation, such as the underlying language: to make sure that our results are not English-specific, we induced grammars in 19 languages. We also explored the impact from the quality of an initial model (using both uniform and ad hoc initializers), the choice of a primary objective (i.e., soft or hard EM), and the quantity and complexity of training data (shorter versus both short and long sentences). Appendix A gives the full details. 4.1 Data Sets We use all 23 train/test splits from the 2006/7 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007),6 which cover 19 different languages.7 We splice out all punctuation labeled in the data, as is standard practice (Paskin, 2001; Klein and Manning, 2004), introducing new arcs from grandmothers to grand-daughters where necessary, both in train- and test-sets. Evaluation is always against the taken by the two stronger state-of-the-art systems): for words seen at least 100 times in the training corpus, gold part-ofspeech tags are augmented with lexical items. 6 These disjoint splits require smoothing; in the WSJ setting, training and test sets overlapped (Klein and Manning,"
D11-1117,P07-1036,0,0.0816075,"Missing"
D11-1117,N09-1009,0,0.292693,"Missing"
D11-1117,W99-0613,0,0.152196,"eratively), and guarantees not to hurt a well-defined objective, at every step.13 Co-training classically relies on two views of the data — redundant feature sets that allow different algorithms to label examples for each other, yielding “probably approximately correct” (PAC)-style guarantees under certain (strong) assumptions. In contrast, lateen EM uses the same data, features, model and essentially the same algorithms, changing only their objective functions: it makes no assumptions, but guarantees not to harm the primary objective. Some of these distinctions have become blurred with time: Collins and Singer (1999) introduced an objective function (also based on agreement) into co-training; Goldman and Zhou (2000), Ng and Cardie (2003) and Chan et al. (2004) made do without redundant views; Balcan et al. (2004) relaxed other strong assumptions; and Zhou and Goldman (2004) generalized co-training to accommodate three and more algorithms. Several such methods have been applied to dependency parsing (Søgaard and Rishøj, 2010), constituent parsing (Sarkar, 11 We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push"
D11-1117,A94-1009,0,0.156184,"Missing"
D11-1117,N09-1012,0,0.547138,"Missing"
D11-1117,W09-0103,0,0.0156792,"al., 2009, inter alia) and learning algorithms (Gillenwater et al., 2010; Cohen and Smith, 2009, inter alia). Future work could explore other NLP tasks — such as clustering, sequence labeling, segmentation and alignment — that often employ EM. Our meta-heuristics are multi-faceted, featuring aspects of iterated local search, deterministic annealing, cross-validation, contrastive estimation and co-training. They may be generally useful in machine learning and non-convex optimization. Appendix A. Experimental Design Statistical techniques are vital to many aspects of computational linguistics (Johnson, 2009; Charniak, 1997; Abney, 1996, inter alia). We used factorial designs,14 which are standard throughout the natural and social sciences, to assist with experimental design and statistical analyses. Combined with ordinary regressions, these methods provide succinct and interpretable summaries that explain which settings meaningfully contribute to changes in dependent variables, such as running time and accuracy. 14 We used full factorial designs for clarity of exposition. But many fewer experiments would suffice, especially in regression models without interaction terms: for the more efficient f"
D11-1117,P04-1061,0,0.893561,"are smoothed immediately prior to evaluation; some of the baseline models are also smoothed during training. In both cases, we use the “add-one” (a.k.a. Laplace) smoothing algorithm. 4.4 Model Baselines Algorithms Standard Convergence We always halt an optimizer once a change in its objective’s consecutive cross-entropy values falls below 2−20 bpt (at which point we consider it “stuck”). 4.5 Lateen Models Scoring Function We report directed accuracies — fractions of correctly guessed (unlabeled) dependency arcs, including arcs from sentence root symbols, as is standard practice (Paskin, 2001; Klein and Manning, 2004). Punctuation does not affect scoring, as it had been removed from all parse trees in our data (see §4.1). B3 B2 B1 A1 A2 A3 A4 A5 Soft EM ∆a ∆i -2.7 ×0.2 +0.6 ×0.7 0.0 ×2.0 0.0 ×1.3 -0.0 ×1.3 0.0 ×0.7 0.0 ×0.8 0.0 ×1.2 Hard EM ∆a ∆i -2.0 ×0.3 +0.6 ×1.2 +0.8 ×3.7 +5.5 ×6.5 +1.5 ×3.6 -0.1 ×0.7 +3.0 ×2.1 +2.9 ×3.8 Table 2: Estimated additive changes in directed dependency accuracy (∆a) and multiplicative changes in the number of iterations before terminating (∆i) for all baseline models and lateen algorithms, relative to standard training: soft EM (left) and hard EM (right). Bold entries are sta"
D11-1117,P08-1100,0,0.0162637,"al optima at once. After a modest number of such iterations, however, EM lands close to an attractor. Next, its convergence rate necessarily suffers: Disproportionately many (and ever-smaller) steps are needed to finally approach this fixed point, which is almost invariably a local optimum. Deciding when to terminate EM often involves guesswork; and finding ways out of local optima requires trial and error. We propose several strategies that address both limitations. Unsupervised objectives are, at best, loosely correlated with extrinsic performance (Pereira and Schabes, 1992; Merialdo, 1994; Liang and Klein, 2008, inter alia). This fact justifies (occasionally) deviating from a prescribed training course. For example, since multiple equi-plausible objectives are usually available, a learner could cycle through them, optimizing alternatives when the primary objective function gets stuck; or, instead of trying to escape, it could aim to avoid local optima in the first place, by halting search early if an improvement to one objective would come at the expense of harming another. We test these general ideas by focusing on nonconvex likelihood optimization using EM. This setting is standard and has natural"
D11-1117,J93-2004,0,0.0406745,"Missing"
D11-1117,J94-2001,0,0.0603724,"passing many local optima at once. After a modest number of such iterations, however, EM lands close to an attractor. Next, its convergence rate necessarily suffers: Disproportionately many (and ever-smaller) steps are needed to finally approach this fixed point, which is almost invariably a local optimum. Deciding when to terminate EM often involves guesswork; and finding ways out of local optima requires trial and error. We propose several strategies that address both limitations. Unsupervised objectives are, at best, loosely correlated with extrinsic performance (Pereira and Schabes, 1992; Merialdo, 1994; Liang and Klein, 2008, inter alia). This fact justifies (occasionally) deviating from a prescribed training course. For example, since multiple equi-plausible objectives are usually available, a learner could cycle through them, optimizing alternatives when the primary objective function gets stuck; or, instead of trying to escape, it could aim to avoid local optima in the first place, by halting search early if an improvement to one objective would come at the expense of harming another. We test these general ideas by focusing on nonconvex likelihood optimization using EM. This setting is s"
D11-1117,W10-2915,0,0.0279659,"Missing"
D11-1117,N03-1023,0,0.0246443,"the data — redundant feature sets that allow different algorithms to label examples for each other, yielding “probably approximately correct” (PAC)-style guarantees under certain (strong) assumptions. In contrast, lateen EM uses the same data, features, model and essentially the same algorithms, changing only their objective functions: it makes no assumptions, but guarantees not to harm the primary objective. Some of these distinctions have become blurred with time: Collins and Singer (1999) introduced an objective function (also based on agreement) into co-training; Goldman and Zhou (2000), Ng and Cardie (2003) and Chan et al. (2004) made do without redundant views; Balcan et al. (2004) relaxed other strong assumptions; and Zhou and Goldman (2004) generalized co-training to accommodate three and more algorithms. Several such methods have been applied to dependency parsing (Søgaard and Rishøj, 2010), constituent parsing (Sarkar, 11 We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push probability mass towards training examples: when most likely parse trees begin to benefit at the expense of their sentence"
D11-1117,P92-1017,0,0.291869,"rameter space, sometimes bypassing many local optima at once. After a modest number of such iterations, however, EM lands close to an attractor. Next, its convergence rate necessarily suffers: Disproportionately many (and ever-smaller) steps are needed to finally approach this fixed point, which is almost invariably a local optimum. Deciding when to terminate EM often involves guesswork; and finding ways out of local optima requires trial and error. We propose several strategies that address both limitations. Unsupervised objectives are, at best, loosely correlated with extrinsic performance (Pereira and Schabes, 1992; Merialdo, 1994; Liang and Klein, 2008, inter alia). This fact justifies (occasionally) deviating from a prescribed training course. For example, since multiple equi-plausible objectives are usually available, a learner could cycle through them, optimizing alternatives when the primary objective function gets stuck; or, instead of trying to escape, it could aim to avoid local optima in the first place, by halting search early if an improvement to one objective would come at the expense of harming another. We test these general ideas by focusing on nonconvex likelihood optimization using EM. T"
D11-1117,P09-1057,0,0.0607599,"sner (2004) used a “temperature” β to anneal a flat uniform distribution (β = 0) into soft EM’s non-convex objective (β = 1). In their framework, hard EM corresponds to β −→ ∞, so the algorithms differ only in their β -schedule: DA’s is continuous, from 0 to 1; lateen EM’s is a discrete alternation, of 1 and +∞.10 8.2 Terminating Early, Before Convergence EM is rarely run to (even numerical) convergence. Fixing a modest number of iterations a priori (Klein, 2005, §5.3.4), running until successive likelihood ratios become small (Spitkovsky et al., 2009, §4.1) or using a combination of the two (Ravi and Knight, 2009, §4, Footnote 5) is standard practice in NLP. Elworthy’s (1994, §5, Figure 1) analysis of part-ofspeech tagging showed that, in most cases, a small number of iterations is actually preferable to convergence, in terms of final accuracies: “regularization by early termination” had been suggested for image deblurring algorithms in statistical astronomy (Lucy, 1974, §2); and validation against held-out data — a strategy proposed much earlier, in psychology (Larson, 1931), has also been used as a halting criterion in NLP (Yessenalina et al., 2010, §4.2, 5.2). 10 One can think of this as a kind of"
D11-1117,N01-1023,0,0.0561749,"Missing"
D11-1117,P11-1067,0,0.0206079,"two regressions, for two types of dependent variables: to summarize performance, we predict accuracies; and to summarize efficiency, we predict (logarithms of) iterations before termination. In the performance regression, we used four different scores for the dependent variable. These include both directed accuracies and undirected accuracies, each computed in two ways: (i) using a best parse tree; and (ii) using all parse trees. These four types of scores provide different kinds of information. Undirected scores ignore polarity of parentchild relations (Paskin, 2001; Klein and Manning, 2004; Schwartz et al., 2011), partially correcting for some effects of alternate analyses (e.g., systematic choices between modals and main verbs for heads of sentences, determiners for noun phrases, etc.). And integrated scoring, using the inside-outside algorithm (Baker, 1979) to compute expected accuracy across all — not just best — parse trees, has the advantage of incorporating probabilities assigned to individual arcs: This metric is more sensitive to the margins that separate best from next-best parse trees, and is not affected by tie-breaking. We tag scores using two binary predictors in a simple (first order, mu"
D11-1117,P04-1062,0,0.0957599,"run, to escape local optima; in the long run, it also does no harm, by construction (as it returns the best model seen). Of the meta-heuristics that use more than a standard, scalar objective, deterministic annealing (DA) (Rose, 1998) is closest to lateen EM. DA perturbs objective functions, instead of manipulating solutions directly. As other continuation methods (Allgower and Georg, 1990), it optimizes an easy (e.g., convex) function first, then “rides” that optimum by gradually morphing functions towards the difficult objective; each step reoptimizes from the previous approximate solution. Smith and Eisner (2004) employed DA to improve part-of-speech disambiguation, but found that objectives had to be further “skewed,” using domain knowledge, before it helped (constituent) grammar induction. (For this reason, we did not experiment with DA, despite its strong similarities to lateen EM.) Smith and Eisner (2004) used a “temperature” β to anneal a flat uniform distribution (β = 0) into soft EM’s non-convex objective (β = 1). In their framework, hard EM corresponds to β −→ ∞, so the algorithms differ only in their β -schedule: DA’s is continuous, from 0 to 1; lateen EM’s is a discrete alternation, of 1 and"
D11-1117,P05-1044,0,0.094939,"e primary objective. Some of these distinctions have become blurred with time: Collins and Singer (1999) introduced an objective function (also based on agreement) into co-training; Goldman and Zhou (2000), Ng and Cardie (2003) and Chan et al. (2004) made do without redundant views; Balcan et al. (2004) relaxed other strong assumptions; and Zhou and Goldman (2004) generalized co-training to accommodate three and more algorithms. Several such methods have been applied to dependency parsing (Søgaard and Rishøj, 2010), constituent parsing (Sarkar, 11 We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push probability mass towards training examples: when most likely parse trees begin to benefit at the expense of their sentence yields (or vice versa), optimizers halt. 12 For a recently proposed instance of EM that uses crossvalidation (CV) to optimize smoothed data likelihoods (in learning synchronous PCFGs, for phrase-based machine translation), see Mylonakis and Sima’an’s (2010, §3.1) CV-EM algorithm. 13 Some authors (Nigam and Ghani, 2000; Ng and Cardie, 2003; Smith and Eisner, 2005a, §5.2, 7; §2; §6) draw a har"
D11-1117,C10-1120,0,0.0221843,"gorithms, changing only their objective functions: it makes no assumptions, but guarantees not to harm the primary objective. Some of these distinctions have become blurred with time: Collins and Singer (1999) introduced an objective function (also based on agreement) into co-training; Goldman and Zhou (2000), Ng and Cardie (2003) and Chan et al. (2004) made do without redundant views; Balcan et al. (2004) relaxed other strong assumptions; and Zhou and Goldman (2004) generalized co-training to accommodate three and more algorithms. Several such methods have been applied to dependency parsing (Søgaard and Rishøj, 2010), constituent parsing (Sarkar, 11 We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push probability mass towards training examples: when most likely parse trees begin to benefit at the expense of their sentence yields (or vice versa), optimizers halt. 12 For a recently proposed instance of EM that uses crossvalidation (CV) to optimize smoothed data likelihoods (in learning synchronous PCFGs, for phrase-based machine translation), see Mylonakis and Sima’an’s (2010, §3.1) CV-EM algorithm. 13 Some aut"
D11-1117,W10-2902,1,0.810254,"ateen sail worked like a wing (with high pressure on one side and low pressure on the other), allowing a ship to go almost directly into a headwind. By tacking, in a zig-zag pattern, it became possible to sail in any direction, provided there was some wind at all (left). For centuries seafarers expertly combined both sails to traverse extensive distances, greatly increasing the reach of medieval navigation.1 likelihoods of observed data across assignments to hidden variables, whereas hard EM focuses on most likely completions.2 These objectives are plausible, yet both can be provably “wrong” (Spitkovsky et al., 2010a, §7.3). Thus, it is permissible for lateen EM to maneuver between their gradients, for example by tacking around local attractors, in a zig-zag fashion. We propose several strategies that use a secondary objective to improve over standard EM training. For hard EM, the secondary objective is that of soft EM; and vice versa if soft EM is the primary algorithm. Algorithm #1: Simple Lateen EM Simple lateen EM begins by running standard EM to convergence, using a user-supplied initial model, primary objective and definition of convergence. Next, the algorithm alternates. A single lateen alternati"
D11-1117,P10-1130,1,0.855913,"ateen sail worked like a wing (with high pressure on one side and low pressure on the other), allowing a ship to go almost directly into a headwind. By tacking, in a zig-zag pattern, it became possible to sail in any direction, provided there was some wind at all (left). For centuries seafarers expertly combined both sails to traverse extensive distances, greatly increasing the reach of medieval navigation.1 likelihoods of observed data across assignments to hidden variables, whereas hard EM focuses on most likely completions.2 These objectives are plausible, yet both can be provably “wrong” (Spitkovsky et al., 2010a, §7.3). Thus, it is permissible for lateen EM to maneuver between their gradients, for example by tacking around local attractors, in a zig-zag fashion. We propose several strategies that use a secondary objective to improve over standard EM training. For hard EM, the secondary objective is that of soft EM; and vice versa if soft EM is the primary algorithm. Algorithm #1: Simple Lateen EM Simple lateen EM begins by running standard EM to convergence, using a user-supplied initial model, primary objective and definition of convergence. Next, the algorithm alternates. A single lateen alternati"
D11-1117,D10-1102,0,0.0150182,"et al., 2009, §4.1) or using a combination of the two (Ravi and Knight, 2009, §4, Footnote 5) is standard practice in NLP. Elworthy’s (1994, §5, Figure 1) analysis of part-ofspeech tagging showed that, in most cases, a small number of iterations is actually preferable to convergence, in terms of final accuracies: “regularization by early termination” had been suggested for image deblurring algorithms in statistical astronomy (Lucy, 1974, §2); and validation against held-out data — a strategy proposed much earlier, in psychology (Larson, 1931), has also been used as a halting criterion in NLP (Yessenalina et al., 2010, §4.2, 5.2). 10 One can think of this as a kind of “beam search” (Lowerre, 1976), with soft EM expanding and hard EM pruning a frontier. 1274 Early-stopping lateen EM tethers termination to a sign change in the direction of a secondary objective, similarly to (cross-)validation (Stone, 1974; Geisser, 1975; Arlot and Celisse, 2010), but without splitting data — it trains using all examples, at all times.11,12 8.3 Training with Multiple Views Lateen strategies may seem conceptually related to co-training (Blum and Mitchell, 1998). However, bootstrapping methods generally begin with some labeled"
D11-1117,J03-4003,0,\N,Missing
D11-1117,D07-1096,0,\N,Missing
D11-1118,P10-1132,0,0.0225564,"Missing"
D11-1118,J00-1004,0,0.04052,"Missing"
D11-1118,W11-0103,1,0.834714,"Missing"
D11-1118,C04-1080,0,0.011003,"reassignment, most frequent pair (mfp), allows up to two of the most common tags into a token’s label set (with ties, once again, resolved lexicographically). This intermediate approach performs strictly worse than union all, in both regimes. 3.2 Experiment #2: Lexicalization Baselines Our next set of experiments assesses the benefits of categorization, turning to lexicalized baselines that avoid grouping words altogether. All three models discussed below estimated the DMV without using the gold tags in any way (see Table 1: lexicalized). 2 Some of these are annotation errors in the treebank (Banko and Moore, 2004, Figure 2): such (mis)taggings can severely degrade the accuracy of part-of-speech disambiguators, without additional supervision (Banko and Moore, 2004, §5, Table 1). 3 Kupiec (1992) found that the 50,000-word vocabulary of the Brown corpus similarly reduces to ∼400 ambiguity classes. First, not surprisingly, a fully-lexicalized model over nearly 50,000 unique words is able to essentially memorize the training set, supervised. (Without smoothing, it is possible to deterministically attach most rare words in a dependency tree correctly, etc.) Of course, local search is unlikely to find good i"
D11-1118,P06-1109,0,0.0439286,"scovery. Our methods are applicable to vast quantities of unlabeled monolingual text. Not all research on these problems has been fully unsupervised. For example, to the best of our knowledge, every new state-of-the-art dependency grammar inducer since Klein and Manning (2004) relied on gold part-of-speech tags. For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that “good enough” partsof-speech induction methods exist, justifying the focus on grammar induction with supervised part-ofspeech tags (Bod, 2006), pace (Cramer, 2007). One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al., 2011). In turn, not all unsupervised taggers actually induce word categories: Many systems — known as part-of-speech disambiguators (Merialdo, 1994) — rely on external dictionaries of possible tags. Our work builds on two older part-of-speech inducers — word clustering algorithms of Clark (2000) and Brown et al. (1992) — that were recently shown to be more robust than other w"
D11-1118,J92-4003,0,0.364159,"the focus on grammar induction with supervised part-ofspeech tags (Bod, 2006), pace (Cramer, 2007). One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al., 2011). In turn, not all unsupervised taggers actually induce word categories: Many systems — known as part-of-speech disambiguators (Merialdo, 1994) — rely on external dictionaries of possible tags. Our work builds on two older part-of-speech inducers — word clustering algorithms of Clark (2000) and Brown et al. (1992) — that were recently shown to be more robust than other well-known fully unsupervised techniques (Christodoulopoulos et al., 2010). We investigate which properties of gold part-ofspeech tags are useful in grammar induction and parsing, and how these properties could be introduced into induced tags. We also explore the number of word classes that is good for grammar induction: in particular, whether categorization is needed at all. By removing the “unrealistic simplification” of using gold tags (Petrov et al., 2011, §3.2, Footnote 4), we will go on to demonstrate why grammar induction from pla"
D11-1118,C02-1126,0,0.0864194,"Missing"
D11-1118,D10-1056,0,0.0608639,"wbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al., 2011). In turn, not all unsupervised taggers actually induce word categories: Many systems — known as part-of-speech disambiguators (Merialdo, 1994) — rely on external dictionaries of possible tags. Our work builds on two older part-of-speech inducers — word clustering algorithms of Clark (2000) and Brown et al. (1992) — that were recently shown to be more robust than other well-known fully unsupervised techniques (Christodoulopoulos et al., 2010). We investigate which properties of gold part-ofspeech tags are useful in grammar induction and parsing, and how these properties could be introduced into induced tags. We also explore the number of word classes that is good for grammar induction: in particular, whether categorization is needed at all. By removing the “unrealistic simplification” of using gold tags (Petrov et al., 2011, §3.2, Footnote 4), we will go on to demonstrate why grammar induction from plain text is no longer “still too difficult.” Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,"
D11-1118,W00-0717,0,0.129435,"xist, justifying the focus on grammar induction with supervised part-ofspeech tags (Bod, 2006), pace (Cramer, 2007). One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al., 2011). In turn, not all unsupervised taggers actually induce word categories: Many systems — known as part-of-speech disambiguators (Merialdo, 1994) — rely on external dictionaries of possible tags. Our work builds on two older part-of-speech inducers — word clustering algorithms of Clark (2000) and Brown et al. (1992) — that were recently shown to be more robust than other well-known fully unsupervised techniques (Christodoulopoulos et al., 2010). We investigate which properties of gold part-ofspeech tags are useful in grammar induction and parsing, and how these properties could be introduced into induced tags. We also explore the number of word classes that is good for grammar induction: in particular, whether categorization is needed at all. By removing the “unrealistic simplification” of using gold tags (Petrov et al., 2011, §3.2, Footnote 4), we will go on to demonstrate why gr"
D11-1118,P07-3008,0,0.0191516,"ds are applicable to vast quantities of unlabeled monolingual text. Not all research on these problems has been fully unsupervised. For example, to the best of our knowledge, every new state-of-the-art dependency grammar inducer since Klein and Manning (2004) relied on gold part-of-speech tags. For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that “good enough” partsof-speech induction methods exist, justifying the focus on grammar induction with supervised part-ofspeech tags (Bod, 2006), pace (Cramer, 2007). One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al., 2011). In turn, not all unsupervised taggers actually induce word categories: Many systems — known as part-of-speech disambiguators (Merialdo, 1994) — rely on external dictionaries of possible tags. Our work builds on two older part-of-speech inducers — word clustering algorithms of Clark (2000) and Brown et al. (1992) — that were recently shown to be more robust than other well-known fully unsup"
D11-1118,P11-1061,0,0.0463192,"Missing"
D11-1118,N09-1037,0,0.0124122,"sh Gigaword corpora.5 The second is a hierarchical clustering — binary strings up to eighteen bits long — constructed by running Brown et al.’s (1992) algorithm over 43 million words from the BLLIP corpus, minus WSJ.6 4.1 Experiment #3: A Flat Word Clustering Our main purely unsupervised results are with a flat clustering (Clark, 2000) that groups words having similar context distributions, according to KullbackLeibler divergence. (A word’s context is an ordered pair: its left- and right-adjacent neighboring words.) To avoid overfitting, we employed an implementation from previous literature (Finkel and Manning, 2009). The number of clusters (200) and the sufficient amount of training data (several hundredmillion words) were tuned to a task (NER) that is not directly related to dependency parsing. (Table 3 shows representative entries for two of the clusters.) We added one more category (#0) for unknown words. Now every token in WSJ could again be replaced by a coarse identifier (one of at most 201, instead of just 36), in both supervised and unsupervised training. (Our training code did not change.) The resulting supervised model, though not as good as the fully-lexicalized DMV, was more than five points"
D11-1118,P07-1035,0,0.0351281,"Missing"
D11-1118,D08-1036,0,0.0129208,"variant of EM (HMM-EM), perform quite well,8 on average, across different grammar induction tasks. Such sequence models incorporate a sensitivity to context via state transition probabilities PTRAN (ti |ti−1 ), capturing the likelihood that a tag ti immediately follows the tag ti−1 ; emission probabilities PEMIT (wi |ti ) capture the likelihood that a word of type ti is wi . 7 We also briefly comment on this result in the “punctuation” paper (Spitkovsky et al., 2011, §7), published concurrently. 8 They are also competitive with Bayesian estimators, on larger data sets, with cross-validation (Gao and Johnson, 2008). (§5) #5 (§5.1) #6 (§5.2) System Description “punctuation” “punctuation” with monosemous induced tags “punctuation” with context-sensitive induced tags (Spitkovsky et al., 2011) Accuracy 58.4 58.2 (-0.2) 59.1 (+0.7) Table 5: Directed accuracies on Section 23 of WSJ (all sentences) for experiments with the state-of-the-art system. We need a context-sensitive tagger, and HMM models are good — relative to other tag-inducers. However, they are not better than gold tags, at least when trained using a modest amount of data.9 For this reason, we decided to relax the monosemous flat clustering, plugg"
D11-1118,C08-1042,0,0.0248918,"’t a parameter for the number of categories), we doubt that there was a strong enough risk of overfitting to question the clustering’s unsupervised nature. As there isn’t a set number of categories, we used binary prefixes of length k from each word’s address in the computed hierarchy as cluster labels. Results for 7 ≤ k ≤ 9 bits (approximately 100–250 nonempty clusters, close to the 200 we used before) are 1285 similar to those of flat clusters (see Table 1: hierarchical). Outside of this range, however, performance can be substantially worse (see Figure 2), consistent with earlier findings: Headden et al. (2008) demonstrated that (constituent) grammar induction, using the singular-value decomposition (SVD-based) tagger of Sch¨utze (1995), also works best with 100–200 clusters. Important future research directions may include learning to automatically select a good number of word categories (in the case of flat clusterings) and ways of using multiple clustering assignments, perhaps of different granularities/resolutions, in tandem (e.g., in the case of a hierarchical clustering). 4.3 Further Evaluation It is important to enable easy comparison with previous and future work. Since WSJ15 is not a standa"
D11-1118,N09-1012,0,0.439358,"∼400 ambiguity classes. First, not surprisingly, a fully-lexicalized model over nearly 50,000 unique words is able to essentially memorize the training set, supervised. (Without smoothing, it is possible to deterministically attach most rare words in a dependency tree correctly, etc.) Of course, local search is unlikely to find good instantiations for so many parameters, causing unsupervised accuracy for this model to drop in half. For our next experiment, we tried an intermediate, partially-lexicalized approach. We mapped frequent words — those seen at least 100 times in the training corpus (Headden et al., 2009) — to their own individual categories, lumping the rest into a single “unknown” cluster, for a total of under 200 groups. This model is significantly worse for supervised learning, compared even with the monosemous clusters derived from gold tags; yet it is only slightly more learnable than the broken fully-lexicalized variant. Finally, for completeness, we trained a model that maps every token to the same one “unknown” category. As expected, such a trivial “clustering” is ineffective in supervised training; however, it outperforms both lexicalized variants unsupervised,4 strongly suggesting t"
D11-1118,W07-2416,0,0.0731167,"Missing"
D11-1118,P04-1061,0,0.954687,"or machine translation. And since the advent of the web, algorithms that induce structure from unlabeled data have continued to steadily gain importance. In this paper we focus on unsupervised part-of-speech tagging and dependency parsing — two related prob1281 ‡ Department of Linguistics Stanford University Stanford, CA, 94305 lems of syntax discovery. Our methods are applicable to vast quantities of unlabeled monolingual text. Not all research on these problems has been fully unsupervised. For example, to the best of our knowledge, every new state-of-the-art dependency grammar inducer since Klein and Manning (2004) relied on gold part-of-speech tags. For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that “good enough” partsof-speech induction methods exist, justifying the focus on grammar induction with supervised part-ofspeech tags (Bod, 2006), pace (Cramer, 2007). One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al., 2011). In turn, not all unsupervised taggers"
D11-1118,P95-1037,0,0.0426732,"Missing"
D11-1118,J93-2004,0,0.0424162,"sed learning via inside-outside re-estimation (Baker, 1979). Klein and Manning (2004) initialized their system using an “ad-hoc harmonic” completion, followed by training using 40 steps of EM (Klein, 2005). We reproduce this set-up, iterating without actually verifying convergence, in most of our experiments (#1–4, §3–4). 1282 Experiments #5–6 (§5) employ our new state-ofthe-art grammar inducer (Spitkovsky et al., 2011), which uses constrained Viterbi EM (details in §5). 2.3 Training Data The DMV is usually trained on a customized subset of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Following Klein and Manning (2004), we begin with reference constituent parses, prune out all empty sub-trees and remove punctuation and terminals (tagged # and $) that are not pronounced where they appear. We then train only on the remaining sentence yields consisting of no more than fifteen tokens (WSJ15), in most of our experiments (#1–4, §3–4); by contrast, Klein and Manning’s (2004) original system was trained using less data: sentences up to length ten (WSJ10).1 Our final experiments (#5–6, §5) employ a simple scaffolding strategy (Spitkovsky et al., 2010a) that follows up initial trai"
D11-1118,D11-1006,0,0.0126205,"-art dependency grammar inducer since Klein and Manning (2004) relied on gold part-of-speech tags. For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that “good enough” partsof-speech induction methods exist, justifying the focus on grammar induction with supervised part-ofspeech tags (Bod, 2006), pace (Cramer, 2007). One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al., 2011). In turn, not all unsupervised taggers actually induce word categories: Many systems — known as part-of-speech disambiguators (Merialdo, 1994) — rely on external dictionaries of possible tags. Our work builds on two older part-of-speech inducers — word clustering algorithms of Clark (2000) and Brown et al. (1992) — that were recently shown to be more robust than other well-known fully unsupervised techniques (Christodoulopoulos et al., 2010). We investigate which properties of gold part-ofspeech tags are useful in grammar induction and parsing, and how these properties could be introduced int"
D11-1118,J94-2001,0,0.405333,"caused by switching to automatically induced word categories have been interpreted as indications that “good enough” partsof-speech induction methods exist, justifying the focus on grammar induction with supervised part-ofspeech tags (Bod, 2006), pace (Cramer, 2007). One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al., 2011). In turn, not all unsupervised taggers actually induce word categories: Many systems — known as part-of-speech disambiguators (Merialdo, 1994) — rely on external dictionaries of possible tags. Our work builds on two older part-of-speech inducers — word clustering algorithms of Clark (2000) and Brown et al. (1992) — that were recently shown to be more robust than other well-known fully unsupervised techniques (Christodoulopoulos et al., 2010). We investigate which properties of gold part-ofspeech tags are useful in grammar induction and parsing, and how these properties could be introduced into induced tags. We also explore the number of word classes that is good for grammar induction: in particular, whether categorization is needed"
D11-1118,P93-1024,0,0.786348,"e original monosemous tags for 22,280 (of 1,028,348 non-punctuation) tokens in WSJ. For ex9 All of Headden et al.’s (2008) grammar induction experiments with induced parts-of-speech were worse than their best results using gold part-of-speech tags, most likely because they used a very small corpus (half of WSJ10) to cluster words. 10 We chose the sampling split (80:10:10) and replication parameter (100) somewhat arbitrarily, so better results could likely be obtained with tuning. However, we suspect that the real gains would come from using soft clustering techniques (Hinton and Roweis, 2003; Pereira et al., 1993, inter alia) and propagating (joint) estimates of tag distributions into a parser. Our ad-hoc approach is intended to serve solely as a proof of concept. 11 David Elworthy’s C+ tagger, with options -i t -G -l, available from http://friendly-moose.appspot.com/ code/NewCpTag.zip. 1287 ample, the first changed sentence is #3 (of 49,208): Some “circuit breakers” installed after the October 1987 crash failed their first test, traders say, unable to cool the selling panic in both stocks and futures. Above, the word cool gets relabeled as #188 (from #173 — see Table 3), since its context is more sug"
D11-1118,D10-1069,1,0.902503,"Missing"
D11-1118,D10-1067,0,0.0200465,"Missing"
D11-1118,E09-1080,0,0.024863,"Missing"
D11-1118,E95-1020,0,0.246627,"Missing"
D11-1118,P11-1067,0,0.0377234,"Missing"
D11-1118,P07-1049,0,0.117869,"Missing"
D11-1118,N10-1116,1,0.68458,"nk’s Wall Street Journal portion (Marcus et al., 1993). Following Klein and Manning (2004), we begin with reference constituent parses, prune out all empty sub-trees and remove punctuation and terminals (tagged # and $) that are not pronounced where they appear. We then train only on the remaining sentence yields consisting of no more than fifteen tokens (WSJ15), in most of our experiments (#1–4, §3–4); by contrast, Klein and Manning’s (2004) original system was trained using less data: sentences up to length ten (WSJ10).1 Our final experiments (#5–6, §5) employ a simple scaffolding strategy (Spitkovsky et al., 2010a) that follows up initial training at WSJ15 (“less is more”) with an additional training run (“leapfrog”) that incorporates most sentences of the data set, at WSJ45. 2.4 Evaluation Methods Evaluation is against the training set, as is standard practice in unsupervised learning, in part because Klein and Manning (2004, §3) did not smooth the DMV (Klein, 2005, §6.2). For most of our experiments (#1–4, §3–4), this entails starting with the reference trees from WSJ15 (as modified in §2.3), automatically converting their labeled constituents into unlabeled dependencies using deterministic “headper"
D11-1118,W10-2902,1,0.909952,"nk’s Wall Street Journal portion (Marcus et al., 1993). Following Klein and Manning (2004), we begin with reference constituent parses, prune out all empty sub-trees and remove punctuation and terminals (tagged # and $) that are not pronounced where they appear. We then train only on the remaining sentence yields consisting of no more than fifteen tokens (WSJ15), in most of our experiments (#1–4, §3–4); by contrast, Klein and Manning’s (2004) original system was trained using less data: sentences up to length ten (WSJ10).1 Our final experiments (#5–6, §5) employ a simple scaffolding strategy (Spitkovsky et al., 2010a) that follows up initial training at WSJ15 (“less is more”) with an additional training run (“leapfrog”) that incorporates most sentences of the data set, at WSJ45. 2.4 Evaluation Methods Evaluation is against the training set, as is standard practice in unsupervised learning, in part because Klein and Manning (2004, §3) did not smooth the DMV (Klein, 2005, §6.2). For most of our experiments (#1–4, §3–4), this entails starting with the reference trees from WSJ15 (as modified in §2.3), automatically converting their labeled constituents into unlabeled dependencies using deterministic “headper"
D11-1118,W11-0303,1,0.803972,"projective trees. A root token ♦ generates the head of the sentence as its left (and only) child (see Figure 1 for a simple, concrete example). 2.2 Learning Algorithms The DMV lends itself to unsupervised learning via inside-outside re-estimation (Baker, 1979). Klein and Manning (2004) initialized their system using an “ad-hoc harmonic” completion, followed by training using 40 steps of EM (Klein, 2005). We reproduce this set-up, iterating without actually verifying convergence, in most of our experiments (#1–4, §3–4). 1282 Experiments #5–6 (§5) employ our new state-ofthe-art grammar inducer (Spitkovsky et al., 2011), which uses constrained Viterbi EM (details in §5). 2.3 Training Data The DMV is usually trained on a customized subset of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Following Klein and Manning (2004), we begin with reference constituent parses, prune out all empty sub-trees and remove punctuation and terminals (tagged # and $) that are not pronounced where they appear. We then train only on the remaining sentence yields consisting of no more than fifteen tokens (WSJ15), in most of our experiments (#1–4, §3–4); by contrast, Klein and Manning’s (2004) original s"
D11-1118,W03-3023,0,0.229543,"Missing"
D11-1118,J03-4003,0,\N,Missing
D11-1118,petrov-etal-2012-universal,0,\N,Missing
D11-1118,D07-1096,0,\N,Missing
D12-1063,J00-1004,0,0.0512203,"all sentences are assigned to one of two classes: complete and incomplete (comp ∈ {T, F}, for now taken as exogenous). This model assumes that wordword (i.e., head-dependent) interactions in the two domains are the same. However, sentence lengths — for which stopping probabilities are responsible — and distributions of root words may be different. Consequently, an additional comp parameter is added to the context of two relevant types of factors: PATTACH (cr |⋄; L, comp), remain the same as in DBM-2. 2.4 Summary of DBMs and Related Models Head-outward automata (Alshawi, 1996a; Alshawi, 1996b; Alshawi et al., 2000) played a central part as generative models for probabilistic grammars, starting with their early adoption in supervised split-head constituent parsers (Collins, 1997; Collins, 2003). Table 1 lists some parameterizations that have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. PSTOP ( · |dir; adj, ce , comp); 3 Experimental Set-Up and Methodology and PATTACH (cr |⋄; L, comp). We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into For example,"
D12-1063,P10-1131,0,0.048861,"Missing"
D12-1063,N10-1083,0,0.0405064,"Missing"
D12-1063,J93-2003,0,0.0495073,"sence of intervening punctuation; and (b) presence of intermediate words. 694 6.2 Experimental Results Postponed As we mentioned earlier (see §3), there is little point in testing DBM-3 with shorter sentences, since most sentence-internal punctuation occurs in longer inputs. Instead, we will test this model in a final step of a staged training strategy, with more data (see §7.3). 7 A Curriculum Strategy for DBMs We propose to train up to DBM-3 iteratively — by beginning with DBM-1 and gradually increasing model complexity through DBM-2, drawing on the intuitions of IBM translation models 1–4 (Brown et al., 1993). Instead of using sentences of up to 15 tokens, as in all previous experiments (§4–5), we will now make use of nearly all available training data: up to length 45 (out of concern for efficiency), during later stages. In the first stage, however, we will use only a subset of the data with DBM-1, in a process sometimes called curriculum learning (Bengio et al., 2009; Krueger and Dayan, 2009, inter alia). Our grammar inducers will thus be “starting small” in both senses suggested by Elman (1993): simultaneously scaffolding on model- and data-complexity. 7.1 Scaffolding Stage #1: DBM-1 We begin b"
D12-1063,W06-2920,0,0.0901787,"eterizations of the split-head-outward generative process used by DBMs and in previous models. grammar induction experiments. Although motivating solely from this treebank biases our discussion towards a very specific genre of just one language, it has the advantage of allowing us to make concrete claims that are backed up by significant statistics. In the grammar induction experiments that follow, we will test each model’s incremental contribution to accuracies empirically, across many disparate languages. We worked with all 23 (disjoint) train/test splits from the 2006/7 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages.4 For each data set, we induced a baseline grammar using the DMV. We excluded all training sentences with more than 15 tokens to create a conservative bias, because in this set-up the baseline is known to excel (Spitkovsky et al., 2009). Grammar inducers were initialized using (the same) uniformly-at-random chosen parse trees of training sentences (Cohen and Smith, 2010); thereafter, we applied “add one” smoothing at every training step. To fairly compare the models under consideration — which could have quite different starting perplexities and ens"
D12-1063,N09-1009,0,0.0778514,"ore powerful (e.g., they recognize the language an bn in finite state) than the split-head variants, which process one side before the other. 689 mented as both head-outward and head-inward automata. (In fact, arbitrary permutations of siblings to a given side of their parent would not affect the likelihood of the modified tree, with the DMV.) We propose to make fuller use of split-head automata’s head-outward nature by drawing on information in partially-generated parses, which contain useful predictors that, until now, had not been exploited even in featurized systems for grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010). Some of these predictors, including the identity — or even number (McClosky, 2008) — of alreadygenerated siblings, can be prohibitively expensive in sentences above a short length k. For example, they break certain modularity constraints imposed by the charts used in O(k3 )-optimized algorithms (Paskin, 2001a; Eisner, 2000). However, in bottom-up parsing and training from text, everything about the yield — i.e., the ordered sequence of all already-generated descendants, on the side of the head that is in the process of spawning off an additional child — is not"
D12-1063,P10-1152,0,0.210078,"model’s incremental contribution to accuracies empirically, across many disparate languages. We worked with all 23 (disjoint) train/test splits from the 2006/7 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages.4 For each data set, we induced a baseline grammar using the DMV. We excluded all training sentences with more than 15 tokens to create a conservative bias, because in this set-up the baseline is known to excel (Spitkovsky et al., 2009). Grammar inducers were initialized using (the same) uniformly-at-random chosen parse trees of training sentences (Cohen and Smith, 2010); thereafter, we applied “add one” smoothing at every training step. To fairly compare the models under consideration — which could have quite different starting perplexities and ensuing consecutive relative likelihoods — we experimented with two termination strategies. In one case, we blindly ran each learner through 40 steps of inside-outside re-estimation, ignoring any convergence criteria; in the other case, we ran until numerical convergence of soft EM’s objective function or until the likelihood of resulting Viterbi parse trees suffered — an “early-stopping lateen EM” strategy (Spitkovsk"
D12-1063,D11-1005,0,0.094358,"Missing"
D12-1063,P97-1003,0,0.686471,"interactions in the two domains are the same. However, sentence lengths — for which stopping probabilities are responsible — and distributions of root words may be different. Consequently, an additional comp parameter is added to the context of two relevant types of factors: PATTACH (cr |⋄; L, comp), remain the same as in DBM-2. 2.4 Summary of DBMs and Related Models Head-outward automata (Alshawi, 1996a; Alshawi, 1996b; Alshawi et al., 2000) played a central part as generative models for probabilistic grammars, starting with their early adoption in supervised split-head constituent parsers (Collins, 1997; Collins, 2003). Table 1 lists some parameterizations that have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. PSTOP ( · |dir; adj, ce , comp); 3 Experimental Set-Up and Methodology and PATTACH (cr |⋄; L, comp). We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into For example, the new stopping factors could capture the fact that incomplete fragments — such as the noun-phrases George Morton, headlines Energy and Odds and Ends, a line item c"
D12-1063,J03-4003,0,0.116975,"n the two domains are the same. However, sentence lengths — for which stopping probabilities are responsible — and distributions of root words may be different. Consequently, an additional comp parameter is added to the context of two relevant types of factors: PATTACH (cr |⋄; L, comp), remain the same as in DBM-2. 2.4 Summary of DBMs and Related Models Head-outward automata (Alshawi, 1996a; Alshawi, 1996b; Alshawi et al., 2000) played a central part as generative models for probabilistic grammars, starting with their early adoption in supervised split-head constituent parsers (Collins, 1997; Collins, 2003). Table 1 lists some parameterizations that have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. PSTOP ( · |dir; adj, ce , comp); 3 Experimental Set-Up and Methodology and PATTACH (cr |⋄; L, comp). We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into For example, the new stopping factors could capture the fact that incomplete fragments — such as the noun-phrases George Morton, headlines Energy and Odds and Ends, a line item c - Domestic car,"
D12-1063,P99-1059,0,0.0830677,"then generates its own subtree recursively and the whole process continues, moving away from the head, until ch fails to generate a left-dependent. At that point, an analogous procedure is repeated to ch ’s right, this time using stopping factors PSTOP ( · |R; · · · ). All parse trees derived in this way are guaranteed to be projective and can be described by split-head grammars. Instances of these split-head automata have been heavily used in grammar induction (Paskin, 2001b; Klein and Manning, 2004; Headden et al., 2009, inter alia), in part because they allow for efficient implementations (Eisner and Satta, 1999, §8) of the inside-outside re-estimation algorithm (Baker, 1979). The basic tenet of split-head grammars is that every head word generates its left-dependents independently of its right-dependents. This assumption implies, for instance, that words’ leftand right-valences — their numbers of children to each side — are also independent. But it does not imply that descendants that are closer to the head cannot influence the generation of farther dependents on the same side. Nevertheless, many popular grammars for unsupervised parsing behave as if a word had to generate all of its children (to on"
D12-1063,W10-2901,0,0.022147,"Missing"
D12-1063,N09-1012,0,0.662666,"entity of the parent ch and possibly other parameters (again represented by dots). The child then generates its own subtree recursively and the whole process continues, moving away from the head, until ch fails to generate a left-dependent. At that point, an analogous procedure is repeated to ch ’s right, this time using stopping factors PSTOP ( · |R; · · · ). All parse trees derived in this way are guaranteed to be projective and can be described by split-head grammars. Instances of these split-head automata have been heavily used in grammar induction (Paskin, 2001b; Klein and Manning, 2004; Headden et al., 2009, inter alia), in part because they allow for efficient implementations (Eisner and Satta, 1999, §8) of the inside-outside re-estimation algorithm (Baker, 1979). The basic tenet of split-head grammars is that every head word generates its left-dependents independently of its right-dependents. This assumption implies, for instance, that words’ leftand right-valences — their numbers of children to each side — are also independent. But it does not imply that descendants that are closer to the head cannot influence the generation of farther dependents on the same side. Nevertheless, many popular g"
D12-1063,P04-1061,0,0.934254,"-maximization algorithms. 688 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 688–698, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics 2 The Dependency and Boundary Models Our models follow a standard generative story for head-outward automata (Alshawi, 1996a), restricted to the split-head case (see below),1 over lexical word classes {cw }: first, a sentence root cr is chosen, with probability PATTACH (cr |⋄; L); ⋄ is a special start symbol that, by convention (Klein and Manning, 2004; Eisner, 1996), produces exactly one child, to its left. Next, the process recurses. Each (head) word ch generates a left-dependent with probability 1 − PSTOP ( · |L; · · · ), where dots represent additional parameterization on which it may be conditioned. If the child is indeed generated, its identity cd is chosen with probability PATTACH (cd |ch ; · · · ), influenced by the identity of the parent ch and possibly other parameters (again represented by dots). The child then generates its own subtree recursively and the whole process continues, moving away from the head, until ch fails to gene"
D12-1063,J93-2004,0,0.0405584,"(Alshawi, 1996a; Alshawi, 1996b; Alshawi et al., 2000) played a central part as generative models for probabilistic grammars, starting with their early adoption in supervised split-head constituent parsers (Collins, 1997; Collins, 2003). Table 1 lists some parameterizations that have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. PSTOP ( · |dir; adj, ce , comp); 3 Experimental Set-Up and Methodology and PATTACH (cr |⋄; L, comp). We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into For example, the new stopping factors could capture the fact that incomplete fragments — such as the noun-phrases George Morton, headlines Energy and Odds and Ends, a line item c - Domestic car, dollar 690 3 We converted labeled constituents into unlabeled dependencies using deterministic “head-percolation” rules (Collins, Split-Head Dependency Grammar GB (Paskin, 2001b) DMV (Klein and Manning, 2004) EVG (Headden et al., 2009) DBM-1 (§2.1) DBM-2 (§2.2) DBM-3 (§2.3) PATTACH (head-root) 1 / |{w}| cr |⋄; L cr |⋄; L cr |⋄; L cr |⋄; L, comp cr |⋄; L, comp PATTACH (dependent-h"
D12-1063,W11-3901,0,0.33874,"Missing"
D12-1063,D11-1006,0,0.0813659,"average of bests) Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1–3 trained with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual, including SCAJ (Spitkovsky et al., 2011a, Tables 5–6) and SAJ (Spitkovsky et al., 2011b); (ii) rely on gold POS-tag identities to discourage noun roots (Mareˇcek and Zabokrtsk´y, 2011, MZ) or to encourage verbs (Rasooli and Faili, 2012, RF); and (iii) transfer delexicalized parsers (Søgaard, 2011a, S) from resource-rich languages with translations (McDonald et al., 2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3 trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints. 7.2 Scaffolding Stage #2: DBM-2 ← DBM-1 Next, we trained on all sentences up to length 45. Since these inputs are punctuation-rich, in both remaining stages we used the constrained Viterbi EM set-up suggested by Spitkovsky et al. (2011b) instead of plain soft EM; we employ an early termination strategy, quitting hard EM as soon as soft EM’s objective suffers (Spitkovsky et al., 2011a). Punctuation was converted"
D12-1063,D10-1120,0,0.43584,"Missing"
D12-1063,P92-1017,0,0.760879,"Missing"
D12-1063,W12-0701,0,0.340933,"MZNR — 53.0 MZ 54.6 MZNR 68.4 MPHit 50.0 SCAJ6 34.3 RFH1&2 68.0 MPHm:p 40.9 SAJ 61.3 RFH1 — 48.8 SCAJ6 — — 38.2 SCAJ6 (best average, not an average of bests) Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1–3 trained with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual, including SCAJ (Spitkovsky et al., 2011a, Tables 5–6) and SAJ (Spitkovsky et al., 2011b); (ii) rely on gold POS-tag identities to discourage noun roots (Mareˇcek and Zabokrtsk´y, 2011, MZ) or to encourage verbs (Rasooli and Faili, 2012, RF); and (iii) transfer delexicalized parsers (Søgaard, 2011a, S) from resource-rich languages with translations (McDonald et al., 2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3 trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints. 7.2 Scaffolding Stage #2: DBM-2 ← DBM-1 Next, we trained on all sentences up to length 45. Since these inputs are punctuation-rich, in both remaining stages we used the constrained Viterbi EM set-up suggested by Spitkovsky et al. (2011b) instead of plain soft EM; we employ an"
D12-1063,N12-1087,0,0.036528,"Missing"
D12-1063,P11-2120,0,0.0332253,"0.9 SAJ 61.3 RFH1 — 48.8 SCAJ6 — — 38.2 SCAJ6 (best average, not an average of bests) Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1–3 trained with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual, including SCAJ (Spitkovsky et al., 2011a, Tables 5–6) and SAJ (Spitkovsky et al., 2011b); (ii) rely on gold POS-tag identities to discourage noun roots (Mareˇcek and Zabokrtsk´y, 2011, MZ) or to encourage verbs (Rasooli and Faili, 2012, RF); and (iii) transfer delexicalized parsers (Søgaard, 2011a, S) from resource-rich languages with translations (McDonald et al., 2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3 trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints. 7.2 Scaffolding Stage #2: DBM-2 ← DBM-1 Next, we trained on all sentences up to length 45. Since these inputs are punctuation-rich, in both remaining stages we used the constrained Viterbi EM set-up suggested by Spitkovsky et al. (2011b) instead of plain soft EM; we employ an early termination strategy, quitting hard EM as soon as soft E"
D12-1063,W11-1109,0,0.273856,"0.9 SAJ 61.3 RFH1 — 48.8 SCAJ6 — — 38.2 SCAJ6 (best average, not an average of bests) Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1–3 trained with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual, including SCAJ (Spitkovsky et al., 2011a, Tables 5–6) and SAJ (Spitkovsky et al., 2011b); (ii) rely on gold POS-tag identities to discourage noun roots (Mareˇcek and Zabokrtsk´y, 2011, MZ) or to encourage verbs (Rasooli and Faili, 2012, RF); and (iii) transfer delexicalized parsers (Søgaard, 2011a, S) from resource-rich languages with translations (McDonald et al., 2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3 trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints. 7.2 Scaffolding Stage #2: DBM-2 ← DBM-1 Next, we trained on all sentences up to length 45. Since these inputs are punctuation-rich, in both remaining stages we used the constrained Viterbi EM set-up suggested by Spitkovsky et al. (2011b) instead of plain soft EM; we employ an early termination strategy, quitting hard EM as soon as soft E"
D12-1063,D11-1117,1,0.942736,"th, 2010); thereafter, we applied “add one” smoothing at every training step. To fairly compare the models under consideration — which could have quite different starting perplexities and ensuing consecutive relative likelihoods — we experimented with two termination strategies. In one case, we blindly ran each learner through 40 steps of inside-outside re-estimation, ignoring any convergence criteria; in the other case, we ran until numerical convergence of soft EM’s objective function or until the likelihood of resulting Viterbi parse trees suffered — an “early-stopping lateen EM” strategy (Spitkovsky et al., 2011a, §2.3). We evaluated against all sentences of the blind test sets (except one 145-token item in Arabic ’07 data). Table 2 shows experimental results, averaged over 1999), discarding any empty nodes, etc., as is standard practice. 4 We did not test on WSJ data because such evaluation would not be blind, as parse trees from the PTB are our motivating examples; instead, performance on WSJ serves as a strong baseline in a separate study (Spitkovsky et al., 2012a): bootstrapping of DBMs from mostly incomplete inter-punctuation fragments. 691 all 19 languages, for the DMV baselines and DBM-1 and 2"
D12-1063,W11-0303,1,0.909365,"th, 2010); thereafter, we applied “add one” smoothing at every training step. To fairly compare the models under consideration — which could have quite different starting perplexities and ensuing consecutive relative likelihoods — we experimented with two termination strategies. In one case, we blindly ran each learner through 40 steps of inside-outside re-estimation, ignoring any convergence criteria; in the other case, we ran until numerical convergence of soft EM’s objective function or until the likelihood of resulting Viterbi parse trees suffered — an “early-stopping lateen EM” strategy (Spitkovsky et al., 2011a, §2.3). We evaluated against all sentences of the blind test sets (except one 145-token item in Arabic ’07 data). Table 2 shows experimental results, averaged over 1999), discarding any empty nodes, etc., as is standard practice. 4 We did not test on WSJ data because such evaluation would not be blind, as parse trees from the PTB are our motivating examples; instead, performance on WSJ serves as a strong baseline in a separate study (Spitkovsky et al., 2012a): bootstrapping of DBMs from mostly incomplete inter-punctuation fragments. 691 all 19 languages, for the DMV baselines and DBM-1 and 2"
D12-1063,W12-1903,1,0.763219,"soft EM’s objective function or until the likelihood of resulting Viterbi parse trees suffered — an “early-stopping lateen EM” strategy (Spitkovsky et al., 2011a, §2.3). We evaluated against all sentences of the blind test sets (except one 145-token item in Arabic ’07 data). Table 2 shows experimental results, averaged over 1999), discarding any empty nodes, etc., as is standard practice. 4 We did not test on WSJ data because such evaluation would not be blind, as parse trees from the PTB are our motivating examples; instead, performance on WSJ serves as a strong baseline in a separate study (Spitkovsky et al., 2012a): bootstrapping of DBMs from mostly incomplete inter-punctuation fragments. 691 all 19 languages, for the DMV baselines and DBM-1 and 2. We did not test DBM-3 in this set-up because most sentence-internal punctuation occurs in longer sentences; instead, DBM-3 will be tested later (see §7), using most sentences,5 in the final training step of a curriculum strategy (Bengio et al., 2009) that we will propose for DBMs. For the three models tested on shorter inputs (up to 15 tokens) both terminating criteria exhibited the same trend; lateen EM consistently scored slightly higher than 40 EM iterat"
D12-1063,H01-1014,0,0.0836018,"Missing"
D12-1063,J04-4004,0,\N,Missing
D12-1063,D07-1096,0,\N,Missing
D13-1204,P98-2240,0,0.0696678,"form sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can handle problems with equally important but conflicting criteria (Stadler, 1988), using Pareto-optimal ensembles. They are especially well-suited to language, which evolves under pressures from competing (e.g., speaker, listener and learner) constraints, and have been used to model configurations of vowels and tone systems (Ke et al., 2003). Our"
D13-1204,N09-3008,0,0.0138562,"ut also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat dise"
D13-1204,D10-1117,0,0.416692,"Missing"
D13-1204,W06-2920,0,0.0164983,"e, and hard EM’s objective (L = H) is the one better suited to long inputs (Spitkovsky et al., 2010). Our decoders always force an inter-punctuation fragment to derive itself (Spitkovsky et al., 2011b, §2.2).4 In evaluation, such (loose) constraints may help attach sometimes and philology to called (and the science... to is). In training, stronger (strict) constraints also disallow attachment of fragments’ heads by non-heads, to connect Linguistics, called and is (assuming each piece got parsed correctly). 3.4 Final Evaluation and Metrics Evaluation is against held-out CoNLL shared task data (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages. We compute performance as directed dependency accuracies (DDA), fractions of correct unlabeled arcs in parsed output (an extrinsic metric).5 For most WSJ experiments we include also sentence and parse tree cross-entropies (soft and hard EMs’ intrinsic metrics), in bits per token (bpt). 4 But these constraints do not impact training with shorter inputs, since there is no internal punctuation in Dsplit or Dsimp . 5 We converted gold labeled constituents in WSJ to unlabeled reference dependencies using deterministic “head-percolation” rules (Collins,"
D13-1204,W05-0620,0,0.0269794,"Missing"
D13-1204,W00-0717,0,0.0219365,"and Klein, 2009). Elidan et al. (2002) suggested how example-reweighing could cause “informed” changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements. We exploited these properties of the combiners to reconcile"
D13-1204,E03-1009,0,0.017774,"Missing"
D13-1204,P10-1152,0,0.0138972,"are shaded black. to larger — or possibly more complex — data sets. We first fork off two variations of the incoming model based on D0 : (i) a filtered view, which focuses on cleaner, simpler data (transform #1); and (ii) a symmetrized view that backs off to word associations (transform #2). Next is grammar induction over D. We optimize a full DBM instance starting from the first fork, and bootstrap a reduced DBM0 from the second. Finally, the two new induced sets of parse trees, for D, are merged (lexicalized join): causes initial parse trees to be chosen uniformly at random, as suggested by Cohen and Smith (2010): 5.2 Iterated Fork/Join (IFJ) Our second network daisy-chains grammar inductors, starting from the single-word inter-punctuation 1 2 fragments in Dsplit , then retraining on Dsplit , and so 15 forth, until finally stopping at Dsplit , as before: (10) C C1′ HL·DBM D D0 C2′ 0 SLDBM D S C2 The idea here is to prepare for two scenarios: an incoming grammar that is either good or bad for D. If the model is good, DBM should be able to hang on to it and make improvements. But if it is bad, DBM could get stuck fitting noise, whereas DBM0 might be more likely to ramp up to a good alternative. Since we"
D13-1204,W99-0613,0,0.0224629,"-batches that partition a data set helps avoid some local optima (Liang and Klein, 2009). Elidan et al. (2002) suggested how example-reweighing could cause “informed” changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements."
D13-1204,P10-1106,0,0.0213139,"Missing"
D13-1204,W95-0102,0,0.195744,"Missing"
D13-1204,C04-1022,0,0.02598,"erform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can handle problems with equally important but conflicting criteria (Stadler, 1988), using Pareto-optimal ensembles. They are especially well-suited to language, which evolves under pressures from competing (e.g., speaker, listener and learner) constraints, and have been used to model configurations of vowels and tone systems (Ke et al., 2003). Our transform and join mechanisms also exhibit some features of genetic search, and make use of competing objectives: go"
D13-1204,W09-1803,0,0.0186141,"om jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling o"
D13-1204,N12-1069,0,0.37929,"them can be reset to uniform distributions, by discarding associated counts from C . In text classification, this could correspond to eliminating frequent or rare tokens from bags-of-words. We use circular shapes to represent such model ablation operators: Our joining technique could do better than either C1∗ or C2∗ , by entertaining also a third possibility, (6) LD 3 The Task and Methodology We apply transform and join paradigms to grammar induction, an important problem of computational linguistics that involves notoriously difficult objectives (Pereira and Schabes, 1992; de Marcken, 1995; Gimpel and Smith, 2012, inter alia). The goal is to induce grammars capable of parsing unseen text. Input, in both training and testing, is a sequence of tokens labeled as: (i) a lexical item and its category, (w, cw ); (ii) a punctuation mark; or (iii) a sentence boundary. Output is unlabeled dependency trees. 3.1 Models and Data We constrain all parse structures to be projective, via dependency-and-boundary grammars (Spitkovsky et al., 2012a; 2012b): DBMs 0–3 are head-outward generative parsing models (Alshawi, 1996) that distinguish complete sentences from incomplete fragments in a corpus D: Dcomp comprises inpu"
D13-1204,P12-2004,0,0.159618,"Missing"
D13-1204,P13-1044,0,0.147108,"tworks (IFJ and GT), previous state-of-the-art systems of Spitkovsky et al. (2012b) and ˇ Mareˇcek and Zabokrtsk´ y (2012), and three-way combination with SAJ (CS, including results up to length ten). a search algorithm for encoding decipherment problems that guarantees to quickly converge on optimal solutions. Convexity can be ideal for comparative analyses, by eliminating dependence on initial conditions. But for many NLP tasks, including grammar induction, the most relevant known objective functions are still riddled with local optima. Renewed efforts to find exact solutions (Eisner, 2012; Gormley and Eisner, 2013) may be a good fit for the smaller and simpler, earlier stages of our iterative networks. Multi-start methods (Solis and Wets, 1981) can recover certain global extrema almost surely (i.e., with probability approaching one). Moreover, random restarts via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine 1991"
D13-1204,N09-1012,0,0.208322,"Missing"
D13-1204,Y12-1061,0,0.0212499,"Missing"
D13-1204,J03-1001,0,0.0287918,"otactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can handle problems with equally important but conflicting criteria (Stadler, 1988), using Pareto-optimal ensembles. They are especially well-suited to language, which evolves under pressures from competing (e.g., speaker, listener and learner) constraints, and have been used to model configurations of vowels and tone systems (Ke et al., 2003). Our transform and join mechanisms also exhibit some features of genetic search, and make use of competing objectives: good sets of parse trees must make sense both lexicalized and with word categories, to rich and impoverished models of grammar, and for both long, complex sentences and short, simple text fragments. This selection of text filters is a specialized case of more general “data perturbation” techniques — even cycling over randomly chosen mini-batches that partition a data set helps avoid some local optima (Liang and Klein, 2009). Elidan et al. (2002) suggested how example-reweighi"
D13-1204,C10-2062,0,0.0106301,"s via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine 1991 translation (but also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks a"
D13-1204,P02-1017,0,0.655036,"Missing"
D13-1204,P04-1061,0,0.872735,"root dependency arcs of one plus how often the left and right tokens (are expected to) appear connected: C S (8) The idea behind the symmetrizer is to glean information from skeleton parses. Grammar inducers can sometimes make good progress in resolving undirected parse structures despite being wrong about the polarities of most arcs (Spitkovsky et al., 2009, Figure 3: Uninformed). Symmetrization offers an extra chance to make heads or tails of syntactic relations, after learning which words tend to go together. 6 A related approach — initializing EM training with an M-step — was advocated by Klein and Manning (2004, §3). 1986 a attaches z on At each instance where a word (say) the right, our implementation attributes half its y a z , reserving weight to the intended construction, z attachthe other half for the symmetric structure, x a to its left: a z . For the desired effect, these ing aggregated counts are left unnormalized, while all other counts (of word fertilities and sentence roots) get discarded. To see why we don’t turn word attachment scores into probabilities, consider sentences a z and c z . The fact that z co-occurs with a z c introduces an asymmetry into ’s relation with : z | c ) = 1 diff"
D13-1204,N09-1069,0,0.0386621,"n used to model configurations of vowels and tone systems (Ke et al., 2003). Our transform and join mechanisms also exhibit some features of genetic search, and make use of competing objectives: good sets of parse trees must make sense both lexicalized and with word categories, to rich and impoverished models of grammar, and for both long, complex sentences and short, simple text fragments. This selection of text filters is a specialized case of more general “data perturbation” techniques — even cycling over randomly chosen mini-batches that partition a data set helps avoid some local optima (Liang and Klein, 2009). Elidan et al. (2002) suggested how example-reweighing could cause “informed” changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 200"
D13-1204,J93-2004,0,0.0454347,"nguists. Only the last piece of text would still be considered complete, isolating its contribution to sentence root and boundary word distributions from those of incomplete fragments. The sparse model, DBM-0, assumes a uniform distribution for roots of incomplete inputs and reduces conditioning contexts of stopping probabilities, which works well with split data. We will exploit both DBM-0 and the full DBM,2 drawing also on split, simple and raw views of input text. All experiments prior to final multi-lingual evaluation will use the Penn English Treebank’s Wall Street Journal (WSJ) portion (Marcus et al., 1993) as the underlying tokenized and sentence-broken corpus D. Instead of gold parts-of-speech, we plugged in 200 context-sensitive unsupervised tags, from Spitkovsky et al. (2011c),3 for the word categories. 3.2 Smoothing and Lexicalization All unlexicalized instances of DBMs will be estimated with “add one” (a.k.a. Laplace) smoothing, 2 We use the short-hand DBM to refer to DBM-3, which is equivalent to DBM-2 if D has no internally-punctuated sentences (D=Dsplit), and DBM-1 if all inputs also have trailing punctuation (D=Dsimp ); DBM0 is our short-hand for DBM-0. 3 http://nlp.stanford.edu/pubs/g"
D13-1204,P13-1028,0,0.522451,"Missing"
D13-1204,W11-3901,0,0.0320373,"Missing"
D13-1204,D12-1028,0,0.193911,"Missing"
D13-1204,C10-2094,0,0.0160932,"ity measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine 1991 translation (but also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, d"
D13-1204,P10-1158,0,0.0232872,"lement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can handle problems with equally important but conflicting criteria (Stadler, 1988), using Pareto-optimal ensembles. They are especially well-suited to language, which evolves under pressures from competing (e.g., speaker, listener and learner) constraints, and have been used to model configurations of vowels and tone systems (Ke et al., 2003). Our transform and join mechanisms also exhibit some features of genetic search, and make use of competing objectives: good sets of parse trees must make sense both lexicalized and with wor"
D13-1204,W98-1411,0,0.138449,"Missing"
D13-1204,C08-1074,0,0.0276753,"still riddled with local optima. Renewed efforts to find exact solutions (Eisner, 2012; Gormley and Eisner, 2013) may be a good fit for the smaller and simpler, earlier stages of our iterative networks. Multi-start methods (Solis and Wets, 1981) can recover certain global extrema almost surely (i.e., with probability approaching one). Moreover, random restarts via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine 1991 translation (but also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” mo"
D13-1204,P92-1017,0,0.349801,"(e.g., several multinomials) then a subset of them can be reset to uniform distributions, by discarding associated counts from C . In text classification, this could correspond to eliminating frequent or rare tokens from bags-of-words. We use circular shapes to represent such model ablation operators: Our joining technique could do better than either C1∗ or C2∗ , by entertaining also a third possibility, (6) LD 3 The Task and Methodology We apply transform and join paradigms to grammar induction, an important problem of computational linguistics that involves notoriously difficult objectives (Pereira and Schabes, 1992; de Marcken, 1995; Gimpel and Smith, 2012, inter alia). The goal is to induce grammars capable of parsing unseen text. Input, in both training and testing, is a sequence of tokens labeled as: (i) a lexical item and its category, (w, cw ); (ii) a punctuation mark; or (iii) a sentence boundary. Output is unlabeled dependency trees. 3.1 Models and Data We constrain all parse structures to be projective, via dependency-and-boundary grammars (Spitkovsky et al., 2012a; 2012b): DBMs 0–3 are head-outward generative parsing models (Alshawi, 1996) that distinguish complete sentences from incomplete fra"
D13-1204,N10-1003,0,0.0123214,"iants (Neal and Hinton, 1999), or “dropout training” (Hinton et al., 2012; Wang and Manning, 2013), which is important in supervised settings. Future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction. As frequently employed in tasks like semantic role labeling (Carreras and M`arquez, 2005) and relation extraction (Sun et al., 2011), it may be easier to first establish existence, before trying to understand its nature. Other key next steps may include exploring more intelligent ways of combining systems (Surdeanu and Manning, 2010; Petrov, 2010) and automating the operator discovery process. Furthermore, we are optimistic that both count transforms and model recombination could be usefully incorporated into sampling methods: although symmetrized models may have higher cross-entropies, hence prone to rejection in vanilla MCMC, they could work well as seeds in multi-chain designs; existing algorithms, such as MCMCMC (Geyer, 1991), which switch contents of adjacent chains running at different temperatures, may also benefit from introducing the option to combine solutions, in addition to just swapping them. Acknowledgments We thank Yun-H"
D13-1204,P11-1108,0,0.218727,"Missing"
D13-1204,P09-1057,0,0.0169352,"e networks. Multi-start methods (Solis and Wets, 1981) can recover certain global extrema almost surely (i.e., with probability approaching one). Moreover, random restarts via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine 1991 translation (but also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help wi"
D13-1204,P07-1049,0,0.187846,"Missing"
D13-1204,W10-2902,1,0.709401,"improve its own objective without harming the other’s. This approach does not require tuning termination thresholds, allowing optimizers to run to numerical convergence if necessary, and handles only our shorter inputs (l ≤ 15), starting with soft EM (L = SL, for “soft lateen”). Lexicalized models will cover full data (l ≤ 45) and employ “early-stopping lateen” EM (2011a, §2.3), re-estimating via hard EM until soft EM’s objective suffers. Alternating EMs would be expensive here, since updates take (at least) O(l3 ) time, and hard EM’s objective (L = H) is the one better suited to long inputs (Spitkovsky et al., 2010). Our decoders always force an inter-punctuation fragment to derive itself (Spitkovsky et al., 2011b, §2.2).4 In evaluation, such (loose) constraints may help attach sometimes and philology to called (and the science... to is). In training, stronger (strict) constraints also disallow attachment of fragments’ heads by non-heads, to connect Linguistics, called and is (assuming each piece got parsed correctly). 3.4 Final Evaluation and Metrics Evaluation is against held-out CoNLL shared task data (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages. We compute performance as dire"
D13-1204,D11-1117,1,0.889051,"e fragments. The sparse model, DBM-0, assumes a uniform distribution for roots of incomplete inputs and reduces conditioning contexts of stopping probabilities, which works well with split data. We will exploit both DBM-0 and the full DBM,2 drawing also on split, simple and raw views of input text. All experiments prior to final multi-lingual evaluation will use the Penn English Treebank’s Wall Street Journal (WSJ) portion (Marcus et al., 1993) as the underlying tokenized and sentence-broken corpus D. Instead of gold parts-of-speech, we plugged in 200 context-sensitive unsupervised tags, from Spitkovsky et al. (2011c),3 for the word categories. 3.2 Smoothing and Lexicalization All unlexicalized instances of DBMs will be estimated with “add one” (a.k.a. Laplace) smoothing, 2 We use the short-hand DBM to refer to DBM-3, which is equivalent to DBM-2 if D has no internally-punctuated sentences (D=Dsplit), and DBM-1 if all inputs also have trailing punctuation (D=Dsimp ); DBM0 is our short-hand for DBM-0. 3 http://nlp.stanford.edu/pubs/goldtags-data.tar.bz2 1985 using only the word category cw to represent a token. Fully-lexicalized grammars (L-DBM) are left unsmoothed, and represent each token as both a word"
D13-1204,W11-0303,1,0.916587,"e fragments. The sparse model, DBM-0, assumes a uniform distribution for roots of incomplete inputs and reduces conditioning contexts of stopping probabilities, which works well with split data. We will exploit both DBM-0 and the full DBM,2 drawing also on split, simple and raw views of input text. All experiments prior to final multi-lingual evaluation will use the Penn English Treebank’s Wall Street Journal (WSJ) portion (Marcus et al., 1993) as the underlying tokenized and sentence-broken corpus D. Instead of gold parts-of-speech, we plugged in 200 context-sensitive unsupervised tags, from Spitkovsky et al. (2011c),3 for the word categories. 3.2 Smoothing and Lexicalization All unlexicalized instances of DBMs will be estimated with “add one” (a.k.a. Laplace) smoothing, 2 We use the short-hand DBM to refer to DBM-3, which is equivalent to DBM-2 if D has no internally-punctuated sentences (D=Dsplit), and DBM-1 if all inputs also have trailing punctuation (D=Dsimp ); DBM0 is our short-hand for DBM-0. 3 http://nlp.stanford.edu/pubs/goldtags-data.tar.bz2 1985 using only the word category cw to represent a token. Fully-lexicalized grammars (L-DBM) are left unsmoothed, and represent each token as both a word"
D13-1204,D11-1118,1,0.860561,"e fragments. The sparse model, DBM-0, assumes a uniform distribution for roots of incomplete inputs and reduces conditioning contexts of stopping probabilities, which works well with split data. We will exploit both DBM-0 and the full DBM,2 drawing also on split, simple and raw views of input text. All experiments prior to final multi-lingual evaluation will use the Penn English Treebank’s Wall Street Journal (WSJ) portion (Marcus et al., 1993) as the underlying tokenized and sentence-broken corpus D. Instead of gold parts-of-speech, we plugged in 200 context-sensitive unsupervised tags, from Spitkovsky et al. (2011c),3 for the word categories. 3.2 Smoothing and Lexicalization All unlexicalized instances of DBMs will be estimated with “add one” (a.k.a. Laplace) smoothing, 2 We use the short-hand DBM to refer to DBM-3, which is equivalent to DBM-2 if D has no internally-punctuated sentences (D=Dsplit), and DBM-1 if all inputs also have trailing punctuation (D=Dsimp ); DBM0 is our short-hand for DBM-0. 3 http://nlp.stanford.edu/pubs/goldtags-data.tar.bz2 1985 using only the word category cw to represent a token. Fully-lexicalized grammars (L-DBM) are left unsmoothed, and represent each token as both a word"
D13-1204,D12-1063,1,0.625809,"n paradigms to grammar induction, an important problem of computational linguistics that involves notoriously difficult objectives (Pereira and Schabes, 1992; de Marcken, 1995; Gimpel and Smith, 2012, inter alia). The goal is to induce grammars capable of parsing unseen text. Input, in both training and testing, is a sequence of tokens labeled as: (i) a lexical item and its category, (w, cw ); (ii) a punctuation mark; or (iii) a sentence boundary. Output is unlabeled dependency trees. 3.1 Models and Data We constrain all parse structures to be projective, via dependency-and-boundary grammars (Spitkovsky et al., 2012a; 2012b): DBMs 0–3 are head-outward generative parsing models (Alshawi, 1996) that distinguish complete sentences from incomplete fragments in a corpus D: Dcomp comprises inputs ending with punctuation; Dfrag = D − Dcomp is everything 1 If desired, a scaling factor could be used to bias C+ towards either C1∗ or C2∗ , for example based on their likelihood ratio. else. The “complete” subset is further partitioned into simple sentences, Dsimp ⊆ Dcomp , with no internal punctuation, and others, which may be complex. As an example, consider the beginning of an article from (simple) Wikipedia: (i)"
D13-1204,P11-1053,0,0.0155927,"ok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is “partial EM,” which only suppresses updates, other EM variants (Neal and Hinton, 1999), or “dropout training” (Hinton et al., 2012; Wang and Manning, 2013), which is important in supervised settings. Future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction. As frequently employed in tasks like semantic role labeling (Carreras and M`arquez, 2005) and relation extraction (Sun et al., 2011), it may be easier to first establish existence, before trying to understand its nature. Other key next steps may include exploring more intelligent ways of combining systems (Surdeanu and Manning, 2010; Petrov, 2010) and automating the operator discovery process. Furthermore, we are optimistic that both count transforms and model recombination could be usefully incorporated into sampling methods: although symmetrized models may have higher cross-entropies, hence prone to rejection in vanilla MCMC, they could work well as seeds in multi-chain designs; existing algorithms, such as MCMCMC (Geyer"
D13-1204,N10-1091,0,0.0201769,"resses updates, other EM variants (Neal and Hinton, 1999), or “dropout training” (Hinton et al., 2012; Wang and Manning, 2013), which is important in supervised settings. Future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction. As frequently employed in tasks like semantic role labeling (Carreras and M`arquez, 2005) and relation extraction (Sun et al., 2011), it may be easier to first establish existence, before trying to understand its nature. Other key next steps may include exploring more intelligent ways of combining systems (Surdeanu and Manning, 2010; Petrov, 2010) and automating the operator discovery process. Furthermore, we are optimistic that both count transforms and model recombination could be usefully incorporated into sampling methods: although symmetrized models may have higher cross-entropies, hence prone to rejection in vanilla MCMC, they could work well as seeds in multi-chain designs; existing algorithms, such as MCMCMC (Geyer, 1991), which switch contents of adjacent chains running at different temperatures, may also benefit from introducing the option to combine solutions, in addition to just swapping them. Acknowledgments"
D13-1204,D12-1121,0,0.24257,"Missing"
D13-1204,P08-1061,0,0.228516,"Missing"
D13-1204,P10-1076,0,0.0238705,"techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements. We exploited these properties of the combiners to reconcile grammars induced by different views of data (Blum and Mitchell, 1998). One such view retains just the simple sentences, making it easier to recognize root words. Another splits text into many inter-punctuation fragments, helping learn word associations. The induced dependency trees can themselves also be viewed not only as directed structures but also as skeleton parses, facilitating the recovery of correct polarities for unlabeled dependency arcs. By"
D13-1204,P95-1026,0,0.0376466,"data set helps avoid some local optima (Liang and Klein, 2009). Elidan et al. (2002) suggested how example-reweighing could cause “informed” changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements. We exploited thes"
D13-1204,J03-4003,0,\N,Missing
D13-1204,C98-2235,0,\N,Missing
D13-1204,D07-1096,0,\N,Missing
D13-1204,O01-1007,0,\N,Missing
I05-3005,W00-1201,0,0.0138143,"Missing"
I05-3005,J93-2004,0,0.0267039,"Missing"
I05-3005,W04-3236,0,0.0200273,"Missing"
I05-3005,W96-0213,0,0.283886,"ish suffix tokens in WSJ have more than one tag. Most English suffixes are derivational and inflectional suffixes like able, -s and -ed. Such functional suffixes are used to indicate word classes or syntactic function. Chinese, however, has no inflectional suffixes and only a few derivational suffixes and so suffixes may not be as good a cue for word classes. Finally, since Chinese has no derivational morpheme for nominalization, it is difficult to distinguish a nominalization and a verb. 5.2 Our model builds on research into loglinear models by Ng and Low (2004), Toutanova et al., (2003) and Ratnaparkhi (1996). The first research uses independent maximum entropy classifiers, with a sequence model imposing categorical valid tag sequence constraints. The latter two use maximum entropy Markov models (MEMM) (McCallum et al., 2000), that use log-linear models to obtain the probabilities of a state transition given an observation and the previous state, as illustrated in Figure 1 (a). These points suggest that morpheme identity, which is the major feature used in previous research on unknown words in English and German, will be insufficient in Chinese. This suggests the need for more sophisticated featur"
I05-3005,P99-1023,0,0.0284072,"am prefixes and suffixes for n up to 4.10 An example is: Sequence features We examined several tag sequence features from both left and right side of the current word. We use the term lexical features to refer to features derived from the identity of a word, and tag sequence features refer to features derived from the tags of surrounding words. 䌘㹟 INFORMATION-BAG &quot;folder&quot; Wi=䌘㹟 “a folder” FAFFIX={(prefix1,䌘), (prefix2,䌘), (prefix3,䌘 㹟), (suffix1,㹟), (suffix2,㹟), (suffix3,䌘㹟)} These features have been shown to be useful in previous research on English (Toutanova et al, 2003, Brants 2000, Thede and Harper 1999) 5.5.2 CTBMorph (CTBM) The models9 in Table 7 list the different tag sequence features used; they also use the same lexical features from the model 2Rw+2Lw shown in Table 6. The table shows that Model Lt+LLt conditioning on the previous tag and the conjunction of the two previous While affix information can be very informative, we showed earlier that affixes in Chinese are sparse, short, and ambiguous. Thus as our first new feature we used a POS-vector of the set of tags a given affix could have. We used the training set to build a morpheme/POS dictionary with the possible tags for each 8 We a"
I05-3005,N03-1033,1,0.296525,"Missing"
I05-3005,C02-1145,0,0.0347657,"Missing"
I05-3005,A00-1031,0,\N,Missing
I05-3027,P03-2039,0,0.0148303,"of unsegmented characters, Z(X) is a normalization term, fk is a feature function, and c indexes into characters in the sequence being labeled. A CRF allows us to utilize a large number of n-gram features and different state sequence 168 based features and also provides an intuitive framework for the use of morphological features. 3 3.1 Feature engineering Features The linguistic features used in our model fall into three categories: character identity n-grams, morphological and character reduplication features. For each state, the character identity features (Ng & Low 2004, Xue & Shen 2003, Goh et al. 2003) are represented using feature functions that key off of the identity of the character in the current, proceeding and subsequent positions. Specifically, we used four types of unigram feature functions, designated as C0 (current character), C1 (next character), C-1 (previous character), C-2 (the character two characters back). Furthermore, four types of bi-gram features were used, and are notationally designated here as conjunctions of the previously specified unigram features, C0C1, C-1C0, C-1C1, C-2C-1, and C2C0. Given that unknown words are normally more than one character long, when repres"
I05-3027,W04-3236,0,0.101092,"the sentence, X is the sequence of unsegmented characters, Z(X) is a normalization term, fk is a feature function, and c indexes into characters in the sequence being labeled. A CRF allows us to utilize a large number of n-gram features and different state sequence 168 based features and also provides an intuitive framework for the use of morphological features. 3 3.1 Feature engineering Features The linguistic features used in our model fall into three categories: character identity n-grams, morphological and character reduplication features. For each state, the character identity features (Ng & Low 2004, Xue & Shen 2003, Goh et al. 2003) are represented using feature functions that key off of the identity of the character in the current, proceeding and subsequent positions. Specifically, we used four types of unigram feature functions, designated as C0 (current character), C1 (next character), C-1 (previous character), C-2 (the character two characters back). Furthermore, four types of bi-gram features were used, and are notationally designated here as conjunctions of the previously specified unigram features, C0C1, C-1C0, C-1C1, C-2C-1, and C2C0. Given that unknown words are normally more t"
I05-3027,C04-1081,0,0.795857,"as character identity, morphological and character reduplication features. Because our morphological features were extracted from the training corpora automatically, our system was not biased toward any particular variety of Mandarin. Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers. Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR). 1 2 Algorithm Our system builds on research into conditional random field (CRF), a statistical sequence modeling framework first introduced by Lafferty et al. (2001). Work by Peng et al. (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. Gaussian priors were used to prevent overfitting and a quasi-Newton method was used for parameter optimization. The probability assigned to a label sequence for a particular sequence of characters by a CRF is given by the equation below: Introduction The 2005 Sighan Bakeoff included four different corpora, Academia Sinica (AS), City University of Hong Kong (HK), Peking University (PK), and Microsoft"
I05-3027,W03-1719,0,0.00712342,", such that each character is labeled either as the beginning of a word or the continuation of one. Gaussian priors were used to prevent overfitting and a quasi-Newton method was used for parameter optimization. The probability assigned to a label sequence for a particular sequence of characters by a CRF is given by the equation below: Introduction The 2005 Sighan Bakeoff included four different corpora, Academia Sinica (AS), City University of Hong Kong (HK), Peking University (PK), and Microsoft Research Asia (MSR), each of which has its own definition of a word. In the 2003 Sighan Bakeoff (Sproat & Emerson 2003), no single model performed well on all corpora included in the task. Rather, systems tended to do well on corpora largely drawn from a set of similar Mandarin varieties to the one they were originally developed for. Across corPλ (Y |X ) = 1 § exp ¨ ¦¦ λ Z(X ) © c∈C k k · f k (Y , X , c )¸ c  Y is the label sequence for the sentence, X is the sequence of unsegmented characters, Z(X) is a normalization term, fk is a feature function, and c indexes into characters in the sequence being labeled. A CRF allows us to utilize a large number of n-gram features and different state sequence 168 based f"
I05-3027,W03-1728,0,0.0089699,"X is the sequence of unsegmented characters, Z(X) is a normalization term, fk is a feature function, and c indexes into characters in the sequence being labeled. A CRF allows us to utilize a large number of n-gram features and different state sequence 168 based features and also provides an intuitive framework for the use of morphological features. 3 3.1 Feature engineering Features The linguistic features used in our model fall into three categories: character identity n-grams, morphological and character reduplication features. For each state, the character identity features (Ng & Low 2004, Xue & Shen 2003, Goh et al. 2003) are represented using feature functions that key off of the identity of the character in the current, proceeding and subsequent positions. Specifically, we used four types of unigram feature functions, designated as C0 (current character), C1 (next character), C-1 (previous character), C-2 (the character two characters back). Furthermore, four types of bi-gram features were used, and are notationally designated here as conjunctions of the previously specified unigram features, C0C1, C-1C0, C-1C1, C-2C-1, and C2C0. Given that unknown words are normally more than one character"
I05-3027,P04-1059,0,\N,Missing
I05-3027,W03-1726,0,\N,Missing
J00-3003,J96-1002,0,0.0546144,"Missing"
J00-3003,P93-1035,0,0.0509692,"Missing"
J00-3003,J96-2004,0,0.271477,"Missing"
J00-3003,A88-1019,0,0.039726,"(the forward-backward algorithm) The Viterbi algorithm for HMMs (Viterbi 1967) finds the globally most probable state sequence. When applied to a discourse model with locally decomposable likelihoods and Markovian discourse grammar, it will therefore find precisely the DA 348 Stolcke et al. Dialogue Act Modeling sequence with the highest posterior probability: U* = argmaxP(UIE ) u (4) The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988). It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995). To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1. . . . . n. We can compute the per-utterance posterior DA probabilities by summing: P(u[E) = E P(UIE) (5) U: Ui=u where the summation is over all sequences U whose ith element matches the label in question. The summati"
J00-3003,J95-2001,0,0.0226748,"ly decomposable likelihoods and Markovian discourse grammar, it will therefore find precisely the DA 348 Stolcke et al. Dialogue Act Modeling sequence with the highest posterior probability: U* = argmaxP(UIE ) u (4) The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988). It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995). To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1. . . . . n. We can compute the per-utterance posterior DA probabilities by summing: P(u[E) = E P(UIE) (5) U: Ui=u where the summation is over all sequences U whose ith element matches the label in question. The summation is efficiently carried out by the forward-backward algorithm for HMMs (Baum et al. 1970). 3 For zeroth-order (unigram) discourse grammars, Viterbi decoding and forwardbackward decoding n"
J00-3003,J86-3001,0,0.084288,"h within this framework can be characterized by which of these simplifications are addressed. Dialogue grammars for conversational speech need to be made more aware of the temporal properties of utterances. For example, we are currently not modeling the fact that utterances by the conversants may actually overlap (e.g., backchannels interrupting an ongoing utterance). In addition, we should model more of the nonlocal aspects of discourse structure, despite our negative results so far. For example, a context-free discourse grammar could potentially account for the nested structures proposed in Grosz and Sidner (1986). 1° The standard n-gram models for DA discrimination with lexical cues are probably suboptimal for this task, simply because they are trained in the maximum likelihood framework, without explicitly optimizing discrimination between DA types. This may be overcome by using discriminative training procedures (Warnke et al. 1999; Ohler, Harbeck, and Niemann 1999). Training neural networks directly with posterior probability (Ries 1999a) seems to be a more principled approach and it also offers much easier integration with other knowledge sources. Prosodic features, for example, can simply be adde"
J00-3003,J93-3003,0,0.0155965,"Missing"
J00-3003,H94-1014,0,0.0448551,"Missing"
J00-3003,W98-0319,1,0.400792,"Missing"
J00-3003,W99-0306,1,0.141442,"although efforts are under way to develop DA labeling systems that are domain-independent, such as the Discourse Resource Initiative&apos;s DAMSL architecture (Core and Allen 1997). While not constituting dialogue understanding in any deep sense, DA tagging seems clearly useful to a range of applications. For example, a meeting summarizer needs to keep track of who said what to whom, and a conversational agent needs to know whether it was asked a question or ordered to do something. In related work DAs are used as a first processing step to infer dialogue games (Carlson 1983; Levin and Moore 1977; Levin et al. 1999), a slightly higher level unit that comprises a small number of DAs. Interactional dominance (Linell 1990) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand. In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words. Another important role of DA information could be feedback to lower-level processing. For example, a speech recognizer could be constrained by expectations of likely DAs in a given context, constraining the potential re"
J00-3003,P98-2188,0,0.0191727,"Missing"
J00-3003,J01-1002,0,\N,Missing
J00-3003,C98-2183,0,\N,Missing
J02-3001,P98-1013,0,0.372385,"Missing"
J02-3001,A00-2031,0,0.0493528,"iven approach to information extraction, Riloff (1993) builds a dictionary of patterns for filling slots in a specific domain such as terrorist attacks, and Riloff and Schmelzenbach (1998) extend this technique to derive automatically entire “case frames” for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approach to generalizing beyond the relatively small number of frames considered in the tasks. More recently, a domain-independent system has been trained by Blaheta and Charniak (2000) on the function tags, such as Manner and Temporal, included in the Penn Treebank corpus. Some of these tags correspond to FrameNet semantic roles, but the Treebank tags do not include all the arguments of most predicates. In this article, we aim to develop a statistical system for automatically learning to identify all semantic roles for a wide variety of predicates in unrestricted text. 251 Computational Linguistics Volume 28, Number 3 4. Probability Estimation for Roles In this section we describe the first, basic version of our statistically trained system for automatically identifying fra"
J02-3001,W98-1505,0,0.0480626,"nique is based on the expectation that words with similar semantics will tend to co-occur with the same other sets of words. For example, nouns describing foods will tend to occur as direct objects of verbs such as eat devour, and savor. The clustering algorithm attempts to find such patterns of co-occurrence from the counts of grammatical relations between pairs of specific words in the corpus, without the use of any external knowledge or semantic representation. We extracted verb–direct object relations from an automatically parsed version of the British National Corpus, using the parser of Carroll and Rooth (1998).4 Clustering 4 We are indebted to Mats Rooth and Sabine Schulte im Walde for providing us with the parsed corpus. 267 Computational Linguistics Volume 28, Number 3 was performed using the probabilistic model of co-occurrence described in detail by Hofmann and Puzicha (1998). (For other natural language processing [NLP] applications of the probabilistic clustering algorithm, see, e.g., Rooth [1995], Rooth et al. [1999]; for application to language modeling, see Gildea and Hofmann [1999]. According to this model, the two observed variables, in this case the verb and the head noun of its object,"
J02-3001,P97-1003,0,0.094406,"Missing"
J02-3001,P99-1001,0,0.188406,"e. For example, Lapata and Brew (1999) and others have shown that the different syntactic subcategorization frames of a verb such as serve can be used to help disambiguate a particular instance of the word. Adding semantic role subcategorization information to this syntactic information could extend this idea to 246 Gildea and Jurafsky Automatic Labeling of Semantic Roles use richer semantic knowledge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging field of text data mining (TDM) (Hearst 1999). Finally, incorporating semantic roles into probabilistic models of language may eventually yield more accurate parsers and better language models for speech recognition. This article describes an algorithm for identifying the semantic roles filled by constituents in a sentence. We apply statistical techniques that have been successful for the related problems of syntactic parsing, part-of-speech tagging, and word sense disambiguation, including probabilistic parsing and statistical classification. Our statistical algorithms are trained on a hand-labeled data set: the FrameNet database (Baker"
J02-3001,W00-2021,0,0.0773412,"eneralizations relevant to argument linking can be made. Our results for unseen predicates within the same frame are encouraging, indicating that the predicates are semantically similar in ways that result in similar argument structure, as the semantically based theories of linking advocated by Levin (1993) and Levin and Rappaport Hovav (1996) would predict. We hope that corpus-based systems such as ours can provide a way of testing and elaborating such theories in the future. We believe that some level of skeletal representation of the relevant aspects of a word’s meaning, along the lines of Kipper et al. (2000) and of the frame hierarchy being developed by the FrameNet project, could be used in the future to help a statistical system generalize from similar words for which training data are available. 10. Conclusion Our system is able to label semantic roles automatically with fairly high accuracy, indicating promise for applications in various natural language tasks. Semantic roles do not seem to be simple functions of a sentence’s syntactic tree structure, and lexical statistics were found to be extremely valuable, as has been the case in other natural language processing applications. Although le"
J02-3001,W99-0632,0,0.07496,"me. Such common frames might allow a question-answering system to take a question like (5) and discover that (6) is relevant in constructing an answer to the question: (5) Which party sent absentee ballots to voters? (6) Both Democratic and Republican voters received absentee ballots from their party. This shallow semantic level of interpretation has additional uses outside of generalizing information extraction, question answering, and semantic dialogue systems. One such application is in word sense disambiguation, where the roles associated with a word can be cues to its sense. For example, Lapata and Brew (1999) and others have shown that the different syntactic subcategorization frames of a verb such as serve can be used to help disambiguate a particular instance of the word. Adding semantic role subcategorization information to this syntactic information could extend this idea to 246 Gildea and Jurafsky Automatic Labeling of Semantic Roles use richer semantic knowledge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging field of text data mining (TDM) (Hearst 1999). Finally, incorporating s"
J02-3001,H94-1020,0,0.0601571,"f the string, the part of speech of the target word and, as the last element, the phrase type or syntactic category of the sentence constituent marked as a frame element. After some experimentation, we settled on a version of the path feature that collapses the various part-of-speech tags for verbs, including past-tense verb (VBD), third-person singular present-tense verb (VBZ), other present-tense verb (VBP), and past participle (VBN), into a single verb tag denoted “VB.” Our path feature is dependent on the syntactic representation used, which in our case is the Treebank-2 annotation style (Marcus et al. 1994), as our parser is trained on this later version of the Treebank data. Figure 4 shows the annotation for the sentence “They expect him to cut costs throughout the organization,” which exhibits 254 Gildea and Jurafsky Automatic Labeling of Semantic Roles S VP NP PRP NP VB DT He ate some NN pancakes Figure 3 In this example, the path from the target word ate to the frame element He can be represented as VB↑VP↑S↓NP, with ↑ indicating upward movement in the parse tree and ↓ downward movement. The NP corresponding to He is found as described in Section 4.1.1. Figure 4 Treebank annotation of raising"
J02-3001,J93-2004,0,0.0651427,"Missing"
J02-3001,A00-2034,0,0.0507751,"matic clustering described above can be seen as an imperfect method of deriving semantic classes from the vocabulary, and we might expect a hand-developed set of classes to do better. We tested this hypothesis using WordNet (Fellbaum 1998), a freely available semantic hierarchy. The basic technique, when presented with a head word for which no training examples had been seen, was to ascend the type hierarchy until reaching a level for which training data are available. To do this, counts of training data were percolated up the semantic hierarchy in a technique similar to that of, for example, McCarthy (2000). For each training example, the count # (r, s, pt, t) was incremented in a table indexed by the semantic role r, WordNet sense s, phrase type pt, and target word t, for each WordNet sense s above the head word h in the hypernym hierarchy. In fact, the WordNet hierarchy is not a tree, but rather includes multiple inheritance. For example, person has as hypernyms both life form and causal agent. In such cases, we simply took the first hypernym listed, effectively converting the structure into a tree. A further complication is that several WordNet senses are possible for a given head word. We si"
J02-3001,A00-2030,0,0.176136,"stem. In this section, we explore 276 Gildea and Jurafsky Automatic Labeling of Semantic Roles the interaction between semantic roles and syntactic parsing by integrating the parser with the semantic-role probability model. This allows the semantic-role assignment to affect the syntactic attachment decisions made by the parser, with the hope of improving the accuracy of the complete system. Although most statistical parsing work measures performance in terms of syntactic trees without semantic information, an assignment of role fillers has been incorporated into a statistical parsing model by Miller et al. (2000) for the domain-specific templates of the Message Understanding Conference (Defense Advanced Research Projects Agency 1998) task. A key finding of Miller et al.’s work was that a system developed by annotating role fillers in text and training a statistical system performed at the same level as one based on writing a large system of rules, which requires much more highly skilled labor to design. 8.1 Incorporating Roles into the Parsing Model We use as the baseline of all our parsing experiments the model described in Collins (1999). The algorithm is a form of chart parsing, which uses dynamic"
J02-3001,P93-1024,0,0.118089,"Missing"
J02-3001,P99-1014,0,0.205967,"Missing"
J02-3001,C69-0201,0,0.146482,"the from airport, to airport, or depart time discussed above, or verb-specific roles such as eater and eaten for the verb eat. The opposite end of the spectrum consists of theories with only two “proto-roles” or “macroroles”: Proto-Agent and Proto-Patient (Van Valin 1993; Dowty 1991). In between lie many theories with approximately 10 roles, such as Fillmore’s (1971) list of nine: Agent, Experiencer, Instrument, Object, Source, Goal, Location, Time, and Path.1 1 There are scores of other theories with slightly different sets of roles, including those of Fillmore (1968), Jackendoff (1972), and Schank (1972); see Somers (1987) for an excellent summary. 247 Computational Linguistics Domain: Volume 28, Number 3 Communication Frame: Frame: Conversation Frame Elements: Protagonist−1 Protagonist−2 Protagonists Topic Medium debate−v tiff−n banter−v dispute−n converse−v gossip−v discussion−n Cognition Questioning Frame Elements: Frame: argue−v Domain: Speaker Addressee Message Topic Medium Statement Frame Elements: Speaker Addressee Message Topic Medium Frame: Judgment Frame Elements: blame−v admire−v admiration−n Judge Evaluee Reason Role Frame: Categorization Frame Elements: Cognizer Item Category Cri"
J02-3001,A00-1010,0,0.0747477,"ersity of Colorado, Boulder, CO 80309. E-mail: jurafsky@colorado.edu c 2002 Association for Computational Linguistics  Computational Linguistics Volume 28, Number 3 far away as it once was, thanks to the development of large semantic databases such as WordNet (Fellbaum 1998) and progress in domain-independent machine learning algorithms. Current information extraction and dialogue understanding systems, however, are still based on domain-specific frame-and-slot templates. Systems for booking airplane information use domain-specific frames with slots like orig city, dest city, or depart time (Stallard 2000). Systems for studying mergers and acquisitions use slots like products, relationship, joint venture company, and amount (Hobbs et al. 1997). For natural language understanding tasks to proceed beyond these specific domains, we need semantic frames and semantic understanding systems that do not require a new set of slots for each new application domain. In this article we describe a shallow semantic interpreter based on semantic roles that are less domain specific than to airport or joint venture company. These roles are defined at the level of semantic frames of the type introduced by Fillmor"
J02-3001,H89-1033,0,0.0892135,"to appear in tasks in which understanding and semantics play a greater role. For example, there has been widespread commercial deployment of simple speech-based natural language understanding systems that answer questions about flight arrival times, give directions, report on bank balances, or perform simple financial transactions. More sophisticated research systems generate concise summaries of news articles, answer fact-based questions, and recognize complex semantic and dialogue structure. But the challenges that lie ahead are still similar to the challenge that the field has faced since Winograd (1972): moving away from carefully hand-crafted, domaindependent systems toward robustness and domain independence. This goal is not as ∗ Currently at Institute for Research in Cognitive Science, University of Pennsylvania, 3401 Walnut Street, Suite 400A, Philadelphia, PA 19104. E-mail: dgildea@cis.upenn.edu † Departments of Linguistics and Computer Science, University of Colorado, Boulder, CO 80309. E-mail: jurafsky@colorado.edu c 2002 Association for Computational Linguistics  Computational Linguistics Volume 28, Number 3 far away as it once was, thanks to the development of large semantic databa"
J02-3001,W98-1106,0,\N,Missing
J02-3001,J03-4003,0,\N,Missing
J02-3001,C98-1013,0,\N,Missing
J02-3001,P96-1008,0,\N,Missing
J96-4003,J95-4011,0,0.0207904,"cal rules as finitestate transducers that accept underlying forms as input and generate surface forms as output. Johnson (1972) first observed that traditional phonological rewrite rules can be expressed as regular (finite-state) relations if one accepts the constraint that no rule may reapply directly to its own output. This means that finite-state transducers (FSTs) can be used to represent phonological rules, greatly simplifying the problem of parsing the output of phonological rules in order to obtain the underlying, lexical forms (Koskenniemi 1983; Karttunen 1993; Pulman and Hepple 1993; Bird 1995; Bird and Ellison 1994). The fact that the weaker generative capacity of FSTs makes them easier to 498 Gildea and Jurafsky Learning Bias and Phonological-Rule Induction learn than arbitrary context-sensitive rules has allowed the development of a number of learning algorithms including those for deterministic finite-state automata (FSAs) (Freund et al. 1993), deterministic transducers (Oncina, Garcia, and Vidal 1993), as well as nondeterministic (stochastic) FSAs (Stolcke and Omohundro 1993; Stolcke and Omohundro 1994; Ron, Singer, and Tishby 1994). Like the empiricist models discussed above,"
J96-4003,J94-1003,0,0.0278125,"s finitestate transducers that accept underlying forms as input and generate surface forms as output. Johnson (1972) first observed that traditional phonological rewrite rules can be expressed as regular (finite-state) relations if one accepts the constraint that no rule may reapply directly to its own output. This means that finite-state transducers (FSTs) can be used to represent phonological rules, greatly simplifying the problem of parsing the output of phonological rules in order to obtain the underlying, lexical forms (Koskenniemi 1983; Karttunen 1993; Pulman and Hepple 1993; Bird 1995; Bird and Ellison 1994). The fact that the weaker generative capacity of FSTs makes them easier to 498 Gildea and Jurafsky Learning Bias and Phonological-Rule Induction learn than arbitrary context-sensitive rules has allowed the development of a number of learning algorithms including those for deterministic finite-state automata (FSAs) (Freund et al. 1993), deterministic transducers (Oncina, Garcia, and Vidal 1993), as well as nondeterministic (stochastic) FSAs (Stolcke and Omohundro 1993; Stolcke and Omohundro 1994; Ron, Singer, and Tishby 1994). Like the empiricist models discussed above, these algorithms are al"
J96-4003,J92-4003,0,0.0880468,"onology (e.g., optimality constraints). Second, we assume that a cognitive model of automaton induction would be more stochastic and hence more robust than the OSTIA algorithm underlying our work. 1 Rather, our model is intended to suggest the kind of biases that may be added to empiricist induction models to build a learning model for phonological rules that is cognitively and computationally plausible. Furthermore, our model is not necessarily nativist; these biases may be innate, but they may also be the product of some other earlier learning algorithm, as the results of Ellison (1992) and Brown et al. (1992) suggest (see Section 5.2). So our results suggest that assuming in the system some very general and fundamental properties of phonological knowledge (whether innate or previously learned) and learning others empirically may provide a basis for future learning models. Ellison (1994), for example, has shown how to map the optimality constraints of Prince and Smolensky (1993) to finite-state automata; given this result, models of 1 Although our assumption of the simultaneous presentation of surface and underlying forms to the learner may seem at first glance to be unnatural as well, it is quite"
J96-4003,J94-3007,0,0.0180628,"Missing"
J96-4003,C94-2163,0,0.0820543,"tate transducers that accept underlying forms as input and generate surface forms as output. Johnson (1972) first observed that traditional phonological rewrite rules can be expressed as regular (finite-state) relations if one accepts the constraint that no rule may reapply directly to its own output. This means that finite-state transducers (FSTs) can be used to represent phonological rules, greatly simplifying the problem of parsing the output of phonological rules in order to obtain the underlying, lexical forms (Koskenniemi 1983; Karttunen 1993; Pulman and Hepple 1993; Bird 1995; Bird and Ellison 1994). The fact that the weaker generative capacity of FSTs makes them easier to 498 Gildea and Jurafsky Learning Bias and Phonological-Rule Induction learn than arbitrary context-sensitive rules has allowed the development of a number of learning algorithms including those for deterministic finite-state automata (FSAs) (Freund et al. 1993), deterministic transducers (Oncina, Garcia, and Vidal 1993), as well as nondeterministic (stochastic) FSAs (Stolcke and Omohundro 1993; Stolcke and Omohundro 1994; Ron, Singer, and Tishby 1994). Like the empiricist models discussed above, these algorithms are al"
J96-4003,P84-1070,0,0.572124,"in the transducer system. 8. Related Work Recent work in the machine learning of phonology includes algorithms for learning both segmental and nonsegmental information. Nonsegmental approaches include those of Daelemans, Gillis, and Durieux (1994) for learning stress systems, as well as approaches to learning morphology such as Gasser&apos;s (1993) system for inducing Semitic morphology, and Ellison&apos;s (1992) extensive work on syllabicity, sonority, and harmony. Since our approach learns only segmental structure, a more relevant comparison is with other algorithms for inducing segmental structure. Johnson (1984) gives one of the first computational algorithms for phonological rule induction. His algorithm works for rules of the form (15) a --* b/C where C is the feature matrix of the segments around a. Johnson&apos;s algorithm sets up a system of constraint equations that C must satisfy, by considering both the positive contexts, i.e., all the contexts Ci in which a b occurs on the surface, as well as all the negative contexts Cj in which an a occurs on the surface. The set of all positive and negative contexts will not generally determine a unique rule, but will determine a set of possible rules. Johnson"
J96-4003,J94-3001,0,0.0163909,"te rules. For example, in American English an underlying t is realized as a flap (a tap of the tongue on the alveolar ridge) after a stressed vowel and zero or more r&apos;s, and before an unstressed vowel. In the rewrite-rule formalism of Chomsky and Halle (1968), this rule would be represented as in (1). (1) t --~ d x / Q r* __ V Since Johnson&apos;s (1972) work, researchers have proposed a number of different ways to represent such phonological rules by transducers. The most popular method is the two-level formalism of Koskenniemi (1983), based on Johnson (1972) and the (belatedly published) work of Kaplan and Kay (1994), and various implementations and extensions (summarized and contrasted in Karttunen [1993]). The basic intuition of two-level phonology is that a rule that rewrites an underlying string as a surface string can be implemented as a transducer that reads from an underlying tape and writes to a surface tape. Figure 1 shows an example of a transducer that implements the flapping rule in (1). Each arc has an input symbol and an output symbol, separated by a colon. A single symbol (such as t or V) is a shorthand for a symbol that is the same in the input and output (i.e., t : t or V:V). Either the i"
N01-1024,W99-0904,0,0.0941976,"morphological sub-problems in German). 2 Previous Approaches Previous morphology induction approaches have fallen into three categories. These categories differ depending on whether human input is provided and on whether the goal is to obtain affixes or complete morphological analysis. We here briefly describe work in each category. 2.1 Using a Knowledge Source to Bootstrap Some researchers begin with some initial humanlabeled source from which they induce other morphological components. In particular, Xu and Croft (1998) use word context derived from a corpus to refine Porter stemmer output. Gaussier (1999) induces derivational morphology using an inflectional lexicon which includes part of speech information. Grabar and Zweigenbaum (1999) use the SNOMED corpus of semantically-arranged medical terms to find semantically-motivated morphological relationships. Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language’s inflectional parts of speech, and the canonical suffixes for each part of speech. 2.2 Affix Inventories A second, knowledge-free category of research ha"
N01-1024,W00-0712,1,0.84234,"at a stem has ended when the number of characters following it exceed some given threshold and identifies any residual following the stems as suffixes. 2.3 Complete morphological analysis Due to the existence of morphological ambiguity (such as with the word “caring” whose stem is “care” rather than “car”), finding affixes alone does not constitute a complete morphological analysis. Hence, the last category of research is also knowledge-free but attempts to induce, for each word of a corpus, a complete analysis. Since our approach falls into this category (expanding upon our earlier approach (Schone and Jurafsky, 2000)), we describe work in this area in more detail. 2.3.1 Jacquemin’s multiword approach Jacquemin (1997) deems pairs of word n-grams as morphologically related if two words in the first ngram have the same first few letters (or stem) as two words in the second n-gram and if there is a suffix for each stem whose length is less than k. He also clusters groups of words having the same kinds of word endings, which gives an added performance boost. He applies his algorithm to a French term list and scores based on sampled, by-hand evaluation. 2.3.2. Goldsmith: EM and MDLs Goldsmith (1997/2000) tries"
N01-1024,P00-1027,0,0.489079,"nalysis. We here briefly describe work in each category. 2.1 Using a Knowledge Source to Bootstrap Some researchers begin with some initial humanlabeled source from which they induce other morphological components. In particular, Xu and Croft (1998) use word context derived from a corpus to refine Porter stemmer output. Gaussier (1999) induces derivational morphology using an inflectional lexicon which includes part of speech information. Grabar and Zweigenbaum (1999) use the SNOMED corpus of semantically-arranged medical terms to find semantically-motivated morphological relationships. Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language’s inflectional parts of speech, and the canonical suffixes for each part of speech. 2.2 Affix Inventories A second, knowledge-free category of research has focused on obtaining affix inventories. Brent, et al. (1995) used minimum description length (MDL) to find the most data-compressing suffixes. Kazakov (1997) does something akin to this using MDL as a fitness metric for evolutionary computing. DéJean (1998) uses a strategy similar to that of"
N01-1024,W98-1239,0,\N,Missing
N01-1024,J01-2001,0,\N,Missing
N04-1032,W00-1201,0,0.0527764,"Missing"
N04-1032,C02-1126,0,0.114738,"Missing"
N04-1032,J02-3001,1,0.343478,"25 sentences in the training set and 113 sentences in the test set, and each test set verb has been seen in the training set. The list of verbs chosen and their number of senses, argument numbers and frequencies are given in Table 2. Table 2 Verb List of verbs for experiments # of Arg Freq senses number 成立/set up 1 2 106 出现/emerge 1 1 80 发表/publish 1 2 113 给予/give 2 3/2 41 建成/build into 2 2/3 113 进入/enter 1 2 123 举行/take place 1 2 230 通过/pass 3 2 75 希望/hope 1 2 90 增加/increase 1 2 167 3 Semantic Parsing 3.1 Architecture and Classifier Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem. For each (non-aux/non-copula) verb in each sentence, our classifier examines each node in the syntactic parse tree for the sentence and assigns it a semantic role label. Most constituents are not arguments of the verb, and so the most common label is NULL. Our architecture is based on a Support Vector Machine classifier, following Pradhan et al. (2003). Since SVMs are binary classifiers, we represent this 1-of-19 classification problem (18 roles plus NULL) by training 19 binary one-versus-all classifiers. Following Pradha"
N04-1032,P02-1031,0,0.0531504,"Missing"
N04-1032,W00-0730,0,0.0754079,"ssification problem. For each (non-aux/non-copula) verb in each sentence, our classifier examines each node in the syntactic parse tree for the sentence and assigns it a semantic role label. Most constituents are not arguments of the verb, and so the most common label is NULL. Our architecture is based on a Support Vector Machine classifier, following Pradhan et al. (2003). Since SVMs are binary classifiers, we represent this 1-of-19 classification problem (18 roles plus NULL) by training 19 binary one-versus-all classifiers. Following Pradhan et al. (2003), we used tinySVM along with YamCha (Kudo and Matsumoto 2000, 2001) as the SVM training and test software. The system uses a polynominal kernel with degree 2; the cost per unit violation of the margin, C=1; tolerance of the termination criterion e=0.001. 3.2 Features The literature on semantic parsing in English relies on a number of features extracted from the input sentence and its parse. These include the constituent’s syntactic phrase type, head word, and governing category, the syntactic path in the parse tree connecting it to the verb, whether the constitutent is before or after the verb, the subcategorization bias of the verb, and the voice (act"
N04-1032,N01-1025,0,0.108751,"Missing"
N04-1032,P03-1056,0,0.0897404,"Missing"
N04-1032,C02-1145,0,0.0849612,"Missing"
N04-1032,W03-1707,0,0.20202,"nseen verbs, given hand-corrected (Chinese TreeBank) syntactic parses. We then describe our port of the Collins (1999) parser to Chinese. Finally, we apply our SVM semantic parser to a matching English corpus, and discuss the differences between English and Chinese that lead to significantly better performance on Chinese. 2 Semantic Annotation and the Corpus Work on semantic parsing in English has generally related on the PropBank, a portion of the Penn TreeBank in which the arguments of each verb are annotated with semantic roles. Although a project to produce a Chinese PropBank is underway (Xue and Palmer 2003), this data is not expected to be available for another year. For these experiments, we therefore hand-labeled a small corpus following the Penn Chinese Propbank labeling guidelines (Xue, 2002). In this section, we first describe the semantic roles we used in the annotation and then introduce the data for our experiments. 2.1 Semantic roles Semantic roles in the English (Kingsbury et al 2002) and Chinese (Xue 2002) PropBanks are grouped into two major types: (1) arguments, which represent central participants in an event. A verb may require one, two or more arguments and they are represented w"
N04-1032,J03-4003,0,\N,Missing
N04-1032,P03-1002,0,\N,Missing
N04-1032,P99-1065,0,\N,Missing
N04-4036,P01-1017,0,0.047049,"Missing"
N04-4036,P00-1065,1,0.910949,"Missing"
N04-4036,J02-3001,1,0.584303,"Missing"
N04-4036,P02-1031,0,0.086694,"Missing"
N04-4036,N03-2009,1,0.830112,"Missing"
N04-4036,kingsbury-palmer-2002-treebank,0,0.0553927,"Missing"
N04-4036,P98-1013,0,0.180142,"Missing"
N04-4036,W00-0730,0,0.0872495,"Missing"
N04-4036,N01-1025,0,0.0931378,"Missing"
N04-4036,C98-1013,0,\N,Missing
N04-4036,J02-3004,0,\N,Missing
N04-4036,P03-1002,0,\N,Missing
N04-4038,N03-2009,1,0.224384,"ied to the problem of POS tagging and BP Chunking. Such problems are cast as a classification problem where, given a number of features extracted from a predefined linguistic context, the task is to predict the class of a token. Support Vector Machines (SVMs) (Vapnik, 1995) are one class of such model. SVMs are a supervised learning algorithm that has the advantage of being robust where it can handle a large number of (overlapping) features with good generalization performance. Consequently, SVMs have been applied in many NLP tasks with great success (Joachims, 1998; Kudo and Matsumato, 2000; Hacioglu and Ward, 2003). We adopt a tagging perspective for the three tasks. Thereby, we address them using the same SVM experimental setup which comprises a standard SVM as a multiclass classifier (Allwein et al., 2000). The difference for the three tasks lies in the input, context and features. None of the features utilized in our approach is explicitly language dependent. The following subsections illustrate the different tasks and their corresponding features and tag sets. 4.1 Word Tokenization We approach word tokenization (segmenting off clitics) as a one-of-six classification task, in which each letter in a w"
N04-4038,W00-0730,0,0.043484,"arning approaches are applied to the problem of POS tagging and BP Chunking. Such problems are cast as a classification problem where, given a number of features extracted from a predefined linguistic context, the task is to predict the class of a token. Support Vector Machines (SVMs) (Vapnik, 1995) are one class of such model. SVMs are a supervised learning algorithm that has the advantage of being robust where it can handle a large number of (overlapping) features with good generalization performance. Consequently, SVMs have been applied in many NLP tasks with great success (Joachims, 1998; Kudo and Matsumato, 2000; Hacioglu and Ward, 2003). We adopt a tagging perspective for the three tasks. Thereby, we address them using the same SVM experimental setup which comprises a standard SVM as a multiclass classifier (Allwein et al., 2000). The difference for the three tasks lies in the input, context and features. None of the features utilized in our approach is explicitly language dependent. The following subsections illustrate the different tasks and their corresponding features and tag sets. 4.1 Word Tokenization We approach word tokenization (segmenting off clitics) as a one-of-six classification task, i"
N04-4038,W00-0726,0,0.165552,"Missing"
N10-1080,W08-0312,0,0.027162,"nfigurations of TERp, WER, several configurations of METEOR, as well as additive combinations of these metrics. The TERp configurations include the default configuration of TERp and TERpA: the configuration of TERp that was trained to match human judgments for NIST Metrics MATR (Matthew Snover and Schwartz, 2008; Przybocki et al., 2008). For METEOR, we used the standard METEOR English parameters (α = 0.8, β = 2.5, γ = 0.4), and the English parameters for the ranking METEOR (α = 0.95, β = 0.5, γ = 0.5),4 which was tuned to maximize the metric’s correlation with WMT-07 human ranking judgements (Agarwal and Lavie, 2008). The default METEOR parameters favor longer translations than the other metrics, since high α values place much more weight on unigram recall than precision. Since this may put models tuned to METEOR at a disadvantage when being evaluated by the other metrics, we also use a variant of the standard English model and of ranking METEOR with α set to 0.5, as this weights both recall and precision equally. For each iteration of MERT, 20 random restarts were used in addition to the best performing point discovered during earlier iterations of training.5 4 Agarwal and Lavie (2008) report γ = 0.45, h"
N10-1080,E06-1032,0,0.137662,"c. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate (TER) (Snover et al., 2006; Snover et al., 2009) and METEOR1 (Lavie and Denkowski, 2009) perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU (Snover et al., 2009; Lavie and Denkowski, 2009; Przybocki et al., 2008; Snover et al., 2006)."
N10-1080,D09-1030,0,0.210252,".0 NIST vs. BLEU:4 51.5 WER vs. TERp 51.5 METR:0.5 vs METR 51.5 TERp vs. BLEU:4 51.0 BLEU:4 vs. METR R:0.5 50.5 p-value 0.0028 0.02 0.089 0.089 0.11 0.11 0.11 0.22 0.26 0.26 0.31 0.31 0.36 0.36 0.47 < 0.001 0.052 0.069 0.11 0.14 0.36 0.36 0.36 0.42 0.47 Table 5: Select pairwise preference for models trained to different evaluation metrics. For A vs. B, preferred indicates how often A was preferred to B. We bold the better training metric for statistically significant differences. duced by experts by having multiple workers complete each HIT and then combining their answers (Snow et al., 2008; Callison-Burch, 2009). We perform a pairwise comparison of the translations produced for the first 200 sentences of our Chinese to English test data (MT03) and our Arabic to English test data (dev07). The HITs consist of a pair of machine translated sentences and a single human generated reference translation. The reference is chosen at random from those available for each sentence. Capitalization of the translated sentences is restored using an HMM based truecaser (Lita et al., 2003). Turkers are instructed to “. . . select the machine translation generated sentence that is easiest to read and best conveys what i"
N10-1080,N10-2003,1,0.719283,"hat restart points are provided, we use the same series of random restart points for each model. During each iteration of MERT, the random seed is based on the MERT iteration number. Thus, while a different set of random points is selected during each MERT iteration, on any given iteration all models use the same set of points. This prevents models from doing better or worse just because they received different starting points. However, it is still possible that certain random starting points are better for some evaluation metrics than others. 4 Experiments Experiments were run using Phrasal (Cer et al., 2010), a left-to-right beam search decoder that achieves a matching BLEU score to Moses (Koehn et al., 2007) on a variety of data sets. During decoding we made use of a stack size of 100, set the distortion limit to 6, and retrieved 20 translation options for each unique source phrase. Using the selected metrics, we train both Chinese to English and Arabic to English models.6 The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the"
N10-1080,W08-0336,1,0.371777,"-EM aligner (Liang et al., 2006). Phrases were extracted using the grow heuristic (Koehn et al., 2003). However, we threw away all phrases that have a P (e|f ) < 0.0001 in order to reduce the size of the phrase table. From the aligned data, we also extracted a hierarchical reordering model that is similar to popular lexical reordering models (Koehn et al., 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT. 6 Given the amount of time required to train a TERpA model, we only present TERpA results for Chinese to English. 558 Manning, 2008). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) using all of the English material from the parallel data employed to train the phrase table as well as Xinhua Chinese English Parallel News (LDC2002E18).7 The resulting decoding model has 16 features that are optimized during MERT. 4.2 Chinese to English For our Chinese to English system, our phrase table was built using 1,140,693 sentence pairs sampled from the GALE Y2 training data. The Chinese sentences were word segmented using the 2008 version of Stanford Chinese Word Segmenter (Chang et al., 2008"
N10-1080,D08-1024,0,0.0433274,"e language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 1 Introduction Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a num"
N10-1080,N09-1025,0,0.0142183,"rtance the metric assigns to precision and recall. The fact that the WER, TER and TERp models perform very similarly suggests that current phrasebased translation systems lack either the features or the model structure to take advantage of swap edit operations. The situation might be improved by using a model that does a better job of both capturing the structure of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al., 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. Human results indicate that edit distance trained models such as WER and TERp tend to produce lower quality translations than BLEU or NIST trained models. Tuning to METEOR works reasonably well for Chinese, but is not a good choice for Arabic. We suspect that the newer RYPT metric (Zaidan and Callison-Burch, 2009), which directly makes use of human adequacy judgements of substrings, would obtain better human results than the automated metrics presented here. However, like other metrics, we expect performance gains still will be sensitiv"
N10-1080,D08-1089,1,0.909049,"Missing"
N10-1080,P06-1121,0,0.0202158,"o METEOR can be made more robust by setting α to 0.5, which 562 balances the importance the metric assigns to precision and recall. The fact that the WER, TER and TERp models perform very similarly suggests that current phrasebased translation systems lack either the features or the model structure to take advantage of swap edit operations. The situation might be improved by using a model that does a better job of both capturing the structure of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al., 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. Human results indicate that edit distance trained models such as WER and TERp tend to produce lower quality translations than BLEU or NIST trained models. Tuning to METEOR works reasonably well for Chinese, but is not a good choice for Arabic. We suspect that the newer RYPT metric (Zaidan and Callison-Burch, 2009), which directly makes use of human adequacy judgements of substrings, would obtain better human results than the automated metrics presented here"
N10-1080,N03-1017,0,0.0603839,"ned using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the standalone metrics used during training. 4.1 Arabic to English Our Arabic to English system was based on a well ranking 2009 NIST submission (Galley et al., 2009). The phrase table was extracted using all of the allowed resources for the constrained Arabic to English track. Word alignment was performed using the Berkeley cross-EM aligner (Liang et al., 2006). Phrases were extracted using the grow heuristic (Koehn et al., 2003). However, we threw away all phrases that have a P (e|f ) < 0.0001 in order to reduce the size of the phrase table. From the aligned data, we also extracted a hierarchical reordering model that is similar to popular lexical reordering models (Koehn et al., 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT. 6 Given the amount of time required to train a TERpA model, we only present TERpA results for Chinese to English. 558 Manning, 2008). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2"
N10-1080,P07-2045,0,0.0214612,"ng each iteration of MERT, the random seed is based on the MERT iteration number. Thus, while a different set of random points is selected during each MERT iteration, on any given iteration all models use the same set of points. This prevents models from doing better or worse just because they received different starting points. However, it is still possible that certain random starting points are better for some evaluation metrics than others. 4 Experiments Experiments were run using Phrasal (Cer et al., 2010), a left-to-right beam search decoder that achieves a matching BLEU score to Moses (Koehn et al., 2007) on a variety of data sets. During decoding we made use of a stack size of 100, set the distortion limit to 6, and retrieved 20 translation options for each unique source phrase. Using the selected metrics, we train both Chinese to English and Arabic to English models.6 The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the standalone metrics used during training. 4.1 Arabic to English Our Arabic to English system was based"
N10-1080,N06-1014,0,0.0140937,"and Arabic to English models.6 The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the standalone metrics used during training. 4.1 Arabic to English Our Arabic to English system was based on a well ranking 2009 NIST submission (Galley et al., 2009). The phrase table was extracted using all of the allowed resources for the constrained Arabic to English track. Word alignment was performed using the Berkeley cross-EM aligner (Liang et al., 2006). Phrases were extracted using the grow heuristic (Koehn et al., 2003). However, we threw away all phrases that have a P (e|f ) < 0.0001 in order to reduce the size of the phrase table. From the aligned data, we also extracted a hierarchical reordering model that is similar to popular lexical reordering models (Koehn et al., 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT. 6 Given the amount of time required to train a TERpA model, we only present TERpA results for Chinese to English. 558 Manning, 2008). A 5-gram languag"
N10-1080,P03-1020,0,0.00807775,"differences. duced by experts by having multiple workers complete each HIT and then combining their answers (Snow et al., 2008; Callison-Burch, 2009). We perform a pairwise comparison of the translations produced for the first 200 sentences of our Chinese to English test data (MT03) and our Arabic to English test data (dev07). The HITs consist of a pair of machine translated sentences and a single human generated reference translation. The reference is chosen at random from those available for each sentence. Capitalization of the translated sentences is restored using an HMM based truecaser (Lita et al., 2003). Turkers are instructed to “. . . select the machine translation generated sentence that is easiest to read and best conveys what is stated in the reference”. Differences between the two machine translations are emphasized by being underlined and bold faced.9 The resulting HITs are made available only to workers in the United States, as pilot experiments indicated this results in more consistent preference judgments. Three preference judgments are obtained for each pair of translations and are combined using weighted majority vote. As shown in table 5, in many cases the quality of the transla"
N10-1080,niessen-etal-2000-evaluation,0,0.0382506,"to tune the metric to human judgments on a specific language and variation of the evaluation task (e.g., ranking candidate translations vs. reproducing judgments of translations adequacy and fluency). 2.3 Translation Edit Rate TER (Snover et al., 2006) searches for the shortest sequence of edit operations needed to turn a candidate translation into one of the reference translations. The allowable edits are the insertion, deletion, and substitution of individual words and swaps of adjacent sequences of words. The swap operation differentiates TER from the simpler word error rate (WER) metric (Nießen et al., 2000), which only makes use of insertions, deletions, and substitutions. Swaps prevent phrase reorderings from being excessively penalized. Once the shortest sequence of operations is found,3 TER is calculated simply as the number of required edits divided by the reference translation length, or average reference translation length when multiple are available (4). TER = min edits avg ref length (4) TER-Plus (TERp) (Snover et al., 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric’s agreement with human judgments. TERp also introduces three new edit"
N10-1080,J03-1002,0,0.00225018,"ing toolkit (Stolcke, 2002) using all of the English material from the parallel data employed to train the phrase table as well as Xinhua Chinese English Parallel News (LDC2002E18).7 The resulting decoding model has 16 features that are optimized during MERT. 4.2 Chinese to English For our Chinese to English system, our phrase table was built using 1,140,693 sentence pairs sampled from the GALE Y2 training data. The Chinese sentences were word segmented using the 2008 version of Stanford Chinese Word Segmenter (Chang et al., 2008; Tseng et al., 2005). Phrases were extracted by running GIZA++ (Och and Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic (Koehn et al., 2003). From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Koehn et al., 2007). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. The resulting decoding model has 14 features to be trained. 5 Results As seen in tables 1 and 2, the evaluation metric we use during training has a substantia"
N10-1080,P03-1021,0,0.771788,"based metrics like TER or WER. Human preferences for METEOR trained models varies depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 1 Introduction Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgment"
N10-1080,P02-1040,0,0.0968379,"ole in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate"
N10-1080,2006.amta-papers.25,0,0.790114,"tively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate (TER) (Snover et al., 2006; Snover et al., 2009) and METEOR1 (Lavie and Denkowski, 2009) perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU (Snover et al., 2009; Lavie and Denkowski, 2009; Przybocki et al., 2008; Snover et al., 2006). Their better correlations suggest that we might obtain higher quality translations by making use of these new metrics when training our models. We expect that training on a specific metric will produce the best performing model according to that met1 MET"
N10-1080,W09-0441,0,0.314159,"g translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate (TER) (Snover et al., 2006; Snover et al., 2009) and METEOR1 (Lavie and Denkowski, 2009) perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU (Snover et al., 2009; Lavie and Denkowski, 2009; Przybocki et al., 2008; Snover et al., 2006). Their better correlations suggest that we might obtain higher quality translations by making use of these new metrics when training our models. We expect that training on a specific metric will produce the best performing model according to that met1 METEOR: Metric for Evalua"
N10-1080,D08-1027,1,0.129613,"Missing"
N10-1080,D07-1080,0,0.0464296,"depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 1 Introduction Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However"
N10-1080,D09-1006,0,0.0246061,"ture of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al., 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. Human results indicate that edit distance trained models such as WER and TERp tend to produce lower quality translations than BLEU or NIST trained models. Tuning to METEOR works reasonably well for Chinese, but is not a good choice for Arabic. We suspect that the newer RYPT metric (Zaidan and Callison-Burch, 2009), which directly makes use of human adequacy judgements of substrings, would obtain better human results than the automated metrics presented here. However, like other metrics, we expect performance gains still will be sensitive to how the mechanics of the metric interact with the structure and feature set of the decoding model being used. BLEU and NIST’s strong showing in both the machine and human evaluation results indicates that they are still the best general choice for training model parameters. We emphasize that improved metric correlations with human judgments do not imply that models"
N10-1080,I05-3027,1,\N,Missing
N10-1116,J93-2003,0,0.0250106,"ting memory). In both cases, gradually increasing complexity allowed artificial neural networks to master a pseudo-natural grammar they otherwise failed to learn. Initiallylimited capacity resembled maturational changes in working memory and attention span that occur over time in children (Kail, 1984), in line with the “less is more” proposal (Newport, 1988; 1990). Although Rohde and Plaut (1999) failed to replicate this3 result with simple recurrent networks, other machine learning techniques reliably benefit from scaffolded model complexity on a variety of language tasks. In word-alignment, Brown et al. (1993) used IBM Models 1-4 as “stepping stones” to training Model 5. Other prominent examples include “coarse-to-fine” 2 This is akin to McClosky et al.’s (2006) “Goldilocks effect.” Worse, they found that limiting input hindered language acquisition. And making the grammar more English-like (by introducing and strengthening semantic constraints), increased the already significant advantage for “starting large!” With iterative training invoking the optimizer multiple times, creating extra opportunities to converge, Rohde and Plaut (1999) suspected that Elman’s (1993) simulations simply did not allow"
N10-1116,P05-1022,0,0.0127271,"ificant advantage for “starting large!” With iterative training invoking the optimizer multiple times, creating extra opportunities to converge, Rohde and Plaut (1999) suspected that Elman’s (1993) simulations simply did not allow networks exposed exclusively to complex inputs sufficient training time. Our extremely generous, low termination threshold for EM (see §5.1) addresses this concern. However, given the DMV’s purely syntactic POS tag-based approach (see §5), it would be prudent to re-test Baby Steps with a lexicalized model. 3 approaches to parsing, translation and speech recognition (Charniak and Johnson, 2005; Charniak et al., 2006; Petrov et al., 2008; Petrov, 2009), and recently unsupervised POS tagging (Ravi and Knight, 2009). Initial models tend to be particularly simple,4 and each refinement towards a full model introduces only limited complexity, supporting incrementality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1"
N10-1116,N06-1022,0,0.00576195,"ing large!” With iterative training invoking the optimizer multiple times, creating extra opportunities to converge, Rohde and Plaut (1999) suspected that Elman’s (1993) simulations simply did not allow networks exposed exclusively to complex inputs sufficient training time. Our extremely generous, low termination threshold for EM (see §5.1) addresses this concern. However, given the DMV’s purely syntactic POS tag-based approach (see §5), it would be prudent to re-test Baby Steps with a lexicalized model. 3 approaches to parsing, translation and speech recognition (Charniak and Johnson, 2005; Charniak et al., 2006; Petrov et al., 2008; Petrov, 2009), and recently unsupervised POS tagging (Ravi and Knight, 2009). Initial models tend to be particularly simple,4 and each refinement towards a full model introduces only limited complexity, supporting incrementality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, h"
N10-1116,N09-1009,0,0.666233,"sion that targets dependencies and ∗ Partially funded by NSF award IIS-0811974; first author supported by the Fannie & John Hertz Foundation Fellowship. assumes partial annotation, e.g., sentence boundaries, tokenization and typically even part-of-speech (POS) tagging, has received much attention, eliciting a diverse array of techniques (Smith and Eisner, 2005; Seginer, 2007; Cohen et al., 2008). Klein and Manning’s (2004) Dependency Model with Valence (DMV) was the first to beat a simple parsing heuristic — the right-branching baseline. Today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009) are still rooted in the DMV. Despite recent advances, unsupervised parsers lag far behind their supervised counterparts. Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al., 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: Headden et al.’s (2009) Extended Valence Grammar (EVG) combats data sparsity with smoothing alone, training on the same small subset of the tree-bank as the classic implementation of the DMV; Cohen and Smith (2009) use more complicated algorithms"
N10-1116,A94-1009,0,0.145373,"Missing"
N10-1116,N09-1012,0,0.659349,"tion. A restricted version that targets dependencies and ∗ Partially funded by NSF award IIS-0811974; first author supported by the Fannie & John Hertz Foundation Fellowship. assumes partial annotation, e.g., sentence boundaries, tokenization and typically even part-of-speech (POS) tagging, has received much attention, eliciting a diverse array of techniques (Smith and Eisner, 2005; Seginer, 2007; Cohen et al., 2008). Klein and Manning’s (2004) Dependency Model with Valence (DMV) was the first to beat a simple parsing heuristic — the right-branching baseline. Today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009) are still rooted in the DMV. Despite recent advances, unsupervised parsers lag far behind their supervised counterparts. Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al., 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: Headden et al.’s (2009) Extended Valence Grammar (EVG) combats data sparsity with smoothing alone, training on the same small subset of the tree-bank as the classic implementation of the DMV; Cohen and Smith (2009) use more"
N10-1116,P04-1061,0,0.733391,"rd training. Structural annealing works well, but requires a handtuned annealing schedule and direct manipulation of the objective function; Baby Steps works “out of the box,” its locality biases a natural consequence of a complexity/data-guided tour of optimization problems. Skewed DA incorporates a good initializer by interpolating between two probability distributions, whereas our hybrid, Leapfrog, admits multiple initializers by mixing structures instead. “Less is More” is novel and confirms the tacit consensus implicit in training on small data sets (e.g., WSJ10). 4 Data Sets and Metrics Klein and Manning (2004) both trained and tested the DMV on the same customized subset (WSJ10) of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Its 49,208 annotated parse trees were pruned 5 down to 7,422 sentences of at most 10 terminals, spanning 35 unique POS tags. Following standard practice, automatic “headpercolation” rules (Collins, 1999) were used to convert the remaining trees into dependencies. Forced to produce a single “best” parse, their algorithm was judged on accuracy: its directed score was the fraction of correct dependencies; a more flattering 6 undirected score was also"
N10-1116,P08-1100,0,0.0292811,"Missing"
N10-1116,J93-2004,0,0.0440533,"works “out of the box,” its locality biases a natural consequence of a complexity/data-guided tour of optimization problems. Skewed DA incorporates a good initializer by interpolating between two probability distributions, whereas our hybrid, Leapfrog, admits multiple initializers by mixing structures instead. “Less is More” is novel and confirms the tacit consensus implicit in training on small data sets (e.g., WSJ10). 4 Data Sets and Metrics Klein and Manning (2004) both trained and tested the DMV on the same customized subset (WSJ10) of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Its 49,208 annotated parse trees were pruned 5 down to 7,422 sentences of at most 10 terminals, spanning 35 unique POS tags. Following standard practice, automatic “headpercolation” rules (Collins, 1999) were used to convert the remaining trees into dependencies. Forced to produce a single “best” parse, their algorithm was judged on accuracy: its directed score was the fraction of correct dependencies; a more flattering 6 undirected score was also used. We employ the same metrics, emphasizing directed scores, and generalize WSJk to be the subset of pre-processed sentences with at most k term"
N10-1116,N06-1020,0,0.0131797,"Missing"
N10-1116,H05-1066,0,0.0229124,"t decisions: (i) initial direction dir ∈ {L, R} in which to attach children, via probability PORDER (ch ); (ii) whether to seal dir, stopping with probability PSTOP (ch , dir, adj), conditioned on adj ∈ {T, F} (true iff considering dir’s first, i.e., adjacent, child); and (iii) attachments (of class ca ), according to PATTACH (ch , dir, ca ). This produces only projective trees.7 A root token ♦ generates the head of a sentence as its left (and only) child. Figure 2 displays an example that ignores (sums out) PORDER . The DMV lends itself to unsupervised learn7 Unlike spanning tree algorithms (McDonald et al., 2005), DMV’s chart-based method disallows crossing dependencies. 754 ing via inside-outside re-estimation (Baker, 1979). Klein and Manning did not use smoothing and started with an “ad-hoc harmonic” completion: aiming for balanced trees, non-root heads attached dependents in inverse proportion to (a constant plus) their distance; ♦ generated heads uniformly at random. This non-distributional heuristic created favorable initial conditions that nudged EM towards typical linguistic dependency structures. 5.1 Algorithm #0: Ad-Hoc∗ — A Variation on Original Ad-Hoc Initialization Since some of the import"
N10-1116,J94-2001,0,0.0947758,"Missing"
N10-1116,D08-1012,0,0.00545508,"ive training invoking the optimizer multiple times, creating extra opportunities to converge, Rohde and Plaut (1999) suspected that Elman’s (1993) simulations simply did not allow networks exposed exclusively to complex inputs sufficient training time. Our extremely generous, low termination threshold for EM (see §5.1) addresses this concern. However, given the DMV’s purely syntactic POS tag-based approach (see §5), it would be prudent to re-test Baby Steps with a lexicalized model. 3 approaches to parsing, translation and speech recognition (Charniak and Johnson, 2005; Charniak et al., 2006; Petrov et al., 2008; Petrov, 2009), and recently unsupervised POS tagging (Ravi and Knight, 2009). Initial models tend to be particularly simple,4 and each refinement towards a full model introduces only limited complexity, supporting incrementality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “"
N10-1116,P09-1057,0,0.0097246,"ities to converge, Rohde and Plaut (1999) suspected that Elman’s (1993) simulations simply did not allow networks exposed exclusively to complex inputs sufficient training time. Our extremely generous, low termination threshold for EM (see §5.1) addresses this concern. However, given the DMV’s purely syntactic POS tag-based approach (see §5), it would be prudent to re-test Baby Steps with a lexicalized model. 3 approaches to parsing, translation and speech recognition (Charniak and Johnson, 2005; Charniak et al., 2006; Petrov et al., 2008; Petrov, 2009), and recently unsupervised POS tagging (Ravi and Knight, 2009). Initial models tend to be particularly simple,4 and each refinement towards a full model introduces only limited complexity, supporting incrementality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “method of successive approximations.” Ideas that gradually make a task more di"
N10-1116,P07-1049,0,0.25807,"archical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version that targets dependencies and ∗ Partially funded by NSF award IIS-0811974; first author supported by the Fannie & John Hertz Foundation Fellowship. assumes partial annotation, e.g., sentence boundaries, tokenization and typically even part-of-speech (POS) tagging, has received much attention, eliciting a diverse array of techniques (Smith and Eisner, 2005; Seginer, 2007; Cohen et al., 2008). Klein and Manning’s (2004) Dependency Model with Valence (DMV) was the first to beat a simple parsing heuristic — the right-branching baseline. Today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009) are still rooted in the DMV. Despite recent advances, unsupervised parsers lag far behind their supervised counterparts. Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al., 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: He"
N10-1116,P04-1062,0,0.112291,"t sets of weights on the examples — could act as a continuation method (Allgower and Georg, 1990), helping find better local optima for nonconvex objectives. Elman’s learners constrained the peaky solution space by focusing on just the right data (simple sentences that introduced basic representational categories) at just the right time (early on, when their plasticity was greatest). Self-shaping, they simplified tasks through deliberate omission (or misunderstanding). Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004). Its curriculum of binary weights initially discards complex examples responsible for “high-frequency noise,” with earlier, “smoothed” objectives revealing more of the global picture. There are important differences between our results and prior work. In contrast to Elman, we use a 4 Brown et al.’s (1993) Model 1 (and, similarly, the first baby step) has a global optimum that can be computed exactly, so that no initial or subsequent parameters depend on initialization. 753 large data set (WSJ) of real English. Unlike Bengio et al. and Krueger and Dayan, we shape a parser, not a language model"
N10-1116,P06-1072,0,0.228623,"that a well-chosen sequence of training criteria — different sets of weights on the examples — could act as a continuation method (Allgower and Georg, 1990), helping find better local optima for nonconvex objectives. Elman’s learners constrained the peaky solution space by focusing on just the right data (simple sentences that introduced basic representational categories) at just the right time (early on, when their plasticity was greatest). Self-shaping, they simplified tasks through deliberate omission (or misunderstanding). Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004). Its curriculum of binary weights initially discards complex examples responsible for “high-frequency noise,” with earlier, “smoothed” objectives revealing more of the global picture. There are important differences between our results and prior work. In contrast to Elman, we use a 4 Brown et al.’s (1993) Model 1 (and, similarly, the first baby step) has a global optimum that can be computed exactly, so that no initial or subsequent parameters depend on initialization. 753 large data set (WSJ) of real English. Unlike Bengio et al. and"
N10-1116,D09-1058,0,0.0104437,"lly even part-of-speech (POS) tagging, has received much attention, eliciting a diverse array of techniques (Smith and Eisner, 2005; Seginer, 2007; Cohen et al., 2008). Klein and Manning’s (2004) Dependency Model with Valence (DMV) was the first to beat a simple parsing heuristic — the right-branching baseline. Today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009) are still rooted in the DMV. Despite recent advances, unsupervised parsers lag far behind their supervised counterparts. Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al., 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: Headden et al.’s (2009) Extended Valence Grammar (EVG) combats data sparsity with smoothing alone, training on the same small subset of the tree-bank as the classic implementation of the DMV; Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data). We explore what can be achieved through judicious use of data and simple, scalable techniqu"
N10-1116,J03-4003,0,\N,Missing
N10-2003,W09-2307,1,0.558961,"ensions allow us to triple the distortion limit and provide a statistically significant improvement over the baseline (Green et al., 2010). Discriminative Reordering with Chinese Grammatical Relations During translation, a source sentence can be more accurately reordered if the system knows something about the syntactic relationship between the words in the phrases being reordered. The discriminative reordering with Chinese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Ra"
N10-2003,D08-1089,1,0.894263,"construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/sof"
N10-2003,P09-1087,1,0.142109,"rget language do a poor job of modeling long distance syntactic relationships. For example, if there are a number of intervening words between a verb and its subject, n-gram language models will often not be of much help in selecting the verb form that agrees with the subject. The target side dependency language model feature captures these long distance relationships by providing a dependency score for the target translations produced by the decoder. This is done using an efficient quadratic time algorithm that operates within the main decoding loop rather than in a separate reranking stage (Galley and Manning, 2009). Discriminative Distortion The standard distortion cost model used in phrase-based MT systems such as Moses has two problems. First, it does not estimate the future cost of known required moves, thus increasing search errors. Second, the model penalizes distortion linearly, even when appropriate reorderings are performed. To address these problems, we used the Phrasal feature API to design a new discriminative distortion model that predicts word movement during translation and that estimates future cost. These extensions allow us to triple the distortion limit and provide a statistically sign"
N10-2003,N10-1129,1,0.124195,"d distortion cost model used in phrase-based MT systems such as Moses has two problems. First, it does not estimate the future cost of known required moves, thus increasing search errors. Second, the model penalizes distortion linearly, even when appropriate reorderings are performed. To address these problems, we used the Phrasal feature API to design a new discriminative distortion model that predicts word movement during translation and that estimates future cost. These extensions allow us to triple the distortion limit and provide a statistically significant improvement over the baseline (Green et al., 2010). Discriminative Reordering with Chinese Grammatical Relations During translation, a source sentence can be more accurately reordered if the system knows something about the syntactic relationship between the words in the phrases being reordered. The discriminative reordering with Chinese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models"
N10-2003,N03-1017,0,0.00907943,"ases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license"
N10-2003,P07-2045,0,0.0488706,"Missing"
N10-2003,koen-2004-pharaoh,0,0.121277,"f) (model_name) Running this command will first create word level alignments for the sentences in source.txt and target.txt using the Berkeley cross-EM aligner 1 http://www.itl.nist.gov/iad/mig/tests /mt/2009/ResultsRelease/currentArabic.html 9 Proceedings of the NAACL HLT 2010: Demonstration Session, pages 9–12, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 3 Decoder Decoding Engines The package includes two decoding engines, one that implements the left-toright beam search algorithm that was first introduced with the Pharaoh machine translation system (Koehn, 2004), and another that provides a recently developed decoding algorithm for translating with discontinuous phrases (Galley and Manning, 2010). Both engines use features written to a common but extensible feature API, which allows features to be written once and then loaded into either engine. Discontinuous phrases provide a mechanism for systematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading"
N10-2003,N06-1014,0,0.141897,"ematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by"
N10-2003,niessen-etal-2000-evaluation,0,0.0200338,"ature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Rather than first building a massive phrase table from a parallel corpus and then filtering it down to just what is needed for a specific data set, our toolkit supports the extraction of just those phrases that might be used on a given evaluation set. In doing so, it dramatically reduces the time required to build the phrase table and related data structures such as reordering models. Feature Extraction API In order to assist in the development of new"
N10-2003,J03-1002,0,0.00159764,"e and then loaded into either engine. Discontinuous phrases provide a mechanism for systematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to m"
N10-2003,P03-1021,0,0.0139183,"and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/software/phrasal. tranlations per minute Figure 1: Chinese-to-English translation using discontinuous phrases. 1 2 3 4 5 6 7 8 Cores Figure 2: Multicore translations per minute on a system with two Intel Xeon L5530 processors running at 2.40GHz. threads past four results in only marginal additional gains as the cost of managing the resources shared betwe"
N10-2003,P02-1040,0,0.102389,"ord-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/software/phrasal. tranlations per minute Figure 1: Chinese-to-English translation using discontinuous phrases. 1 2 3 4 5 6 7 8 Cores Figure 2: Multicore translations per minute on a system with two Intel Xeon L5530 processors running at 2.40GHz. threads past four results in only marginal additional gains as the cost of managing the resources shared between the threads is starting to overwhelm the value provided b"
N10-2003,W09-0441,0,0.0154617,"ese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Rather than first building a massive phrase table from a parallel corpus and then filtering it down to just what is needed for a specific data set, our toolkit supports the extraction of just those phrases that might be used on a given evaluation set. In doing so, it dramatically reduces the time required to build the phrase table and related data structures such as reordering models. Feature Extraction API In order to assi"
N12-1049,C04-1180,0,0.0112467,"then applying before to week and last week. We construct a paradigm for parsing temporal phrases consisting of a standard PCFG over temporal types with each parse rule defining a function to apply to the child nodes, or the word being generated. At the root of the tree, we recursively apply the functions in the parse tree to obtain a final temporal value. One can view this formalism as a rule449 to-rule translation (Bach, 1976; Allen, 1995, p. 263), or a constrained Synchronous PCFG (Yamada and Knight, 2001). Our approach contrasts with common approaches, such as CCG grammars (Steedman, 2000; Bos et al., 2004; Kwiatkowski et al., 2011), giving us more flexibility in the composition rules. Figure 2 shows an example of the grammar. Formally, we define our temporal grammar G = (Σ, S, V, W, R, θ). The alphabet Σ and start symbol S retain their usual interpretations. We define a set V to be the set of types, as described in Section 3.1 – these act as our nonterminals. For each v ∈ V we define an (infinite) set Wv corresponding to the possible instances of type v. Concretely, if v = Sequence, our set Wv ∈ W could contain elements corresponding to Friday, last Friday, Nov. 27th , etc. Each node in the tr"
N12-1049,W10-2903,0,0.0247231,"rm. This logical form importantly contains all the predicates and entities used in their parse. We loosen the supervision required in these systems by allowing the parse to be entirely latent; the annotation of the grounded time neither defines, nor gives any direct cues about the elements of the parse, since many parses evaluate to the same grounding. To demonstrate, the grounding for a week ago could be described by specifying a month and day, or as a week ago, or as last x – substituting today’s day of the week for x. Each of these correspond to a completely different parse. Recent work by Clarke et al. (2010) and Liang et al. (2011) similarly relax supervision to require only annotated answers rather than full logical forms. For example, Liang et al. (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our syst"
N12-1049,S10-1074,0,0.0411276,"logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide par447 ticularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of time; a type system is described in"
N12-1049,W05-1506,0,0.0117311,"on, e.g., 5 weeks. • Combining a non-Nil and Nil element with no change to the temporal expression, e.g., a week. The lexicalization of the Nil type allows the algorithm to take hints from these supporting words. We proceed to describe learning the parameters of this grammar. 4 Learning We present a system architecture, described in Figure 3. We detail the inference procedure in Section 4.1 and training in Section 4.2. 4.1 Inference To provide a list of candidate expressions with their associated probabilities, we employ a k-best CKY parser. Specifically, we implement Algorithm 3 described in Huang and Chiang (2005), providing an O(Gn3 k log k) algorithm with respect to the grammar size G, phrase length n, and beam size k. We set the beam size to 2000. 1 In the case of complex sequences (e.g., Friday the 13th ) an A∗ search is performed to find overlapping ranges in the two sequences; the origin rs (0) is updated to refer to the closest such match to the reference time. 451 Revisiting the notion of pragmatic ambiguity, in a sense the most semantically complete output of the system would be a distribution – an utterance of Friday would give a distribution over Fridays rather than a best guess of its groun"
N12-1049,P04-1061,1,0.622234,"hood update; while the update for µ incorporates a Bayesian prior N (µ0 , σ0 ): ¯ θ ,M ¯ µ,σ ) begin M-Step(M 0 ¯ θ , α) θ := bayesianPosterior(M 0 ¯ σ := mlePosterior(Mµ,σ ) ¯ µ,σ , σ 0 , N ) µ0 := bayesianPosterior(M return (θ0 , µ0 , σ 0 ) end v u σ =u t 1 P 0 X p µ = split on the characters ‘-’ and ‘/,’ which often delimit a boundary between temporal entities. Beyond this preprocessing, no language-specific information about the meanings of the words are introduced, including syntactic parses, POS tags, etc. The algorithm operates similarly to the EM algorithms used for grammar induction (Klein and Manning, 2004; Carroll and Charniak, 1992). However, unlike grammar induction, we are allowed a certain amount of supervision by requiring that the predicted temporal expression match the annotation. Our expected statistics are therefore more accurately our normalized expected counts of valid parses. 452 σ 02 µ0 + σ02 P σ02 P σ 02 + (4) ¯ µ,σ (i,p)∈M ¯ µ,σ (i,p)∈M 0 (i − µ0 )2 · p ¯ µ,σ (i,p)∈M ¯ µ,σ (i,p)∈M i·p p (5) As the parameters improve, the parser more efficiently prunes incorrect parses and the beam incorporates valid parses for longer and longer phrases. For instance, in the first iteration the m"
N12-1049,S10-1072,0,0.0770558,"our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide par447 ticularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of time; a type system is described in Section 3.1 while the grammar is outlined in Section 3.2 and described in detail in Sections 3.3 and 3.4. 3.1 Temporal Expression Types We represent temporal expressions as either a Range, Sequence, or Duration. We describe these, the Function type, and the miscellaneous Number and Nil types below: Range [and Instant] A period between two dates (or times)"
N12-1049,D11-1140,0,0.0264037,"re to week and last week. We construct a paradigm for parsing temporal phrases consisting of a standard PCFG over temporal types with each parse rule defining a function to apply to the child nodes, or the word being generated. At the root of the tree, we recursively apply the functions in the parse tree to obtain a final temporal value. One can view this formalism as a rule449 to-rule translation (Bach, 1976; Allen, 1995, p. 263), or a constrained Synchronous PCFG (Yamada and Knight, 2001). Our approach contrasts with common approaches, such as CCG grammars (Steedman, 2000; Bos et al., 2004; Kwiatkowski et al., 2011), giving us more flexibility in the composition rules. Figure 2 shows an example of the grammar. Formally, we define our temporal grammar G = (Σ, S, V, W, R, θ). The alphabet Σ and start symbol S retain their usual interpretations. We define a set V to be the set of types, as described in Section 3.1 – these act as our nonterminals. For each v ∈ V we define an (infinite) set Wv corresponding to the possible instances of type v. Concretely, if v = Sequence, our set Wv ∈ W could contain elements corresponding to Friday, last Friday, Nov. 27th , etc. Each node in the tree defines a pair (v, w) su"
N12-1049,P11-1060,0,0.0599823,"ortantly contains all the predicates and entities used in their parse. We loosen the supervision required in these systems by allowing the parse to be entirely latent; the annotation of the grounded time neither defines, nor gives any direct cues about the elements of the parse, since many parses evaluate to the same grounding. To demonstrate, the grounding for a week ago could be described by specifying a month and day, or as a week ago, or as last x – substituting today’s day of the week for x. Each of these correspond to a completely different parse. Recent work by Clarke et al. (2010) and Liang et al. (2011) similarly relax supervision to require only annotated answers rather than full logical forms. For example, Liang et al. (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither"
N12-1049,P00-1010,0,0.225962,"lar in structure to a dependency grammar, but representing a logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide par447 ticularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositi"
N12-1049,J88-2003,0,0.129585,"number without any temporal meaning attached. This comes into play representing expressions such as 2 weeks. The other is the Nil type – denoting terms which are not directly contributing to the semantic meaning of the expression. This is intended for words such as a or the, which serve as cues without bearing temporal content themselves. The Nil type is lexicalized with the word it generates. Omitted Phenomena The representation described is a simplification of the complexities of time. Notably, a body of work has focused on reasoning about events or states relative to temporal expressions. Moens and Steedman (1988) describes temporal expressions relating to changes of state; Condoravdi (2010) explores NPI licensing in temporal expressions. Broader context is also not Range f (Duration) : Range Duration catRight Number Duration next Numn∗100 Day 2 days (a) catRight(t, 2D ) catRight(t, −) next 2D Num(2) 1D 2 days (b) Figure 2: The grammar – (a) describes the CFG parse of the temporal types. Words are tagged with their nonterminal entry, above which only the types of the expressions are maintained; (b) describes the corresponding combination of the temporal instances. The parse in (b) is deterministic give"
N12-1049,puscasu-2004-framework,0,0.15957,"representing a logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide par447 ticularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of time; a type s"
N12-1049,S10-1071,0,0.121794,"Missing"
N12-1049,S10-1062,0,0.0889829,"than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide par447 ticularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of time; a type system is described in Section 3.1 while the grammar is outlined in Section 3.2 and described in detail in Sections 3.3 and 3.4. 3.1 Temporal Expression Types We represent temporal expressions as either a Range, Sequence, or Duration. We descr"
N12-1049,P01-1067,0,0.0952896,"last week. We can construct a meaning by applying the modifier last to week – creating the previous week; and then applying before to week and last week. We construct a paradigm for parsing temporal phrases consisting of a standard PCFG over temporal types with each parse rule defining a function to apply to the child nodes, or the word being generated. At the root of the tree, we recursively apply the functions in the parse tree to obtain a final temporal value. One can view this formalism as a rule449 to-rule translation (Bach, 1976; Allen, 1995, p. 263), or a constrained Synchronous PCFG (Yamada and Knight, 2001). Our approach contrasts with common approaches, such as CCG grammars (Steedman, 2000; Bos et al., 2004; Kwiatkowski et al., 2011), giving us more flexibility in the composition rules. Figure 2 shows an example of the grammar. Formally, we define our temporal grammar G = (Σ, S, V, W, R, θ). The alphabet Σ and start symbol S retain their usual interpretations. We define a set V to be the set of types, as described in Section 3.1 – these act as our nonterminals. For each v ∈ V we define an (infinite) set Wv corresponding to the possible instances of type v. Concretely, if v = Sequence, our set W"
N12-1049,D07-1071,0,0.0752271,"rk Our approach draws inspiration from a large body of work on parsing expressions into a logical form. The latent parse parallels the formal semantics in previous work, e.g., Montague semantics. Like these representations, a parse – in conjunction with the reference time – defines a set of matching entities, in this case the grounded time. The matching times can be thought of as analogous to the entities in a logical model which satisfy a given expression. Supervised approaches to logical parsing prominently include Zelle and Mooney (1996), Zettlemoyer and Collins (2005), Kate et al. (2005), Zettlemoyer and Collins (2007), inter alia. For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. This logical form importantly contains all the predicates and entities used in their parse. We loosen the supervision required in these systems by allowing the parse to be entirely latent; the annotation of the grounded time neither defines, nor gives any direct cues about the elements of the parse, since many parses evaluate to the same grounding. To demonstrate, the grounding for a week ago could be described by specifying a month and day, or as a week ago, or as last x – substit"
N12-1049,S10-1010,0,\N,Missing
N12-1049,chang-manning-2012-sutime,1,\N,Missing
N13-1110,P12-1041,0,0.289243,"ce resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary. The idea of creating a repository of extracted concept-instance relations appears in Fleischman et al. (2003), but restricted to person-role pairs, e.g. Yasser Arafat and leader. Although it was originally designed for answering who-is questions, Daum´e III and Marcu (2005) successfully used it for coreference resolution. The coreference relations that we extract might overlap but go beyond those detected by Bansal and Klein (2012)’s Web-based features. First, they focus on NP headwords, while we extract full NPs, including multi-word mentions. Second, the fact that they use the Google n-gram corpus means that the two headwords must appear at most four words apart, thus ruling out coreferent mentions that can only appear far from each other. Finally, while their extraction patterns focus on synonymy and hypernymy relations, we discover other types of semantic relations that are relevant for coreference (Section 2.5). coreference. We therefore need semantic resources specifically targeted at coreference. We proposed a ne"
N13-1110,P01-1008,0,0.137885,"ause of both a precision and a recall problem (Lee et al., 2011; Uryupina et al., 2011). On the other hand, using data-driven methods such as distributional semantics for coreference resolution suffers especially from a precision problem (Ng, 2007). Our work combines ideas from distributional semantics and paraphrase acquisition methods in order to efficiently use contextual information to extract coreference relations. The main idea that we borrow from paraphrase acquisition is the use of monolingual (non-parallel) comparable corpora, which have been exploited to extract both sentence-level (Barzilay and McKeown, 2001) and sub-sentential-level paraphrases (Shinyama and Sekine, 2003; Wang and CallisonBurch, 2011). To ensure that the NPs are coreferent, we limit the meaning of comparable corpora to collections of documents that report on the very same story, as opposed to collections of documents that are about the same (general) topic. However, the distinguishing factor is that while most paraphrasing studies, including Lin and Pantel (2001), use NEs— or nouns in general—as pivots to learn paraphrases of their surrounding context, we use verbs as pivots to learn coreference relations at the NP level. There a"
N13-1110,N04-1038,0,0.0573572,"ing context, we use verbs as pivots to learn coreference relations at the NP level. There are many similarities between paraphrase and coreference, and our work is most similar to that by Wang and Callison-Burch (2011). However, some paraphrases that might not be considered to be valid (e.g., under $200 and around $200) can be acceptable coreference relations. Unlike Wang and Callison-Burch (2011), we do not work on document pairs but on sets of at least five (comparable) documents, and we do not require sentence alignment, but just verb alignment. Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary. The idea of creating a repository of extracted concept-instance relations appears in Fleischman et al. (2003), but restricted to person-role pairs, e.g. Yasser Arafat and leader. Although it was originally designed for answering who-is questions, Daum´e III and Marcu (2005) successful"
N13-1110,H05-1013,0,0.149376,"Missing"
N13-1110,W12-4502,0,0.0775214,"Missing"
N13-1110,P03-1001,0,0.0103164,"cument pairs but on sets of at least five (comparable) documents, and we do not require sentence alignment, but just verb alignment. Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary. The idea of creating a repository of extracted concept-instance relations appears in Fleischman et al. (2003), but restricted to person-role pairs, e.g. Yasser Arafat and leader. Although it was originally designed for answering who-is questions, Daum´e III and Marcu (2005) successfully used it for coreference resolution. The coreference relations that we extract might overlap but go beyond those detected by Bansal and Klein (2012)’s Web-based features. First, they focus on NP headwords, while we extract full NPs, including multi-word mentions. Second, the fact that they use the Google n-gram corpus means that the two headwords must appear at most four words apart, thus ruling out coreferent mentions"
N13-1110,N10-1061,0,0.120865,"Missing"
N13-1110,P11-1079,0,0.253175,"Missing"
N13-1110,W11-1902,0,0.186483,"Missing"
N13-1110,C00-1072,0,0.0506448,"luded in the coreference dictionary, but we need a method for taking the surrounding context into account. In the next section we present our preliminary work in this direction. 5.2 Context fit To help the coreference system choose the right antecedent in examples like (6), we exploit the fact that the company is closely followed by Windows 8, which is a clue for selecting Microsoft instead of Apple as the antecedent. We devise a contextual constraint that rules out a mention pair if the contexts are incompatible. To check for context compatibility, we borrow the idea of topic signatures from Lin and Hovy (2000) and that Agirre et al. (2001) used for Word Sense Disambiguation. Instead of identifying the keywords of a topic, we find the NEs that tend to co-occur with another NE. For example, the signature for Apple should include terms like iPhone, MacBook, iOS, Steve Jobs, etc. This is what we call the NE signature for Apple. To construct NE signatures, we first compute the log-likelihood ratio (LLR) statistic between NEs in our corpus (the same one used to build the dictionary). Then, the signature for a NE, w, is the list of k other NEs that have the highest LLR with w. The 1) LLR between two NEs,"
N13-1110,H05-1004,0,0.272162,"27 7837 1360 2279 Test 24 24 8547 1341 2452 Table 4: Dataset statistics: development (dev) and test. 4.2 Evaluation measures We evaluated using six coreference measures, as they sometimes provide different results and there is no agreement on a standard. We used the scorer of the CoNLL-2011 Shared Task (Pradhan et al., 2011). • MUC (Vilain et al., 1995). Link-based metric that measures how many links the true and system partitions have in common. • B3 (Bagga and Baldwin, 1998). Mention-based metric that measures the proportion of mention overlap between gold and predicted entities. • CEAF-φ3 (Luo, 2005). Mention-based metric that, unlike B3 , enforces a one-to-one alignment between gold and predicted entities. • CEAF-φ4 (Luo, 2005). The entity-based version of the above metric. • BLANC (Recasens and Hovy, 2011). Linkbased metric that considers both coreference and non-coreference links. • CoNLL (Denis and Baldridge, 2009). Average of MUC, B3 and CEAF-φ4 . It was the official metric of the CoNLL-2011 Shared Task. 4.3 Results We always start from the baseline, which corresponds to the Stanford system with the sieves listed in Table 3. This is the set of sieves that won the CoNLL-2011 Shared Ta"
N13-1110,W04-2407,0,0.00926953,"ons and events of the story (e.g., block out). Since these verbs are representative of the story, different instances across documents in the cluster are likely to refer to the same events (Sprint blocks out. . . and the carrier blocks out. . . ). By the same logic, the subjects and objects of the verbs are also likely to be coreferent (Sprint and the carrier). 2.1 Comparable corpus After building our corpus, we used Stanford’s CoreNLP tools4 to tokenize the text and annotate it with POS tags and named entity types. We parsed the text using the MaltParser 1.7, a linear time dependency parser (Nivre et al., 2004).5 We then extracted the representative verbs of each story by ranking the verbs in each story according to their tf-idf scores. We took the top ten to be the representative set. For each of these verbs, we clustered together its subjects and objects (separately) across instances of the verb in the document cluster, excluding pronouns and NPs headed by the same noun. For example, suppose that crawl is a representative verb and that in one document we have Google crawls web pages and The search giant crawls sites in another document. We will create the clusters {Google, the search giant} and {w"
N13-1110,N06-1025,0,0.329338,"Missing"
N13-1110,W11-1901,0,0.0491044,"ifiers), excluding appositive phrases and predicate nominals. Only premodifiers that were proper nouns or possessive phrases were annotated. We extended the OntoNotes guidelines by also annotating singletons. Table 4 shows the dataset statistics. 901 Stories Docs Tokens Entities Mentions Dev 4 27 7837 1360 2279 Test 24 24 8547 1341 2452 Table 4: Dataset statistics: development (dev) and test. 4.2 Evaluation measures We evaluated using six coreference measures, as they sometimes provide different results and there is no agreement on a standard. We used the scorer of the CoNLL-2011 Shared Task (Pradhan et al., 2011). • MUC (Vilain et al., 1995). Link-based metric that measures how many links the true and system partitions have in common. • B3 (Bagga and Baldwin, 1998). Mention-based metric that measures the proportion of mention overlap between gold and predicted entities. • CEAF-φ3 (Luo, 2005). Mention-based metric that, unlike B3 , enforces a one-to-one alignment between gold and predicted entities. • CEAF-φ4 (Luo, 2005). The entity-based version of the above metric. • BLANC (Recasens and Hovy, 2011). Linkbased metric that considers both coreference and non-coreference links. • CoNLL (Denis and Baldrid"
N13-1110,P11-1082,0,0.111462,"Missing"
N13-1110,P10-1144,1,0.854415,"Missing"
N13-1110,W03-1609,0,0.119625,"ryupina et al., 2011). On the other hand, using data-driven methods such as distributional semantics for coreference resolution suffers especially from a precision problem (Ng, 2007). Our work combines ideas from distributional semantics and paraphrase acquisition methods in order to efficiently use contextual information to extract coreference relations. The main idea that we borrow from paraphrase acquisition is the use of monolingual (non-parallel) comparable corpora, which have been exploited to extract both sentence-level (Barzilay and McKeown, 2001) and sub-sentential-level paraphrases (Shinyama and Sekine, 2003; Wang and CallisonBurch, 2011). To ensure that the NPs are coreferent, we limit the meaning of comparable corpora to collections of documents that report on the very same story, as opposed to collections of documents that are about the same (general) topic. However, the distinguishing factor is that while most paraphrasing studies, including Lin and Pantel (2001), use NEs— or nouns in general—as pivots to learn paraphrases of their surrounding context, we use verbs as pivots to learn coreference relations at the NP level. There are many similarities between paraphrase and coreference, and our"
N13-1110,P09-1074,0,0.153556,"Missing"
N13-1110,M95-1005,0,0.763133,"phrases and predicate nominals. Only premodifiers that were proper nouns or possessive phrases were annotated. We extended the OntoNotes guidelines by also annotating singletons. Table 4 shows the dataset statistics. 901 Stories Docs Tokens Entities Mentions Dev 4 27 7837 1360 2279 Test 24 24 8547 1341 2452 Table 4: Dataset statistics: development (dev) and test. 4.2 Evaluation measures We evaluated using six coreference measures, as they sometimes provide different results and there is no agreement on a standard. We used the scorer of the CoNLL-2011 Shared Task (Pradhan et al., 2011). • MUC (Vilain et al., 1995). Link-based metric that measures how many links the true and system partitions have in common. • B3 (Bagga and Baldwin, 1998). Mention-based metric that measures the proportion of mention overlap between gold and predicted entities. • CEAF-φ3 (Luo, 2005). Mention-based metric that, unlike B3 , enforces a one-to-one alignment between gold and predicted entities. • CEAF-φ4 (Luo, 2005). The entity-based version of the above metric. • BLANC (Recasens and Hovy, 2011). Linkbased metric that considers both coreference and non-coreference links. • CoNLL (Denis and Baldridge, 2009). Average of MUC, B3"
N13-1110,W11-1208,0,0.0152521,"To ensure that the NPs are coreferent, we limit the meaning of comparable corpora to collections of documents that report on the very same story, as opposed to collections of documents that are about the same (general) topic. However, the distinguishing factor is that while most paraphrasing studies, including Lin and Pantel (2001), use NEs— or nouns in general—as pivots to learn paraphrases of their surrounding context, we use verbs as pivots to learn coreference relations at the NP level. There are many similarities between paraphrase and coreference, and our work is most similar to that by Wang and Callison-Burch (2011). However, some paraphrases that might not be considered to be valid (e.g., under $200 and around $200) can be acceptable coreference relations. Unlike Wang and Callison-Burch (2011), we do not work on document pairs but on sets of at least five (comparable) documents, and we do not require sentence alignment, but just verb alignment. Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on corefere"
N13-1110,P07-1067,0,0.12158,"Missing"
N13-1127,P05-3001,0,0.0338163,"rive the maxims (or perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in multi-agent decision-theoretic models and their efficient implementation. With the current paper, we seek to show"
N13-1127,D10-1040,0,0.268756,"l, Max Bodoia, Christopher Potts, and Dan Jurafsky Stanford University Stanford, CA, USA {acvogel,mbodoia,cgpotts,jurafsky}@stanford.edu Abstract traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). Grice characterized communication in terms of the cooperative principle, which enjoins speakers to make only contributions that serve the evolving conversational goals. We show that the cooperative principle and the associated maxims of relevance, quality, and quantity emerge from multi-agent decision theory. We utilize the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) model of multi-agent decision making which relies only on basic definitions of"
N13-1127,J86-3001,0,0.426842,", relevant, clear, and concise. Since then, there have been many attempts to derive the maxims (or perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in multi-agent decision-theoretic models"
N13-1127,J12-1006,0,0.0492721,"Missing"
N13-1127,J80-3003,0,\N,Missing
P00-1065,P98-1013,0,0.0933979,"more accurate parsers and better language models for speech recognition. This paper proposes an algorithm for automatic semantic analysis, assigning a semantic role to constituents in a sentence. Our approach to semantic analysis is to treat the problem of semantic role labeling like the similar problems of parsing, part of speech tagging, and word sense disambiguation. We apply statistical techniques that have been successful for these tasks, including probabilistic parsing and statistical classi cation. Our statistical algorithms are trained on a hand-labeled dataset: the FrameNet database (Baker et al., 1998). The FrameNet database de nes a tagset of semantic roles called frame elements, and includes roughly 50,000 sentences from the British National Corpus which have been hand-labeled with these frame elements. The next section describes the set of frame elements/semantic roles used by our system. In the rest of this paper we report on our current system, as well as a number of preliminary experiments on extensions to the system. 2 Semantic Roles Historically, two types of semantic roles have been studied: abstract roles such as Agent and Patient, and roles speci c to individual verbs such as Eat"
P00-1065,A00-2031,0,0.0145381,"ilo (1993) builds a dictionary of patterns for lling slots in a speci c domain such as terrorist attacks, and Rilo and Schmelzenbach (1998) extend this technique to automatically derive entire case frames for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approach to generalize beyond the relatively small number of frames considered in the tasks. More recently, a domain independent system has been trained on general function tags such as Manner and Temporal by Blaheta and Charniak (2000). 4 Methodology We divide the task of labeling frame elements into two subtasks: that of identifying the boundaries of the frame elements in the sentences, and that of labeling each frame element, given its boundaries, with the correct role. We rst give results for a system which Domain: Communication Frame: Frame: Conversation Frame Elements: Protagonist−1 Protagonist−2 Protagonists Topic Medium debate−v tiff−n dispute−n converse−v gossip−v discussion−n Speaker Addressee Message Topic Medium Statement Frame Elements: confer−v Cognition Questioning Frame Elements: Frame: talk−v Domain: Speaker"
P00-1065,W98-1505,0,0.0161728,"Missing"
P00-1065,P97-1003,0,0.0374372,"nd Pat had an argument about politics Kim and Pat argued in French Kim and pat had an argument in French Medium Table 1: Examples of semantic roles, or frame elements, for target words argue&quot; and argument&quot; from the conversation&quot; frame labels roles using human-annotated boundaries, returning to the question of automatically identifying the boundaries in Section 5.3. 4.1 Features Used in Assigning Semantic Roles The system is a statistical one, based on training a classi er on a labeled training set, and testing on an unlabeled test set. The system is trained by rst using the Collins parser (Collins, 1997) to parse the 36,995 training sentences, matching annotated frame elements to parse constituents, and extracting various features from the string of words and the parse tree. During testing, the parser is run on the test sentences and the same features extracted. Probabilities for each possible semantic role r are then computed from the features. The probability computation will be described in the next section; the features include: Phrase Type: This feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP), and claus"
P00-1065,P99-1001,0,0.0270721,"e the roles associated with a word can be cues to its sense. For example, Lapata and Brew (1999) and others have shown that the di erent syntactic subcatgorization frames of a verb like serve&quot; can be used to help disambiguate a particular instance of the word serve&quot;. Adding semantic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging eld of Text Data Mining (TDM) (Hearst, 1999). Finally, incorporating semantic roles into probabilistic models of language should yield more accurate parsers and better language models for speech recognition. This paper proposes an algorithm for automatic semantic analysis, assigning a semantic role to constituents in a sentence. Our approach to semantic analysis is to treat the problem of semantic role labeling like the similar problems of parsing, part of speech tagging, and word sense disambiguation. We apply statistical techniques that have been successful for these tasks, including probabilistic parsing and statistical classi cation"
P00-1065,W99-0632,0,0.0104052,"emplates to extract facts about, for example, nancial news or interesting political events. A shallow semantic level of representation is a more domain-independent, robust level of representation. Identifying these roles, for example, could allow a system to determine that in the sentence The rst one crashed&quot; the subject is the vehicle, but in the sentence The rst one crashed it&quot; the subject is the agent, which would help in information extraction in this domain. Another application is in wordsense disambiguation, where the roles associated with a word can be cues to its sense. For example, Lapata and Brew (1999) and others have shown that the di erent syntactic subcatgorization frames of a verb like serve&quot; can be used to help disambiguate a particular instance of the word serve&quot;. Adding semantic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging eld of Text Data Mining (TDM) (Hearst, 1999). Finally, incorporating semantic roles into probabilistic models of language shoul"
P00-1065,P98-2127,0,0.141466,"Missing"
P00-1065,P96-1008,0,0.00852743,"tations of uni cation-based grammars such as HPSG (Pollard and Sag, 1994), rely on handdeveloped grammars which must anticipate each way in which semantic roles may be realized syntactically. Writing such grammars is time-consuming, and typically such systems have limited coverage. Data-driven techniques have recently been applied to template-based semantic interpretation in limited domains by shallow&quot; systems that avoid complex feature structures, and often perform only shallow syntactic analysis. For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al. (1996) computed the probability that a constituent such as Atlanta&quot; lled a semantic slot such as Destination in a semantic frame for air travel. In a data-driven approach to information extraction, Rilo (1993) builds a dictionary of patterns for lling slots in a speci c domain such as terrorist attacks, and Rilo and Schmelzenbach (1998) extend this technique to automatically derive entire case frames for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approach to gen"
P00-1065,W98-1106,0,0.010611,"applied to template-based semantic interpretation in limited domains by shallow&quot; systems that avoid complex feature structures, and often perform only shallow syntactic analysis. For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al. (1996) computed the probability that a constituent such as Atlanta&quot; lled a semantic slot such as Destination in a semantic frame for air travel. In a data-driven approach to information extraction, Rilo (1993) builds a dictionary of patterns for lling slots in a speci c domain such as terrorist attacks, and Rilo and Schmelzenbach (1998) extend this technique to automatically derive entire case frames for words in the domain. These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses. They show promise for a more sophisticated approach to generalize beyond the relatively small number of frames considered in the tasks. More recently, a domain independent system has been trained on general function tags such as Manner and Temporal by Blaheta and Charniak (2000). 4 Methodology We divide the task of labeling frame elements into two subtasks: that of identifying the boundar"
P00-1065,A00-2030,0,\N,Missing
P00-1065,J93-2004,0,\N,Missing
P00-1065,A00-2034,0,\N,Missing
P00-1065,J03-4003,0,\N,Missing
P00-1065,H94-1020,0,\N,Missing
P00-1065,C98-1013,0,\N,Missing
P00-1065,P99-1014,0,\N,Missing
P00-1065,C98-2122,0,\N,Missing
P00-1065,W00-2021,0,\N,Missing
P00-1065,A00-1010,0,\N,Missing
P00-1065,P93-1024,0,\N,Missing
P05-1072,P00-1065,1,0.378562,"Missing"
P05-1072,J02-3001,1,0.885159,"/software/yamcha/ 582 (hhhh ( (( NP VP ``` ((((hhhh VBD `VP T he acquisition ARG1 XXX was NULL VBN PP completed predicate in September ARGM−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. P REDICATE L EMMA PATH: Path from the constituent to the predicate in the parse tree. P OSITION: Whether the constituent is before or after the predicate. VOICE P REDICATE SUB - CATEGORIZATION P REDICATE C LUSTER H EAD W ORD: Head word of the constituent. H EAD W ORD POS: POS of the head word NAMED E NTITIES IN C ONSTITUENTS: 7 named entities as 7 binary features. PARTIAL PATH: Path from the constituent to the lowest common a"
P05-1072,P02-1031,0,0.536652,"Missing"
P05-1072,C04-1186,1,0.578802,"man, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT, A RG 1 is the P ROTO -PATIENT, etc. In addition to"
P05-1072,N03-2009,1,0.791276,"Missing"
P05-1072,W04-2416,1,0.736909,"Missing"
P05-1072,hockenmaier-steedman-2002-acquiring,0,0.0421125,"organized as follows. We first describe a baseline system based on the best published techniques. We then report on two sets of experiments using techniques that improve performance on the problem of finding arguments when they are present in the syntactic analysis. In the first set of experiments we explore new 581 Proceedings of the 43rd Annual Meeting of the ACL, pages 581–588, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics S features, including features extracted from a parser that provides a different syntactic view – a Combinatory Categorial Grammar (CCG) parser (Hockenmaier and Steedman, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu,"
P05-1072,kingsbury-palmer-2002-treebank,0,0.0227165,"lities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT, A RG 1 is the P ROTO -PATIENT, etc. In addition to these C ORE A RGUMENTS, additional A DJUNCTIVE A RGUMENTS, referred to as A RG Ms are also marked. Some examples are A RG M-L OC, for locatives; A RG M-T MP, for temporals; A RG MM NR"
P05-1072,W00-0730,0,0.0185823,"tion. Table 9: Head-word based performance using Charniak and Minipar parses. 5.2 Chunk-based Semantic Labeler Hacioglu has previously described a chunk based semantic labeling method (Hacioglu et al., 2004). This system uses SVM classifiers to first chunk input text into flat chunks or base phrases, each labeled with a syntactic tag. A second SVM is trained to assign semantic labels to the chunks. The system is trained 586 SVMs were trained for begin (B) and inside (I) classes of all arguments and outside (O) class for a total of 78 one-vs-all classifiers. Again, TinySVM5 along with YamCha6 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used as the SVM training and test software. Table 11 presents the system performances on the PropBank test set for the chunk-based system. 5 6 http://chasen.org/˜taku/software/TinySVM/ http://chasen.org/˜taku/software/yamcha/ 6 Combining Semantic Labelers We combined the semantic parses as follows: i) scores for arguments were converted to calibrated probabilities, and arguments with scores below a threshold value were deleted. Separate thresholds were used for each parser. ii) For the remaining arguments, the more probable ones among overlapping ones were selec"
P05-1072,N01-1025,0,0.456011,"lti-class classification problem using Support Vector Machine (SVM) classifier (Hacioglu et al., 2003; Pradhan et al., 2003; Pradhan et al., 2004) TinySVM2 along with YamCha3 (Kudo and Mat1 2 3 http://www.cis.upenn.edu/˜ace/ http://chasen.org/˜taku/software/TinySVM/ http://chasen.org/˜taku/software/yamcha/ 582 (hhhh ( (( NP VP ``` ((((hhhh VBD `VP T he acquisition ARG1 XXX was NULL VBN PP completed predicate in September ARGM−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. P REDICATE L EMMA PATH: Path from the constituent to the predicate in the parse tree. P OSITION: Whether the constituent is before or afte"
P05-1072,H94-1020,0,0.0271304,"ntic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT, A RG 1 is the P ROTO -PATIENT, etc. In addition to these C ORE A RGUMENTS, additional A DJUNCTIVE A RGUMENTS, referred to as A RG Ms are also marked. Some examples are A RG M-L OC, for locatives; A RG M-T MP, for temporals; A RG MM NR, for manner, etc. Figure 1 shows a syntax tree along with the argument labels for an example extracted from PropBank. We use Sections 02-21 for training, Section 00 for de"
P05-1072,J05-1004,0,0.248645,"periments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT, A RG 1 is the P ROTO -PATIENT, etc. In addition to these C ORE A RGUMENTS, additional A DJUNCTIVE A RGUMENTS, referred to as A RG Ms are also marked. Some examples are A RG M-L OC, for locatives; A RG M-T MP, for temporals; A RG MM NR, for manner, etc. Fig"
P05-1072,N04-1030,1,0.905723,"O -PATIENT, etc. In addition to these C ORE A RGUMENTS, additional A DJUNCTIVE A RGUMENTS, referred to as A RG Ms are also marked. Some examples are A RG M-L OC, for locatives; A RG M-T MP, for temporals; A RG MM NR, for manner, etc. Figure 1 shows a syntax tree along with the argument labels for an example extracted from PropBank. We use Sections 02-21 for training, Section 00 for development and Section 23 for testing. We formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifier (Hacioglu et al., 2003; Pradhan et al., 2003; Pradhan et al., 2004) TinySVM2 along with YamCha3 (Kudo and Mat1 2 3 http://www.cis.upenn.edu/˜ace/ http://chasen.org/˜taku/software/TinySVM/ http://chasen.org/˜taku/software/yamcha/ 582 (hhhh ( (( NP VP ``` ((((hhhh VBD `VP T he acquisition ARG1 XXX was NULL VBN PP completed predicate in September ARGM−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers"
P05-1072,P03-1002,0,0.108903,"tion ARG1 XXX was NULL VBN PP completed predicate in September ARGM−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. P REDICATE L EMMA PATH: Path from the constituent to the predicate in the parse tree. P OSITION: Whether the constituent is before or after the predicate. VOICE P REDICATE SUB - CATEGORIZATION P REDICATE C LUSTER H EAD W ORD: Head word of the constituent. H EAD W ORD POS: POS of the head word NAMED E NTITIES IN C ONSTITUENTS: 7 named entities as 7 binary features. PARTIAL PATH: Path from the constituent to the lowest common ancestor of the predicate and the constituent. V ERB S ENSE I NFORMATI"
P05-1072,W04-3212,0,0.685533,"M−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. P REDICATE L EMMA PATH: Path from the constituent to the predicate in the parse tree. P OSITION: Whether the constituent is before or after the predicate. VOICE P REDICATE SUB - CATEGORIZATION P REDICATE C LUSTER H EAD W ORD: Head word of the constituent. H EAD W ORD POS: POS of the head word NAMED E NTITIES IN C ONSTITUENTS: 7 named entities as 7 binary features. PARTIAL PATH: Path from the constituent to the lowest common ancestor of the predicate and the constituent. V ERB S ENSE I NFORMATION: Oracle verb sense information from PropBank H EAD W ORD OF PP:"
P05-1072,A00-2018,0,0.16996,"luding features extracted from a parser that provides a different syntactic view – a Combinatory Categorial Grammar (CCG) parser (Hockenmaier and Steedman, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected Tr"
P05-1072,W03-1006,0,0.252892,"Missing"
P05-1072,W03-1008,0,\N,Missing
P05-1072,P02-1043,0,\N,Missing
P06-1101,W04-3205,0,0.341856,"Missing"
P06-1101,W03-1022,0,0.00856129,"ps directly implied by hypernym evidence or the 5 relationships implied by the coordinate ev4.1 Methodology We evaluate the quality of our acquired hyponyms by direct judgment. In four separate annotation sessions, two judges labeled {50,100,100,100} samples uniformly generated from the first {100,1000,10000,20000} single links added by our algorithm. For the direct measure of fine-grained precision, we simply ask for each link H(X, Y ) added by the system, is X a Y ? In addition to the fine-grained precision, we give a coarse-grained evaluation, inspired by the idea of supersense-tagging in (Ciaramita and Johnson, 2003). The 26 supersenses used in WordNet 2.1 are listed in Table 1; we label a hyponym link as correct in the coarse-grained evaluation if the novel hyponym is placed under the appropriate supersense. This evaluation task 7 Checking whether or not Ris ∈ I(Hil1 ) may be efficiently computed by checking whether s is in the hypernym ancestors of l or if it shares a least common subsumer with l within 7 steps. 806 1 Tops 2 act 3 animal 4 artifact 5 attribute 6 body 7 cognition 8 communication 9 event 10 feeling 11 food 12 group 13 location 14 motive 15 object 16 person 17 phenomenon 18 plant 19 posses"
P06-1101,N03-1011,0,0.589768,"Missing"
P06-1101,C02-1130,0,0.079598,"Missing"
P06-1101,C92-2082,0,0.457857,"e been constructed by hand at great cost; while a crucial source of knowledge about the relations between words, these taxonomies still suffer from sparse coverage. Many algorithms with the potential for automatically extending lexical resources have been proposed, including work in lexical acquisition (Riloff and Shepherd, 1997; Roark and Charniak, 1998) and in discovering instances, named entities, and alternate glosses (Etzioni et al., 2005; Pasc¸a, 2005). Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hyponyms (Hearst, 1992), meronyms (Girju, 2003), 801 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 801–808, c Sydney, July 2006. 2006 Association for Computational Linguistics subsumer (LCS)2 is within exactly m and n links, mn to denote respectively.3 We use the notation Cij that i and j are (m, n)-cousins. Thus coordinate terms are (1, 1)-cousins; technically the hypernym relation may also be seen as a specific case of this representation; an immediate parent in the hypernym hierarchy is a (1, 0)-cousin, and the k-th ancestor is a (k, 0)-cou"
P06-1101,P90-1034,0,0.548845,"Eij ij ij all n is the possibility of adding a novel hyponym to an overly-specific hypernym, which might still H ) for a very large n. In orsatisfy P (Hijn |Eij der to discourage unnecessary overspecification, H ) by a we penalize each probability P (Hijk |Eij factor λk−1 for some λ &lt; 1, and renormalize: H ) ∝ λk−1 P (H |E H ). In our experiP (Hijk |Eij ij ij ments we set λ = 0.95. 3.2 (m, n)-cousin Classification The classifier for learning coordinate terms relies on the notion of distributional similarity, i.e., the idea that two words with similar meanings will be used in similar contexts (Hindle, 1990). We extend this notion to suggest that words with similar meanings should be near each other in a semantic taxonomy, and in particular will likely share a hypernym as a near parent. Our classifier for (m, n)-cousins is derived from the algorithm and corpus given in (Ravichandran et al., 2005). In that work an efficient randomized algorithm is derived for computing clusters of similar nouns. We use a set of more than 1000 distinct clusters of English nouns collected by their algorithm over 70 million webpages6 , with each noun i having a score representing its cosine similarity to the centroid"
P06-1101,W97-0313,0,0.154866,"g structured relational knowledge about lexical terms has been the motivating force underlying many projects in lexical acquisition, information extraction, and the construction of semantic taxonomies. Broad-coverage semantic taxonomies such as WordNet (Fellbaum, 1998) and CYC (Lenat, 1995) have been constructed by hand at great cost; while a crucial source of knowledge about the relations between words, these taxonomies still suffer from sparse coverage. Many algorithms with the potential for automatically extending lexical resources have been proposed, including work in lexical acquisition (Riloff and Shepherd, 1997; Roark and Charniak, 1998) and in discovering instances, named entities, and alternate glosses (Etzioni et al., 2005; Pasc¸a, 2005). Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hyponyms (Hearst, 1992), meronyms (Girju, 2003), 801 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 801–808, c Sydney, July 2006. 2006 Association for Computational Linguistics subsumer (LCS)2 is within exactly m and n links, mn to denote respectively.3 We use the no"
P06-1101,P98-2182,0,0.209243,"wledge about lexical terms has been the motivating force underlying many projects in lexical acquisition, information extraction, and the construction of semantic taxonomies. Broad-coverage semantic taxonomies such as WordNet (Fellbaum, 1998) and CYC (Lenat, 1995) have been constructed by hand at great cost; while a crucial source of knowledge about the relations between words, these taxonomies still suffer from sparse coverage. Many algorithms with the potential for automatically extending lexical resources have been proposed, including work in lexical acquisition (Riloff and Shepherd, 1997; Roark and Charniak, 1998) and in discovering instances, named entities, and alternate glosses (Etzioni et al., 2005; Pasc¸a, 2005). Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hyponyms (Hearst, 1992), meronyms (Girju, 2003), 801 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 801–808, c Sydney, July 2006. 2006 Association for Computational Linguistics subsumer (LCS)2 is within exactly m and n links, mn to denote respectively.3 We use the notation Cij that i and j are"
P06-1101,W03-0415,0,\N,Missing
P06-1101,P99-1016,0,\N,Missing
P06-1101,P05-1077,0,\N,Missing
P06-1101,C98-2177,0,\N,Missing
P09-1113,P07-1073,0,0.181614,"s of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. The resulting patterns often suffer from low precision and semantic drift. We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches. Distant supervision is an extension of the paradigm used by Snow et al. (2005) for exploiting WordNet to extract hypernym (is-a) relations between entities, and is similar to the use of w"
P09-1113,P05-1045,0,0.0933753,"• For each entity, one ‘window’ node that is not part of the dependency path A window node is a node connected to one of the two entities and not part of the dependency path. We generate one conjunctive feature for each pair of left and right window nodes, as well as features which omit one or both of them. Thus each syntactic row in Table 3 represents a single syntactic feature. Named entity tag features Every feature contains, in addition to the content described above, named entity tags for the two entities. We perform named entity tagging using the Stanford four-class named entity tagger (Finkel et al., 2005). The tagger provides each word with a label from {person, location, organization, miscellaneous, none}. 5.4 Feature conjunction Rather than use each of the above features in the classifier independently, we use only conjunctive features. Each feature consists of the conjunction of several attributes of the sentence, plus the named entity tags. For two features to match, all of their conjuncts must match exactly. This yields low-recall but high-precision features. With a small amount of data, this approach would be problematic, since most features would only be seen once, rendering them useles"
P09-1113,N03-1011,0,0.084268,"xical features, thereby avoiding the computational expense of parsing (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005), and the few systems that have used unsupervised IE have not compared the performance of these two types of feature. 2 Previous work Except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation extraction have typically relied on relatively small datasets, or on only a small number of distinct relations. Approaches based on WordNet have often only looked at the hypernym (is-a) or meronym (part-of) relation (Girju et al., 2003; Snow et al., 2005), while those based on the ACE program (Doddington et al., 2004) have been restricted in their evaluation to a small number of relation instances and corpora of less than a million words. Many early algorithms for relation extraction used little or no syntactic information. For example, the DIPRE algorithm by Brin (1998) used string-based regular expressions in order to recognize relations such as author-book, while the S NOWBALL algorithm by Agichtein and Gravano (2000) learned similar regular expression patterns over words and named entity tags. Hearst (1992) used a small"
P09-1113,C92-2082,0,0.125126,"lation (Girju et al., 2003; Snow et al., 2005), while those based on the ACE program (Doddington et al., 2004) have been restricted in their evaluation to a small number of relation instances and corpora of less than a million words. Many early algorithms for relation extraction used little or no syntactic information. For example, the DIPRE algorithm by Brin (1998) used string-based regular expressions in order to recognize relations such as author-book, while the S NOWBALL algorithm by Agichtein and Gravano (2000) learned similar regular expression patterns over words and named entity tags. Hearst (1992) used a small number of regular expressions over words and part-of-speech tags to find examples of the hypernym relation. The use of these patterns has been widely replicated in successful systems, for example by Etzioni et al. (2005). Other work 1004 Relation name /people/person/nationality /location/location/contains /people/person/profession /people/person/place of birth /dining/restaurant/cuisine /business/business chain/location /biology/organism classification rank /film/film/genre /film/film/language /biology/organism higher classification /film/film/country /film/writer/film /film/dire"
P09-1113,P06-1015,0,0.0844068,"noir Enter the Phoenix, Cantonese Calopteryx, Calopterygidae Turtle Diary, United States Irving Shulman, Rebel Without a Cause Michael Mann, Collateral Diane Eskenazi, Aladdin John W. Kern, Asheville The Octopus Project, Austin Joseph Chartrand, Catholicism Paul Auster, Travels in the Scriptorium Midfielder, Chen Tao Richard Daintree, Tuberculosis Pony Soldiers, Science fiction Stavisky, Stephen Sondheim ATS Medical, Health care Table 2: The 23 largest Freebase relations we use, with their size and an instance of each relation. such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007). Perhaps most similar to our distant supervision algorithm is the effective method of Wu and Weld (2007) who extract relations from a Wikipedia page by using supervision"
P09-1113,W06-3909,0,0.0834812,"between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. The resulting patterns often suffer from low precision and semantic drift. We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches. Distant supervision is an extension of the paradigm used by Snow et al. (2005) for exploiting WordNet to extract hypernym (is-a) relations between entities, and"
P09-1113,P02-1006,0,0.0867227,"information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. The resulting patterns often suffer from low precision and semantic drift. We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches. Distant supervision is an extension of the paradigm used by Snow et al. (2005) for exploiting WordNet to ex"
P09-1113,N06-1039,0,0.0714155,"lly combining relation mentions (Zhou et al., 2005; Zhou et al., 2007; Surdeanu and Ciaramita, 2007). Supervised relation extraction suffers from a number of problems, however. Labeled training data is expensive to produce and thus limited in quantity. Also, because the relations are labeled on a particular corpus, the resulting classifiers tend to be biased toward that text domain. An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a ne"
P09-1113,D08-1027,1,0.0866059,"Missing"
P09-1113,P05-1053,0,0.790878,"a corpus are first hand-labeled for the presence of entities and the relations between them. The NIST Automatic Content Extraction (ACE) RDC 2003 and 2004 corpora, for example, include over 1,000 documents in which pairs of entities have been labeled with 5 to 7 major relation types and 23 to 24 subrelations, totaling 16,771 relation instances. ACE systems then extract a wide variety of lexical, syntactic, and semantic features, and use supervised classifiers to label the relation mention holding between a given pair of entities in a test set sentence, optionally combining relation mentions (Zhou et al., 2005; Zhou et al., 2007; Surdeanu and Ciaramita, 2007). Supervised relation extraction suffers from a number of problems, however. Labeled training data is expensive to produce and thus limited in quantity. Also, because the relations are labeled on a particular corpus, the resulting classifiers tend to be biased toward that text domain. An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Un"
P09-1113,D07-1076,0,0.0721049,"hand-labeled for the presence of entities and the relations between them. The NIST Automatic Content Extraction (ACE) RDC 2003 and 2004 corpora, for example, include over 1,000 documents in which pairs of entities have been labeled with 5 to 7 major relation types and 23 to 24 subrelations, totaling 16,771 relation instances. ACE systems then extract a wide variety of lexical, syntactic, and semantic features, and use supervised classifiers to label the relation mention holding between a given pair of entities in a test set sentence, optionally combining relation mentions (Zhou et al., 2005; Zhou et al., 2007; Surdeanu and Ciaramita, 2007). Supervised relation extraction suffers from a number of problems, however. Labeled training data is expensive to produce and thus limited in quantity. Also, because the relations are labeled on a particular corpus, the resulting classifiers tend to be biased toward that text domain. An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approach"
P09-1113,doddington-etal-2004-automatic,0,\N,Missing
P10-1046,P99-1014,0,0.25135,"Missing"
P10-1046,P98-1013,0,0.00545487,"ficult to classify than random choices. Nakov and Hearst (2003) further illustrated how random confounders are easier to identify than those selected from semantically ambiguous, yet related concepts. Our approach evaluates selectional preferences, not WSD, but our results complement these findings. We identified three methods of confounder selection based on varying levels of corpus fre4 Recent work does include some seen data. Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al., 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). Neither evaluates all of the seen data, however. 448 C(vd , n) as the number of times v and n (ignoring d) appear in the same n-gram. We propose a conditional probability baseline: ( P (n|vd ) = C(vd ,n) C(vd ,∗) if C(vd , n) > 0 0 otherwise 5.3 We implemented the current state-of-the-art smoothing model of Erk (2007). The model is based on the idea that the arguments of a particular verb slot tend to be similar to each other. Given two potential arguments for a verb, the correct one sh"
P10-1046,D08-1007,0,0.410392,"WSD has shown that confounder choice can make the pseudo-disambiguation task significantly easier. Gaustad (2001) showed that human-generated pseudo-words are more difficult to classify than random choices. Nakov and Hearst (2003) further illustrated how random confounders are easier to identify than those selected from semantically ambiguous, yet related concepts. Our approach evaluates selectional preferences, not WSD, but our results complement these findings. We identified three methods of confounder selection based on varying levels of corpus fre4 Recent work does include some seen data. Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al., 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). Neither evaluates all of the seen data, however. 448 C(vd , n) as the number of times v and n (ignoring d) appear in the same n-gram. We propose a conditional probability baseline: ( P (n|vd ) = C(vd ,n) C(vd ,∗) if C(vd , n) > 0 0 otherwise 5.3 We implemented the current state-of-the-art smoothing model of Erk (2007). The m"
P10-1046,C00-2137,0,0.186311,"Missing"
P10-1046,P09-2019,0,0.0979729,"Missing"
P10-1046,P07-1028,0,0.564095,"wed that human-generated pseudo-words are more difficult to classify than random choices. Nakov and Hearst (2003) further illustrated how random confounders are easier to identify than those selected from semantically ambiguous, yet related concepts. Our approach evaluates selectional preferences, not WSD, but our results complement these findings. We identified three methods of confounder selection based on varying levels of corpus fre4 Recent work does include some seen data. Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al., 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). Neither evaluates all of the seen data, however. 448 C(vd , n) as the number of times v and n (ignoring d) appear in the same n-gram. We propose a conditional probability baseline: ( P (n|vd ) = C(vd ,n) C(vd ,∗) if C(vd , n) > 0 0 otherwise 5.3 We implemented the current state-of-the-art smoothing model of Erk (2007). The model is based on the idea that the arguments of a particular verb slot tend to be similar to each other. G"
P10-1046,J03-3005,0,0.469918,"he first to test only on unseen pairs. Several papers followed with differing methods of choosing a test pair (v, n) and its confounder v 0 . Dagan et al. (1999) tested all unseen (v, n) occurrences of the most frequent 1000 verbs in his corpus. They then sorted verbs by corpus frequency and chose the neighboring verb v 0 of v as the confounder to ensure the closest frequency match possible. Rooth et al. (1999) tested 3000 random (v, n) pairs, but required the verbs and nouns to appear between 30 and 3000 times in training. They also chose confounders randomly so that the new pair was unseen. Keller and Lapata (2003) specifically addressed the impact of unseen data by using the web to first ‘see’ the data. They evaluated unseen pseudowords by attempting to first observe them in a larger corpus (the Web). One modeling difference was to disambiguate the nouns as selectional preferences instead of the verbs. Given a test pair (v, n) and its confounder (v, n0 ), they used web searches such as “v Det n” to make the decision. Results beat or matched current results at the time. We present a similarly motivated, but new webbased approach later. Very recent work with pseudo-words (Erk, 2007; Bergsma et al., 2008)"
P10-1046,E99-1005,0,0.0446049,"erb slot tend to be similar to each other. Given two potential arguments for a verb, the correct one should correlate higher with the arguments observed with the verb during training. Formally, given a verb v and a grammatical dependency d, the score for a noun n is defined: where C(vd , n) is the number of times the head word n was seen as an argument to the predicate v, and C(vd , ∗) is the number of times vd was seen with any argument. Given a test (vd , n) and its confounder (vd , n0 ), choose n if P (n|vd ) > P (n0 |vd ), and n0 otherwise. If P (n|vd ) = P (n0 |vd ), randomly choose one. Lapata et al. (1999) showed that corpus frequency and conditional probability correlate with human decisions of adjective-noun plausibility, and Dagan et al. (1999) appear to propose a very similar baseline for verb-noun selectional preferences, but the paper evaluates unseen data, and so the conditional probability model is not studied. We later analyze this baseline against a more complicated smoothing approach. 5.2 Smoothing Model Svd (n) = X sim(n, w) ∗ C(vd , w) (1) w∈Seen(vd ) where sim(n, w) is a noun-noun similarity score, Seen(vd ) is the set of seen head words filling the slot vd during training, and C("
P10-1046,N03-2023,0,0.405902,"Missing"
P10-1046,P93-1024,0,0.048268,"utze (1992) simply called the words, ‘artificial ambiguous words’, but Gale et al. (1992) proposed the succinct name, pseudo-word. Both papers cited the sparsity and difficulty of creating large labeled datasets as the motivation behind pseudo-words. Gale et al. selected unambiguous words from the corpus and paired them with random words from different thesaurus categories. Sch¨utze paired his words with confounders that were ‘comparable in frequency’ and ‘distinct semantically’. Gale et al.’s pseudo-word term continues today, as does Sch¨utze’s frequency approach to selecting the confounder. Pereira et al. (1993) soon followed with a selectional preference proposal that focused on a language model’s effectiveness on unseen data. The work studied clustering approaches to assist in similarity decisions, predicting which of two verbs 3 How Frequent is Unseen Data? Most NLP tasks evaluate their entire datasets, but as described above, most selectional preference evaluations have focused only on unseen data. This section investigates the extent of unseen examples in a typical training/testing environment 446 of newspaper articles. The results show that even with a small training size, seen examples dominat"
P10-1046,C98-1013,0,\N,Missing
P10-1083,P09-1110,0,0.0796294,"(2009). Our task is to learn a policy, or mapping Introduction Spatial language usage is a vital component for physically grounded language understanding systems. Spoken language interfaces to robotic assistants (Wei et al., 2009) and Geographic Information Systems (Wang et al., 2004) must cope with the inherent ambiguity in spatial descriptions. The semantics of imperative and spatial language is heavily dependent on the physical setting it is situated in, motivating automated learning approaches to acquiring meaning. Traditional accounts of learning typically rely on linguistic annotation (Zettlemoyer and Collins, 2009) or word distributions (Curran, 2003). In contrast, we present an apprenticeship learning system which learns to imitate human instruction following, without linguistic annotation. Solved using a reinforcement learning algorithm, our system acquires the meaning of spatial words through 806 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 806–814, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics from world state to action, which most closely follows the reference route. Our state space combines world and linguistic"
P10-1083,P09-1010,0,\N,Missing
P10-1130,P05-1022,0,0.00813,"y annotations of fractional words (e.g., &lt;i>basmachi&lt;/i> s) and tokens (e.g., &lt;i>Sesame Street&lt;/i>-like), we broke up all markup that crossed sentence boundaries (i.e., loosely speaking, replaced constructs like &lt;u>...][S ...&lt;/u> with &lt;u>...&lt;/u> ][S &lt;u>...&lt;/u>) and discarded any 4 http://danielpipes.org/art/year/all http://danielpipes.org/article print.php? id=. . . 5 tags left covering entire sentences. We finalized two versions of the data: BLOGt, tagged with the Stanford tagger (Toutanova and Manning, 2000; Toutanova et al., 2003),6 and BLOGp, parsed with Charniak’s parser (Charniak, 2001; Charniak and Johnson, 2005).7 The reason for this dichotomy was to use state-of-the-art parses to analyze the relationship between syntax and mark-up, yet to prevent jointly tagged (and non-standard AUX[G]) POS sequences from interfering with our (otherwise unsupervised) training. 8 4.2 Scaled up Quantity: The (English) Web We built a large (see Table 1) but messy data set, WEB — English-looking web-pages, pre-crawled by a search engine. To avoid machine-generated spam, we excluded low quality sites flagged by the indexing system. We kept only sentence-like runs of words (satisfying punctuation and capitalization constr"
P10-1130,P01-1017,0,0.0129575,"ter throwing away annotations of fractional words (e.g., &lt;i>basmachi&lt;/i> s) and tokens (e.g., &lt;i>Sesame Street&lt;/i>-like), we broke up all markup that crossed sentence boundaries (i.e., loosely speaking, replaced constructs like &lt;u>...][S ...&lt;/u> with &lt;u>...&lt;/u> ][S &lt;u>...&lt;/u>) and discarded any 4 http://danielpipes.org/art/year/all http://danielpipes.org/article print.php? id=. . . 5 tags left covering entire sentences. We finalized two versions of the data: BLOGt, tagged with the Stanford tagger (Toutanova and Manning, 2000; Toutanova et al., 2003),6 and BLOGp, parsed with Charniak’s parser (Charniak, 2001; Charniak and Johnson, 2005).7 The reason for this dichotomy was to use state-of-the-art parses to analyze the relationship between syntax and mark-up, yet to prevent jointly tagged (and non-standard AUX[G]) POS sequences from interfering with our (otherwise unsupervised) training. 8 4.2 Scaled up Quantity: The (English) Web We built a large (see Table 1) but messy data set, WEB — English-looking web-pages, pre-crawled by a search engine. To avoid machine-generated spam, we excluded low quality sites flagged by the indexing system. We kept only sentence-like runs of words (satisfying punctuat"
P10-1130,W95-0113,0,0.191609,"Missing"
P10-1130,N09-1009,0,0.567321,"oduction Unsupervised learning of hierarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version of this problem that targets dependencies and assumes partial annotation — sentence boundaries and part-of-speech (POS) tagging — has received much attention. Klein and Manning (2004) were the first to beat a simple parsing heuristic, the right-branching baseline; today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010a) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM. Pereira and Schabes (1992) outlined three major problems with classic EM, applied to a related problem, constituent parsing. They extended classic inside-outside re-estimation (Baker, 1979) to respect any bracketing constraints included with a training corpus. This conditioning on partial parses addressed all three problems, leading to: (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgments of sentence struct"
P10-1130,W09-1108,0,0.0296066,"Missing"
P10-1130,P09-1041,0,0.0244549,"Missing"
P10-1130,P99-1059,0,0.181591,"Missing"
P10-1130,N09-1037,0,0.0135559,"2007), query segmentation (Tan and Peng, 2008), plus semantic relatedness and word-sense disambiguation (Gabrilovich and Markovitch, 2007; Yeh et al., 2009). Yet several, seemingly natural, candidate core NLP tasks — tokenization, CJK segmentation, noun-phrase chunking, and (until now) parsing — remained conspicuously uninvolved. Approaches related to ours arise in applications that combine parsing with named-entity recognition (NER). For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al., 2009), that couple chunking with (dependency) parsing. Although restricted to proper noun phrases, dates, times and quantities, we suspect that constituents identified by trained (supervised) NER systems would also 1285 Model DMV EVG Incarnation WSJ10 Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 Leapfrog (Spitkovsky et al., 2010a) 57.1 default INIT=0,GENRE=0,SCOPE=0,CONSTR=0,TRIM=0,ADAPT=0 55.9 WSJ-best INIT=1,GENRE=0,SCOPE=2,CONSTR=0,TRIM=0,ADAPT=0 65.3 BLOGt -best INIT=1,GENRE=1,SCOPE=2,CON"
P10-1130,J93-2004,0,0.043261,"hnologies such as HTML and XML. As noted, web annotations can be indicative of phrase boundaries, e.g., in a complicated sentence: one of each nested tag’s boundaries aligns; and Toronto Star’s neglected determiner could be forgiven, certainly within a dependency formulation. 3 A High-Level Outline of Our Approach Our idea is to implement the DMV (Klein and Manning, 2004) — a standard unsupervised grammar inducer. But instead of learning the unannotated test set, we train with text that contains web mark-up, using various ways of converting HTML into parsing constraints. We still test on WSJ (Marcus et al., 1993), in the standard way, and also check generalization against a hidden data set — the Brown corpus (Francis and Kucera, 1979). Our parsing constraints come from a blog — a new corpus we created, the web and news (see Table 1 for corpora’s sentence and token counts). To facilitate future work, we make the final models and our manually-constructed blog data publicly available.3 Although we are unable to share larger-scale resources, our main results should be reproducible, as both linguistic analysis and our best model rely exclusively on the blog. In 1998, however, as I &lt;a>[VP established in &lt;i>"
P10-1130,N06-1020,0,0.120298,"Missing"
P10-1130,P09-1113,1,0.167247,"ambiguation (Gabrilovich and Markovitch, 2007; Yeh et al., 2009). Yet several, seemingly natural, candidate core NLP tasks — tokenization, CJK segmentation, noun-phrase chunking, and (until now) parsing — remained conspicuously uninvolved. Approaches related to ours arise in applications that combine parsing with named-entity recognition (NER). For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al., 2009), that couple chunking with (dependency) parsing. Although restricted to proper noun phrases, dates, times and quantities, we suspect that constituents identified by trained (supervised) NER systems would also 1285 Model DMV EVG Incarnation WSJ10 Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 Leapfrog (Spitkovsky et al., 2010a) 57.1 default INIT=0,GENRE=0,SCOPE=0,CONSTR=0,TRIM=0,ADAPT=0 55.9 WSJ-best INIT=1,GENRE=0,SCOPE=2,CONSTR=0,TRIM=0,ADAPT=0 65.3 BLOGt -best INIT=1,GENRE=1,SCOPE=2,CONSTR=3,TRIM=1,ADAPT=1 69.3 NEWS-best INIT=1,GENRE=2,SCOPE=0,CONSTR=3,TRIM=1,ADAPT=1 67."
P10-1130,P92-1017,0,0.581647,"ion promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version of this problem that targets dependencies and assumes partial annotation — sentence boundaries and part-of-speech (POS) tagging — has received much attention. Klein and Manning (2004) were the first to beat a simple parsing heuristic, the right-branching baseline; today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010a) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM. Pereira and Schabes (1992) outlined three major problems with classic EM, applied to a related problem, constituent parsing. They extended classic inside-outside re-estimation (Baker, 1979) to respect any bracketing constraints included with a training corpus. This conditioning on partial parses addressed all three problems, leading to: (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgments of sentence structure, which is underdetermined by unannotated text; (ii) fewer iterations needed to reach a good grammar, countering convergence properties that sha"
P10-1130,D08-1093,0,0.0391762,"Missing"
P10-1130,A97-1004,0,0.0609459,"rk-up, we began constructing a new one by downloading articles4 from a newsstyle blog. Although limited to a single genre — political opinion, danielpipes.org is clean, consistently formatted, carefully edited and larger than WSJ (see Table 1). Spanning decades, Pipes’ editorials are mostly in-domain for POS taggers and tree-bank-trained parsers; his recent (internetera) entries are thoroughly cross-referenced, conveniently providing just the mark-up we hoped to study via uncluttered (printer-friendly) HTML. 5 After extracting moderately clean text and mark-up locations, we used MxTerminator (Reynar and Ratnaparkhi, 1997) to detect sentence boundaries. This initial automated pass begot multiple rounds of various semi-automated clean-ups that involved fixing sentence breaking, modifying parser-unfriendly tokens, converting HTML entities and non-ASCII text, correcting typos, and so on. After throwing away annotations of fractional words (e.g., &lt;i>basmachi&lt;/i> s) and tokens (e.g., &lt;i>Sesame Street&lt;/i>-like), we broke up all markup that crossed sentence boundaries (i.e., loosely speaking, replaced constructs like &lt;u>...][S ...&lt;/u> with &lt;u>...&lt;/u> ][S &lt;u>...&lt;/u>) and discarded any 4 http://danielpipes.org/art/year/"
P10-1130,P02-1035,0,0.018331,"exicalized (Headden et al., 2009) 68.8 WSJ20 48.0 48.7 45.8 53.8 56.8 56.2 52.7 WSJ∞ 42.2 45.0 41.6 47.9 50.4 50.1 46.3 Brown100 43.6 40.5 50.8 53.3 51.6 46.9 Table 8: Accuracies on Section 23 of WSJ{10, 20,∞ } and Brown100 for three recent state-of-the-art systems, our default run, and our best runs (judged by accuracy on WSJ45) for each of four training sets. be helpful in constraining grammar induction. Following Pereira and Schabes’ (1992) success with partial annotations in training a model of (English) constituents generatively, their idea has been extended to discriminative estimation (Riezler et al., 2002) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005). There was demand for partially bracketed corpora. Chen and Lee (1995) constructed one such corpus by learning to partition (English) POS sequences into chunks (Abney, 1991); Inui and Kotani (2001) used n-gram statistics to split (Japanese) clauses. We combine the two intuitions, using the web to build a partially parsed corpus. Our approach could be called lightly-supervised, since it does not require manual annotation of a single complete parse tree. In contrast, traditional semi-supervised methods rely on fully-anno"
P10-1130,I05-1008,0,0.27944,"Missing"
P10-1130,P07-1049,0,0.0614075,"Missing"
P10-1130,N10-1116,1,0.714456,"earning of hierarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version of this problem that targets dependencies and assumes partial annotation — sentence boundaries and part-of-speech (POS) tagging — has received much attention. Klein and Manning (2004) were the first to beat a simple parsing heuristic, the right-branching baseline; today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010a) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM. Pereira and Schabes (1992) outlined three major problems with classic EM, applied to a related problem, constituent parsing. They extended classic inside-outside re-estimation (Baker, 1979) to respect any bracketing constraints included with a training corpus. This conditioning on partial parses addressed all three problems, leading to: (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgments of sentence structure, which is underdeterm"
P10-1130,W01-0521,0,0.0354984,"Missing"
P10-1130,W10-2902,1,0.858668,"earning of hierarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version of this problem that targets dependencies and assumes partial annotation — sentence boundaries and part-of-speech (POS) tagging — has received much attention. Klein and Manning (2004) were the first to beat a simple parsing heuristic, the right-branching baseline; today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010a) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM. Pereira and Schabes (1992) outlined three major problems with classic EM, applied to a related problem, constituent parsing. They extended classic inside-outside re-estimation (Baker, 1979) to respect any bracketing constraints included with a training corpus. This conditioning on partial parses addressed all three problems, leading to: (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgments of sentence structure, which is underdeterm"
P10-1130,N09-1012,0,0.558495,"ability in NLP. 1 Introduction Unsupervised learning of hierarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version of this problem that targets dependencies and assumes partial annotation — sentence boundaries and part-of-speech (POS) tagging — has received much attention. Klein and Manning (2004) were the first to beat a simple parsing heuristic, the right-branching baseline; today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010a) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM. Pereira and Schabes (1992) outlined three major problems with classic EM, applied to a related problem, constituent parsing. They extended classic inside-outside re-estimation (Baker, 1979) to respect any bracketing constraints included with a training corpus. This conditioning on partial parses addressed all three problems, leading to: (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgm"
P10-1130,Y01-1031,0,0.0579646,"Missing"
P10-1130,P04-1061,0,0.373999,"on annotates a sentence. As it happens, a non-trivial fraction of the world’s population routinely annotates text diligently, if only partially and informally.1 They inject hyper-links, vary font sizes, and toggle colors and styles, using mark-up technologies such as HTML and XML. As noted, web annotations can be indicative of phrase boundaries, e.g., in a complicated sentence: one of each nested tag’s boundaries aligns; and Toronto Star’s neglected determiner could be forgiven, certainly within a dependency formulation. 3 A High-Level Outline of Our Approach Our idea is to implement the DMV (Klein and Manning, 2004) — a standard unsupervised grammar inducer. But instead of learning the unannotated test set, we train with text that contains web mark-up, using various ways of converting HTML into parsing constraints. We still test on WSJ (Marcus et al., 1993), in the standard way, and also check generalization against a hidden data set — the Brown corpus (Francis and Kucera, 1979). Our parsing constraints come from a blog — a new corpus we created, the web and news (see Table 1 for corpora’s sentence and token counts). To facilitate future work, we make the final models and our manually-constructed blog da"
P10-1130,W00-1308,0,0.0100307,"r-unfriendly tokens, converting HTML entities and non-ASCII text, correcting typos, and so on. After throwing away annotations of fractional words (e.g., &lt;i>basmachi&lt;/i> s) and tokens (e.g., &lt;i>Sesame Street&lt;/i>-like), we broke up all markup that crossed sentence boundaries (i.e., loosely speaking, replaced constructs like &lt;u>...][S ...&lt;/u> with &lt;u>...&lt;/u> ][S &lt;u>...&lt;/u>) and discarded any 4 http://danielpipes.org/art/year/all http://danielpipes.org/article print.php? id=. . . 5 tags left covering entire sentences. We finalized two versions of the data: BLOGt, tagged with the Stanford tagger (Toutanova and Manning, 2000; Toutanova et al., 2003),6 and BLOGp, parsed with Charniak’s parser (Charniak, 2001; Charniak and Johnson, 2005).7 The reason for this dichotomy was to use state-of-the-art parses to analyze the relationship between syntax and mark-up, yet to prevent jointly tagged (and non-standard AUX[G]) POS sequences from interfering with our (otherwise unsupervised) training. 8 4.2 Scaled up Quantity: The (English) Web We built a large (see Table 1) but messy data set, WEB — English-looking web-pages, pre-crawled by a search engine. To avoid machine-generated spam, we excluded low quality sites flagged b"
P10-1130,N03-1033,0,0.0309083,"ng HTML entities and non-ASCII text, correcting typos, and so on. After throwing away annotations of fractional words (e.g., &lt;i>basmachi&lt;/i> s) and tokens (e.g., &lt;i>Sesame Street&lt;/i>-like), we broke up all markup that crossed sentence boundaries (i.e., loosely speaking, replaced constructs like &lt;u>...][S ...&lt;/u> with &lt;u>...&lt;/u> ][S &lt;u>...&lt;/u>) and discarded any 4 http://danielpipes.org/art/year/all http://danielpipes.org/article print.php? id=. . . 5 tags left covering entire sentences. We finalized two versions of the data: BLOGt, tagged with the Stanford tagger (Toutanova and Manning, 2000; Toutanova et al., 2003),6 and BLOGp, parsed with Charniak’s parser (Charniak, 2001; Charniak and Johnson, 2005).7 The reason for this dichotomy was to use state-of-the-art parses to analyze the relationship between syntax and mark-up, yet to prevent jointly tagged (and non-standard AUX[G]) POS sequences from interfering with our (otherwise unsupervised) training. 8 4.2 Scaled up Quantity: The (English) Web We built a large (see Table 1) but messy data set, WEB — English-looking web-pages, pre-crawled by a search engine. To avoid machine-generated spam, we excluded low quality sites flagged by the indexing system. We"
P10-1130,P07-1031,0,0.0148878,"emoirs]&lt;/a>, Netanyahu changed his mind and ... Corpus WSJ∞ Section 23 WSJ45 WSJ15 Brown100 BLOGp BLOGt 45 BLOGt 15 NEWS45 NEWS15 WEB45 WEB15 In doing so, mark-up sometimes offers useful cues even for low-level tokenization decisions: [NP [NP Libyan ruler] &lt;a>[NP Mu‘ammar al-Qaddafi]&lt;/a>] referred to ... (NP (ADJP (NP (JJ Libyan) (NN ruler)) (JJ Mu)) (‘‘ ‘) (NN ammar) (NNS al-Qaddafi)) Above, a backward quote in an Arabic name confuses the Stanford parser. 2 Yet mark-up lines up with the broken noun phrase, signals cohesion, and moreover sheds light on the internal structure of a compound. As Vadas and Curran (2007) point out, such details are frequently omitted even from manually compiled tree-banks that err on the side of flat annotations of base-NPs. Admittedly, not all boundaries between HTML tags and syntactic constituents match up nicely: ..., but [S [NP the &lt;a>&lt;i>Toronto Star&lt;/i>][VP reports [NP this][PP in the softest possible way]&lt;/a>,[S stating only that ...]]] Combining parsing with mark-up may not be straight-forward, but there is hope: even above, 1 Even when (American) grammar schools lived up to their name, they only taught dependencies. This was back in the days before constituent grammar"
P10-1130,D07-1068,0,0.0129862,"ique POS tags, data sparsity is hardly an issue. Extra examples of lexical items help little and hurt when they are mistagged. 8 Related Work The wealth of new annotations produced in many languages every day already fuels a number of NLP applications. Following their early and wide-spread use by search engines, in service of spam-fighting and retrieval, anchor text and link data enhanced a variety of traditional NLP techniques: cross-lingual information retrieval (Nie and Chen, 2002), translation (Lu et al., 2004), both named-entity recognition (Mihalcea and Csomai, 2007) and categorization (Watanabe et al., 2007), query segmentation (Tan and Peng, 2008), plus semantic relatedness and word-sense disambiguation (Gabrilovich and Markovitch, 2007; Yeh et al., 2009). Yet several, seemingly natural, candidate core NLP tasks — tokenization, CJK segmentation, noun-phrase chunking, and (until now) parsing — remained conspicuously uninvolved. Approaches related to ours arise in applications that combine parsing with named-entity recognition (NER). For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finke"
P10-1130,W09-3206,0,0.0106125,"new annotations produced in many languages every day already fuels a number of NLP applications. Following their early and wide-spread use by search engines, in service of spam-fighting and retrieval, anchor text and link data enhanced a variety of traditional NLP techniques: cross-lingual information retrieval (Nie and Chen, 2002), translation (Lu et al., 2004), both named-entity recognition (Mihalcea and Csomai, 2007) and categorization (Watanabe et al., 2007), query segmentation (Tan and Peng, 2008), plus semantic relatedness and word-sense disambiguation (Gabrilovich and Markovitch, 2007; Yeh et al., 2009). Yet several, seemingly natural, candidate core NLP tasks — tokenization, CJK segmentation, noun-phrase chunking, and (until now) parsing — remained conspicuously uninvolved. Approaches related to ours arise in applications that combine parsing with named-entity recognition (NER). For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al., 2009), that couple chunking with (dependency) pars"
P10-1130,J03-4003,0,\N,Missing
P10-1130,D08-1107,0,\N,Missing
P10-1130,A00-1031,0,\N,Missing
P95-1001,J94-3007,0,0.0622685,"Missing"
P95-1001,H93-1013,0,0.0137369,"on a corpus of lexically transcribed speech using the computational resources of a speech recognition system. This algorithm belongs to the class of techniques we call Exploratory Computational Phonology, which use statistical pattern recognition tools to explore phonological spaces. We describe the details of our probability estimation algorithm and also present the probabilities the system has learned for ten common phonological rules which model reductions and coarticulation effects. Our probabilities are derived from a corpus of 7203 sentences of read speech from the Wall Street Journal (NIST 1993). We also benchmark the probabilities generated by our system against probabilities from phonetically hand-transcribed data, and show a relatively good fit. Finally, we analyze the probability differences between rule use in male ver1Note that this is true whether phonological theory considers these true phonological rules or rather rules of ~phonetic interpretation&quot;. sus female speech, and suggest that the differences are caused by differing average rates of speech. 2 The erate phone sequences from word orthography as an additional source of pronunciations. [IPAIARPAIICSI b b b d d d g g g p"
P95-1002,P84-1070,0,0.0968189,"Missing"
P95-1002,J94-3001,0,\N,Missing
P95-1002,J94-3007,0,\N,Missing
P98-2184,A97-1052,0,0.119973,"Missing"
P98-2184,P96-1025,0,0.07355,"Missing"
P98-2184,P97-1003,0,0.0941533,"Missing"
P98-2184,P98-1071,0,0.0369165,"Missing"
P98-2184,C94-1024,0,0.0603214,"Missing"
P98-2184,J93-2004,0,0.0520385,"Missing"
P98-2184,W93-0109,0,0.114032,"Missing"
P98-2184,H94-1020,0,\N,Missing
P98-2184,P93-1032,0,\N,Missing
P98-2184,J93-2001,0,\N,Missing
P98-2184,C98-1068,0,\N,Missing
P98-2184,H92-1045,0,\N,Missing
reschke-etal-2014-event,D12-1042,1,\N,Missing
reschke-etal-2014-event,P09-1113,1,\N,Missing
reschke-etal-2014-event,W11-0307,0,\N,Missing
W00-0712,J92-3008,0,0.0595087,"Missing"
W00-0712,P99-1037,0,0.0287398,"Missing"
W00-0712,A00-1030,0,0.0428723,"Missing"
W00-0712,W98-1239,0,0.0813223,"Missing"
W00-0712,W99-0904,0,0.0417632,"ntic Analysis P a t r i c k S c h o n e and D a n i e l J u r a f s k y University of Colorado Boulder, Colorado 80309 {schone, jurafsky}@cs.colorado.edu to take advantage of regularities&quot; (Sproat, 1992, p. xiii). Hence, automatic morphological analysis is also critical for selecting appropriate and non-redundant MRD headwords. For the reasons expressed above, we are interested in knowledge-free morphology induction. Thus, in this paper, we show how to automatically induce morphological relationships between words. Previous morphology induction approaches (Goldsmith, 1997, 2000; D4Jean, 1998; Gaussier, 1999) have focused on inflectional languages and have used statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Several problems can arise using only stem-and-affix statistics: (1) valid affixes may be applied inappropriately (&quot;ally&quot; stemming to &quot;all&quot;), (2) morphological ambiguity may arise (&quot;rating&quot; conflating with &quot;rat&quot; instead of &quot;rate&quot;), and (3) non-productive affixes may get accidentally pruned (the relationship between &quot;dirty&quot; and &quot;dirt&quot; may be lost)3 Some of these problems could be resolved if one could incorporate word semantics. For instance, &quot;all&quot; i"
W00-0712,J00-4006,0,\N,Missing
W00-0712,J01-2001,0,\N,Missing
W00-0905,J93-2001,0,0.298295,"&apos;Department of Linguistics Computer Science, 3Institute of Cognitive Harvard University Science Cambridge MA 02138 University of Colorado sgahl @fas.harvard.edu Boulder, CO 80309-0295 {douglas.roland, jurafslQ1, lise.menn, elizabeth.elder, christopher.b.riddoch }@colorado.edu Abstract Previous research, however, has shown that subcategorization probabilities vary widely in different corpora. Studies such as Merlo (1994), Gibson et al. (1996), and Roland & Jurafsky (1997) have found subcategorization frequency differences between traditional corpus data and data from psychological experiments. Biber (1993) and Biber et al. (1998) have shown that that word frequency, word sense (as defined by collocates), the distribution of synonymous words and the use of syntactic structures varies with corpus genre. Roland & Jurafsky (1998, 2000 in press) showed that there were subcategorization frequency differences between We explore the differences in verb subeategorization frequencies across several corpora in an effort to obtain stable cross corpus subcategonzation probabilities for use in norming psychological experiments. For the 64 single sense verbs we looked at, subeategorizatlon preferences were re"
W00-0905,W98-1114,0,0.0448077,"Missing"
W00-0905,P96-1025,0,0.0269394,"ubtle) word sense differences. This is an interesting observation in itself, various written and spoken corpora, and furthermore showed that that these subcategorization frequency differences are caused by variation in word sense as well as genre and discourse type differences among the and also suggests that stable cross corpus subcategorization frequencies may be found when verb sense is adequately controlled. corpora. Introduction Verb subcategorizafion probabilities play an important role in both computational linguistic applications (e.g. Carroll, Minnen, and Briscoe 1998, Charniak 1997, Collins 1996/1997, Joshi and Srinivas 1994, Kim, Srinivas, and Tmeswell 1997, Stolcke et al. 1997) and psycholinguisfic models of language processing (e.g. Boland 1997, Clifton et al. 1984, Ferreira & McClure 1997, Fodor 1978, Garnsey et al. 1997, Jurafsky 1996, MacDonald 1994, Mitchell & Holmes 1985, Tanenhaus et al. 1990, Trueswell et al. 1993). 28 While the subcategorization probabilities in a computational language model can be adjusted to match a particular corpus, cross corpus differences in such probabilities pose an important problem when using corpora for norming psychological experiments. If eac"
W00-0905,P97-1003,0,0.057481,"Missing"
W00-0905,C94-1024,0,0.0365075,"Missing"
W00-0905,J93-2004,0,0.0313158,"et Journal corpus caused certain verbs to appear more often in particular senses that had a strong effect on its subcategorization frequencies. Even after controlfing for the broad sense of the verb, we found subcategorization differences caused by the &quot;micro-differences&quot; in sense, including quite specific arguments to the verb. 1 Data Data for 64 verbs (shown in Table 1) was collected from three corpora; The British National Corpus (BNC) (http&apos;J/info.ox.ac.uk/bnc/index.html), the Penn Treehank parsed version of the Brown Corpus (Brown), and the Penn Treebank Wall Street Journal corpas (WSJ) (Marcus et al. 1993). The 64 verbs were chosen on the basis of the requirements of separate psychological experiments including having a single dominant sense, being easily imagable&quot; and participating in one of several subcategorization alternations. A random sample of 100 examples of each verb was selected from each of the three corpora. When the corpus contained less than 100 tokens of the verb, as was frequently the case in the Brown and WSJ corpora, the entire available data was used. This data was coded for several properties: Transitive/Intransitive&quot; Active/Passive&quot; and whether the example involved the majo"
W00-0905,P98-2184,1,\N,Missing
W00-0905,C98-2179,1,\N,Missing
W01-0513,J90-1003,0,0.276728,"Missing"
W01-0513,J93-1003,0,0.106426,"Missing"
W01-0513,P90-1034,0,0.111123,"Missing"
W01-0513,P97-1004,0,0.0232267,"Missing"
W01-0513,W97-0121,0,0.0541708,"Missing"
W01-0513,W00-0712,1,0.821657,"best algorithm (filtered, cross-sourced Zscore) and eliminate semantically compositional or modifiable MWU hypotheses. Deerwester, et al (1990) introduced Latent Semantic Analysis (LSA) as a computational technique for inducing semantic relationships between words and documents. It forms highdimensional vectors using word counts and uses singular value decomposition to project those vectors into an optimal k-dimensional, “semantic” subspace (see Landauer, et al, 1998). Following an approach from Schütze (1993), we showed how one could compute latent semantic vectors for any word in a corpus (Schone and Jurafsky, 2000). Using the same approach, we compute semantic vectors for every proposed word n-gram C=X1X2...Xn. Since LSA involves word counts, we can also compute semantic vectors (denoted by ) for C’s subcomponents. These can either include ({X i} n ) or exclude ({X i } n ) C’s i 1 i 1 counts. We seek to see if induced semantics can help eliminate incorrectly-chosen MWUs. As will be shown, the effort using semantics in this nature has a very small payoff for the expended cost. 5.1 Non-compositionality Non-compositionality is a key component of valid MWUs, so we may desire to emphasize n-grams that are se"
W01-0513,P97-1061,0,0.0127441,"Missing"
W01-0513,J93-1007,0,0.130064,"Missing"
W01-0513,J96-3004,0,0.0339705,"stream of symbols and segmentation as a means of placing delimiters in that stream so as to separate logical groupings of symbols from one another. A segmentation process may find that a symbol stream should not be delimited even though subcomponents of the stream have been seen elsewhere. In such cases, these larger units may be MWUs. The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et. al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others). Such efforts have employed various strategies for segmentation, including the use of hidden Markov models, minimum description length, dictionary-based approaches, probabilistic automata, transformation-based learning, and text compression. Some of these approaches require significant sources of human knowledge, though others, especially those that follow data compression or HMM schemes, do not. These approaches could be applied to languages where word delimiters exist (such as in European languages delimited by t"
W01-0513,J00-3004,0,0.0081836,"limiters in that stream so as to separate logical groupings of symbols from one another. A segmentation process may find that a symbol stream should not be delimited even though subcomponents of the stream have been seen elsewhere. In such cases, these larger units may be MWUs. The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et. al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others). Such efforts have employed various strategies for segmentation, including the use of hidden Markov models, minimum description length, dictionary-based approaches, probabilistic automata, transformation-based learning, and text compression. Some of these approaches require significant sources of human knowledge, though others, especially those that follow data compression or HMM schemes, do not. These approaches could be applied to languages where word delimiters exist (such as in European languages delimited by the space character). However, in such languages, it seems mor"
W04-2416,W04-2412,0,0.211539,"Missing"
W04-2416,W95-0107,0,0.0497133,"(i.e. base phrases) instead of words or the constituents derived from syntactic trees. This system is referred to as the phraseby-phrase (P-by-P) semantic role classifier. We participate in the “closed challenge” of the CoNLL-2004 shared task and report results on both development and test sets. A detailed description of the task, data and related work can be found in (Carreras and M`arquez, 2004). 2 System Description 2.1 Data Representation In this paper, we change the representation of the original data as follows: • Bracketed representation of roles is converted into IOB2 representation (Ramhsaw and Marcus, 1995; Sang and Veenstra, 1995) • Word tokens are collapsed into base phrase (BP) tokens. Since the semantic annotation in the PropBank corpus does not have any embedded structure there is no loss of information in the first change. However, this results in a simpler representation with a reduced set of tagging labels. In the second change, it is possible to miss some information in cases where the semantic chunks do not align with the sequence of BPs. However, in Section 3.2 we show that the loss in performance due to the misalignment is much less than the gain in performance that can be achieved"
W04-2416,E99-1023,0,0.098933,"Missing"
W04-2416,N04-1030,1,\N,Missing
W05-0634,W05-0620,0,0.305505,"Missing"
W05-0634,J02-3001,1,0.761681,"Missing"
W05-0634,N03-2009,1,0.749207,"tracted from the input. This approach is neutral to the particular set of labels used, and will learn to tag input according We have previously reported on using SVM classifiers for semantic role labeling. In this work, we formulate the semantic labeling problem as a multiclass classification problem using Support Vector Machine (SVM) classifiers. Some of these systems use features based on syntactic constituents produced by a Charniak parser (Pradhan et al., 2003; Pradhan et al., 2004) and others use only a flat syntactic representation produced by a syntactic chunker (Hacioglu et al., 2003; Hacioglu and Ward, 2003; Hacioglu, 2004; Hacioglu et al., 2004). The latter approach lacks the information provided by the hierarchical syntactic structure, and the former imposes a limitation that the possible candidate roles should be one of the nodes already present in the syntax tree. We found that, while the chunk based systems are very efficient and robust, the systems that use features based on full syntactic parses are generally more accurate. Analysis of the source of errors for the parse constituent based systems showed that incorrect parses were a major source of error. The syntactic parser did not produc"
W05-0634,W04-2416,1,0.861755,"neutral to the particular set of labels used, and will learn to tag input according We have previously reported on using SVM classifiers for semantic role labeling. In this work, we formulate the semantic labeling problem as a multiclass classification problem using Support Vector Machine (SVM) classifiers. Some of these systems use features based on syntactic constituents produced by a Charniak parser (Pradhan et al., 2003; Pradhan et al., 2004) and others use only a flat syntactic representation produced by a syntactic chunker (Hacioglu et al., 2003; Hacioglu and Ward, 2003; Hacioglu, 2004; Hacioglu et al., 2004). The latter approach lacks the information provided by the hierarchical syntactic structure, and the former imposes a limitation that the possible candidate roles should be one of the nodes already present in the syntax tree. We found that, while the chunk based systems are very efficient and robust, the systems that use features based on full syntactic parses are generally more accurate. Analysis of the source of errors for the parse constituent based systems showed that incorrect parses were a major source of error. The syntactic parser did not produce any constituent that corresponded to t"
W05-0634,kingsbury-palmer-2002-treebank,0,0.0855298,"Missing"
W05-0634,W00-0730,0,0.0185759,"erent syntactic views. Our goal is to preserve the robustness and flexibility of the segmentation of the phrase-based chunker, but to take advantage of features from full syntactic parses. We also want to combine features from different syntactic parses to gain additional robustness. To this end, we use features generated from a Charniak parser and a Collins parser, as supplied for the CoNLL-2005 closed task. 2 System Description We again formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifiers. TinySVM1 along with YamCha2 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The general framework is to train separate semantic role labeling systems for each of the parse tree views, and then to use the role arguments output by these systems as additional features in a semantic role classifier using a flat syntactic view. The constituent based classifiers walk a syntactic parse tree and classify each node as N ULL (no role) or as one of the set of"
W05-0634,N01-1025,0,0.0421455,"goal is to preserve the robustness and flexibility of the segmentation of the phrase-based chunker, but to take advantage of features from full syntactic parses. We also want to combine features from different syntactic parses to gain additional robustness. To this end, we use features generated from a Charniak parser and a Collins parser, as supplied for the CoNLL-2005 closed task. 2 System Description We again formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifiers. TinySVM1 along with YamCha2 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The general framework is to train separate semantic role labeling systems for each of the parse tree views, and then to use the role arguments output by these systems as additional features in a semantic role classifier using a flat syntactic view. The constituent based classifiers walk a syntactic parse tree and classify each node as N ULL (no role) or as one of the set of semantic roles. Chunk base"
W05-0634,N04-1030,1,0.950041,"e in that sentence. Our approach is to use supervised machine learning classifiers to produce the role labels based on features extracted from the input. This approach is neutral to the particular set of labels used, and will learn to tag input according We have previously reported on using SVM classifiers for semantic role labeling. In this work, we formulate the semantic labeling problem as a multiclass classification problem using Support Vector Machine (SVM) classifiers. Some of these systems use features based on syntactic constituents produced by a Charniak parser (Pradhan et al., 2003; Pradhan et al., 2004) and others use only a flat syntactic representation produced by a syntactic chunker (Hacioglu et al., 2003; Hacioglu and Ward, 2003; Hacioglu, 2004; Hacioglu et al., 2004). The latter approach lacks the information provided by the hierarchical syntactic structure, and the former imposes a limitation that the possible candidate roles should be one of the nodes already present in the syntax tree. We found that, while the chunk based systems are very efficient and robust, the systems that use features based on full syntactic parses are generally more accurate. Analysis of the source of errors fo"
W05-0634,P05-1072,1,0.75393,"y the hierarchical syntactic structure, and the former imposes a limitation that the possible candidate roles should be one of the nodes already present in the syntax tree. We found that, while the chunk based systems are very efficient and robust, the systems that use features based on full syntactic parses are generally more accurate. Analysis of the source of errors for the parse constituent based systems showed that incorrect parses were a major source of error. The syntactic parser did not produce any constituent that corresponded to the correct segmentation for the semantic argument. In Pradhan et al. (2005), we reported on a first attempt to overcome this problem by combining semantic role labels produced from different syntactic parses. The hope is that the syntactic parsers will make different errors, and that combining their outputs will improve on 217 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 217–220, Ann Arbor, June 2005. 2005 Association for Computational Linguistics either system alone. This initial attempt used features from a Charniak parser, a Minipar parser and a chunk based parser. It did show some improvement from the combination,"
W05-0634,W95-0107,0,0.0248543,"The general framework is to train separate semantic role labeling systems for each of the parse tree views, and then to use the role arguments output by these systems as additional features in a semantic role classifier using a flat syntactic view. The constituent based classifiers walk a syntactic parse tree and classify each node as N ULL (no role) or as one of the set of semantic roles. Chunk based systems classify each base phrase as being the B(eginning) of a semantic role, I(nside) a semantic role, or O(utside) any semantic role (ie. N ULL). This is referred to as an IOB representation (Ramshaw and Marcus, 1995). The constituent level roles are mapped to the IOB representation used by the chunker. The IOB tags are then used as features for a separate base-phase semantic role labeler (chunker), in addition to the standard set of features used by the chunker. An n-fold cross-validation paradigm is used to train the constituent based role classifiers 1 2 http://chasen.org/˜taku/software/TinySVM/ http://chasen.org/˜taku/software/yamcha/ 218 and the chunk based classifier. For the system reported here, two full syntactic parsers were used, a Charniak parser and a Collins parser. Features were extracted by"
W05-0634,P03-1002,0,0.0642219,"Missing"
W05-0634,W04-3212,0,0.189369,"Missing"
W09-0404,W05-0909,0,0.112089,"tion. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an MT evaluation metric that is inspired by the similarity between this task and the textual entailment task (Dagan et al., 2005), which 2 2.1 Textual Entailment for MT Evaluation Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between tw"
W09-0404,P03-1021,0,0.00708576,"del. We plan to assess the additional benefit of the full entailment feature set against the T RAD M T feature set extended by a proper lexical similarity metric, such as METEOR. The computation of entailment features is more heavyweight than traditional MT evaluation metrics. We found the speed (about 6 s per hypothesis on a current PC) to be sufficient for easily judging the quality of datasets of the size conventionally used for MT evaluation. However, this may still be too expensive as part of an MT model that directly optimizes some performance measure, e.g., minimum error rate training (Och, 2003). Feature Weights. Finally, we assessed the importance of the different entailment feature groups in the RTE model.1 Since the presence of correlated features makes the weights difficult to interpret, we restrict ourselves to two general observations. First, we find high weights not only for the score of the alignment between hypothesis and reference, but also for a number of syntacto-semantic match and mismatch features. This means that we do get an additional benefit from the presence of these features. For example, features with a negative effect include dropping adjuncts, unaligned root no"
W09-0404,W07-0411,0,0.0306635,"Missing"
W09-0404,E06-1032,0,0.0370947,"quality of machine translations have been a major enabler in improving the performance of MT systems, leading to many varied approaches to develop such metrics. Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (Papineni et al., 2002), NIST (Doddington, 2002). These measures rate systems hypotheses by measuring the overlap in surface word sequences shared between hypothesis and reference translation. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an M"
W09-0404,P02-1040,0,0.0809369,"combination of lexical and structural features that model the matches and mismatches between system output and reference translation. We use supervised regression models to combine these features and analyze feature weights to obtain further insights into the usefulness of different feature types. Introduction Automatic metrics to assess the quality of machine translations have been a major enabler in improving the performance of MT systems, leading to many varied approaches to develop such metrics. Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (Papineni et al., 2002), NIST (Doddington, 2002). These measures rate systems hypotheses by measuring the overlap in surface word sequences shared between hypothesis and reference translation. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement"
W09-0404,W08-0309,0,0.108416,"Missing"
W09-0404,2006.amta-papers.25,0,0.157306,"Missing"
W09-0404,P08-1007,0,0.0429762,"of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an MT evaluation metric that is inspired by the similarity between this task and the textual entailment task (Dagan et al., 2005), which 2 2.1 Textual Entailment for MT Evaluation Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences ("
W09-0404,P06-1057,0,0.0216052,"valuation Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if a human reading P would infer that H is most likely true. Information about the presence or absence of entailment between two sentences has been found to be beneficial for a range of NLP tasks such as Word Sense Disambiguation or Question Answering (Dagan et al., 2006; Harabagiu and Hickl, 2006). Our intuition is that this idea can also be fruitful in MT Evaluation, as illustrated in Figure 1. Very good MT output should entail the reference translation. In contrast, missing hypothesis material breaks forward entailment; additional material breaks backward entailment; and for bad translations, entailment fails in both directions. Work on the recognition of textual entailment (RTE) has consistently found that the integration of more syntactic and semantic knowledge can yield gains over ∗ This paper is based on work funded by the Defense Advanced Research Pro"
W09-0404,W08-0332,0,0.0303371,"Missing"
W09-0404,P06-1114,0,0.0608647,"tailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if a human reading P would infer that H is most likely true. Information about the presence or absence of entailment between two sentences has been found to be beneficial for a range of NLP tasks such as Word Sense Disambiguation or Question Answering (Dagan et al., 2006; Harabagiu and Hickl, 2006). Our intuition is that this idea can also be fruitful in MT Evaluation, as illustrated in Figure 1. Very good MT output should entail the reference translation. In contrast, missing hypothesis material breaks forward entailment; additional material breaks backward entailment; and for bad translations, entailment fails in both directions. Work on the recognition of textual entailment (RTE) has consistently found that the integration of more syntactic and semantic knowledge can yield gains over ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. Th"
W09-0404,C08-1066,1,0.823965,"re are also substantial differences between TE and MT evaluation. Crucially, TE assumes the premise and hypothesis to be well-formed sentences, which is not true in MT evaluation. Thus, a possible criticism to the use of TE methods is that the features could become unreliable for ill-formed MT output. However, there is a second difference between the tasks that works to our advantage. Due to its strict compositional nature, TE requires an accurate semantic analysis of all sentence parts, since, for example, one misanalysed negation or counterfactual embedding can invert the entailment status (MacCartney and Manning, 2008). In contrast, human MT judgments behave more additively: failure of a translation with respect to a single semantic dimension (e.g., polarity or tense) degrades its quality, but usually not crucially so. We therefore expect that even noisy entailment features can be predictive in MT evaluation. 2.2 Table 1: Entailment feature groups provided by the Stanford RTE system, with number of features quadratic in the number of systems. On the other hand, it can be trained on more reliable pairwise preference judgments. In a second step, we combine the individual decisions to compute the highest-likel"
W09-0404,N06-1006,1,\N,Missing
W09-0436,P05-1033,0,0.310081,"lation (MT) from Chinese to English has been a difficult problem: structural differences between Chinese and English, such as the different orderings of head nouns and relative clauses, cause BLEU scores to be consistently lower than for other difficult language pairs like Arabic-English. Many of these structural differences are related to the ubiquitous Chinese d(DE) construction, used for a wide range of noun modification constructions (both single word and clausal) and other uses. Part of the solution to dealing with these ordering issues is hierarchical decoding, such as the Hiero system (Chiang, 2005), a method motivated by d(DE) examples like the one in Figure 1. In this case, the translation goal is to rotate the noun head and the preceding relative clause around d(DE), so that we can translate to “[one of few countries] d [have diplomatic relations with North Korea]”. Hiero can learn this kind of lexicalized synchronous grammar rule. But use of hierarchical decoders has not solved the DE construction translation problem. We analyzed the errors of three state-of-the-art systems • For DNPs (consisting of“XP+DEG”): – Reorder if XP is PP or LCP; – Reorder if XP is a non-pronominal NP • For"
W09-0436,P05-1066,0,0.0926019,"Missing"
W09-0436,D08-1089,1,0.831613,"this example, both systems decide to reorder, but DE-Annotated had the extra information that this d is a drelc . In Figure 4 we can see that in WANG-NP, “d” is being translated as “for”, and the translation afterwards is not grammatically correct. On the other hand, the bottom of Figure 4 shows that with the DE-Annotated preprocessing, now “drelc ” is translated into “which was” and well connected with the later translation. This shows that disambiguating d helps in choosing a better English translation. conduct additional experiments with a hierarchical phrase reordering model introduced by Galley and Manning (2008). The hierarchical phrase reordering model can handle the key examples often used to motivated syntax-based systems; therefore we think it is valuable to see if the DE annotation can still improve on top of that. In Table 5, BASELINE+Hier gives consistent BLEU improvement over BASELINE. Using DE annotation on top of the hierarchical phrase reordering models (DE-Annotated+Hier) provides extra gain over BASELINE+Hier. This shows the DE annotation can help a hierarchical system. We think similar improvements are likely to occur with other hierarchical systems. 5 5.1 Analysis (IP (NP (NN ddd)) (VP"
W09-0436,N03-1017,0,0.00267307,"ng (WANG-NP) showed consistent improvement in Table 5 on all test sets, with BLEU point gains ranging from 0.15 to 0.40. This confirms that having reordering around DEs in NP helps Chinese-English MT. Table 4: The confusion matrix for 5-class DE classification This could be due to the fact that there are some cases where the translation is correct both ways, but also could be because the features we added have not captured the difference well enough. 4 Machine Translation Experiments 4.1 Baseline Experiments Experimental Setting 4.3 For our MT experiments, we used a reimplementation of Moses (Koehn et al., 2003), a state-of-the-art phrase-based system. The alignment is done by the Berkeley word aligner (Liang et al., 2006) and then we symmetrized the word alignment using the grow-diag heuristic. For features, we incorporate Moses’ standard eight features as well as the lexicalized reordering model. Parameter tuning is done with Minimum Error Rate Training (MERT) (Och, 2003). The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences. We evaluate the result with MT02 (878 sentences), MT03 (919 sentences), and MT05 (1082 sentences). Our MT training corpus contains 1,560,071 senten"
W09-0436,P03-1056,1,0.480711,"There are 12,259,997 words on the English side. Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al., 2008). After segmentation, there are 11,061,792 words on the Chinese side. We use a 5-gram language model trained on the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) and also the English side of all the LDC parallel data permissible under the NIST08 rules. Documents of Gigaword released during the epochs of MT02, MT03, MT05, and MT06 were removed. To run the DE classifier, we also need to parse the Chinese texts. We use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data and the tuning and test sets. Experiments with 5-class DE annotation We use the best setting of the DE classifier described in Section 3 to annotate DEs in NPs in the MT training data as well as the NIST tuning and test sets.10 If a DE is in an NP, we use the annotation of dAB , dAsB , dBprepA , drelc , or dAprepB to replace the original DE character. Once we have the DEs labeled, we preprocess the Chinese sentences by reordering them.11 Note that not all DEs in the Chinese data are in NPs, therefore not all DEs are annotated with the extra la"
W09-0436,N06-1014,0,0.00752865,"o 0.40. This confirms that having reordering around DEs in NP helps Chinese-English MT. Table 4: The confusion matrix for 5-class DE classification This could be due to the fact that there are some cases where the translation is correct both ways, but also could be because the features we added have not captured the difference well enough. 4 Machine Translation Experiments 4.1 Baseline Experiments Experimental Setting 4.3 For our MT experiments, we used a reimplementation of Moses (Koehn et al., 2003), a state-of-the-art phrase-based system. The alignment is done by the Berkeley word aligner (Liang et al., 2006) and then we symmetrized the word alignment using the grow-diag heuristic. For features, we incorporate Moses’ standard eight features as well as the lexicalized reordering model. Parameter tuning is done with Minimum Error Rate Training (MERT) (Och, 2003). The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences. We evaluate the result with MT02 (878 sentences), MT03 (919 sentences), and MT05 (1082 sentences). Our MT training corpus contains 1,560,071 sentence pairs from various parallel corpora from LDC.9 There are 12,259,997 words on the English side. Chinese word se"
W09-0436,W08-0336,1,0.812091,"e, both systems decide to reorder, but DE-Annotated had the extra information that this d is a drelc . In Figure 4 we can see that in WANG-NP, “d” is being translated as “for”, and the translation afterwards is not grammatically correct. On the other hand, the bottom of Figure 4 shows that with the DE-Annotated preprocessing, now “drelc ” is translated into “which was” and well connected with the later translation. This shows that disambiguating d helps in choosing a better English translation. conduct additional experiments with a hierarchical phrase reordering model introduced by Galley and Manning (2008). The hierarchical phrase reordering model can handle the key examples often used to motivated syntax-based systems; therefore we think it is valuable to see if the DE annotation can still improve on top of that. In Table 5, BASELINE+Hier gives consistent BLEU improvement over BASELINE. Using DE annotation on top of the hierarchical phrase reordering models (DE-Annotated+Hier) provides extra gain over BASELINE+Hier. This shows the DE annotation can help a hierarchical system. We think similar improvements are likely to occur with other hierarchical systems. 5 5.1 Analysis (IP (NP (NN ddd)) (VP"
W09-0436,W05-0908,0,0.0133154,"+1.00) 33.75(+1.24) 33.63(+0.88) 32.96 33.10 32.93 33.96(+1.00) 34.33(+1.23) 33.88(+0.95) Translation Error Rate (TER) MT06(tune) MT02 MT03 61.10 63.11 62.09 59.78(−1.32) 62.58(−0.53) 61.36(−0.73) 58.21(−2.89) 61.17(−1.94) 60.27(−1.82) MT05 31.42 31.68(+0.26) 32.91(+1.49) 32.23 33.01(+0.77) MT05 64.06 62.35(−1.71) 60.78(−3.28) Table 5: MT experiments of different settings on various NIST MT evaluation datasets. We used both the BLEU and TER metrics for evaluation. All differences between DE-Annotated and BASELINE are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005) shows an example that contains a DE construction that translates into a relative clause in English.12 The automatic parse tree of the sentence is listed in Figure 3. The reordered sentences of WANG-NP and DE-Annotated appear on the top and bottom in Figure 4. For this example, both systems decide to reorder, but DE-Annotated had the extra information that this d is a drelc . In Figure 4 we can see that in WANG-NP, “d” is being translated as “for”, and the translation afterwards is not grammatically correct. On the other hand, the bottom of Figure 4 shows that with the DE-Annotated preprocessi"
W09-0436,I05-3005,1,0.751428,"Missing"
W09-0436,D07-1077,0,0.651725,"m 2: ‘the local a bad reputation secondary school’ Team 3: ‘a local stigma secondary schools’ None of the teams reordered “bad reputation” and “middle school” around the d. We argue that this is because it is not sufficient to have a formalism which supports phrasal reordering, but it is also necessary to have sufficient linguistic modeling that the system knows when and how much to rearrange. An alternative way of dealing with structural differences is to reorder source language sentences to minimize structural divergence with the target language, (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007). For example Wang et al. (2007) introduced a set of rules to decide if a d(DE) construction should be reordered or not before translating to English: Introduction Machine translation (MT) from Chinese to English has been a difficult problem: structural differences between Chinese and English, such as the different orderings of head nouns and relative clauses, cause BLEU scores to be consistently lower than for other difficult language pairs like Arabic-English. Many of these structural differences are related to the ubiquitous Chinese d(DE) construction, used for a wide range of noun modifica"
W09-0436,C04-1073,0,0.0387517,"Missing"
W09-0436,D08-1076,0,\N,Missing
W10-2902,W95-0102,0,0.564583,"Missing"
W10-2902,A94-1009,0,0.400994,"Missing"
W10-2902,N09-1012,0,0.628626,"Missing"
W10-2902,P04-1061,1,0.926389,"R, T)) (1 − PSTOP (IN, R, T)) PSTOP (VBD, L, F) PSTOP (NNS, L, T) PSTOP (IN, L, T) PSTOP (NN, L, T) PSTOP (⋄, L, F) {z } | POS Tokens 110,760 136,310 163,715 336,555 540,895 730,099 860,053 942,801 986,830 1,028,054 48,201 391,796 PATTACH (⋄, L, VBD) PATTACH (VBD, L, NNS) PATTACH (VBD, R, IN) PATTACH (IN, R, NN) PSTOP (VBD, R, F) PSTOP (NNS, R, T) PSTOP (IN, R, F) PSTOP (NN, R, T) PSTOP (⋄, R, T) . | {z } 3 Standard Data Sets and Evaluation The DMV is traditionally trained and tested on customized subsets of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Following Klein and Manning (2004), we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty sub-trees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic “head-percolation” rules (Collins, 1999) to convert the rest, as is standard practice. We experiment with WSJk (sentences with at most k tokens), for 1 ≤ k ≤ 45, and Section 23 of WSJ∞ (all sentence lengths). We also evaluate on Brown100, similarly derived from the parsed portion of"
W10-2902,P08-1100,0,0.0570334,"Missing"
W10-2902,J93-2004,0,0.0375615,"OP (VBD, L, T)) (1 − PSTOP (VBD, R, T)) (1 − PSTOP (IN, R, T)) PSTOP (VBD, L, F) PSTOP (NNS, L, T) PSTOP (IN, L, T) PSTOP (NN, L, T) PSTOP (⋄, L, F) {z } | POS Tokens 110,760 136,310 163,715 336,555 540,895 730,099 860,053 942,801 986,830 1,028,054 48,201 391,796 PATTACH (⋄, L, VBD) PATTACH (VBD, L, NNS) PATTACH (VBD, R, IN) PATTACH (IN, R, NN) PSTOP (VBD, R, F) PSTOP (NNS, R, T) PSTOP (IN, R, F) PSTOP (NN, R, T) PSTOP (⋄, R, T) . | {z } 3 Standard Data Sets and Evaluation The DMV is traditionally trained and tested on customized subsets of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Following Klein and Manning (2004), we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty sub-trees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic “head-percolation” rules (Collins, 1999) to convert the rest, as is standard practice. We experiment with WSJk (sentences with at most k tokens), for 1 ≤ k ≤ 45, and Section 23 of WSJ∞ (all sentence lengths). We also evaluate on Brown100, similarly"
W10-2902,N06-1020,0,0.0330881,"performance guarantees than exact training methods. If our task were not parsing but language modeling, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM would not be doing as badly, compared to Viterbi. Viterbi training is not only faster and more accurate but also free of inside-outside’s recursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featurerich approaches that target conditional likelihoods, essentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such “learning by doing” approaches may be relevant to understanding human language acquisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human probabilistic parsing are massively pruned (Jurafsky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM — or the very limited parallelism of k-best Viterbi — may be more appropriate in modeling this task than the fully-integrated inside-outside solution. ` 16 ´ 1 3 1 5 = = 0.213 2"
W10-2902,P06-1043,0,0.0163891,"performance guarantees than exact training methods. If our task were not parsing but language modeling, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM would not be doing as badly, compared to Viterbi. Viterbi training is not only faster and more accurate but also free of inside-outside’s recursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featurerich approaches that target conditional likelihoods, essentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such “learning by doing” approaches may be relevant to understanding human language acquisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human probabilistic parsing are massively pruned (Jurafsky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM — or the very limited parallelism of k-best Viterbi — may be more appropriate in modeling this task than the fully-integrated inside-outside solution. ` 16 ´ 1 3 1 5 = = 0.213 2"
W10-2902,J93-2003,0,0.0156613,"ns: (i) initial direction dir ∈ {L, R} in which to attach children, via probability PORDER (ch ); (ii) whether to seal dir, stopping with probability PSTOP (ch , dir, adj), conditioned on adj ∈ {T, F} (true iff considering dir ’s first, i.e., adjacent, child); and (iii) attachments (of class ca ), according to PATTACH (ch , dir, ca ). This produces only projective trees. A root token ♦ generates the head of a sentence as its left (and only) child. Figure 2 displays a simple example. The DMV lends itself to unsupervised learning via inside-outside re-estimation (Baker, 1979). Viterbi training (Brown et al., 1993) re-estimates each next model as if supervised by the previous best parse trees. And supervised learning from 1 The expected number of trials needed to get one Bernoulli(p) success is n ∼ Geometric(p), with n ∈ Z+ , P(n) = (1 − p)n−1 p and E(n) = p−1 ; MoM and MLE agree, pˆ = (# of successes)/(# of trials). 10 60 50 Oracle Ad-Hoc∗ 40 30 20 Uninformed 10 5 10 15 20 25 30 35 40 Ad-Hoc∗ 150 Uninformed 100 50 10 15 20 25 30 35 40 Uninformed 40 30 20 10 5 10 15 20 25 30 35 40 WSJk (training on all WSJ sentences up to k tokens in length) (d) Iterations for Viterbi (Hard EM) 200 150 Ad-Hoc∗ 100 50 (c"
W10-2902,J94-2001,0,0.790451,"Missing"
W10-2902,N03-1023,0,0.0441837,"del gives even better performance guarantees than exact training methods. If our task were not parsing but language modeling, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM would not be doing as badly, compared to Viterbi. Viterbi training is not only faster and more accurate but also free of inside-outside’s recursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featurerich approaches that target conditional likelihoods, essentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such “learning by doing” approaches may be relevant to understanding human language acquisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human probabilistic parsing are massively pruned (Jurafsky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM — or the very limited parallelism of k-best Viterbi — may be more appropriate in modeling this task than the fully-integrated inside-outside solution. ` 1"
W10-2902,P92-1017,0,0.684899,"Missing"
W10-2902,W03-0407,0,0.0169857,"thod to train the model gives even better performance guarantees than exact training methods. If our task were not parsing but language modeling, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM would not be doing as badly, compared to Viterbi. Viterbi training is not only faster and more accurate but also free of inside-outside’s recursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featurerich approaches that target conditional likelihoods, essentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such “learning by doing” approaches may be relevant to understanding human language acquisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human probabilistic parsing are massively pruned (Jurafsky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM — or the very limited parallelism of k-best Viterbi — may be more appropriate in modeling this task than the fully-integrated inside-"
W10-2902,P10-1130,1,0.698579,"nships beyond local trees, e.g., agreement. Spitkovsky et al. (2009) suggest that classic EM breaks down as sentences get longer precisely because the model makes unwarranted independence assumptions. They hypothesize that the DMV reserves too much probability mass for what should be unlikely productions. Since EM faithfully allocates such re-distributions across the possible parse trees, once sentences grow sufficiently long, this process begins to deplete what began as likelier structures. But medium lengths avoid a flood of exponentially-confusing longer sentences (and 5 In a sister paper, Spitkovsky et al. (2010) improve performance by incorporating parsing constraints harvested from the web into Viterbi training; nevertheless, results presented in this paper remain the best of models trained purely on WSJ. 6 Klein and Manning (2004) originally trained the DMV on WSJ10 and Gillenwater et al. (2009) found it useful to discard data from WSJ3, which is mostly incomplete sentences. 13 7 Proofs (by Construction) This objective function is not convex and in general does not have a unique peak, so in practice one usually settles for θ˜UNS — a fixed point. There is no reason why θˆSUP should agree with θˆUNS"
W10-2902,N09-1009,0,0.394501,"Missing"
W10-2902,P10-1152,0,0.138496,"Missing"
W10-2902,J03-4003,0,\N,Missing
W11-0303,N10-1083,0,0.0396422,"Missing"
W11-0303,D10-1117,0,0.473444,"tin@cs.stanford.edu Hiyan Alshawi Google Inc. Mountain View, CA, 94043, USA hiyan@google.com Daniel Jurafsky Departments of Linguistics and Computer Science Stanford University, Stanford, CA, 94305, USA jurafsky@stanford.edu Abstract as utterances transcribed by speech recognizers, are often difficult even for humans (Kim and Woodland, 2002). Therefore, one would expect grammar inducers to exploit any available linguistic meta-data. And yet in unsupervised dependency parsing, sentenceinternal punctuation has long been ignored (Carroll and Charniak, 1992; Paskin, 2001; Klein and Manning, 2004; Blunsom and Cohn, 2010, inter alia). HTML is another kind of meta-data that is ordinarily stripped out in pre-processing. However, recently Spitkovsky et al. (2010b) demonstrated that web markup can successfully guide hierarchical syntactic structure discovery, observing, for example, that anchors often match linguistic constituents: We show how punctuation can be used to improve unsupervised dependency parsing. Our linguistic analysis confirms the strong connection between English punctuation and phrase boundaries in the Penn Treebank. However, approaches that naively include punctuation marks in the grammar (as i"
W11-0303,W06-2920,0,0.0362853,"Missing"
W11-0303,P07-1036,0,0.00549498,"odels and algorithms, combined with common-sense constraints. They reinforce insights from joint modeling in supervised learning, where simplified, independent models, Viterbi decoding and expressive constraints excel at sequence labeling tasks (Roth and Yih, 2005). Such evidence is particularly welcome in unsupervised settings (Punyakanok et al., 2005), where it is crucial that systems scale gracefully to volumes of data, on top of the usual desiderata — ease of implementation, extension, understanding and debugging. Future work could explore softening constraints (Hayes and Mouradian, 1980; Chang et al., 2007), perhaps using features (Eisner and Smith, 2005; Berg-Kirkpatrick et al., 2010) or by learning to associate different settings with various marks: Simply adding a hidden tag for “ordinary” versus “divide” types of punctuation (Li et al., 2005) may already usefully extend our model. Acknowledgments Partially funded by the Air Force Research Laboratory (AFRL), under prime contract no. FA8750-09-C-0181, and by NSF, via award #IIS-0811974. We thank Omri Abend, Slav Petrov and anonymous reviewers for many helpful suggestions, and we are especially grateful to Jenny R. Finkel for shaming us into us"
W11-0303,A00-2018,0,0.151064,"th, we used punctuation-insensitive scoring (§3.2). 25 We did not detect synergy between the two improvements. However, note that without constrained training, “full” data sets do not help, on average, despite having more data and lexicalization. Furthermore, after constrained training, we detected no evidence of benefits to additional retraining: not with the relaxed sprawl constraint, nor unconstrained. 9 Related Work Punctuation has been used to improve parsing since rule-based systems (Jones, 1994). Statistical parsers reap dramatic gains from punctuation (Engel et al., 2002; Roark, 2001; Charniak, 2000; Johnson, 1998; Collins, 1997, inter alia). And it is even known to help in unsupervised constituent parsing (Seginer, 2007). But for dependency grammar induction, until now, punctuation remained unexploited. Parsing Techniques Most-Similar to Constraints A “divide-and-rule” strategy that relies on punctuation has been used in supervised constituent parsing of long Chinese sentences (Li et al., 2005). For English, there has been interest in balanced punctuation (Briscoe, 1994), more recently using rule-based filters (White and Rajkumar, 2008) in a combinatory categorial grammar (CCG). Our foc"
W11-0303,I05-1073,0,0.0258738,"n NLP Punctuation is hard to predict,9 partly because it can signal long-range dependences (Lu and Ng, 2010). It often provides valuable cues to NLP tasks such as part-of-speech tagging and named-entity recognition (Hillard et al., 2006), information extraction (Favre et al., 2008) and machine translation (Lee et al., 2006; Matusov et al., 2006). Other applications have included Japanese sentence analysis (Ohyama et al., 1986), genre detection (Stamatatos et al., 2000), bilingual sentence alignment (Yeh, 2003), semantic role labeling (Pradhan et al., 2005), Chinese creation-title recognition (Chen and Chen, 2005) and word segmentation (Li and Sun, 2009), plus, recently, automatic vandalism de9 Punctuation has high semantic entropy (Melamed, 1997); for an analysis of the many roles played in the WSJ by the comma — the most frequent and unpredictable punctuation mark in that data set — see Beeferman et al. (1998, Table 2). 26 tection in Wikipedia (Wang and McKeown, 2010). 10 Conclusions and Future Work Punctuation improves dependency grammar induction. Many unsupervised (and supervised) parsers could be easily modified to use sprawl-constrained decoding in inference. It applies to pre-trained models and"
W11-0303,W00-0717,0,0.0478241,"Missing"
W11-0303,P97-1003,0,0.0832933,"tive scoring (§3.2). 25 We did not detect synergy between the two improvements. However, note that without constrained training, “full” data sets do not help, on average, despite having more data and lexicalization. Furthermore, after constrained training, we detected no evidence of benefits to additional retraining: not with the relaxed sprawl constraint, nor unconstrained. 9 Related Work Punctuation has been used to improve parsing since rule-based systems (Jones, 1994). Statistical parsers reap dramatic gains from punctuation (Engel et al., 2002; Roark, 2001; Charniak, 2000; Johnson, 1998; Collins, 1997, inter alia). And it is even known to help in unsupervised constituent parsing (Seginer, 2007). But for dependency grammar induction, until now, punctuation remained unexploited. Parsing Techniques Most-Similar to Constraints A “divide-and-rule” strategy that relies on punctuation has been used in supervised constituent parsing of long Chinese sentences (Li et al., 2005). For English, there has been interest in balanced punctuation (Briscoe, 1994), more recently using rule-based filters (White and Rajkumar, 2008) in a combinatory categorial grammar (CCG). Our focus is specifically on unsuperv"
W11-0303,P99-1059,0,0.0658582,"Missing"
W11-0303,W05-1504,0,0.00755169,"nse constraints. They reinforce insights from joint modeling in supervised learning, where simplified, independent models, Viterbi decoding and expressive constraints excel at sequence labeling tasks (Roth and Yih, 2005). Such evidence is particularly welcome in unsupervised settings (Punyakanok et al., 2005), where it is crucial that systems scale gracefully to volumes of data, on top of the usual desiderata — ease of implementation, extension, understanding and debugging. Future work could explore softening constraints (Hayes and Mouradian, 1980; Chang et al., 2007), perhaps using features (Eisner and Smith, 2005; Berg-Kirkpatrick et al., 2010) or by learning to associate different settings with various marks: Simply adding a hidden tag for “ordinary” versus “divide” types of punctuation (Li et al., 2005) may already usefully extend our model. Acknowledgments Partially funded by the Air Force Research Laboratory (AFRL), under prime contract no. FA8750-09-C-0181, and by NSF, via award #IIS-0811974. We thank Omri Abend, Slav Petrov and anonymous reviewers for many helpful suggestions, and we are especially grateful to Jenny R. Finkel for shaming us into using punctuation, to Christopher D. Manning for r"
W11-0303,W02-1007,0,0.0308013,"s of the source treebanks. For both, we used punctuation-insensitive scoring (§3.2). 25 We did not detect synergy between the two improvements. However, note that without constrained training, “full” data sets do not help, on average, despite having more data and lexicalization. Furthermore, after constrained training, we detected no evidence of benefits to additional retraining: not with the relaxed sprawl constraint, nor unconstrained. 9 Related Work Punctuation has been used to improve parsing since rule-based systems (Jones, 1994). Statistical parsers reap dramatic gains from punctuation (Engel et al., 2002; Roark, 2001; Charniak, 2000; Johnson, 1998; Collins, 1997, inter alia). And it is even known to help in unsupervised constituent parsing (Seginer, 2007). But for dependency grammar induction, until now, punctuation remained unexploited. Parsing Techniques Most-Similar to Constraints A “divide-and-rule” strategy that relies on punctuation has been used in supervised constituent parsing of long Chinese sentences (Li et al., 2005). For English, there has been interest in balanced punctuation (Briscoe, 1994), more recently using rule-based filters (White and Rajkumar, 2008) in a combinatory cate"
W11-0303,N09-1037,0,0.00927134,"Missing"
W11-0303,P80-1026,0,0.185124,"score the power of simple models and algorithms, combined with common-sense constraints. They reinforce insights from joint modeling in supervised learning, where simplified, independent models, Viterbi decoding and expressive constraints excel at sequence labeling tasks (Roth and Yih, 2005). Such evidence is particularly welcome in unsupervised settings (Punyakanok et al., 2005), where it is crucial that systems scale gracefully to volumes of data, on top of the usual desiderata — ease of implementation, extension, understanding and debugging. Future work could explore softening constraints (Hayes and Mouradian, 1980; Chang et al., 2007), perhaps using features (Eisner and Smith, 2005; Berg-Kirkpatrick et al., 2010) or by learning to associate different settings with various marks: Simply adding a hidden tag for “ordinary” versus “divide” types of punctuation (Li et al., 2005) may already usefully extend our model. Acknowledgments Partially funded by the Air Force Research Laboratory (AFRL), under prime contract no. FA8750-09-C-0181, and by NSF, via award #IIS-0811974. We thank Omri Abend, Slav Petrov and anonymous reviewers for many helpful suggestions, and we are especially grateful to Jenny R. Finkel f"
W11-0303,C08-1042,0,0.137877,"Missing"
W11-0303,N09-1012,0,0.562062,"Missing"
W11-0303,J98-4004,0,0.0425444,"tuation-insensitive scoring (§3.2). 25 We did not detect synergy between the two improvements. However, note that without constrained training, “full” data sets do not help, on average, despite having more data and lexicalization. Furthermore, after constrained training, we detected no evidence of benefits to additional retraining: not with the relaxed sprawl constraint, nor unconstrained. 9 Related Work Punctuation has been used to improve parsing since rule-based systems (Jones, 1994). Statistical parsers reap dramatic gains from punctuation (Engel et al., 2002; Roark, 2001; Charniak, 2000; Johnson, 1998; Collins, 1997, inter alia). And it is even known to help in unsupervised constituent parsing (Seginer, 2007). But for dependency grammar induction, until now, punctuation remained unexploited. Parsing Techniques Most-Similar to Constraints A “divide-and-rule” strategy that relies on punctuation has been used in supervised constituent parsing of long Chinese sentences (Li et al., 2005). For English, there has been interest in balanced punctuation (Briscoe, 1994), more recently using rule-based filters (White and Rajkumar, 2008) in a combinatory categorial grammar (CCG). Our focus is specifica"
W11-0303,C94-1069,0,0.294932,"e leaves of the dependency trees; in ’07, it matched original annotations of the source treebanks. For both, we used punctuation-insensitive scoring (§3.2). 25 We did not detect synergy between the two improvements. However, note that without constrained training, “full” data sets do not help, on average, despite having more data and lexicalization. Furthermore, after constrained training, we detected no evidence of benefits to additional retraining: not with the relaxed sprawl constraint, nor unconstrained. 9 Related Work Punctuation has been used to improve parsing since rule-based systems (Jones, 1994). Statistical parsers reap dramatic gains from punctuation (Engel et al., 2002; Roark, 2001; Charniak, 2000; Johnson, 1998; Collins, 1997, inter alia). And it is even known to help in unsupervised constituent parsing (Seginer, 2007). But for dependency grammar induction, until now, punctuation remained unexploited. Parsing Techniques Most-Similar to Constraints A “divide-and-rule” strategy that relies on punctuation has been used in supervised constituent parsing of long Chinese sentences (Li et al., 2005). For English, there has been interest in balanced punctuation (Briscoe, 1994), more rece"
W11-0303,H05-1030,0,0.0483028,"data set and language. Tightly interwoven into the fabric of writing systems, punctuation frames most unannotated plaintext. We showed that rules for converting markup into accurate parsing constraints are still optimal for inter-punctuation fragments. Punctuation marks are more ubiquitous and natural than web markup: what little punctuation-induced constraints lack in precision, they more than make up in recall — perhaps both types of constraints would work better yet in tandem. For language acquisition, a natural question is whether prosody could similarly aid grammar induction from speech (Kahn et al., 2005). Our results underscore the power of simple models and algorithms, combined with common-sense constraints. They reinforce insights from joint modeling in supervised learning, where simplified, independent models, Viterbi decoding and expressive constraints excel at sequence labeling tasks (Roth and Yih, 2005). Such evidence is particularly welcome in unsupervised settings (Punyakanok et al., 2005), where it is crucial that systems scale gracefully to volumes of data, on top of the usual desiderata — ease of implementation, extension, understanding and debugging. Future work could explore soft"
W11-0303,P04-1061,0,0.857991,"ity and Google Inc. valentin@cs.stanford.edu Hiyan Alshawi Google Inc. Mountain View, CA, 94043, USA hiyan@google.com Daniel Jurafsky Departments of Linguistics and Computer Science Stanford University, Stanford, CA, 94305, USA jurafsky@stanford.edu Abstract as utterances transcribed by speech recognizers, are often difficult even for humans (Kim and Woodland, 2002). Therefore, one would expect grammar inducers to exploit any available linguistic meta-data. And yet in unsupervised dependency parsing, sentenceinternal punctuation has long been ignored (Carroll and Charniak, 1992; Paskin, 2001; Klein and Manning, 2004; Blunsom and Cohn, 2010, inter alia). HTML is another kind of meta-data that is ordinarily stripped out in pre-processing. However, recently Spitkovsky et al. (2010b) demonstrated that web markup can successfully guide hierarchical syntactic structure discovery, observing, for example, that anchors often match linguistic constituents: We show how punctuation can be used to improve unsupervised dependency parsing. Our linguistic analysis confirms the strong connection between English punctuation and phrase boundaries in the Penn Treebank. However, approaches that naively include punctuation ma"
W11-0303,J09-4006,0,0.0202422,"y because it can signal long-range dependences (Lu and Ng, 2010). It often provides valuable cues to NLP tasks such as part-of-speech tagging and named-entity recognition (Hillard et al., 2006), information extraction (Favre et al., 2008) and machine translation (Lee et al., 2006; Matusov et al., 2006). Other applications have included Japanese sentence analysis (Ohyama et al., 1986), genre detection (Stamatatos et al., 2000), bilingual sentence alignment (Yeh, 2003), semantic role labeling (Pradhan et al., 2005), Chinese creation-title recognition (Chen and Chen, 2005) and word segmentation (Li and Sun, 2009), plus, recently, automatic vandalism de9 Punctuation has high semantic entropy (Melamed, 1997); for an analysis of the many roles played in the WSJ by the comma — the most frequent and unpredictable punctuation mark in that data set — see Beeferman et al. (1998, Table 2). 26 tection in Wikipedia (Wang and McKeown, 2010). 10 Conclusions and Future Work Punctuation improves dependency grammar induction. Many unsupervised (and supervised) parsers could be easily modified to use sprawl-constrained decoding in inference. It applies to pre-trained models and, so far, helped every data set and langu"
W11-0303,I05-2002,0,0.0697345,"ained. 9 Related Work Punctuation has been used to improve parsing since rule-based systems (Jones, 1994). Statistical parsers reap dramatic gains from punctuation (Engel et al., 2002; Roark, 2001; Charniak, 2000; Johnson, 1998; Collins, 1997, inter alia). And it is even known to help in unsupervised constituent parsing (Seginer, 2007). But for dependency grammar induction, until now, punctuation remained unexploited. Parsing Techniques Most-Similar to Constraints A “divide-and-rule” strategy that relies on punctuation has been used in supervised constituent parsing of long Chinese sentences (Li et al., 2005). For English, there has been interest in balanced punctuation (Briscoe, 1994), more recently using rule-based filters (White and Rajkumar, 2008) in a combinatory categorial grammar (CCG). Our focus is specifically on unsupervised learning of dependency grammars and is similar, in spirit, to Eisner and Smith’s (2005) “vine grammar” formalism. An important difference is that instead of imposing static limits on allowed dependency lengths, our restrictions are dynamic — they disallow some long (and some short) arcs that would have otherwise crossed nearby punctuation. Incorporating partial brack"
W11-0303,D10-1018,0,0.024347,"ndency fragments. To help prevent overfitting, a non-parametric Bayesian prior, defined by a hierarchical Pitman-Yor process (Pitman and Yor, 1997), is trusted to nudge training towards fewer and smaller grammatical productions. We pursued a complementary strategy: using Klein and Manning’s (2004) much simpler Dependency Model with Valence (DMV), but persistently steering training away from certain constructions, as guided by punctuation, to help prevent underfitting. Various Other Uses of Punctuation in NLP Punctuation is hard to predict,9 partly because it can signal long-range dependences (Lu and Ng, 2010). It often provides valuable cues to NLP tasks such as part-of-speech tagging and named-entity recognition (Hillard et al., 2006), information extraction (Favre et al., 2008) and machine translation (Lee et al., 2006; Matusov et al., 2006). Other applications have included Japanese sentence analysis (Ohyama et al., 1986), genre detection (Stamatatos et al., 2000), bilingual sentence alignment (Yeh, 2003), semantic role labeling (Pradhan et al., 2005), Chinese creation-title recognition (Chen and Chen, 2005) and word segmentation (Li and Sun, 2009), plus, recently, automatic vandalism de9 Punct"
W11-0303,J93-2004,0,0.0409812,"ces ordinary Viterbi parse trees, at every iteration of EM, with the output of a constrained decoder. In all experiments other than #2 (§5) we train with the loose constraint. Spitkovsky et al. (2010b) found this setting to be best for markup-induced constraints. We apply it to constraints induced by inter-punctuation fragments. Constrained Inference Spitkovsky et al. (2010b) recommended using the sprawl constraint in inference. Once again, we follow their advice in all experiments except #2 (§5). 3.2 Data Sets and Scoring We trained on the Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). To evaluate, we automatically converted its labeled constituents into unlabeled dependencies, using deterministic “head-percolation” rules (Collins, 1999), discarding punctuation, any empty nodes, etc., as is standard practice (Paskin, 2001; Klein and Manning, 2004). We also evaluated against the parsed portion of the Brown corpus (Francis and Kuˇcera, 1979), used as a blind, out-of-domain evaluation set,3 similarly derived from labeled constituent parse trees. We report directed accuracies — fractions of correctly guessed arcs, including the root, in unlabeled reference dependency parse tre"
W11-0303,2006.iwslt-papers.1,0,0.0221546,"rsued a complementary strategy: using Klein and Manning’s (2004) much simpler Dependency Model with Valence (DMV), but persistently steering training away from certain constructions, as guided by punctuation, to help prevent underfitting. Various Other Uses of Punctuation in NLP Punctuation is hard to predict,9 partly because it can signal long-range dependences (Lu and Ng, 2010). It often provides valuable cues to NLP tasks such as part-of-speech tagging and named-entity recognition (Hillard et al., 2006), information extraction (Favre et al., 2008) and machine translation (Lee et al., 2006; Matusov et al., 2006). Other applications have included Japanese sentence analysis (Ohyama et al., 1986), genre detection (Stamatatos et al., 2000), bilingual sentence alignment (Yeh, 2003), semantic role labeling (Pradhan et al., 2005), Chinese creation-title recognition (Chen and Chen, 2005) and word segmentation (Li and Sun, 2009), plus, recently, automatic vandalism de9 Punctuation has high semantic entropy (Melamed, 1997); for an analysis of the many roles played in the WSJ by the comma — the most frequent and unpredictable punctuation mark in that data set — see Beeferman et al. (1998, Table 2). 26 tection i"
W11-0303,W97-0207,0,0.143853,"to NLP tasks such as part-of-speech tagging and named-entity recognition (Hillard et al., 2006), information extraction (Favre et al., 2008) and machine translation (Lee et al., 2006; Matusov et al., 2006). Other applications have included Japanese sentence analysis (Ohyama et al., 1986), genre detection (Stamatatos et al., 2000), bilingual sentence alignment (Yeh, 2003), semantic role labeling (Pradhan et al., 2005), Chinese creation-title recognition (Chen and Chen, 2005) and word segmentation (Li and Sun, 2009), plus, recently, automatic vandalism de9 Punctuation has high semantic entropy (Melamed, 1997); for an analysis of the many roles played in the WSJ by the comma — the most frequent and unpredictable punctuation mark in that data set — see Beeferman et al. (1998, Table 2). 26 tection in Wikipedia (Wang and McKeown, 2010). 10 Conclusions and Future Work Punctuation improves dependency grammar induction. Many unsupervised (and supervised) parsers could be easily modified to use sprawl-constrained decoding in inference. It applies to pre-trained models and, so far, helped every data set and language. Tightly interwoven into the fabric of writing systems, punctuation frames most unannotated"
W11-0303,P86-1024,0,0.0248614,"ency Model with Valence (DMV), but persistently steering training away from certain constructions, as guided by punctuation, to help prevent underfitting. Various Other Uses of Punctuation in NLP Punctuation is hard to predict,9 partly because it can signal long-range dependences (Lu and Ng, 2010). It often provides valuable cues to NLP tasks such as part-of-speech tagging and named-entity recognition (Hillard et al., 2006), information extraction (Favre et al., 2008) and machine translation (Lee et al., 2006; Matusov et al., 2006). Other applications have included Japanese sentence analysis (Ohyama et al., 1986), genre detection (Stamatatos et al., 2000), bilingual sentence alignment (Yeh, 2003), semantic role labeling (Pradhan et al., 2005), Chinese creation-title recognition (Chen and Chen, 2005) and word segmentation (Li and Sun, 2009), plus, recently, automatic vandalism de9 Punctuation has high semantic entropy (Melamed, 1997); for an analysis of the many roles played in the WSJ by the comma — the most frequent and unpredictable punctuation mark in that data set — see Beeferman et al. (1998, Table 2). 26 tection in Wikipedia (Wang and McKeown, 2010). 10 Conclusions and Future Work Punctuation im"
W11-0303,P92-1017,0,0.707802,"ation (Briscoe, 1994), more recently using rule-based filters (White and Rajkumar, 2008) in a combinatory categorial grammar (CCG). Our focus is specifically on unsupervised learning of dependency grammars and is similar, in spirit, to Eisner and Smith’s (2005) “vine grammar” formalism. An important difference is that instead of imposing static limits on allowed dependency lengths, our restrictions are dynamic — they disallow some long (and some short) arcs that would have otherwise crossed nearby punctuation. Incorporating partial bracketings into grammar induction is an idea tracing back to Pereira and Schabes (1992). It inspired Spitkovsky et al. (2010b) to mine parsing constraints from the web. In that same vein, we prospected a more abundant and natural language-resource — punctuation, using constraintbased techniques they developed for web markup. Modern Unsupervised Dependency Parsing State-of-the-art in unsupervised dependency parsing (Blunsom and Cohn, 2010) uses tree substitution grammars. These are powerful models, capable of learning large dependency fragments. To help prevent overfitting, a non-parametric Bayesian prior, defined by a hierarchical Pitman-Yor process (Pitman and Yor, 1997), is tr"
W11-0303,W05-0634,1,0.254333,"p prevent underfitting. Various Other Uses of Punctuation in NLP Punctuation is hard to predict,9 partly because it can signal long-range dependences (Lu and Ng, 2010). It often provides valuable cues to NLP tasks such as part-of-speech tagging and named-entity recognition (Hillard et al., 2006), information extraction (Favre et al., 2008) and machine translation (Lee et al., 2006; Matusov et al., 2006). Other applications have included Japanese sentence analysis (Ohyama et al., 1986), genre detection (Stamatatos et al., 2000), bilingual sentence alignment (Yeh, 2003), semantic role labeling (Pradhan et al., 2005), Chinese creation-title recognition (Chen and Chen, 2005) and word segmentation (Li and Sun, 2009), plus, recently, automatic vandalism de9 Punctuation has high semantic entropy (Melamed, 1997); for an analysis of the many roles played in the WSJ by the comma — the most frequent and unpredictable punctuation mark in that data set — see Beeferman et al. (1998, Table 2). 26 tection in Wikipedia (Wang and McKeown, 2010). 10 Conclusions and Future Work Punctuation improves dependency grammar induction. Many unsupervised (and supervised) parsers could be easily modified to use sprawl-constrained d"
W11-0303,P07-1049,0,0.266059,"that without constrained training, “full” data sets do not help, on average, despite having more data and lexicalization. Furthermore, after constrained training, we detected no evidence of benefits to additional retraining: not with the relaxed sprawl constraint, nor unconstrained. 9 Related Work Punctuation has been used to improve parsing since rule-based systems (Jones, 1994). Statistical parsers reap dramatic gains from punctuation (Engel et al., 2002; Roark, 2001; Charniak, 2000; Johnson, 1998; Collins, 1997, inter alia). And it is even known to help in unsupervised constituent parsing (Seginer, 2007). But for dependency grammar induction, until now, punctuation remained unexploited. Parsing Techniques Most-Similar to Constraints A “divide-and-rule” strategy that relies on punctuation has been used in supervised constituent parsing of long Chinese sentences (Li et al., 2005). For English, there has been interest in balanced punctuation (Briscoe, 1994), more recently using rule-based filters (White and Rajkumar, 2008) in a combinatory categorial grammar (CCG). Our focus is specifically on unsupervised learning of dependency grammars and is similar, in spirit, to Eisner and Smith’s (2005) “v"
W11-0303,1993.iwpt-1.22,0,0.304664,"Missing"
W11-0303,W10-2902,1,0.565198,"Computer Science Stanford University, Stanford, CA, 94305, USA jurafsky@stanford.edu Abstract as utterances transcribed by speech recognizers, are often difficult even for humans (Kim and Woodland, 2002). Therefore, one would expect grammar inducers to exploit any available linguistic meta-data. And yet in unsupervised dependency parsing, sentenceinternal punctuation has long been ignored (Carroll and Charniak, 1992; Paskin, 2001; Klein and Manning, 2004; Blunsom and Cohn, 2010, inter alia). HTML is another kind of meta-data that is ordinarily stripped out in pre-processing. However, recently Spitkovsky et al. (2010b) demonstrated that web markup can successfully guide hierarchical syntactic structure discovery, observing, for example, that anchors often match linguistic constituents: We show how punctuation can be used to improve unsupervised dependency parsing. Our linguistic analysis confirms the strong connection between English punctuation and phrase boundaries in the Penn Treebank. However, approaches that naively include punctuation marks in the grammar (as if they were words) do not perform well with Klein and Manning’s Dependency Model with Valence (DMV). Instead, we split a sentence at punctuat"
W11-0303,P10-1130,1,0.905035,"Computer Science Stanford University, Stanford, CA, 94305, USA jurafsky@stanford.edu Abstract as utterances transcribed by speech recognizers, are often difficult even for humans (Kim and Woodland, 2002). Therefore, one would expect grammar inducers to exploit any available linguistic meta-data. And yet in unsupervised dependency parsing, sentenceinternal punctuation has long been ignored (Carroll and Charniak, 1992; Paskin, 2001; Klein and Manning, 2004; Blunsom and Cohn, 2010, inter alia). HTML is another kind of meta-data that is ordinarily stripped out in pre-processing. However, recently Spitkovsky et al. (2010b) demonstrated that web markup can successfully guide hierarchical syntactic structure discovery, observing, for example, that anchors often match linguistic constituents: We show how punctuation can be used to improve unsupervised dependency parsing. Our linguistic analysis confirms the strong connection between English punctuation and phrase boundaries in the Penn Treebank. However, approaches that naively include punctuation marks in the grammar (as if they were words) do not perform well with Klein and Manning’s Dependency Model with Valence (DMV). Instead, we split a sentence at punctuat"
W11-0303,C00-2117,0,0.0339085,"istently steering training away from certain constructions, as guided by punctuation, to help prevent underfitting. Various Other Uses of Punctuation in NLP Punctuation is hard to predict,9 partly because it can signal long-range dependences (Lu and Ng, 2010). It often provides valuable cues to NLP tasks such as part-of-speech tagging and named-entity recognition (Hillard et al., 2006), information extraction (Favre et al., 2008) and machine translation (Lee et al., 2006; Matusov et al., 2006). Other applications have included Japanese sentence analysis (Ohyama et al., 1986), genre detection (Stamatatos et al., 2000), bilingual sentence alignment (Yeh, 2003), semantic role labeling (Pradhan et al., 2005), Chinese creation-title recognition (Chen and Chen, 2005) and word segmentation (Li and Sun, 2009), plus, recently, automatic vandalism de9 Punctuation has high semantic entropy (Melamed, 1997); for an analysis of the many roles played in the WSJ by the comma — the most frequent and unpredictable punctuation mark in that data set — see Beeferman et al. (1998, Table 2). 26 tection in Wikipedia (Wang and McKeown, 2010). 10 Conclusions and Future Work Punctuation improves dependency grammar induction. Many u"
W11-0303,C10-1129,0,0.00685326,"cations have included Japanese sentence analysis (Ohyama et al., 1986), genre detection (Stamatatos et al., 2000), bilingual sentence alignment (Yeh, 2003), semantic role labeling (Pradhan et al., 2005), Chinese creation-title recognition (Chen and Chen, 2005) and word segmentation (Li and Sun, 2009), plus, recently, automatic vandalism de9 Punctuation has high semantic entropy (Melamed, 1997); for an analysis of the many roles played in the WSJ by the comma — the most frequent and unpredictable punctuation mark in that data set — see Beeferman et al. (1998, Table 2). 26 tection in Wikipedia (Wang and McKeown, 2010). 10 Conclusions and Future Work Punctuation improves dependency grammar induction. Many unsupervised (and supervised) parsers could be easily modified to use sprawl-constrained decoding in inference. It applies to pre-trained models and, so far, helped every data set and language. Tightly interwoven into the fabric of writing systems, punctuation frames most unannotated plaintext. We showed that rules for converting markup into accurate parsing constraints are still optimal for inter-punctuation fragments. Punctuation marks are more ubiquitous and natural than web markup: what little punctuat"
W11-0303,W08-1703,0,0.0464732,"atic gains from punctuation (Engel et al., 2002; Roark, 2001; Charniak, 2000; Johnson, 1998; Collins, 1997, inter alia). And it is even known to help in unsupervised constituent parsing (Seginer, 2007). But for dependency grammar induction, until now, punctuation remained unexploited. Parsing Techniques Most-Similar to Constraints A “divide-and-rule” strategy that relies on punctuation has been used in supervised constituent parsing of long Chinese sentences (Li et al., 2005). For English, there has been interest in balanced punctuation (Briscoe, 1994), more recently using rule-based filters (White and Rajkumar, 2008) in a combinatory categorial grammar (CCG). Our focus is specifically on unsupervised learning of dependency grammars and is similar, in spirit, to Eisner and Smith’s (2005) “vine grammar” formalism. An important difference is that instead of imposing static limits on allowed dependency lengths, our restrictions are dynamic — they disallow some long (and some short) arcs that would have otherwise crossed nearby punctuation. Incorporating partial bracketings into grammar induction is an idea tracing back to Pereira and Schabes (1992). It inspired Spitkovsky et al. (2010b) to mine parsing constr"
W11-0303,O03-2004,0,0.0175714,"ns, as guided by punctuation, to help prevent underfitting. Various Other Uses of Punctuation in NLP Punctuation is hard to predict,9 partly because it can signal long-range dependences (Lu and Ng, 2010). It often provides valuable cues to NLP tasks such as part-of-speech tagging and named-entity recognition (Hillard et al., 2006), information extraction (Favre et al., 2008) and machine translation (Lee et al., 2006; Matusov et al., 2006). Other applications have included Japanese sentence analysis (Ohyama et al., 1986), genre detection (Stamatatos et al., 2000), bilingual sentence alignment (Yeh, 2003), semantic role labeling (Pradhan et al., 2005), Chinese creation-title recognition (Chen and Chen, 2005) and word segmentation (Li and Sun, 2009), plus, recently, automatic vandalism de9 Punctuation has high semantic entropy (Melamed, 1997); for an analysis of the many roles played in the WSJ by the comma — the most frequent and unpredictable punctuation mark in that data set — see Beeferman et al. (1998, Table 2). 26 tection in Wikipedia (Wang and McKeown, 2010). 10 Conclusions and Future Work Punctuation improves dependency grammar induction. Many unsupervised (and supervised) parsers could"
W11-0303,J03-4003,0,\N,Missing
W11-0303,D07-1096,0,\N,Missing
W11-1516,D08-1038,1,0.874041,"tion from prominent researchers and textbook writers in the field, we demonstrate that our system is able to differentiate between the various types of collaborations in our suggested taxonomy, based only on words used, at low but statistically significant accuracy. We use this same similarity score to analyze the ACL community by sub-field, finding significant deviations. 2 Related Work In recent years, popular topic models such as Latent Dirichlet Allocation (Blei et al., 2003) have been increasingly used to study the history of science by observing the changing trends in term based topics (Hall et al., 2008), (Gerrish and Blei, 2010). In the case of Hall et al., regular LDA topic models were trained over the ACL anthology on a per year basis, and the changing trends in topics were studied from year to year. Gerrish and Blei’s work computed a measure of influence by using Dynamic Topic Models (Blei and Lafferty, 2006) and studying the change of statistics of the language used in a corpus. These models propose interesting ideas for utilizing topic modeling to understand aspects of scientific history. However, our primary interest, in this paper, is the study of academic collaboration between differ"
W11-1516,W09-3607,0,0.385966,"terial, assume different roles, and experience the collaboration in different ways. In this paper, we present a new frame for thinking about the variation in collaboration types and develop a computational metric to characterize the distinct contributions and roles of each collaborator within the scholarly material they produce. The topic of understanding collaborations has attracted much interest in the social sciences over the years. Recently, it has gained traction in computer science, too, in the form of social network analysis. Much work focuses on studying networks formed via citations (Radev et al., 2009; White and Mccain, 1998), as well as co-authorship links (Nascimento et al., 2003; Liu et al., 2005). However, these works focus largely on the graphical structure derived from paper citations and author co-occurrences, and less on the textual content of the papers themselves. In this work, we examine the nature of academic collaboration using text as a primary component. We propose a theoretical framework for determining the types of collaboration present in a document, based on factors such as the number of established authors, the presence of unestablished authors and the similarity of the"
W11-1516,D09-1026,1,0.525933,"for classifying papers into these types, as well as a description of the intuition behind each collaboration class. 124 Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 124–132, c Portland, OR, USA, 24 June 2011. 2011 Association for Computational Linguistics We explore our theory with a computational method to categorize collaborative works into their collaboration types using an approach based on topic modeling, where we model every paper as a latent mixture of its authors. For our system, we use Labeled-LDA (LLDA (Ramage et al., 2009)) to train models over the ACL corpus for every year of the words best attributed to each author in all the papers they write. We use the resulting author signatures as a basis for several metrics that can classify each document by its collaboration type. We qualitatively analyze our results by examining the categorization of several high impact papers. With consultation from prominent researchers and textbook writers in the field, we demonstrate that our system is able to differentiate between the various types of collaborations in our suggested taxonomy, based only on words used, at low but"
W12-1903,W06-2920,0,0.28816,"Missing"
W12-1903,N09-1009,0,0.167267,"e information). Combining capitalization with punctuation-induced constraints in inference further improved parsing performance, attaining state-of-the-art levels for many languages. 1 Introduction Dependency grammar induction and related problems of unsupervised syntactic structure discovery are attracting increasing attention (Rasooli and Faili, 2012; Mareˇcek and Zabokrtsk´y, 2011, inter alia). Since sentence structure is underdetermined by raw text, there have been efforts to simplify the task, via (i) pooling features of syntax across languages (Cohen et al., 2011; McDonald et al., 2011; Cohen and Smith, 2009); as well as (ii) identifying universal rules (Naseem et al., 2010) — such as verbocentricity (Gimpel and Smith, 2011) — that need not be learned at all. Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs. As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Ponvert et al., 2010; Søgaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important. Examples of useful POS-agnostic clues include punctuation boundaries (P"
W12-1903,P10-1152,0,0.372168,"Missing"
W12-1903,D11-1005,0,0.261212,"Missing"
W12-1903,J93-2004,0,0.0418649,"Missing"
W12-1903,W11-3901,0,0.304582,"Missing"
W12-1903,D11-1006,0,0.155163,"14 languages having case information). Combining capitalization with punctuation-induced constraints in inference further improved parsing performance, attaining state-of-the-art levels for many languages. 1 Introduction Dependency grammar induction and related problems of unsupervised syntactic structure discovery are attracting increasing attention (Rasooli and Faili, 2012; Mareˇcek and Zabokrtsk´y, 2011, inter alia). Since sentence structure is underdetermined by raw text, there have been efforts to simplify the task, via (i) pooling features of syntax across languages (Cohen et al., 2011; McDonald et al., 2011; Cohen and Smith, 2009); as well as (ii) identifying universal rules (Naseem et al., 2010) — such as verbocentricity (Gimpel and Smith, 2011) — that need not be learned at all. Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs. As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Ponvert et al., 2010; Søgaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important. Examples of useful POS-agnostic clues include p"
W12-1903,D10-1120,0,0.0504744,"straints in inference further improved parsing performance, attaining state-of-the-art levels for many languages. 1 Introduction Dependency grammar induction and related problems of unsupervised syntactic structure discovery are attracting increasing attention (Rasooli and Faili, 2012; Mareˇcek and Zabokrtsk´y, 2011, inter alia). Since sentence structure is underdetermined by raw text, there have been efforts to simplify the task, via (i) pooling features of syntax across languages (Cohen et al., 2011; McDonald et al., 2011; Cohen and Smith, 2009); as well as (ii) identifying universal rules (Naseem et al., 2010) — such as verbocentricity (Gimpel and Smith, 2011) — that need not be learned at all. Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs. As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Ponvert et al., 2010; Søgaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important. Examples of useful POS-agnostic clues include punctuation boundaries (Ponvert et al., 2011; Spitkovsky et al., 2011b; 16 Briscoe, 1994) an"
W12-1903,P92-1017,0,0.567286,"tely most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs. As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Ponvert et al., 2010; Søgaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important. Examples of useful POS-agnostic clues include punctuation boundaries (Ponvert et al., 2011; Spitkovsky et al., 2011b; 16 Briscoe, 1994) and various kinds of bracketing constraints (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b; Pereira and Schabes, 1992). We propose adding capitalization to this growing list of sources of partial bracketings. Our intuition stems from English, where (maximal) spans of capitalized words — such as Apple II, World War I, Mayor William H. Hudnut III, International Business Machines Corp. and Alexandria, Va — tend to demarcate proper nouns. Consider a motivating example (all of our examples are from WSJ) without punctuation, in which all (eight) capitalized word clumps and uncased numerals match base noun phrase constituent boundaries: [NP Jay Stevens] of [NP Dean Witter] actually cut his per-share earnings estimat"
W12-1903,P11-1108,0,0.0131883,"); as well as (ii) identifying universal rules (Naseem et al., 2010) — such as verbocentricity (Gimpel and Smith, 2011) — that need not be learned at all. Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs. As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Ponvert et al., 2010; Søgaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important. Examples of useful POS-agnostic clues include punctuation boundaries (Ponvert et al., 2011; Spitkovsky et al., 2011b; 16 Briscoe, 1994) and various kinds of bracketing constraints (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b; Pereira and Schabes, 1992). We propose adding capitalization to this growing list of sources of partial bracketings. Our intuition stems from English, where (maximal) spans of capitalized words — such as Apple II, World War I, Mayor William H. Hudnut III, International Business Machines Corp. and Alexandria, Va — tend to demarcate proper nouns. Consider a motivating example (all of our examples are from WSJ) without punctuation, in which all (eight) ca"
W12-1903,W12-0701,0,0.456489,"MZ 76.9 Sbg 33.6 SCAJ5 34.6 MZNR 50.0 SCAJ6 66.8 MPHpt 40.9 SAJ 61.3 RFH1 45.2 58.9 45.2 57.2∗ Table 5: Unsupervised parsing with both capitalizationand punctuation-induced constraints in inference, tested against the 14 held-out sets from 2006/7 CoNLL shared tasks, and state-of-the-art results (all sentence lengths) for systems that: (i) are also POS-agnostic and monolingual, including SCAJ (Spitkovsky et al., 2011a, Tables 5–6) and SAJ (Spitkovsky et al., 2011b); and (ii) rely on gold POS-tag identities to (a) discourage noun roots (Mareˇcek and Zabokrtsk´y, 2011, MZ), (b) encourage verbs (Rasooli and Faili, 2012, RF), or (c) transfer delexicalized parsers (Søgaard, 2011a, S) from resource-rich languages with parallel translations (McDonald et al., 2011, MPH). the German particle von is not capitalized, although the Dutch van is, unless preceded by a given name or initial — hence Van Gogh, yet Vincent van Gogh. 7.1 Constraint Accuracies Across Languages Since even related languages (e.g., Flemish, Dutch, German and English) can have quite different conventions regarding capitalization, one would not expect the same simple strategy to be uniformly useful — or useful in the same way — across disparate l"
W12-1903,P07-1049,0,0.0338474,"ter alia). Since sentence structure is underdetermined by raw text, there have been efforts to simplify the task, via (i) pooling features of syntax across languages (Cohen et al., 2011; McDonald et al., 2011; Cohen and Smith, 2009); as well as (ii) identifying universal rules (Naseem et al., 2010) — such as verbocentricity (Gimpel and Smith, 2011) — that need not be learned at all. Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs. As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Ponvert et al., 2010; Søgaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important. Examples of useful POS-agnostic clues include punctuation boundaries (Ponvert et al., 2011; Spitkovsky et al., 2011b; 16 Briscoe, 1994) and various kinds of bracketing constraints (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b; Pereira and Schabes, 1992). We propose adding capitalization to this growing list of sources of partial bracketings. Our intuition stems from English, where (maximal) spans of capitalized words — such as Apple II, Worl"
W12-1903,P11-2120,0,0.226286,"s underdetermined by raw text, there have been efforts to simplify the task, via (i) pooling features of syntax across languages (Cohen et al., 2011; McDonald et al., 2011; Cohen and Smith, 2009); as well as (ii) identifying universal rules (Naseem et al., 2010) — such as verbocentricity (Gimpel and Smith, 2011) — that need not be learned at all. Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs. As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Ponvert et al., 2010; Søgaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important. Examples of useful POS-agnostic clues include punctuation boundaries (Ponvert et al., 2011; Spitkovsky et al., 2011b; 16 Briscoe, 1994) and various kinds of bracketing constraints (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b; Pereira and Schabes, 1992). We propose adding capitalization to this growing list of sources of partial bracketings. Our intuition stems from English, where (maximal) spans of capitalized words — such as Apple II, World War I, Mayor William H. Hudnut III,"
W12-1903,W11-1109,0,0.389276,"s underdetermined by raw text, there have been efforts to simplify the task, via (i) pooling features of syntax across languages (Cohen et al., 2011; McDonald et al., 2011; Cohen and Smith, 2009); as well as (ii) identifying universal rules (Naseem et al., 2010) — such as verbocentricity (Gimpel and Smith, 2011) — that need not be learned at all. Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs. As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Ponvert et al., 2010; Søgaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important. Examples of useful POS-agnostic clues include punctuation boundaries (Ponvert et al., 2011; Spitkovsky et al., 2011b; 16 Briscoe, 1994) and various kinds of bracketing constraints (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b; Pereira and Schabes, 1992). We propose adding capitalization to this growing list of sources of partial bracketings. Our intuition stems from English, where (maximal) spans of capitalized words — such as Apple II, World War I, Mayor William H. Hudnut III,"
W12-1903,N10-1116,1,0.927286,"learned at all. Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs. As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Ponvert et al., 2010; Søgaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important. Examples of useful POS-agnostic clues include punctuation boundaries (Ponvert et al., 2011; Spitkovsky et al., 2011b; 16 Briscoe, 1994) and various kinds of bracketing constraints (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b; Pereira and Schabes, 1992). We propose adding capitalization to this growing list of sources of partial bracketings. Our intuition stems from English, where (maximal) spans of capitalized words — such as Apple II, World War I, Mayor William H. Hudnut III, International Business Machines Corp. and Alexandria, Va — tend to demarcate proper nouns. Consider a motivating example (all of our examples are from WSJ) without punctuation, in which all (eight) capitalized word clumps and uncased numerals match base noun phrase constituent boundaries: [NP Jay Stevens] of [NP Dean Witter] actually cut h"
W12-1903,P10-1130,1,0.924388,"learned at all. Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs. As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Ponvert et al., 2010; Søgaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important. Examples of useful POS-agnostic clues include punctuation boundaries (Ponvert et al., 2011; Spitkovsky et al., 2011b; 16 Briscoe, 1994) and various kinds of bracketing constraints (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b; Pereira and Schabes, 1992). We propose adding capitalization to this growing list of sources of partial bracketings. Our intuition stems from English, where (maximal) spans of capitalized words — such as Apple II, World War I, Mayor William H. Hudnut III, International Business Machines Corp. and Alexandria, Va — tend to demarcate proper nouns. Consider a motivating example (all of our examples are from WSJ) without punctuation, in which all (eight) capitalized word clumps and uncased numerals match base noun phrase constituent boundaries: [NP Jay Stevens] of [NP Dean Witter] actually cut h"
W12-1903,D11-1117,1,0.932484,"d by raw text, there have been efforts to simplify the task, via (i) pooling features of syntax across languages (Cohen et al., 2011; McDonald et al., 2011; Cohen and Smith, 2009); as well as (ii) identifying universal rules (Naseem et al., 2010) — such as verbocentricity (Gimpel and Smith, 2011) — that need not be learned at all. Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs. As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Ponvert et al., 2010; Søgaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important. Examples of useful POS-agnostic clues include punctuation boundaries (Ponvert et al., 2011; Spitkovsky et al., 2011b; 16 Briscoe, 1994) and various kinds of bracketing constraints (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b; Pereira and Schabes, 1992). We propose adding capitalization to this growing list of sources of partial bracketings. Our intuition stems from English, where (maximal) spans of capitalized words — such as Apple II, World War I, Mayor William H. Hudnut III, International Business Ma"
W12-1903,W11-0303,1,0.893252,"d by raw text, there have been efforts to simplify the task, via (i) pooling features of syntax across languages (Cohen et al., 2011; McDonald et al., 2011; Cohen and Smith, 2009); as well as (ii) identifying universal rules (Naseem et al., 2010) — such as verbocentricity (Gimpel and Smith, 2011) — that need not be learned at all. Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs. As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Ponvert et al., 2010; Søgaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important. Examples of useful POS-agnostic clues include punctuation boundaries (Ponvert et al., 2011; Spitkovsky et al., 2011b; 16 Briscoe, 1994) and various kinds of bracketing constraints (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b; Pereira and Schabes, 1992). We propose adding capitalization to this growing list of sources of partial bracketings. Our intuition stems from English, where (maximal) spans of capitalized words — such as Apple II, World War I, Mayor William H. Hudnut III, International Business Ma"
W12-1903,D11-1118,1,0.929567,"d by raw text, there have been efforts to simplify the task, via (i) pooling features of syntax across languages (Cohen et al., 2011; McDonald et al., 2011; Cohen and Smith, 2009); as well as (ii) identifying universal rules (Naseem et al., 2010) — such as verbocentricity (Gimpel and Smith, 2011) — that need not be learned at all. Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs. As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Ponvert et al., 2010; Søgaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important. Examples of useful POS-agnostic clues include punctuation boundaries (Ponvert et al., 2011; Spitkovsky et al., 2011b; 16 Briscoe, 1994) and various kinds of bracketing constraints (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b; Pereira and Schabes, 1992). We propose adding capitalization to this growing list of sources of partial bracketings. Our intuition stems from English, where (maximal) spans of capitalized words — such as Apple II, World War I, Mayor William H. Hudnut III, International Business Ma"
W12-1903,D12-1063,1,0.689519,"tire treebank. Columns markup (Spitkovsky et al., 2010b) and punct (Spitkovsky et al., 2011b) indicate that capitalization yields across-the-board more accurate constraints (for English) compared with fragments derived from punctuation or markup (i.e., anchor text, bold, italics and underline tags in HTML), for which such constraints were originally intended. 4 Pilot Experiments on Supervised Parsing To further test the potential of capitalization-induced constraints, we applied them in the Viterbi-decoding phase of a simple (unlexicalized) supervised dependency parser — an instance of DBM-1 (Spitkovsky et al., 2012, §2.1), trained on WSJ sentences with up punct.: thread none: 71.8 74.3 capital:thread 72.3 74.6 tear 72.4 74.7 sprawl 72.4 74.7 loose 72.4 74.8 strict′ 71.4 73.7 strict 71.0 73.1 tear 74.4 74.7 74.7 74.7 74.7 73.7 73.1 sprawl 74.5 74.9 74.9 74.9 74.9 73.9 73.2 loose 73.3 73.6 73.6 73.4 73.3 72.7 72.1 Table 3: Supervised (directed) accuracy on Section 23 of WSJ using capitalization-induced constraints (vertical) jointly with punctuation (horizontal) in Viterbi-decoding. CoNLL Year & Language German 2006 Czech ’6 English ’7 Bulgarian ’6 Danish ’6 Greek ’7 Dutch ’6 Italian ’7 Catalan ’7 Turkish"
W12-1903,J03-4003,0,\N,Missing
W12-1903,D07-1096,0,\N,Missing
W98-0319,J97-1002,0,0.0403231,"Finally, our investigation of the syntax of assessments suggests that at least some dialog acts have a very constrained syntactic realization, a per-dialog act 'microsyntax'. 1 Introduction The structure of a discourse is reflected in many aspects of its linguistic realization. These include 'cue phrases', words like now and well which can indicate discourse structure, as well as other lexical, prosodic, or syntactic 'discourse markers'. Multiparty dialog contains a particular kind of discourse structure, the dialog act, related to the speech acts of Searle (1969), the conversational moves of Carletta et al. (1997), and the adjacency pair-parts of Schegloff (1968) Sacks et al. (1974) (see also e.g. Allen and Core (1997; Nagata and Morimoto (1994)). Like other types of structure, the dialog act sequence of a conversation is also reflected in its lexical, prosodic, and syntactic realization. This paper presents a preliminary investigation into the realization of a particular class of dialog acts which play an essential structuring role in dialog, the backehannels or acknowledgements tokens. We discuss the importance of words like yeah as cue-phrases for dialog structure, the role of prosodic knowledge, an"
W98-0319,J96-2004,0,0.109783,"Missing"
W98-0319,J93-3003,0,0.0406846,"the degree to which speaker accepts some previous proposal, plan, opinion, or statement. Because SWBD consists of free conversation and not task-oriented dialog, the majority of our tokens were agree/accepts, which for convenience we will refer to as agreements. These are used to indicate the speaker's agreement with a statement or opinion expressed by another speaker, or the acceptance of a proposal. Table 5 shows an example. 3 Lexical Cues to Dialog Act Identity Perhaps the most studied cue for discourse structure are lexical cues, also called 'cue phrases', which are defined as follows by Hirschberg and Litman (1993): ""Cue phrases are linguistic expressions a telling (Drummondand Hopper, 1993b) such as NOW and WELL that function as explicit indicators of the structure of a discourse"". This section examines the role of lexical cues in distinguishing four common DAs with considerable overlap in lexical realizations. These are continuers, agreements, yes-answers, and incipient-speakership. What makes these four types so difficult to distinguish is that they all can be realized by common words like uh.huh, yeah, right, yes, okay. But while some tokens (like yeah) are highly ambiguous, others, (like uh-huh or"
