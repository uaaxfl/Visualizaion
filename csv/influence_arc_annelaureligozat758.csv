2004.jeptalnrecital-recital.1,P97-1004,0,0.0755718,"Missing"
2004.jeptalnrecital-recital.1,2002.jeptalnrecital-long.17,0,0.0459359,"Missing"
2006.jeptalnrecital-long.20,W02-1033,0,0.0939832,"Missing"
2007.jeptalnrecital-poster.17,C00-1039,0,0.0313156,"Missing"
2011.jeptalnrecital-court.7,P06-1114,0,0.0337413,"Missing"
2011.jeptalnrecital-court.7,quintard-etal-2010-question,1,0.881271,"Missing"
2011.jeptalnrecital-long.27,P04-1054,0,0.13033,"Missing"
2011.jeptalnrecital-long.27,N10-1004,0,0.0433158,"Missing"
2011.jeptalnrecital-long.27,E06-1015,0,0.0603226,"Missing"
2011.jeptalnrecital-long.27,W08-0602,0,0.0392329,"Missing"
2011.jeptalnrecital-long.27,N06-1037,0,0.0421661,"Missing"
2011.jeptalnrecital-long.27,P05-1053,0,0.0863018,"Missing"
2011.jeptalnrecital-long.31,W08-0607,0,0.0304446,"Missing"
2011.jeptalnrecital-long.31,W09-1418,0,0.0463343,"Missing"
2011.jeptalnrecital-long.31,W04-3103,0,0.0873363,"Missing"
2011.jeptalnrecital-long.31,P07-1125,0,0.0554435,"Missing"
2011.jeptalnrecital-long.31,W09-1304,0,0.0434246,"Missing"
2011.jeptalnrecital-long.31,D09-1019,0,0.0224095,"Missing"
2011.jeptalnrecital-long.31,W02-1011,0,0.0116152,"Missing"
2011.jeptalnrecital-long.31,N07-2036,0,0.0274297,"Missing"
2011.jeptalnrecital-long.31,P08-1033,0,0.0248729,"Missing"
2011.jeptalnrecital-long.31,W08-0606,0,0.0456914,"Missing"
2015.jeptalnrecital-long.11,P05-1045,0,0.0406432,"Missing"
2015.jeptalnrecital-long.11,O97-1002,0,0.509187,"Missing"
2015.jeptalnrecital-long.11,W06-1416,0,0.0810715,"Missing"
2015.jeptalnrecital-long.11,P03-1054,0,0.0234065,"Missing"
2015.jeptalnrecital-long.11,levy-andrew-2006-tregex,0,0.061128,"Missing"
2015.jeptalnrecital-long.11,P97-1009,0,0.173308,"Missing"
2015.jeptalnrecital-long.11,W03-0203,0,0.139574,"Missing"
2015.jeptalnrecital-long.11,W09-0207,0,0.0375078,"Missing"
2015.jeptalnrecital-long.11,pho-etal-2014-multiple,1,0.834521,"Missing"
2016.jeptalnrecital-demo.18,P12-3007,0,0.0728341,"Missing"
2016.jeptalnrecital-demo.18,L16-1433,1,0.870596,"Missing"
2016.jeptalnrecital-demo.18,L16-1147,0,0.0196495,"Missing"
2016.jeptalnrecital-long.17,W14-1206,1,0.889865,"Missing"
2016.jeptalnrecital-long.17,francois-etal-2014-flelex,1,0.889021,"Missing"
2016.jeptalnrecital-long.17,N07-1058,0,0.0924425,"Missing"
2016.jeptalnrecital-long.17,P10-1023,0,0.0319453,"Missing"
2016.jeptalnrecital-long.17,W14-1821,0,0.0297761,"Missing"
2016.jeptalnrecital-long.17,P13-3015,0,0.0471564,"Missing"
2016.jeptalnrecital-long.17,L16-1035,1,0.86958,"Missing"
2017.jeptalnrecital-long.13,P14-1023,0,0.0994656,"Missing"
2017.jeptalnrecital-long.13,D07-1074,0,0.239996,"Missing"
2017.jeptalnrecital-long.13,K16-1026,0,0.0357843,"Missing"
2017.jeptalnrecital-long.13,Q15-1016,0,0.091516,"Missing"
2017.jeptalnrecital-long.13,Q15-1023,0,0.0405613,"Missing"
2017.jeptalnrecital-long.13,P09-1113,0,0.0726146,"Missing"
2017.jeptalnrecital-long.13,Q14-1019,0,0.0635932,"Missing"
2017.jeptalnrecital-long.13,D14-1167,0,0.0489415,"Missing"
2017.jeptalnrecital-long.13,K16-1025,0,0.0395777,"Missing"
2018.jeptalnrecital-long.6,Y09-1013,0,0.0696944,"Missing"
2018.jeptalnrecital-long.6,K16-1018,0,0.0535501,"Missing"
2018.jeptalnrecital-long.6,D17-1076,0,0.0610997,"Missing"
2018.jeptalnrecital-long.6,N16-1030,0,0.0826828,"Missing"
2018.jeptalnrecital-long.6,D17-1010,0,0.036195,"Missing"
2018.jeptalnrecital-long.6,P17-2035,0,0.0263336,"Missing"
2018.jeptalnrecital-long.6,N03-1033,0,0.107712,"Missing"
2019.jeptalnrecital-court.29,W16-3905,0,0.0442456,"Missing"
2019.jeptalnrecital-court.29,L18-1619,1,0.88887,"Missing"
2019.jeptalnrecital-court.29,F12-2024,0,0.0605087,"Missing"
2019.jeptalnrecital-court.29,L18-1347,0,0.0464786,"Missing"
2019.jeptalnrecital-court.29,W16-1715,0,0.0630444,"Missing"
2019.jeptalnrecital-court.29,C94-1097,0,0.736067,"Missing"
2019.jeptalnrecital-court.29,L16-1262,0,0.0602156,"Missing"
2019.jeptalnrecital-court.29,L18-1279,0,0.022254,"Missing"
2019.jeptalnrecital-court.29,K17-3009,0,0.0550734,"Missing"
2019.jeptalnrecital-court.29,taule-etal-2008-ancora,0,0.140256,"Missing"
2020.jeptalnrecital-taln.33,W18-7002,0,0.0527161,"Missing"
2020.jeptalnrecital-taln.33,P17-4012,0,0.0298304,"Missing"
2020.jeptalnrecital-taln.33,W11-2132,0,0.085714,"Missing"
2020.jeptalnrecital-taln.33,moore-2002-fast,0,0.241101,"Missing"
2020.jeptalnrecital-taln.33,P14-1041,0,0.0591115,"Missing"
2020.jeptalnrecital-taln.33,P17-2014,0,0.0355399,"Missing"
2020.jeptalnrecital-taln.33,P02-1040,0,0.112683,"Missing"
2020.jeptalnrecital-taln.33,L18-1553,0,0.0326444,"Missing"
2020.jeptalnrecital-taln.33,P18-2113,0,0.0332244,"Missing"
2020.jeptalnrecital-taln.33,W16-2323,0,0.0779722,"Missing"
2020.jeptalnrecital-taln.33,P19-1198,0,0.0545531,"Missing"
2020.jeptalnrecital-taln.33,P12-1107,0,0.0564794,"Missing"
2020.jeptalnrecital-taln.33,Q15-1021,0,0.0343622,"Missing"
2020.jeptalnrecital-taln.33,D17-1062,0,0.0371706,"Missing"
2020.jeptalnrecital-taln.33,D18-1355,0,0.0274487,"Missing"
2020.jeptalnrecital-taln.33,C10-1152,0,0.104219,"Missing"
2021.sustainlp-1.2,galibert-etal-2010-named,0,0.0167447,"Missing"
2021.sustainlp-1.2,N18-1131,0,0.0151111,"d in a scientific publication so we have decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognition (Ma and Hovy, 2016) and one that addresses both flat and nested entity recognition, introduced by (Yu et al., 2020). (Yu et al., 2020) adapt the biaffine dependency parsing model of (Dozat and Manning, 2017) to Named Entity Recognition by reformulating this task as the task of identifying start and en"
2021.sustainlp-1.2,2020.sustainlp-1.19,0,0.0322306,"count the number of experiments, conversion to interpretable numbers, comparison with other locations No, default value of PUE = 1.67 (2019) PC, local server, cloud No Yes, carbon intensity from carbonfootprint Dec 17, 2020 Yes Yes (online) No install needed Fair CC-BY-4.0 Generates a statement to report the results 2021 4 1 (Liu et al., 2021) Dynamic use Hardware only Asserting certain hardware Yes. Default PUE (1.58) can be adjusted Install dependent No Yes, carbon intensity from electricitymap April 29, 2021 Yes No Fair Fair MIT Generates a statement and graphs to report results 2020 33 3 (Cao et al., 2020; Prasanna et al., 2020; Peng et al., 2021) Experiment Impact Tracker (Henderson et al., 2020) Dynamic use Hardware only Partly, for Google, Amazon, Azure cloud providers. 3 specific providers, private infrastructure No Yes Yes, pointers supplied to user, including electricitymap May 4, 2021 Yes Yes (online) No install needed Fair MIT Generates text and LATEXcode to report the results 2019 35 4 (Sarti, 2020; Selby et al., 2021; Chaudhary et al., 2020; Gencoglu, 2020) (Lacoste et al., 2019) ML CO2 Impact Dynamic use Hardware only Year for the data, comparison with other locations No PUE used bu"
2021.sustainlp-1.2,N16-1030,0,0.0287126,"arbon footprint of computer use based on user supplied information including hardware, runtime, cloud provider and location of the computing facilities operated. We are aware that a new version of the tool is being developed under the umbrella of the Code Carbon 3 initiative. However, it is not yet described in a scientific publication so we have decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognitio"
2021.sustainlp-1.2,2020.findings-emnlp.152,0,0.0564346,"Missing"
2021.sustainlp-1.2,2021.findings-emnlp.71,0,0.0472671,"Missing"
2021.sustainlp-1.2,N18-1202,0,0.0207387,"user supplied information including hardware, runtime, cloud provider and location of the computing facilities operated. We are aware that a new version of the tool is being developed under the umbrella of the Code Carbon 3 initiative. However, it is not yet described in a scientific publication so we have decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognition (Ma and Hovy, 2016) and one that addre"
2021.sustainlp-1.2,D15-1102,0,0.0165078,"is not yet described in a scientific publication so we have decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognition (Ma and Hovy, 2016) and one that addresses both flat and nested entity recognition, introduced by (Yu et al., 2020). (Yu et al., 2020) adapt the biaffine dependency parsing model of (Dozat and Manning, 2017) to Named Entity Recognition by reformulating this task as the task of identif"
2021.sustainlp-1.2,2020.emnlp-main.259,0,0.0312547,"Missing"
2021.sustainlp-1.2,2020.coling-main.78,0,0.0179515,"ation including hardware, runtime, cloud provider and location of the computing facilities operated. We are aware that a new version of the tool is being developed under the umbrella of the Code Carbon 3 initiative. However, it is not yet described in a scientific publication so we have decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognition (Ma and Hovy, 2016) and one that addresses both flat and nested"
2021.sustainlp-1.2,P16-1101,0,0.489156,"mputer use based on user supplied information including hardware, runtime, cloud provider and location of the computing facilities operated. We are aware that a new version of the tool is being developed under the umbrella of the Code Carbon 3 initiative. However, it is not yet described in a scientific publication so we have decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognition (Ma and Hovy, 201"
2021.sustainlp-1.2,M93-1032,0,0.697737,"Missing"
2021.sustainlp-1.2,P19-1527,0,0.0273959,"Missing"
2021.sustainlp-1.2,P19-1355,0,0.0935406,"y the tools to assess the impact of named entity recognition experiments in order to compare the measurement obtained in two computational set-ups. Introduction Modern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the functional performance they offer for a variety of tasks, including text classification or named entity recognition (Tourille et al., 2018). Deep learning programs can have a high environmental impact in terms of Greenhouse Gas (GHG) emissions due in particular to the energy consumption of the computational facilities used to run them (Strubell et al., 2019). The impact has been increasing over the years (Schwartz et al., 2019) and is affecting populations that can be different from those generating the impact (Bender et al., 2021). In a recent medical imaging study, Selvan (2021) suggests that the increase of large model carbon footprint does not translate into proportional accuracy gains. Measuring this impact is a first step for raising awareness and controlling the impact of NLP experiments and operations. Some 2 Environmental impact due to deep learning programs As for any Information and Communication Technology (ICT) service, a deep learni"
2021.sustainlp-1.2,W18-5622,1,0.848153,"iments. 1 • We identify tools available for measuring the environmental impact of NLP experiments • We characterize impact measurement tools with respect to scope of the impact information provided and usability • We apply the tools to assess the impact of named entity recognition experiments in order to compare the measurement obtained in two computational set-ups. Introduction Modern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the functional performance they offer for a variety of tasks, including text classification or named entity recognition (Tourille et al., 2018). Deep learning programs can have a high environmental impact in terms of Greenhouse Gas (GHG) emissions due in particular to the energy consumption of the computational facilities used to run them (Strubell et al., 2019). The impact has been increasing over the years (Schwartz et al., 2019) and is affecting populations that can be different from those generating the impact (Bender et al., 2021). In a recent medical imaging study, Selvan (2021) suggests that the increase of large model carbon footprint does not translate into proportional accuracy gains. Measuring this impact is a first step f"
2021.sustainlp-1.2,2020.emnlp-demos.6,0,0.0207593,"Missing"
2021.sustainlp-1.2,2020.acl-main.577,0,0.114583,"decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognition (Ma and Hovy, 2016) and one that addresses both flat and nested entity recognition, introduced by (Yu et al., 2020). (Yu et al., 2020) adapt the biaffine dependency parsing model of (Dozat and Manning, 2017) to Named Entity Recognition by reformulating this task as the task of identifying start and end indices and associating a category to"
ayari-etal-2010-fine,costa-sarmento-2006-component,0,\N,Missing
ayari-etal-2010-fine,gillard-etal-2006-question,0,\N,Missing
ayari-etal-2010-fine,grau-etal-2006-frasques,1,\N,Missing
ayari-etal-2010-fine,quintard-etal-2010-question,1,\N,Missing
ayari-etal-2010-fine,W01-0905,1,\N,Missing
ayari-etal-2010-fine,W04-3101,0,\N,Missing
C16-1094,J08-1001,0,0.0953665,"the LT of a text as the mean value of the Positive Normalized Pointwise Mutual Information for all pairs of content-word tokens in a text. It represents “the degree to which a text tends to use words that are highly inter-associated in the language”. They obtained a good correlation between this new cohesive metric and the grade levels on two corpora (respectively r = −546 and r = −0, 441). Interestingly, they also show that LT works better to discriminate between literary texts than informative ones. Another approach is to detect co-reference chains and compute some of their characteristics. Barzilay and Lapata (2008) considered a text as a matrix of discourse entities present in each sentence. The cohesive level of a text is then computed based on the transitions between those entities. Pitler and Nenkova (2008) implemented this model through 17 readability variables, but none was significantly correlated with difficulty. Feng et al. (2009) also replicated this technique, without getting more efficient features. Dascalu et al. (2013) computed other characteristics of lexical chains and co-reference pairs (such as the number of chains, the distance between entities, the average word length of entities, etc"
C16-1094,E09-1027,0,0.154132,"itive readability formulas improved performance. Chall and Dale (1995, 111) had a more mixed opinion, arguing that variables based on higher textual dimensions “discriminate better among materials requiring greater maturity in reading ability”, while classic lexico-syntactic variables work better to discriminate at lower levels of difficulty. Recently, taking advantage of the opportunities offered by Natural Language Processing (NLP) techniques, readability studies have tried to leverage the semantic and discursive properties of texts to better model text difficulty (Pitler and Nenkova, 2008; Feng et al., 2009). Among those high-level dimensions that have attracted substantial attention are the level of cohesion and coherence of texts. Although psycholinguistic experiments have shown that a higher level of cohesion and coherence between a pair of related sentences decreases their reading time (Kintsch et al., 1975; Mason and Just, 2004), the added value of these textual dimensions for readability models (compared to traditional features) remains unclear, as it will be covered in more details in Section 2. This is why this paper aims at further investigating the importance of cohesion aspects for the"
C16-1094,W13-1504,0,0.0306626,"o measured its association with text difficult and obtained a non significant correlation (r = −0.1). Later, McNamara et al. (2010) reached a similar conclusion, showing that an LSA-based variable has not much of a predictive power. On the opposite, Franc¸ois and Fairon (2012; 2013) obtained a higher correlation (r = 0.63) for an L2 corpus, while Dascalu et al. (2013) got good discriminating features using both LSA and LDA (Latent Dirichlet Allocation), when classifying TASA (Touchstone Applied Science Associates) texts. An alternative approach to LSA, Lexical Tightness (LT), was suggested by Flor et al. (2013). They define the LT of a text as the mean value of the Positive Normalized Pointwise Mutual Information for all pairs of content-word tokens in a text. It represents “the degree to which a text tends to use words that are highly inter-associated in the language”. They obtained a good correlation between this new cohesive metric and the grade levels on two corpora (respectively r = −546 and r = −0, 441). Interestingly, they also show that LT works better to discriminate between literary texts than informative ones. Another approach is to detect co-reference chains and compute some of their cha"
C16-1094,E09-3003,1,0.866157,"Missing"
C16-1094,D12-1043,1,0.931401,"Missing"
C16-1094,J95-2003,0,0.785124,", perishing when the ship sank on the 15th April 1912. (Wikipedia) Figure 1: Example of anaphoric and of co-reference chain. These three devices strengthen the links between several utterances and contribute to the overall understanding of the text (Charolles, 1995). Lexical chains are effective mechanisms to find the main domain or theme of the document. Cohesive devices such as anaphora or co-reference chains correspond to one entity expressed by various linguistic expressions (so called mentions). These expressions are related by complex morpho-syntactic, syntactic or semantic constraints (Grosz et al., 1995). Mentioning the same entity several times reinforces text cohesion (Poesio et al., 2004), (Hobbs, 1979). Cohesive devices reinforce local coherence relations in some specific genres (persuasive genres) (Berzlnovich and Redeker, 2012). An interesting characteristic of cohesive devices is that their use is dependent on the type or genre of texts (Carter-Thomas, 1994). For instance, informative texts use specific referential expressions such as definite or demonstrative noun phrases as mentions, while narrative texts contain more chains composed of proper nouns or personal pronouns (Schnedecker,"
C16-1094,muzerelle-etal-2014-ancor,0,0.0161241,"umptive anaphora or groups (the pronoun ils in Fig. 2 refers to the group composed of Antoine and Catherine). Based on these guidelines, a common batch, composed of 10 randomly selected files, was annotated by all the annotators. It was used to identify annotation divergences between annotators1 and to correct the annotation guide. We computed the overall inter-annotator agreement on this common batch using the mean Krippendorff’s alpha on each text and we obtained 0.47, which corresponds to a moderate agreement between annotators. Such value is however not unusual in co-reference annotation (Muzerelle et al., 2014). Then, following the annotation guide, each expert annotated a batch of 12 texts from the corpus. At the end of the process, the principal annotator checked all batches against the guidelines, thus creating the reference for our experimentation. [Antoine] 1/S/NPr/partie(3) fait la connaissance de [Catherine] 2/CN/NPr/partie(3). [Antoine] 1/S/NPr est [un beau parleur ] 1/X/GNI et [la jeune fille] 2/S/GND [s’] 2/X/Pronref int´eresse a` [lui] 1/OI/Pron. [Ils] 3/S/Pron vont au cin´ema ensemble. Figure 2: Example of annotated data : the number of the entity, the syntactic function and the category"
C16-1094,D08-1020,0,0.15088,"tic features in their cognitive readability formulas improved performance. Chall and Dale (1995, 111) had a more mixed opinion, arguing that variables based on higher textual dimensions “discriminate better among materials requiring greater maturity in reading ability”, while classic lexico-syntactic variables work better to discriminate at lower levels of difficulty. Recently, taking advantage of the opportunities offered by Natural Language Processing (NLP) techniques, readability studies have tried to leverage the semantic and discursive properties of texts to better model text difficulty (Pitler and Nenkova, 2008; Feng et al., 2009). Among those high-level dimensions that have attracted substantial attention are the level of cohesion and coherence of texts. Although psycholinguistic experiments have shown that a higher level of cohesion and coherence between a pair of related sentences decreases their reading time (Kintsch et al., 1975; Mason and Just, 2004), the added value of these textual dimensions for readability models (compared to traditional features) remains unclear, as it will be covered in more details in Section 2. This is why this paper aims at further investigating the importance of cohe"
C16-1094,J04-3003,0,0.0378274,"anaphoric and of co-reference chain. These three devices strengthen the links between several utterances and contribute to the overall understanding of the text (Charolles, 1995). Lexical chains are effective mechanisms to find the main domain or theme of the document. Cohesive devices such as anaphora or co-reference chains correspond to one entity expressed by various linguistic expressions (so called mentions). These expressions are related by complex morpho-syntactic, syntactic or semantic constraints (Grosz et al., 1995). Mentioning the same entity several times reinforces text cohesion (Poesio et al., 2004), (Hobbs, 1979). Cohesive devices reinforce local coherence relations in some specific genres (persuasive genres) (Berzlnovich and Redeker, 2012). An interesting characteristic of cohesive devices is that their use is dependent on the type or genre of texts (Carter-Thomas, 1994). For instance, informative texts use specific referential expressions such as definite or demonstrative noun phrases as mentions, while narrative texts contain more chains composed of proper nouns or personal pronouns (Schnedecker, 2005). The composition, the length or the choice of the first mention of the co-referenc"
deleger-etal-2014-annotation,W12-4304,0,\N,Missing
deleger-etal-2014-annotation,W12-2411,0,\N,Missing
deleger-etal-2014-annotation,E12-2021,0,\N,Missing
F12-2001,C10-1089,0,0.0390148,"Missing"
F12-2001,W08-0602,0,0.0719198,"Missing"
F12-2001,W11-0201,0,0.0332349,"Missing"
F12-2001,P08-1040,0,0.0574046,"Missing"
F12-2001,D11-1038,0,0.0623467,"Missing"
F12-2001,N06-1037,0,0.0848249,"Missing"
F12-2016,C10-2013,0,0.0266869,"Missing"
F12-2016,E99-1042,0,0.125646,"Missing"
F12-2016,C96-2183,0,0.314639,"Missing"
F12-2016,Y09-1013,0,0.0206499,"Missing"
F12-2016,W09-1802,0,0.0832225,"Missing"
F12-2016,W03-1602,0,0.150315,"Missing"
F12-2016,N09-2045,0,0.0586264,"Missing"
F12-2016,levy-andrew-2006-tregex,0,0.0718403,"Missing"
F12-2016,E06-1021,0,0.082197,"Missing"
F12-2016,D11-1038,0,0.145607,"Missing"
F12-2016,C10-1152,1,0.927434,"Missing"
F13-1036,P11-2087,0,0.0575878,"Missing"
F13-1036,E99-1042,0,0.183517,"Missing"
F13-1036,W12-2202,0,0.0436198,"Missing"
F13-1036,S12-1066,0,0.0236179,"Missing"
F13-1036,S12-1068,1,0.888679,"Missing"
F13-1036,quasthoff-etal-2006-corpus,0,0.0705231,"Missing"
F13-1036,D11-1038,0,0.0383291,"Missing"
F13-1036,N10-1056,0,0.0755902,"Missing"
garcia-fernandez-etal-2014-construction,E12-1034,0,\N,Missing
garcia-fernandez-etal-2014-construction,W10-0208,0,\N,Missing
garcia-fernandez-etal-2014-construction,W10-0212,0,\N,Missing
garcia-fernandez-etal-2014-construction,Y09-1013,0,\N,Missing
garcia-fernandez-etal-2014-construction,D11-1067,0,\N,Missing
garcia-fernandez-etal-2014-construction,W10-0205,0,\N,Missing
garcia-fernandez-etal-2014-construction,H05-1073,0,\N,Missing
garcia-fernandez-etal-2014-construction,Y11-1029,0,\N,Missing
garcia-fernandez-etal-2014-construction,el-maarouf-villaneau-2012-french,0,\N,Missing
garcia-fernandez-etal-2014-construction,doukhan-etal-2012-designing,0,\N,Missing
garcia-fernandez-etal-2014-construction,P13-1167,0,\N,Missing
garcia-fernandez-etal-2014-construction,N12-1038,0,\N,Missing
L16-1035,C12-1023,0,0.0953608,"Missing"
L16-1035,W14-1206,1,0.926562,"Missing"
L16-1035,francois-etal-2014-flelex,1,0.558779,"Missing"
L16-1035,S12-1066,0,0.109604,"Missing"
L16-1035,S12-1068,1,0.885579,"Missing"
L16-1035,P13-3015,0,0.407975,"Missing"
L16-1035,shardlow-2014-open,0,0.0241181,"Missing"
L16-1035,S12-1046,0,0.0434548,"Missing"
L16-1035,C04-1146,0,0.0599767,"notated entirely by nonnative speakers. We therefore reduced the initial corpus to a smaller sample of texts, while ensuring that the texts’ vocabulary remained as much varied as possible. To this end, we used a greedy selection algorithm which retrieved from the combined corpus a subset of texts that had the best possible lexical diversity. Given a limitation to the number of lexical units allowed in the final subset, the algorithm searches in the space of all possible subsets to identify a subset of texts that portrays the least lexical overlap. Following the similarity measures proposed by Weeds et al. (2004), we compute the overlap between the vocabulary V of each pair of texts i and j using the Jaccard coefficient (Equation 1). Each vocabulary V is defined as a set of unique (lemma, POS-tag) combinations. overlap(Vi , Vj ) = |Vi ∩ Vj | |Vi ∪ Vj | (1) The algorithm uses each document d in the corpus as a starting point to build a new subset T and then iteratively integrates into T a new document t that shares the least lexical units with the documents already included. After having constructed a complete subset T , the algorithm compares this subset to the previously constructed subsets in order"
L16-1035,D11-1038,0,0.0621475,"Missing"
L16-1035,C10-1152,0,0.118631,"Missing"
L16-1366,I13-1077,0,0.0232293,"o rank; Terminology 1. Introduction The difference between the language used by health care professionals and that used by patients is cited as a source of miscommunication (Elhadad and Sutaria, 2007) or difficulty when mining patient forums (Nikfarjam et al., 2015). For example, lay people tend to use idiomatic expressions such as “mal de chien”(Fr) [literally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical students, we are creating a virtual patient who must answer student questions in a natural way, hence using patient language (Campillos-Llanos et al., 2015; Campillos-Llanos et al., 2016). This is yet another application which requires a way to assess which term among a set of equivalent expressions would be more likely to be used by a patient. Nevertheless, such resources remain scarce and thus far, only one, the"
L16-1366,W15-4660,1,0.410631,"such as “mal de chien”(Fr) [literally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical students, we are creating a virtual patient who must answer student questions in a natural way, hence using patient language (Campillos-Llanos et al., 2015; Campillos-Llanos et al., 2016). This is yet another application which requires a way to assess which term among a set of equivalent expressions would be more likely to be used by a patient. Nevertheless, such resources remain scarce and thus far, only one, the Consumer Health Vocabulary (CHV, Keselman et al. (2007)), aims at estimating how familiar a term is. However, the CHV only covers the English language, and limited attempts have been made to cover other languages such as French and Portuguese. Tapi Nzali et al. (2015) mention the creation of a French CHV, but actually address a differe"
L16-1366,L16-1505,1,0.819925,"terally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical students, we are creating a virtual patient who must answer student questions in a natural way, hence using patient language (Campillos-Llanos et al., 2015; Campillos-Llanos et al., 2016). This is yet another application which requires a way to assess which term among a set of equivalent expressions would be more likely to be used by a patient. Nevertheless, such resources remain scarce and thus far, only one, the Consumer Health Vocabulary (CHV, Keselman et al. (2007)), aims at estimating how familiar a term is. However, the CHV only covers the English language, and limited attempts have been made to cover other languages such as French and Portuguese. Tapi Nzali et al. (2015) mention the creation of a French CHV, but actually address a different problem: the identification o"
L16-1366,W09-3102,1,0.945102,"sed by health care professionals and that used by patients is cited as a source of miscommunication (Elhadad and Sutaria, 2007) or difficulty when mining patient forums (Nikfarjam et al., 2015). For example, lay people tend to use idiomatic expressions such as “mal de chien”(Fr) [literally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical students, we are creating a virtual patient who must answer student questions in a natural way, hence using patient language (Campillos-Llanos et al., 2015; Campillos-Llanos et al., 2016). This is yet another application which requires a way to assess which term among a set of equivalent expressions would be more likely to be used by a patient. Nevertheless, such resources remain scarce and thus far, only one, the Consumer Health Vocabulary (CHV, Keselman et al. (2007)), aims at estimating how fami"
L16-1366,W07-1007,0,0.0400165,"luation of this approach is conducted on 134 terms from the UMLS Metathesaurus and 868 terms from the Eugloss thesaurus. The Normalized Discounted Cumulative Gain obtained by our system is over 0.8 on both test sets. Besides, thanks to the learning-to-rank approach, adding morphological features to the language model features improves the results on the Eugloss thesaurus. Keywords: Technicality of Medical Terms; Learning to rank; Terminology 1. Introduction The difference between the language used by health care professionals and that used by patients is cited as a source of miscommunication (Elhadad and Sutaria, 2007) or difficulty when mining patient forums (Nikfarjam et al., 2015). For example, lay people tend to use idiomatic expressions such as “mal de chien”(Fr) [literally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical s"
L16-1366,W10-3304,0,0.0619834,"Missing"
L16-1433,P12-3007,0,0.226857,"Missing"
L16-1433,P05-1045,0,0.00891247,"Missing"
L16-1433,W13-4039,0,0.0534255,"Missing"
L16-1433,D13-1036,0,0.0262914,"Missing"
L18-1619,N13-1014,0,0.0243344,"nguages considered. Then, a tagset has to be created or adapted to the language, which requires linguistic expertise. Finally, the annotation also requires annotators with linguistic expertise. Crowdsourcing can be used for part-of-speech annotation (Hovy et al., 2014), and was even used for Alsatian (Millour et al., 2017). Yet, crowdsourcing necessitates an adapted platform, and communication to possible speakers, who for example in the case of Picard, are rare. A possible direction for POS tagging could be to create a minimum tag dictionary for the most frequent word types, such as used by (Garrette and Baldridge, 2013). This kind of approach still requires a test corpus to evaluate the tagger; and the performance remains low compared to more resourced languages. 6. Conclusion We have presented our methodology for producing corpora with POS annotations for three regional languages of France, namely Alsatian, Occitan and Picard. The tagsets are based on an extended version of the Universal POS tags, with some language-specific additions to account for particular linguistic phenomena. The annotation guidelines as well as the manually annotated corpora are freely available. We plan to use these corpora to devel"
L18-1619,P14-2062,0,0.0289724,"difficulties. First, it requires assembling a large textual corpus, which can be a challenge for these languages which have few electronic resources. Work on part-of-speech tagging for under-resourced languages is often based on parallel corpora, following (Yarowsky et al., 2001), but there are no such existing electronic corpora for the three languages considered. Then, a tagset has to be created or adapted to the language, which requires linguistic expertise. Finally, the annotation also requires annotators with linguistic expertise. Crowdsourcing can be used for part-of-speech annotation (Hovy et al., 2014), and was even used for Alsatian (Millour et al., 2017). Yet, crowdsourcing necessitates an adapted platform, and communication to possible speakers, who for example in the case of Picard, are rare. A possible direction for POS tagging could be to create a minimum tag dictionary for the most frequent word types, such as used by (Garrette and Baldridge, 2013). This kind of approach still requires a test corpus to evaluate the tagger; and the performance remains low compared to more resourced languages. 6. Conclusion We have presented our methodology for producing corpora with POS annotations fo"
L18-1619,C94-1097,0,0.819681,"Missing"
L18-1619,L16-1262,0,0.116016,"Missing"
L18-1619,W14-5303,1,0.882391,"Missing"
L18-1619,H01-1035,0,0.160961,"potlights that they lit the pit . Table 10: Annotation example for Picard The annotation guidelines and the corpora are available for all three languages on the Zenodo platform, in the RESTAURE project community (see Section 9. for the corpus list).17 5. Related Work Creating annotated corpora for under-resourced languages presents several difficulties. First, it requires assembling a large textual corpus, which can be a challenge for these languages which have few electronic resources. Work on part-of-speech tagging for under-resourced languages is often based on parallel corpora, following (Yarowsky et al., 2001), but there are no such existing electronic corpora for the three languages considered. Then, a tagset has to be created or adapted to the language, which requires linguistic expertise. Finally, the annotation also requires annotators with linguistic expertise. Crowdsourcing can be used for part-of-speech annotation (Hovy et al., 2014), and was even used for Alsatian (Millour et al., 2017). Yet, crowdsourcing necessitates an adapted platform, and communication to possible speakers, who for example in the case of Picard, are rare. A possible direction for POS tagging could be to create a minimu"
P13-2076,W06-1906,0,0.0402062,"Missing"
P13-2076,C02-1150,0,0.0942134,"Many methods are based on supervised machine learning which is made possible by the great amount of resources for this language. While developing a question answering system for French, we were thus limited by the lack of resources for this language. Some were created, for example for answer validation (Grappy et al., 2011). Yet, for question classification, although question corpora in French exist, only a small part of them is annotated with question classes, and such an annotation is costly. We thus wondered if it was possible to use existing English corpora, in this case the data used in (Li and Roth, 2002), to create a classification module for French. Transfering knowledge from one language to another is usually done by exploiting parallel corpora; yet in this case, few such corpora exists (CLEF QA datasets could be used, but question classes are not very precise). We thus investigated the possibility of using machine translation to create a parallel corpus, as has been done for spoken This paper is organized as follows: The problem of Question Classification is defined in section 2. The proposed methods are presented in section 3, and the experiments in section 4. Section 5 details the relate"
P13-2076,C04-1201,0,0.0600453,"Missing"
pho-etal-2014-multiple,W12-2704,0,\N,Missing
pho-etal-2014-multiple,P03-1054,0,\N,Missing
pho-etal-2014-multiple,P13-2076,1,\N,Missing
pho-etal-2014-multiple,E12-2021,0,\N,Missing
pho-etal-2014-multiple,W06-1416,0,\N,Missing
pho-etal-2014-multiple,P05-1045,0,\N,Missing
R11-1086,W08-0602,0,0.305199,"ve for precision without broad generalization capacity. So, other approaches are based on supervised machine learning. (Uzuner et al., 2010) use SVM (Support Vector Machines) to class relations between medical problems, tests and treatments in clinical reports. They defined surface features (ordering of the concepts, distance, etc.), lexical features (lexical trigrams, tokens-in-concepts, etc.), and shallow syntactic features (verbs, syntactic bigrams, syntactic link path, etc.). Results show an F-measure from 0.60 to 0.85, but for under-represented relations the classification did not work. (Roberts et al., 2008) also use a SVM to extract relations in the corpus of the Clinical E-Science Framework (CLEF) project that hold between entities (e.g. condition, drug, result) and modifiers (e.g. negation) in clinical records of cancer patients. There are seven classes of relations and each entity pair can be linked by one relation only (except between an investigation and a condition). So the classification task is considered as a binary classification (i.e. the detection of relation) between a type of relation and the nonrelation class. The classification is also based on lexical, morpho-syntactic and seman"
R11-1086,P05-1053,0,0.460645,"n the corpus of the Clinical E-Science Framework (CLEF) project that hold between entities (e.g. condition, drug, result) and modifiers (e.g. negation) in clinical records of cancer patients. There are seven classes of relations and each entity pair can be linked by one relation only (except between an investigation and a condition). So the classification task is considered as a binary classification (i.e. the detection of relation) between a type of relation and the nonrelation class. The classification is also based on lexical, morpho-syntactic and semantic features. In the general domain, (Zhou et al., 2005) use SVM to identify relations between people, organizations and places, etc. on the ACE corpus. Our system also uses SVM to classify finegrained relations. We make use of classical features as well as features specific to the domain, as the semantic types of the UMLS3 and medical abbreviation lists, and features specific to the writing style of texts, for handling concept coordination. 3 TrNAP TrWP TrCP TrAP TeCP TeRP PIP Table 1: The eight relations to identify and test (749 instances of relations). For the final evaluation, i2b2 organizers gave participants a corpus of 477 documents (9070 i"
S12-1068,C10-1152,1,0.84074,"utions performed by non-native English speakers, we tried to use linguistic resources that best fit this kind of data. In this way, we made the hypothesis that training our system on documents written by or written for nonnative English speakers would be useful. The use of the Simple English version from Wikipedia seems to be a good solution as it is targeted at people who do not have English as their mother tongue. Our hypothesis seems to be correct due to the results we obtained. Morevover, the Simple English Wikipedia has been used previously in work on automatic text simplification, e.g. (Zhu et al., 2010). 1 http://infolingu.univ-mlv.fr/ DonneesLinguistiques/Dictionnaires/ telechargement.html First, we produced a plain text version of the Simple English Wikipedia. We downloaded the dump dated February 27, 2012 and extracted the textual contents using the wikipedia2text tool.2 The final plaintext file contains approximately 10 million words. We extracted word n-grams (n ranging from 1 to 3) and their frequencies from this corpus thanks to the Text-NSP Perl module 3 and its count.pl program, which produces the list of n-grams of a document, with their frequencies. Table 1 gives the number of n-g"
S12-1068,S12-1046,0,\N,Missing
W06-1904,1999.tc-1.8,0,0.0248958,"ies, the only way for us to get their translation is to combine all the different term translations. The main drawback of this approach is the generated noise, for none of the terms constituting the biterm is disambiguated. For example, three different translations are found for the biterm Conseil de d´efense : defense council, defense advice and defense counsel ; but only the first of those should be finally retained by our system. To reduce this noise, an interesting possibility is to validate the obtained biterms by searching them or their variants in the complete collection of documents. (Grefenstette, 1999) reports a quite similar experiment in the context of a machine translation task : he uses the Web in order to order the possible translations of noun phrases, and in particular noun biterms. Fastr (Jacquemin, 1996) is a parser which takes as input a corpus and a list of terms (multi or monoterms) and outputs the indexed corpus in which terms and their variants are recognized. Hence, Fastr is quite adequate for biterms validation : it tags all the biterms present in the collection, whether in their original form or in a variant that can be semantic or syntactic. In order to validate the biterm"
W06-1904,2006.jeptalnrecital-long.20,1,0.820536,"Missing"
W09-4504,M98-1001,0,0.0400933,"in the texts, and link them to QKDB values, mostly automatically, for example with WordNet (for some inflections and derivations). The different variations of a term will thus be normalized to a standard form. Besides recognizing the terms of the domain, we will have to work on the selection of relevant results, so that an annotator who will use our assistant tool will not have too erroneous propositions to discard. We will have to define with experts where to draw the line between recall over precision. 5 Relevant work Template based IE systems were developped during the MUC conferences (see [1] for MUC-7 definition task and [7] for MUC-7 results). In MUC7, the Template Relation Task was dedicated to extract relational information on employee of, manufacture of, and location of relations as the Scenario Template Task consisted in extracting prespecified event information and relating the event information to particular organizations, persons, or artifact entities involved in the event. Some of these systems as LaSIE [6] have been adapted to extract biological information, designing 26 PASTA [3]. PASTA aim at extracting information about the roles of residues in protein molecules. The"
W09-4504,W02-0311,0,0.163446,"be decomposed into two tasks: QKDB website: http://physiome.ibisc.fr/qkdb/ • highlighting the including passages and the values of the descriptors for a selected result. The information we look for can be modelled by a template that represents the description of an experimentation in kidney studies. Even if many systems apply IE techniques to scientific papers, they are generally dedicated to the domain of molecular biology and they often look for specific entities and some relations between these entities and not for a complex template. We can find such a problem in systems (see for example [3]) issued from MUC evaluations [6], in which most entities were named entities such as person, organization or location names, that can be recognized using gazetteers and rules relying on linguistic features. In our case, if the result value corresponds to a named entity, other descriptors are domain-specific terms, whose recognition would require to refer to an ontology dedicated to this domain that does not exist currently. Furthermore, it also requires the modelling of the relations between an experimentation and each of its descriptors. Most systems only use abstracts for the extraction tas"
W09-4504,M98-1007,0,0.384286,"DB website: http://physiome.ibisc.fr/qkdb/ • highlighting the including passages and the values of the descriptors for a selected result. The information we look for can be modelled by a template that represents the description of an experimentation in kidney studies. Even if many systems apply IE techniques to scientific papers, they are generally dedicated to the domain of molecular biology and they often look for specific entities and some relations between these entities and not for a complex template. We can find such a problem in systems (see for example [3]) issued from MUC evaluations [6], in which most entities were named entities such as person, organization or location names, that can be recognized using gazetteers and rules relying on linguistic features. In our case, if the result value corresponds to a named entity, other descriptors are domain-specific terms, whose recognition would require to refer to an ontology dedicated to this domain that does not exist currently. Furthermore, it also requires the modelling of the relations between an experimentation and each of its descriptors. Most systems only use abstracts for the extraction task; only few of them analyse full-"
W09-4504,M98-1002,0,0.051171,"B values, mostly automatically, for example with WordNet (for some inflections and derivations). The different variations of a term will thus be normalized to a standard form. Besides recognizing the terms of the domain, we will have to work on the selection of relevant results, so that an annotator who will use our assistant tool will not have too erroneous propositions to discard. We will have to define with experts where to draw the line between recall over precision. 5 Relevant work Template based IE systems were developped during the MUC conferences (see [1] for MUC-7 definition task and [7] for MUC-7 results). In MUC7, the Template Relation Task was dedicated to extract relational information on employee of, manufacture of, and location of relations as the Scenario Template Task consisted in extracting prespecified event information and relating the event information to particular organizations, persons, or artifact entities involved in the event. Some of these systems as LaSIE [6] have been adapted to extract biological information, designing 26 PASTA [3]. PASTA aim at extracting information about the roles of residues in protein molecules. The extraction task consists of filli"
W09-4504,M95-1017,0,\N,Missing
W14-1206,W03-1602,0,0.0795097,"ilar to the Wikipedia-Vikidia corpus. The two corpora created are relevant for a manual analysis, as done in the next section, but they are too small for automatic processing. We plan to implement a method to align automatically the narrative texts in the near future and thus be able to collect a larger corpus. 2.2 levels of transformation: lexical, discursive and syntactic, which can be further divided into subcategories. It is worth mentioning that in previous work, simplification is commonly regarded as pertaining to two categories of phenomena: lexical and syntactic (Carroll et al., 1999; Inui et al., 2003; De Belder and Moens, 2010). Little attention has been paid to discourse in the area of automatic simplification (Siddharthan, 2006). The typology is summarized in Table 1. As regards the lexicon, the phenomena we observed involve four types of substitution. First, difficult terms can be replaced by a synonym or an hypernym perceived as simpler. Second, some anaphoric expressions, considered simpler or more explicit, are preferred to their counterparts in the original texts. For example, in our three tales, simplified nominal anaphora are regularly used instead of pronominal anaphora. Third,"
W14-1206,N09-2045,0,0.0696264,"escribe a general typology of simplification derived from our corpora (Section 2.2). Then, we present the system based on the syntactic part of the typology, which operates in two steps: overgeneration of all possible simplified sentences (Section 2.3.1) and selection of the best subset of candidates using readability criteria (Section 2.3.2) and ILP. Finally, we evaluate the quality of the syntactically simplified sentences as regards grammaticality, before performing some error analysis (Section 3). of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts) and the Simple English Wikiped"
W14-1206,levy-andrew-2006-tregex,0,0.0172946,"ies global changes to the text. For instance, when a simple past is replaced by a present form, we must also adapt the verbs in the surrounding context in accordance with tense agreement. This requires to consider the whole text, or at least the paragraph that contains the modified verbal form, and be able to automatically model tense agreement. Otherwise, we may alter the coherence of the text and decrease its readability. This leaves us with 19 simplification rules.6 To apply them, the candidate structures for simplification first need to be detected using regular expressions, via Tregex 7 (Levy and Andrew, 2006) that allows the retrieval of elements and relationships in a parse tree. In a second step, syntactic trees in which a structure requires simplification are modified according a set of operations implemented through Tsurgeon. The operations to perform depend on the type of rules: 1. For the deletion cases, simply deleting all the elements involved is sufficient (via the delete operation in Tsurgeon). The elements affected by the deletion rules are adverbial clauses, clauses between brackets, some of the subordinate clauses, clauses between commas or introduced by words such as “comme” (as), “v"
W14-1206,E06-1021,0,0.0743117,"terms of language and content. It is available at the address http://fr.vikidia.org 48 WikiExtractor 3 was then applied to the articles to discard the wiki syntax and only keep the raw texts. This corpus comprises 13,638 texts (7,460 from Vikidia and only 6,178 from Wikipedia, since some Vikidia articles had no counterpart in Wikipedia). These articles were subsequently processed to identify parallel sentences (Wikipedia sentence with a simplified equivalent in Vikidia). The alignment has been made partly manually and partly automatically with the monolingual alignment algorithm described in Nelken and Shieber (2006), which relies on a cosine similarity between sentence vectors weighted with the tf-idf. This program outputs alignments between sentences, along with a confidence score. Among these files, twenty articles or excerpts from Wikipedia were selected along with their equivalent in Vikidia. This amounts to 72 sentences for the former and 80 sentences for the latter. The second corpus is composed of 16 narrative texts, and more specifically tales, by Perrault, Maupassant, and Daudet. We used tales since their simplified version was closer to the original than those of longer novels, which made the s"
W14-1206,seretan-2012-acquisition,0,0.0654496,"ia (for original texts) and the Simple English Wikipedia, a simplified version for children and non-native speakers that currently comprises more than 100,000 articles. Similar resources exist for French, such as Vikidia and Wikimini, but texts are far less numerous in these as in their English counterpart. Moreover, the original and simplified versions of an article are not strictly parallel, which further complicates machine learning. This is why, so far, there was no attempt to adapt this machine learning methodology to French. The only previous work on French, to our knowledge, is that of Seretan (2012), which analysed a corpus of newspapers to semi-automatically detect complex structures that has to be simplified. However, her system of rules has not been implemented and evaluated. 2 Methodology 2.1 Corpus Description We based our typology of simplification rules on the analysis of two corpora. More specifically, since our aim is to identify and classify the various strategies used to transform a complex sentence into a more simple one, the corpora had to include parallel sentences. The reason why we analysed two corpora is to determine whether different genres of texts lead to different si"
W14-1206,W13-1502,0,0.169277,"Missing"
W14-1206,C12-1023,0,0.108235,"Missing"
W14-1206,candito-etal-2010-statistical,0,0.0266653,"Missing"
W14-1206,E99-1042,0,0.167523,"us a size roughly similar to the Wikipedia-Vikidia corpus. The two corpora created are relevant for a manual analysis, as done in the next section, but they are too small for automatic processing. We plan to implement a method to align automatically the narrative texts in the near future and thus be able to collect a larger corpus. 2.2 levels of transformation: lexical, discursive and syntactic, which can be further divided into subcategories. It is worth mentioning that in previous work, simplification is commonly regarded as pertaining to two categories of phenomena: lexical and syntactic (Carroll et al., 1999; Inui et al., 2003; De Belder and Moens, 2010). Little attention has been paid to discourse in the area of automatic simplification (Siddharthan, 2006). The typology is summarized in Table 1. As regards the lexicon, the phenomena we observed involve four types of substitution. First, difficult terms can be replaced by a synonym or an hypernym perceived as simpler. Second, some anaphoric expressions, considered simpler or more explicit, are preferred to their counterparts in the original texts. For example, in our three tales, simplified nominal anaphora are regularly used instead of pronomina"
W14-1206,C96-2183,0,0.874017,"Missing"
W14-1206,Y09-1013,0,0.0159614,"f syntactic simplification for French sentences. The simplification is performed as a two-step process. First, for each sentence of the text, we generate the set of all possible simplifications (overgeneration step), and then, we select the best subset of simplified sentences using several criteria. 2.3.1 Generation of the Simplified Sentences The sentence overgeneration module is based on a set of rules (19 rules), which rely both on morphosyntactic features of words and on syntactic relationships within sentences. To obtain this information, the texts from our corpus are analyzed by MELT 4 (Denis and Sagot, 2009) and Bonsai 5 (Candito et al., 2010) during a preprocessing phase. As a result, texts are represented as syntax trees that include the information necessary to apply our simplification rules. After preprocessing, the set of simplification rules is applied recursively, one sentence at a time, until there is no further structure to simplify. All simplified sentences produced by a given rule are saved and gathered in a set of variants. The rules for syntactic simplification included in our program are of three kinds: deletion rules (12 rules), modification rules (3 rules) and splitting rules (4 r"
W14-1206,D12-1043,1,0.897268,"Missing"
W14-1206,W09-1802,0,0.0352869,"e more than one simplified variant for a given sentence. In this case, the next step consists in selecting the most suitable variant to substitute the original one. The selection process is described in the next section. 2.3.2 Selection of the Best Simplifications Given a set of candidate simplified sentences for a text, our goal is to select the best subset of simplified sentences, that is to say the subset that maximizes some measure of readability. More precisely, text readability is measured through different criteria, which are optimized with an Integer Linear Programming (ILP) approach (Gillick and Favre, 2009). These criteria are rather simple in 6 These 19 rules are available at http://cental.fltr.ucl.ac.be/team/ lbrouwers/rules.pdf 7 http://nlp.stanford.edu/software/tregex.shtml 8 This software is available at the address http://sarrazip.com/dev/ verbiste.html under GNU general public license and was developed by Pierre Sarrazin. 51 this approach. They are used to ensure that not only the syntactic difficulty, but also the lexical complexity decrease, since syntactic transformations may cause lexical or discursive alterations in the text. We considered four criteria to select the most suitable se"
W14-1206,D11-1038,0,0.639109,"the syntactically simplified sentences as regards grammaticality, before performing some error analysis (Section 3). of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts) and the Simple English Wikipedia, a simplified version for children and non-native speakers that currently comprises more than 100,000 articles. Similar resources exist for French, such as Vikidia and Wikimini, but texts are far less numerous in these as in their English counterpart. Moreover, the original and simplified versions of an article are not strictly parallel, which further complicates machine learning. Thi"
W14-1206,C10-1152,1,0.964006,"lly, we evaluate the quality of the syntactically simplified sentences as regards grammaticality, before performing some error analysis (Section 3). of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts) and the Simple English Wikipedia, a simplified version for children and non-native speakers that currently comprises more than 100,000 articles. Similar resources exist for French, such as Vikidia and Wikimini, but texts are far less numerous in these as in their English counterpart. Moreover, the original and simplified versions of an article are not strictly parallel, which"
