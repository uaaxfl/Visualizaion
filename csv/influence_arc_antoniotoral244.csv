2011.eamt-1.11,P07-2045,0,0.00648783,"Missing"
2011.eamt-1.11,N06-1014,0,0.0304209,"ents. It generates only 1:1 alignments. This tool takes three parameters (filenames for the source corpus, target corpus and an alignment probability threshold). All of them are offered by the webservice developed. The threshold is set to 0.5 by default. The output of this tool was altered by modifying the script filter-final-aligned-sents.pl 5 http://nlp.cs.nyu.edu/GMA/ http://members.unine.ch/jacques.savoy/ clef/index.html 7 http://research.microsoft. com/en-us/downloads/ aafd5dcf-4dcc-49b2-8a22-f7055113e656/ 6 http://mokk.bme.hu/resources/hunalign 64 2.2.2 BerkeleyAligner BerkeleyAligner9 (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) is a word alignment toolkit combining unsupervised as well as supervised approaches to word alignment. It features joint training of conditional alignment models (cross-EM), syntactic distortion model, as well as posterior decoding heuristics. Similarly to GIZA++, this tool provides a plethora of options (see documentation/manual.txt in its distribution for details). Most of them are not considered in the webservice interface. The only parameters that are offered by the webservice are the mandatory ones following the common interface and the num"
2011.eamt-1.11,W96-0201,0,0.0261497,"flows due to the common interface designed within the project. Table 1 describes the mandatory parameters of the interface shared across all the webservices. In addition, a webservice might accept optional parameters that allow to exploit specific functionality of the aligner wrapped by that webservice. Name source language source corpus source language target corpus 2.1.2 GMA GMA – Geometric Mapping and Alignment (Argyle et al., 2004)5 – is an implementation of the Smooth Injective Map Recognizer (Melamed, 1997) algorithm for mapping bitext correspondence and the Geometric Segment Alignment (Melamed, 1996) post-processor for converting general bitext maps to monotonic segment alignments. The tool employs word correspondences, cognates, as well as information from bilingual dictionaries. This tool accepts a pair of parameters for the source and target corpus. Apart from that, it needs a parameter pointing to a configuration file, which contains several parameters, including language-dependent lists of stop words. The webservice offers two parameters for the source and target languages; these denote a language pair, which is internally assigned a configuration file. GMA provides configuration fil"
2011.eamt-1.11,P97-1063,0,0.0245084,"for a range of widely-used state-of-the-art aligners which can be easily exchanged in user workflows due to the common interface designed within the project. Table 1 describes the mandatory parameters of the interface shared across all the webservices. In addition, a webservice might accept optional parameters that allow to exploit specific functionality of the aligner wrapped by that webservice. Name source language source corpus source language target corpus 2.1.2 GMA GMA – Geometric Mapping and Alignment (Argyle et al., 2004)5 – is an implementation of the Smooth Injective Map Recognizer (Melamed, 1997) algorithm for mapping bitext correspondence and the Geometric Segment Alignment (Melamed, 1996) post-processor for converting general bitext maps to monotonic segment alignments. The tool employs word correspondences, cognates, as well as information from bilingual dictionaries. This tool accepts a pair of parameters for the source and target corpus. Apart from that, it needs a parameter pointing to a configuration file, which contains several parameters, including language-dependent lists of stop words. The webservice offers two parameters for the source and target languages; these denote a"
2011.eamt-1.11,moore-2002-fast,0,0.0299673,"This tool requires three parameters (filenames for the source corpus, target corpus and bilingual dictionary, although the dictionary file can be empty). As a webservice, the first two parameters are mandatory, together with the source and target languages. The bilingual dictionary file is optional, if none is provided the webservice will create and use an empty file. Hunalign also provides a set of optional parameters. Some of them are offered by the webservice (bisent, cautious and text), while two of them are activated internally (realign and 4 2.1.3 BSA BSA – Bilingual Sentence Aligner7 (Moore, 2002) – is a three-step hybrid approach. First, sentence-length based alignment is performed; second, statistical word alignment model is trained on the high probability aligned sentences and third, all sentences are realigned based on the word alignments. It generates only 1:1 alignments. This tool takes three parameters (filenames for the source corpus, target corpus and an alignment probability threshold). All of them are offered by the webservice developed. The threshold is set to 0.5 by default. The output of this tool was altered by modifying the script filter-final-aligned-sents.pl 5 http://"
2011.eamt-1.11,P07-1003,0,0.0126663,"nly 1:1 alignments. This tool takes three parameters (filenames for the source corpus, target corpus and an alignment probability threshold). All of them are offered by the webservice developed. The threshold is set to 0.5 by default. The output of this tool was altered by modifying the script filter-final-aligned-sents.pl 5 http://nlp.cs.nyu.edu/GMA/ http://members.unine.ch/jacques.savoy/ clef/index.html 7 http://research.microsoft. com/en-us/downloads/ aafd5dcf-4dcc-49b2-8a22-f7055113e656/ 6 http://mokk.bme.hu/resources/hunalign 64 2.2.2 BerkeleyAligner BerkeleyAligner9 (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) is a word alignment toolkit combining unsupervised as well as supervised approaches to word alignment. It features joint training of conditional alignment models (cross-EM), syntactic distortion model, as well as posterior decoding heuristics. Similarly to GIZA++, this tool provides a plethora of options (see documentation/manual.txt in its distribution for details). Most of them are not considered in the webservice interface. The only parameters that are offered by the webservice are the mandatory ones following the common interface and the number of iterations to run"
2011.eamt-1.11,J03-1002,0,0.00596422,"Missing"
2011.eamt-1.11,P91-1023,0,0.187551,"d for English, French, German, Spanish and Italian have been obtained from Universit´e de Neuchˆatel.6 Type 2 character ISO code text 2 character ISO code text Table 1: Shared mandatory parameters. Webservices created for sentential aligners (Hunalign, GMA and BSA) are covered in section 2.1, while section 2.2 deals with sub-sentential aligners (GIZA++, BerkeleyAligner and OpenMaTrEx chunk aligner). 2.1 2.1.1 Sentential alignment Hunalign Hunalign (Varga et al., 2005)4 can work in two modes. If a bilingual dictionary is available, this information is combined with sentence-length information (Gale and Church, 1991) and used to identify sentence alignment. In the absence of a bilingual dictionary, it first identifies the alignment using sentence-length information only, then builds an automatic dictionary based on this alignment and finally realigns the text using this dictionary. This tool requires three parameters (filenames for the source corpus, target corpus and bilingual dictionary, although the dictionary file can be empty). As a webservice, the first two parameters are mandatory, together with the source and target languages. The bilingual dictionary file is optional, if none is provided the webs"
2011.eamt-1.11,P09-1104,0,0.0224796,"tool takes three parameters (filenames for the source corpus, target corpus and an alignment probability threshold). All of them are offered by the webservice developed. The threshold is set to 0.5 by default. The output of this tool was altered by modifying the script filter-final-aligned-sents.pl 5 http://nlp.cs.nyu.edu/GMA/ http://members.unine.ch/jacques.savoy/ clef/index.html 7 http://research.microsoft. com/en-us/downloads/ aafd5dcf-4dcc-49b2-8a22-f7055113e656/ 6 http://mokk.bme.hu/resources/hunalign 64 2.2.2 BerkeleyAligner BerkeleyAligner9 (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) is a word alignment toolkit combining unsupervised as well as supervised approaches to word alignment. It features joint training of conditional alignment models (cross-EM), syntactic distortion model, as well as posterior decoding heuristics. Similarly to GIZA++, this tool provides a plethora of options (see documentation/manual.txt in its distribution for details). Most of them are not considered in the webservice interface. The only parameters that are offered by the webservice are the mandatory ones following the common interface and the number of iterations to run the model (default valu"
2011.eamt-1.4,W05-0909,0,0.233131,"Missing"
2011.eamt-1.4,E06-1032,0,0.0374124,"ystems over rule-based ones (CallisonBurch et al., 2006). Using BLEU is fast and intuitive, but while this metric has been shown to produce good correlations with human judgment at the document level (Papineni et al., 2002), especially when a large number of reference translations are available, correlation at sentence level is generally low. The NIST evaluation metric (Doddington, 2002) is also string-based, and gives more weight in the evaluation to less frequent n-grams. While this metric has a strong bias in favour of statistical systems, it provides better adequacy correlation than BLEU (Callison-Burch et al., 2006). The GTM metric (Turian et al., 2003) is based on standard measures adopted in other NLP applications (precision, recall and F-measure), which makes its use rather straightforward for NLP practitioners. It focuses on unigrams and rewards sequences of correct unigrams, applying moderate penalties for incorrect word order. METEOR (Banerjee and Lavie, 2005) uses stemming and synonymy relations to provide a 14 more fine-grained evaluation at the lexical level, which reduces its bias towards statistical systems. One drawback of this metric is that it is language-dependent since it requires a stemm"
2011.eamt-1.4,niessen-etal-2000-evaluation,0,0.060211,"r et al., 2006) adopts a different approach, in that it computes the number of substitutions, insertions, deletions and shifts that are required to modify the output translation so that it completely matches the reference translation(s). Its results are affected less by the number of reference translations than is the case for BLEU, and the rationale behind this evaluation metric is quite simple to understand for people who are not MT experts, as it provides an estimation of the amount of post-editing effort needed by an end-user. Another metric based on error rates which preceded TER is WER (Nießen et al., 2000). We omitted WER and its extension mWER (Nießen et al., 2000) from the experiments reported here as they seem to have been superceded by more recent metrics. TER-plus (Snover et al., 2009) is an extension of TER using phrasal substitutions relying on automatically generated paraphrases, stemming, synonyms and relaxed shifting constraints. This metric is language-dependent and requires WordNet. It has been shown to have the highest average rank in terms of Pearson and Spearman correlation (Przybocki et al., 2008). The DCU-LFG metric (Owczarzak et al., 2007) exploits LFG dependencies and has onl"
2011.eamt-1.4,W07-0718,0,0.0281462,"verview of some of the most widely used automatic MT evaluation metrics, discussing their advantages as well as drawbacks, laying particular emphasis on the metrics used in the comparative evaluation presented in Section 5. The performance of the CoSyne MT system in the early stages of its development can be measured, and its improvement can be monitored over time, against these standard metrics in a reliable and replicable fashion. To ensure the best possible coverage, we decided to use a wide array of metrics, particularly those judged best by recent meta-evaluation exercises (e.g. Callison-Burch et al., 2007; Callison-Burch et al., 2010), without confining ourselves to prominent n-gram based metrics. Since there is no consensus on a single individual metric which is thought to accurately measure MT performance, we decided to adopt an inclusive approach, considering the results of a variety of measures. This should provide a picture that is as reliable and fine-grained as possible. One of the most widely used automatic MT evaluation metrics is BLEU (Papineni et al., 2002), a string-based metric which has come to represent something of a de facto standard in the last few years. This is not surprisi"
2011.eamt-1.4,W07-0411,1,0.87845,"Missing"
2011.eamt-1.4,P02-1040,0,0.0893689,"erage, we decided to use a wide array of metrics, particularly those judged best by recent meta-evaluation exercises (e.g. Callison-Burch et al., 2007; Callison-Burch et al., 2010), without confining ourselves to prominent n-gram based metrics. Since there is no consensus on a single individual metric which is thought to accurately measure MT performance, we decided to adopt an inclusive approach, considering the results of a variety of measures. This should provide a picture that is as reliable and fine-grained as possible. One of the most widely used automatic MT evaluation metrics is BLEU (Papineni et al., 2002), a string-based metric which has come to represent something of a de facto standard in the last few years. This is not surprising given that today most MT research and development efforts are concentrated on statistical approaches; BLEU’s critics argue that it tends to favour statistical systems over rule-based ones (CallisonBurch et al., 2006). Using BLEU is fast and intuitive, but while this metric has been shown to produce good correlations with human judgment at the document level (Papineni et al., 2002), especially when a large number of reference translations are available, correlation"
2011.eamt-1.4,W10-1751,0,0.018734,"tioners. It focuses on unigrams and rewards sequences of correct unigrams, applying moderate penalties for incorrect word order. METEOR (Banerjee and Lavie, 2005) uses stemming and synonymy relations to provide a 14 more fine-grained evaluation at the lexical level, which reduces its bias towards statistical systems. One drawback of this metric is that it is language-dependent since it requires a stemmer and WordNet,3 and it can currently be applied in full only to English, and partly to French, Spanish and Czech, due to the limited availability of synonymy and paraphrase modules. METEORNEXT (Denkowski and Lavie, 2010) is an updated version of the same metric. The TER metric (Snover et al., 2006) adopts a different approach, in that it computes the number of substitutions, insertions, deletions and shifts that are required to modify the output translation so that it completely matches the reference translation(s). Its results are affected less by the number of reference translations than is the case for BLEU, and the rationale behind this evaluation metric is quite simple to understand for people who are not MT experts, as it provides an estimation of the amount of post-editing effort needed by an end-user."
2011.eamt-1.4,2006.amta-papers.25,0,0.0482843,"oderate penalties for incorrect word order. METEOR (Banerjee and Lavie, 2005) uses stemming and synonymy relations to provide a 14 more fine-grained evaluation at the lexical level, which reduces its bias towards statistical systems. One drawback of this metric is that it is language-dependent since it requires a stemmer and WordNet,3 and it can currently be applied in full only to English, and partly to French, Spanish and Czech, due to the limited availability of synonymy and paraphrase modules. METEORNEXT (Denkowski and Lavie, 2010) is an updated version of the same metric. The TER metric (Snover et al., 2006) adopts a different approach, in that it computes the number of substitutions, insertions, deletions and shifts that are required to modify the output translation so that it completely matches the reference translation(s). Its results are affected less by the number of reference translations than is the case for BLEU, and the rationale behind this evaluation metric is quite simple to understand for people who are not MT experts, as it provides an estimation of the amount of post-editing effort needed by an end-user. Another metric based on error rates which preceded TER is WER (Nießen et al.,"
2011.eamt-1.4,W09-0441,0,0.0132577,"completely matches the reference translation(s). Its results are affected less by the number of reference translations than is the case for BLEU, and the rationale behind this evaluation metric is quite simple to understand for people who are not MT experts, as it provides an estimation of the amount of post-editing effort needed by an end-user. Another metric based on error rates which preceded TER is WER (Nießen et al., 2000). We omitted WER and its extension mWER (Nießen et al., 2000) from the experiments reported here as they seem to have been superceded by more recent metrics. TER-plus (Snover et al., 2009) is an extension of TER using phrasal substitutions relying on automatically generated paraphrases, stemming, synonyms and relaxed shifting constraints. This metric is language-dependent and requires WordNet. It has been shown to have the highest average rank in terms of Pearson and Spearman correlation (Przybocki et al., 2008). The DCU-LFG metric (Owczarzak et al., 2007) exploits LFG dependencies and has only a moderate bias towards statistical systems. It requires a dependency parser. It should be noted that among the above measures, METEOR, METEOR-NEXT, TER-plus and DCU-LFG can only be used"
2011.eamt-1.4,2007.mtsummit-papers.27,1,0.605945,"our free online MT systems were used for the baseline evaluation of the CoSyne MT system developed by the University of Amsterdam (Martzoukos and Monz, 2010): • Google Translate13 • Bing Translator14 • Systran15 • FreeTranslation16 These four online MT services were selected first of all because they all cover the three language pairs addressed in year 1 of the CoSyne project (German—English, Italian—English and Dutch—English in both directions). In addition, these are among the most popular free web-based MT systems and are heavily used by the general public of Internet users (Gaspari, 2006; Gaspari and Hutchins, 2007). A final consideration was that three of these five systems are statistical (CoSyne, Google Translate and Bing Translator), while the other two are rule-based (FreeTranslation and Systran). As a result, this mixture of systems offers a good picture of the MT quality currently offered by state-of-the-art representatives of both approaches. 4 Dutch—English NISV provided three different data sets: België Diplomatie consists of 418 HTML document pairs extracted from the Belgian Foreign Affairs website.9 • Video Active is an XML file containing 1,076 document pairs concerning the description of te"
2011.eamt-1.4,2003.mtsummit-papers.51,0,0.0834814,"Missing"
2011.eamt-1.4,W10-1753,1,0.874768,"Missing"
2011.eamt-1.4,C08-1141,0,0.0128615,"and/or language directions needing improvement. By repeating evaluations based on the well-established metrics presented in Section 2 at regular intervals, the improvement of the CoSyne MT system will be gradually monitored and its overall success measured. This evaluation study has shown that rulebased MT systems are outperformed by statistical MT systems for data from the news domain. Plans currently underway to extend the evaluation of the CoSyne MT system include the development of a methodology for diagnostic MT evaluation based on linguistic checkpoints, similar to the one presented in Zhou et al. (2008), who used an ad-hoc tool called Woodpecker. 0.9000 0.8000 0.7000 0.6000 Google Bing Systran Freetranslation CoSyne M12 0.5000 0.4000 0.3000 0.2000 0.1000 0.0000 BLEU NIST METEOR METEOR-NEXT TERp TER GTM DCU-LFG For the Dutch—English translation task, the three statistical MT systems consistently and clearly outperform Systran and FreeTranslation based on all the automatic evaluation metrics. Google outperforms Bing for only three of the metrics (NIST, TER and GTM), whereas for the others Bing receives the higher score. Interestingly, based on TER, the CoSyne MT system does better than Bing, b"
2011.eamt-1.4,P04-1077,0,0.0745498,"Missing"
2011.eamt-1.4,W10-1703,0,\N,Missing
2011.eamt-1.4,2010.iwslt-evaluation.28,0,\N,Missing
2011.eamt-1.4,W07-0700,0,\N,Missing
2011.eamt-1.40,W05-0909,0,0.038572,"was about 5–10 times faster than translating the sentences from scratch. Detailed statistics of the test and development sets obtained by the procedure described above are given in Table 4. 5 Experiments and results The described approach was evaluated in eight different scenarios involving: two language pairs (English–Greek, English–French), both translation directions (to English and from English), and the two domains (Natural Environment, Labour Legislation), using the following automatic evaluation measures: WER, PER, and BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005). The baseline MT systems (denoted as v0) were evaluated using these test sets and results are shown in Table 5. The BLEU, METEOR, PER, and WER scores are percentages; WER and PER are error rates; OOV (out-of-vocabulary) is a ratio of unknown words, i.e. the occurrence of words which do not appear in the parallel training data and thus cannot be translated. The scores among different systems are not freely comparable but they give us some idea of how difficult translation is for particular languages or domains. languages English → French French → English English → Greek Greek → English dom env"
2011.eamt-1.40,2010.amta-papers.16,1,0.556343,"Missing"
2011.eamt-1.40,baroni-bernardini-2004-bootcat,0,0.0135991,"extracted 209 terms for the env domain and 86 for the lab domain. The weights assigned to the terms were signed integers indicating the relevance of each term to a topic-class. Topic-classes correspond to possible sub-categories of the domain. The other input for the crawler is a list of seed URLs relevant to the domain. The seeds for the env domain were selected from relevant lists in the Open Directory Project,5 a repository maintained by volunteer editors. For the lab domain, similar lists were not so easy to find. We therefore adopted a different method, namely using the BootCat toolkit (Baroni and Bernardini, 2004) to create random tuples (i.e. n-combinations of terms) from the terms included in the topic definition. We then ran a query for each tuple on the Yahoo! search engine,6 kept the first five URLs returned for each query and finally constructed the seed list with these URLs. Normalization, the next step in the workflow, concerned encoding identification based on the content_charset header of each document, and, if needed, conversion to UTF-8. Language identification was performed by a modified version of the n-gram-based Lingua::Identify7 tool, which was 4 http://eurovoc.europa.eu/ http://www.dm"
2011.eamt-1.40,eck-etal-2004-language,0,0.413861,"same domain, of the same genre, and the same style as that it is applied on. For many domains, such training resources (monolingual and parallel data) are not available in large enough amounts to train a system of a sufficient quality. However, even small amounts of such data can be used to adapt © 2011 European Association for Machine Translation. {vpapa,prokopis,mgiagkou}@ilsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p."
2011.eamt-1.40,W08-0334,0,0.0158822,"for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) PB-SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models also into Moses. In this work, log-linear features were derived to distinguish between phrases of multiple domains by applying data source indicator features. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. Domain adaptation of SMT can be approached in various ways depending on the availability of domain-specific data and their type. If the data is available, it can be directly used to improve components of the MT system: word alignment and phrase extraction (Wu and Wang, 2004), language models (Koehn and Schroeder, 2007), and translation models (Nakov, 2008), usually by m"
2011.eamt-1.40,2005.eamt-1.19,0,0.704171,"e enough amounts to train a system of a sufficient quality. However, even small amounts of such data can be used to adapt © 2011 European Association for Machine Translation. {vpapa,prokopis,mgiagkou}@ilsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens"
2011.eamt-1.40,P05-1058,0,0.192602,"Missing"
2011.eamt-1.40,W07-0733,0,0.0609231,"utomatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl corpus statistics for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) PB-SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models also into Moses. In this work, log-linear features were derived to distinguish between phrases of multiple domains by applying data source indicator features. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general mode"
2011.eamt-1.40,P07-2045,0,0.00586506,".) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl corpus statistics for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) PB-SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models also into Moses. In this work, log-linear features were derived to distinguish between phrases of multiple domains by applying data source indicator features. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. Doma"
2011.eamt-1.40,2005.mtsummit-papers.11,0,0.00681904,"eprocessing, training, tuning, decoding, postprocessing, and evaluation. 3.1 General-domain data As for other data-driven MT systems, MaTrEx requires certain data to be trained on, namely parallel data for translation models, monolingual data for language models, and parallel development data for tuning of system parameters. Parameter tuning is not strictly required but has a big influence on system performance. For the baseline system we decided to exploit the widely used data provided by the organizers of the series of SMT workshops (WPT 2005, WMT 2006–2010)1 : the Europarl parallel corpus (Koehn, 2005) version 5 as training data for translation models and language models, and WPT 2005 test set as the development data for parameter optimization. The Europarl parallel corpus is extracted from the proceedings of the European Parliament. For practical reasons we consider this corpus to contain general-domain texts. Version 5 released in Spring 2010 includes texts in 11 European languages including all languages of our interest (English, French, and Greek; see Table 1). Note that the amount of parallel data for English and Greek is only about one half of what is available for English and French."
2011.eamt-1.40,W02-1405,0,0.604769,"nology but also in grammar. In order to achieve optimal performance, an SMT system must be trained on data from the same domain, of the same genre, and the same style as that it is applied on. For many domains, such training resources (monolingual and parallel data) are not available in large enough amounts to train a system of a sufficient quality. However, even small amounts of such data can be used to adapt © 2011 European Association for Machine Translation. {vpapa,prokopis,mgiagkou}@ilsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere,"
2011.eamt-1.40,J05-4003,0,0.0388536,"lsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl corpus statistics for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus"
2011.eamt-1.40,W08-0320,0,0.0609529,"gium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl corpus statistics for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) PB-SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models also into Moses. In this work, log-linear features were derived to distinguish between phrases of multiple domains by applying data source indicator features. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. Domain adaptation of SMT can be approached in various ways depending on the availability of domain-specific da"
2011.eamt-1.40,P03-1021,0,0.0040274,"8.1 Table 2: Web-crawled monolingual data statistics. lowercased) versions of the target sides of the parallel data are kept for training the Moses recaser. The lowercased versions of the target sides are used for training an interpolated 5-gram language model with Kneser-Ney discounting using the SRILM toolkit (Stolcke, 2002). Translation models are trained on the relevant parts of the Europarl corpus, lowercased and filtered on sentence level; we kept all sentence pairs having less than 100 words on each side and with length ratio within the interval h0.11,9.0i. Minimum error rate training (Och, 2003, MERT) is employed to optimize the model parameters on the development set. For decoding, test sentences are tokenized, lowercased, and translated by the trained system. Letter casing is then reconstructed by the recaser and extra blank spaces in the tokenized text are removed in order to produce human-readable text. 4 4.1 Acquisition of in-domain resources Web crawling of monolingual data Our workflow for acquiring in-domain monolingual data consists of the following steps: focused web crawling, text normalization, language identification, document clean-up and near-duplicate detection. For"
2011.eamt-1.40,P02-1040,0,0.114242,"a different domain. The correctors confirmed that the process was about 5–10 times faster than translating the sentences from scratch. Detailed statistics of the test and development sets obtained by the procedure described above are given in Table 4. 5 Experiments and results The described approach was evaluated in eight different scenarios involving: two language pairs (English–Greek, English–French), both translation directions (to English and from English), and the two domains (Natural Environment, Labour Legislation), using the following automatic evaluation measures: WER, PER, and BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005). The baseline MT systems (denoted as v0) were evaluated using these test sets and results are shown in Table 5. The BLEU, METEOR, PER, and WER scores are percentages; WER and PER are error rates; OOV (out-of-vocabulary) is a ratio of unknown words, i.e. the occurrence of words which do not appear in the parallel training data and thus cannot be translated. The scores among different systems are not freely comparable but they give us some idea of how difficult translation is for particular languages or domains. languages English →"
2011.eamt-1.40,wu-wang-2004-improving-domain,0,0.512028,"ll amounts of such data can be used to adapt © 2011 European Association for Machine Translation. {vpapa,prokopis,mgiagkou}@ilsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl c"
2011.eamt-1.40,W10-1720,1,\N,Missing
2011.freeopmt-1.12,E06-1032,0,0.0135857,"Apertium is approximately ten absolute points over Google for TER and GTM. For these metrics Apertium-i is between the other two systems, roughly four points below Apertium and six over Google. The differences in GTM both between Apertium and Apertium-i and between Apertium-i and Google are significant. 7 http://www.ark.cs.cmu.edu/MT/ http://www.computing.dcu.ie/ ˜nstroppa/index.php?page=softwares 9 http://translate.google.com 8 On the other hand, Google is the best system according to BLEU and NIST scores. It is worth mentioning that these metrics are known to be biased towards SMT systems (Callison-Burch and Osborne, 2006). Google obtains 1.69 absolute BLEU points over Apertium but the difference is not statistically significant. With respect to NIST, the difference is of 0.44 points and it is significant. Apertium is significantly better than Apertium-i both for BLEU (7.98 points) and for NIST (1.18 points). 5 Conclusions This paper has presented an Italian→Catalan RBMT system obtained by automatically deriving its linguistic data from existing Italian– Spanish and Catalan–Spanish systems. Only a limited amount of manual work was carried out to (i) correct the inconsistencies found in the resulting dictionarie"
2011.freeopmt-1.12,A92-1018,0,0.0157289,"s and propose lines of future work. 2 Background Apertium is an open-source rule-based machine translation platform initially built for related lanF. S´ anchez-Mart´ınez, J.A. P´ erez-Ortiz (eds.) Proceedings of the Second International Workshop on Free/Open-Source Rule-Based Machine Translation, p. 77–81 Barcelona, Spain, January 2011. http://hdl.handle.net/10609/5650 guage pairs (such as Spanish–Portuguese), but later expanded to deal with more divergent pairs. It uses finite-state transducers (Roche and Schabes, 1997) for lexical processing, hidden Markov models for part-of-speech tagging (Cutting et al., 1992), and multi-stage finite-state chunking for structural transfer. The linguistic data needed to create a machine translation system between two languages in Apertium are: morphological dictionaries for the source language and for the target language, a bilingual dictionary, structural transfer rules and a tagger definition file with optional linguistic restrictions to train an optimal statistical part-ofspeech tagger. Since its first version in 2005, the number of language pairs available has grown steadily and today (as of 20th November, 2010) there are 25 released stable language pairs, with"
2011.freeopmt-1.12,2005.mtsummit-papers.11,0,0.193007,"nt words that are missing according to a corpus analysis. The system is evaluated on the KDE4 corpus and outperforms Google Translate by approximately ten absolute points in terms of both TER and GTM. 1 existing language pairs. Our approach builds an MT system for a language pair a–b given existing systems for the language pairs a–c and b–c. Specifically, we have built a new language pair for the Apertium RBMT engine, Italian–Catalan, by exploiting the existing Spanish–Italian and Catalan–Spanish language pairs. It is worth mentioning the lack of parallel resources for Catalan (e.g. Europarl (Koehn, 2005) is the most widely used resource of parallel documents for European languages, but it does not cover Catalan). Our motivation can be then summarised by the following two basic ideas: • RBMT is a competitive and useful approach for those languages for which there are no parallel corpora available (Forcada, 2006). • Reutilising data from similar existing language pairs can significantly reduce the amount of work required to build a new language pair. Introduction One of the most common criticisms towards RuleBased Machine Translation (RBMT) regards the amount of work necessary to build a system"
2011.freeopmt-1.12,J10-4005,0,0.0128417,"rallel corpora available (Forcada, 2006). • Reutilising data from similar existing language pairs can significantly reduce the amount of work required to build a new language pair. Introduction One of the most common criticisms towards RuleBased Machine Translation (RBMT) regards the amount of work necessary to build a system for a new language pair (Somers, 2003). In fact, in a traditional scenario, linguists with expertise in the source and target language need to manually build all the dictionary entries and transfer rules. Conversely, in the Statistical Machine Translation (SMT) approach (Koehn, 2010), no such effort is required as the system can be automatically built from parallel corpora. However, this approach is only applicable for those language pairs for which big amounts of parallel text are available. In this paper we present an automatically built RBMT system by exploiting linguistic data from ∗ This research has been partially funded by the EU project PANACEA (7FP-ITC-248064). The rest of the paper is structured as follows. The following section presents the RBMT system Apertium, emphasising on approaches that consider reuse of resources and automatic acquisition of linguistic d"
2011.freeopmt-1.12,P02-1040,0,0.0829786,"Missing"
2011.freeopmt-1.12,2006.amta-papers.25,0,0.0374957,"Missing"
2011.freeopmt-1.12,tiedemann-nygaard-2004-opus,0,0.0803559,"Missing"
2011.freeopmt-1.12,2003.mtsummit-papers.51,0,0.0739646,"Missing"
2011.freeopmt-1.12,2009.eamt-1.17,1,0.83112,"tion pairs and for other language technologies. Source-language morphological dictionaries are theoretically independent from the target language, although in practice some bias does exist towards the target language; bilingual dictionaries and structural transfer rules have to be created specifically for each translation pair. Several papers describe the creation of data for new Apertium language pairs, using a variety of approaches, including the reuse of existing free/open source resources (S´anchez-Mart´ınez et al., 2008; S´anchez-Mart´ınez and Forcada, 2009; Ginest´ı-Rosell et al., 2009; Tyers et al., 2009; Tyers and Donnelly, 2009; Unhammer and Trosterud, 2009) and the use of Crossdics (Armentano and Forcada, 2008),2 a program provided in the Apertium platform that, given two existing systems between the language pairs a– c and b–c, is used to obtain dictionaries for a new translation pair a–b. This is the method we used to 1 wiki.apertium.org http://wiki.apertium.org/wiki/ Crossdics 2 create the Italian–Catalan translation pair, using the available Apertium translation pairs Spanish– Italian and Spanish–Catalan. According to (Armentano and Forcada, 2008), using Crossdics to cross dictionaries"
2011.freeopmt-1.12,2009.freeopmt-1.7,0,0.0147808,". Source-language morphological dictionaries are theoretically independent from the target language, although in practice some bias does exist towards the target language; bilingual dictionaries and structural transfer rules have to be created specifically for each translation pair. Several papers describe the creation of data for new Apertium language pairs, using a variety of approaches, including the reuse of existing free/open source resources (S´anchez-Mart´ınez et al., 2008; S´anchez-Mart´ınez and Forcada, 2009; Ginest´ı-Rosell et al., 2009; Tyers et al., 2009; Tyers and Donnelly, 2009; Unhammer and Trosterud, 2009) and the use of Crossdics (Armentano and Forcada, 2008),2 a program provided in the Apertium platform that, given two existing systems between the language pairs a– c and b–c, is used to obtain dictionaries for a new translation pair a–b. This is the method we used to 1 wiki.apertium.org http://wiki.apertium.org/wiki/ Crossdics 2 create the Italian–Catalan translation pair, using the available Apertium translation pairs Spanish– Italian and Spanish–Catalan. According to (Armentano and Forcada, 2008), using Crossdics to cross dictionaries and adding some manual work to correct and improve the r"
2011.freeopmt-1.7,atserias-etal-2006-freeling,0,0.0206673,"Missing"
2011.freeopmt-1.7,attia-etal-2010-automatically,1,0.846162,"Missing"
2011.freeopmt-1.7,W10-1751,0,0.017299,"without NEs (en–es no nes). NE-enriched systems are built with different values for the thresholds minimum of occurrences (25, 50, 100 and 200) and minimum percentage of occurrences capitalised (.75, .8, and .85). These values are chosen empirically. We evaluate the systems on the News Commentary 2007 English–Spanish test set (nc-2007) from WMT08,6 which contains 2,000 sentence pairs. The following metrics are used in our experiments:, BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), GTM (Turian et al., 2003), METEOR (Lavie and Denkowski, 2009),7 METEOR-Next (Denkowski and Lavie, 2010)7 and DCU-LFG (He et al., 2010).7 Furthermore, we provide for each system execution the amount of unknown tokens (UNK) in the source side of the test set. Statistical significance tests are carried out for BLEU and NIST (with ARK’s code)8 and for GTM (using FastMtEval).9 P-value is set 5 http://sourceforge.net/projects/ apertium/files/apertium-en-es/0.7/ apertium-en-es-0.7.1.tar.gz 6 http://www.statmt.org/wmt08/devsets. tgz 7 These are only applied when the target language is English. 8 http://www.ark.cs.cmu.edu/MT/ 9 http://www.computing.dcu.ie/ 39 to 0.05. 5.2 Experiments Prior to running th"
2011.freeopmt-1.7,W10-1753,1,0.873722,"Missing"
2011.freeopmt-1.7,ruimy-etal-2002-clips,0,0.0120316,"http://hdl.handle.net/10609/5644 the evaluation, and compare the performance of the new system to vanilla Apertium. Finally we outline some conclusions and propose lines of future work. 2 MINELex The Multilingual and Interoperable Named Entity Lexicon (MINELex) (Toral et al., 2008; Attia et al., 2010) is a language resource made up of NEs automatically acquired from Wikipedia for 11 languages2 and connected to semantic units of four computational lexicons (English WordNet (Fellbaum, 1998), Spanish WordNet (Verdejo, 1999), Arabic WordNet (Rodr´ıguez et al., 2008) and the Italian PAROLE-SIMPLE (Ruimy et al., 2002)) and to nodes of two ontologies (SUMO (Niles and Pease, 2001) and SIMPLE (Lenci et al., 2000)). In addition, equivalent NEs in different languages are connected by means of interlingual links. Each NE is associated with confidence scores (the number of occurrences of the NE in a corpus and the percentage of times it occurs capitalised), thus allowing the selection of different subsets of the resource according to the requirements and purpose of the application. Table 1 summarises the number of NEs, variants of these NEs (different written forms) and relations of these NEs for English and Span"
2011.freeopmt-1.7,2006.amta-papers.25,0,0.0213084,"s the Apertium engine without any modification (en–es nes), while the second is the Apertium engine without NEs (en–es no nes). NE-enriched systems are built with different values for the thresholds minimum of occurrences (25, 50, 100 and 200) and minimum percentage of occurrences capitalised (.75, .8, and .85). These values are chosen empirically. We evaluate the systems on the News Commentary 2007 English–Spanish test set (nc-2007) from WMT08,6 which contains 2,000 sentence pairs. The following metrics are used in our experiments:, BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), GTM (Turian et al., 2003), METEOR (Lavie and Denkowski, 2009),7 METEOR-Next (Denkowski and Lavie, 2010)7 and DCU-LFG (He et al., 2010).7 Furthermore, we provide for each system execution the amount of unknown tokens (UNK) in the source side of the test set. Statistical significance tests are carried out for BLEU and NIST (with ARK’s code)8 and for GTM (using FastMtEval).9 P-value is set 5 http://sourceforge.net/projects/ apertium/files/apertium-en-es/0.7/ apertium-en-es-0.7.1.tar.gz 6 http://www.statmt.org/wmt08/devsets. tgz 7 These are only applied when the target language is English. 8 htt"
2011.freeopmt-1.7,2005.mtsummit-papers.11,0,0.0178986,"Missing"
2011.freeopmt-1.7,toral-etal-2008-named,1,0.898893,"Missing"
2011.freeopmt-1.7,2003.mtsummit-papers.51,0,0.0141898,"ut any modification (en–es nes), while the second is the Apertium engine without NEs (en–es no nes). NE-enriched systems are built with different values for the thresholds minimum of occurrences (25, 50, 100 and 200) and minimum percentage of occurrences capitalised (.75, .8, and .85). These values are chosen empirically. We evaluate the systems on the News Commentary 2007 English–Spanish test set (nc-2007) from WMT08,6 which contains 2,000 sentence pairs. The following metrics are used in our experiments:, BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), GTM (Turian et al., 2003), METEOR (Lavie and Denkowski, 2009),7 METEOR-Next (Denkowski and Lavie, 2010)7 and DCU-LFG (He et al., 2010).7 Furthermore, we provide for each system execution the amount of unknown tokens (UNK) in the source side of the test set. Statistical significance tests are carried out for BLEU and NIST (with ARK’s code)8 and for GTM (using FastMtEval).9 P-value is set 5 http://sourceforge.net/projects/ apertium/files/apertium-en-es/0.7/ apertium-en-es-0.7.1.tar.gz 6 http://www.statmt.org/wmt08/devsets. tgz 7 These are only applied when the target language is English. 8 http://www.ark.cs.cmu.edu/MT/"
2011.freeopmt-1.7,bel-etal-2000-simple,0,0.0097498,"to vanilla Apertium. Finally we outline some conclusions and propose lines of future work. 2 MINELex The Multilingual and Interoperable Named Entity Lexicon (MINELex) (Toral et al., 2008; Attia et al., 2010) is a language resource made up of NEs automatically acquired from Wikipedia for 11 languages2 and connected to semantic units of four computational lexicons (English WordNet (Fellbaum, 1998), Spanish WordNet (Verdejo, 1999), Arabic WordNet (Rodr´ıguez et al., 2008) and the Italian PAROLE-SIMPLE (Ruimy et al., 2002)) and to nodes of two ontologies (SUMO (Niles and Pease, 2001) and SIMPLE (Lenci et al., 2000)). In addition, equivalent NEs in different languages are connected by means of interlingual links. Each NE is associated with confidence scores (the number of occurrences of the NE in a corpus and the percentage of times it occurs capitalised), thus allowing the selection of different subsets of the resource according to the requirements and purpose of the application. Table 1 summarises the number of NEs, variants of these NEs (different written forms) and relations of these NEs for English and Spanish. NEs Variants Instance relations English 948,410 1,541,993 1,366,899 Spanish 99,330 128,79"
2011.freeopmt-1.7,W02-1111,0,0.0968766,"Missing"
2011.freeopmt-1.7,P02-1040,0,0.0804392,"ng (0.7.1).5 Two baselines are considered. The first is the Apertium engine without any modification (en–es nes), while the second is the Apertium engine without NEs (en–es no nes). NE-enriched systems are built with different values for the thresholds minimum of occurrences (25, 50, 100 and 200) and minimum percentage of occurrences capitalised (.75, .8, and .85). These values are chosen empirically. We evaluate the systems on the News Commentary 2007 English–Spanish test set (nc-2007) from WMT08,6 which contains 2,000 sentence pairs. The following metrics are used in our experiments:, BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), GTM (Turian et al., 2003), METEOR (Lavie and Denkowski, 2009),7 METEOR-Next (Denkowski and Lavie, 2010)7 and DCU-LFG (He et al., 2010).7 Furthermore, we provide for each system execution the amount of unknown tokens (UNK) in the source side of the test set. Statistical significance tests are carried out for BLEU and NIST (with ARK’s code)8 and for GTM (using FastMtEval).9 P-value is set 5 http://sourceforge.net/projects/ apertium/files/apertium-en-es/0.7/ apertium-en-es-0.7.1.tar.gz 6 http://www.statmt.org/wmt08/devsets. tgz 7 These are onl"
2011.mtsummit-papers.60,ruimy-etal-2002-clips,0,0.0411229,"Missing"
2011.mtsummit-papers.60,2011.eamt-1.36,0,0.0501854,"rors based on the use of morpho-syntactic information, which shows that their linguistically-informed evaluation measures provide useful insights to understand the weaknesses of their MT system, while also indicating the best ways and methods to take remedial action. Popoviü and Ney (2007) propose a method to zoom in on translation errors involving different Part-of-Speech (PoS) classes in the output. They apply this method to the estimation of inﬂectional errors and to the distribution of missing targetlanguage words over PoS classes. Following the hierarchy proposed in (Vilar et al., 2006), Popoviü and Burchardt (2011) present a tool that classifies errors into five categories. Parton and McKeown (2010) describe a novel algorithm to detect MT errors, focusing specifically on content words that are deleted. Xiong et al. (2010) attempt to automatically detect incorrect segments Linguistic Checkpoints-based Diagnostic Evaluation In this section, we first give an overview of linguistic checkpoints and then detail the evaluation framework and the key components of the system. 3.1 Linguistic Checkpoints A linguistic checkpoint can be defined as a linguistically-motivated unit, (e.g. an ambiguous word, a verb-obje"
2011.mtsummit-papers.60,W06-3101,0,0.421307,"Missing"
2011.mtsummit-papers.60,W07-0707,0,0.0460502,"word order in translation from Chinese into English. Farrús et al. (2011) carry out a manual error analysis on an MT system for Spanish— Catalan and classify the errors into linguistic levels (orthographic, morphological, lexical, semantic, and syntactic). Popoviü et al. (2006) adopt a framework for the automatic analysis of MT errors based on the use of morpho-syntactic information, which shows that their linguistically-informed evaluation measures provide useful insights to understand the weaknesses of their MT system, while also indicating the best ways and methods to take remedial action. Popoviü and Ney (2007) propose a method to zoom in on translation errors involving different Part-of-Speech (PoS) classes in the output. They apply this method to the estimation of inﬂectional errors and to the distribution of missing targetlanguage words over PoS classes. Following the hierarchy proposed in (Vilar et al., 2006), Popoviü and Burchardt (2011) present a tool that classifies errors into five categories. Parton and McKeown (2010) describe a novel algorithm to detect MT errors, focusing specifically on content words that are deleted. Xiong et al. (2010) attempt to automatically detect incorrect segments"
2011.mtsummit-papers.60,vilar-etal-2006-error,0,0.183659,"Missing"
2011.mtsummit-papers.60,W10-3301,0,0.114671,"ord alignment. This is an XML format for linguistic analysis (e.g., PoS tagging, parsing, etc.) and alignment (sentence/word) based on XCES. 8 Scripts to convert the output of well-established tools (GIZA++, Treetagger, etc.) are available. (iii) It uses the KYOTO Annotation Format (KAF) (Bosma et al., 2009), established in the FP7 KYOTO project, 9 to represent textual analysis. KAF represents each level of linguistic analysis based on ISO standards (i.e. MAF, SynAF, SemAF) and it is compatible with the Linguistic Annotation Framework (LAF) (Ide and Romary, 2003). (iv) It makes use of Kybots (Vossen et al., 2010), established in the FP7 KYOTO project, to define the evaluation targets (linguistic check3 http://research.microsoft.com/en-us/downloads/ad240799a9a7-4a14-a556-d6a7c7919b4a/MSR%20noncommercial%20license%20agreement.txt 4 http://www.computing.dcu.ie/~atoral/delic4mt (under the license GPL-v3). 5 http://www.ims.unistuttgart.de/projekte/corplex/TreeTagger/ 6 http://code.google.com/p/giza-pp/ 7 http://panacea-lr.eu/ 8 http://www.xces.org/ 9 http://www.kyoto-project.eu/ points). A Kybot profile can be thought of as a regular expression over elements and attributes in KAF documents. The benefits of"
2011.mtsummit-papers.60,P10-1062,0,0.0464647,"best ways and methods to take remedial action. Popoviü and Ney (2007) propose a method to zoom in on translation errors involving different Part-of-Speech (PoS) classes in the output. They apply this method to the estimation of inﬂectional errors and to the distribution of missing targetlanguage words over PoS classes. Following the hierarchy proposed in (Vilar et al., 2006), Popoviü and Burchardt (2011) present a tool that classifies errors into five categories. Parton and McKeown (2010) describe a novel algorithm to detect MT errors, focusing specifically on content words that are deleted. Xiong et al. (2010) attempt to automatically detect incorrect segments Linguistic Checkpoints-based Diagnostic Evaluation In this section, we first give an overview of linguistic checkpoints and then detail the evaluation framework and the key components of the system. 3.1 Linguistic Checkpoints A linguistic checkpoint can be defined as a linguistically-motivated unit, (e.g. an ambiguous word, a verb-object collocation, a POS-n-gram, a constituent, etc.) which is predefined in a linguistic taxonomy for diagnostic evaluation. Such a taxonomy is an inventory of linguistic phenomena of the source language that can"
2011.mtsummit-papers.60,W03-1901,0,0.0286593,"ablished in the FP7 PANACEA project,7 to represent word alignment. This is an XML format for linguistic analysis (e.g., PoS tagging, parsing, etc.) and alignment (sentence/word) based on XCES. 8 Scripts to convert the output of well-established tools (GIZA++, Treetagger, etc.) are available. (iii) It uses the KYOTO Annotation Format (KAF) (Bosma et al., 2009), established in the FP7 KYOTO project, 9 to represent textual analysis. KAF represents each level of linguistic analysis based on ISO standards (i.e. MAF, SynAF, SemAF) and it is compatible with the Linguistic Annotation Framework (LAF) (Ide and Romary, 2003). (iv) It makes use of Kybots (Vossen et al., 2010), established in the FP7 KYOTO project, to define the evaluation targets (linguistic check3 http://research.microsoft.com/en-us/downloads/ad240799a9a7-4a14-a556-d6a7c7919b4a/MSR%20noncommercial%20license%20agreement.txt 4 http://www.computing.dcu.ie/~atoral/delic4mt (under the license GPL-v3). 5 http://www.ims.unistuttgart.de/projekte/corplex/TreeTagger/ 6 http://code.google.com/p/giza-pp/ 7 http://panacea-lr.eu/ 8 http://www.xces.org/ 9 http://www.kyoto-project.eu/ points). A Kybot profile can be thought of as a regular expression over elemen"
2011.mtsummit-papers.60,C08-1141,0,0.178031,"nce between the two languages involved in the translation process. The level of detail and the specific linguistic phenomena included in the taxonomy can vary, depending on what the developers and/or the end-users want to investigate as part of the diagnostic evaluation and on the number of aspects that they are interested in. Linguistic checkpoints form the basis of linguistic test suites which are the means by which the MT output is evaluated. 3.2 Diagnostic Evaluation Framework This approach evaluates a system’s ability to handle various linguistic checkpoints. These were first proposed by Zhou et al. (2008), who developed Woodpecker,2 a tool supporting diagnostic evaluation based on linguistic checkpoints. However, this tool has two important drawbacks. Firstly, language-dependent data for English–Chinese (the language pair considered in their paper) is hardcoded in the software, which means that adapting it 2 http://research.microsoft.com/en-us/downloads/ad240799a9a7-4a14-a556-d6a7c7919b4a/ 530 to other language pairs is not straightforward. Secondly, its license (MSR-LA)3 is quite restrictive, to the extent that researchers would not be able to publicly release their adaptations of the tool. F"
2011.mtsummit-papers.60,J03-1002,0,0.00348558,"ation of the matching checkpoints. The requirements we stipulated for this new tool include: (i) the code had to be well-organized and fully documented; (ii) creating new evaluation targets for any language pair has to be as easy as possible (no coding involved); and (iii) the tool should support different evaluation metrics. Our novel tool, DELiC4MT (Diagnostic Evaluation using Linguistic Checkpoints For Machine Translation), 4 makes extensive use of already available components and representation standards. (i) It uses state-of-the-art PoS taggers and word aligners. Treetagger5 and GIZA++6 (Och and Ney, 2003), respectively, are used in the current version, although any similar tool could be used. (ii) It exploits the Travelling Object (TO) format, established in the FP7 PANACEA project,7 to represent word alignment. This is an XML format for linguistic analysis (e.g., PoS tagging, parsing, etc.) and alignment (sentence/word) based on XCES. 8 Scripts to convert the output of well-established tools (GIZA++, Treetagger, etc.) are available. (iii) It uses the KYOTO Annotation Format (KAF) (Bosma et al., 2009), established in the FP7 KYOTO project, 9 to represent textual analysis. KAF represents each l"
2012.eamt-1.10,P09-1104,0,0.0252006,"r for his help on using the cluster. This research has been partially funded by the EU project PANACEA (7FP-ITC-248064). ∗ c 2012 European Association for Machine Translation. 57 100,000 proving the architecture by implementing limitation mechanisms that take into account the results. Time (seconds) 2 10,000 Evaluation We have integrated a range of state-of-the-art sentence and word aligners into the web service architecture. The sentence aligners included are Hunalign (Varga et al., 2005), GMA1 and BSA (Moore, 2002). As for word aligners, they are GIZA++ (Och and Ney, 2003), BerkeleyAligner (Haghighi et al., 2009) and Anymalign (Lardilleux and Lepage, 2009). For a detailed description of the integration please refer to (Toral et al., 2011). In order to evaluate the efficiency of the aligners, we have run them over different amounts of sentences of a bilingual corpus (from 5k to 100k adding 5k at a time for sentence alignment and from 100k to 1.7M adding 100k at a time for word alignment). For all the experiments we use sentences from the Europarl English–Spanish corpus,2 which contains over 1.7M sentence pairs. The aligners are executed using the default values for their parameters. All the experiments"
2012.eamt-1.10,moore-2002-fast,0,0.0491488,"eir feedback on Hunalign and Anymalign, respectively. We would like to thank Joachim Wagner for his help on using the cluster. This research has been partially funded by the EU project PANACEA (7FP-ITC-248064). ∗ c 2012 European Association for Machine Translation. 57 100,000 proving the architecture by implementing limitation mechanisms that take into account the results. Time (seconds) 2 10,000 Evaluation We have integrated a range of state-of-the-art sentence and word aligners into the web service architecture. The sentence aligners included are Hunalign (Varga et al., 2005), GMA1 and BSA (Moore, 2002). As for word aligners, they are GIZA++ (Och and Ney, 2003), BerkeleyAligner (Haghighi et al., 2009) and Anymalign (Lardilleux and Lepage, 2009). For a detailed description of the integration please refer to (Toral et al., 2011). In order to evaluate the efficiency of the aligners, we have run them over different amounts of sentences of a bilingual corpus (from 5k to 100k adding 5k at a time for sentence alignment and from 100k to 1.7M adding 100k at a time for word alignment). For all the experiments we use sentences from the Europarl English–Spanish corpus,2 which contains over 1.7M sentence"
2012.eamt-1.10,J03-1002,0,0.00898825,"We would like to thank Joachim Wagner for his help on using the cluster. This research has been partially funded by the EU project PANACEA (7FP-ITC-248064). ∗ c 2012 European Association for Machine Translation. 57 100,000 proving the architecture by implementing limitation mechanisms that take into account the results. Time (seconds) 2 10,000 Evaluation We have integrated a range of state-of-the-art sentence and word aligners into the web service architecture. The sentence aligners included are Hunalign (Varga et al., 2005), GMA1 and BSA (Moore, 2002). As for word aligners, they are GIZA++ (Och and Ney, 2003), BerkeleyAligner (Haghighi et al., 2009) and Anymalign (Lardilleux and Lepage, 2009). For a detailed description of the integration please refer to (Toral et al., 2011). In order to evaluate the efficiency of the aligners, we have run them over different amounts of sentences of a bilingual corpus (from 5k to 100k adding 5k at a time for sentence alignment and from 100k to 1.7M adding 100k at a time for word alignment). For all the experiments we use sentences from the Europarl English–Spanish corpus,2 which contains over 1.7M sentence pairs. The aligners are executed using the default values"
2012.eamt-1.10,2011.eamt-1.11,1,0.815927,"ional resources consumed (e.g. execution time, use of memory). However, this assessment is critical if the aligners are to be exploited in an industrial scenario. This work is part of a wider project, whose objective is to automate the stages involved in the acquisition, production, updating and maintenance of language resources required by MT systems. This is done by creating a platform, designed as a dedicated workflow manager, for the composition of a number of processes for the production of language resources, based on combinations of different web services. The present work builds upon (Toral et al., 2011), where we presented a web service architecture for sentence and word alignment. Here we extend this proposal by evaluating the efficiency of the aligners integrated, and subsequently imThis paper presents a novel efficiencybased evaluation of sentence and word aligners. This assessment is critical in order to make a reliable use in industrial scenarios. The evaluation shows that the resources required by aligners differ rather broadly. Subsequently, we establish limitation mechanisms on a set of aligners deployed as web services. These results, paired with the quality expected from the aligne"
2012.eamt-1.38,W05-0909,0,0.00939362,"s confirmed that the manual corrections were about 5–10 times faster than translating the sentences from scratch, so this can be viewed as low-cost method for acquiring in-domain test and development sets for SMT. 4 Domain adaptation experiments In this section, we present experiments that exploit all the acquired in-domain data in eight different evaluation scenarios involving two domains (env, lab) and two language pairs (EN–FR, EN–EL) in both directions. Our primary evaluation measure is BLEU (Papineni et al., 2002). For detailed analysis we also present NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) in Table 8. 4.1 pair dom set sents L1 tokens / voc L2 tokens / voc env train 10,240 300,760 10,963 362,899 14,209 dev 1,392 41,382 4,660 49,657 5,542 test 2,000 58,865 5,483 70,740 6,617 lab train 20,261 709,893 12,746 836,634 17,139 dev 1,411 52,156 4,478 61,191 5,535 test 2,000 71,688 5,277 84,397 6,630 env train 9,653 240,822 10,932 267,742 20,185 dev 1,000 27,865 3,586 30,510 5,467 test 2,000 58,073 4,893 63,551 8,229 lab train 7,064 233,145 7,136 244,396 14,456 dev 506 15,129 2,227 16,089 3,333 test 2,000 62,953 4,022 66,770 7,056 English–French EN–EL / env 53.49 34.15 3.00 5.09 4.28 Sys"
2012.eamt-1.38,2010.amta-papers.16,1,0.890714,"Missing"
2012.eamt-1.38,2008.tc-1.1,0,0.351485,"proposed the STRAND system, in which they used Altavista to search for multilingual websites and examined the similarity of the HTML structures of the fetched web pages in order to identify pairs of potentially parallel pages. Similarly, Esplà-Gomis and Forcada (2010) proposed Bitextor, a system that exploits shallow features (file size, text length, tag structure, and list of numbers in a web page) to mine parallel documents from multilingual web sites. Besides structure similarity, other systems either filter fetched web pages by keeping only those containing language markers in their URLs (Désilets et al., 2008), or employ a predefined bilingual wordlist (Chen et al., 2004), or a naive aligner (Zhang et al., 2006) in order to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation mo"
2012.eamt-1.38,eck-etal-2004-language,0,0.0421934,"st of numbers in a web page) to mine parallel documents from multilingual web sites. Besides structure similarity, other systems either filter fetched web pages by keeping only those containing language markers in their URLs (Désilets et al., 2008), or employ a predefined bilingual wordlist (Chen et al., 2004), or a naive aligner (Zhang et al., 2006) in order to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as"
2012.eamt-1.38,W08-0334,0,0.0350418,"an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models into Moses. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. In general, all approaches to domain adaptation of SMT depend on the availability of domain-specific data. If the data is available, it can be directly used to improve components of the MT system. Otherwise, it can be extracted from a pool of texts from different domains or even from the web, which is also the case in our work. 3 Resources and their acquisition In this"
2012.eamt-1.38,2009.mtsummit-commercial.5,0,0.0940661,"Missing"
2012.eamt-1.38,2005.eamt-1.19,0,0.0499412,"those containing language markers in their URLs (Désilets et al., 2008), or employ a predefined bilingual wordlist (Chen et al., 2004), or a naive aligner (Zhang et al., 2006) in order to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-d"
2012.eamt-1.38,P05-1058,0,0.0700041,"Missing"
2012.eamt-1.38,W07-0733,0,0.108639,"2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models into Moses. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. In general, all approaches to"
2012.eamt-1.38,P07-2045,0,0.0439871,"cally contain vocabulary that is not likely to be found in texts from other domains (Banerjee et al., 2010). Other problems can be caused by divergence in style or genre where the difference is not only in lexis but also in grammar. In order to achieve optimal performance, an SMT system should be trained on data from the same domain, genre, and style as it is applied to. For many domains, though, in-domain data of a size sufficient to train a full system is hard to find. Recent experiments have shown that even small amounts of such data can be used to adapt a system to the domain of interest (Koehn et al., 2007). In this work, we present a strategy for automatic web-crawling and cleaning of domain-specific data. Further, our exhaustive experiments, carried out for the Natural Environment (env) and Labour Legislation (lab) domains and English– French (EN–FR) and English–Greek (EN–EL) language pairs (in both directions), demonstrate how the crawled data improves SMT quality. After an overview of related work, we discuss the possibility of adapting a general-domain SMT system by using various types of in-domain data. Then, we present our web-crawling procedure followed by a description of a series of ex"
2012.eamt-1.38,2005.mtsummit-papers.11,0,0.192466,"Missing"
2012.eamt-1.38,W02-1405,0,0.111907,"that exploits shallow features (file size, text length, tag structure, and list of numbers in a web page) to mine parallel documents from multilingual web sites. Besides structure similarity, other systems either filter fetched web pages by keeping only those containing language markers in their URLs (Désilets et al., 2008), or employ a predefined bilingual wordlist (Chen et al., 2004), or a naive aligner (Zhang et al., 2006) in order to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn"
2012.eamt-1.38,J05-4003,0,0.0461806,"er to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models into Moses. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and decl"
2012.eamt-1.38,W08-0320,0,0.0403881,". Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models into Moses. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. In general, all approaches to domain adaptation of SMT depend on the availability of domain-specific data. If the data is available, it can be directly used to improve components of the MT system. Otherwise, it can be extracted from a pool of texts from dif"
2012.eamt-1.38,P03-1021,0,0.009744,"th length ratio within the interval h0.11,9.0i. The maximum 149 English–Greek category 1. perfect translation 2. minor corrections done 3. major corrections needed 4. misaligned sentence pair 5. wrong domain Table 6: Details of the in-domain parallel data sets obtained by web-crawling and manual correction: sentence pairs (sents), source (L1 ) and target (L2 ) tokens and vocabulary size (voc). length of aligned phrases is set to 7 and the reordering models are generated using parameters: distance, orientation-bidirectional-fe. The model parameters are optimized by Minimum Error Rate Training (Och, 2003, MERT) on development sets. For decoding, test sentences are tokenized, lowercased, and translated by the tuned system. Letter casing is then reconstructed by the recaser and extra blank spaces in the tokenized text are removed in order to produce human-readable text. 4.2 Using out-of-domain test data A number of previous experiments (Wu et al., 2008; Banerjee et al., 2010, e.g.) showed significant degradation of translation quality if an SMT system was applied to out-of-domain data. In order to verify this observation we trained and tuned our system on general-domain data and compared its pe"
2012.eamt-1.38,P02-1040,0,0.0968791,"ected for corrections were used as training sets. See further statistics in Table 6. The correctors confirmed that the manual corrections were about 5–10 times faster than translating the sentences from scratch, so this can be viewed as low-cost method for acquiring in-domain test and development sets for SMT. 4 Domain adaptation experiments In this section, we present experiments that exploit all the acquired in-domain data in eight different evaluation scenarios involving two domains (env, lab) and two language pairs (EN–FR, EN–EL) in both directions. Our primary evaluation measure is BLEU (Papineni et al., 2002). For detailed analysis we also present NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) in Table 8. 4.1 pair dom set sents L1 tokens / voc L2 tokens / voc env train 10,240 300,760 10,963 362,899 14,209 dev 1,392 41,382 4,660 49,657 5,542 test 2,000 58,865 5,483 70,740 6,617 lab train 20,261 709,893 12,746 836,634 17,139 dev 1,411 52,156 4,478 61,191 5,535 test 2,000 71,688 5,277 84,397 6,630 env train 9,653 240,822 10,932 267,742 20,185 dev 1,000 27,865 3,586 30,510 5,467 test 2,000 58,073 4,893 63,551 8,229 lab train 7,064 233,145 7,136 244,396 14,456 dev 506 15,129 2,227 16,089"
2012.eamt-1.38,2011.eamt-1.40,1,0.250717,"Missing"
2012.eamt-1.38,J03-3002,0,0.202473,"ent as relevant to a domain or not also affects the acquisition of domain-specific resources, on the assumption that relevant pages are more likely to contain links to more pages in the same domain. Qi and Davison (2009) review features and algorithms used in web page classification. In most of the algorithms reviewed, on-page features (i.e. textual content and HTML tags) are used to construct a corresponding feature vector and then, several machine-learning approaches, such as SVMs, Decision Trees, and Neural Networks, are employed (Yu et al., 2004). Considering the Web as a parallel corpus, Resnik and Smith (2003) proposed the STRAND system, in which they used Altavista to search for multilingual websites and examined the similarity of the HTML structures of the fetched web pages in order to identify pairs of potentially parallel pages. Similarly, Esplà-Gomis and Forcada (2010) proposed Bitextor, a system that exploits shallow features (file size, text length, tag structure, and list of numbers in a web page) to mine parallel documents from multilingual web sites. Besides structure similarity, other systems either filter fetched web pages by keeping only those containing language markers in their URLs"
2012.eamt-1.38,C08-1125,0,0.0633981,"Missing"
2012.eamt-1.38,wu-wang-2004-improving-domain,0,\N,Missing
2012.eamt-1.38,W04-3250,0,\N,Missing
2012.eamt-1.38,W10-1720,1,\N,Missing
2012.eamt-1.38,2006.eamt-1.31,0,\N,Missing
2012.eamt-1.38,2010.amta-papers.27,1,\N,Missing
2012.eamt-1.67,2008.iwslt-papers.1,0,0.0151173,"e transfer method (Utiyama and Isahara, 2007; Khalilov et al., 2008) translates the text in the SL to the PL using a SL–PL translation model and then to the TL using a PL–TL translation model. A source sentence s can be translated into n PL sentences. Each of these n sentences can then be translated into m TL sentences. Therefore we have n×m translation candidates which can be rescored using the translation scores from both the SL–PL and PL–TL models. The translation that gets the highest ranking is considered to be the best translation. The synthetic corpus method (Gispert and Mari˜no, 2006; Bertoldi et al., 2008; Utiyama et al., 2008) obtains a SL–TL corpus using the SL– PL or the PL–TL corpora. One way to do this is to translate the PL sentences in the SL–PL corpus into TL with the PL–TL system. Another possibility is to translate the PL sentences in the PL–TL corpus into SL with the SL–PL system. Obviously, both methods could be applied and the two resulting synthetic corpora be merged into a single SL– TL corpus. Wu and Wang (2009) compare the performance of the phrase table multiplication, transfer and synthetic corpus methods. They also present a hybrid method that combines RBMT and SMT to fill"
2012.eamt-1.67,P96-1041,0,0.243623,"Missing"
2012.eamt-1.67,P07-1092,0,0.0194098,"this paper is organised as follows. Section 2 presents an overview of the stateof-the-art for pivot-based MT. This is followed by the description of our methodology. Subsequently, we carry out the evaluation and present the results of the proposal. Finally, we conclude and outline lines of future work. 2 Related Work Pivot-based strategies that use SMT systems can be classified into three categories (Wu and Wang, 2009): phrase table multiplication (also known as triangulation), transfer (also referred to as cascade) and synthetic corpus. Phrase table multiplication methods (Wu and Wang, 2007; Cohn and Lapata, 2007) induce a new SL–TL translation model by combining the corresponding translation probabilities of the translations models for SL–PL and PL–TL. The transfer method (Utiyama and Isahara, 2007; Khalilov et al., 2008) translates the text in the SL to the PL using a SL–PL translation model and then to the TL using a PL–TL translation model. A source sentence s can be translated into n PL sentences. Each of these n sentences can then be translated into m TL sentences. Therefore we have n×m translation candidates which can be rescored using the translation scores from both the SL–PL and PL–TL models."
2012.eamt-1.67,P02-1040,0,0.0886126,"Missing"
2012.eamt-1.67,P10-1064,0,0.0493231,"Missing"
2012.eamt-1.67,2011.freeopmt-1.9,0,0.0278589,"best n · m translations (m = 200), therefore it is not guaranteed that n different translations will be found (in fact, for some sentences we obtain a number of translations slightly lower than n). Apart from this, we use Moses’ default settings. The translations in PL are recased using Moses’ built-in recaser trained on the target side of the SL–PL training data. For System2 in both scenarios we use Apertium, a RBMT system that uses a shallowtransfer engine (Forcada et al., 2011).3 We use Apertium systems developed for Spanish– Catalan (Corb´ı-bellot et al., 2005) and Bulgarian– Macedonian (Rangelov, 2011). The development and test sets for it–es–ca are extracted from the KDE4 multilingual documentation corpus in the OPUS project (Tiedemann, 2009).4 The Italian–Catalan bilingual corpus contains 146,372 sentence pairs. We discarded sentence pairs where the source or target side is 1 http://www.statmt.org/moses/ http://www.statmt.org/europarl/ 3 http://www.apertium.org/ 4 http://urd.let.rug.nl/tiedeman/OPUS/ KDE4v2.php 2 Experimental Setting The experiments have been carried out for two scenarios (involving different languages and do323 shorter than 10 words5 or longer than 30,6 where the differe"
2012.eamt-1.67,P07-2045,0,0.00894179,"reached or when both MT scores at α1 and α2 are equal. The best value of α selected during the procedure is then used to select the translations for the test set. 4 4.1 Evaluation mains). The first scenario translates from Italian (SL) to Catalan (TL), passing through Spanish (PL). The test set consists of technical documentation data. We refer to this scenario as it–es–ca. The second scenario involves English as the SL, Bulgarian as the PL and Macedonian as the TL. The test set consists of newswire data. This scenario is referred to as en–bg–mk. For System1 we use the phrase-based SMT Moses (Koehn et al., 2007)1 in both scenarios. This system is trained and tuned on Europarl (Koehn, 2005)2 Italian–Spanish for the first scenario and Europarl English–Bulgarian for the second. The corpora are tokenised and lowercased, and sentences where the source or the target is longer than 40 words are discarded. From the sentences extracted, we set aside 1,000 as development set for parameter tuning using MERT (Och, 2003) and we use the rest for training, i.e. 1,278,411 sentences for Italian–Spanish and 196,113 for English–Bulgarian. For each SL sentence we obtain the n-best (up to 3,000) PL translations. We ensur"
2012.eamt-1.67,W04-3250,0,0.209283,"Missing"
2012.eamt-1.67,2005.mtsummit-papers.11,0,0.0827489,"ring the procedure is then used to select the translations for the test set. 4 4.1 Evaluation mains). The first scenario translates from Italian (SL) to Catalan (TL), passing through Spanish (PL). The test set consists of technical documentation data. We refer to this scenario as it–es–ca. The second scenario involves English as the SL, Bulgarian as the PL and Macedonian as the TL. The test set consists of newswire data. This scenario is referred to as en–bg–mk. For System1 we use the phrase-based SMT Moses (Koehn et al., 2007)1 in both scenarios. This system is trained and tuned on Europarl (Koehn, 2005)2 Italian–Spanish for the first scenario and Europarl English–Bulgarian for the second. The corpora are tokenised and lowercased, and sentences where the source or the target is longer than 40 words are discarded. From the sentences extracted, we set aside 1,000 as development set for parameter tuning using MERT (Och, 2003) and we use the rest for training, i.e. 1,278,411 sentences for Italian–Spanish and 196,113 for English–Bulgarian. For each SL sentence we obtain the n-best (up to 3,000) PL translations. We ensure that all translations in the n-best list are different (using the Moses param"
2012.eamt-1.67,2010.iwslt-papers.12,0,0.0229581,"independent. In this approach, RBMT systems are used to translate the PL sentences in the SL–PL or PL–TL corpus into TL or SL sentences, respectively. Then these synthetic corpora can be used to enrich the initial SL–PL and PL–TL corpora so that the SMT systems can take advantage of the availability of additional bilingual data. System combination has also been exploited to improve pivot-based MT. Wu and Wang (2009) build systems following the three aforementioned approaches (phrase table multiplication, transfer and synthetic corpus) and combine the outputs produced by the different systems. Leusch et al. (2010) generate intermediate translations in several PLs, then translate them separately into the TL, and finally generate a consensus translation out of all of them. 322 The closest research strand to the work presented in this paper is the transfer method. The main difference is that the transfer method uses nbest lists and features from both systems and language pairs (SL–PL and PL–TL) in order to obtain the best translation while our proposal only has access to the n-best list and to internal features of the MT system for the language pair SL–PL. In our approach we treat the MT system for PL–TL"
2012.eamt-1.67,P03-1021,0,0.00531698,"as the SL, Bulgarian as the PL and Macedonian as the TL. The test set consists of newswire data. This scenario is referred to as en–bg–mk. For System1 we use the phrase-based SMT Moses (Koehn et al., 2007)1 in both scenarios. This system is trained and tuned on Europarl (Koehn, 2005)2 Italian–Spanish for the first scenario and Europarl English–Bulgarian for the second. The corpora are tokenised and lowercased, and sentences where the source or the target is longer than 40 words are discarded. From the sentences extracted, we set aside 1,000 as development set for parameter tuning using MERT (Och, 2003) and we use the rest for training, i.e. 1,278,411 sentences for Italian–Spanish and 196,113 for English–Bulgarian. For each SL sentence we obtain the n-best (up to 3,000) PL translations. We ensure that all translations in the n-best list are different (using the Moses parameter distinct). In order to obtain different translations, Moses considers the best n · m translations (m = 200), therefore it is not guaranteed that n different translations will be found (in fact, for some sentences we obtain a number of translations slightly lower than n). Apart from this, we use Moses’ default settings."
2012.eamt-1.67,N07-1061,0,0.0234069,"ry out the evaluation and present the results of the proposal. Finally, we conclude and outline lines of future work. 2 Related Work Pivot-based strategies that use SMT systems can be classified into three categories (Wu and Wang, 2009): phrase table multiplication (also known as triangulation), transfer (also referred to as cascade) and synthetic corpus. Phrase table multiplication methods (Wu and Wang, 2007; Cohn and Lapata, 2007) induce a new SL–TL translation model by combining the corresponding translation probabilities of the translations models for SL–PL and PL–TL. The transfer method (Utiyama and Isahara, 2007; Khalilov et al., 2008) translates the text in the SL to the PL using a SL–PL translation model and then to the TL using a PL–TL translation model. A source sentence s can be translated into n PL sentences. Each of these n sentences can then be translated into m TL sentences. Therefore we have n×m translation candidates which can be rescored using the translation scores from both the SL–PL and PL–TL models. The translation that gets the highest ranking is considered to be the best translation. The synthetic corpus method (Gispert and Mari˜no, 2006; Bertoldi et al., 2008; Utiyama et al., 2008)"
2012.eamt-1.67,2008.iwslt-evaluation.11,0,0.0150948,"ama and Isahara, 2007; Khalilov et al., 2008) translates the text in the SL to the PL using a SL–PL translation model and then to the TL using a PL–TL translation model. A source sentence s can be translated into n PL sentences. Each of these n sentences can then be translated into m TL sentences. Therefore we have n×m translation candidates which can be rescored using the translation scores from both the SL–PL and PL–TL models. The translation that gets the highest ranking is considered to be the best translation. The synthetic corpus method (Gispert and Mari˜no, 2006; Bertoldi et al., 2008; Utiyama et al., 2008) obtains a SL–TL corpus using the SL– PL or the PL–TL corpora. One way to do this is to translate the PL sentences in the SL–PL corpus into TL with the PL–TL system. Another possibility is to translate the PL sentences in the PL–TL corpus into SL with the SL–PL system. Obviously, both methods could be applied and the two resulting synthetic corpora be merged into a single SL– TL corpus. Wu and Wang (2009) compare the performance of the phrase table multiplication, transfer and synthetic corpus methods. They also present a hybrid method that combines RBMT and SMT to fill up the data gap, assumi"
2012.eamt-1.67,P07-1108,0,0.021994,". The remainder of this paper is organised as follows. Section 2 presents an overview of the stateof-the-art for pivot-based MT. This is followed by the description of our methodology. Subsequently, we carry out the evaluation and present the results of the proposal. Finally, we conclude and outline lines of future work. 2 Related Work Pivot-based strategies that use SMT systems can be classified into three categories (Wu and Wang, 2009): phrase table multiplication (also known as triangulation), transfer (also referred to as cascade) and synthetic corpus. Phrase table multiplication methods (Wu and Wang, 2007; Cohn and Lapata, 2007) induce a new SL–TL translation model by combining the corresponding translation probabilities of the translations models for SL–PL and PL–TL. The transfer method (Utiyama and Isahara, 2007; Khalilov et al., 2008) translates the text in the SL to the PL using a SL–PL translation model and then to the TL using a PL–TL translation model. A source sentence s can be translated into n PL sentences. Each of these n sentences can then be translated into m TL sentences. Therefore we have n×m translation candidates which can be rescored using the translation scores from both the"
2012.eamt-1.67,P09-1018,0,0.0828316,"online MT systems, or when the second system does not provide the required data (such as n-best lists), which is the case for many rule-based machine translation systems (RBMT). The remainder of this paper is organised as follows. Section 2 presents an overview of the stateof-the-art for pivot-based MT. This is followed by the description of our methodology. Subsequently, we carry out the evaluation and present the results of the proposal. Finally, we conclude and outline lines of future work. 2 Related Work Pivot-based strategies that use SMT systems can be classified into three categories (Wu and Wang, 2009): phrase table multiplication (also known as triangulation), transfer (also referred to as cascade) and synthetic corpus. Phrase table multiplication methods (Wu and Wang, 2007; Cohn and Lapata, 2007) induce a new SL–TL translation model by combining the corresponding translation probabilities of the translations models for SL–PL and PL–TL. The transfer method (Utiyama and Isahara, 2007; Khalilov et al., 2008) translates the text in the SL to the PL using a SL–PL translation model and then to the TL using a PL–TL translation model. A source sentence s can be translated into n PL sentences. Eac"
2013.mtsummit-papers.17,W05-0909,0,0.100328,"evaluation results on growdiag-final alignments a n v r dt misc pro avg w-avg m-ratio Google 0.2748b,c 0.3108b,c 0.2423c 0.3191b 0.4787b,c 0.4916b,c 0.4281b 0.3636 0.3575 0.6873 Systems Moses 0.2281 0.2690 0.2305c 0.3016 0.4324 0.4453 0.3865 0.3276 0.3218 0.6661 Systran 0.2195 0.2650 0.2113 0.2937 0.4552 0.4447 0.4272 0.3309 0.3214 0.6674 Table 3: Diagnostic evaluation results on union alignments 5.2 Automatic Metrics We also evaluated the performances of the MT systems using a set of state-of-the-art automatic evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). Table 5 presents the system-level evaluation results for the different types of metrics considered (automatic, diagnostic and human judgements). For diagnostic evaluation it reports the weighted averages (see w-avg in Tables 1, 2, 3 and 4). According to BLEU, NIST and METEOR, Google 139 Systems Moses 0.4365 0.5042 0.4261c 0.5431 0.5926 0.6628 0.5493 0.5307 0.5285 1.0940 Systran 0.4365 0.4989 0.3496 0.5603 0.6248 0.6542 0.6030 0.5325 0.5183 1.0764 Table 4: Diagnostic evaluation results on intersection alignments is the best system, followed by Moses and Systran,"
2013.mtsummit-papers.17,E06-1032,0,0.0994004,"Missing"
2013.mtsummit-papers.17,fishel-etal-2012-terra,0,0.0426114,"Missing"
2013.mtsummit-papers.17,2005.mtsummit-papers.11,0,0.00728791,"or this study consists of 447 English–French word-aligned sentence pairs drawn from the Canadian Hansard Corpus, consisting of parliamentary debates (Och and Ney, 2000), for a total of 7,020 tokens in English and 7,761 in French. It should be noted that we did not differentiate between ‘sure’ and ‘probable’ word alignments in this dataset and treat them as having the same weight. Choosing a bilingual dataset from the domain of parliamentary speeches allowed us to conduct a 137 fair and direct comparison with a closely related baseline English–French MT system built using the Europarl corpus4 (Koehn, 2005). 4 4.1 Experimental Setup MT Systems We experimented with three MT systems: Google Translate5 , Systran6 and a baseline Moses7 system. Among the three MT systems, Google Translate and Moses are statistical MT systems while Systran is predominantly a rule-based system. The Moses system used for our experiments was trained on 3.6 million English–French sentence pairs taken from Europarl, the News Commentary corpus and a randomly selected section of the UN corpus. The system was tuned on a heldout development set consisting of 1,025 sentence pairs and used a 5-gram language model built using the"
2013.mtsummit-papers.17,max-etal-2010-contrastive,0,0.0192167,"rrelation between their automatic measures and human judgements across various error classes for different MT output. Popovi´c (2011) describes a tool for automatic classification of MT errors, which are grouped into five classes (morphological, lexical, reordering, omissions and unnecessary additions). The tool needs full-form reference translation(s) and hypotheses with their corresponding base forms. Additional information at the word level (such as PoS tags) can be used for a more delicate analysis. The tool computes the number of errors for each class at the document and sentence levels. Max et al. (2010) propose an approach to contrastive diagnostic MT evaluation based on comparing the ability of different systems (or implementations of the same system) to correctly translate source-language words. Their contrastive lexical evaluation method does not rely on the direct comparison of the system’s hypotheses with the reference translations, but for each sourcelanguage word it identifies which of the MT systems under consideration provide the correct output matching the reference. Their study is devoted to English–French and they point out the crucial role played by the quality of the alignment,"
2013.mtsummit-papers.17,W03-0301,0,0.0467017,"ions in terms of manually annotated aligned English–French data to serve as gold standard, and considered, for example, using Biblical texts made available as part of the Blinker Annotation Project (Melamed, 1998). However, the syntax and vocabulary of this dataset presented some specific features which were not in line with actual uses envisaged for diagnostic evaluation in research or industrial settings. The dataset that was chosen for our experiment was initially created for the shared task on word alignment held as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003). The dataset used for this study consists of 447 English–French word-aligned sentence pairs drawn from the Canadian Hansard Corpus, consisting of parliamentary debates (Och and Ney, 2000), for a total of 7,020 tokens in English and 7,761 in French. It should be noted that we did not differentiate between ‘sure’ and ‘probable’ word alignments in this dataset and treat them as having the same weight. Choosing a bilingual dataset from the domain of parliamentary speeches allowed us to conduct a 137 fair and direct comparison with a closely related baseline English–French MT system built using th"
2013.mtsummit-papers.17,C00-2163,0,0.0506058,"(Melamed, 1998). However, the syntax and vocabulary of this dataset presented some specific features which were not in line with actual uses envisaged for diagnostic evaluation in research or industrial settings. The dataset that was chosen for our experiment was initially created for the shared task on word alignment held as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003). The dataset used for this study consists of 447 English–French word-aligned sentence pairs drawn from the Canadian Hansard Corpus, consisting of parliamentary debates (Och and Ney, 2000), for a total of 7,020 tokens in English and 7,761 in French. It should be noted that we did not differentiate between ‘sure’ and ‘probable’ word alignments in this dataset and treat them as having the same weight. Choosing a bilingual dataset from the domain of parliamentary speeches allowed us to conduct a 137 fair and direct comparison with a closely related baseline English–French MT system built using the Europarl corpus4 (Koehn, 2005). 4 4.1 Experimental Setup MT Systems We experimented with three MT systems: Google Translate5 , Systran6 and a baseline Moses7 system. Among the three MT s"
2013.mtsummit-papers.17,2011.eamt-1.36,0,0.0403556,"Missing"
2013.mtsummit-papers.17,W06-3101,0,0.0563408,"Missing"
2013.mtsummit-papers.17,2006.amta-papers.25,0,0.0409265,"l alignments a n v r dt misc pro avg w-avg m-ratio Google 0.2748b,c 0.3108b,c 0.2423c 0.3191b 0.4787b,c 0.4916b,c 0.4281b 0.3636 0.3575 0.6873 Systems Moses 0.2281 0.2690 0.2305c 0.3016 0.4324 0.4453 0.3865 0.3276 0.3218 0.6661 Systran 0.2195 0.2650 0.2113 0.2937 0.4552 0.4447 0.4272 0.3309 0.3214 0.6674 Table 3: Diagnostic evaluation results on union alignments 5.2 Automatic Metrics We also evaluated the performances of the MT systems using a set of state-of-the-art automatic evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). Table 5 presents the system-level evaluation results for the different types of metrics considered (automatic, diagnostic and human judgements). For diagnostic evaluation it reports the weighted averages (see w-avg in Tables 1, 2, 3 and 4). According to BLEU, NIST and METEOR, Google 139 Systems Moses 0.4365 0.5042 0.4261c 0.5431 0.5926 0.6628 0.5493 0.5307 0.5285 1.0940 Systran 0.4365 0.4989 0.3496 0.5603 0.6248 0.6542 0.6030 0.5325 0.5183 1.0764 Table 4: Diagnostic evaluation results on intersection alignments is the best system, followed by Moses and Systran, while TER ranks Systran over M"
2013.mtsummit-papers.17,C08-1141,0,0.0184393,"provide the correct output matching the reference. Their study is devoted to English–French and they point out the crucial role played by the quality of the alignment, suggesting that inaccuracies in the automatic alignment are bound to impair the reliability of this approach for lexical diagnostic evaluation. Fishel et al. (2012) provide an overview of the field of diagnostic evaluation of MT, presenting a collection of freely available translation errorannotation corpora for various language pairs and comparing the performance of two state-of-the-art tools on automatic error analysis of MT. Zhou et al. (2008) describe a tool for diagnostic MT evaluation called Woodpecker,1 which is based on linguistic checkpoints. These are particularly interesting (or problematic) linguistic phenomena for MT processing identified by the user or developer who conducts the evaluation, e.g. ambiguous words, challenging collocations or PoS-ngram constructs, etc. One needs to define a linguistic taxonomy which describes the phenomena to be captured in the diagnostic evaluation, deciding which elements of the source language one wants to investigate. This scheme is extremely flexible, and can be formulated at different"
2013.mtsummit-papers.17,J03-1002,0,0.00562696,"consisting of 1,025 sentence pairs and used a 5-gram language model built using the SRILM toolkit (Stolcke, 2002). 4.2 Word Alignment The diagnostic evaluation was carried out using both gold standard human alignments and three sets of automatic alignments. Thus, in total we carried out experiments on 4 different sets of word alignments. The idea behind this study was primarily to show whether the different possible alignments had an impact on the effectiveness of the diagnostic MT evaluation metric, also in comparison with gold-standard manual alignment and human evaluation. We used GIZA++8 (Och and Ney, 2003) to derive the automatic alignments between the source and target sides of the testset. We extracted three sets of alignments using the union, intersection and grow-diag-final heuristics, as implemented by the Moses training scripts. Since the testset is far too small to be accurately word-aligned using a statistical word-aligner and would suffer from data sparseness, additional parallel training data from the Europarl corpus was used. The additional training data was first tokenised, filtered (using source-target length ratio) and lower-cased. The testset was also subjected to tokenisation an"
2013.mtsummit-papers.17,P02-1040,0,0.0955639,"ed across Tables 2, 3 and 4 as well. Table 2: Diagnostic evaluation results on growdiag-final alignments a n v r dt misc pro avg w-avg m-ratio Google 0.2748b,c 0.3108b,c 0.2423c 0.3191b 0.4787b,c 0.4916b,c 0.4281b 0.3636 0.3575 0.6873 Systems Moses 0.2281 0.2690 0.2305c 0.3016 0.4324 0.4453 0.3865 0.3276 0.3218 0.6661 Systran 0.2195 0.2650 0.2113 0.2937 0.4552 0.4447 0.4272 0.3309 0.3214 0.6674 Table 3: Diagnostic evaluation results on union alignments 5.2 Automatic Metrics We also evaluated the performances of the MT systems using a set of state-of-the-art automatic evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). Table 5 presents the system-level evaluation results for the different types of metrics considered (automatic, diagnostic and human judgements). For diagnostic evaluation it reports the weighted averages (see w-avg in Tables 1, 2, 3 and 4). According to BLEU, NIST and METEOR, Google 139 Systems Moses 0.4365 0.5042 0.4261c 0.5431 0.5926 0.6628 0.5493 0.5307 0.5285 1.0940 Systran 0.4365 0.4989 0.3496 0.5603 0.6248 0.6542 0.6030 0.5325 0.5183 1.0764 Table 4: Diagnostic evaluation results on intersection al"
2014.amta-wptp.5,W05-0909,0,0.0206611,"rent points (as was likely e.g. with the biographies included in our data sets), they might, more or less consciously, end up translating them differently. This variable behaviour applies even more to post-editors: the degree and the type of corrections made by the same as well as by different individuals to the MT output for one language pair are likely to be unpredictably inconsistent. 68 We thus evaluated the raw MT output against both the human translations and the post-edited MT output using three state-of-the-art automatic evaluation metrics, namely BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). The automatic MT evaluation scores are shown in Figure 3. Following the conventions used in Snover et al. (2006), the scores against references translated from scratch are named after the metric (i.e. BLEU, METEOR and TER), while the scores against the post-edited references are named appending the prefix H (i.e. HBLEU, HMETEOR and HTER, respectively). If we compare the human translation scores against the PE scores, we can see that the PE scores are consistently better than the human translation scores for all the translation directions across all the metrics ("
2014.amta-wptp.5,2009.mtsummit-btm.7,0,0.312419,"Missing"
2014.amta-wptp.5,2009.mtsummit-papers.8,0,0.0774514,"involved. Guerberof (2009) studied the effectiveness of using MT output as opposed to translation memory fuzzy matches for the purpose of post-editing in an EnglishĺSpanish translation task. She used Language Weaver’s statistical MT engine and trained it on the same TM, performing both quantitative and qualitative analyses. The main result was that the productivity of the translators as well as the quality of the translation improved when post-editing MT output, compared to when processing fuzzy matches from the translation memory database. 1 http://langtech.autodesk.com/productivity.html. 61 Koehn and Haddow (2009) describe Caitra, a tool that makes suggestions for sentence completion, shows word and phrase translation options, and supports PE of MT output. They report a user study carried out with the tool involving 7 translators for the English–French language pair. Among the different types of assistance offered by Caitra, users prefer the prediction of sentence completion and the options from the translation table over the other types of assistance available for post-editing MT output. To the authors’ surprise, PE received the lowest scores among all the options, both in terms of enjoyment and subje"
2014.amta-wptp.5,W12-3123,0,0.52659,"t with the tool involving 7 translators for the English–French language pair. Among the different types of assistance offered by Caitra, users prefer the prediction of sentence completion and the options from the translation table over the other types of assistance available for post-editing MT output. To the authors’ surprise, PE received the lowest scores among all the options, both in terms of enjoyment and subjective usefulness, although PE was as productive as the other types of assistance. In an effort to extend the initial insights presented in particular by Koehn and Haddow (2009) and Koponen (2012), this paper investigates perceived vs real productivity gains brought about by post-editing MT output compared against manual translation from scratch in the relatively open – and thus particularly challenging – news-oriented domain. 3 Set-up of the Study 3.1 Methodology and Materials Output from the CoSyne statistical MT systems (Martzoukos and Monz, 2010) was used in this experiment, and a facility was in place to track the time required by the users to post-edit MT output and to perform manual translations from scratch on texts of similar length and complexity. The texts chosen for the stu"
2014.amta-wptp.5,2012.amta-wptp.2,0,0.115752,"Missing"
2014.amta-wptp.5,2013.mtsummit-wptp.10,0,0.227975,"Missing"
2014.amta-wptp.5,2010.iwslt-evaluation.28,0,0.0213712,"west scores among all the options, both in terms of enjoyment and subjective usefulness, although PE was as productive as the other types of assistance. In an effort to extend the initial insights presented in particular by Koehn and Haddow (2009) and Koponen (2012), this paper investigates perceived vs real productivity gains brought about by post-editing MT output compared against manual translation from scratch in the relatively open – and thus particularly challenging – news-oriented domain. 3 Set-up of the Study 3.1 Methodology and Materials Output from the CoSyne statistical MT systems (Martzoukos and Monz, 2010) was used in this experiment, and a facility was in place to track the time required by the users to post-edit MT output and to perform manual translations from scratch on texts of similar length and complexity. The texts chosen for the study were extracted from “Today in History/Kalenderblatt” and “Beeld en Geluidwiki”, the public wiki sites of the two media organizations that acted as end-user partners in the CoSyne project, namely Deutsche Welle (DW) and the Netherlands Institute for Sound and Vision (NISV).2 These two bilingual wiki sites cover news, accounts of historical events, biograph"
2014.amta-wptp.5,P02-1040,0,0.102721,"cross identical phrases at different points (as was likely e.g. with the biographies included in our data sets), they might, more or less consciously, end up translating them differently. This variable behaviour applies even more to post-editors: the degree and the type of corrections made by the same as well as by different individuals to the MT output for one language pair are likely to be unpredictably inconsistent. 68 We thus evaluated the raw MT output against both the human translations and the post-edited MT output using three state-of-the-art automatic evaluation metrics, namely BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). The automatic MT evaluation scores are shown in Figure 3. Following the conventions used in Snover et al. (2006), the scores against references translated from scratch are named after the metric (i.e. BLEU, METEOR and TER), while the scores against the post-edited references are named appending the prefix H (i.e. HBLEU, HMETEOR and HTER, respectively). If we compare the human translation scores against the PE scores, we can see that the PE scores are consistently better than the human translation scores for all the translation"
2014.amta-wptp.5,2012.amta-wptp.7,0,0.104374,"Missing"
2014.amta-wptp.5,2006.amta-papers.25,0,0.404541,"age input, revising and improving it as required to obtain a final target text of publishable quality. The purpose of this was to add the final revised translation to the public wiki of their respective media organization; hence, the scenario was that of full PE, aiming for optimal quality of the final revised text (Allen, 2003: 306). In addition, it should be noted that while all participants in the experiment had experience in manual translation, none of them had been specifically trained to carry out PE in a realistic professional task. This is quite different from previous studies such as Snover et al. (2006: 227), where monolingual annotators “were coached on how to minimize the edit rate”. To sum up, our study focused on a scenario in which (i) the translators were not trained specifically on PE, and (ii) the objective was publishable quality, as a means of investigating the role of full PE in industrial settings, especially in terms of the perceived vs actual productivity gains. 4 Questionnaire Results 4.1 Profiles of the Participants At the time of completing the questionnaire, the youngest DW staff member was 38 years of age, and the oldest was 59. Overall, the average age of DW staff who co"
2014.amta-wptp.5,2010.jec-1.6,0,0.561873,"Missing"
2014.amta-wptp.5,2012.amta-wptp.10,0,0.0730721,"d translations (columns PEMT), for which the picture is rather mixed (4 expected correlations, 5 no correlations and 3 unexpected ones). Aggregating the data for all the translation directions, we observe consistent results regardless of the metric (TER, BLEU and METEOR) or the translation method (PEMT, HT): all the correlations are as expected, their values ranging from ±0.23 to ±0.42. 6 Conclusions We have presented a study of real vs perceived PE productivity gains for the German— English and Dutch—English bidirectional language pairs. Previous studies such as Plitt and Masselot (2010) and Zhechev (2012) had looked at PE productivity gains compared to manual translation. However, in a similar vein to Koehn and Haddow (2009) and Koponen (2012), this study has crucially brought into the picture the perceptions of the users in terms of PE effort and speed, comparing them to the actual PE time gains. We have found a bias in favour of translation from scratch across all four translation directions for all the levels of perception considered (speed, effort and favourite working method). While the perception of speed and effort seems to correspond to the actual gains to some extent, the favourite wo"
2014.eamt-1.45,espla-gomis-etal-2014-comparing,1,0.53776,"Missing"
2014.eamt-1.45,P07-2045,0,0.0196031,"36 0.2945 0.2927 0.3583 0.2456 0.3767 TER 0.5601 0.5295 0.4848 0.5016 0.5755 0.5756 0.4726 0.6582 0.4451 OOV 9.5 7.6 7.2 12.6 12.4 6.3 23.1 4.1 Table 2: SMT results. 4 Table 1: Statistics of the parallel datasets. For each dataset the first line corresponds to statistics for Croatian and the second to English. two additional datasets: union and intersection. These are the union and intersection of datasets 10best and reliable. 3 BLEU 0.4092 0.4382 0.5304 0.5176 0.4064 0.4105 0.5448 0.3224 0.5722 Machine Translation Systems Phrase-based statistical MT (PB-SMT) systems are built with Moses 2.1 (Koehn et al., 2007). Tuning is carried out on the development set with minimum error rate training (Och, 2003). All the MT systems use an English language model (LM) from our system for French→English at the WMT-2014 translation shared task (Rubino et al., 2014).9 We built individual LMs on each dataset provided at WMT-2014 and then interpolated them on a development set of the news domain (news2012). Most systems are built on a single dataset, hence they have one phrase table and one reordering table. These systems include a baseline built on the general-domain data (gen), four systems built on the crawled data"
2014.eamt-1.45,2010.eamt-1.35,1,0.878655,"Missing"
2014.eamt-1.45,P03-1021,0,0.0380974,"4451 OOV 9.5 7.6 7.2 12.6 12.4 6.3 23.1 4.1 Table 2: SMT results. 4 Table 1: Statistics of the parallel datasets. For each dataset the first line corresponds to statistics for Croatian and the second to English. two additional datasets: union and intersection. These are the union and intersection of datasets 10best and reliable. 3 BLEU 0.4092 0.4382 0.5304 0.5176 0.4064 0.4105 0.5448 0.3224 0.5722 Machine Translation Systems Phrase-based statistical MT (PB-SMT) systems are built with Moses 2.1 (Koehn et al., 2007). Tuning is carried out on the development set with minimum error rate training (Och, 2003). All the MT systems use an English language model (LM) from our system for French→English at the WMT-2014 translation shared task (Rubino et al., 2014).9 We built individual LMs on each dataset provided at WMT-2014 and then interpolated them on a development set of the news domain (news2012). Most systems are built on a single dataset, hence they have one phrase table and one reordering table. These systems include a baseline built on the general-domain data (gen), four systems built on the crawled datasets (1best, 10best, reliable and all) and two systems built on the union and intersection"
2014.eamt-1.45,W13-2506,0,0.182025,"ur legislation and natural environment for English–French and English– Greek (Pecina et al., 2012) and automotive for German to Italian and French (L¨aubli et al., 2013). The rest of the paper is organised as follows. Section 2 presents the crawled datasets used in this study and details the processing undertaken to prepare them for MT. Section 3 details the different MT systems built. Section 4 shows and comments the results obtained. Finally, Section 5 draws conclusions and outlines future lines of work. 2 Crawled Datasets Datasets were crawled using two crawlers: ILSP Focused Crawler (FC) (Papavassiliou et al., 2013) and Bitextor (Espl`a-Gomis et al., 2010). The detection of parallel documents was carried out with two settings for each crawler: 10best and 1best for Bitextor and reliable and all for FC (see (Espl`aGomis et al., 2014) for further details). It is worth mentioning that reliable and 1best are subsets of all and 10best, respectively. These subsets were obtained with a more strict configuration of each crawler and, therefore, are expected to contain higher quality parallel text. In addition, a set of parallel segments was obtained by aligning only those pairs of documents which were checked manu"
2014.eamt-1.45,P02-1040,0,0.0979486,"Missing"
2014.eamt-1.45,2012.eamt-1.38,1,0.898626,"Missing"
2014.eamt-1.45,E12-1055,0,0.0212094,"taset, hence they have one phrase table and one reordering table. These systems include a baseline built on the general-domain data (gen), four systems built on the crawled datasets (1best, 10best, reliable and all) and two systems built on the union and intersection of the best performing10 dataset of each crawler: 10best and reliable. There is also one system (gen+u) built on two datasets, the general-domain (gen) dataset and a domain-specific dataset (union). Phrase tables from the individual systems gen and union are interpolated so that the perplexity on the development set is minimised (Sennrich, 2012). 9 http://www.statmt.org/wmt14/ translation-task.html 10 According to the BLEU score on the development set. The MT systems are evaluated with a set of stateof-the-art evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009). For each system we also report the percentage of out-ofvocabulary (OOV) tokens. Table 2 shows the scores obtained by each MT system. We compare our systems to two baselines: a PB-SMT system built on general-domain data (gen) and an on-line MT system, Google Translate11 (google). Systems built solely on in-domain d"
2014.eamt-1.45,2006.amta-papers.25,0,0.202709,"Missing"
2014.tc-1.23,2013.mtsummit-papers.5,0,0.0284717,"ly worse for more open and unpredictable domains 1 such as news (cf. WMT translation task series ). We suggest to study the applicability of SMT to literary text by comparing the degree of freedom and narrowness of parallel corpora for literature to other domains widely studied in the area of MT (technical documentation and news). Such a corpus study can be carried out by using a set of automatic measures. The degree of freedom of the translation can be approximated by the perplexity of the word alignment. The narrowness of the domain can be assessed by using measures such as repetition rate (Bertoldi et al., 2013) and perplexity with respect to a language model (Ruiz and Federico, 2014). Therefore, in order to assess the translatability of literary text with MT, we put the problem in perspective by comparing it to the translatability of other widely studied types of text. Instead of considering the translatability of literature as a whole, we root the study along two axes: 1. Relatedness of the language pair: from pairs of languages that belong to the same family (e.g. Romance languages), through languages that belong to the same group (e.g. Romance and Germanic languages of the Indo-European group) to"
2014.tc-1.23,D10-1016,0,0.0155805,"esults in improved translation productivity, at least for technical domains (Plitt and Masselot, 2010). Having reached this level of maturity, we explore the viability of current state-of-the-art MT for literature, the last bastion of human translation. To what extent is MT useful for literature? At first glance, these two terms (MT and literature) might seem incompatible, but the truth is – to the best of our knowledge – that the applicability of MT to literature has not been studied rigorously from a empirical point of view. 2. Background The first work on MT for literature we are aware of (Genzel et al., 2010) translates poetry by constraining a SMT system to produce translations that obey to particular length, meter and rhyming constraints. Form is preserved at the price of producing a worse translation. However, this work does not study the viability of MT to assist with the translation of poetry. 174 Translating and The Computer 36 The only other work on MT for literature we are aware of (Besacier, 2014) presents a pilot study where MT followed by post-editing is used to translate a short story from English to French. Post-editing is performed by non-professional translators and the author concl"
2014.tc-1.23,P02-1040,0,0.0940549,"Between related languages, translations should be more literal and complex phenomena (e.g. metaphors) might simply transfer to the target language, while they might have more 1 http://www.statmt.org/wmt14/translation-task.html 175 Translating and The Computer 36 complex translations between unrelated languages. Regarding literary genres, in poetry the preservation of form might be considered relevant while in novels it may not. As a preliminary study, we evaluated the translation of a recent best-selling novel for a related language pair (Spanish to Catalan). The scores obtained – 66.2 BLEU (Papineni et al., 2002) points and 23.2 TER (Snover et al., 2006) points – would be considered, in an industrial setting, as very useful for assisting human translation (e.g. by means of post-editing or interactive MT). We expect these scores to generalise to other related language pairs such as Spanish–Portuguese or Spanish–Italian. 2 4. Conclusion In summary, we have proposed a methodology to assess the applicability of MT to literature which aims to give an indication of how well SMT could be expected to perform on literary texts compared to the performance of this technology on technical documentation and news."
2014.tc-1.23,2014.eamt-1.39,0,0.0117314,"translation task series ). We suggest to study the applicability of SMT to literary text by comparing the degree of freedom and narrowness of parallel corpora for literature to other domains widely studied in the area of MT (technical documentation and news). Such a corpus study can be carried out by using a set of automatic measures. The degree of freedom of the translation can be approximated by the perplexity of the word alignment. The narrowness of the domain can be assessed by using measures such as repetition rate (Bertoldi et al., 2013) and perplexity with respect to a language model (Ruiz and Federico, 2014). Therefore, in order to assess the translatability of literary text with MT, we put the problem in perspective by comparing it to the translatability of other widely studied types of text. Instead of considering the translatability of literature as a whole, we root the study along two axes: 1. Relatedness of the language pair: from pairs of languages that belong to the same family (e.g. Romance languages), through languages that belong to the same group (e.g. Romance and Germanic languages of the Indo-European group) to unrelated languages (e.g. Germanic and Sino-Tibetan languages). 2. Litera"
2014.tc-1.23,2006.amta-papers.25,0,0.0231595,"uld be more literal and complex phenomena (e.g. metaphors) might simply transfer to the target language, while they might have more 1 http://www.statmt.org/wmt14/translation-task.html 175 Translating and The Computer 36 complex translations between unrelated languages. Regarding literary genres, in poetry the preservation of form might be considered relevant while in novels it may not. As a preliminary study, we evaluated the translation of a recent best-selling novel for a related language pair (Spanish to Catalan). The scores obtained – 66.2 BLEU (Papineni et al., 2002) points and 23.2 TER (Snover et al., 2006) points – would be considered, in an industrial setting, as very useful for assisting human translation (e.g. by means of post-editing or interactive MT). We expect these scores to generalise to other related language pairs such as Spanish–Portuguese or Spanish–Italian. 2 4. Conclusion In summary, we have proposed a methodology to assess the applicability of MT to literature which aims to give an indication of how well SMT could be expected to perform on literary texts compared to the performance of this technology on technical documentation and news. While we may be far from having MT that is"
2015.eamt-1.45,E06-1032,0,\N,Missing
2015.eamt-1.45,W10-1751,0,\N,Missing
2015.eamt-1.45,W14-3301,0,\N,Missing
2015.eamt-1.45,P02-1040,0,\N,Missing
2015.eamt-1.45,W14-3319,1,\N,Missing
2015.eamt-1.45,P11-1105,0,\N,Missing
2015.eamt-1.45,P10-2041,0,\N,Missing
2015.eamt-1.45,W05-0909,0,\N,Missing
2015.eamt-1.45,P07-2045,0,\N,Missing
2015.eamt-1.45,W07-0718,0,\N,Missing
2015.eamt-1.45,C14-1111,0,\N,Missing
2015.eamt-1.45,P12-3005,0,\N,Missing
2015.eamt-1.45,2012.eamt-1.67,1,\N,Missing
2015.eamt-1.45,2014.eamt-1.4,1,\N,Missing
2015.eamt-1.45,W14-3320,0,\N,Missing
2015.eamt-1.45,2005.mtsummit-papers.11,0,\N,Missing
2015.eamt-1.45,ljubesic-etal-2014-tweetcat,1,\N,Missing
2015.eamt-1.45,W15-3036,1,\N,Missing
2015.eamt-1.45,rubino-etal-2014-quality,1,\N,Missing
2015.eamt-1.45,W15-3022,1,\N,Missing
2015.eamt-1.45,W15-4903,1,\N,Missing
2015.eamt-1.45,2015.eamt-1.4,1,\N,Missing
2015.eamt-1.45,espla-gomis-etal-2014-comparing,1,\N,Missing
2015.eamt-1.45,W15-3001,0,\N,Missing
2015.eamt-1.45,ljubesic-toral-2014-cawac,1,\N,Missing
2015.eamt-1.45,W14-0405,1,\N,Missing
2015.eamt-1.45,2005.iwslt-1.8,0,\N,Missing
2015.eamt-1.45,W16-3421,1,\N,Missing
2015.eamt-1.45,D07-1078,0,\N,Missing
2015.eamt-1.45,W08-0509,0,\N,Missing
2015.eamt-1.45,W11-2123,0,\N,Missing
2015.eamt-1.45,P14-1129,0,\N,Missing
2015.eamt-1.45,W16-2347,0,\N,Missing
2015.eamt-1.45,W16-2375,1,\N,Missing
2015.eamt-1.45,W16-2367,1,\N,Missing
2015.eamt-1.45,W16-3423,1,\N,Missing
2015.eamt-1.7,P11-1103,0,0.0205204,"a large-scale Engish-to-Farsi translation task. The rest of this paper is organised as follows. Section 2 reviews the related work and contextualises our work. Section 3 outlines the main reordering issues due to syntactic differences between English and Farsi. Section 4 presents our reordering model, which is then evaluated in Section 5. Finally, Section 6 concludes the paper and outlines avenues of future work. 2 Related Work Phrase-based systems can perform local (short distance) reordering inside the phrases but they are inherently weak at non-local (medium and long distance) reordering (Birch and Osborne, 2011). Previous work to address reordering in PB-SMT can generally be categorised into two groups. Approaches in the first group perform reordering in a pre-processing step (i.e. before decoding) by applying some reordering rules to the source sentences to make them in order more similar to that of the target language (Xia and McCord, 2004; Collins et al., 2005; Genzel, 2010). Although all these approaches have reported improvements, there is a fundamental problem with separating the reordering task into a pre-processing component as every faulty decision in the pre-processing step will be passed a"
2015.eamt-1.7,W09-0434,0,0.0214,"long- and even medium-distance reorderings, since they try to find suitable reorderings only between adjacent phrases. The first limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The first approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this li"
2015.eamt-1.7,D14-1082,0,0.0384929,"Missing"
2015.eamt-1.7,P05-1033,0,0.129487,"lexical models, they have two important limitations (Birch, 2011). First, since these models are conditioned on actual phrases, they have no ability to be generalised to unseen phrases. Second, these models still fail to capture long- and even medium-distance reorderings, since they try to find suitable reorderings only between adjacent phrases. The first limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The first approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our"
2015.eamt-1.7,P05-1066,0,0.177195,"Missing"
2015.eamt-1.7,W14-3348,0,0.0422753,"irs and features examined (cf. Table 6) and two additional systems that model the reordering for both types of constituent pairs (rows all) with (ws) and without (wos) surface forms. We compare our systems to two baselines, a standard HPB-SMT system (HPB) and a HPBSMT system with added swap glue grammar rule (HPB sgg) as in Equation 4. The swap glue rule allows adjacent phrases to be reversed. X → (X1 X2 , X2 X1 ) (4) Table 7 shows the results obtained by each of the MT systems according to four automatic evaluation metrics: BLEU, NIST (Doddington, 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2014). For each system and evaluation metric we show its relative improvement over the baseline HPB (columns diff). The scores obtained by systems that implement our novel reordering between pairs of dependents (columns dd) are better than those of the baseline, both with (ws) and without (wos) surface forms, accross all the four evaluation metrics. The same is true for models that implement reordering between both pairs of constituent types (columns all), except for the system all wos according to BLEU. The results for systems that perform reordering between pairs of head and dependent offer a mix"
2015.eamt-1.7,D08-1089,0,0.0302344,"espite the satisfactory performance of lexical models, they have two important limitations (Birch, 2011). First, since these models are conditioned on actual phrases, they have no ability to be generalised to unseen phrases. Second, these models still fail to capture long- and even medium-distance reorderings, since they try to find suitable reorderings only between adjacent phrases. The first limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The first approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hann"
2015.eamt-1.7,D11-1079,0,0.0203606,"rs from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pair so far, Chinese-to-English. This paper contributes to this research line on two dimensions. First, we extend the work of (Gao et al., 2011), who studied reordering of headdependent pairs (i.e. parent and child elements in the dependency tree), and consider also the reordering of pairs of dependents (i.e. sibling elements in the dependency tree). Second, this is the first paper in this line of work to be applied to a language pair o"
2015.eamt-1.7,C10-1043,0,0.0214098,"outlines avenues of future work. 2 Related Work Phrase-based systems can perform local (short distance) reordering inside the phrases but they are inherently weak at non-local (medium and long distance) reordering (Birch and Osborne, 2011). Previous work to address reordering in PB-SMT can generally be categorised into two groups. Approaches in the first group perform reordering in a pre-processing step (i.e. before decoding) by applying some reordering rules to the source sentences to make them in order more similar to that of the target language (Xia and McCord, 2004; Collins et al., 2005; Genzel, 2010). Although all these approaches have reported improvements, there is a fundamental problem with separating the reordering task into a pre-processing component as every faulty decision in the pre-processing step will be passed along as a hard decision to the translation system. This also violates the main principle behind statistical modelling in SMT, i.e. to avoid any hard choices and having the ability to reverse early faulty choices. Approaches in the second group try to handle reordering in the decoding step, as a part of the translation process. They implement a probabilistic reordering mo"
2015.eamt-1.7,J03-1002,0,0.0100549,"g. Table 4 presents the details about this dataset. We parsed the source side (English) of the corpus using the Stanford dependency parser (Chen 1 tribes wandered http://dadegan.ir/catalog/mizan 47 wife wandered unit English Farsi sentences 1,016,758 1,016,758 Train words 13,919,071 14,043,499 sentences 3,000 3,000 Tune words 40,831 41,670 sentences 1,000 1,000 Test words 13,165 13,444 Table 4: Mizan parallel corpus statistics and Manning, 2014) and used the “collapsed representation” of the parser output to obtain direct dependencies between the words in the source sentences. We used GIZA++ (Och and Ney, 2003) to align the words in the corpus. Then we extracted 6,391,255 head−dependent pairs and 5,247,137 dependent−dependent pairs from train dataset and determined the orientation for each pair based on Equation 1. In order to measure the impact of different features on the accuracy of our reordering model (as will be described in Section 5.2), we used the Naive Bayes classifier with standard settings from the Weka machine learning toolkit (Hall et al., 2009). We trained the classifier separately for head−dependent and dependent−dependent pairs. Our baseline MT system was the Moses implementation of"
2015.eamt-1.7,P03-1021,0,0.00715687,"the accuracy of our reordering model (as will be described in Section 5.2), we used the Naive Bayes classifier with standard settings from the Weka machine learning toolkit (Hall et al., 2009). We trained the classifier separately for head−dependent and dependent−dependent pairs. Our baseline MT system was the Moses implementation of HPM model with default settings (Hoang et al., 2009). We used a 5-gram target language model trained on the Farsi side of the training data. In all experiments, the weights of our reordering feature-function and the builtin feature-functions was tuned with MERT (Och, 2003). 5.2 Impact of different features Since the proposed reordering model has to classify the head−dependent and dependent−dependent pairs into their correct monotone or swap orientation classes, its task can be seen as a binary classification task. We used the Naive Bayes algorithm to build such an orientation classifier. We then used different feature sets in each classification experiment to determine their impact on the accuracy of the model. The features that were examined in this paper are shown in Table 5. All of these features are entirely based on the source sentence and source dependenc"
2015.eamt-1.7,P05-1034,0,0.112252,"lement pairs presented by Dryer (1992). Dryer has shown that these pairs can be used to distinguish SOV and SVO languages. 4 Dependency-based Reordering Model Our reordering model is based on the source dependency tree, an example of which is shown in Figure 1. The dependency tree of a sentence shows the grammatical relations between the head and dependent words of that sentence. For example in Figure 1, the arrow from “he” to “bought” with label “nsubj”, expresses that the 45 dependent word “he” is the subject of the head word “bought”. Under the assumption that constituents move as a whole (Quirk et al., 2005), our proposed reordering model aims to predict the orientation of each dependent word with respect to its head (head−dependent), and also with respect to the other dependents of that head (dependent−dependent orientation). For example, for the sentence in Figure 1 we try to predict the appropriate orientations between the headdependent and dependent-dependent pairs shown in Tables 2 and 3, respectively. Our motivation for using dependency structure as the basis of our reordering model is based on the assumption that, if it is the case that a reordering pattern is employed for one English–Fars"
2015.eamt-1.7,2006.amta-papers.25,0,0.0362609,"r according to the constituent pairs and features examined (cf. Table 6) and two additional systems that model the reordering for both types of constituent pairs (rows all) with (ws) and without (wos) surface forms. We compare our systems to two baselines, a standard HPB-SMT system (HPB) and a HPBSMT system with added swap glue grammar rule (HPB sgg) as in Equation 4. The swap glue rule allows adjacent phrases to be reversed. X → (X1 X2 , X2 X1 ) (4) Table 7 shows the results obtained by each of the MT systems according to four automatic evaluation metrics: BLEU, NIST (Doddington, 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2014). For each system and evaluation metric we show its relative improvement over the baseline HPB (columns diff). The scores obtained by systems that implement our novel reordering between pairs of dependents (columns dd) are better than those of the baseline, both with (ws) and without (wos) surface forms, accross all the four evaluation metrics. The same is true for models that implement reordering between both pairs of constituent types (columns all), except for the system all wos according to BLEU. The results for systems that perform reordering between"
2015.eamt-1.7,N04-4026,0,0.0599434,"re-processing step will be passed along as a hard decision to the translation system. This also violates the main principle behind statistical modelling in SMT, i.e. to avoid any hard choices and having the ability to reverse early faulty choices. Approaches in the second group try to handle reordering in the decoding step, as a part of the translation process. They implement a probabilistic reordering model that can be used in combination with the other models in SMT to find the best translation. These approaches range from distortion models (Koehn et al., 2003) to lexical reordering models (Tillmann, 2004). 44 Distortion models generally prefer monotone translation which, while may work for related languages, is not a realistic assumption for translating between languages with different grammatical structure. On top of this limitation, these models do not take the content into consideration, and thus they do not generalise well. Lexical reordering models take content into account and condition reordering on actual phrases. They try to learn local orientations for each adjacent phrase from training data. Despite the satisfactory performance of lexical models, they have two important limitations"
2015.eamt-1.7,N13-1029,0,0.0131754,"008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The first approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pai"
2015.eamt-1.7,J10-2004,0,0.0151409,"cess to the necessary structural information to perform long-distance reordering. However, due to the complexity of the decoding algorithm, they have very low performance on large-scale translations. In order to overcome some of these deficiencies, we propose a dependency-based reordering model for HPB-SMT. Our model uses the dependency structure of the source sentence to capture the medium- and long-distance reorderings between the dependent parts of the sentence. Unlike the syntax-based models that impose harsh syntactic limits on rule extraction and require serious efforts to be optimised (Wang et al., 2010), we use syntactic information only in the reordering model and augment the HPB model with soft dependency constraints. We report experimental results on a large-scale Engish-to-Farsi translation task. The rest of this paper is organised as follows. Section 2 reviews the related work and contextualises our work. Section 3 outlines the main reordering issues due to syntactic differences between English and Farsi. Section 4 presents our reordering model, which is then evaluated in Section 5. Finally, Section 6 concludes the paper and outlines avenues of future work. 2 Related Work Phrase-based s"
2015.eamt-1.7,2009.iwslt-papers.4,0,0.0290221,"Then we extracted 6,391,255 head−dependent pairs and 5,247,137 dependent−dependent pairs from train dataset and determined the orientation for each pair based on Equation 1. In order to measure the impact of different features on the accuracy of our reordering model (as will be described in Section 5.2), we used the Naive Bayes classifier with standard settings from the Weka machine learning toolkit (Hall et al., 2009). We trained the classifier separately for head−dependent and dependent−dependent pairs. Our baseline MT system was the Moses implementation of HPM model with default settings (Hoang et al., 2009). We used a 5-gram target language model trained on the Farsi side of the training data. In all experiments, the weights of our reordering feature-function and the builtin feature-functions was tuned with MERT (Och, 2003). 5.2 Impact of different features Since the proposed reordering model has to classify the head−dependent and dependent−dependent pairs into their correct monotone or swap orientation classes, its task can be seen as a binary classification task. We used the Naive Bayes algorithm to build such an orientation classifier. We then used different feature sets in each classificatio"
2015.eamt-1.7,D13-1053,0,0.0189721,"nslation rules. The first approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pair so far, Chinese-to-English. This paper contributes to this research line on two dimensions. First, we extend the work of (Gao et al., 2011), who studied reordering of headdependent pairs (i.e. parent and child elements in the dependency tree), and consider also the reordering of pairs of dependents (i.e. sibling elements in the dependency tree). Second, this is the f"
2015.eamt-1.7,N03-1017,0,0.026867,"essing component as every faulty decision in the pre-processing step will be passed along as a hard decision to the translation system. This also violates the main principle behind statistical modelling in SMT, i.e. to avoid any hard choices and having the ability to reverse early faulty choices. Approaches in the second group try to handle reordering in the decoding step, as a part of the translation process. They implement a probabilistic reordering model that can be used in combination with the other models in SMT to find the best translation. These approaches range from distortion models (Koehn et al., 2003) to lexical reordering models (Tillmann, 2004). 44 Distortion models generally prefer monotone translation which, while may work for related languages, is not a realistic assumption for translating between languages with different grammatical structure. On top of this limitation, these models do not take the content into consideration, and thus they do not generalise well. Lexical reordering models take content into account and condition reordering on actual phrases. They try to learn local orientations for each adjacent phrase from training data. Despite the satisfactory performance of lexica"
2015.eamt-1.7,W04-3250,0,0.178946,"Missing"
2015.eamt-1.7,P06-1066,0,0.0271572,"ring on actual phrases. They try to learn local orientations for each adjacent phrase from training data. Despite the satisfactory performance of lexical models, they have two important limitations (Birch, 2011). First, since these models are conditioned on actual phrases, they have no ability to be generalised to unseen phrases. Second, these models still fail to capture long- and even medium-distance reorderings, since they try to find suitable reorderings only between adjacent phrases. The first limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The first approach results in improvements but suffers from the same issues presented above for pr"
2015.eamt-1.7,P12-1095,0,0.0201087,"ssing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pair so far, Chinese-to-English. This paper contributes to this research line on two dimensions. First, we extend the work of (Gao et al., 2011), who studied reordering of headdependent pairs (i.e. parent and child elements in the dependency tree), and consider also the reordering of pairs of dependents (i.e. sibling elements in the dependency tree). Second, this is the first paper in this line of work to be applied to a language pair other than Chinese-to-English. Our language pair, Engli"
2015.eamt-1.7,N09-1028,0,0.0190702,"nly between adjacent phrases. The first limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The first approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source e"
2015.eamt-1.7,C04-1073,0,\N,Missing
2015.eamt-1.7,N13-1060,0,\N,Missing
2016.gwc-1.24,P05-1033,0,0.149341,"SMT (PB-SMT) is arguably the most widely used approach to SMT to date. In this model, the translation operates on phrases, i.e. sequences of words whose length is between 1 and a maximum upper limit. In PB-SMT, reordering is generally captured by distance-based models (Koehn et al., 2003) and lexical phrase-based models (Tillmann, 2004; Koehn et al., 2005), which are able to perform local reordering but they cannot capture non-local (long-distance) reordering. The weakness of PB-SMT systems on handling long-distance reordering led to proposing the Hierarchical Phrase-based SMT (HPB-SMT) model(Chiang, 2005), in which the translation operates on tree structures (either derived from a syntactic parser or unsupervised). Despite the relatively good performance offered by HPB-SMT in medium-range reordering, they are still weak on long-distance reordering (Birch et al., 2009). A great deal of work has been carried out to address the reordering problem by incorporating reordering models (RM) into SMT systems. A RM tries to capture the differences in word order in a probabilistic framework and assigns a probability to each possible order of words in the target sentence. Most of the reordering models can"
2016.gwc-1.24,P11-2031,0,0.0192647,"sets of the head and dependent words Table 4: Features for (dep-dep) constituent pairs mantic features (WordNet synsets) on the quality of the MT system. Three different feature sets were examined in this paper, including information from (i) surface forms (surface), (ii) synsets (synset) and (iii) both surface forms and synsets (both). We build six MT systems, as shown in Table 6, according to the constituent pairs and feature sets examined. We compared our MT systems to the standard HPB-SMT system. Each MT system is tuned three times and we report the average scores obtained with multeval3 (Clark et al., 2011) on the MT outputs. The results obtained by each of the MT systems according to two widely used automatic evaluation metrics (BLEU (Papineni et al., 2002), and TER (Snover et al., 2006)) are shown in Table 7. The relative improvement of each evaluation metric over the baseline HPB is shown in columns dif f . Compared to the use of surface features, our novel semantic features based on WordNet synsets lead to better scores for both (head- dep) and (depdep) constituent pairs according to both evaluation metrics, BLEU and TER (except for the dd system in terms of TER, where there is a slight but"
2016.gwc-1.24,D11-1079,0,0.0891,"e target language, these models can not generalize to unseen words with equivalent meaning in the same context. In order to improve syntactic and semantic generalization of the RM, it is necessary to incorporate syntactic and semantic features into the model. While there has been some encouraging work on integrating syntactic features into the RM, to the best of our knowledge, there has been no previous work on integrating semantic Reordering Model Zens and Ney (2006) Features Types lexical Cherry (2013) lexical Green et al. (2010) lexical syntactic Bisazza and Federico and Goto et al. (2013) Gao et al. (2011) and Kazemi et al. (2015) The proposed method (2013) lexical syntactic lexical syntactic lexical syntactic semantic Features surface forms of the source and target words unsupervised class of the source and target words surface forms of frequent source and target words unsupervised class of rare source and target words surface forms of the source words, POS tags of the source words, relative position of the source words sentence length surface forms and POS tags of the source words surface forms and POS tags of the source context words surface forms of the source words dependency relation surf"
2016.gwc-1.24,N10-1129,0,0.137079,"se, if two words in the source language follow a specific reordering pattern in the target language, these models can not generalize to unseen words with equivalent meaning in the same context. In order to improve syntactic and semantic generalization of the RM, it is necessary to incorporate syntactic and semantic features into the model. While there has been some encouraging work on integrating syntactic features into the RM, to the best of our knowledge, there has been no previous work on integrating semantic Reordering Model Zens and Ney (2006) Features Types lexical Cherry (2013) lexical Green et al. (2010) lexical syntactic Bisazza and Federico and Goto et al. (2013) Gao et al. (2011) and Kazemi et al. (2015) The proposed method (2013) lexical syntactic lexical syntactic lexical syntactic semantic Features surface forms of the source and target words unsupervised class of the source and target words surface forms of frequent source and target words unsupervised class of rare source and target words surface forms of the source words, POS tags of the source words, relative position of the source words sentence length surface forms and POS tags of the source words surface forms and POS tags of the"
2016.gwc-1.24,2009.iwslt-papers.4,0,0.0307022,"ad-dep) and 5,247,526 (dep-dep) pairs from our training data set and determined the orientation for each pair based on Equation 1. We then trained a Maximum Entropy classifier (Manning and Klein, 2003) (henceforth MaxEnt) on the extracted constituent pairs from the training data set and use it to predict the orientation probability of each pair of constituents in the tune and test data sets. As mentioned earlier, we used WordNet in order to determine the synset of the English words in the data set. Our baseline SMT system is the Moses implementation of the HPB-SMT model with default settings (Hoang et al., 2009). We used a 5-gram language model and trained it on the Farsi side of the training data set. All experiments used MIRA for tuning the weights of the features used in the HPB model (Cherry and Foster, 2012). The semantic features (synsets) are extracted from WordNet 3.0. For each word, we take the synset that corresponds to its first sense, i.e. the most common one. An alternative would be to apply a word sense disambiguation algorithm. However, these have been shown to perform worse than the first-sense heuristic when WordNet is the inventory of word senses, e.g. (Pedersen and Kolhatkar, 2009;"
2016.gwc-1.24,W15-4906,1,0.880771,"se models can not generalize to unseen words with equivalent meaning in the same context. In order to improve syntactic and semantic generalization of the RM, it is necessary to incorporate syntactic and semantic features into the model. While there has been some encouraging work on integrating syntactic features into the RM, to the best of our knowledge, there has been no previous work on integrating semantic Reordering Model Zens and Ney (2006) Features Types lexical Cherry (2013) lexical Green et al. (2010) lexical syntactic Bisazza and Federico and Goto et al. (2013) Gao et al. (2011) and Kazemi et al. (2015) The proposed method (2013) lexical syntactic lexical syntactic lexical syntactic semantic Features surface forms of the source and target words unsupervised class of the source and target words surface forms of frequent source and target words unsupervised class of rare source and target words surface forms of the source words, POS tags of the source words, relative position of the source words sentence length surface forms and POS tags of the source words surface forms and POS tags of the source context words surface forms of the source words dependency relation surface forms of the source w"
2016.gwc-1.24,N03-1017,0,0.0672375,"Missing"
2016.gwc-1.24,2005.iwslt-1.8,0,0.0456312,"of the translation, especially between languages with major differences in word order. Although SMT systems deliver state-of-theart performance in machine translation nowadays, they perform relatively weakly at addressing the reordering problem. Phrased-based SMT (PB-SMT) is arguably the most widely used approach to SMT to date. In this model, the translation operates on phrases, i.e. sequences of words whose length is between 1 and a maximum upper limit. In PB-SMT, reordering is generally captured by distance-based models (Koehn et al., 2003) and lexical phrase-based models (Tillmann, 2004; Koehn et al., 2005), which are able to perform local reordering but they cannot capture non-local (long-distance) reordering. The weakness of PB-SMT systems on handling long-distance reordering led to proposing the Hierarchical Phrase-based SMT (HPB-SMT) model(Chiang, 2005), in which the translation operates on tree structures (either derived from a syntactic parser or unsupervised). Despite the relatively good performance offered by HPB-SMT in medium-range reordering, they are still weak on long-distance reordering (Birch et al., 2009). A great deal of work has been carried out to address the reordering problem"
2016.gwc-1.24,C10-1081,0,0.0127693,"use a classifier to predict the probability of the orientation between each pair of constituents to be monotone or swap. This probability is used as one feature in the log-linear framework of the HPB-SMT model. Using a classifier enables us to incorporate fine-grained information in the form of features into our RM. Table 3 and Table 4 show the features that we use to characterize (head-dep) and (dep-dep) pairs respectively. As Table 3 and Table 4 show, we use three types of features: lexical, syntactic and semantic. While semantic structures have been previously used for MT reordering, e.g. (Liu and Gilda, 2010), to the best of our knowledge, this is the first work that includes semantic features jointly with lexical and syntactic features in the framework of a syntax-based RM. Using syntactic features, such as dependency relations, enables the RM to make syntactic generalizations. For instance, the RM can learn that in translating between subject-verbobject (SVO) and subject-object-verb (SOV) languages, the object and the verb should be swapped. On top of this syntactic generalization, the RM should be able to make semantic generalizations. To this end, we use WordNet synsets as an additional featur"
2016.gwc-1.24,N03-5008,0,0.0115234,"ed GIZA++ (Och and Ney, 2003) to align the words in the English and Farsi sentences. We parsed the English sentences of our parallel corpus with the Stanford dependency parser (Chen and Manning, 2014) and used the “collapsed representation” of its output which shows the direct dependencies between the words in the English sentence. Having obtained both dependency trees and the word alignments, we extracted 6,391,956 (head-dep) and 5,247,526 (dep-dep) pairs from our training data set and determined the orientation for each pair based on Equation 1. We then trained a Maximum Entropy classifier (Manning and Klein, 2003) (henceforth MaxEnt) on the extracted constituent pairs from the training data set and use it to predict the orientation probability of each pair of constituents in the tune and test data sets. As mentioned earlier, we used WordNet in order to determine the synset of the English words in the data set. Our baseline SMT system is the Moses implementation of the HPB-SMT model with default settings (Hoang et al., 2009). We used a 5-gram language model and trained it on the Farsi side of the training data set. All experiments used MIRA for tuning the weights of the features used in the HPB model (C"
2016.gwc-1.24,J03-1002,0,0.00595312,"in Figure 1. around one million sentences extracted from English novel books and their translation in Farsi. We randomly held out 3,000 and 1,000 sentence pairs for tuning and testing, respectively, and used the remaining sentence pairs for training. Table 5 shows statistics (number of words and sentences) of the data sets used for training, tuning and testing. Train Tune Test Unit sentences words sentences words sentences words English 1,016,758 13,919,071 3,000 40,831 1,000 13,165 Farsi 1,016,758 14,043,499 3,000 41,670 1,000 13,444 Table 5: Mizan parallel corpus statistics We used GIZA++ (Och and Ney, 2003) to align the words in the English and Farsi sentences. We parsed the English sentences of our parallel corpus with the Stanford dependency parser (Chen and Manning, 2014) and used the “collapsed representation” of its output which shows the direct dependencies between the words in the English sentence. Having obtained both dependency trees and the word alignments, we extracted 6,391,956 (head-dep) and 5,247,526 (dep-dep) pairs from our training data set and determined the orientation for each pair based on Equation 1. We then trained a Maximum Entropy classifier (Manning and Klein, 2003) (hen"
2016.gwc-1.24,P02-1040,0,0.094951,"m. Three different feature sets were examined in this paper, including information from (i) surface forms (surface), (ii) synsets (synset) and (iii) both surface forms and synsets (both). We build six MT systems, as shown in Table 6, according to the constituent pairs and feature sets examined. We compared our MT systems to the standard HPB-SMT system. Each MT system is tuned three times and we report the average scores obtained with multeval3 (Clark et al., 2011) on the MT outputs. The results obtained by each of the MT systems according to two widely used automatic evaluation metrics (BLEU (Papineni et al., 2002), and TER (Snover et al., 2006)) are shown in Table 7. The relative improvement of each evaluation metric over the baseline HPB is shown in columns dif f . Compared to the use of surface features, our novel semantic features based on WordNet synsets lead to better scores for both (head- dep) and (depdep) constituent pairs according to both evaluation metrics, BLEU and TER (except for the dd system in terms of TER, where there is a slight but insignificant increase (79.8 vs. 79.7)). As for future work, we propose to work mainly along the following two directions. First, an investigation of the"
2016.gwc-1.24,N09-5005,0,0.0354819,"settings (Hoang et al., 2009). We used a 5-gram language model and trained it on the Farsi side of the training data set. All experiments used MIRA for tuning the weights of the features used in the HPB model (Cherry and Foster, 2012). The semantic features (synsets) are extracted from WordNet 3.0. For each word, we take the synset that corresponds to its first sense, i.e. the most common one. An alternative would be to apply a word sense disambiguation algorithm. However, these have been shown to perform worse than the first-sense heuristic when WordNet is the inventory of word senses, e.g. (Pedersen and Kolhatkar, 2009; Snyder and Palmer, 2004). 4.2 Evaluation: MT Results We selected different feature sets for (head-dep) and (dep-dep) pairs from Table 3 and Table 4 respectively, then we used them in our MaxEnt classifier to determine the impact of our novel seFeatures lex(head),lex(dep) depRel(dep) syn(head),syn(dep) Type lexical syntactic semantic Description surface forms of the head and dependent word dependency relation of the dependent word synsets of the head and dependent word Table 3: Features for (head-dep) constituent pairs Features lex(head),lex(dep1),lex(dep2) depRel(dep1),depRel(dep2) syn(head)"
2016.gwc-1.24,P05-1034,0,0.0636829,"that follow the same semantic structure. 3 Method Following Kazemi et al. (2015) we implement a syntax-based RM for HPB-SMT based on the dependency tree of the source sentence. The dependency tree of a sentence shows the grammatical relation between pairs of head and dependent words in the sentence. As an example, Figure 1 shows the dependency tree of an English sentence. In this figure, the arrow with label “nsubj” from “fox” to “jumped” indicates that the dependent word “fox” is the subject of the head word “jumped”. Given the assumption that constituents move as a whole during translation (Quirk et al., 2005), we take the dependency tree of the source sentence and try to find the ordering of each dependent word with respect to its head (head-dep) and also with respect to the other dependants of that head (dep-dep). For example, for the English sentence in Figure 1, we try to predict the orientation between (head-dep) and (dep-dep) pairs as shown in Table 2. We consider two orientation types between the constituents: monotone and swap. If the order of two constituents in the source sentence is the same as the order of their translation in the target sentence, the orientation is monotone and otherwi"
2016.gwc-1.24,2006.amta-papers.25,0,0.0360075,"ere examined in this paper, including information from (i) surface forms (surface), (ii) synsets (synset) and (iii) both surface forms and synsets (both). We build six MT systems, as shown in Table 6, according to the constituent pairs and feature sets examined. We compared our MT systems to the standard HPB-SMT system. Each MT system is tuned three times and we report the average scores obtained with multeval3 (Clark et al., 2011) on the MT outputs. The results obtained by each of the MT systems according to two widely used automatic evaluation metrics (BLEU (Papineni et al., 2002), and TER (Snover et al., 2006)) are shown in Table 7. The relative improvement of each evaluation metric over the baseline HPB is shown in columns dif f . Compared to the use of surface features, our novel semantic features based on WordNet synsets lead to better scores for both (head- dep) and (depdep) constituent pairs according to both evaluation metrics, BLEU and TER (except for the dd system in terms of TER, where there is a slight but insignificant increase (79.8 vs. 79.7)). As for future work, we propose to work mainly along the following two directions. First, an investigation of the extent to which using a WordNet"
2016.gwc-1.24,W04-0811,0,0.0164981,"We used a 5-gram language model and trained it on the Farsi side of the training data set. All experiments used MIRA for tuning the weights of the features used in the HPB model (Cherry and Foster, 2012). The semantic features (synsets) are extracted from WordNet 3.0. For each word, we take the synset that corresponds to its first sense, i.e. the most common one. An alternative would be to apply a word sense disambiguation algorithm. However, these have been shown to perform worse than the first-sense heuristic when WordNet is the inventory of word senses, e.g. (Pedersen and Kolhatkar, 2009; Snyder and Palmer, 2004). 4.2 Evaluation: MT Results We selected different feature sets for (head-dep) and (dep-dep) pairs from Table 3 and Table 4 respectively, then we used them in our MaxEnt classifier to determine the impact of our novel seFeatures lex(head),lex(dep) depRel(dep) syn(head),syn(dep) Type lexical syntactic semantic Description surface forms of the head and dependent word dependency relation of the dependent word synsets of the head and dependent word Table 3: Features for (head-dep) constituent pairs Features lex(head),lex(dep1),lex(dep2) depRel(dep1),depRel(dep2) syn(head),syn(dep1),syn(dep2) Type"
2016.gwc-1.24,N04-4026,0,0.0558381,"t on the quality of the translation, especially between languages with major differences in word order. Although SMT systems deliver state-of-theart performance in machine translation nowadays, they perform relatively weakly at addressing the reordering problem. Phrased-based SMT (PB-SMT) is arguably the most widely used approach to SMT to date. In this model, the translation operates on phrases, i.e. sequences of words whose length is between 1 and a maximum upper limit. In PB-SMT, reordering is generally captured by distance-based models (Koehn et al., 2003) and lexical phrase-based models (Tillmann, 2004; Koehn et al., 2005), which are able to perform local reordering but they cannot capture non-local (long-distance) reordering. The weakness of PB-SMT systems on handling long-distance reordering led to proposing the Hierarchical Phrase-based SMT (HPB-SMT) model(Chiang, 2005), in which the translation operates on tree structures (either derived from a syntactic parser or unsupervised). Despite the relatively good performance offered by HPB-SMT in medium-range reordering, they are still weak on long-distance reordering (Birch et al., 2009). A great deal of work has been carried out to address t"
2016.gwc-1.24,W06-3108,0,0.21848,"ta to be able to perform required reordering between them. Likewise, if two words in the source language follow a specific reordering pattern in the target language, these models can not generalize to unseen words with equivalent meaning in the same context. In order to improve syntactic and semantic generalization of the RM, it is necessary to incorporate syntactic and semantic features into the model. While there has been some encouraging work on integrating syntactic features into the RM, to the best of our knowledge, there has been no previous work on integrating semantic Reordering Model Zens and Ney (2006) Features Types lexical Cherry (2013) lexical Green et al. (2010) lexical syntactic Bisazza and Federico and Goto et al. (2013) Gao et al. (2011) and Kazemi et al. (2015) The proposed method (2013) lexical syntactic lexical syntactic lexical syntactic semantic Features surface forms of the source and target words unsupervised class of the source and target words surface forms of frequent source and target words unsupervised class of rare source and target words surface forms of the source words, POS tags of the source words, relative position of the source words sentence length surface forms a"
2020.eamt-1.10,N09-1003,0,0.0834306,"ate-of-the-art SMT systems also require initialization from pretrained embeddings. Therefore, we expect the same trend would appear. 8 We modify the test set by truecasing it in order to match our models. Word Similarity EN - MEN EN - WS353 EN - SIMLEX DE - SIMLEX DE Amount of Data (M) 0.1 1 10 0.138 0.421 0.705 0.018 0.461 0.628 0.011 0.232 0.300 0.017 0.051 0.293 Table 1: The Spearman correlation of the similarity of word pairs (measured by cosine similarity) and human evaluation. Evaluation done using: https://github.com/ kudkudak/word-embeddings-benchmarks MEN (Bruni et al., 2014), WS353 (Agirre et al., 2009), and SIMLEX999 (Hill et al., 2015). We also use Multilingual SIMLEX999 (Leviant and Reichart, 2015) for German and denote this as SIMLEX_DE. As we can see in Table 1, the correlation to human judgment on similarity tasks decreases dramatically as the amount of data used to train the models decreases. The poor correlation when data is limited explains V EC M AP’s poor alignment, as it relies on word similarity being relatively equivalent across languages for its initialization step. 4 Getting More out of Scarce Data With the source of the problem established as the drop in quality of embedding"
2020.eamt-1.10,P17-1042,0,0.506241,"T systems under the assumption c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. that there is a large amount of parallel data available, which is only the case for a select number of language pairs. Recently, there have been approaches that do away with this assumption, requiring only monolingual data, with the first methods based solely around neural MT (NMT), using aligned pretrained embeddings to bootstrap the translation process, and refining the translation with a neural model via denoising and back-translation (Artetxe et al., 2017b; Lample et al., 2017). More recently, statistical MT (SMT) approaches as well as hybrid approaches, combining SMT and NMT, have proven more successful (Lample et al., 2018; Artetxe et al., 2019). While the unsupervised approaches so far have done away with the assumption of parallel data, they still assume an abundance of monolingual data for the two languages, typically assuming at least 10 million sentences per language. This amount of data is not available for every language, notably languages without much of a digital presence. For example, Fulah is a language spoken in West and Central"
2020.eamt-1.10,J82-2005,0,0.719427,"Missing"
2020.eamt-1.10,P18-1073,0,0.0201349,"ential steps: 1. Train monolingual embeddings for each language 2. Align embeddings with a mapping algorithm 3. Train NMT system, initialized with aligned embeddings In the first step, monolingual embeddings (which we will also refer to as pretrained embeddings) are most often trained in the style of WORD 2 VEC ’s skip-gram algorithm (Mikolov et al., 2013). To incorporate sub-word information, Lample et al. (2018) use FAST T EXT (Bojanowski et al., 2017), which formulates a word’s embedding as the sum of its character n-gram embeddings. Artetxe (2019) uses a WORD 2 VEC extension PHRASE 2 VEC (Artetxe et al., 2018b), which learns embeddings of word n-grams up to trigrams, effectively creating embeddings for phrases. The second step involves the alignment of the two monolingual embeddings such that the embeddings of words with identical or similar meaning across language appear close in the shared embedding space. Artetxe et al. achieve this using V EC M AP (Artetxe et al., 2018a), which learns a linear transformation between the two embeddings into a shared space. If there is a large shared vocabulary between the two languages, it is also possible to concatenate the monolingual corpora and train a sing"
2020.eamt-1.10,D18-1399,0,0.03782,"Missing"
2020.eamt-1.10,P19-1019,0,0.523048,"el data available, which is only the case for a select number of language pairs. Recently, there have been approaches that do away with this assumption, requiring only monolingual data, with the first methods based solely around neural MT (NMT), using aligned pretrained embeddings to bootstrap the translation process, and refining the translation with a neural model via denoising and back-translation (Artetxe et al., 2017b; Lample et al., 2017). More recently, statistical MT (SMT) approaches as well as hybrid approaches, combining SMT and NMT, have proven more successful (Lample et al., 2018; Artetxe et al., 2019). While the unsupervised approaches so far have done away with the assumption of parallel data, they still assume an abundance of monolingual data for the two languages, typically assuming at least 10 million sentences per language. This amount of data is not available for every language, notably languages without much of a digital presence. For example, Fulah is a language spoken in West and Central Africa by over 20 million people, however there is a scarce amount of data freely available online. This motivates a new paradigm in unsupervised MT: Low-Resource Unsupervised MT (LRUMT). In this"
2020.eamt-1.10,Q17-1010,0,0.045391,"in Section 5, we present our conclusions and lines for future work. 2 An Unsupervised MT Overview The typical unsupervised NMT pipeline can be broken down into 3 sequential steps: 1. Train monolingual embeddings for each language 2. Align embeddings with a mapping algorithm 3. Train NMT system, initialized with aligned embeddings In the first step, monolingual embeddings (which we will also refer to as pretrained embeddings) are most often trained in the style of WORD 2 VEC ’s skip-gram algorithm (Mikolov et al., 2013). To incorporate sub-word information, Lample et al. (2018) use FAST T EXT (Bojanowski et al., 2017), which formulates a word’s embedding as the sum of its character n-gram embeddings. Artetxe (2019) uses a WORD 2 VEC extension PHRASE 2 VEC (Artetxe et al., 2018b), which learns embeddings of word n-grams up to trigrams, effectively creating embeddings for phrases. The second step involves the alignment of the two monolingual embeddings such that the embeddings of words with identical or similar meaning across language appear close in the shared embedding space. Artetxe et al. achieve this using V EC M AP (Artetxe et al., 2018a), which learns a linear transformation between the two embeddings"
2020.eamt-1.10,P19-1070,0,0.0172496,"tem can achieve a BLEU score of around 6 using embeddings trained on 10 million sentences, even when the NMT system is only trained on 100 thousand sentences per language. We also provide Figure 2, showing the BLI scores of the aligned embeddings (using the English→German test set from Artetxe et al. (2017a)8 ) as we vary the amount of training data used for the embeddings. We can see that the BLI scores decrease dramatically as the amount of sentences decreases, matching the trend of the results from Figure 1. Although BLI has been criticized for not always correlating with downstream tasks (Glavas et al., 2019), in this case, poor alignment corresponds to poor MT performance. In these experiments, we use V EC M AP for aligning embeddings. V EC M AP’s algorithm begins by initializing a bilingual dictionary, which uses a word’s relations to the other words in the same language, with the idea being that “apple” would be close to “pear” but far from “motorcycle” in every language, for example. However, if the quality of embeddings is poor, the random initialization of embeddings has a greater dampening effect. Using embedding similarity tasks (shown in Table 1), we find this to be the case. We measure t"
2020.eamt-1.10,D18-1160,0,0.0135703,"-resource setting. In the following experiments, we use the same settings as mentioned in Section 3, apart from those explicitly mentioned. With the addition of dependency parsing into the pipeline, we apply a parser on the tokenized sentences, while truecasing is learned prior to but applied after parsing. We use the StanfordNLP parser (Qi et al., 2019), using the pretrained English and German models provided to parse our data. Although the dependency parser that we use is supervised, therefore requiring dependency data, it is possible to train a dependency parser in an unsupervised fashion (He et al., 2018). Regardless, a dependency parser extracts linguistic information that is present in a sentence, thus our dependencybased method can still show whether using such linguistic information for training embeddings is useful for their alignment. For training dependency-based word embeddings, we apply Levy and Goldberg (2014)’s dependency-based WORD 2 VEC, and compare this against the standard WORD 2 VEC. For the dependency-based embeddings, we use the same hyperparameters as we use for WORD 2 VEC. To achieve considerable results in unsupervised NMT, it is necessary that we apply Byte-Pair Encoding"
2020.eamt-1.10,J15-4004,0,0.0174303,"re initialization from pretrained embeddings. Therefore, we expect the same trend would appear. 8 We modify the test set by truecasing it in order to match our models. Word Similarity EN - MEN EN - WS353 EN - SIMLEX DE - SIMLEX DE Amount of Data (M) 0.1 1 10 0.138 0.421 0.705 0.018 0.461 0.628 0.011 0.232 0.300 0.017 0.051 0.293 Table 1: The Spearman correlation of the similarity of word pairs (measured by cosine similarity) and human evaluation. Evaluation done using: https://github.com/ kudkudak/word-embeddings-benchmarks MEN (Bruni et al., 2014), WS353 (Agirre et al., 2009), and SIMLEX999 (Hill et al., 2015). We also use Multilingual SIMLEX999 (Leviant and Reichart, 2015) for German and denote this as SIMLEX_DE. As we can see in Table 1, the correlation to human judgment on similarity tasks decreases dramatically as the amount of data used to train the models decreases. The poor correlation when data is limited explains V EC M AP’s poor alignment, as it relies on word similarity being relatively equivalent across languages for its initialization step. 4 Getting More out of Scarce Data With the source of the problem established as the drop in quality of embeddings, we ask ourselves: how can we pre"
2020.eamt-1.10,P07-2045,0,0.00908381,"g our experiments can be found at: https://github.com/Leukas/LRUMT Figure 1: English→German BLEU scores of unsupervised NMT systems where the amount of training data used for the pre-trained embedding training and the amount used for the NMT model training is varied. 2016 for testing, following Lample et al. (2018). The training data is filtered such that sentences that contain between 3-80 words are kept. We then truncate the corpora to sizes ranging from 0.1 to 10 million sentences per language, specified as necessary. We used UDP IPE (Straka and Strakov´a, 2017) for tokenization2 , M OSES (Koehn et al., 2007) for truecasing, and we apply 60 thousand BPE joins (following Lample et al. (2018)) across both corpora using fastBPE.3,4 We train the word embeddings using the WORD 2 VEC skipgram model, with the same hyperparameters as used in Artetxe et al. (2017b), except using an embedding dimension size of 512.5 For embedding alignment, we use the completely unsupervised version of V EC M AP with default parameters. We then train our unsupervised NMT models using Lample et al. (2018)’s implementation, using the default parameters, with the exception of 10 backtranslation processors rather than 30 due to"
2020.eamt-1.10,P14-2050,0,0.333319,"g process. Word embedding algorithms typically define a context-target pair as a word and its neighboring words in a sentence, respectively. While this method works with a large amount of data available, it relies on the fact that a word is seen in several different contexts in order to be represented in the embedding space with respect to its meaning. When data is limited, the contexts contain too much variability to allow for a meaningful representation to be learned. To test this, we use an embedding strategy that has a different definition of the context: dependency-based word embeddings (Levy and Goldberg, 2014). These embeddings model the syntactic similarity between words rather than semantic similarity, providing an embedding representation complementary to standard embeddings. This section presents our findings using Figure 3: Example of a dependency-parsed sentence. dependency-based embeddings (4.1). We also consider the effect of using sub-word information via FAST T EXT (4.2). With the previous two approaches, we find that ensembling models can be useful, and investigate this further (4.3). Finally, we vary context window size and report on its effect (4.4). 4.1 Dependency-Based Embeddings Dep"
2020.eamt-1.10,N15-1144,0,0.0190232,"Missing"
2020.eamt-1.10,P03-1021,0,0.0103026,"c ), and the model is trained to reconstruct the original source sentence, minimizing the difference between s00src and ssrc . Denoising and back-translation are carried out alternately during training. The unsupervised SMT approach is fairly similar, with a replacement of step 3 (or in the hybrid approach, a step added between steps 2 and 3). In Artetxe et al. (2019) for example, a phrase-based SMT model is built by constructing a phrase table that is initialized using the aligned cross-lingual phrase embeddings, and tuning it using an unsupervised variant of the Minimum Error Rate Training (Och, 2003) method. For the hybrid model, the SMT system can then create pseudo-parallel data used to train the NMT model, alongside denoising and back-translation. In the remainder of this paper, we focus on the purely NMT approach to unsupervised MT. 3 The Role of Pretrained Embeddings in Unsupervised MT With the pipeline established, we now turn to the LRUMT setting. In LRUMT, the existing unsupervised approaches fail somewhere along the pipeline, but simply measuring MT performance does not make it clear where this failure occurs. We speculate that the failure is relative to the quality of the pretra"
2020.eamt-1.10,K17-3009,0,0.0219999,"Missing"
2020.eamt-1.14,D16-1025,0,0.0178907,", CCBY-ND. Antonio Toral Center for Language and Cognition University of Groningen The Netherlands a.toral.ruiz@rug.nl networks (RNN) with attention (Bahdanau et al., 2014) while the second, referred to as Transformer, makes use of the self-attention mechanism in nonrecurrent networks (Vaswani et al., 2017). Several studies have analysed in depth, using both automatic and human evaluation methods, the resulting translations of NMT systems under the recurrent architecture and compared them to the translations of the previous mainstream approach to MT: statistical MT (Koehn et al., 2003), e.g. (Bentivogli et al., 2016; Castilho et al., 2017; Klubiˇcka et al., 2018; Popovi´c, 2017; Shterionov et al., 2018). However, while the Transformer architecture has brought, at least when trained with sufficient data, considerable gains over the recurrent architecture (Vaswani et al., 2017), the research conducted to date that analyses the resulting translations of these two neural approaches is, to the best of our knowledge, limited to automatic approaches (Burlot et al., 2018; Lakew et al., 2018; Tang et al., 2018a; Tang et al., 2018b; Tran et al., 2018; Yang et al., 2019). In this paper we conduct a detailed human a"
2020.eamt-1.14,W17-4705,0,0.0167803,"on the semantic task. The latter finding was corroborated by Tang et al. (2018b). Tran et al. (2018) compared the recurrent and Transformer architectures with respect to their ability to model hierarchical structure in a monolingual setting, by means of two tasks: subjectverb agreement and logical inference. On both tasks, the recurrent system outperformed Transformer, slightly but consistently. Burlot et al. (2018) confronted English→Czech Transformer- and recurrent-based MT systems submitted to WMT20181 on a test suite that addresses morphological competence, based on the error typology by Burlot and Yvon (2017). The recurrent system outperformed Transformer on cases that involve number, gender and tense, while both architectures performed similarly on agreement. It is worth noting that agreement here regards local agreement (e.g. an adjective immediately followed by a noun), while the aforementioned cases of agreement in which a recurrent system outperforms Transformer (Tang et al., 2018a; Tran et al., 2018) regard long-distance agreement. Yang et al. (2019) assessed the ability of both architectures to learn word order. When trained on a specific task related to word order, word reordering detectio"
2020.eamt-1.14,W18-6433,0,0.0396345,"Missing"
2020.eamt-1.14,W07-0718,0,0.0632536,"improving the level of agreement notably. IAA RNN Calibration set Evaluation set 0.31 0.45 Transformer (PATECH) 0.22 0.43 Both 0.27 0.44 Table 3: Total and average inter-annotator agreement (Cohen’s κvalues) for the MQM calibration set and evaluation set. As shown in Table 3, the difference of IAA scores between Transformer and RNN is slight in our evaluation set. The average IAA value (0.44), corresponds to moderate agreement, according to Cohen (1960). When interpreting these results, it should be taken into account that IAA scores are known to be low in human evaluation of MT. For example, Callison-Burch et al. (2007) observed fair agreements for fluency and accuracy for eight language pairs, and, though the MQM framework is rigorously defined and supported by clear guidelines, in the experiments by Lommel and Burchardt (2014) MQM led to relatively low IAA, due to span-level difference, ambiguous categorisation and differences of opinion. Klubiˇcka et al. (2018) reported a moderate agreement on English–Croatian, higher than that by Lommel and Burchardt (2014), probably because the agreement was calculated on errors annotated for each sentence, thus not taking the spans of the annotations into account. Our"
2020.eamt-1.14,2010.eamt-1.12,0,0.027983,"Missing"
2020.eamt-1.14,W19-5317,0,0.0310483,"Missing"
2020.eamt-1.14,D18-1458,0,0.0577043,"Missing"
2020.eamt-1.14,N03-1017,0,0.0195381,"rivative works, attribution, CCBY-ND. Antonio Toral Center for Language and Cognition University of Groningen The Netherlands a.toral.ruiz@rug.nl networks (RNN) with attention (Bahdanau et al., 2014) while the second, referred to as Transformer, makes use of the self-attention mechanism in nonrecurrent networks (Vaswani et al., 2017). Several studies have analysed in depth, using both automatic and human evaluation methods, the resulting translations of NMT systems under the recurrent architecture and compared them to the translations of the previous mainstream approach to MT: statistical MT (Koehn et al., 2003), e.g. (Bentivogli et al., 2016; Castilho et al., 2017; Klubiˇcka et al., 2018; Popovi´c, 2017; Shterionov et al., 2018). However, while the Transformer architecture has brought, at least when trained with sufficient data, considerable gains over the recurrent architecture (Vaswani et al., 2017), the research conducted to date that analyses the resulting translations of these two neural approaches is, to the best of our knowledge, limited to automatic approaches (Burlot et al., 2018; Lakew et al., 2018; Tang et al., 2018a; Tang et al., 2018b; Tran et al., 2018; Yang et al., 2019). In this pape"
2020.eamt-1.14,C18-1054,0,0.0869057,"ompared them to the translations of the previous mainstream approach to MT: statistical MT (Koehn et al., 2003), e.g. (Bentivogli et al., 2016; Castilho et al., 2017; Klubiˇcka et al., 2018; Popovi´c, 2017; Shterionov et al., 2018). However, while the Transformer architecture has brought, at least when trained with sufficient data, considerable gains over the recurrent architecture (Vaswani et al., 2017), the research conducted to date that analyses the resulting translations of these two neural approaches is, to the best of our knowledge, limited to automatic approaches (Burlot et al., 2018; Lakew et al., 2018; Tang et al., 2018a; Tang et al., 2018b; Tran et al., 2018; Yang et al., 2019). In this paper we conduct a detailed human analysis of the outputs produced by state-of-the-art recurrent and Transformer NMT systems. Namely, we manually annotate the errors found according to a detailed error taxonomy which is compliant with the hierarchical listing of issue types defined as part of the Multidimensional Quality Metrics (MQM) framework (Lommel et al., 2014). We carry out this analysis for the news domain in the English-to-Chinese translation direction. To this end, we define an error taxonomy that"
2020.eamt-1.14,W09-0433,0,0.0317785,"2). 2.1 Human Error Analyses of MT for Chinese One of the first taxonomies of MT errors, by Vilar et al. (2006), had a specific error typology for the Chinese-to-English translation direction, in accordance with the specific relevant phenomena of this language pair. Compared to their base taxonomy, a refined categorisation of word order was added to mark syntactic mistakes that appear in translations of questions, infinitives, declarative and subordinate sentences. In addition, the error type Unknown words was refined into four sub-types: Person, Location, Organisation and Other proper names. Li et al. (2009) carried out an error analysis for the Chinese-to-Korean translation direction with only three categories from the taxonomy of Vilar et al. (2006) (Missing words, Wrong word order and Incorrect words), and they replaced Incorrect words with two more specific categories: one for both wrong lexical choices and extra words and another for wrong modality. The simplified taxonomy was used to check if their method of reordering verb phrases, prepositional phrases and modality-bearing words in the Chinese data resulted in an improved MT system. Hsu (2014) adapted the classification scheme of Farr´us"
2020.eamt-1.14,W18-6304,0,0.278012,"translations of the previous mainstream approach to MT: statistical MT (Koehn et al., 2003), e.g. (Bentivogli et al., 2016; Castilho et al., 2017; Klubiˇcka et al., 2018; Popovi´c, 2017; Shterionov et al., 2018). However, while the Transformer architecture has brought, at least when trained with sufficient data, considerable gains over the recurrent architecture (Vaswani et al., 2017), the research conducted to date that analyses the resulting translations of these two neural approaches is, to the best of our knowledge, limited to automatic approaches (Burlot et al., 2018; Lakew et al., 2018; Tang et al., 2018a; Tang et al., 2018b; Tran et al., 2018; Yang et al., 2019). In this paper we conduct a detailed human analysis of the outputs produced by state-of-the-art recurrent and Transformer NMT systems. Namely, we manually annotate the errors found according to a detailed error taxonomy which is compliant with the hierarchical listing of issue types defined as part of the Multidimensional Quality Metrics (MQM) framework (Lommel et al., 2014). We carry out this analysis for the news domain in the English-to-Chinese translation direction. To this end, we define an error taxonomy that is relevant to the"
2020.eamt-1.14,D18-1503,0,0.0453318,"Missing"
2020.eamt-1.14,vilar-etal-2006-error,0,0.150932,"Missing"
2020.eamt-1.14,P19-1354,0,0.0567181,"tistical MT (Koehn et al., 2003), e.g. (Bentivogli et al., 2016; Castilho et al., 2017; Klubiˇcka et al., 2018; Popovi´c, 2017; Shterionov et al., 2018). However, while the Transformer architecture has brought, at least when trained with sufficient data, considerable gains over the recurrent architecture (Vaswani et al., 2017), the research conducted to date that analyses the resulting translations of these two neural approaches is, to the best of our knowledge, limited to automatic approaches (Burlot et al., 2018; Lakew et al., 2018; Tang et al., 2018a; Tang et al., 2018b; Tran et al., 2018; Yang et al., 2019). In this paper we conduct a detailed human analysis of the outputs produced by state-of-the-art recurrent and Transformer NMT systems. Namely, we manually annotate the errors found according to a detailed error taxonomy which is compliant with the hierarchical listing of issue types defined as part of the Multidimensional Quality Metrics (MQM) framework (Lommel et al., 2014). We carry out this analysis for the news domain in the English-to-Chinese translation direction. To this end, we define an error taxonomy that is relevant to the problematic linguistic phenomena of this translation direct"
2020.eamt-1.20,W19-5301,0,0.0669819,"Missing"
2020.eamt-1.20,D16-1025,0,0.031896,"of a reference translation. We then conduct a modified evaluation taking these issues into account. Our results indicate that all the claims of human parity and super-human performance made at WMT 2019 should be refuted, except the claim of human parity for English→German. Based on our findings, we put forward a set of recommendations and open questions for future assessments of human parity in machine translation. 1 Introduction The quality of the translations produced by machine translation (MT) systems has improved considerably since the adoption of architectures based on neural networks (Bentivogli et al., 2016). To the extent that, in the last two years, there have been claims of MT systems reaching human parity and even super-human performance (Hassan et al., 2018; Bojar et al., 2018; Barrault et al., 2019). Following Hassan et al. (2018), we consider that human parity is achieved for a given task t if the performance attained by a computer on t is equivalent c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. to that of a human, i.e. there is no significant difference between the performance obtained by human and by machine."
2020.eamt-1.20,2020.lrec-1.461,0,0.467022,"Missing"
2020.eamt-1.20,P16-2013,0,0.0665736,"a reference-based evaluation is that it can be carried out by monolingual speakers, since only proficiency in the target language is required. However, the dependence on reference translations in this type of evaluation can lead to reference bias. Such a bias is hypothesised to result in (i) inflated scores for candidate translations that happen to be similar to the reference translation (e.g. in terms of syntactic structure and lexical choice) and to (ii) penalise correct translations that diverge from the reference translation. Recent research has found both evicence that this is the case (Fomicheva and Specia, 2016; Bentivogli et al., 2018) and that it is not (Ma et al., 2017). In the context of WMT 2019, in the translation directions that followed a reference-free human evaluation, the human translation (used as reference for the automatic evaluation) could be compared to MT systems in the human evaluation, just by being part of the pool of translations to be evaluated. However, in the translation directions that followed a reference-based human evaluation, such as German→English, the reference translation could not be evaluated against the MT systems, since it was itself the gold standard. A second hu"
2020.eamt-1.20,W18-6412,0,0.0239703,"rticle is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. to that of a human, i.e. there is no significant difference between the performance obtained by human and by machine. Super-human performance is achieved for t if the performance achieved by a computer is significantly better than that of a human. Two claims of human parity in MT were reported in 2018. One by Microsoft, on news translation for Chinese→English (Hassan et al., 2018), and another at the news translation task of WMT for English→Czech (Bojar et al., 2018), in which MT systems Uedin (Haddow et al., 2018) and Cuni-Transformer (Kocmi et al., 2018) reached human parity and super-human performance, respectively. In 2019 there were additional claims at the news translation task of WMT (Barrault et al., 2019): human parity for German→English, by several of the submitted systems, and for English→Russian, by system Facebook-FAIR (Ng et al., 2019), as well as super-human performance for English→German, again by Facebook-FAIR. The claims of human parity and super-human performance in MT made in 2018 (Hassan et al., 2018; Bojar et al., 2018) have been since refuted given three issues in their evaluation"
2020.eamt-1.20,W18-6416,0,0.019435,"3.0 licence, no derivative works, attribution, CCBY-ND. to that of a human, i.e. there is no significant difference between the performance obtained by human and by machine. Super-human performance is achieved for t if the performance achieved by a computer is significantly better than that of a human. Two claims of human parity in MT were reported in 2018. One by Microsoft, on news translation for Chinese→English (Hassan et al., 2018), and another at the news translation task of WMT for English→Czech (Bojar et al., 2018), in which MT systems Uedin (Haddow et al., 2018) and Cuni-Transformer (Kocmi et al., 2018) reached human parity and super-human performance, respectively. In 2019 there were additional claims at the news translation task of WMT (Barrault et al., 2019): human parity for German→English, by several of the submitted systems, and for English→Russian, by system Facebook-FAIR (Ng et al., 2019), as well as super-human performance for English→German, again by Facebook-FAIR. The claims of human parity and super-human performance in MT made in 2018 (Hassan et al., 2018; Bojar et al., 2018) have been since refuted given three issues in their evaluation setups (L¨aubli et al., 2018; Toral et al"
2020.eamt-1.20,D18-1512,0,0.0980889,"Missing"
2020.eamt-1.20,D17-1262,0,0.405762,"l speakers, since only proficiency in the target language is required. However, the dependence on reference translations in this type of evaluation can lead to reference bias. Such a bias is hypothesised to result in (i) inflated scores for candidate translations that happen to be similar to the reference translation (e.g. in terms of syntactic structure and lexical choice) and to (ii) penalise correct translations that diverge from the reference translation. Recent research has found both evicence that this is the case (Fomicheva and Specia, 2016; Bentivogli et al., 2018) and that it is not (Ma et al., 2017). In the context of WMT 2019, in the translation directions that followed a reference-free human evaluation, the human translation (used as reference for the automatic evaluation) could be compared to MT systems in the human evaluation, just by being part of the pool of translations to be evaluated. However, in the translation directions that followed a reference-based human evaluation, such as German→English, the reference translation could not be evaluated against the MT systems, since it was itself the gold standard. A second human translation was used to this end. In a nutshell, for Englis"
2020.eamt-1.20,W19-5333,0,0.0300987,"n. Two claims of human parity in MT were reported in 2018. One by Microsoft, on news translation for Chinese→English (Hassan et al., 2018), and another at the news translation task of WMT for English→Czech (Bojar et al., 2018), in which MT systems Uedin (Haddow et al., 2018) and Cuni-Transformer (Kocmi et al., 2018) reached human parity and super-human performance, respectively. In 2019 there were additional claims at the news translation task of WMT (Barrault et al., 2019): human parity for German→English, by several of the submitted systems, and for English→Russian, by system Facebook-FAIR (Ng et al., 2019), as well as super-human performance for English→German, again by Facebook-FAIR. The claims of human parity and super-human performance in MT made in 2018 (Hassan et al., 2018; Bojar et al., 2018) have been since refuted given three issues in their evaluation setups (L¨aubli et al., 2018; Toral et al., 2018): (i) part of the source text of the test set was not original text but translationese, (ii) the sentences were evaluated in isolation, and (iii) the evaluation was not conducted by translators. However, the evaluation setup of WMT 2019 was modified to address some of these issues: the firs"
2020.eamt-1.20,P02-1040,0,0.106843,"an translations, one was used as reference and the other was evaluated against the MT systems, to which we refer to as R EF and H UMAN, respectively. The claim of parity for German→English results therefore from the fact that H UMAN and the output of an MT system (Facebook-FAIR) were compared separately to a gold standard translation, R EF, and the overall ratings that they obtained were not significantly different from each other. If there was reference bias in this case, it could be that H UMAN was penalised for being different than R EF. To check whether this could be the case we use BLEU (Papineni et al., 2002) as a proxy to measure the similarity between all the pairs of the three relevant translations: R EF, H UMAN and the best MT system. Table 1 shows the three pairwise scores.1 HUMAN appears to be markedly differ1 We use the multi-bleu.perl implementation of BLEU, giving as parameters one of the translations as the reference and the other as the hypothesis. Changing the order of the parameters results in very minor variations in the score. ent than MT and REF, which are more similar to each other. MT, R EF 35.9 MT, H UMAN 26.5 R EF, H UMAN 21.9 Table 1: BLEU scores between pairs of three transla"
2020.eamt-1.20,W18-6312,1,0.944184,"al., 2018) reached human parity and super-human performance, respectively. In 2019 there were additional claims at the news translation task of WMT (Barrault et al., 2019): human parity for German→English, by several of the submitted systems, and for English→Russian, by system Facebook-FAIR (Ng et al., 2019), as well as super-human performance for English→German, again by Facebook-FAIR. The claims of human parity and super-human performance in MT made in 2018 (Hassan et al., 2018; Bojar et al., 2018) have been since refuted given three issues in their evaluation setups (L¨aubli et al., 2018; Toral et al., 2018): (i) part of the source text of the test set was not original text but translationese, (ii) the sentences were evaluated in isolation, and (iii) the evaluation was not conducted by translators. However, the evaluation setup of WMT 2019 was modified to address some of these issues: the first issue (translationese) was fully addressed, while the second (sentences evaluated in isolation) was partially addressed, as we will motivate in Section 2.1, whereas the third (human evaluation conducted by non-translators) was not acted upon. Given that some of the issues that led to refute the claims of h"
2020.eamt-1.20,W16-2301,0,\N,Missing
2020.eamt-1.20,W18-6401,0,\N,Missing
2020.eamt-1.20,W18-6425,0,\N,Missing
2020.emnlp-main.371,D17-1130,0,0.0540793,"Missing"
2020.emnlp-main.371,D15-1041,0,0.0506521,"uage identification (Dunning, 1994), topical similarity prediction (Cavnar, 1994), named entity recognition (Klein et al., 2003), authorship attribution (Peng et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representations are employed, they are commonly used in combination with non-contextual word-level representations (Lewis et al., 2016; Ballesteros and Al-Onaizan, 2017; Groschwitz et al., 2018; Cai and Lam, 2019). There are a few recent studies that did use character-level representations in combination with BERT (Zhang et al., 2019a,b; Cai and Lam, 2020), though only Zhang et al. (2019a) provided an"
2020.emnlp-main.371,W13-2322,0,0.0528518,"hat adding character-level representations generally improved performance, though we did not find a clear preference for either the oneencoder or two-encoder model. We believe that, given the better performance of the two-encoder model on the fairly short documents of the nonEnglish languages (see Figure 3), this model is likely the most useful in semantic parsing tasks with single sentences, such as SQL parsing (Zelle and Mooney, 1996; Iyer et al., 2017; FineganDollak et al., 2018), while the one encoder charCNN model has more potential for tasks with longer sentences/documents, such as AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013) and GMB-based DRS parsing (Bos et al., 2017; Liu et al., 2018, 2019a). The latter model also has more potential to be applicable for other (semantic parsing) systems as it can be applied to all systems that form token-level representations from a document. In this sense, we hope that our findings here are also applicable for other, more structured, encoder-decoder models developed for semantic parsing (e.g., Yin and Neubig, 2017; Krishnamurthy et al., 2017; Dong and Lapata, 2018; Liu et al., 2019a). An unexpected finding is that the BERT models outperformed t"
2020.emnlp-main.371,Q17-1010,0,0.0950073,"Missing"
2020.emnlp-main.371,W08-2222,1,0.610515,"h, with more fine-grained and language-neutral DRSs. Semantic tags are used during annotation (Bjerva et al., 2016; Abzianidze and Bos, 2017), and all non-logical DRS symbols 4588 are grounded in either WordNet (Fellbaum, 1998) or VerbNet (Bonial et al., 2011). Moreover, its releases contain gold standard DRSs. For these reasons, we take the PMB as our corpus of choice to evaluate our DRS parsers. DRS parsing Early approaches to DRS parsing employed rule-based systems for small English texts (Johnson and Klein, 1986; Wada and Asher, 1986; Bos, 2001). The first open domain DRS parser is Boxer (Bos, 2008, 2015), which is a combination of rule-based and statistical models. Le and Zuidema (2012) used a probabilistic parsing model that used dependency structures to parse GMB data as graphs. More recently, Liu et al. (2018) proposed a neural model that produces (treestructured) DRSs in three steps by first learning the general (box) structure of a DRS, after which specific conditions and referents are filled in. In followup work (Liu et al., 2019a) they extend this work by adding an improved attention mechanism and constraining the decoder to ensure well-formed output. This model achieved impress"
2020.emnlp-main.371,W15-1841,1,0.896704,"Missing"
2020.emnlp-main.371,A00-1031,0,0.0390252,"h 0 529 483 1,301 21,550 3.3 Sent I Linguistic features We want to contrast our method of character-level information to adding sources of linguistic information. Based on van Noord et al. (2019), we employ these five sources: part-of-speech tags (POS), dependency parses (DEP), lemmas (LEM), CCG supertags (CCG) and semantic tags (SEM). For the first three sources, we use Stanford CoreNLP (Manning et al., 2014) to parse the documents in our dataset. The CCG supertags are obtained by using easyCCG (Lewis and Steedman, 2014). For semantic tagging, we train our own trigram-based tagger using TnT (Brants, 2000).7 Table 2 shows a tagged example sentence for all five sources of information. Moreover, we also include non-contextual GLOVE and FASTTEXT embeddings as an extra source of information. We add these sources of linguistic information in the same way as we add the character-level information, in either one or two encoders (see Section 3.1). In two encoders, we can use the exact same architecture. For one encoder, we (obviously) do not use the char-CNN, but learn a separate embedding for the tags (of size 200), that is then concatenated to the token-level representation, i.e., ei = [etoki ; eling"
2020.emnlp-main.371,D19-1393,0,0.020546,"anowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representations are employed, they are commonly used in combination with non-contextual word-level representations (Lewis et al., 2016; Ballesteros and Al-Onaizan, 2017; Groschwitz et al., 2018; Cai and Lam, 2019). There are a few recent studies that did use character-level representations in combination with BERT (Zhang et al., 2019a,b; Cai and Lam, 2020), though only Zhang et al. (2019a) provided an ablation score without the characters. Moreover, it is not clear if this small improvement was significant. van Noord and Bos (2017) and van Noord et al. (2018b), on the other hand, used solely character-level representations in an end-to-end fashion, using a bi-LSTM sequence-to-sequence model, which outperformed word-based models that employed non-contextual embeddings. 2.2 Discourse Representation Struc"
2020.emnlp-main.371,2020.acl-main.119,0,0.0169374,"Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representations are employed, they are commonly used in combination with non-contextual word-level representations (Lewis et al., 2016; Ballesteros and Al-Onaizan, 2017; Groschwitz et al., 2018; Cai and Lam, 2019). There are a few recent studies that did use character-level representations in combination with BERT (Zhang et al., 2019a,b; Cai and Lam, 2020), though only Zhang et al. (2019a) provided an ablation score without the characters. Moreover, it is not clear if this small improvement was significant. van Noord and Bos (2017) and van Noord et al. (2018b), on the other hand, used solely character-level representations in an end-to-end fashion, using a bi-LSTM sequence-to-sequence model, which outperformed word-based models that employed non-contextual embeddings. 2.2 Discourse Representation Structures DRSs are formal meaning representations introduced by Discourse Representation Theory (Kamp and Reyle, 1993) with the aim to capture the me"
2020.emnlp-main.371,D18-1327,0,0.0127154,"ion function (dot-product) and dj the final decoder hidden state at step j. This model can easily be extended to more than two encoders, which we will experiment with in Section 4. This type of multi-source model is commonly used to represent different languages, e.g., in machine translation (Zoph and Knight, 2016; Firat et al., 2016) and semantic parsing (Susanto and Lu, 2017; Duong et al., 2017), though it has also been successfully applied in multi-modal translation (Libovick´y and Helcl, 2017), multi-framework semantic parsing (Stanovsky and Dagan, 2018) and adding linguistic information (Currey and Heafield, 2018; van Noord et al., 2019). To the best of our knowledge, we are the first to represent the characters as a source of extra information in a multisource sequence-to-sequence model. Transformer We also experiment with the Transformer model (Vaswani et al., 2017), using the stacked self attention model as implemented in AllenNLP. A possible advantage of this model is that it might handle longer sentences and documents better. However, it might be harder to tune (Popel and Bojar, 2018)2 and its improved performance has mainly been shown for large data sets, as 2 Also see: https://twitter.com/srush"
2020.emnlp-main.371,P17-1175,0,0.0232468,"Missing"
2020.emnlp-main.371,D18-1461,0,0.0134369,"g et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representations are employed, they are commonly used in combination with non-contextual word-level representations (Lewis et al., 2016; Ballesteros and Al-Onaizan, 2017; Groschwitz et al., 2018; Cai and Lam, 2019). There are a few recent studies that did use character-level representations in combination with BERT (Zhang et al., 2019a,b; Cai and Lam, 2020), though only Zhang et al. (2019a) provided an ablation score without the characters. Moreover, it is not clear if this small improvement was significant. van Noord and Bos (2017) and van Noord et al."
2020.emnlp-main.371,P16-1160,0,0.0503453,"94), named entity recognition (Klein et al., 2003), authorship attribution (Peng et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representations are employed, they are commonly used in combination with non-contextual word-level representations (Lewis et al., 2016; Ballesteros and Al-Onaizan, 2017; Groschwitz et al., 2018; Cai and Lam, 2019). There are a few recent studies that did use character-level representations in combination with BERT (Zhang et al., 2019a,b; Cai and Lam, 2020), though only Zhang et al. (2019a) provided an ablation score without the characters. Moreover, it is not clear if this"
2020.emnlp-main.371,P93-1001,0,0.0463629,"d quantification, as has been advocated in formal semantics. Moreover, DRSs can be translated to formal logic, which allows for automatic forms of inference by third parties. Lastly, annotated DRSs are available in four languages (Abzianidze et al., 2017, see Section 3.3), allowing us to evaluate our models on multiple languages. 2 2.1 Background Character-level models The power of character-level representations has long been known in the field. In earlier work, they were successfully used in a range of tasks, including text-to-speech (Sejnowski and Rosenberg, 1987), parallel text alignment (Church, 1993), grapheme to phoneme conversion (Kaplan and Kay, 1994), language identification (Dunning, 1994), topical similarity prediction (Cavnar, 1994), named entity recognition (Klein et al., 2003), authorship attribution (Peng et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos"
2020.emnlp-main.371,2020.acl-main.747,0,0.0671246,"Missing"
2020.emnlp-main.371,P16-2058,0,0.0461228,"Missing"
2020.emnlp-main.371,P18-1033,0,0.0284404,"Missing"
2020.emnlp-main.371,N16-1101,0,0.0192089,"s in the decoder (also see Figure 2):  d0j = LSTM1 dj−1 , etj−1    aj = ATT Ctok , d0j ; ATT Cchar , d0j  dj = LSTM2 d0j , aj Here, etj−1 is the embedding of the previously decoded symbol t, C the set of encoder hidden states for either the tokens or characters, ATT the attention function (dot-product) and dj the final decoder hidden state at step j. This model can easily be extended to more than two encoders, which we will experiment with in Section 4. This type of multi-source model is commonly used to represent different languages, e.g., in machine translation (Zoph and Knight, 2016; Firat et al., 2016) and semantic parsing (Susanto and Lu, 2017; Duong et al., 2017), though it has also been successfully applied in multi-modal translation (Libovick´y and Helcl, 2017), multi-framework semantic parsing (Stanovsky and Dagan, 2018) and adding linguistic information (Currey and Heafield, 2018; van Noord et al., 2019). To the best of our knowledge, we are the first to represent the characters as a source of extra information in a multisource sequence-to-sequence model. Transformer We also experiment with the Transformer model (Vaswani et al., 2017), using the stacked self attention model as impleme"
2020.emnlp-main.371,2020.acl-main.609,0,0.17243,"a (2012) used a probabilistic parsing model that used dependency structures to parse GMB data as graphs. More recently, Liu et al. (2018) proposed a neural model that produces (treestructured) DRSs in three steps by first learning the general (box) structure of a DRS, after which specific conditions and referents are filled in. In followup work (Liu et al., 2019a) they extend this work by adding an improved attention mechanism and constraining the decoder to ensure well-formed output. This model achieved impressive performance on both sentence-level and document-level DRS parsing on GMB data. Fu et al. (2020) in turn improve on this work by employing a Graph Attention Network during both encoding and decoding. The introduction of gold standard DRSs in the PMB enabled a principled comparison of approaches. In our previous work (van Noord et al., 2018b), we showed that sequence-to-sequence models can successfully learn to produce DRSs, with characters as the preferred representation. In follow-up work, we improved on these scores by adding linguistic features (van Noord et al., 2019). The first shared task on DRS parsing (Abzianidze et al., 2019) sparked more interested in the topic, with a system b"
2020.emnlp-main.371,C12-1094,0,0.0213732,"otation (Bjerva et al., 2016; Abzianidze and Bos, 2017), and all non-logical DRS symbols 4588 are grounded in either WordNet (Fellbaum, 1998) or VerbNet (Bonial et al., 2011). Moreover, its releases contain gold standard DRSs. For these reasons, we take the PMB as our corpus of choice to evaluate our DRS parsers. DRS parsing Early approaches to DRS parsing employed rule-based systems for small English texts (Johnson and Klein, 1986; Wada and Asher, 1986; Bos, 2001). The first open domain DRS parser is Boxer (Bos, 2008, 2015), which is a combination of rule-based and statistical models. Le and Zuidema (2012) used a probabilistic parsing model that used dependency structures to parse GMB data as graphs. More recently, Liu et al. (2018) proposed a neural model that produces (treestructured) DRSs in three steps by first learning the general (box) structure of a DRS, after which specific conditions and referents are filled in. In followup work (Liu et al., 2019a) they extend this work by adding an improved attention mechanism and constraining the decoder to ensure well-formed output. This model achieved impressive performance on both sentence-level and document-level DRS parsing on GMB data. Fu et al"
2020.emnlp-main.371,D14-1107,0,0.0180428,",620 885 898 97,598 146,371 German 1,159 417 403 5,250 121,111 Italian 0 515 547 2,772 64,305 Dutch 0 529 483 1,301 21,550 3.3 Sent I Linguistic features We want to contrast our method of character-level information to adding sources of linguistic information. Based on van Noord et al. (2019), we employ these five sources: part-of-speech tags (POS), dependency parses (DEP), lemmas (LEM), CCG supertags (CCG) and semantic tags (SEM). For the first three sources, we use Stanford CoreNLP (Manning et al., 2014) to parse the documents in our dataset. The CCG supertags are obtained by using easyCCG (Lewis and Steedman, 2014). For semantic tagging, we train our own trigram-based tagger using TnT (Brants, 2000).7 Table 2 shows a tagged example sentence for all five sources of information. Moreover, we also include non-contextual GLOVE and FASTTEXT embeddings as an extra source of information. We add these sources of linguistic information in the same way as we add the character-level information, in either one or two encoders (see Section 3.1). In two encoders, we can use the exact same architecture. For one encoder, we (obviously) do not use the char-CNN, but learn a separate embedding for the tags (of size 200),"
2020.emnlp-main.371,P17-2031,0,0.0196596,"Missing"
2020.emnlp-main.371,P18-1040,0,0.359196,"Fellbaum, 1998) or VerbNet (Bonial et al., 2011). Moreover, its releases contain gold standard DRSs. For these reasons, we take the PMB as our corpus of choice to evaluate our DRS parsers. DRS parsing Early approaches to DRS parsing employed rule-based systems for small English texts (Johnson and Klein, 1986; Wada and Asher, 1986; Bos, 2001). The first open domain DRS parser is Boxer (Bos, 2008, 2015), which is a combination of rule-based and statistical models. Le and Zuidema (2012) used a probabilistic parsing model that used dependency structures to parse GMB data as graphs. More recently, Liu et al. (2018) proposed a neural model that produces (treestructured) DRSs in three steps by first learning the general (box) structure of a DRS, after which specific conditions and referents are filled in. In followup work (Liu et al., 2019a) they extend this work by adding an improved attention mechanism and constraining the decoder to ensure well-formed output. This model achieved impressive performance on both sentence-level and document-level DRS parsing on GMB data. Fu et al. (2020) in turn improve on this work by employing a Graph Attention Network during both encoding and decoding. The introduction"
2020.emnlp-main.371,P19-1629,0,0.183763,"sing employed rule-based systems for small English texts (Johnson and Klein, 1986; Wada and Asher, 1986; Bos, 2001). The first open domain DRS parser is Boxer (Bos, 2008, 2015), which is a combination of rule-based and statistical models. Le and Zuidema (2012) used a probabilistic parsing model that used dependency structures to parse GMB data as graphs. More recently, Liu et al. (2018) proposed a neural model that produces (treestructured) DRSs in three steps by first learning the general (box) structure of a DRS, after which specific conditions and referents are filled in. In followup work (Liu et al., 2019a) they extend this work by adding an improved attention mechanism and constraining the decoder to ensure well-formed output. This model achieved impressive performance on both sentence-level and document-level DRS parsing on GMB data. Fu et al. (2020) in turn improve on this work by employing a Graph Attention Network during both encoding and decoding. The introduction of gold standard DRSs in the PMB enabled a principled comparison of approaches. In our previous work (van Noord et al., 2018b), we showed that sequence-to-sequence models can successfully learn to produce DRSs, with characters"
2020.emnlp-main.371,W19-1203,0,0.185792,"sing employed rule-based systems for small English texts (Johnson and Klein, 1986; Wada and Asher, 1986; Bos, 2001). The first open domain DRS parser is Boxer (Bos, 2008, 2015), which is a combination of rule-based and statistical models. Le and Zuidema (2012) used a probabilistic parsing model that used dependency structures to parse GMB data as graphs. More recently, Liu et al. (2018) proposed a neural model that produces (treestructured) DRSs in three steps by first learning the general (box) structure of a DRS, after which specific conditions and referents are filled in. In followup work (Liu et al., 2019a) they extend this work by adding an improved attention mechanism and constraining the decoder to ensure well-formed output. This model achieved impressive performance on both sentence-level and document-level DRS parsing on GMB data. Fu et al. (2020) in turn improve on this work by employing a Graph Attention Network during both encoding and decoding. The introduction of gold standard DRSs in the PMB enabled a principled comparison of approaches. In our previous work (van Noord et al., 2018b), we showed that sequence-to-sequence models can successfully learn to produce DRSs, with characters"
2020.emnlp-main.371,E03-1053,0,0.165795,"s (Abzianidze et al., 2017, see Section 3.3), allowing us to evaluate our models on multiple languages. 2 2.1 Background Character-level models The power of character-level representations has long been known in the field. In earlier work, they were successfully used in a range of tasks, including text-to-speech (Sejnowski and Rosenberg, 1987), parallel text alignment (Church, 1993), grapheme to phoneme conversion (Kaplan and Kay, 1994), language identification (Dunning, 1994), topical similarity prediction (Cavnar, 1994), named entity recognition (Klein et al., 2003), authorship attribution (Peng et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al.,"
2020.emnlp-main.371,D14-1162,0,0.0853187,"that (possible) improved performance is not simply due to larger model capacity, since during tuning of the baseline models a larger RNN hidden size did not result in better performance. 3.2 Representations We will experiment with five well-known pretrained language models: ELMO (Peters et al., 2018), BERT base/large (Devlin et al., 2019) and ROBERTA base/large (Liu et al., 2019c).4 The performance of these five large LMs is contrasted with results of a character-level model and three wordbased models. The word-based models either learn the embeddings from scratch or use non-contextual GLOVE (Pennington et al., 2014) or FASTTEXT (Grave et al., 2018) embeddings. Pre- and postprocessing of the DRSs is done using the method described in van Noord et al. (2018b).5 The DRSs are linearized, after which the variables are rewritten to a relative representation. The character-level model 3 See Appendix B for specific hyperparameter settings. We are aware that there exist several other large pretrained language models (e.g., Yang et al., 2019; Raffel et al., 2020; Clark et al., 2020), but we believe that the models we used have had the largest impact on the field. 5 https://github.com/RikVN/Neural_DRS/ 4590 4 Gold"
2020.emnlp-main.371,N18-1202,0,0.326977,"arger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena. 1 Introduction Character-level models have obtained impressive performance on a number of NLP tasks, ranging from the classic POS-tagging (Santos and Zadrozny, 2014) to complex tasks such as Discourse Representation Structure (DRS) parsing (van Noord et al., 2018b). However, this was before the large pretrained language models (Peters et al., 2018; Devlin et al., 2019) took over the field, with the consequence that for most NLP tasks, state-ofthe-art performance is now obtained by fine-tuning on one of these models (e.g., Conneau et al., 2020). Does this mean that, despite a long tradition of being used in language-related tasks (see Section 2.1), character-level representations are no longer useful? We try to answer this question by looking at semantic parsing, specifically DRS parsing (Abzianidze et al., 2017; van Noord et al., 2018a). We aim to answer the following research questions: 1. Do pretrained language models (LMs) outperfor"
2020.emnlp-main.371,P16-2067,0,0.0320971,"e conversion (Kaplan and Kay, 1994), language identification (Dunning, 1994), topical similarity prediction (Cavnar, 1994), named entity recognition (Klein et al., 2003), authorship attribution (Peng et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representations are employed, they are commonly used in combination with non-contextual word-level representations (Lewis et al., 2016; Ballesteros and Al-Onaizan, 2017; Groschwitz et al., 2018; Cai and Lam, 2019). There are a few recent studies that did use character-level representations in combination with BERT (Zhang et al., 2019a,b; Cai and Lam, 2020),"
2020.emnlp-main.371,E17-3017,0,0.0262593,"Missing"
2020.emnlp-main.371,Q19-1002,0,0.0167484,"vel representations are no longer useful? We try to answer this question by looking at semantic parsing, specifically DRS parsing (Abzianidze et al., 2017; van Noord et al., 2018a). We aim to answer the following research questions: 1. Do pretrained language models (LMs) outperform character-level models for DRS parsing? Why semantic parsing? Semantic parsing is the task of automatically mapping natural language utterances to interpretable meaning representations. The produced meaning representations can then potentially be used to improve downstream NLP applications (e.g., Issa et al., 2018; Song et al., 2019; Mihaylov and Frank, 2019), though the introduction of large pretrained language models has shown that explicit formal meaning representations might not be a necessary component to achieve high accuracy. However, it is now known that these models lack reasoning capabilities, often simply exploiting statistical artifacts in the data sets, instead of actually understanding language (Niven and Kao, 2019; McCoy et al., 2019). Moreover, Ettinger (2020) found that the popular BERT model (Devlin et al., 2019) completely failed to acquire a general understanding of negation. Related, Bender and Kolle"
2020.emnlp-main.371,D18-1263,0,0.0227515,"den states for either the tokens or characters, ATT the attention function (dot-product) and dj the final decoder hidden state at step j. This model can easily be extended to more than two encoders, which we will experiment with in Section 4. This type of multi-source model is commonly used to represent different languages, e.g., in machine translation (Zoph and Knight, 2016; Firat et al., 2016) and semantic parsing (Susanto and Lu, 2017; Duong et al., 2017), though it has also been successfully applied in multi-modal translation (Libovick´y and Helcl, 2017), multi-framework semantic parsing (Stanovsky and Dagan, 2018) and adding linguistic information (Currey and Heafield, 2018; van Noord et al., 2019). To the best of our knowledge, we are the first to represent the characters as a source of extra information in a multisource sequence-to-sequence model. Transformer We also experiment with the Transformer model (Vaswani et al., 2017), using the stacked self attention model as implemented in AllenNLP. A possible advantage of this model is that it might handle longer sentences and documents better. However, it might be harder to tune (Popel and Bojar, 2018)2 and its improved performance has mainly been shown"
2020.emnlp-main.371,P17-2007,0,0.0282557,"j = LSTM1 dj−1 , etj−1    aj = ATT Ctok , d0j ; ATT Cchar , d0j  dj = LSTM2 d0j , aj Here, etj−1 is the embedding of the previously decoded symbol t, C the set of encoder hidden states for either the tokens or characters, ATT the attention function (dot-product) and dj the final decoder hidden state at step j. This model can easily be extended to more than two encoders, which we will experiment with in Section 4. This type of multi-source model is commonly used to represent different languages, e.g., in machine translation (Zoph and Knight, 2016; Firat et al., 2016) and semantic parsing (Susanto and Lu, 2017; Duong et al., 2017), though it has also been successfully applied in multi-modal translation (Libovick´y and Helcl, 2017), multi-framework semantic parsing (Stanovsky and Dagan, 2018) and adding linguistic information (Currey and Heafield, 2018; van Noord et al., 2019). To the best of our knowledge, we are the first to represent the characters as a source of extra information in a multisource sequence-to-sequence model. Transformer We also experiment with the Transformer model (Vaswani et al., 2017), using the stacked self attention model as implemented in AllenNLP. A possible advantage of t"
2020.emnlp-main.371,D18-1278,0,0.0295909,"Missing"
2020.emnlp-main.371,W07-0705,0,0.0526411,"s to evaluate our models on multiple languages. 2 2.1 Background Character-level models The power of character-level representations has long been known in the field. In earlier work, they were successfully used in a range of tasks, including text-to-speech (Sejnowski and Rosenberg, 1987), parallel text alignment (Church, 1993), grapheme to phoneme conversion (Kaplan and Kay, 1994), language identification (Dunning, 1994), topical similarity prediction (Cavnar, 1994), named entity recognition (Klein et al., 2003), authorship attribution (Peng et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representa"
2020.wmt-1.130,P07-2045,0,0.00826844,"Missing"
2020.wmt-1.130,P10-2041,0,0.351832,"ences. The 10 million German sentences include all of the data from years 2007 and 2010, and the remaining sentences are taken from 2014.5 Our initial model achieves BLEU scores of 17.43 and 19.05 for DE→HSB and HSB→DE respectively. 3.1 Data Selection We apply two forms of data selection: sentencelevel and document-level. As we have an abundance of German data (D) and limited Upper Sorbian data (H), we are only concerned with data selection for German. To select from D, we first must score our data in terms of its potential to improve the performance of our NMT model. Drawing inspiration from Moore and Lewis (2010), our scoring function is as follows: Score(s) = LMH→D0 (s) − LMD (s) |s| In this equation, s refers to any sentence in the German data, |s |to its token length, LMX (s) to the log probability of s using a language model trained on dataset X , and H → D0 to the dataset obtained by translating H into German using the initial system. A high scoring sentence is thus a sentence that has a high probability according to the Upper Sorbian language model compared to that of the German language model.6 3 The max length increase was found to perform slightly better in early testing. 4 Both steps are lim"
2020.wmt-1.130,P16-1162,0,0.016714,"ak this broad question down into 3 concrete sub-questions, tailored for the unsupervised setting. They are as follows: Setup For Upper Sorbian, we use the 3 monolingual datasets provided by the Sorbian Institute, the Witaj Sprachzentrum, and the web data from CIS, LMU. We also use the Upper Sorbian side of the parallel corpus from train.hsb-de.hsb.gz. For German, we use monolingual data from News Crawl and Common Crawl. For validation and testing, we use the data provided in devtest.tar.gz. All data is tokenized and truecased using the Moses toolkit (Koehn et al., 2007). For BPE segmentation (Sennrich et al., 2016), we apply a joint segmentation for both languages. This is done by first taking a sample of the German data of the same length as the Upper Sorbian data (around 750 thousand sentences). The BPE codes are learned and applied using FastBPE.1 After BPE is applied, we remove duplicate sentences while retaining the order of the corpora.2 We used the XLM model (Conneau and Lample, 2019) using the default parameters, with the excep1 https://github.com/glample/fastBPE For document-level filtering, we do not remove duplicates. 2 1099 Proceedings of the 5th Conference on Machine Translation (WMT), page"
2020.wmt-1.130,P13-2121,0,0.0232964,"st News Crawl data available. 6 The intuition behind subtracting the score of the German language model is that without it a sentence may have a high score due to it containing frequent words in general (e.g. “the”) rather than words that are particularly frequent in the Upper Sorbian dataset (e.g. “Sorbia”). DE→HSB 5.21 16.98 15.08 9.32 17.03 17.60 HSB→DE 5.91 18.45 18.05 8.46 18.19 19.23 Table 1: BLEU scores for XLM trained on data selected with the lowest and highest sentence and documentlevel scores, as well as randomly selected sentences and documents. The language model we use is KenLM (Heafield et al., 2013). We use a trigram model, with all other parameters being the default values. Since we require a portion of the German dataset to train the model, we choose N sentences randomly, with N being equal to the number of sentences in H.7 These sentences are not included during the selection process. For sentence-level selection, we simply order each sentence based on score and select the sentences with the highest scores. For document-level selection, we score each document by averaging its sentence-level scores, and select the documents with the highest scores. To answer our first research question"
2020.wmt-1.29,W16-2302,0,0.0399754,"Missing"
2020.wmt-1.29,W02-0603,0,0.0308975,"tation may be particularly beneficial for translation with polysynthetic languages because it could provide more consistent isolation of concepts into subwords. We evaluated a broad pool of segmenters to determine how close various methods can achieve linguistically correct segmentation, comparing results to reference segmentations obtained from the Inuktitut Computing GitHub repository3 . This repository contains 1096 Inuktitut words, manually segmented at the National Research Council of Canada (NRC). Our experiments include: Rule-based with Uqailaut4 ; Morfessor Baseline (semi-supervised) (Creutz and Lagus, 2002); Morfessor FlatCat (semisupervised) (Gr¨onroos et al., 2014); LMVR (unsupervised) (Ataman et al., 2017); and Neural Transformer segmentation (supervised). We used Uqailaut’s rule-based segmenter to create additional annotated segmentations used to train the supervised and semi-supervised systems. In total 600,000 segmentations of unique words from the Hansard training dataset were created. All semisupervised and unsupervised systems were trained with the Hansard training corpus. For training semisupervised methods, we use 60,000 of the collected segmentations with Uqailaut as annotated traini"
2020.wmt-1.29,N19-1423,0,0.00525999,"ages present numerous challenges for NMT. We present the results of our systems for the English–Inuktitut language pair for the WMT 2020 translation tasks. We investigated the importance of correct morphological segmentation, whether or not adding data from a related language (Greenlandic) helps, and whether using contextual word embeddings improves translation. While each method showed some promise, the results are mixed. 1 • RQ3. Does the translation benefit from using contextual word embeddings? The use of such embeddings has proven beneficial for many tasks in natural language processing (Devlin et al., 2019), including MT (Zhu et al., 2020), so we deem it sensible to test this for a polysynthetic language, which we will do by means of masked language modelling pre-training. Introduction This paper presents the neural machine translation (NMT) systems submitted by the University of Groningen to the WMT 2020 translation task1 between Inuktitut and English in both directions (EN↔IU), describing both constrained and unconstrained systems where we investigated the following research questions: • RQ1. Does morphological segmentation benefit translation with polysynthetic languages? Existing NMT researc"
2020.wmt-1.29,C14-1111,0,0.0383512,"Missing"
2020.wmt-1.29,2020.lrec-1.312,0,0.128072,"000 update steps and apply early stopping after the validation cost stalls 5 times in a row. The model with the best translation score on the validation set (Section 2) is stored for each experiment. 4.1 Constrained Systems Our constrained systems can be divided into four groups according to the techniques used: tags, backtranslation and domain-specific data (section 4.1.1), morphological segmentation (4.1.2), contextual word embeddings (4.1.3) and ensembling and fine tuning (4.1.4). Model BPE 5K Morfessor FlatCat LMVR Trf. (single) 3-St.+LMVR 4.1.1 Initial Systems In these systems, following Joanis et al. (2020), we segment the training data with BPE (Sennrich et al., 2016) separately on each language. 5,000 and 2,000 merges are performed on both languages for MT systems into EN and IU, respectively. Table 4 shows our initial constrained systems and their results on the development set. System 1 2 3 4 IU→EN News Hansards 14.73 29.62 17.96 29.7 17.24 28.88 22.24 30.05 EN→IU News Hansards 40.29 52.97 47.47 54.20 51.31 53.86 NA NA IU→EN News Hans. 14.77 28.31 13.39 26.82 12.86 26.49 14.98 27.50 11.31 24.56 15.25 28.06 EN→IU News Hans. 32.52 39.81 28.75 38.20 23.25 29.88 34.84 41.25 31.34 39.33 34.51 40."
2020.wmt-1.29,Q17-1024,0,0.0309592,"Missing"
2020.wmt-1.29,P18-4020,0,0.0212495,"Missing"
2020.wmt-1.29,N18-1005,0,0.0279411,"Missing"
2020.wmt-1.29,W18-4808,0,0.0140157,"ic has been shown to correlate better than BLEU with human evaluation when the target language is agglutinative (Bojar et al., 2016). BLEU is our primary evaluation metric for IU→EN systems, as the correlations with human evaluation of BLEU and CHRF are roughly on par for EN as the target language. Prior to evaluation the MT output is detruecased (only EN) and detokenized with Moses’ scripts. 3 Segmentation with intrinsic evaluation Like many polysynthetic languages, Inuktitut has a high degree of inflection and agglutination, leading to very long words with a very high morpheme-toword ratio (Mager et al., 2018). By our estimation, Inuktitut has an average of around 4.39 morphemes per word. This means on average there are more potential boundaries, as well as more actual segmentation boundaries to locate per word, making segmentation particularly challenging. Inconsistent segmentation harms an NMT model’s ability to extract knowledge, because it reduces the frequency and activation of all vocabulary items during training, such that for each individual element in the vocabulary is found in fewer contexts. At inference, inconsistent segmentation can result in morphs that are out-of-vocabulary, resultin"
2020.wmt-1.29,W17-0114,0,0.028099,"600,000 segmentations, suggesting that the consistency of the data is more important than the quantity. The other segmenters clearly struggled with the long words, often splitting words into a combination of very long root, and very short morphs. FlatCat scored the highest of the existing methods on both F1 and accuracy. Unfortunately, both the neural and rule-based models sometimes fail to segment the input word. This makes them unfit to use in a translation system; since some words are left unsegmented, and this leads to a very large vocabulary size which hurts the translation performance. Micher (2017) previously explored improving the coverage of the Uqailaut morphological analyser with the use of an RNN based approach. In Micher (2018), an SRNN extension to the Uqailaut morphological analyzer is used in an SMT system, and yields a statistically significant improvement for IU→EN translation compared to the unextended rule-based analysis. Similar to their approach, we combined the best performing models of the intrinsic evaluation, to construct a custom 3-step segmenter to improve the coverage. This method initially applies the rule-based segmenter. If the rule-based segmenter fails, it fal"
2020.wmt-1.29,W18-4807,0,0.0287984,"ggled with the long words, often splitting words into a combination of very long root, and very short morphs. FlatCat scored the highest of the existing methods on both F1 and accuracy. Unfortunately, both the neural and rule-based models sometimes fail to segment the input word. This makes them unfit to use in a translation system; since some words are left unsegmented, and this leads to a very large vocabulary size which hurts the translation performance. Micher (2017) previously explored improving the coverage of the Uqailaut morphological analyser with the use of an RNN based approach. In Micher (2018), an SRNN extension to the Uqailaut morphological analyzer is used in an SMT system, and yields a statistically significant improvement for IU→EN translation compared to the unextended rule-based analysis. Similar to their approach, we combined the best performing models of the intrinsic evaluation, to construct a custom 3-step segmenter to improve the coverage. This method initially applies the rule-based segmenter. If the rule-based segmenter fails, it falls back on the Transformer (unambigu5 https://marian-nmt.github.io/ Out of the 600,000 words, Uqailaut produces unambiguous segmentations"
2020.wmt-1.29,P02-1040,0,0.106397,"newsdevtrain and newsdevdev, respectively. Tables 1 and 2 show the parallel and monolingual datasets, respectively, used for training after preprocessing. Words Corpus Sentences EN IU Hansards 769810 17303903 8236210 Newsdevtrain 1859 40154 24121 Table 1: Preprocessed EN–IU parallel training data. Lang. Corpus Sentences Words IU Common Crawl 28391 381805 EN Newscrawl 5000000 143776337 Table 2: Preprocessed monolingual training data. During development, we evaluated our systems on the news and Hansards portions of the development set, separately. We used two automatic evaluation metrics: BLEU (Papineni et al., 2002) and CHRF (Popovi´c, 2015). CHRF is our primary evaluation metric for EN→IU, due to the fact that this metric has been shown to correlate better than BLEU with human evaluation when the target language is agglutinative (Bojar et al., 2016). BLEU is our primary evaluation metric for IU→EN systems, as the correlations with human evaluation of BLEU and CHRF are roughly on par for EN as the target language. Prior to evaluation the MT output is detruecased (only EN) and detokenized with Moses’ scripts. 3 Segmentation with intrinsic evaluation Like many polysynthetic languages, Inuktitut has a high"
2020.wmt-1.29,W15-3049,0,0.0593128,"Missing"
2020.wmt-1.29,W16-2322,1,0.901115,"Missing"
2020.wmt-1.29,P16-1162,0,0.331035,"olysynthetic language, which we will do by means of masked language modelling pre-training. Introduction This paper presents the neural machine translation (NMT) systems submitted by the University of Groningen to the WMT 2020 translation task1 between Inuktitut and English in both directions (EN↔IU), describing both constrained and unconstrained systems where we investigated the following research questions: • RQ1. Does morphological segmentation benefit translation with polysynthetic languages? Existing NMT research showed that morphological segmentation outperforms bytepair encoding (BPE) (Sennrich et al., 2016) for some agglutinative languages. For example, rule-based morphological segmentation improved English-to-Finnish translation (S´anchez-Cartagena and Toral, 2016). and unsupervised morphological segmentation improved Turkish-to-English translation (Ataman et al., 2017). We investigate if morphological segmentation also improves translation performance for polysynthetic languages, and if effects differ depending on translation direction. 1 http://www.statmt.org/wmt20/ translation-task.html In section 2 we present the main data and evaluation measures used. In section 3 we present experiments wi"
2020.wmt-1.29,2020.eamt-1.61,0,0.0172622,"KL. These texts were also manually extracted. Thirdly, parallel data from 21 multilingual websites containing DA and KL texts, was crawled using bitextor11 . 4.2.2 MT with Unconstrained Data These datasets are pre-processed just like the ones from the constrained setup. In addition, we select a subset using their sentence alignment confidence score.12 The KL crawl is paired with Danish. We performed language classification on the Danish data using LangID13 , removing any sentence pairs not classified as Danish. Danish was translated into English with a pretrained DA→EN system14 from OPUS-MT (Tiedemann and Thottingal, 2020). Dataset details are presented in Table 10. Words Corpus Sentences EN IU/KL IU Magazine 1134 29312 18152 KL Magazine 657 13009 7491 KL crawl 14778 277159 163468 Table 9: Results of the constrained systems that use ensembling (referred to as ens) and fine tuning (FT) for both translation directions and both dev sets. The scores are BLEU (IU→EN) and CHRF (EN→IU). Best results shown in bold. Ensembles are built by training the same system with different seeds (4 into EN and 3 into IU) and picking the model from each training seed with the highest score. These bring consistent improvements for bo"
2020.wmt-1.29,D16-1163,0,0.0250856,"nio Toral‡ †Institute for Artificial Intelligence ‡Center for Language and Cognition, University of Groningen The Netherlands c.roest@student.rug.nl, j.l.edman@rug.nl, g.f.minnema@rug.nl kevin.kelly@live.se, j.spenader@ai.rug.nl, a.toral.ruiz@rug.nl Abstract • RQ2. Does the use of additional data from a related language, Greenlandic (KL), improve the outcome? Due to the scarcity of EN–IU parallel data, we investigate if adding Greenlandic data to the Inuktitut data to train a multilingual NMT system (Johnson et al., 2017), improves the performance of the NMT systems on the unconstrained task (Zoph et al., 2016). Translating to and from low-resource polysynthetic languages present numerous challenges for NMT. We present the results of our systems for the English–Inuktitut language pair for the WMT 2020 translation tasks. We investigated the importance of correct morphological segmentation, whether or not adding data from a related language (Greenlandic) helps, and whether using contextual word embeddings improves translation. While each method showed some promise, the results are mixed. 1 • RQ3. Does the translation benefit from using contextual word embeddings? The use of such embeddings has proven"
2021.acl-short.62,2020.acl-main.703,0,0.0905078,"Missing"
2021.acl-short.62,P18-1080,0,0.0353935,"Missing"
2021.acl-short.62,N18-1012,0,0.399325,"re popular. These include disentangling style and content by learning a distinct representation for each (Shen et al., 2017; Fu et al., 2018; John et al., 2019), and back translation (Zhang et al., 2018; Lample et al., 2019; Luo et al., 2019; Prabhumoye et al., 2018). A common strategy to enhance style accuracy is to introduce a reward in the form of a style classifier (Lample et al., 2019; Gong et al., 2019; Luo et al., 2019; Wu et al., 2019; Sancheti et al., 2020). As a result, unsupervised models achieve good accuracy in style strength. Content preservation is however usually unsuccessful (Rao and Tetreault, 2018). Parallel data can help to preserve content, but is limited. Niu et al. (2018) combine the train sets of two different domains and incorporate machine translation to train their models with a multi-task learning schema, plus model ensembles. Sancheti et al. (2020) use it to train a supervised sequence-tosequence model, and in addition to the commonly used style strength reward, they include a reward based on BLEU (Papineni et al., 2002) to enhance content preservation. Shang et al. (2019) propose a semi-supervised model combining parallel data with large amounts of non-parallel data. Pre-trai"
2021.acl-short.62,2020.acl-main.704,0,0.0572608,"Missing"
2021.acl-short.62,D19-1499,0,0.0347138,"Missing"
2021.acl-short.62,D19-1365,0,0.0357564,"Missing"
2021.acl-short.62,2020.coling-main.203,0,0.0854563,"Missing"
2021.acl-short.62,P19-1482,0,0.0449996,"Missing"
2021.emnlp-main.349,W18-2703,0,0.023279,"nce on Empirical Methods in Natural Language Processing, pages 4241–4254 c November 7–11, 2021. 2021 Association for Computational Linguistics In practice, we propose a framework that adopts a multi-step procedure which builds upon a general-purpose pre-trained sequence-to-sequence (seq2seq) model. First, we strengthen the model’s ability to rewrite by conducting a second phase of pre-training on natural pairs derived from an existing collection of generic paraphrases, as well as on synthetic pairs created using a general-purpose lexical resource. Second, through an iterative backtranslation (Hoang et al., 2018) approach, we train two models, each in a transfer direction, so that they can provide each other with synthetically generated pairs on-the-fly. Lastly, we use our best resulting model to generate static synthetic pairs, which are then used offline as parallel training data. be used as pairs to train the model of the opposite transfer direction. Another common strategy is to use style-wordediting (Li et al., 2018; Xu et al., 2018; Wu et al., 2019; Lee, 2020) to explicitly separate content and style. These approaches first detect relevant words in the source and then do operations like deleting"
2021.emnlp-main.349,K19-1005,0,0.0612565,"Missing"
2021.emnlp-main.349,P19-1041,0,0.0217944,"e transfer direction. Another common strategy is to use style-wordediting (Li et al., 2018; Xu et al., 2018; Wu et al., 2019; Lee, 2020) to explicitly separate content and style. These approaches first detect relevant words in the source and then do operations like deleting, inserting and combining to create the pair’s target. Back-transferring is generally used to reconstruct the source sentence for training, so that pairs are also made on-the-fly. Lample et al. (2019) provide evidence that disentangling style and content to learn distinct representations (Shen et al., 2017; Fu et al., 2018; John et al., 2019; Yi et al., 2020) is not necessary. Reconstructing the source, instead, appears beneficial: Contributions Using a large pre-trained seq2seq it is used by Dai et al. (2019) who pre-train a model model (1) we achieve state-of-the-art results for the two most popular style transfer tasks without task- on style transfer data with the Transformer architecture (Vaswani et al., 2017); and by Zhou et al. specific parallel data. We show that (2) generic (2020), who use an attentional seq2seq model that resources can be leveraged to derive parallel data pre-trains the model to reconstruct the source se"
2021.emnlp-main.349,D14-1181,0,0.00370936,"this dataset in its entirety or filtered (models M1.1 and M1.2 in Table 3). In the first case, the whole of the paraphrase pairs from PARABANK 2 are used to further pretrain the model. In the second case, we follow the rationale that not all pairs are equally relevant for our tasks, and selecting task-specific ones could be beneficial. For instance, while both PARABANK 2 pairs in Table 1 are good examples of rewriting, the one on the right is more meaningful in terms of formality transfer. Therefore, we train two binary style classifiers, one for formality and one for polarity, using TextCNN (Kim, 2014) on the training sets of GYAFC and YELP. These classifiers are then used to automatically select more strongly style-opposed pairs. The resulting filtered paraphrase subset Dp is such a set of pairs: Dp = {(x, y)|(p(s1 |x) + p(s2 |y))/2 > σ} (2) σ = 0.85 in our experiments. Synthetic Pairs for Polarity Swap Due to the nature of polarity swap, we expect that even filtered paraphrases might not benefit polarity swap as much as formality. We therefore add another strategy to enhance polarity swap rewriting and create pairs for further pre-training exploiting a general-purpose lexical resource (mo"
2021.emnlp-main.349,2020.inlg-1.25,0,0.0170189,"s, as well as on synthetic pairs created using a general-purpose lexical resource. Second, through an iterative backtranslation (Hoang et al., 2018) approach, we train two models, each in a transfer direction, so that they can provide each other with synthetically generated pairs on-the-fly. Lastly, we use our best resulting model to generate static synthetic pairs, which are then used offline as parallel training data. be used as pairs to train the model of the opposite transfer direction. Another common strategy is to use style-wordediting (Li et al., 2018; Xu et al., 2018; Wu et al., 2019; Lee, 2020) to explicitly separate content and style. These approaches first detect relevant words in the source and then do operations like deleting, inserting and combining to create the pair’s target. Back-transferring is generally used to reconstruct the source sentence for training, so that pairs are also made on-the-fly. Lample et al. (2019) provide evidence that disentangling style and content to learn distinct representations (Shen et al., 2017; Fu et al., 2018; John et al., 2019; Yi et al., 2020) is not necessary. Reconstructing the source, instead, appears beneficial: Contributions Using a larg"
2021.emnlp-main.349,2020.acl-main.703,0,0.24799,"NLP tasks, large pre-trained models have been shown to provide an excellent base for fine- ident which strategy works best for creating partuning in a supervised setting (Chawla and Yang, allel data, whether offline or on-the-fly, and the simultaneous advantage of both strategies has not 2020; Lai et al., 2021). been fully explored. Lastly, Chawla and Yang Since parallel data for fine-tuning such large (2020) develop a semi-supervised model based on models for style transfer is scarce, a substantial amount of work has gone into methods for creat- sequence-to-sequence pre-trained model (BART, Lewis et al. (2020)) using parallel training data and ing artificial sentence pairs so that models can be large amounts of non-parallel data, which achieves trained in a supervised regime. One way to do this is to artificially generate par- a significant performance. In previous work, we have also shown that a sequence-to-sequence preallel data via back-translation, so that training pairs trained model (BART) outperforms a language are created on-the-fly during the training process itself (Zhang et al., 2018; Lample et al., 2019; Prab- model (GPT-2) in content preservation and overall performance when task-speci"
2021.emnlp-main.349,P02-1040,0,0.112176,"tly to modelling and evaluation metrics. 4243 PARABANK 2 Step 1: Further pre-training Aligned X–? "" modified using WordNet / X–Y from Paraphrase data Step 3: Final Training Step 2: IBT training Model A Model A X→? "" Model A Model B Model B Y→X "" Model B Figure 1: General overview of our pipeline. 3.2 Task Evaluation The performance of text style transfer is commonly assessed on style strength and content preservation. For style strength, using a pre-trained style classifier is the most popular automatic evaluation strategy. For content preservation, n-gram-based matching metrics such as BLEU (Papineni et al., 2002) are most commonly used. However, these metrics usually fail to recognise information beyond the lexical level. Since word embeddings (Mikolov et al., 2013; Pennington et al., 2014) have become the prime alternative to n-gram-based matching to capture similarity, embeddings-based metrics have also been developed (Fu et al., 2018). However, embedding-based metrics like cosine similarity still work at the token-level, and might fail to capture the overall semantics of a sentence. To overcome such limitations, recent work has developed learnable metrics, which attempt to directly optimize correla"
2021.emnlp-main.349,D14-1162,0,0.0850711,": IBT training Model A Model A X→? "" Model A Model B Model B Y→X "" Model B Figure 1: General overview of our pipeline. 3.2 Task Evaluation The performance of text style transfer is commonly assessed on style strength and content preservation. For style strength, using a pre-trained style classifier is the most popular automatic evaluation strategy. For content preservation, n-gram-based matching metrics such as BLEU (Papineni et al., 2002) are most commonly used. However, these metrics usually fail to recognise information beyond the lexical level. Since word embeddings (Mikolov et al., 2013; Pennington et al., 2014) have become the prime alternative to n-gram-based matching to capture similarity, embeddings-based metrics have also been developed (Fu et al., 2018). However, embedding-based metrics like cosine similarity still work at the token-level, and might fail to capture the overall semantics of a sentence. To overcome such limitations, recent work has developed learnable metrics, which attempt to directly optimize correlation with human judgments. These metrics, with the prime examples of BLEURT (Sellam et al., 2020) and COMET (Rei et al., 2020), have recently shown promising results in machine tran"
2021.emnlp-main.349,2020.emnlp-main.213,0,0.0181968,"l. Since word embeddings (Mikolov et al., 2013; Pennington et al., 2014) have become the prime alternative to n-gram-based matching to capture similarity, embeddings-based metrics have also been developed (Fu et al., 2018). However, embedding-based metrics like cosine similarity still work at the token-level, and might fail to capture the overall semantics of a sentence. To overcome such limitations, recent work has developed learnable metrics, which attempt to directly optimize correlation with human judgments. These metrics, with the prime examples of BLEURT (Sellam et al., 2020) and COMET (Rei et al., 2020), have recently shown promising results in machine translation evaluation. To the best of our knowledge, only our previous work used BLEURT in the evaluation of formality style transfer models (Lai et al., 2021); we are now proposing to use it also for the evaluation of polarity swap, and to add COMET to the pool of evaluation metrics to be systematically adopted in the evaluation of text style transfer tasks. Therefore, in addition to BLEU, which allows us to compare to previous work, we also use BLEURT and COMET. Let us bear in mind that “content preservation” does not mean exactly the same"
2021.emnlp-main.349,2020.acl-main.704,0,0.0248722,"ormation beyond the lexical level. Since word embeddings (Mikolov et al., 2013; Pennington et al., 2014) have become the prime alternative to n-gram-based matching to capture similarity, embeddings-based metrics have also been developed (Fu et al., 2018). However, embedding-based metrics like cosine similarity still work at the token-level, and might fail to capture the overall semantics of a sentence. To overcome such limitations, recent work has developed learnable metrics, which attempt to directly optimize correlation with human judgments. These metrics, with the prime examples of BLEURT (Sellam et al., 2020) and COMET (Rei et al., 2020), have recently shown promising results in machine translation evaluation. To the best of our knowledge, only our previous work used BLEURT in the evaluation of formality style transfer models (Lai et al., 2021); we are now proposing to use it also for the evaluation of polarity swap, and to add COMET to the pool of evaluation metrics to be systematically adopted in the evaluation of text style transfer tasks. Therefore, in addition to BLEU, which allows us to compare to previous work, we also use BLEURT and COMET. Let us bear in mind that “content preservation” do"
2021.emnlp-main.349,2020.emnlp-demos.6,0,0.0884038,"Missing"
2021.emnlp-main.349,P19-1482,0,0.0840865,"sfer literature, Text style transfer is, broadly put, the task con- since they do not use manually labelled data. verting a text of one style into another while preWe explore how parallel data can best be derived serving its content. In its recent tradition within and integrated in a general style transfer frameNatural Language Generation (NLG), two tasks work. To do so, we create pairs in a variety of and their corresponding datasets have been com- ways and use them in different stages of our framemonly used (Zhang et al., 2018; Luo et al., 2019; work. A core aspect of our approach is leveragWu et al., 2019; Yi et al., 2020; Zhou et al., 2020). ing generic resources to derive training pairs, both One dataset was specifically created for formality natural and synthetic. On the natural front, we transfer and contains parallel data (GYAFC (Rao use abundant data from a generic rewriting task: and Tetreault, 2018)), while the other one contains paraphrasing. As for synthetic data, we leverage a large amount of non-parallel sentiment labelled a general-purpose computational lexicon using its texts (YELP (Li et al., 2018)), with parallel pairs antonymy relation to generate polarity pairs. 4241 Proceedi"
2021.emnlp-main.349,P18-1090,0,0.021037,"g collection of generic paraphrases, as well as on synthetic pairs created using a general-purpose lexical resource. Second, through an iterative backtranslation (Hoang et al., 2018) approach, we train two models, each in a transfer direction, so that they can provide each other with synthetically generated pairs on-the-fly. Lastly, we use our best resulting model to generate static synthetic pairs, which are then used offline as parallel training data. be used as pairs to train the model of the opposite transfer direction. Another common strategy is to use style-wordediting (Li et al., 2018; Xu et al., 2018; Wu et al., 2019; Lee, 2020) to explicitly separate content and style. These approaches first detect relevant words in the source and then do operations like deleting, inserting and combining to create the pair’s target. Back-transferring is generally used to reconstruct the source sentence for training, so that pairs are also made on-the-fly. Lample et al. (2019) provide evidence that disentangling style and content to learn distinct representations (Shen et al., 2017; Fu et al., 2018; John et al., 2019; Yi et al., 2020) is not necessary. Reconstructing the source, instead, appears beneficia"
2021.emnlp-main.349,2020.acl-main.639,0,0.327674,"er is, broadly put, the task con- since they do not use manually labelled data. verting a text of one style into another while preWe explore how parallel data can best be derived serving its content. In its recent tradition within and integrated in a general style transfer frameNatural Language Generation (NLG), two tasks work. To do so, we create pairs in a variety of and their corresponding datasets have been com- ways and use them in different stages of our framemonly used (Zhang et al., 2018; Luo et al., 2019; work. A core aspect of our approach is leveragWu et al., 2019; Yi et al., 2020; Zhou et al., 2020). ing generic resources to derive training pairs, both One dataset was specifically created for formality natural and synthetic. On the natural front, we transfer and contains parallel data (GYAFC (Rao use abundant data from a generic rewriting task: and Tetreault, 2018)), while the other one contains paraphrasing. As for synthetic data, we leverage a large amount of non-parallel sentiment labelled a general-purpose computational lexicon using its texts (YELP (Li et al., 2018)), with parallel pairs antonymy relation to generate polarity pairs. 4241 Proceedings of the 2021 Conference on Empiric"
attia-etal-2010-automatically,W98-1002,0,\N,Missing
attia-etal-2010-automatically,D08-1030,0,\N,Missing
attia-etal-2010-automatically,W05-0711,0,\N,Missing
attia-etal-2010-automatically,P05-1071,0,\N,Missing
attia-etal-2010-automatically,2007.jeptalnrecital-poster.13,0,\N,Missing
attia-etal-2010-automatically,elkateb-etal-2006-building,0,\N,Missing
attia-etal-2010-automatically,farber-etal-2008-improving,0,\N,Missing
C12-1135,2010.amta-papers.16,1,0.91367,"Missing"
C12-1135,E06-1032,0,0.101057,"Missing"
C12-1135,P11-2071,0,0.0229814,"Missing"
C12-1135,eck-etal-2004-language,0,0.0341917,"), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their relevance to the target domain. Munteanu and Marcu (2005) extract in-domain sentence pairs from comparable corpora. Daumé III and Jagarlamudi (2011) attempt to reduce out-of-vocabulary terms when targeting a specific domain by mining their translations from comparable corpora. Bertoldi et al. (2009) rely on large in-domain m"
C12-1135,W08-0334,0,0.0173916,"can be identified: (i) combination of in-domain and out-of-domain resources for training, (ii) training data selection, and (iii) acquisition of specific-domain data. Below we briefly review a selection of relevant work that falls into these topics. The first attempt to perform domain adaptation was carried out by Langlais (2002), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their re"
C12-1135,D10-1044,0,0.0271916,"ain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their relevance to the target domain. Munteanu and Marcu (2005) extract in-domain sentence pairs from comparable corpora. Daumé III and Jagarlamudi (2011) attempt to reduce out-of-vocabulary terms when targeting a specific domain by mining their translations from comparable corpora. Bertoldi et al. (2009) rely on large in-domain monolingual data to create synthetic parallel corpora for training. 2211 languages (L1-L2) dom English–French gen env lab med English–Greek gen env lab med set train dev test dev test dev test dev test train d"
C12-1135,2010.amta-papers.27,1,0.903095,"Missing"
C12-1135,2005.eamt-1.19,0,0.0282124,"guage models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their relevance to the target domain. Munteanu and Marcu (2005) extract in-domain sentence pairs from comparable corpora. Daumé III and Jagarlamudi (2011) attempt to reduce out-of-vocabulary terms when targeting a specific domain by mining their translations from comparable corpora. Bertoldi et al. (2009) rely on large in-domain monolingual data to create synthetic parallel corpora for training. 2211 languages (L1-L2) dom English–French gen env lab med English–Greek"
C12-1135,D11-1125,0,0.0279752,"e model significantly prefers hypotheses with altered word order (which is consistent with the two preceding observations). 4. Language model weights (h8 ) do not change substantially, its importance remains similar on general-domain and specific-domain data. These findings are highly consistent across domains and language pairs. The weight vectors of the systems tuned on specific-domain data are quite similar but differ substantially from the parameters obtained by tuning on general-domain. This observation can be quantified by measuring cosine similarity (see Figure 2, right) as proposed by Hopkins and May (2011). Lower scores, as in the first rows/columns of each table, indicate low similarity of the vectors – specific-domain tuned weights differ a lot from the general-domain tuned ones; and vice versa – specific-domain tuned parameters are quite similar when compared to each other. 5.5 Analysis of phrase-length distribution From the analysis presented above, we conclude that a PB-SMT system tuned on data from the same domain as the training data strongly prefers to construct translations consisting of long phrases. Such phrases are usually of good translation quality (local mistakes of word alignmen"
C12-1135,W04-3250,0,0.190801,"Missing"
C12-1135,2005.mtsummit-papers.11,0,0.0315462,"et al. (2011) exploit automatically web-crawled in-domain resources for parameter optimization and improving language models. Pecina et al. (2012) extend the work by using the web-crawled resources to also improve translation models. 4 Experimental setup Our experimental setup follows and extends the one used in Pecina et al. (2011). In addition to the two evaluation domains (env, lab) used in that work, and in order to corroborate their earlier findings, we also carry out experiments on medical domain data (med). 4.1 Data Our general-domain system is trained on the Europarl parallel corpus (Koehn, 2005, v5) extracted from the proceedings of the European Parliament and for the purposes of this work considered to contain general-domain texts (it covers a very broad range of topics and it is to a considerable extent spoken language). The general-domain development and test data used for parameter optimization and testing, respectively, are adopted from the WPT 20051 machine translation shared task. These sets were extracted from the same source as Europarl and contain 2,000 sentence pairs each. The specific-domain development and test data for the env and lab domains were acquired by domain-fo"
C12-1135,P07-2045,0,0.0376072,"tem to a specific domain. Finally, given the fact that a general-domain system can only use limited length translation units when translating specific-domain data, we explore limited length training and decoding. After a brief overview of the log-linear model including its parameter optimization and an overview of the state-of-the-art in domain adaptation for SMT, we describe our experiments, present the results, the analysis, explore the resulting research questions with additional experiments, and conclude. 2 Phrase-Based Statistical Machine Translation In PB-SMT, implemented e.g. in Moses (Koehn et al., 2007), an input sentence is segmented into sequences of consecutive words, called phrases. Each phrase is then translated into a target language phrase, which may be reordered with other translated phrases to produce the output. 2210 Formally, the model is based on the noisy channel model. The translation e of an input sentence f is searched for by maximizing the translation probability p(e|f) formulated as a log-linear combination of a set of feature functions hi and their weights λi : Qn p(e|f) = i hi (e, f)λi Typically, the components include features of the following models: phrase translation"
C12-1135,N03-1017,0,0.0327235,"en longer phrases improve translation quality is for the systems trained, tuned and tested on the same (general) domain. In all other cases, the results for phrases up to three words long are as good as for longer phrases. If the domain of the test data does not match the domain of training and tuning data, the maximum phrase length set to three is enough in all scenarios. Longer phrases lead to degradation of translation quality and increase time for training and decoding, as well as memory requirements for building and storing the translation models. A similar result was reported already by Koehn et al. (2003). They observed that limiting the maximum length of a phrase to only three words achieved top performance. However, current state-of-the-art SMT systems usually benefit from longer phrases than three (see e.g. the top curve in Figure 5 which refers to a general-domain system applied to a general-domain test set), and our result applies only to scenarios where the training and test domains do not match; in that case setting the maximum phrase length to three is sufficient. 6 Conclusions In this work, we have analysed domain adaptation of PB-SMT by tuning parameters of the underlying log-linear"
C12-1135,W07-0733,0,0.0683277,"se, it often produces good results (Bertoldi et al., 2009). 3 Domain adaptation in Statistical Machine Translation Domain-adaptation is a very active research topic within the area of SMT. Three main topics can be identified: (i) combination of in-domain and out-of-domain resources for training, (ii) training data selection, and (iii) acquisition of specific-domain data. Below we briefly review a selection of relevant work that falls into these topics. The first attempt to perform domain adaptation was carried out by Langlais (2002), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting simila"
C12-1135,W02-1405,0,0.0358254,"lore the whole parameter space, it may converge to a local maximum; in practise, it often produces good results (Bertoldi et al., 2009). 3 Domain adaptation in Statistical Machine Translation Domain-adaptation is a very active research topic within the area of SMT. Three main topics can be identified: (i) combination of in-domain and out-of-domain resources for training, (ii) training data selection, and (iii) acquisition of specific-domain data. Below we briefly review a selection of relevant work that falls into these topics. The first attempt to perform domain adaptation was carried out by Langlais (2002), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. E"
C12-1135,J05-4003,0,0.0384329,"odel combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their relevance to the target domain. Munteanu and Marcu (2005) extract in-domain sentence pairs from comparable corpora. Daumé III and Jagarlamudi (2011) attempt to reduce out-of-vocabulary terms when targeting a specific domain by mining their translations from comparable corpora. Bertoldi et al. (2009) rely on large in-domain monolingual data to create synthetic parallel corpora for training. 2211 languages (L1-L2) dom English–French gen env lab med English–Greek gen env lab med set train dev test dev test dev test dev test train dev test dev test dev test dev test sentences 1,725,096 2,000 2,000 1,392 2,000 1,411 2,000 1,064 2,000 964,242 2,000 2,000"
C12-1135,W08-0320,0,0.0424553,"Domain-adaptation is a very active research topic within the area of SMT. Three main topics can be identified: (i) combination of in-domain and out-of-domain resources for training, (ii) training data selection, and (iii) acquisition of specific-domain data. Below we briefly review a selection of relevant work that falls into these topics. The first attempt to perform domain adaptation was carried out by Langlais (2002), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the tra"
C12-1135,P03-1021,0,0.0119307,"odel, which ensures that the translations are fluent, reordering (distortion) model, which allows to reorder phrases in the input sentences (e.g. distance-based and lexicalized reordering) and word penalty, which prevents the translations from being too long or too short. These models are trained on either parallel or monolingual training data. The weights of the log-linear combination influence overall translation quality; however, the optimal setting depends on the translation direction and data. A common solution to optimise weights is to use Minimum Error Rate Training (MERT), proposed by Och (2003), which automatically searches for the values that minimize a given error measure (or maximize a given translation quality measure) on a development set of parallel sentences. Theoretically, any automatic measure can be used for this purpose; however, the most commonly used is BLEU (Papineni et al., 2002). The search algorithm is a type of coordinate ascent: considering n-best translation hypotheses for each input sentence, it updates the feature weight most likely promising to improve the objective and iterates until convergence. The error surface is highly non-convex and as the algorithm can"
C12-1135,P02-1040,0,0.0883898,"ned on either parallel or monolingual training data. The weights of the log-linear combination influence overall translation quality; however, the optimal setting depends on the translation direction and data. A common solution to optimise weights is to use Minimum Error Rate Training (MERT), proposed by Och (2003), which automatically searches for the values that minimize a given error measure (or maximize a given translation quality measure) on a development set of parallel sentences. Theoretically, any automatic measure can be used for this purpose; however, the most commonly used is BLEU (Papineni et al., 2002). The search algorithm is a type of coordinate ascent: considering n-best translation hypotheses for each input sentence, it updates the feature weight most likely promising to improve the objective and iterates until convergence. The error surface is highly non-convex and as the algorithm cannot explore the whole parameter space, it may converge to a local maximum; in practise, it often produces good results (Bertoldi et al., 2009). 3 Domain adaptation in Statistical Machine Translation Domain-adaptation is a very active research topic within the area of SMT. Three main topics can be identifi"
C12-1135,2012.eamt-1.38,1,0.722345,"Missing"
C12-1135,2011.eamt-1.40,1,0.781939,"alizace parametr˚u. Proceedings of COLING 2012: Technical Papers, pages 2209–2224, COLING 2012, Mumbai, December 2012. 2209 1 Introduction Statistical Machine Translation (SMT) is an instance of a machine learning application and, in general, will work best if the data for training and testing are drawn from the same distribution (i.e. domain, genre, and style). In practice, however, it is often difficult to obtain sufficient amounts of in-domain data (in particular parallel data required for translation and distortion models) to train a well performing system for a specific domain. Recently, Pecina et al. (2011) showed that just using in-domain development data for parameter tuning improves output quality of a Phrase-Based SMT (PB-SMT) system trained on general-domain data but applied to a specific domain. Although further additional improvements can be realized by using in-domain parallel and/or monolingual training data, parameter tuning on in-domain data requires only a relatively small set of parallel sentences, which is often easier to obtain. They report on a series of experiments carried out on the domains of Natural Environment (env) and Labour Legislation (lab) and two language pairs: Englis"
C12-1135,2011.mtsummit-papers.58,0,0.0536528,"Missing"
C12-1135,C08-1125,0,0.0344385,"Missing"
del-gratta-etal-2008-simple,ruimy-toral-2008-semantic,1,\N,Missing
E12-2001,W10-1840,0,0.0132734,"s been used successfully to build workflows for PoS tagging and alignment. Some WSs, e.g. dependency parsers, require a more complex representation that cannot be handled by the TO. Therefore, a more expressive format has been adopted for these. The Graph Annotation Format (GrAF) (Ide and Suderman, 2007) is a XML representation of a graph that allows different levels of annotation using a “feature– value” paradigm. This system allows different in-house formats to be easily encapsulated in this container-based format. On the other hand, GrAF can be used as a pivot format between other formats (Ide and Bunt, 2010), e.g. there is software to convert GrAF to UIMA and GATE formats (Ide and Suderman, 2009) and it can be used to merge data represented in a graph. Interoperability Interoperability plays a crucial role in a platform of distributed WSs. Soaplab deploys SOAP10 WSs and handles automatically most of the issues involved in this process, while Taverna can combine SOAP and REST11 WSs. Hence, we can say that communication protocols are being handled by the tools. However, parameters and data interoperability need to be addressed. 5.1 Travelling Object Common Interface Both TO and GrAF address syntact"
E12-2001,W07-1501,0,0.0136085,"connect the different tools in the platform regardless of their original input/output formats. We have adopted for TO the XML Corpus Encoding Standard (XCES) format (Ide et al., 2000) because it was the already existing format that required the minimum transduction effort from the in-house formats. The XCES format has been used successfully to build workflows for PoS tagging and alignment. Some WSs, e.g. dependency parsers, require a more complex representation that cannot be handled by the TO. Therefore, a more expressive format has been adopted for these. The Graph Annotation Format (GrAF) (Ide and Suderman, 2007) is a XML representation of a graph that allows different levels of annotation using a “feature– value” paradigm. This system allows different in-house formats to be easily encapsulated in this container-based format. On the other hand, GrAF can be used as a pivot format between other formats (Ide and Bunt, 2010), e.g. there is software to convert GrAF to UIMA and GATE formats (Ide and Suderman, 2009) and it can be used to merge data represented in a graph. Interoperability Interoperability plays a crucial role in a platform of distributed WSs. Soaplab deploys SOAP10 WSs and handles automatica"
E12-2001,W09-3004,0,0.142124,".g. dependency parsers, require a more complex representation that cannot be handled by the TO. Therefore, a more expressive format has been adopted for these. The Graph Annotation Format (GrAF) (Ide and Suderman, 2007) is a XML representation of a graph that allows different levels of annotation using a “feature– value” paradigm. This system allows different in-house formats to be easily encapsulated in this container-based format. On the other hand, GrAF can be used as a pivot format between other formats (Ide and Bunt, 2010), e.g. there is software to convert GrAF to UIMA and GATE formats (Ide and Suderman, 2009) and it can be used to merge data represented in a graph. Interoperability Interoperability plays a crucial role in a platform of distributed WSs. Soaplab deploys SOAP10 WSs and handles automatically most of the issues involved in this process, while Taverna can combine SOAP and REST11 WSs. Hence, we can say that communication protocols are being handled by the tools. However, parameters and data interoperability need to be addressed. 5.1 Travelling Object Common Interface Both TO and GrAF address syntactic interoperability while semantic interoperability is still an open topic. To facilitate"
E12-2001,ide-etal-2000-xces,0,0.0258492,"g there are more than 100 WSs and 30 workflows registered. 5 5.2 A goal of the project is to facilitate the deployment of as many tools as possible in the form of WSs. In many cases, tools performing the same task use in-house formats. We have designed a container, called “Travelling Object” (TO), as the data object that is being transfered between WSs. Any tool that is deployed needs to be adapted to the TO, this way we can interconnect the different tools in the platform regardless of their original input/output formats. We have adopted for TO the XML Corpus Encoding Standard (XCES) format (Ide et al., 2000) because it was the already existing format that required the minimum transduction effort from the in-house formats. The XCES format has been used successfully to build workflows for PoS tagging and alignment. Some WSs, e.g. dependency parsers, require a more complex representation that cannot be handled by the TO. Therefore, a more expressive format has been adopted for these. The Graph Annotation Format (GrAF) (Ide and Suderman, 2007) is a XML representation of a graph that allows different levels of annotation using a “feature– value” paradigm. This system allows different in-house formats"
E14-4036,N10-1062,0,0.159017,"Missing"
E14-4036,W13-2235,0,0.0830041,"entation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), originally used for domain adaptation, and propose an extension to cross entropy difference with a vocabulary saturation filter (Lewis and Eetemadi, 2013). • While much of previous work concentrates on research datasets (e.g. Europarl, News Commentary), we use industry data (techni185 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185–189, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics cal manuals). (Bertoldi et al., 2013) shows that the repetition rate of news is considerably lower than that of technical documentation, which impacts on the results obtained with incremental retraining. method, where s and t stand for source and target, re"
E14-4036,D11-1033,0,0.261389,"al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domains such as technical documentation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), originally used for domain adaptation, and propose an extension to cross entropy difference with a vocabulary saturation filter (Lewis and Eetemadi, 2013). • While much of previous work concentrates on research datasets (e.g. Europarl, News Commentary), we use industry data (techni185 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185–189, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics cal manuals). (Bertoldi et al., 2013) shows that the repetition rate of news is considerably lower th"
E14-4036,W13-2237,0,0.194557,"Missing"
E14-4036,2013.mtsummit-papers.5,0,0.0193829,"Research Machine Translation (MT) has evolved dramatically over the last two decades, especially since the appearance of statistical approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domains such as technical documentation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), originally used for domain adaptation, and propose an extension to cross entropy difference with a vocabulary saturation filter (Lewis and Eetemadi, 2013). • While much of previous work concentrates on research datasets (e.g. Europarl, News Commentary), we use industry data (techni185 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185–189, c Gothenburg, Sweden, April 26-30 2"
E14-4036,N04-1043,0,0.053098,"AL-based selection works for PE-based incrementally retrained MT with overall performance gains around 0.67 to 2.65 TER absolute on average. We use two baselines, i.e. random and sequential. In the random baseline, the batch of sentences at each iteration are selected randomly. In the sequential baseline, the batches of sentences follow the same order as the data. Aside from the Random and Sequential baselines we use the following selection criteria: AL has been successfully applied to many tasks in natural language processing, including parsing (Tang et al., 2002), named entity recognition (Miller et al., 2004), to mention just a few. See (Olsson, 2009) for a comprehensie overview of the application of AL to natural language processing. (Haffari et al., 2009; Bloodgood and CallisonBurch, 2010) apply AL to MT where the aim is to build an optimal MT model from a given, static dataset. To the best of our knowledge, the most relevant previous research is (Gonz´alez-Rubio et al., 2012), which applies AL to interactive MT. In addition to differences in the AL selection criteria and data sets, our goals are fundamentally different: while the previous work aimed at reducing human effort in interactive MT, w"
E14-4036,P10-1088,0,0.0275825,"esented. In this paper, we add to the existing literature addressing the question whether and if so, to what extent, this process can be improved upon by Active Learning, where input is not presented chronologically but dynamically selected according to criteria that maximise performance with respect to (whatever is) the remaining data. We explore novel (source side-only) selection criteria and show performance increases of 0.67-2.65 points TER absolute on average on typical industry data sets compared to sequential PEbased incrementally retrained SMT. 1 • Previous work (Haffari et al., 2009; Bloodgood and Callison-Burch, 2010) shows that, given a (static) training set, AL can improve the quality of MT. By contrast, here we show that AL-based data selection for human PE improves incrementally and dynamically retrained MT, reducing overall PE time of translation jobs in the localisation industry application scenarios. Introduction and Related Research Machine Translation (MT) has evolved dramatically over the last two decades, especially since the appearance of statistical approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domai"
E14-4036,P10-2041,0,0.24501,"al approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domains such as technical documentation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), originally used for domain adaptation, and propose an extension to cross entropy difference with a vocabulary saturation filter (Lewis and Eetemadi, 2013). • While much of previous work concentrates on research datasets (e.g. Europarl, News Commentary), we use industry data (techni185 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185–189, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics cal manuals). (Bertoldi et al., 2013) shows that the repetition rate of news i"
E14-4036,J93-2003,0,0.0267032,"mpared to sequential PEbased incrementally retrained SMT. 1 • Previous work (Haffari et al., 2009; Bloodgood and Callison-Burch, 2010) shows that, given a (static) training set, AL can improve the quality of MT. By contrast, here we show that AL-based data selection for human PE improves incrementally and dynamically retrained MT, reducing overall PE time of translation jobs in the localisation industry application scenarios. Introduction and Related Research Machine Translation (MT) has evolved dramatically over the last two decades, especially since the appearance of statistical approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation industry, as for many relevant domains such as technical documentation, post-editing (PE) of MT output by human translators (compared to human translation from scratch) results in notable productivity gains, as a number of industry studies have shown convincingly, e.g. (Plitt and Masselot, 2010). Furthermore, incremental retraining and update techniques (Bertoldi et al., 2013; Levenberg et al., • We propose novel selection criteria for ALbased PE: we adapt cross-entropy difference (Moore and Lewis, 2010; Axelrod et"
E14-4036,E12-1025,0,0.178427,"Missing"
E14-4036,2013.mtsummit-papers.24,0,0.240964,"Missing"
E14-4036,N09-1047,0,0.217755,"translation job) is presented. In this paper, we add to the existing literature addressing the question whether and if so, to what extent, this process can be improved upon by Active Learning, where input is not presented chronologically but dynamically selected according to criteria that maximise performance with respect to (whatever is) the remaining data. We explore novel (source side-only) selection criteria and show performance increases of 0.67-2.65 points TER absolute on average on typical industry data sets compared to sequential PEbased incrementally retrained SMT. 1 • Previous work (Haffari et al., 2009; Bloodgood and Callison-Burch, 2010) shows that, given a (static) training set, AL can improve the quality of MT. By contrast, here we show that AL-based data selection for human PE improves incrementally and dynamically retrained MT, reducing overall PE time of translation jobs in the localisation industry application scenarios. Introduction and Related Research Machine Translation (MT) has evolved dramatically over the last two decades, especially since the appearance of statistical approaches (Brown et al., 1993). In fact, MT is nowadays succesfully used in the localisation and translation"
E14-4036,2006.amta-papers.25,0,0.175582,"erences in the AL selection criteria and data sets, our goals are fundamentally different: while the previous work aimed at reducing human effort in interactive MT, we aim at reducing the overall PE time in PE-based incremental MT update applications in the localisation industry. In our experiments reported in Section 3 below we want to explore a space consisting of a considerable number of selection strategies and incremental retraining batch sizes. In order to be able to do this, we use the target side of our industry translation memory data to approximate human PE output and automatic TER (Snover et al., 2006) scores as a proxy for human PE times (O’Brien, 2011). 2 • N-gram Overlap. An SMT system will encounter problems translating sentences containing n-grams not seen in the training data. Thus, PEs of sentences with high number of unseen n-grams are considered to be more informative for updating the current MT system. However, for the MT system to translate unseen n-grams accurately, they need to be seen a minimum number V times.2 We use an n-gram overlap function similar to the one described in (Gonz´alez-Rubio et al., 2012) given in Equation 1 where N (T (i) ) and N (S (i) ) return i-grams in t"
E14-4036,P07-2045,0,0.00362596,"t results. 3 Dir EN→FR FR→EN EN→DE DE→EN Random 29.64 27.08 24.00 19.36 Seq. 29.81 27.04 24.08 19.34 Ngram 28.97 26.15 22.34 17.70 CED 29.25 26.63 22.60 17.97 CEDN 29.05 26.39 22.32 17.48 Table 2: TER average scores for Setting 1 Experiments and Results Dir EN→FR FR→EN EN→DE DE→EN We use technical documentation data taken from Symantec translation memories for the English– French (EN–FR) and English–German (EN–DE) language pairs (both directions) for our experiments. The statistics of the data (training and incremental splits) are shown in Table 1. All the systems are trained using the Moses (Koehn et al., 2007) phrase-based statistical MT system, with IRSTLM (Federico et al., 2008) for language modelling (n-grams up to order five) and with the alignment heuristic grow-diag-final-and. Random 36.23 33.26 32.23 27.24 Seq. 36.26 33.34 32.19 27.29 Ngram 35.20 32.26 30.58 26.10 CED 35.48 32.69 31.96 26.73 CEDN 35.17 32.17 29.98 24.94 Table 3: TER average scores for Setting 2 For Setting 1 (Table 2), the best result is obtained by the CEDN criterion for two out of the four directions. For EN→FR, n-gram overlap 4 As this study simulates the post-editing, we use the references of the translated segments inst"
E14-4036,P02-1016,0,0.0611916,"E MT applications. • Our experiments show that AL-based selection works for PE-based incrementally retrained MT with overall performance gains around 0.67 to 2.65 TER absolute on average. We use two baselines, i.e. random and sequential. In the random baseline, the batch of sentences at each iteration are selected randomly. In the sequential baseline, the batches of sentences follow the same order as the data. Aside from the Random and Sequential baselines we use the following selection criteria: AL has been successfully applied to many tasks in natural language processing, including parsing (Tang et al., 2002), named entity recognition (Miller et al., 2004), to mention just a few. See (Olsson, 2009) for a comprehensie overview of the application of AL to natural language processing. (Haffari et al., 2009; Bloodgood and CallisonBurch, 2010) apply AL to MT where the aim is to build an optimal MT model from a given, static dataset. To the best of our knowledge, the most relevant previous research is (Gonz´alez-Rubio et al., 2012), which applies AL to interactive MT. In addition to differences in the AL selection criteria and data sets, our goals are fundamentally different: while the previous work aim"
E17-1100,D16-1025,0,0.674576,"ted (i) automatically with the BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics, and (ii) manually by means of ranking translations (Federmann, 2012) and monolingual semantic similarity (Graham et al., 2016). In all these evaluations, the performance of each system is measured by means of an overall score, which, while giving an indication of the general performance of a given system, does not provide any additional information. In order to understand better the new NMT paradigm and in what respects it provides better (or worse) translation quality than state-of-theart PBMT, Bentivogli et al. (2016) conducted a detailed analysis for the English-to-German language direction. In a nutshell, they found out that NMT (i) decreases post-editing effort, (ii) de∗ Work partly done at his previous position in Dublin City University, Ireland. 1 http://www.statmt.org/wmt16/ translation-task.html We aim to shed light on the strengths and weaknesses of the newly introduced neural machine translation paradigm. To that end, we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art neural machine translation and phrase-based machine translation systems for 9 language d"
E17-1100,P16-1160,0,0.0269023,"hrase-Based Machine Translation for 9 Language Directions Antonio Toral∗ University of Groningen The Netherlands a.toral.ruiz@rug.nl V´ıctor M. S´anchez-Cartagena Prompsit Language Engineering Av. Universitat s/n. Edifici Quorum III E-03202 Elx, Spain vmsanchez@prompsit.com Abstract A new paradigm to statistical machine translation, neural MT (NMT), has emerged very recently and has already surpassed the performance of the mainstream approach in the field, phrasebased MT (PBMT), for a number of language pairs, e.g. (Sennrich et al., 2016b; Luong et al., 2015; Costa-Juss`a and Fonollosa, 2016; Chung et al., 2016). In PBMT (Koehn, 2010) different models (translation, reordering, target language, etc.) are trained independently and combined in a loglinear scheme in which each model is assigned a different weight by a tuning algorithm. On the contrary, in NMT all the components are jointly trained to maximise translation quality. NMT systems have a strong generalisation power because they encode translation units as numeric vectors that represent concepts, whereas in PBMT translation units are encoded as strings. Moreover, NMT systems are able to model long-distance phenomena thanks to the use of recurre"
E17-1100,P16-2058,0,0.0435769,"Missing"
E17-1100,P14-1129,0,0.0151665,"Missing"
E17-1100,W16-2310,0,0.0559267,"Missing"
E17-1100,P11-1105,0,0.0430802,"Missing"
E17-1100,W08-0509,0,0.0139855,"rks make the word order of the translations closer to that of the reference. In order to measure the amount of reordering, we used the Kendall’s tau distance between word alignments obtained from pairs of sentences (Birch, 2011, Sec. 5.3.2). As the distance needs to be computed from permutations,8 we turned word aligments into permutations by means of the algorithm defined by Birch (2011, Sec. 5.2). For each language direction, we computed word alignments between the source-language side of the test set and the target-language reference, the PBMT output and the NMT output by means of MGIZA++ (Gao and Vogel, 2008). As the test sets are rather small for word alignment (1 999 to 3 000 sentence pairs depending on the language pair), we append bigger parallel corpora to help ensure accurate word alignments and avoid data sparseness. For languages for which in-domain 8 A permutation between a source-language sentence and a target-language sentence is defined as the set of operations that need to be carried out over the words in the sourcelanguage sentence to reflect the order of the words in the target-language sentence (Birch, 2011, Sec. 5.2). (news) parallel training data is available (German and Russian)"
E17-1100,W16-2316,0,0.0181049,"proposals for future work. 2 Experimental Setup The experiments are run on the best PBMT and NMT constrained systems submitted to the news translation task of WMT16. We selected such systems according to the human evaluation (Bojar et al., 2016, Sec. 3.4).2 We noted that many of the PBMT systems contain neural features, mainly in the form of language models. If the best PBMT submission contains any neural features we use this as the PBMT system in our analyses as long as none of these features is a fully-fledged NMT system. This was the case of the best submission in terms of BLEU for RU→EN (Junczys-Dowmunt et al., 2016). Out of the 12 language directions at the translation task, we conduct experiments on 9.3 These are the language pairs between English (EN) and Czech (CS), German (DE), Finnish (FI), Romanian (RO) and Russian (RU) in both directions (except for Finnish, where only the EN→FI direction is covered as no NMT system was submitted for the opposite direction, FI→EN). Finally, there was an additional language at the shared task, Turkish, that is not considered here, as either none of the systems submitted was neural (Turkish→EN), or there was one such system but its performance 2 When there are not s"
E17-1100,W04-3250,0,0.719391,"direction as it depends on the number of systems submitted. Namely, we have considered 2 NMT and 2 PBMT into Czech, 3 NMT and 5 PBMT into German, 2 NMT and 4 PBMT into Finnish, 2 NMT and 4 PBMT into Romanian and 2 NMT and 3 PBMT into Russian. PBMT NMT 23.7 25.9 PBMT NMT 30.4 31.4 DE FI RO From EN 30.6 15.3 27.4 34.2 18.0 28.9 Into EN 35.2 23.7 35.4 38.7 - 34.1 RU 24.3 26.0 29.3 28.2 Table 2: BLEU scores of the best NMT and PBMT systems for each language pair at WMT16’s news translation task. If the difference between them is statistically significant according to paired bootstrap resampling (Koehn, 2004) with p = 0.05 and 1 000 iterations, the highest score is shown in bold. automatic evaluation metric.6 In order to make sure that all systems considered are truly different (rather than different runs of the same system) we consider only 1 system per paradigm (NMT and PBMT) submitted by each team for each language direction. We would consider NMT outputs considerably different (with respect to PBMT) if they resemble each other (i.e. high pairwise overlap between NMT outputs) more than they do to PBMT systems (i.e. low overlap between an output by NMT and another by PBMT). This analysis is carr"
E17-1100,J10-4005,0,0.0389008,"n for 9 Language Directions Antonio Toral∗ University of Groningen The Netherlands a.toral.ruiz@rug.nl V´ıctor M. S´anchez-Cartagena Prompsit Language Engineering Av. Universitat s/n. Edifici Quorum III E-03202 Elx, Spain vmsanchez@prompsit.com Abstract A new paradigm to statistical machine translation, neural MT (NMT), has emerged very recently and has already surpassed the performance of the mainstream approach in the field, phrasebased MT (PBMT), for a number of language pairs, e.g. (Sennrich et al., 2016b; Luong et al., 2015; Costa-Juss`a and Fonollosa, 2016; Chung et al., 2016). In PBMT (Koehn, 2010) different models (translation, reordering, target language, etc.) are trained independently and combined in a loglinear scheme in which each model is assigned a different weight by a tuning algorithm. On the contrary, in NMT all the components are jointly trained to maximise translation quality. NMT systems have a strong generalisation power because they encode translation units as numeric vectors that represent concepts, whereas in PBMT translation units are encoded as strings. Moreover, NMT systems are able to model long-distance phenomena thanks to the use of recurrent neural networks, e.g"
E17-1100,W16-2317,0,0.0368764,"Missing"
E17-1100,D15-1166,0,0.193864,"Missing"
E17-1100,P02-1040,0,0.113914,"short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (Chung et al., 2014). The translations produced by NMT systems have been evaluated thus far mostly in terms of overall performance scores, be it by means of automatic or human evaluations. This has been the case of last year’s news translation shared task at the First Conference on Machine Translation (WMT16).1 In this translation task, outputs produced by participant MT systems, the vast majority of which fall under either the phrase-based or neural approaches, were evaluated (i) automatically with the BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics, and (ii) manually by means of ranking translations (Federmann, 2012) and monolingual semantic similarity (Graham et al., 2016). In all these evaluations, the performance of each system is measured by means of an overall score, which, while giving an indication of the general performance of a given system, does not provide any additional information. In order to understand better the new NMT paradigm and in what respects it provides better (or worse) translation quality than state-of-theart PBMT, Bentivogli et al. (2016) conducted a detailed analysis for"
E17-1100,J11-4002,0,0.139156,"Missing"
E17-1100,W15-3049,0,0.0570358,"Missing"
E17-1100,2014.eamt-1.39,0,0.025816,"• more or less monotone translations; • translations with better or worse word order; • better or worse translations depending on sentence length; • less or more errors for different error categories: inflectional, reordering and lexical; Hereunder we specify the main differences and similarities between this work and that of Bentivogli et al. (2016): • Language directions. They considered 1 while our study comprises 9. • Content. They dealt with transcribed speeches while we work with news stories. Previous research has shown that these two types of content pose different challenges for MT (Ruiz and Federico, 2014). • Size of evaluation data. Their test set had 600 sentences while our test sets span from 1 999 to 3 000 depending on the language direction. • Reference type. Their references were both independent from the MT output and also post-edited, while we have access only to single independent references. • Analyses. While some analyses overlap, some are novel in our experiments. Namely, output similarity, fluency and degree of reordering performed. Our analyses are conducted on the best PBMT and NMT systems submitted to the WMT16 translation task for each language direction. This (i) guarantees th"
E17-1100,W16-2322,1,0.823401,"Missing"
E17-1100,P16-1009,0,0.0429097,"between two or more NMT or PBMT systems (i.e. they belong to the same equivalence class), we pick the one with the highest BLEU score. If two NMT or PBMT systems were the best according to BLEU (draw), we pick the one with the best TER score. 3 Some experiments are run on a subset of these languages due to the lack of required tools for some of the languages involved. 1064 Language MT Pair Paradigm PBMT EN→CS NMT EN→DE EN→FI EN→RO EN→RU CS→EN DE→EN RO→EN RU→EN System details Phrase-based, word clusters (Ding et al., 2016) Unsupervised word segmentation and backtranslated monolingual corpora (Sennrich et al., 2016a) hierarchical String-to-tree, neural and dependency language models (Williams et al., PBMT 2016) NMT Same as for EN→CS PBMT Phrase-based, rule-based and unsupervised word segmentation, operation sequence model (Durrani et al., 2011), bilingual neural language model (Devlin et al., 2014), re-ranked with a recurrent neural language model (S´anchez-Cartagena and Toral, 2016) NMT Rule-based word segmentation, backtranslated monolingual corpora (S´anchez-Cartagena and Toral, 2016) PBMT Phrased-based, operation sequence model, monolingual and bilingual neural language models (Williams et al., 2016"
E17-1100,2006.amta-papers.25,0,0.356656,"iter and Schmidhuber, 1997) or gated recurrent units (Chung et al., 2014). The translations produced by NMT systems have been evaluated thus far mostly in terms of overall performance scores, be it by means of automatic or human evaluations. This has been the case of last year’s news translation shared task at the First Conference on Machine Translation (WMT16).1 In this translation task, outputs produced by participant MT systems, the vast majority of which fall under either the phrase-based or neural approaches, were evaluated (i) automatically with the BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics, and (ii) manually by means of ranking translations (Federmann, 2012) and monolingual semantic similarity (Graham et al., 2016). In all these evaluations, the performance of each system is measured by means of an overall score, which, while giving an indication of the general performance of a given system, does not provide any additional information. In order to understand better the new NMT paradigm and in what respects it provides better (or worse) translation quality than state-of-theart PBMT, Bentivogli et al. (2016) conducted a detailed analysis for the English-to-German language"
E17-1100,W16-2327,0,0.188954,"Sennrich et al., 2016a) hierarchical String-to-tree, neural and dependency language models (Williams et al., PBMT 2016) NMT Same as for EN→CS PBMT Phrase-based, rule-based and unsupervised word segmentation, operation sequence model (Durrani et al., 2011), bilingual neural language model (Devlin et al., 2014), re-ranked with a recurrent neural language model (S´anchez-Cartagena and Toral, 2016) NMT Rule-based word segmentation, backtranslated monolingual corpora (S´anchez-Cartagena and Toral, 2016) PBMT Phrased-based, operation sequence model, monolingual and bilingual neural language models (Williams et al., 2016) NMT Same as for EN→CS PBMT Phrase-based, word clusters, bilingual neural language model (Ding et al., 2016) NMT Same as for EN→CS PBMT Same as for EN→CS NMT Same as for EN→CS PBMT Phrase-based, pre-reordering, compound splitting (Williams et al., 2016) NMT Same as for EN→CS plus reranked with a right-to-left model PBMT Phrase-based, operation sequence model, monolingual neural language model (Williams et al., 2016) NMT Same as for EN→CS PBMT Phrase-based, lemmas in word alignment, sparse features, bilingual neural language model and transliteration (Lo et al., 2016) NMT Same as for EN→CS Tabl"
L16-1469,W13-1109,0,0.615616,"Missing"
L16-1469,P13-1018,0,0.0652074,"ies bringing to light the potential and shortcomings of today’s MT techniques applied to tweets, a corpus was compiled in the framework of TweetMT, a workshop and shared task1 on MT applied to tweets. Our parallel corpus includes tweets for the following language pairs: Catalan–Spanish (ca-es), Basque–Spanish (eu-es), Galician–Spanish (gl-es), and Portuguese–Spanish (pt-es). Those are the most common pairings between official languages in the Iberian Peninsula. 2. Collecting parallel tweets To the best of our knowledge, there is no parallel tweet dataset available apart from that produced by (Ling et al., 2013), which differs from our purposes in that they worked on tweets that mix two languages, i.e., providing the translated text within the same tweet. They further improve the quality of the parallel segments by means of crowdsourced annotations (Ling et al., 2014). Since we wanted to work on the translation of entire tweets into new tweets, we generated a corpus for the specific purposes of the TweetMT Workshop. For corpus generation, we developed a semi-automatic method to retrieve and align parallel tweets. The first step of this method consists in identifying multiple Twitter authors that conc"
L16-1469,W14-3356,0,0.045776,"ge pairs: Catalan–Spanish (ca-es), Basque–Spanish (eu-es), Galician–Spanish (gl-es), and Portuguese–Spanish (pt-es). Those are the most common pairings between official languages in the Iberian Peninsula. 2. Collecting parallel tweets To the best of our knowledge, there is no parallel tweet dataset available apart from that produced by (Ling et al., 2013), which differs from our purposes in that they worked on tweets that mix two languages, i.e., providing the translated text within the same tweet. They further improve the quality of the parallel segments by means of crowdsourced annotations (Ling et al., 2014). Since we wanted to work on the translation of entire tweets into new tweets, we generated a corpus for the specific purposes of the TweetMT Workshop. For corpus generation, we developed a semi-automatic method to retrieve and align parallel tweets. The first step of this method consists in identifying multiple Twitter authors that concurrently tweet in multiple languages and crawl those accounts. The second step involves aligning the collected messages. The idea behind this methodology is that the languages involved are official in the Iberian Peninsula, being Catalan, Basque and Galician co"
L16-1469,2010.amta-workshop.1,0,0.0358573,"ing features which are exclusive to the platform, such as hashtags, user mentions, and retweets. These characteristics make the application of MT to tweets a new challenge that requires specific processing techniques to perform effectively. Despite the paucity of research in the specific task of translating tweets, an increasing interest can be observed in the scientific community (Gotti et al., 2013; Peisenieks and Skadin¸sˇ, 2014). Similarly, a related and highly relevant direction of research is the work on MT of SMS texts, such as Munro’s study in the context of the 2010 Haiti earthquake (Munro, 2010). Provided the dearth of benchmark resources and comparison studies bringing to light the potential and shortcomings of today’s MT techniques applied to tweets, a corpus was compiled in the framework of TweetMT, a workshop and shared task1 on MT applied to tweets. Our parallel corpus includes tweets for the following language pairs: Catalan–Spanish (ca-es), Basque–Spanish (eu-es), Galician–Spanish (gl-es), and Portuguese–Spanish (pt-es). Those are the most common pairings between official languages in the Iberian Peninsula. 2. Collecting parallel tweets To the best of our knowledge, there is n"
L16-1469,P02-1040,0,0.103466,"Missing"
L16-1471,W11-1218,0,0.0442929,"Missing"
L16-1471,C12-1013,0,0.0137106,"rbian, and English– Finnish language pairs, while Section 5 describes the evaluation carried out for these new resources and the results obtained. The paper ends with some concluding remarks in Section 6. 2 Related work One of the most usual strategies to crawl parallel data from the Internet is to focus on web sites that make it straightforward to detect parallel documents (Nie et al., 1999; Koehn, 2005; Tiedemann, 2012). Many approaches use content-based metrics (Jiang et al., 2009; Utiyama et al., 2009; Yan et al., 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stable between different translations of the same document (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010;"
L16-1471,2008.tc-1.1,0,0.133345,"Missing"
L16-1471,P14-1129,0,0.0151313,"t, the first 1, 000 segments were manually translated into Croatian by a native speaker. Models. The SMT systems are phrase-based and built with the Moses toolkit (Koehn et al., 2007) using default parameters, except for the following. On top of the default word-based reordering model, our system implements two additional ones, phrase-based and hierarchical (Galley and Manning, 2008). On top of this, two additional resources were obtained from the hrenWaC parallel corpus and used in the MT systems built: an operation sequence model (Durrani et al., 2011) and a bilingual neural language model (Devlin et al., 2014). Results. We evaluate the systems trained in both directions and on both data collections (only hrenWaC and all the training corpora) with two widely used automatic evaluation metrics: BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results are shown in Table 4. As it can be seen, in both translation directions the performance of the new SMT systems built on the hrenWaC corpus obtain results that are comparable to those obtained by the third-party systems. In the harder translation direction, English→Croatian, the newly built SMT systems outperform two of the reference systems"
L16-1471,P11-1105,0,0.0131717,"se languages. Therefore, in order to build a suitable test set, the first 1, 000 segments were manually translated into Croatian by a native speaker. Models. The SMT systems are phrase-based and built with the Moses toolkit (Koehn et al., 2007) using default parameters, except for the following. On top of the default word-based reordering model, our system implements two additional ones, phrase-based and hierarchical (Galley and Manning, 2008). On top of this, two additional resources were obtained from the hrenWaC parallel corpus and used in the MT systems built: an operation sequence model (Durrani et al., 2011) and a bilingual neural language model (Devlin et al., 2014). Results. We evaluate the systems trained in both directions and on both data collections (only hrenWaC and all the training corpora) with two widely used automatic evaluation metrics: BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results are shown in Table 4. As it can be seen, in both translation directions the performance of the new SMT systems built on the hrenWaC corpus obtain results that are comparable to those obtained by the third-party systems. In the harder translation direction, English→Croatian, the new"
L16-1471,espla-gomis-etal-2014-comparing,1,0.894395,"Missing"
L16-1471,D08-1089,0,0.0253675,"f WMT13.22 This corpus consists of a collection of news stories in English which are freely available, translated into other languages. Unfortunately, Croatian was not one of these languages. Therefore, in order to build a suitable test set, the first 1, 000 segments were manually translated into Croatian by a native speaker. Models. The SMT systems are phrase-based and built with the Moses toolkit (Koehn et al., 2007) using default parameters, except for the following. On top of the default word-based reordering model, our system implements two additional ones, phrase-based and hierarchical (Galley and Manning, 2008). On top of this, two additional resources were obtained from the hrenWaC parallel corpus and used in the MT systems built: an operation sequence model (Durrani et al., 2011) and a bilingual neural language model (Devlin et al., 2014). Results. We evaluate the systems trained in both directions and on both data collections (only hrenWaC and all the training corpora) with two widely used automatic evaluation metrics: BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results are shown in Table 4. As it can be seen, in both translation directions the performance of the new SMT syste"
L16-1471,C10-1054,0,0.0197424,"new corpora crated for English– Croatian, English–Slovene, English–Serbian, and English– Finnish language pairs, while Section 5 describes the evaluation carried out for these new resources and the results obtained. The paper ends with some concluding remarks in Section 6. 2 Related work One of the most usual strategies to crawl parallel data from the Internet is to focus on web sites that make it straightforward to detect parallel documents (Nie et al., 1999; Koehn, 2005; Tiedemann, 2012). Many approaches use content-based metrics (Jiang et al., 2009; Utiyama et al., 2009; Yan et al., 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stable between different translations of the same document (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Z"
L16-1471,P09-1098,0,0.0210947,"tion 3 describes the tool Spidextor. Section 4 describes the new corpora crated for English– Croatian, English–Slovene, English–Serbian, and English– Finnish language pairs, while Section 5 describes the evaluation carried out for these new resources and the results obtained. The paper ends with some concluding remarks in Section 6. 2 Related work One of the most usual strategies to crawl parallel data from the Internet is to focus on web sites that make it straightforward to detect parallel documents (Nie et al., 1999; Koehn, 2005; Tiedemann, 2012). Many approaches use content-based metrics (Jiang et al., 2009; Utiyama et al., 2009; Yan et al., 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stable between different translations of the same document (Ma and"
L16-1471,P07-2045,0,0.00393889,"that led to the best results in the development phase. Test set. The test set used for evaluating the SMT systems described in this work was based on the test set used in the evaluation campaign of WMT13.22 This corpus consists of a collection of news stories in English which are freely available, translated into other languages. Unfortunately, Croatian was not one of these languages. Therefore, in order to build a suitable test set, the first 1, 000 segments were manually translated into Croatian by a native speaker. Models. The SMT systems are phrase-based and built with the Moses toolkit (Koehn et al., 2007) using default parameters, except for the following. On top of the default word-based reordering model, our system implements two additional ones, phrase-based and hierarchical (Galley and Manning, 2008). On top of this, two additional resources were obtained from the hrenWaC parallel corpus and used in the MT systems built: an operation sequence model (Durrani et al., 2011) and a bilingual neural language model (Devlin et al., 2014). Results. We evaluate the systems trained in both directions and on both data collections (only hrenWaC and all the training corpora) with two widely used automat"
L16-1471,2005.mtsummit-papers.11,0,0.384697,"sist of collections of texts in different languages which are mutual translations. This resource is specially relevant in the field of statistical machine translation (SMT), where parallel corpora are used to learn translation models automatically. The growing interest in SMT in the last decades has increased the demand of parallel corpora and, as a consequence, new strategies have been proposed to collect such data. Many sources of bitexts have been identified; some examples are: • texts from multilingual institutions, such as the Hansards corpus (Roukos et al., 1995) or the Europarl corpus (Koehn, 2005); • translations of software interfaces and documentation, such as KDE4 and OpenOffice (Tiedemann, 2009); or • news translated into different languages, such as the SETimes corpus (Ljubeˇsi´c, 2009), or the News Commentaries corpus (Bojar et al., 2013). However, one of the most obvious sources for collecting parallel data is the Internet. On the one hand, most of the sources already mentioned are currently available on the Web. In addition to this, it is worth noting that many websites are available in several languages and this translated content is another useful source of parallel data. The"
L16-1471,1999.mtsummit-1.79,0,0.464757,"., 2009; Utiyama et al., 2009; Yan et al., 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stable between different translations of the same document (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012; Papavassiliou et al., 2013). Another useful strategy is to identify language markers in the URLs (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012) that help detecting possible parallel documents. Some authors have used similar approaches to crawl comparable corpora. For instance, Smith et al. (2010) use the links between"
L16-1471,W03-2806,0,0.0298721,"Missing"
L16-1471,J05-4003,0,0.0638318,"rcada, 2010; San Vicente and Manterola, 2012; Papavassiliou et al., 2013). Another useful strategy is to identify language markers in the URLs (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012) that help detecting possible parallel documents. Some authors have used similar approaches to crawl comparable corpora. For instance, Smith et al. (2010) use the links between translated articles in Wikipedia to crawl parallel sentences or words. A more complex strategy is used by Munteanu and Marcu (2005), who compare news published in versions of news websites written in different languages by using a publication time stamp window. In this way, it is possible to retrieve likely on-topic news on which a cross-lingual information retrieval strategy is applied based on word-to-word machine translation. Even though these methods have proven to be useful for specific web sites, the real challenge is to find strategies that allow to extend them to crawl the Web in an unsupervised fashion, therefore allowing to exploit the real potential of this resource. Resnik (1998) uses language anchors, i.e. ph"
L16-1471,W13-2506,0,0.490072,"allel data. Therefore, a considerable scientific effort has been put during the last years in order to exploit the web as a source to automatically acquire new parallel data (see Section 2). Some examples of corpora built from multilingual web pages are the Tourism English–Croatian Parallel Corpus 2.01 (Toral et al., 2014) or the Panacea project’s parallel corpora for English–French and English–Greek in two different domains: environment2 and labour legislation3 (Pecina et al., 2014). There are several tools that can be used for automatically crawling parallel data from multilingual websites (Papavassiliou et al., 2013; Espl`a-Gomis and Forcada, 2010). However, all of them share the same limitation: they require the user to provide the URLs of the multilingual websites to be crawled. Despite the fact that large amounts of parallel data can be obtained from a single website, this requirement implies that these tools will require a list of web pages to crawl and will not be able to exploit the web as a parallel corpus in a fully automated way. To deal with this limitation, we propose a new method that focuses on crawling top-level domains (TLD) for multilingual data, and then detects parallel data inside the"
L16-1471,P02-1040,0,0.0993055,"cept for the following. On top of the default word-based reordering model, our system implements two additional ones, phrase-based and hierarchical (Galley and Manning, 2008). On top of this, two additional resources were obtained from the hrenWaC parallel corpus and used in the MT systems built: an operation sequence model (Durrani et al., 2011) and a bilingual neural language model (Devlin et al., 2014). Results. We evaluate the systems trained in both directions and on both data collections (only hrenWaC and all the training corpora) with two widely used automatic evaluation metrics: BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results are shown in Table 4. As it can be seen, in both translation directions the performance of the new SMT systems built on the hrenWaC corpus obtain results that are comparable to those obtained by the third-party systems. In the harder translation direction, English→Croatian, the newly built SMT systems outperform two of the reference systems, Bing and Yandex, while we do not observe a substantial decrease in the quality of the MT system built solely on the hrenWaC parallel corpus compared to the system built on all the training corpora. direction en→h"
L16-1471,J03-3002,0,0.763102,", 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stable between different translations of the same document (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012; Papavassiliou et al., 2013). Another useful strategy is to identify language markers in the URLs (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012) that help detecting possible parallel documents. Some authors have used similar approaches to crawl comparable corpora. For instance, Smith et al. (2010) use the links between translated articles in Wikipedia to crawl"
L16-1471,resnik-1998-parallel,0,0.221213,"trategy is used by Munteanu and Marcu (2005), who compare news published in versions of news websites written in different languages by using a publication time stamp window. In this way, it is possible to retrieve likely on-topic news on which a cross-lingual information retrieval strategy is applied based on word-to-word machine translation. Even though these methods have proven to be useful for specific web sites, the real challenge is to find strategies that allow to extend them to crawl the Web in an unsupervised fashion, therefore allowing to exploit the real potential of this resource. Resnik (1998) uses language anchors, i.e. phrases in a given language that may be a hint indicating that the translation of a web page is available through a link, such as the link captions “in Chinese” or “Chinese” in a website in English. Resnik (1998) builds queries containing two possible anchors in two languages and queries the Altavista search engine to find potentially parallel websites. Chen and Nie (2000) use a similar approach, but they look for anchors separately, i.e. in two different queries for each language. Once this is done, the URLs in both results are compared in order to obtain the list"
L16-1471,san-vicente-manterola-2012-paco2,0,0.0456805,"Missing"
L16-1471,2010.iwslt-papers.16,0,0.0552492,"Missing"
L16-1471,N10-1063,0,0.0183588,"the same document (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012; Papavassiliou et al., 2013). Another useful strategy is to identify language markers in the URLs (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012) that help detecting possible parallel documents. Some authors have used similar approaches to crawl comparable corpora. For instance, Smith et al. (2010) use the links between translated articles in Wikipedia to crawl parallel sentences or words. A more complex strategy is used by Munteanu and Marcu (2005), who compare news published in versions of news websites written in different languages by using a publication time stamp window. In this way, it is possible to retrieve likely on-topic news on which a cross-lingual information retrieval strategy is applied based on word-to-word machine translation. Even though these methods have proven to be useful for specific web sites, the real challenge is to find strategies that allow to extend them to"
L16-1471,P13-1135,0,0.0614331,"results are compared in order to obtain the lists of websites that might contain parallel documents. Ma and Liberman (1999) use a more direct approach; they download the list of websites of a given top level domain (TLD), download each of them, and apply language identification to keep only the documents in the languages desired. Similarly, Resnik and Smith (2003) use the Internet Archive4 to obtain a list of URLs for several specific TLDs. A set of rules are then applied on the URLs of the different TLDs in order to find parallelisms between them and, therefore, candidate parallel documents. Smith et al. (2013) extend this approach to use it on the Common Crawl corpus (Spiegler, 2013). In this paper we propose a novel strategy for building both parallel and monolingual corpora automatically by crawling TLDs. This strategy consists in combining two different existing tools: the SpiderLing monolingual crawler (Suchomel et al., 2012), which is able to automatically harvest documents from a given TLD starting from a collection of seed URLs, and the Bitextor parallel data crawler (Espl`aGomis et al., 2014). The main differences between this approach and other previous works are as follows: (i) this metho"
L16-1471,2006.amta-papers.25,0,0.0146066,"f the default word-based reordering model, our system implements two additional ones, phrase-based and hierarchical (Galley and Manning, 2008). On top of this, two additional resources were obtained from the hrenWaC parallel corpus and used in the MT systems built: an operation sequence model (Durrani et al., 2011) and a bilingual neural language model (Devlin et al., 2014). Results. We evaluate the systems trained in both directions and on both data collections (only hrenWaC and all the training corpora) with two widely used automatic evaluation metrics: BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results are shown in Table 4. As it can be seen, in both translation directions the performance of the new SMT systems built on the hrenWaC corpus obtain results that are comparable to those obtained by the third-party systems. In the harder translation direction, English→Croatian, the newly built SMT systems outperform two of the reference systems, Bing and Yandex, while we do not observe a substantial decrease in the quality of the MT system built solely on the hrenWaC parallel corpus compared to the system built on all the training corpora. direction en→hr hr→en system Google Bing Yan"
L16-1471,tiedemann-2012-parallel,0,0.0828156,"main approaches to the problem of parallel data crawling. Section 3 describes the tool Spidextor. Section 4 describes the new corpora crated for English– Croatian, English–Slovene, English–Serbian, and English– Finnish language pairs, while Section 5 describes the evaluation carried out for these new resources and the results obtained. The paper ends with some concluding remarks in Section 6. 2 Related work One of the most usual strategies to crawl parallel data from the Internet is to focus on web sites that make it straightforward to detect parallel documents (Nie et al., 1999; Koehn, 2005; Tiedemann, 2012). Many approaches use content-based metrics (Jiang et al., 2009; Utiyama et al., 2009; Yan et al., 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stab"
L16-1471,2014.eamt-1.45,1,0.880004,"Missing"
L16-1471,2009.mtsummit-papers.18,0,0.0205938,"tool Spidextor. Section 4 describes the new corpora crated for English– Croatian, English–Slovene, English–Serbian, and English– Finnish language pairs, while Section 5 describes the evaluation carried out for these new resources and the results obtained. The paper ends with some concluding remarks in Section 6. 2 Related work One of the most usual strategies to crawl parallel data from the Internet is to focus on web sites that make it straightforward to detect parallel documents (Nie et al., 1999; Koehn, 2005; Tiedemann, 2012). Many approaches use content-based metrics (Jiang et al., 2009; Utiyama et al., 2009; Yan et al., 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stable between different translations of the same document (Ma and Liberman, 1999; Nie et"
L16-1721,2007.mtsummit-papers.1,0,0.0551449,"Missing"
L16-1721,N10-1064,0,0.0154845,"lone pre-processing phase for defining correction rules targeting features of UGC negatively impacting SMT (e.g. use of slang, unconventional punctuation, ungrammaticality). Manually defined correction rules based on regular expressions can be either automatically applied (c.f. Jachmann et al., 2014) or they may require some form of intervention. Another strategy is to refine the SMT system by including a pre-processing step in which potential spelling errors are modelled (through a Confusion Network) and subsequently recovered by the decoder on the basis of a character n-gram language model (Bertoldi et al., 2010). However, both approaches have drawbacks. Whereas the first one requires manual intervention and is thus potentially slow and costly in terms of human resources, the second one entails high computational costs related to spelling-error modelling (Bertoldi et al., 2010: 418). Beyond the structural particularities of UG text, further difficulties have to do with the translation of pragmatic nuances usually present in subjective and evaluative discourse. If those nuances are not accurately translated, the opinion contained in the original text may not be recoverable, or, worse, might yield, simi"
L16-1721,C12-1135,1,0.89057,"Missing"
L16-1721,P90-1013,0,0.48575,"spelling-error modelling (Bertoldi et al., 2010: 418). Beyond the structural particularities of UG text, further difficulties have to do with the translation of pragmatic nuances usually present in subjective and evaluative discourse. If those nuances are not accurately translated, the opinion contained in the original text may not be recoverable, or, worse, might yield, similar to lying, false implicatures, with significant consequences on the perlocutionary effects on the readers of the translated text (see e.g. Mejbauer, 2004, for an account on lying in relation with false implicatures, or Reiter, 1990, for a computational approach to avoiding conversational 4 4551 http://accept-project.eu/ implicatures in computer-generated content). In this case, the translation would fail to meet users&apos; expectations in an e-commerce context. The literature on social media analytics has proposed several techniques to leverage pragmatic content from online user interaction. The proposed solutions often rely on deep linguistic processing creating a semantic representation of text (e.g. Delmonte & Pallotta, 2011) and modelling argumentative structure (Pallotta et al., 2011) to capture meaning distributed ove"
L16-1721,seretan-etal-2014-large,0,0.0268506,"matical constraints, and high dependence on the individuals’ writing style, give rise to data sparsity issues affecting the performance of SMT systems, which present a poorer performance with informal genres (van der Wees et al., 2015a). This is why text pre-processing (i.e. text normalisation) is used as a method for improving SMT performance. Different strategies have been deployed so far to enable automatic or semi-automatic text pre-processing for MT. The issue has been deeply studied in the framework of the ACCEPT project,4 aimed at enhancing the translation of UGC in online communities. Seretan et al. (2014), for instance, describe a standalone pre-processing phase for defining correction rules targeting features of UGC negatively impacting SMT (e.g. use of slang, unconventional punctuation, ungrammaticality). Manually defined correction rules based on regular expressions can be either automatically applied (c.f. Jachmann et al., 2014) or they may require some form of intervention. Another strategy is to refine the SMT system by including a pre-processing step in which potential spelling errors are modelled (through a Confusion Network) and subsequently recovered by the decoder on the basis of a"
L16-1721,W15-4304,0,0.0427304,"Missing"
L16-1721,P15-2092,0,0.0419402,"Missing"
L16-1721,N13-1069,0,0.0526415,"Missing"
ljubesic-toral-2014-cawac,W06-1704,0,\N,Missing
ljubesic-toral-2014-cawac,P02-1040,0,\N,Missing
ljubesic-toral-2014-cawac,W09-0432,0,\N,Missing
ljubesic-toral-2014-cawac,P07-2045,0,\N,Missing
ljubesic-toral-2014-cawac,W13-2411,1,\N,Missing
ljubesic-toral-2014-cawac,schafer-bildhauer-2012-building,0,\N,Missing
ljubesic-toral-2014-cawac,W14-0405,1,\N,Missing
ljubesic-toral-2014-cawac,padro-stanilovsky-2012-freeling,0,\N,Missing
ljubesic-toral-2014-cawac,P03-1021,0,\N,Missing
magnini-etal-2008-evaluation,W04-0812,1,\N,Missing
magnini-etal-2008-evaluation,bosco-etal-2008-comparing,1,\N,Missing
magnini-etal-2008-evaluation,W96-0102,0,\N,Missing
magnini-etal-2008-evaluation,W96-0213,0,\N,Missing
magnini-etal-2008-evaluation,bosco-etal-2000-building,1,\N,Missing
magnini-etal-2008-evaluation,W01-0521,0,\N,Missing
magnini-etal-2008-evaluation,A00-1031,0,\N,Missing
magnini-etal-2008-evaluation,W03-0419,0,\N,Missing
magnini-etal-2008-evaluation,P99-1065,0,\N,Missing
magnini-etal-2008-evaluation,D07-1096,0,\N,Missing
N13-3005,W05-0909,0,0.016496,"maintenance of the tool. 1 Automatic Evaluation of Machine Translation beyond Overall Scores Machine translation (MT) output can be evaluated using different approaches, which can essentially be divided into human and automatic, both of which, however, present a number of shortcomings. Human evaluation tends to be more reliable in a number of ways and can be tailored to a variety of situations, but is rather expensive (both in terms of resources and time) and is difficult to replicate. On the other hand, standard automatic MT evaluation metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) are considerably cheaper and provide faster results, but return rather crude scores that are difficult to interpret for MT users and developers alike. Crucially, current standard automatic MT evaluation metrics also lack any diagnostic value, i.e. they cannot identify specific weaknesses in the MT output. Diagnostic information can be extremely valuable for MT develCheckpoint Lexical Syntactic Semantic Relevance for MT Words that can have multiple translations in the target. For example, the preposition “de” in Spanish can be translated into English as “of” or “from” depending on the context."
N13-3005,berka-etal-2012-automatic,0,0.012083,"web service from the user’s perspective. Details regarding its implementation, evaluation, etc. can be found in (Toral et al., 2012; Naskar et al., 2011). 2 ity and input/output parameters (which can be easily included, e.g. as part of an online tutorial). While this paradigm is rather new in the field of computational linguistics, it is quite mature and successful in other fields such as bioinformatics (Oinn et al., 2004; Labarga et al., 2007). Related work includes two web applications in the area of MT evaluation. iBLEU (Madnani, 2011) organises BLEU scoring information in a visual manner. Berka et al. (2012) perform automatic error detection and classification of MT output. Web Services for Language Technology Tools There exist many freely available language processing tools, some of which are distributed under open-source licenses. In order to use these tools, they need to be downloaded, installed, configured and maintained, which results in high cost both in terms of manual effort and computing resources. The requirement for in-depth technical knowledge severely limits the usability of these tools amongst non-technical users, particularly in our case amongst translators and post-editors. Web se"
N13-3005,2011.mtsummit-papers.60,1,0.818593,"oes not seem to have been widely used in the community, in spite of its ability to support diagnostic evaluation. DELiC4MT1 is an open-source software that follows the same approach as Woodpecker. However, DELiC4MT is easily portable to any language pair2 and provides additional functionality such as filtering of noisy checkpoint instances and support for statistical significance tests. This paper focuses on the usage of this tool through a web application and a web service from the user’s perspective. Details regarding its implementation, evaluation, etc. can be found in (Toral et al., 2012; Naskar et al., 2011). 2 ity and input/output parameters (which can be easily included, e.g. as part of an online tutorial). While this paradigm is rather new in the field of computational linguistics, it is quite mature and successful in other fields such as bioinformatics (Oinn et al., 2004; Labarga et al., 2007). Related work includes two web applications in the area of MT evaluation. iBLEU (Madnani, 2011) organises BLEU scoring information in a visual manner. Berka et al. (2012) perform automatic error detection and classification of MT output. Web Services for Language Technology Tools There exist many freely"
N13-3005,J03-1002,0,0.00500722,"rface for the web service. 3 Demo The demo presented in this paper consists of a web service and a web application built on top of DELiC4MT that allow to assess the performance of MT systems on different linguistic phenomena deFigure 2: Screenshot of the web application (visualisation of results). fined by the user. The following subsections detail both parts of the demo. 3.1 Web Service A SOAP-compliant web service3 has been built on top of DELiC4MT. It receives the following input parameters (see Figure 1): 1. Word alignment between the source and target sides of the testset, in the GIZA++ (Och and Ney, 2003) output format. 2. Linguistic checkpoint defined as a Kybot4 (Vossen et al., 2010) profile. 3. Output of the MT system to be evaluated, in plain text, tokenised and one sentence per line. 4. Source and target sides of the testset (or gold standard), in KAF format (Bosma et al., 2009).5 The tool then evaluates the performance of the MT system (input parameter 3) on the linguistic phenomenon (parameter 2) by following this procedure: 3 http://registry.elda.org/services/301 Kybot profiles can be understood as regular expressions over KAF documents, http://kyoto.let.vu.nl/svn/ kyoto/trunk/modules/"
N13-3005,P02-1040,0,0.0897883,"any installation, configuration or maintenance of the tool. 1 Automatic Evaluation of Machine Translation beyond Overall Scores Machine translation (MT) output can be evaluated using different approaches, which can essentially be divided into human and automatic, both of which, however, present a number of shortcomings. Human evaluation tends to be more reliable in a number of ways and can be tailored to a variety of situations, but is rather expensive (both in terms of resources and time) and is difficult to replicate. On the other hand, standard automatic MT evaluation metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) are considerably cheaper and provide faster results, but return rather crude scores that are difficult to interpret for MT users and developers alike. Crucially, current standard automatic MT evaluation metrics also lack any diagnostic value, i.e. they cannot identify specific weaknesses in the MT output. Diagnostic information can be extremely valuable for MT develCheckpoint Lexical Syntactic Semantic Relevance for MT Words that can have multiple translations in the target. For example, the preposition “de” in Spanish can be translated into English as “o"
N13-3005,W10-3301,0,0.0137191,"web service and a web application built on top of DELiC4MT that allow to assess the performance of MT systems on different linguistic phenomena deFigure 2: Screenshot of the web application (visualisation of results). fined by the user. The following subsections detail both parts of the demo. 3.1 Web Service A SOAP-compliant web service3 has been built on top of DELiC4MT. It receives the following input parameters (see Figure 1): 1. Word alignment between the source and target sides of the testset, in the GIZA++ (Och and Ney, 2003) output format. 2. Linguistic checkpoint defined as a Kybot4 (Vossen et al., 2010) profile. 3. Output of the MT system to be evaluated, in plain text, tokenised and one sentence per line. 4. Source and target sides of the testset (or gold standard), in KAF format (Bosma et al., 2009).5 The tool then evaluates the performance of the MT system (input parameter 3) on the linguistic phenomenon (parameter 2) by following this procedure: 3 http://registry.elda.org/services/301 Kybot profiles can be understood as regular expressions over KAF documents, http://kyoto.let.vu.nl/svn/ kyoto/trunk/modules/mining_module/ 5 An XML format for text analysis based on representation standards"
N13-3005,C08-1141,0,0.016194,"anslations in the target language. Polysemous words can be collected from electronic dictionaries such as WordNet (Miller, 1995). Table 1: Linguistic Checkpoints Checkpoints can also be built by combining el20 Proceedings of the NAACL HLT 2013 Demonstration Session, pages 20–23, c Atlanta, Georgia, 10-12 June 2013. 2013 Association for Computational Linguistics ements from different categories. For example, by combining lexical and syntantic elements, we could define a checkpoint for prepositional phrases (syntactic element) which start with the preposition “de” (lexical element). Woodpecker (Zhou et al., 2008) is a tool that performs diagnostic evaluation of MT systems over linguistic checkpoints for English–Chinese. Probably due to its limitation to one language pair, its proprietary nature as well as rather restrictive licensing conditions, Woodpecker does not seem to have been widely used in the community, in spite of its ability to support diagnostic evaluation. DELiC4MT1 is an open-source software that follows the same approach as Woodpecker. However, DELiC4MT is easily portable to any language pair2 and provides additional functionality such as filtering of noisy checkpoint instances and supp"
poch-etal-2012-towards,ide-etal-2000-xces,0,\N,Missing
poch-etal-2012-towards,bramantoro-etal-2010-towards,0,\N,Missing
poch-etal-2012-towards,W07-1501,0,\N,Missing
poch-etal-2012-towards,2011.eamt-1.40,1,\N,Missing
poch-etal-2012-towards,murakami-etal-2010-language,0,\N,Missing
Q18-1043,W13-2322,0,0.0777827,"Reyle, 1993; Muskens, 1996; Van Eijck and Kamp 1997; Kadmon, 2001; Asher and Las-carides, 2003), dealing with many semantic phenomena: quantifiers, negation, scope ambiguities, pronouns, presuppositions, and discourse structure (see Figure 1). DRSs are recursive structures and thus form a challenge for sequence-tosequence models because they need to generate a well-formed structure and not something that looks like one but is not interpretable. The problem that we try to tackle bears similarities to the recently introduced task of mapping sentences to an Abstract Meaning Representation (AMR; Banarescu et al. 2013). But there are notable differences between DRS and AMR. Firstly, DRSs contain scope, which results in a more linguistically motivated treatment of modals, quantification, and negation. Secondly, DRSs contain a substantially higher number of variable bindings (reentrant nodes in AMR terminology), which are challenging for learning (Damonte et al., 2017). DRS parsing was attempted in the 1980s for small fragments of English (Johnson and Klein, 1986; Wada and Asher, 1986). Wide-coverage 619 Transactions of the Association for Computational Linguistics, vol. 6, pp. 619–634, 2018. Action Editor: A"
Q18-1043,basile-etal-2012-developing,1,0.830725,"acters or sequences of words; does tokenization help; and what kind of casing is best used? 3. What is the best way of dealing with variables that occur in DRSs? 4. Does adding silver data increase the performance of the neural parser? 5. What parts of semantics are learned and what parts of semantics are still challenging? 2.2 Annotated Corpora Despite a long tradition of formal interest in DRT, it is only recently that textual corpora annotated with DRSs have been made available. The Groningen Meaning Bank (GMB) is a large corpus with DRS annotation for mostly short English newspaper texts (Basile et al., 2012; Bos et al., 2017). The DRSs in this corpus are produced by an existing semantic parser and then partially corrected. The DRSs in the GMB are therefore not gold standard. A similar corpus is the Parallel Meaning Bank (PMB), which provides DRSs for English, German, Dutch, and Italian sentences based on a parallel corpus (Abzianidze et al., 2017). The PMB, too, is constructed using an existing semantic parser, but a part of it is completely manually checked and corrected (i.e., gold standard). In contrast to the GMB, the PMB involves two major We make the following contributions to semantic par"
Q18-1043,P14-1133,0,0.0977959,"Missing"
Q18-1043,E17-2039,1,0.801656,"Missing"
Q18-1043,C16-1333,1,0.844198,"ish, German, Dutch, and Italian sentences based on a parallel corpus (Abzianidze et al., 2017). The PMB, too, is constructed using an existing semantic parser, but a part of it is completely manually checked and corrected (i.e., gold standard). In contrast to the GMB, the PMB involves two major We make the following contributions to semantic parsing:1 (a) The output of our parser consists of interpretable scoped meaning representations, 1 The code is available here: https://github.com/ RikVN/Neural_DRS. 620 additions: (a) its semantics are refined by modeling tense and using semantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017), and (b) the non-logical symbols of the DRSs corresponding to concepts and semantic roles are grounded in WordNet (Fellbaum, 1998) and VerbNet (Bonial et al., 2011), respectively. These additions make the DRSs of the PMB more fine-grained meaning representations. For this reason we choose the PMB (over the GMB) as our corpus for evaluating our semantic parser. Even though the sentences in the current release of the PMB are relatively short, they contain many difficult semantic phenomena that a semantic parser has to deal with: pronoun resolution, quantifiers, scope"
Q18-1043,P17-1112,0,0.0250347,"ently, however, neural methods, and in particular sequenceto-sequence models, have been successfully applied to a wide range of semantic parsing tasks. These include code generation (Ling et al., 2016), question answering (Dong and Lapata, 2016; He and Golub, 2016) and Abstract Meaning Representation parsing (Konstas et al., 2017). Because these models have no intrinsic knowledge of the structure (tree, graph, set) they have to produce, recent work also focused on structured decoding methods, creating neural architectures that always output a graph or a tree (Alvarez-Melis and Jaakkola, 2017; Buys and Blunsom, 2017). These methods often outperform the more general sequence-to-sequence models but are tailored to specific meaning representations. This paper will focus on parsing Discourse Representation Structures (DRSs) proposed in Discourse Representation Theory (DRT), a wellstudied formalism developed in formal semantics (Kamp, 1984; Van der Sandt, 1992; Asher, 1993; Kamp and Reyle, 1993; Muskens, 1996; Van Eijck and Kamp 1997; Kadmon, 2001; Asher and Las-carides, 2003), dealing with many semantic phenomena: quantifiers, negation, scope ambiguities, pronouns, presuppositions, and discourse structure (se"
Q18-1043,P13-2131,0,0.0992049,"onze data to further push the score of our best systems. 3.2 3.3 Evaluation A DRS parser is evaluated by comparing its output DRS to a gold standard DRS using the Counter tool (van Noord et al., 2018). Counter calculates an F-score over matching clauses. Because variable names are meaningless, obtaining the matching clauses essentially is a search for the best variable mapping between two DRSs. Counter tries to find this mapping by performing a hill-climbing search with a predefined number of restarts to avoid getting stuck in a local optimum, which is similar to the evaluation system SMATCH (Cai and Knight, 2013) for AMR parsing.4 Counter generalizes over WordNet synsets (i.e., a system is not penalized for predicting a word sense that is in the same synset as the gold standard word sense). To calculate whether there is a significant difference between two systems, we perform approximate randomization (Noreen, 1989) with α = 0.05, R = 1,000, and F (model1 ) &gt; F (model2 ) as test statistics for each individual DRS pair. Clausal Form Checker The clausal form of a DRS needs to satisfy a set of constraints in order to correspond to a semantically interpretable DRS, that is, translatable into a first-order"
Q18-1043,P07-2009,1,0.704408,"RSs. 6 6.1 Discussion Comparison In this section, we compare our best neural models (with and without silver data, see Table 6) with two baseline systems and with two DRS parsers: AMR 2 DRS and Boxer. AMR 2 DRS is a parser that obtains DRSs from AMRs by applying a set of rules (van Noord et al., 2018), in our case using AMRs produced by the AMR parser of van Noord and Bos (2017b). Boxer is an existing DRS parser using a statistical combinatory categorical grammar parser for syntactic analysis and a compositional semantics based on λ-calculus, followed by pronoun and presupposition resolution (Curran et al., 2007; Bos, 2008b). SPAR is a baseline parser that outputs the same (fixed) default DRS for each input sentence. We implemented a second baseline model, SIM - SPAR, which outputs, for each sentence in the test set, the DRS of the most similar sentence in the training set. This similarity is calculated by taking the cosine similarity of the average word embedding vector (with removed stopwords) based on the GloVe embeddings (Pennington et al., 2014). Table 8 shows the result of the comparison. The neural models comfortably outperform the baselines. We see that both our neural models 8 Note that we c"
Q18-1043,bos-2008-lets,1,0.924281,"re discourse structure by connecting two units of discourse by a discourse relation (Asher and Lascarides, 2003). Figure 1: DRS parsing in a nutshell. Given a raw text, a system has to generate a DRS in the clause format, a flat version of the standard box notation. The semantic representation formats are made more readable by using various letters for variables: the letters x, e, s, and t are used for discourse referents denoting individuals, events, states, and time, respectively, and b is used for variables denoting DRS boxes. DRS parsers based on supervised machine learning emerged later (Bos, 2008b; Le and Zuidema, 2012; Bos, 2015; Liu et al., 2018). The objectives of this paper are to apply neural methods to DRS parsing. In particular, we are interested in answers to the following research questions (RQs): 1. Are sequence-to-sequence models able to produce formal meaning representations (DRSs)? 2. What is better for input: sequences of characters or sequences of words; does tokenization help; and what kind of casing is best used? 3. What is the best way of dealing with variables that occur in DRSs? 4. Does adding silver data increase the performance of the neural parser? 5. What parts"
Q18-1043,E17-1051,0,0.0337778,"erate a well-formed structure and not something that looks like one but is not interpretable. The problem that we try to tackle bears similarities to the recently introduced task of mapping sentences to an Abstract Meaning Representation (AMR; Banarescu et al. 2013). But there are notable differences between DRS and AMR. Firstly, DRSs contain scope, which results in a more linguistically motivated treatment of modals, quantification, and negation. Secondly, DRSs contain a substantially higher number of variable bindings (reentrant nodes in AMR terminology), which are challenging for learning (Damonte et al., 2017). DRS parsing was attempted in the 1980s for small fragments of English (Johnson and Klein, 1986; Wada and Asher, 1986). Wide-coverage 619 Transactions of the Association for Computational Linguistics, vol. 6, pp. 619–634, 2018. Action Editor: Asli Celikyilmaz. Submission batch: 7/2018; Revision batch: 9/2018; Published 12/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. guaranteed by a specially designed checking tool (§3). (b) We compare different methods of representing input and output in §4. (c) We show in §5 that using additional, non-gold st"
Q18-1043,W08-2222,1,0.945128,"re discourse structure by connecting two units of discourse by a discourse relation (Asher and Lascarides, 2003). Figure 1: DRS parsing in a nutshell. Given a raw text, a system has to generate a DRS in the clause format, a flat version of the standard box notation. The semantic representation formats are made more readable by using various letters for variables: the letters x, e, s, and t are used for discourse referents denoting individuals, events, states, and time, respectively, and b is used for variables denoting DRS boxes. DRS parsers based on supervised machine learning emerged later (Bos, 2008b; Le and Zuidema, 2012; Bos, 2015; Liu et al., 2018). The objectives of this paper are to apply neural methods to DRS parsing. In particular, we are interested in answers to the following research questions (RQs): 1. Are sequence-to-sequence models able to produce formal meaning representations (DRSs)? 2. What is better for input: sequences of characters or sequences of words; does tokenization help; and what kind of casing is best used? 3. What is the best way of dealing with variables that occur in DRSs? 4. Does adding silver data increase the performance of the neural parser? 5. What parts"
Q18-1043,W15-1841,1,0.843442,"ng two units of discourse by a discourse relation (Asher and Lascarides, 2003). Figure 1: DRS parsing in a nutshell. Given a raw text, a system has to generate a DRS in the clause format, a flat version of the standard box notation. The semantic representation formats are made more readable by using various letters for variables: the letters x, e, s, and t are used for discourse referents denoting individuals, events, states, and time, respectively, and b is used for variables denoting DRS boxes. DRS parsers based on supervised machine learning emerged later (Bos, 2008b; Le and Zuidema, 2012; Bos, 2015; Liu et al., 2018). The objectives of this paper are to apply neural methods to DRS parsing. In particular, we are interested in answers to the following research questions (RQs): 1. Are sequence-to-sequence models able to produce formal meaning representations (DRSs)? 2. What is better for input: sequences of characters or sequences of words; does tokenization help; and what kind of casing is best used? 3. What is the best way of dealing with variables that occur in DRSs? 4. Does adding silver data increase the performance of the neural parser? 5. What parts of semantics are learned and what"
Q18-1043,W17-3203,0,0.0153535,"additional data to improve the score. For DRSs, the PMB-2.1.0 release already contains a large set of silver standard data (71,308 instances), containing DRSs that are only partially manually corrected. We then train a model on both the gold and silver standard data, making no distinction between them during training. After training we take the last model and restart the training on only the gold data, in a similar process as described in Konstas et al. (2017) and van Noord and Bos (2017b). In general, restarting the training to fine-tune the weights of the model is a common technique in NMT (Denkowski and Neubig, 2017). We are aware that there are many methods to obtain and utilize additional data. However, our main aim is not to find the optimal method for DRS parsing, but to demonstrate that using additional data is indeed beneficial for neural DRS parsing. Because we are not further fine-tuning our model, we will show results on the test set in this section. Table 6 shows the results of adding the silver data. This results in a large increase in performance, for both the character- and word-level models. We are still reliant on manually annotated data, however, because without the gold data (so training"
Q18-1043,P16-1004,0,0.022972,"creases parser performance. Adding silver training data boosts performance even further. 1 Introduction Semantic parsing is the task of mapping a natural language expression to an interpretable meaning representation. Semantic parsing used to be the domain of symbolic and statistical approaches (Pereira and Shieber, 1987; Zelle and Mooney, 1996; Blackburn and Bos, 2005). Recently, however, neural methods, and in particular sequenceto-sequence models, have been successfully applied to a wide range of semantic parsing tasks. These include code generation (Ling et al., 2016), question answering (Dong and Lapata, 2016; He and Golub, 2016) and Abstract Meaning Representation parsing (Konstas et al., 2017). Because these models have no intrinsic knowledge of the structure (tree, graph, set) they have to produce, recent work also focused on structured decoding methods, creating neural architectures that always output a graph or a tree (Alvarez-Melis and Jaakkola, 2017; Buys and Blunsom, 2017). These methods often outperform the more general sequence-to-sequence models but are tailored to specific meaning representations. This paper will focus on parsing Discourse Representation Structures (DRSs) proposed in D"
Q18-1043,D17-1151,0,0.0137376,"sequence representation of the natural language utterance, while the decoder produces the sequences of the meaning representation. We apply dropout (Srivastava et al., 2014) between both the recurrent encoding and decoding layers to prevent overfitting, and use general attention (Luong et al., 2015) to selectively give more weight to certain parts of the input sentence. An overview of the general framework of the seq2seq model is shown in Figure 3. During decoding we perform beam search with length normalization, which in neural machine translation (NMT) is crucial to obtaining good results (Britz et al., 2017). We experimented with a wide range of parameter settings, of which the final settings can be found in Table 3. We opted against trying to find the best parameter settings for each individual experiment (next to impossible in terms of computing time necessary, as a single 10-fold CV experiment takes 12 hours on GPU), but selected parameter settings that showed good performance for both the initial character and word-level representations (see §4 for details). The parameter search was performed using 10-fold CV on the training set. Training is stopped when there is no more improvement in perple"
Q18-1043,D13-1146,1,0.834418,"haracter-level or wordlevel score (F-scores between 57 and 68), only coming close when using a small number of merges (which is very close to character-level anyway). Therefore this technique was disregarded for further experiments. 4.2 Tokenization An interesting aspect of the PMB data is the way the input sentences are tokenized. In the data set, multiword expressions are tokenized as single words, for example, “New York” is tokenized to “New∼York.” Unfortunately, most off-the-shelf tokenizers (e.g., the Moses tokenizer) are not equipped to deal with this. We experiment with using Elephant (Evang et al., 2013), a tokenizer that can be (re-)trained on individual data sets, using the tokenized sentences of the published silver and gold PMB data set.7 Simultaneously, we are interested in whether character-level models need tokenization at all, which would be a possible advantage of this type of representing the input text. Results of the experiment are shown in Table 5. None of the two tokenization methods yielded a significant advantage for the character-level models, so they will not be used further. The word-level models, however, did benefit from tokenization, but Elephant did not give us an advan"
Q18-1043,P17-4012,0,0.0449941,"word-representation input. SEP is used as a special character to separate clauses in the output. Parameter Value Parameter Value RNN-type encoder-type optimizer layers nodes min freq source min freq target vector size LSTM brnn sgd 2 300 3 3 300 dropout dropout type bridge learning rate learning rate decay max grad norm beam size length normalization 0.2 naive copy 0.7 0.7 5 10 0.9 Table 3: Parameters explored during training and testing with their final values. All other parameters have default values. bidirectional long short-term memory (LSTM) layers and 300 nodes, implemented in OpenNMT (Klein et al., 2017). The network encodes a sequence representation of the natural language utterance, while the decoder produces the sequences of the meaning representation. We apply dropout (Srivastava et al., 2014) between both the recurrent encoding and decoding layers to prevent overfitting, and use general attention (Luong et al., 2015) to selectively give more weight to certain parts of the input sentence. An overview of the general framework of the seq2seq model is shown in Figure 3. During decoding we perform beam search with length normalization, which in neural machine translation (NMT) is crucial to o"
Q18-1043,P17-1014,0,0.365646,"1 Introduction Semantic parsing is the task of mapping a natural language expression to an interpretable meaning representation. Semantic parsing used to be the domain of symbolic and statistical approaches (Pereira and Shieber, 1987; Zelle and Mooney, 1996; Blackburn and Bos, 2005). Recently, however, neural methods, and in particular sequenceto-sequence models, have been successfully applied to a wide range of semantic parsing tasks. These include code generation (Ling et al., 2016), question answering (Dong and Lapata, 2016; He and Golub, 2016) and Abstract Meaning Representation parsing (Konstas et al., 2017). Because these models have no intrinsic knowledge of the structure (tree, graph, set) they have to produce, recent work also focused on structured decoding methods, creating neural architectures that always output a graph or a tree (Alvarez-Melis and Jaakkola, 2017; Buys and Blunsom, 2017). These methods often outperform the more general sequence-to-sequence models but are tailored to specific meaning representations. This paper will focus on parsing Discourse Representation Structures (DRSs) proposed in Discourse Representation Theory (DRT), a wellstudied formalism developed in formal semant"
Q18-1043,D16-1166,0,0.0210458,"nce. Adding silver training data boosts performance even further. 1 Introduction Semantic parsing is the task of mapping a natural language expression to an interpretable meaning representation. Semantic parsing used to be the domain of symbolic and statistical approaches (Pereira and Shieber, 1987; Zelle and Mooney, 1996; Blackburn and Bos, 2005). Recently, however, neural methods, and in particular sequenceto-sequence models, have been successfully applied to a wide range of semantic parsing tasks. These include code generation (Ling et al., 2016), question answering (Dong and Lapata, 2016; He and Golub, 2016) and Abstract Meaning Representation parsing (Konstas et al., 2017). Because these models have no intrinsic knowledge of the structure (tree, graph, set) they have to produce, recent work also focused on structured decoding methods, creating neural architectures that always output a graph or a tree (Alvarez-Melis and Jaakkola, 2017; Buys and Blunsom, 2017). These methods often outperform the more general sequence-to-sequence models but are tailored to specific meaning representations. This paper will focus on parsing Discourse Representation Structures (DRSs) proposed in Discourse Representati"
Q18-1043,C12-1094,0,0.188791,"ure by connecting two units of discourse by a discourse relation (Asher and Lascarides, 2003). Figure 1: DRS parsing in a nutshell. Given a raw text, a system has to generate a DRS in the clause format, a flat version of the standard box notation. The semantic representation formats are made more readable by using various letters for variables: the letters x, e, s, and t are used for discourse referents denoting individuals, events, states, and time, respectively, and b is used for variables denoting DRS boxes. DRS parsers based on supervised machine learning emerged later (Bos, 2008b; Le and Zuidema, 2012; Bos, 2015; Liu et al., 2018). The objectives of this paper are to apply neural methods to DRS parsing. In particular, we are interested in answers to the following research questions (RQs): 1. Are sequence-to-sequence models able to produce formal meaning representations (DRSs)? 2. What is better for input: sequences of characters or sequences of words; does tokenization help; and what kind of casing is best used? 3. What is the best way of dealing with variables that occur in DRSs? 4. Does adding silver data increase the performance of the neural parser? 5. What parts of semantics are learn"
Q18-1043,L18-1473,0,0.019724,"Missing"
Q18-1043,P16-1057,0,0.0469105,"Missing"
Q18-1043,P16-1002,0,0.0679332,"Missing"
Q18-1043,P18-1040,0,0.443509,"s of discourse by a discourse relation (Asher and Lascarides, 2003). Figure 1: DRS parsing in a nutshell. Given a raw text, a system has to generate a DRS in the clause format, a flat version of the standard box notation. The semantic representation formats are made more readable by using various letters for variables: the letters x, e, s, and t are used for discourse referents denoting individuals, events, states, and time, respectively, and b is used for variables denoting DRS boxes. DRS parsers based on supervised machine learning emerged later (Bos, 2008b; Le and Zuidema, 2012; Bos, 2015; Liu et al., 2018). The objectives of this paper are to apply neural methods to DRS parsing. In particular, we are interested in answers to the following research questions (RQs): 1. Are sequence-to-sequence models able to produce formal meaning representations (DRSs)? 2. What is better for input: sequences of characters or sequences of words; does tokenization help; and what kind of casing is best used? 3. What is the best way of dealing with variables that occur in DRSs? 4. Does adding silver data increase the performance of the neural parser? 5. What parts of semantics are learned and what parts of semantics"
Q18-1043,C86-1156,0,0.697349,"The problem that we try to tackle bears similarities to the recently introduced task of mapping sentences to an Abstract Meaning Representation (AMR; Banarescu et al. 2013). But there are notable differences between DRS and AMR. Firstly, DRSs contain scope, which results in a more linguistically motivated treatment of modals, quantification, and negation. Secondly, DRSs contain a substantially higher number of variable bindings (reentrant nodes in AMR terminology), which are challenging for learning (Damonte et al., 2017). DRS parsing was attempted in the 1980s for small fragments of English (Johnson and Klein, 1986; Wada and Asher, 1986). Wide-coverage 619 Transactions of the Association for Computational Linguistics, vol. 6, pp. 619–634, 2018. Action Editor: Asli Celikyilmaz. Submission batch: 7/2018; Revision batch: 9/2018; Published 12/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. guaranteed by a specially designed checking tool (§3). (b) We compare different methods of representing input and output in §4. (c) We show in §5 that using additional, non-gold standard data can improve performance. (d) We perform a thorough analysis of the produced output a"
Q18-1043,D15-1166,0,0.110008,"Missing"
Q18-1043,L18-1267,1,0.641697,"Missing"
Q18-1043,W17-7306,1,0.904386,"Missing"
Q18-1043,E17-1100,1,0.806981,"mple, there are only three sentences that have two words. 628 systems decreases with sentence length, thus corroborating the trends shown in Figure 6 and (ii) the interaction between parser and sentence length is not significant (i.e., none of the parsers decreases significantly more than any other with sentence length). The fact that the performance of the neural parsers degrades with sentence length is not surprising, because they are based on the seq2seq architecture, and models built on this architecture for other tasks, such as machine translation, have been shown to have the same issue (Toral and Sánchez-Cartagena, 2017). 6.3 Phenomenon Negation & modals Scope ambiguity Pronoun resolution Discourse rel. & imp. Embedded clauses # 73 15 31 33 30 Char Word Boxer 0.90 0.73 0.84 0.64 0.77 0.81 0.57 0.77 0.67 0.70 0.89 0.80 0.90 0.82 0.87 Table 10: Manual evaluation of the output of the three semantic parsers on several semantic phenomena. Reported numbers are accuracies. The results of the semantic evaluation of the parsers on the test set is given in Table 10. The character-level parser performs better than the word-level parser on all the phenomena except one. Even though both our neural parsers clearly outperfo"
Q18-1043,D14-1162,0,0.0822051,"atory categorical grammar parser for syntactic analysis and a compositional semantics based on λ-calculus, followed by pronoun and presupposition resolution (Curran et al., 2007; Bos, 2008b). SPAR is a baseline parser that outputs the same (fixed) default DRS for each input sentence. We implemented a second baseline model, SIM - SPAR, which outputs, for each sentence in the test set, the DRS of the most similar sentence in the training set. This similarity is calculated by taking the cosine similarity of the average word embedding vector (with removed stopwords) based on the GloVe embeddings (Pennington et al., 2014). Table 8 shows the result of the comparison. The neural models comfortably outperform the baselines. We see that both our neural models 8 Note that we cannot apply the manual corrections, so in PMB terminology, these data are bronze instead of silver. 627 Word Boxer All clauses 83.6 83.1 74.3 DRS Operators VerbNet roles WordNet synsets nouns verbs, adverbs, adj. 93.2 84.1 79.7 86.1 65.1 93.3 82.5 79.4 88.5 58.7 88.0 71.4 72.5 82.5 49.3 Oracle sense numbers Oracle synsets Oracle roles 86.7 90.7 87.4 85.7 90.9 87.2 78.1 83.8 82.0 0.80 0.75 0.70 0.65 0.60 Table 9: F-scores of fine-grained evalua"
Q18-1043,C86-1127,0,0.598555,"o tackle bears similarities to the recently introduced task of mapping sentences to an Abstract Meaning Representation (AMR; Banarescu et al. 2013). But there are notable differences between DRS and AMR. Firstly, DRSs contain scope, which results in a more linguistically motivated treatment of modals, quantification, and negation. Secondly, DRSs contain a substantially higher number of variable bindings (reentrant nodes in AMR terminology), which are challenging for learning (Damonte et al., 2017). DRS parsing was attempted in the 1980s for small fragments of English (Johnson and Klein, 1986; Wada and Asher, 1986). Wide-coverage 619 Transactions of the Association for Computational Linguistics, vol. 6, pp. 619–634, 2018. Action Editor: Asli Celikyilmaz. Submission batch: 7/2018; Revision batch: 9/2018; Published 12/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. guaranteed by a specially designed checking tool (§3). (b) We compare different methods of representing input and output in §4. (c) We show in §5 that using additional, non-gold standard data can improve performance. (d) We perform a thorough analysis of the produced output and compare our methods"
Q18-1043,W16-2323,0,0.15191,"t (next to impossible in terms of computing time necessary, as a single 10-fold CV experiment takes 12 hours on GPU), but selected parameter settings that showed good performance for both the initial character and word-level representations (see §4 for details). The parameter search was performed using 10-fold CV on the training set. Training is stopped when there is no more improvement in perplexity on the validation set, which in our case occurred after 13–15 epochs. A powerful, well-known technique in the field of NMT is to use an ensemble of models during decoding (Sutskever et al., 2014; Sennrich et al., 2016a). The resulting model averages over the predictions of the individual models, which can balance out some of the errors. In our experiments, we apply this method when decoding on the test set, but not for our experiments of 10-fold CV (this would take too much computation time). 4 Experiments with Data Representations This section describes the experiments we conduct regarding the data representations of the input (English sentences) and output (a DRS) during training. 4.1 Between Characters and Words We first try two (default) representations: characterlevel and word-level. Most semantic par"
Q18-1043,P16-1162,0,0.333781,"t (next to impossible in terms of computing time necessary, as a single 10-fold CV experiment takes 12 hours on GPU), but selected parameter settings that showed good performance for both the initial character and word-level representations (see §4 for details). The parameter search was performed using 10-fold CV on the training set. Training is stopped when there is no more improvement in perplexity on the validation set, which in our case occurred after 13–15 epochs. A powerful, well-known technique in the field of NMT is to use an ensemble of models during decoding (Sutskever et al., 2014; Sennrich et al., 2016a). The resulting model averages over the predictions of the individual models, which can balance out some of the errors. In our experiments, we apply this method when decoding on the test set, but not for our experiments of 10-fold CV (this would take too much computation time). 4 Experiments with Data Representations This section describes the experiments we conduct regarding the data representations of the input (English sentences) and output (a DRS) during training. 4.1 Between Characters and Words We first try two (default) representations: characterlevel and word-level. Most semantic par"
R09-1080,N09-1003,1,0.442367,"mpalmer/projects/verbnet. html http://demo.patrickpantel.com/Content/verbocean/ personalised PageRank is computed by modifying the random jump distribution vector in the traditional PageRank equation. In our case, we concentrate all probability mass in the words present in the target text. Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calculation after 30 iterations. We have not optimised these values for this task. We used all the relations in WordNet 3.06 , including the disambiguated glosses7. This similarity method was used for word similarity [1] which report very good results on word similarity datasets. This baseline calculates similarity between two texts by counting the number of overlapping words. In order to do this we have used the software package Text::Similarity10 . This method has been applied both considering all the words that appear in the texts and discarding stop words. For the last, we have used the list of stop words of the English stemmer Snowball11 . 3.3 3.5 Semantic Vectors Semantic Vectors [19]8 is an open source (BSD license) software package that creates WORDSPACE models from plain text. Its aim is to provide a"
R09-1080,I05-5002,0,0.0239519,"nd semantic similarity (a modified version of the longest common subsequence and Second order Co-occurrence PMI respectively) between words and common-word order similarity. SenseClusters 1 is a language independent and unsupervised tool that clusters short contexts. It represents contexts using first or second order feature vectors. In order to reduce dimensionality it applies Singular Value Decomposition. Apart from the aforementioned systems, it is worth mentioning two datasets that have been used to evaluate approaches to short text similarity. The first is the Microsoft paraphrase corpus [3], extracted from news sources. It is made up of 5,801 pairs of sentences, each together with a human judgement indicating whether the two sentences can be considered paraphrases or not. The second is the Pilot Short Text Semantic Similarity Benchmark Data Set [10], which contains 30 sentence pairs from the Collins Cobuild dictionary. In this case the judgements are not binary, but on a scale (from 0.0 for minimum similarity to 4.0 for maximum similarity). 3 Methods The current section describes the different methods that we have applied. Approaches include Textual Entailment based on lexical a"
R09-1080,W07-1411,1,0.798945,"by defining the concept of Textual Entailment as a one-way meaning relation between two snippets. Moreover, a series of Workshops, called Recognising Textual Entailment (RTE2 ) challenges and the Answer Validation Exercise (AVE3 ) competitions, have been recently proposed with the objective of providing suitable frameworks to evaluate textual entailment systems . To address the specific semantic similarity phenomenon we are dealing with in this research work (i.e. semantic similarity between WordNet glosses and Wikipedia categories), we used our in-house textual entailment system presented in [6]. This system has been previously used to support other NLP applications rather than puristic textual entailment tasks. For 1 2 3 instance, in Question Answering [14] and automatic text summarisation [12] . As a brief system overview, it is worth mentioning the most relevant inferences implemented aimed at solving entailment relations: • Lexical inferences based on lexical distance measures. For instance, the Needleman-Wunsch algorithm, Smith-Waterman algorithm, a matching of consecutive subsequences, Jaro distance, Euclidean distance, IDF specificity based on word frequencies extracted from c"
R09-1080,J98-1004,0,0.307257,"Missing"
R09-1080,toral-etal-2008-named,1,0.391459,"Missing"
R09-1080,widdows-ferraro-2008-semantic,0,0.0138606,"the relations in WordNet 3.06 , including the disambiguated glosses7. This similarity method was used for word similarity [1] which report very good results on word similarity datasets. This baseline calculates similarity between two texts by counting the number of overlapping words. In order to do this we have used the software package Text::Similarity10 . This method has been applied both considering all the words that appear in the texts and discarding stop words. For the last, we have used the list of stop words of the English stemmer Snowball11 . 3.3 3.5 Semantic Vectors Semantic Vectors [19]8 is an open source (BSD license) software package that creates WORDSPACE models from plain text. Its aim is to provide an easy-to-use and efficient tool which can fit both research and production users. It uses a random projection algorithm to perform dimension reduction as this is a simpler and more efficient technique than other alternatives such as Singular Value Decomposition. It relies on Apache Lucene9 for tokenisation and indexing in order to create a term document matrix. Once the reference corpus has been tokenised and indexed, Semantic Vectors creates a WORDSPACE model from the resu"
rubino-etal-2014-quality,quirk-2004-training,0,\N,Missing
rubino-etal-2014-quality,2008.iwslt-papers.1,0,\N,Missing
rubino-etal-2014-quality,W12-3102,0,\N,Missing
rubino-etal-2014-quality,P02-1040,0,\N,Missing
rubino-etal-2014-quality,P07-2045,0,\N,Missing
rubino-etal-2014-quality,C04-1046,0,\N,Missing
rubino-etal-2014-quality,2009.eamt-1.5,0,\N,Missing
rubino-etal-2014-quality,P09-1018,0,\N,Missing
rubino-etal-2014-quality,2005.mtsummit-papers.11,0,\N,Missing
rubino-etal-2014-quality,W04-3250,0,\N,Missing
rubino-etal-2014-quality,2011.eamt-1.15,0,\N,Missing
rubino-etal-2014-quality,I13-1166,1,\N,Missing
rubino-etal-2014-quality,W12-5706,1,\N,Missing
rubino-etal-2014-quality,W03-0413,0,\N,Missing
rubino-etal-2014-quality,W12-3118,0,\N,Missing
ruimy-toral-2008-semantic,ruimy-etal-2002-clips,1,\N,Missing
ruimy-toral-2008-semantic,ruimy-2006-merging,1,\N,Missing
ruimy-toral-2008-semantic,del-gratta-etal-2008-simple,1,\N,Missing
toral-2014-tlaxcala,J03-3002,0,\N,Missing
toral-2014-tlaxcala,W13-2506,0,\N,Missing
toral-etal-2008-named,E06-1002,0,\N,Missing
toral-etal-2008-named,sekine-etal-2002-extended,0,\N,Missing
toral-etal-2008-named,W02-1111,0,\N,Missing
toral-etal-2008-named,J06-1001,0,\N,Missing
W06-2809,W03-0103,0,0.0195823,"easons why we have chosen this encyclopedia are the following: Several research works have been carried out in this direction. An example of this is a NER system which uses trigger gazetteers automatically extracted from WordNet (Magnini et al., 2002) by using wordnet predicates. The advantage in this case is that the resource used is multilingual and thus, porting it to another language is almost straightforward (Negri and Magnini, 2004). There is also a work that deals with automatically building location gazetteers from internet texts by applying text mining procedures (Ourioupina, 2002), (Uryupina, 2003). However, this work uses linguistic patterns, and thus is language dependent. The author claims that the approach may successfully be used to create gazetteers for NER. • It is a big source of information. By December 2005, it has over 2,500,000 definitions. The English version alone has more than 850,000 entries. We agree with (Magnini et al., 2002) that in order to automatically create and maintain trigger gazetteers, using a hierarchy of common nouns is a good approach. Therefore, we want to focus on the automatically creation and maintenance of entity gazetteers. Another reason for this i"
W06-2809,carreras-etal-2004-freeling,0,0.04019,"Missing"
W06-2809,M98-1001,0,0.0539518,"Missing"
W06-2809,W02-1109,0,0.221391,"ntries in an encyclopedia and that some features of their definitions in the encyclopedia can help to classify them into their correct entity category. The encyclopedia used has been Wikipedia1 . According to the English version of Wikipedia 2 , Wikipedia is a multi-lingual web-based, freecontent encyclopedia which is updated continuously in a collaborative way. The reasons why we have chosen this encyclopedia are the following: Several research works have been carried out in this direction. An example of this is a NER system which uses trigger gazetteers automatically extracted from WordNet (Magnini et al., 2002) by using wordnet predicates. The advantage in this case is that the resource used is multilingual and thus, porting it to another language is almost straightforward (Negri and Magnini, 2004). There is also a work that deals with automatically building location gazetteers from internet texts by applying text mining procedures (Ourioupina, 2002), (Uryupina, 2003). However, this work uses linguistic patterns, and thus is language dependent. The author claims that the approach may successfully be used to create gazetteers for NER. • It is a big source of information. By December 2005, it has over"
W06-2809,M98-1021,0,0.0956609,"Missing"
W09-2420,C08-1003,1,0.83081,"Missing"
W09-2420,W06-1615,0,0.185914,"Missing"
W09-2420,P07-1007,0,0.123157,"Missing"
W09-2420,W04-3237,0,0.0779774,"Missing"
W09-2420,P07-1033,0,0.148042,"Missing"
W09-2420,W00-1322,0,0.0374768,"Missing"
W09-2420,S01-1004,0,0.385256,"The difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledgebased WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. With this paper we want to motivate the creation of an allwords test dataset for WSD on the environment domain in several languages, and present the overall design of this SemEval task. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in the last Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007). Specific domains pose fresh challenges to WSD systems: the context in which the senses occur might change, distributions and predominant senses vary, some words tend to occur in fewer senses in specific domains, and new senses and terms might be involved. Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Langu"
W09-2420,H05-1053,0,0.676819,"ated words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Language Processing, including Word Sense Disambiguation. Supervised Word Sense Disambiguation systems trained on general corpora are known to perform worse when applied to specific domains (Escudero et al., 2000; Mart´ınez and Agirre, 2000), and domain adaptation techniques have been proposed as a solution to this problem with mixed results. Current research on applying WSD to specific domains has been evaluated on three available lexicalsample datasets (Ng and Lee, 1996; Weeber et al., 2001; Koeling et al., 2005). This kind of dataset contains hand-labeled examples for a handful of selected target words. As the systems are evaluated on a few words, the actual performance of the systems over complete texts can not be measured. Differences in behavior of WSD systems when applied to lexical-sample and all-words datasets have been observed on previous Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007): supervised systems attain results on the high 80’s and beat the most frequent baseline by a large margin for lexical-sample datasets, but results on the all-wo"
W09-2420,W04-0807,0,0.189564,"ound by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledgebased WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. With this paper we want to motivate the creation of an allwords test dataset for WSD on the environment domain in several languages, and present the overall design of this SemEval task. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in the last Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007). Specific domains pose fresh challenges to WSD systems: the context in which the senses occur might change, distributions and predominant senses vary, some words tend to occur in fewer senses in specific domains, and new senses and terms might be involved. Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Language Processing, includi"
W09-2420,H93-1061,0,0.37412,"cific domain. Good results in this setting would show that supervised domain adaptation is working, and that generic WSD systems can be supplemented with hand-tagged examples from the target domain. There is an additional setting, where a generic WSD system is supplemented with untagged examples from the domain. Good results in this setting would show that semi-supervised domain adaptation works, and that generic WSD systems can be supplemented with untagged examples from the target domain in order to improve their results. Most of current all-words generic supervised WSD systems take SemCor (Miller et al., 1993) as their source corpus, i.e. they are trained on SemCor examples and then applied to new examples. SemCor is the largest publicly available annotated corpus. It’s mainly a subset of the Brown Corpus, plus the novel The Red Badge of Courage. The Brown corpus is balanced, yet not from the general domain, as it comprises 500 documents drawn from different domains, each approximately 2000 words long. Although the Brown corpus is balanced, SemCor is not, as the documents were not chosen at random. 4 State-of-the-art in WSD for specific domains Initial work on domain adaptation for WSD systems show"
W09-2420,P96-1006,0,0.698695,"fer from lack of coverage of domain-related words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Language Processing, including Word Sense Disambiguation. Supervised Word Sense Disambiguation systems trained on general corpora are known to perform worse when applied to specific domains (Escudero et al., 2000; Mart´ınez and Agirre, 2000), and domain adaptation techniques have been proposed as a solution to this problem with mixed results. Current research on applying WSD to specific domains has been evaluated on three available lexicalsample datasets (Ng and Lee, 1996; Weeber et al., 2001; Koeling et al., 2005). This kind of dataset contains hand-labeled examples for a handful of selected target words. As the systems are evaluated on a few words, the actual performance of the systems over complete texts can not be measured. Differences in behavior of WSD systems when applied to lexical-sample and all-words datasets have been observed on previous Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007): supervised systems attain results on the high 80’s and beat the most frequent baseline by a large margin for lexica"
W09-2420,S07-1016,0,0.278613,"ems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledgebased WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. With this paper we want to motivate the creation of an allwords test dataset for WSD on the environment domain in several languages, and present the overall design of this SemEval task. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in the last Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007). Specific domains pose fresh challenges to WSD systems: the context in which the senses occur might change, distributions and predominant senses vary, some words tend to occur in fewer senses in specific domains, and new senses and terms might be involved. Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Language Processing, including Word Sense Disambigu"
W09-2420,rose-etal-2002-reuters,0,0.0411856,"d a total of 192,800 occurrences of these words were tagged with WordNet 1.5 senses, more than 1,000 instances per word in average. The examples from BC comprise 78,080 occurrences of word senses, and examples from WSJ consist on 114,794 occurrences. In domain adaptation experiments, the Brown Corpus examples play the role of general corpora, and the examples from the WSJ play the role of domain-specific examples. Koeling et al. (2005) present a corpus were the examples are drawn from the balanced B NC corpus (Leech, 1992) and the S PORTS and F INANCES sections of the newswire Reuters corpus (Rose et al., 2002), comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns. The nouns were selected because they were 124 salient in either the S PORTS or F INANCES domains, or because they had senses linked to those domains. The occurrences were hand-tagged with the senses from WordNet version 1.7.1 (Fellbaum, 1998). In domain adaptation experiments the B NC examples play the role of general corpora, and the F INANCES and S PORTS examples the role of two specific domain corpora. Finally, a dataset for biomedicine was developed by Weeber et al. (2001), and has been used"
W09-2420,D08-1105,0,0.0322595,"Missing"
W09-2420,W04-0811,0,\N,Missing
W09-2420,vossen-etal-2008-kyoto,1,\N,Missing
W09-2420,E09-1006,1,\N,Missing
W09-2420,E09-1005,1,\N,Missing
W09-2420,E09-1045,0,\N,Missing
W09-2420,W00-0901,0,\N,Missing
W09-2420,W00-1326,1,\N,Missing
W09-2420,J07-4005,0,\N,Missing
W09-2420,W08-2114,0,\N,Missing
W09-2420,S07-1097,0,\N,Missing
W10-3704,attia-etal-2010-automatically,1,0.604545,"Missing"
W10-3704,deksne-etal-2008-dictionary,0,0.29076,"een applied to bigrams and trigrams, and it becomes more problematic to extract MWEs of more than three words. As a consequence, each approach requires specific resources and is suitable for dealing with only one side of a multifaceted problem. Pecina (2010) evaluates 82 lexical association measures for the ranking of collocation candidates and concludes that it is not possible to select a single best universal measure, and that different measures give different results for different tasks depending on data, language, and the types of MWE that the task is focused on. Similarly, Ramisch et al. (2008) investigate the hypothesis that MWEs can be detected solely by looking at the distinct statistical properties of their individual words and conclude that the association measures can only detect trends and preferences in the co-occurrences of words. A lot of effort has concentrated on the task of 2 Data Resources In this project we use three data resources for extracting MWEs. These resources differ widely in nature, size, structure and the main purpose they are used for. In this section we give a brief introduction to each of these data resources. Wikipedia (WK) is a freely-available multili"
W10-3704,elkateb-etal-2006-building,0,0.0380663,"Missing"
W10-3704,W09-2905,0,0.0428556,"Missing"
W10-3704,W04-0411,0,0.0213009,"ne Translation (Deksne, 2008). There are two basic criteria for identifying MWEs: first, component words exhibit statistically significant co-occurrence, and second, they show a certain level of semantic opaqueness or non-compositionality. Statistically significant cooccurrence can give a good indication of how likely a sequence of words is to form an MWE. This is particularly interesting for statistical techniques which utilize the fact that a large number of MWEs are composed of words that co-occur together more often than can be expected by chance. The compositionality, or decomposability (Villavicencio et al. 2004), of MWEs is also a core issue that presents a challenge for NLP applications because the meaning of the expression is not directly predicted from the meaning of the component words. In this respect, compositionalily varies between phrases that are highly comIntroduction A lexicon of multiword expressions (MWEs) has a significant importance as a linguistic resource because MWEs cannot usually be analyzed literally, or word-for-word. In this paper we apply three approaches to the extraction of Arabic MWEs from multilingual, bilingual, and monolingual data sources. We rely on linguistic informat"
W10-3704,W97-0311,0,0.144125,"two well-known among them are “non-substitutability”, when a word in the expression cannot be substituted by a semantically equivalent word, and “single-word paraphrasability”, when the expression can be paraphrased or translated by a single word. These two indications have been exploited differently by different researchers. Van de Cruys and Moiro´ n (2006) develop an unsupervised method for detecting MWEs using clusters of semantically related words and taking the ratio of the word preference over the cluster preference as an indication of how likely a particular expression is to be an MWE. Melamed (1997) investigates techniques for identifying non-compositional compounds in English-French parallel corpora and emphasises that translation models that take noncompositional compounds into account are more accurate. Moir´on and Tiedemann (2006) use word alignment of parallel corpora to locate the translation of an MWE in a target language and decide whether the original expression is idiomatic or literal. The technique used here is inspired by that of Zarrieß and Kuhn (2009) who rely on the linguistic intuition that if a group of words in one language is translated as a single word in another lang"
W10-3704,vintar-fiser-2008-harvesting,0,0.0593775,"Missing"
W10-3704,W06-2405,0,0.0453337,"expression as a whole is utterly  unrelated to the component words, such as,         !  farasu ’l-nabiyyi, “grasshopper”, lit. “the horse of the Prophet”. automatically extracting MWEs for various languages besides English, including Slovene (Vintar and Fiˇser, 2008), Chinese (Duan et al., 2009), Czech (Pecina, 2010), Dutch (Van de Cruys and Moir´on, 2006), Latvian (Deksne, 2008) and German ( Zarrieß and Kuhn, 2009). A few papers, however, focus on Arabic MWEs. Boulaknadel et al. (2009) develop a hybrid multiword term extraction tool for Arabic in the “environment” domain. Attia (2006) reports on the semi-automatic extraction of various types of MWEs in Arabic and how they are used in an LFG-based parser. In this paper we report on three different methods for the extraction of MWEs for Arabic, a less resourced language. Our approach is linguistically motivated and can be applied to other languages. 1.2 Related Work A considerable amount of research has focused on the identification and extraction of MWEs. Given the heterogeneity of MWEs, different approaches were devised. Broadly speaking, work on the extraction of MWEs revolves around four approaches: (a) statistical metho"
W10-3704,W09-2904,0,0.321046,"ase”, and those  that show a degree of idiomaticity, such as,          madiynatu ’l-mal¯ ahiy, “amusement park”, lit. “city of amusements”. In extreme cases the meaning of the expression as a whole is utterly  unrelated to the component words, such as,         !  farasu ’l-nabiyyi, “grasshopper”, lit. “the horse of the Prophet”. automatically extracting MWEs for various languages besides English, including Slovene (Vintar and Fiˇser, 2008), Chinese (Duan et al., 2009), Czech (Pecina, 2010), Dutch (Van de Cruys and Moir´on, 2006), Latvian (Deksne, 2008) and German ( Zarrieß and Kuhn, 2009). A few papers, however, focus on Arabic MWEs. Boulaknadel et al. (2009) develop a hybrid multiword term extraction tool for Arabic in the “environment” domain. Attia (2006) reports on the semi-automatic extraction of various types of MWEs in Arabic and how they are used in an LFG-based parser. In this paper we report on three different methods for the extraction of MWEs for Arabic, a less resourced language. Our approach is linguistically motivated and can be applied to other languages. 1.2 Related Work A considerable amount of research has focused on the identification and extraction of MWEs"
W10-3704,W03-1812,0,\N,Missing
W10-3704,boulaknadel-etal-2008-multi,0,\N,Missing
W11-4417,2006.bcs-1.5,1,0.693113,"ttested in contemporary use. The database is built using a corpus of 1,089,111,204 words, a pre-annotation tool, machine learning techniques, and knowledgebased pattern matching to automatically acquire lexical knowledge. Our morphological transducer is evaluated and compared to LDC’s SAMA (Standard Arabic Morphological Analyser). 1 Introduction Due to its complexity, Arabic morphology has always been a challenge for computational processing and a hard testing ground for morphological analysis technologies. A lexicon is a core component of any morphological analyser (Dichy and Farghaly, 2003; Attia, 2006; Buckwalter, 2004; Beesley, 2001). The quality and coverage of the lexical database determines the quality and coverage of the morphological analyser, and limitations in the lexicon will cascade through to higher levels of processing. In this paper, we present an approach to automatically construct a corpus-based lexical database for Modern Standard Arabic (MSA), focusing on the 1 http://sourceforge.net/projects/aracomlex/ 125 This paper is structured as follows. In the introduction, we differentiate between MSA, the focus of this research, and Classical Arabic (CA) which is a historical vers"
W11-4417,2003.mtsummit-semit.5,0,0.72348,"exical entries no longer attested in contemporary use. The database is built using a corpus of 1,089,111,204 words, a pre-annotation tool, machine learning techniques, and knowledgebased pattern matching to automatically acquire lexical knowledge. Our morphological transducer is evaluated and compared to LDC’s SAMA (Standard Arabic Morphological Analyser). 1 Introduction Due to its complexity, Arabic morphology has always been a challenge for computational processing and a hard testing ground for morphological analysis technologies. A lexicon is a core component of any morphological analyser (Dichy and Farghaly, 2003; Attia, 2006; Buckwalter, 2004; Beesley, 2001). The quality and coverage of the lexical database determines the quality and coverage of the morphological analyser, and limitations in the lexicon will cascade through to higher levels of processing. In this paper, we present an approach to automatically construct a corpus-based lexical database for Modern Standard Arabic (MSA), focusing on the 1 http://sourceforge.net/projects/aracomlex/ 125 This paper is structured as follows. In the introduction, we differentiate between MSA, the focus of this research, and Classical Arabic (CA) which is a hi"
W11-4417,E09-2008,0,0.126089,"technology that makes it especially attractive in dealing with human language morphologies; among these are the ability to handle concatenative and nonconcatenative morphotactics, and the high speed and efficiency in handling large automata of lexicons with their derivations and inflections that can run into millions of paths. The Xerox XFST System (Beesley and Karttunen, 2003) is a well-known finite state compiler, but the disadvantage of this tool is that it is a proprietary software, which limits its use in the larger research community. Fortunately, there is an alternative, namely Foma, (Hulden, 2009), which is an opensource finite-state toolkit that implements the Xerox lexc and xfst utilities. We have developed an opensource morphological analyser for Arabic using the Foma compiler allowing us to share our morphology with third parties. The lexical database, which is being edited and validated, is used to automatically extend and update the morphological analyser, allowing for greater coverage and better capabilities. Arabic words are formed through the amalgamation of two tiers, namely root and pattern. A root is a sequence of three consonants and the pattern is a template of vowels (or"
W11-4417,P08-2030,0,0.204709,"Missing"
W12-5606,W05-0909,0,0.383134,"ally Hindi) has gained tremendous research interest in India and elsewhere. Many English to Hindi and Indian Languages to Indian Languages MT systems have been designed, for example AnglaBharati (Sinha et al., 1995), Anusaaraka2 (Chaudhury et al., 2010), Anuvadaksh3, Google4, Sampark5, MaTra6 (Ananthakrishnan et al., 2006), to name just a few. However, the issue of evaluating the output of these MT systems has remained rather unexplored. The state-of-the-art methods for automatic MT evaluation are represented by BLEU (Papineni et al., 2002) and closely related NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) and TER (Snover et al., 2006). These metrics have been widely accepted as benchmarks for MT system evaluation. However, the research community is also aware of the deficiencies of these metrics (Callison-Burch et al., 2006). Globally, these automatic MT evaluation metrics (BLEU, NIST, TER, METEOR, etc.) are being studied with great interest for different language pairs. But their direct applicability to Hindi, or other Indian languages for that matter, needs proper investigation. Indian languages are characteristically different from English and other related Europea"
W12-5606,sankaran-etal-2008-common,0,0.030143,"a linguistically-motivated unit e.g., it can be an ambiguous word, a verb-particle construction, a noun-noun compound, a PoS n-gram etc. The level of detail and the specific linguistic phenomena included in the taxonomy can vary depending on what the users want to investigate as part of the diagnostic evaluation. However, the taxonomy of automatic diagnostic evaluation should be widely accepted. The categories that are out of scope for current NLP tools to recognize have been ignored in this study. In light of the above consideration, we adopted the taxonomy introduced by Lata et al. (2012), Baskaran et al. (2008) and the IIIT Tagset11 (Bharati et al., 2006) for Hindi. The taxonomy includes typical checkpoints at word level. Some examples of the representative checkpoints at different levels for English and Hindi languages have been presented in the following subsection. 4.1 English to Hindi Checkpoints The implementation of the English to Hindi checkpoint taxonomy can take into account various checkpoints at word and phrase level. However, only 8 word level categories have been considered for this study. The taxonomy is shown in Table 1. In practice, any tag used by parsers (e.g. NP, VP, PP, etc.) can"
W12-5606,2010.eamt-1.12,0,0.0287093,"Missing"
W12-5606,W12-3105,0,0.0126352,"lexical, semantic, and syntactic level. Some automatic methods for error analysis using base forms and PoS tags have been proposed in (Popović et al., 2006; Popović and Ney, 2011). The proposed methods have been used for estimation of inflectional and reordering errors. Popović and Burchardt (2011) present a method for automatic error classification. Popović (2011) describes a tool that classifies errors into five categories based on the hierarchy proposed by Vilar et al. (2006). Popović (2012) describes RGBF, a tool for automatic evaluation of MT output based on n-gram precision and recall. Fishel et al. (2012) quantifies translation quality based on the frequencies of different error categories. Xiong et al. (2010) used a classifier trained with a set of linguistic features to automatically detect incorrect segments in MT output. EAGLES (1996) distinguishes a type of evaluation whose purpose is to discover the reason(s) why a system did not produce the results it was expected to. Working on these lines Zhou et al. (2008) proposed diagnostic evaluation of linguistic checkpoints. Naskar et al. (2011) proposed a framework for diagnostic MT evaluation which offers similar functionality as proposed in ("
W12-5606,W07-0734,0,0.0190737,"mendous research interest in India and elsewhere. Many English to Hindi and Indian Languages to Indian Languages MT systems have been designed, for example AnglaBharati (Sinha et al., 1995), Anusaaraka2 (Chaudhury et al., 2010), Anuvadaksh3, Google4, Sampark5, MaTra6 (Ananthakrishnan et al., 2006), to name just a few. However, the issue of evaluating the output of these MT systems has remained rather unexplored. The state-of-the-art methods for automatic MT evaluation are represented by BLEU (Papineni et al., 2002) and closely related NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) and TER (Snover et al., 2006). These metrics have been widely accepted as benchmarks for MT system evaluation. However, the research community is also aware of the deficiencies of these metrics (Callison-Burch et al., 2006). Globally, these automatic MT evaluation metrics (BLEU, NIST, TER, METEOR, etc.) are being studied with great interest for different language pairs. But their direct applicability to Hindi, or other Indian languages for that matter, needs proper investigation. Indian languages are characteristically different from English and other related European languages for which thes"
W12-5606,2011.mtsummit-papers.60,1,0.818646,"an languages for which these metrics are mostly used. There have been some efforts in this direction for Indian languages (Chatterjee and Balyan, 2011; Gupta et al., 2010; Ananthakrishnan et al., 2007; Chatterjee et al., 2007; Moona et al., 2004). Barring these few exceptions, the subject has not been studied deeply. Most of these approaches, however, either cover human evaluation, or consider modification of existing automatic metrics (like BLEU and METEOR) to make them more suitable for Indian languages. None of these works has been targeted towards diagnostic evaluation (Zhou et al., 2008; Naskar et al., 2011; Popović, 2011), which not only provides quantitative analysis, but also qualitative feedback of the machine translated text. It also provides feedback and detailed analysis of how an MT system performs for different linguistic features like verbs, nouns, compounds etc. Our final aim is to come up with an approach for diagnostic evaluation of MT that can be adapted to Indian languages. In the present work the experiments have been carried out with the DELiC4MT (Toral et al., 2012) toolkit as it is language independent. The experiments have been carried out to adapt the tool for Hindi, which c"
W12-5606,J03-1002,0,0.00270755,"ysis and KAF conversion, word alignment extraction, defining kybots and evaluation. The tool makes extensive use of already available NLP tools and representation standards. The evaluation pipeline proceeds as follows. 7 http://www.computing.dcu.ie/~atoral/delic4mt(under the GPL-v3 license). 63  The source and target sides of the gold standard (test set) are processed by respective PoS taggers (Treetagger8 for English and a shallow parser for Hindi) and converted into KYOTO Annotation Format (KAF) (Bosma et al., 2009) to represent textual analysis. The test set is word aligned using GIZA++ 9(Och and Ney, 2003), and identifiers of the aligned tokens are stored. Kybot10 (Vossen et al., 2010) profiles specifying the linguistic checkpoints to be extracted are run on the KAF text and the matching terms are extracted. The evaluation module takes kybot output, KAF text, word alignments and the output of an MT system (plain text, no word alignment is performed on it) as inputs. It calculates the performance of the MT system over the linguistic checkpoint(s) considered.    The details of the tool regarding KAF files and kybot profiles can be found in Toral et al. (2012). 4 Linguistic Checkpoints A lingui"
W12-5606,P02-1040,0,0.0875737,"dian languages. In the last 15 years or so, MT into Indian languages (especially Hindi) has gained tremendous research interest in India and elsewhere. Many English to Hindi and Indian Languages to Indian Languages MT systems have been designed, for example AnglaBharati (Sinha et al., 1995), Anusaaraka2 (Chaudhury et al., 2010), Anuvadaksh3, Google4, Sampark5, MaTra6 (Ananthakrishnan et al., 2006), to name just a few. However, the issue of evaluating the output of these MT systems has remained rather unexplored. The state-of-the-art methods for automatic MT evaluation are represented by BLEU (Papineni et al., 2002) and closely related NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) and TER (Snover et al., 2006). These metrics have been widely accepted as benchmarks for MT system evaluation. However, the research community is also aware of the deficiencies of these metrics (Callison-Burch et al., 2006). Globally, these automatic MT evaluation metrics (BLEU, NIST, TER, METEOR, etc.) are being studied with great interest for different language pairs. But their direct applicability to Hindi, or other Indian languages for that matter, needs proper investigation. Indian lan"
W12-5606,J11-4002,0,0.0786144,"rk Although diagnostic evaluation of MT has been occasionally addressed in the literature in the last few years, no widely accepted solution seems to have emerged till date. A framework proposed by Vilar et al. (2006) analyzes the errors manually. The scheme covers five top-level classes: missing words, incorrect words, unknown words, word order and punctuation errors. Farrús et al. (2010) classified errors at orthographic, morphological, lexical, semantic, and syntactic level. Some automatic methods for error analysis using base forms and PoS tags have been proposed in (Popović et al., 2006; Popović and Ney, 2011). The proposed methods have been used for estimation of inflectional and reordering errors. Popović and Burchardt (2011) present a method for automatic error classification. Popović (2011) describes a tool that classifies errors into five categories based on the hierarchy proposed by Vilar et al. (2006). Popović (2012) describes RGBF, a tool for automatic evaluation of MT output based on n-gram precision and recall. Fishel et al. (2012) quantifies translation quality based on the frequencies of different error categories. Xiong et al. (2010) used a classifier trained with a set of linguistic f"
W12-5606,2011.eamt-1.36,0,0.0128152,"o widely accepted solution seems to have emerged till date. A framework proposed by Vilar et al. (2006) analyzes the errors manually. The scheme covers five top-level classes: missing words, incorrect words, unknown words, word order and punctuation errors. Farrús et al. (2010) classified errors at orthographic, morphological, lexical, semantic, and syntactic level. Some automatic methods for error analysis using base forms and PoS tags have been proposed in (Popović et al., 2006; Popović and Ney, 2011). The proposed methods have been used for estimation of inflectional and reordering errors. Popović and Burchardt (2011) present a method for automatic error classification. Popović (2011) describes a tool that classifies errors into five categories based on the hierarchy proposed by Vilar et al. (2006). Popović (2012) describes RGBF, a tool for automatic evaluation of MT output based on n-gram precision and recall. Fishel et al. (2012) quantifies translation quality based on the frequencies of different error categories. Xiong et al. (2010) used a classifier trained with a set of linguistic features to automatically detect incorrect segments in MT output. EAGLES (1996) distinguishes a type of evaluation whose"
W12-5606,W06-3101,0,0.0178988,"ure work. 2 Related work Although diagnostic evaluation of MT has been occasionally addressed in the literature in the last few years, no widely accepted solution seems to have emerged till date. A framework proposed by Vilar et al. (2006) analyzes the errors manually. The scheme covers five top-level classes: missing words, incorrect words, unknown words, word order and punctuation errors. Farrús et al. (2010) classified errors at orthographic, morphological, lexical, semantic, and syntactic level. Some automatic methods for error analysis using base forms and PoS tags have been proposed in (Popović et al., 2006; Popović and Ney, 2011). The proposed methods have been used for estimation of inflectional and reordering errors. Popović and Burchardt (2011) present a method for automatic error classification. Popović (2011) describes a tool that classifies errors into five categories based on the hierarchy proposed by Vilar et al. (2006). Popović (2012) describes RGBF, a tool for automatic evaluation of MT output based on n-gram precision and recall. Fishel et al. (2012) quantifies translation quality based on the frequencies of different error categories. Xiong et al. (2010) used a classifier trained wi"
W12-5606,2006.amta-papers.25,0,0.147819,"and elsewhere. Many English to Hindi and Indian Languages to Indian Languages MT systems have been designed, for example AnglaBharati (Sinha et al., 1995), Anusaaraka2 (Chaudhury et al., 2010), Anuvadaksh3, Google4, Sampark5, MaTra6 (Ananthakrishnan et al., 2006), to name just a few. However, the issue of evaluating the output of these MT systems has remained rather unexplored. The state-of-the-art methods for automatic MT evaluation are represented by BLEU (Papineni et al., 2002) and closely related NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) and TER (Snover et al., 2006). These metrics have been widely accepted as benchmarks for MT system evaluation. However, the research community is also aware of the deficiencies of these metrics (Callison-Burch et al., 2006). Globally, these automatic MT evaluation metrics (BLEU, NIST, TER, METEOR, etc.) are being studied with great interest for different language pairs. But their direct applicability to Hindi, or other Indian languages for that matter, needs proper investigation. Indian languages are characteristically different from English and other related European languages for which these metrics are mostly used. The"
W12-5606,vilar-etal-2006-error,0,0.0231686,"ation tool, DELiC4MT, which has been used for this study. In Section 4, the various linguistic checkpoints considered for the study of English and Hindi have been discussed. Section 5 discusses the experimental setup and compares the results obtained on the EnglishHindi test set using DELiC4MT and automatic evaluation metrics. This is followed by conclusions and avenues for future work. 2 Related work Although diagnostic evaluation of MT has been occasionally addressed in the literature in the last few years, no widely accepted solution seems to have emerged till date. A framework proposed by Vilar et al. (2006) analyzes the errors manually. The scheme covers five top-level classes: missing words, incorrect words, unknown words, word order and punctuation errors. Farrús et al. (2010) classified errors at orthographic, morphological, lexical, semantic, and syntactic level. Some automatic methods for error analysis using base forms and PoS tags have been proposed in (Popović et al., 2006; Popović and Ney, 2011). The proposed methods have been used for estimation of inflectional and reordering errors. Popović and Burchardt (2011) present a method for automatic error classification. Popović (2011) descri"
W12-5606,W10-3301,0,0.0169006,"ion. The tool makes extensive use of already available NLP tools and representation standards. The evaluation pipeline proceeds as follows. 7 http://www.computing.dcu.ie/~atoral/delic4mt(under the GPL-v3 license). 63  The source and target sides of the gold standard (test set) are processed by respective PoS taggers (Treetagger8 for English and a shallow parser for Hindi) and converted into KYOTO Annotation Format (KAF) (Bosma et al., 2009) to represent textual analysis. The test set is word aligned using GIZA++ 9(Och and Ney, 2003), and identifiers of the aligned tokens are stored. Kybot10 (Vossen et al., 2010) profiles specifying the linguistic checkpoints to be extracted are run on the KAF text and the matching terms are extracted. The evaluation module takes kybot output, KAF text, word alignments and the output of an MT system (plain text, no word alignment is performed on it) as inputs. It calculates the performance of the MT system over the linguistic checkpoint(s) considered.    The details of the tool regarding KAF files and kybot profiles can be found in Toral et al. (2012). 4 Linguistic Checkpoints A linguistic checkpoint is a linguistically-motivated unit e.g., it can be an ambiguous w"
W12-5606,P10-1062,0,0.0246481,"tags have been proposed in (Popović et al., 2006; Popović and Ney, 2011). The proposed methods have been used for estimation of inflectional and reordering errors. Popović and Burchardt (2011) present a method for automatic error classification. Popović (2011) describes a tool that classifies errors into five categories based on the hierarchy proposed by Vilar et al. (2006). Popović (2012) describes RGBF, a tool for automatic evaluation of MT output based on n-gram precision and recall. Fishel et al. (2012) quantifies translation quality based on the frequencies of different error categories. Xiong et al. (2010) used a classifier trained with a set of linguistic features to automatically detect incorrect segments in MT output. EAGLES (1996) distinguishes a type of evaluation whose purpose is to discover the reason(s) why a system did not produce the results it was expected to. Working on these lines Zhou et al. (2008) proposed diagnostic evaluation of linguistic checkpoints. Naskar et al. (2011) proposed a framework for diagnostic MT evaluation which offers similar functionality as proposed in (Zhou et al., 2008) but is language independent. 3 DELiC4MT: A Diagnostic MT Evaluation Tool DELiC4MT7 (Diag"
W12-5606,C08-1141,0,0.0832219,"ther related European languages for which these metrics are mostly used. There have been some efforts in this direction for Indian languages (Chatterjee and Balyan, 2011; Gupta et al., 2010; Ananthakrishnan et al., 2007; Chatterjee et al., 2007; Moona et al., 2004). Barring these few exceptions, the subject has not been studied deeply. Most of these approaches, however, either cover human evaluation, or consider modification of existing automatic metrics (like BLEU and METEOR) to make them more suitable for Indian languages. None of these works has been targeted towards diagnostic evaluation (Zhou et al., 2008; Naskar et al., 2011; Popović, 2011), which not only provides quantitative analysis, but also qualitative feedback of the machine translated text. It also provides feedback and detailed analysis of how an MT system performs for different linguistic features like verbs, nouns, compounds etc. Our final aim is to come up with an approach for diagnostic evaluation of MT that can be adapted to Indian languages. In the present work the experiments have been carried out with the DELiC4MT (Toral et al., 2012) toolkit as it is language independent. The experiments have been carried out to adapt the to"
W12-5606,E06-1032,0,\N,Missing
W12-5705,P07-1033,0,0.148002,"Missing"
W12-5705,2010.amta-papers.9,0,0.255096,"Missing"
W12-5705,W07-0717,0,0.0804631,"Missing"
W12-5705,2012.amta-monomt.2,0,0.050714,"Missing"
W12-5705,W07-0733,0,0.100456,"Missing"
W12-5705,W02-1019,0,0.0588576,"Missing"
W12-5705,2009.iwslt-evaluation.4,1,0.867836,"Missing"
W12-5705,2011.mtsummit-papers.60,1,0.837006,"Missing"
W12-5705,P09-3009,1,0.805493,"Missing"
W12-5705,okita-2012-annotated,1,0.565616,"Missing"
W12-5705,W10-4006,1,0.50897,"Missing"
W12-5705,P02-1040,0,0.0828711,"Missing"
W12-5705,2012.eamt-1.38,1,0.879438,"Missing"
W12-5705,P07-1040,0,0.162443,"Missing"
W12-5705,W10-2602,0,0.0438618,"Missing"
W13-2227,P10-2041,0,0.156028,"Missing"
W13-2227,D11-1033,0,0.177558,"Missing"
W13-2227,N07-1029,0,0.0348966,"gaword. Each LM is trained with the SRILM toolkit, before interpolating all the LMs according to their weights obtained by minimizing the perplexity on the tuning set (WMT2011 and WMT2012 test sets). As SRILM can only interpolate 10 LMs, we first interpolate a LM with Europarl, News Commentary, News Crawl (20072012, each year individually, 6 separate parts), then we interpolate a new LM with this interpolated LM and LDC Gigawords (we kept the Gigaword subsets separated according to the news sources as distributed by LDC, which leads to 7 corpus). We also use a word-level combination strategy (Rosti et al., 2007) to combine the three translation hypotheses. To combine these systems, we first use the Minimum Bayes-Risk (MBR) (Kumar and Byrne, 2004) decoder to obtain the 5 best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al., 2000). We then use IHMM (He et al., 2008) to choose the backbone build the CN and finally search for and generate the best translation. We tune the system parameters on development set with Simple-Simplex algorithm. The parameters for system weights are set equal. Other parameters like language model, length penalty and combination coefficient are"
W13-2227,P96-1041,0,0.0728056,"scribed. We investigate the use of linguistic information to select parallel data. In Section 3, we present the systems built for the French-English pair in both di2 Setting Our setup uses the M OSES toolkit, version 1.0 (Koehn et al., 2007). We use a pipeline with the phrase-based decoder with standard parameters, unless noted otherwise. The decoder uses cube pruning (-cube-pruning-pop-limit 2000 -s 2000), MBR (-mbr-size 800 -mbr-scale 1) and monotone at punctuation reordering. Individual language models (LMs), 5-gram and smoothed using a simplified version of the improved Kneser-Ney method (Chen and Goodman, 1996), are built for each monolingual corpus using IRSTLM 5.80.01 (Federico et al., 2008). These LMs are then interpolated with IRSTLM using the test set of WMT11 as the development set. Finally, the interpolated LMs are merged into one LM preserving the weights using SRILM (Stolcke, 2002). We use all the parallel corpora available for this language pair: Europarl (EU), News Commentary (NC), United Nations (UN) and Common Crawl (CC). Regarding monolingual corpora, we use the freely available monolingual corpora (EuIntroduction 1 Spanish-English http://www.nclt.dcu.ie/mt/ http://www.prompsit.com/ 21"
W13-2227,P10-4002,0,0.0227851,"rther experiments are still required to determine the minimum sample size needed to outperform both the in-domain system and the combination of the two translation models. Finally, for the German-English language pair, we presents our exploitation of long ordering problem. We compared two hierarchical models with one phrase-based model, and we also use a system combination strategy to further improve 4.2.2 Three baseline systems We use the data set up described by the former subsection and build up three baseline systems, namely PB M OSES (phrase-based), Hiero M OSES (hierarchical) and C DEC (Dyer et al., 2010). The motivation of choosing Hierarchical Models is to address the German-English’s long reorder problem. We want to test the performance of C DEC and Hiero M OSES and choose the best. PB M OSES is used as our benchmark. The three results obtained on the development and test sets for the three baseline system and the system combination are shown in the Table 6. PB M OSES Hiero M OSES C DEC Combination System Combination Test 24.0 24.4 24.4 24.8 Table 6: BLEU scores obtained by our systems on the development and test sets for the German to English translation task. From the Table 6 we can see t"
W13-2227,W13-2803,1,0.864471,"Missing"
W13-2227,D11-1020,1,0.829215,"Comb. 30.0 30.8 29.8 58.9 29.9 30.4 29.3 59.3 29.7 29.6 28.7 61.8 29.6 29.4 28.5 62.0 4 German-English In this section we describe our work on German to English subtask. Firstly we describe the Dependency tree to string method which we tried but unfortunately failed due to short of time. Secondly we discuss the baseline system and the preprocessing we performed. Thirdly a system combination method is described. 4.1 Dependency Tree to String Method Our original plan was to address the long distance reordering problem in German-English translation. We use Xie’s Dependency tree to string method(Xie et al., 2011) which obtains good results on Chinese to English translation and exhibits good performance at long distance reordering as our decoder. We use Stanford dependency parser4 to parse the English side of the data and Mate-Tool5 for the German side. The first set of experiments did not lead to encouraging results and due to insufficient time, we decide to switch to other decoders, based on statistical phrase-based and hierarchical approaches. Table 5: BLEU and TER scores obtained by our systems. BLEUdev is the score obtained on the development set given by MERT, while BLEU, BLEUcased and TER are ob"
W13-2227,W08-0509,0,0.0846195,"Missing"
W13-2227,D08-1011,0,0.0262708,"News Crawl (20072012, each year individually, 6 separate parts), then we interpolate a new LM with this interpolated LM and LDC Gigawords (we kept the Gigaword subsets separated according to the news sources as distributed by LDC, which leads to 7 corpus). We also use a word-level combination strategy (Rosti et al., 2007) to combine the three translation hypotheses. To combine these systems, we first use the Minimum Bayes-Risk (MBR) (Kumar and Byrne, 2004) decoder to obtain the 5 best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al., 2000). We then use IHMM (He et al., 2008) to choose the backbone build the CN and finally search for and generate the best translation. We tune the system parameters on development set with Simple-Simplex algorithm. The parameters for system weights are set equal. Other parameters like language model, length penalty and combination coefficient are chosen when we see a good improvement on development set. 5 Development 22.0 22.1 22.5 23.0 Conclusion This paper presented a set of experiments conducted on Spanish-English, French-English and German-English language pairs. For the SpanishEnglish pair, we have explored the use of linguisti"
W13-2227,P07-2045,0,\N,Missing
W13-2227,N04-1022,0,\N,Missing
W13-2803,P10-2041,0,0.125006,"e linguistic methods we replace the sentences selected (which contain lemmas and/or named entities) with the corresponding sentences in the original corpus (containing only word forms). 4 http://www.statmt.org/wmt13/ translation-task.html 5 http://commoncrawl.org/ n 1 2 3 4 5 f 65076 981077 2624800 3633724 3929751 fn 48945 847720 2382629 3412719 3780064 l 59619 835825 2447759 3523888 3856917 ln 36326 702118 2212709 3325311 3749813 9 (around 2 million sentences both for CC and EU, and around 11 million for UN). This can be considered as a contribution of this paper since previous works such as Moore and Lewis (2010) and, more recently, Axelrod et al. (2011) test the Moore-Lewis method on only one non-domainspecific corpus: LDC Gigaword and an unpublished general-domain corpus, respectively. All the LMs are built with IRSTLM 5.80.01 (Federico et al., 2008), use up to 5-grams and are smoothed using a simplified version of the improved Kneser-Ney method (Chen and Goodman, 1996). For lemmatisation and named entity recognition we use Freeling 3.0 (Padr´o and Stanilovsky, 2012). The corpora are tokenised and truecased using scripts from the Moses toolkit (Koehn et al., 2007). 1500 Perplexity 1400 1100 1000 64"
W13-2803,D11-1033,0,0.238839,"es selected (which contain lemmas and/or named entities) with the corresponding sentences in the original corpus (containing only word forms). 4 http://www.statmt.org/wmt13/ translation-task.html 5 http://commoncrawl.org/ n 1 2 3 4 5 f 65076 981077 2624800 3633724 3929751 fn 48945 847720 2382629 3412719 3780064 l 59619 835825 2447759 3523888 3856917 ln 36326 702118 2212709 3325311 3749813 9 (around 2 million sentences both for CC and EU, and around 11 million for UN). This can be considered as a contribution of this paper since previous works such as Moore and Lewis (2010) and, more recently, Axelrod et al. (2011) test the Moore-Lewis method on only one non-domainspecific corpus: LDC Gigaword and an unpublished general-domain corpus, respectively. All the LMs are built with IRSTLM 5.80.01 (Federico et al., 2008), use up to 5-grams and are smoothed using a simplified version of the improved Kneser-Ney method (Chen and Goodman, 1996). For lemmatisation and named entity recognition we use Freeling 3.0 (Padr´o and Stanilovsky, 2012). The corpora are tokenised and truecased using scripts from the Moses toolkit (Koehn et al., 2007). 1500 Perplexity 1400 1100 1000 64 Perplexity 700 650 600 2 1 Size 1/x Figure"
W13-2803,padro-stanilovsky-2012-freeling,0,0.0483412,"Missing"
W13-2803,P96-1041,0,0.240407,"35825 2447759 3523888 3856917 ln 36326 702118 2212709 3325311 3749813 9 (around 2 million sentences both for CC and EU, and around 11 million for UN). This can be considered as a contribution of this paper since previous works such as Moore and Lewis (2010) and, more recently, Axelrod et al. (2011) test the Moore-Lewis method on only one non-domainspecific corpus: LDC Gigaword and an unpublished general-domain corpus, respectively. All the LMs are built with IRSTLM 5.80.01 (Federico et al., 2008), use up to 5-grams and are smoothed using a simplified version of the improved Kneser-Ney method (Chen and Goodman, 1996). For lemmatisation and named entity recognition we use Freeling 3.0 (Padr´o and Stanilovsky, 2012). The corpora are tokenised and truecased using scripts from the Moses toolkit (Koehn et al., 2007). 1500 Perplexity 1400 1100 1000 64 Perplexity 700 650 600 2 1 Size 1/x Figure 1: Results of the different methods on CC In all the figures, the results are very similar regardless of the use of lemmas. The use of named entities, however, produces substantially different results. The models that do not use named entity categories obtain the best results for lower thresholds (up to 1/32 for CC, and u"
W13-2803,2012.iwslt-evaluation.6,0,0.0413415,"Missing"
W13-2803,eisele-chen-2010-multiun,0,0.0365629,"Missing"
W13-2803,P07-2045,0,0.00427842,"per since previous works such as Moore and Lewis (2010) and, more recently, Axelrod et al. (2011) test the Moore-Lewis method on only one non-domainspecific corpus: LDC Gigaword and an unpublished general-domain corpus, respectively. All the LMs are built with IRSTLM 5.80.01 (Federico et al., 2008), use up to 5-grams and are smoothed using a simplified version of the improved Kneser-Ney method (Chen and Goodman, 1996). For lemmatisation and named entity recognition we use Freeling 3.0 (Padr´o and Stanilovsky, 2012). The corpora are tokenised and truecased using scripts from the Moses toolkit (Koehn et al., 2007). 1500 Perplexity 1400 1100 1000 64 Perplexity 700 650 600 2 1 Size 1/x Figure 1: Results of the different methods on CC In all the figures, the results are very similar regardless of the use of lemmas. The use of named entities, however, produces substantially different results. The models that do not use named entity categories obtain the best results for lower thresholds (up to 1/32 for CC, and up to 1/16 both for 6 An additional threshold, tions corpus 1 , 128 1300 64 32 16 8 4 2 1 Figure 3: Results of the different methods on UN 750 4 f fn l ln Size 1/x f fn l ln 8 1 1500 900 128 950 16 2"
W13-2803,2005.mtsummit-papers.11,0,0.0328982,"Missing"
W13-2803,2011.iwslt-papers.5,0,\N,Missing
W14-3319,D11-1033,0,0.199736,"nt sets, to obtain our final SMT system. Our submission for the English to French translation task was ranked second amongst nine teams and a total of twenty submissions. 1 To train the LMs, monolingual corpora and the target side of the parallel corpora are first used individually to train models. Then the individual models are interpolated according to perplexity minimisation on the development sets. To train the TMs, first a baseline is built using the News Commentary parallel corpus. Second, each remaining parallel corpus is processed individually using bilingual cross-entropy difference (Axelrod et al., 2011) in order to separate pseudo in-domain and out-of-domain sentence pairs, and filtering the pseudo out-ofdomain instances with the vocabulary saturation approach (Lewis and Eetemadi, 2013). Third, synthetic translation rules are automatically extracted from the development set and used to train another translation model following a novel approach (S´anchez-Cartagena et al., 2014). Finally, we interpolate the four translation models (baseline, in-domain, filtered out-of-domain and rules) by minimising the perplexity obtained on the development sets and investigate the best tuning and decoding pa"
W14-3319,P11-1105,0,0.180288,"encodes a different degree of generalisation over the particular example it has been extracted from. Finally, the minimum set of rules which correctly reproduces all the bilingual phrases is found based on integer linear programming search (Garfinkel and Nemhauser, 1972). Once the rules have been inferred, the phrase table is built from them and the original rulebased MT dictionaries, following the method by S´anchez-Cartagena et al. (2011), which was one of winning systems4 (together with two online SMT systems) in the pairwise manual evaluation of the WMT11 English–Spanish translation task (Callison-Burch et al., 2011). This phrasetable is then interpolated with the baseline TM and the results are presented in Table 5. A slight improvement over the baseline is observed, which motivates the use of synthetic rules in our final MT system. This small improvement may be related to the small coverage of the Apertium dictionaries: the English–French bilingual dictionary has a low number of entries compared to more mature language pairs in Apertium which have around 20 times more bilingual entries. System the number of n-bests used by MERT. Results obtained on the development set newstest2013 are reported in Table"
W14-3319,W13-2212,0,0.0160957,"except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over sequences of operations, implemented in M OSES Sentences (k) Words (M) Monolingual Data – English Europarl v7 2,218.2 News Commentary v8 304.2 News Shuffled 2007 3,782.5 News Shuffled 2008 12,954.5 News Shuffled 2009 14,680.0 News Shuffled 2010 6,797.2 News Shuffled 2011 15,437.7 News Shuffled 2012 14,869.7 News Shuffled 2013 21,688.4 LDC afp 7,184.9 LDC apw 8,829.4 LDC cna 618.4 LDC ltw 986.9 LDC nyt 5,327.7 LDC wpb 108.8 LDC xin 5,121.9 59.9 7.4 90.2 308.1 347.0 157.8 358.1 345.5"
W14-3319,W08-0509,0,0.170818,"test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use K EN LM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use M OSES (Koehn et al., 2007) version 2.1 with MGIZA ++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we ma"
W14-3319,P13-2121,0,0.0606903,"Missing"
W14-3319,P07-2045,0,0.0136523,"with the focus on the English to French direction. Language models (LMs) and translation 171 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 171–177, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics 2 Datasets and Tools Corpus We use all the monolingual and parallel datasets in English and French provided by the shared task organisers, as well as the LDC Gigaword for the same languages1 . For each language, a true-case model is trained using all the data, using the traintruecaser.perl script included in the M OSES toolkit (Koehn et al., 2007). Punctuation marks of all the monolingual and parallel corpora are then normalised using the script normalize-punctuation.perl provided by the organisers, before being tokenised and true-cased using the scripts distributed with the M OSES toolkit. The same pre-processing steps are applied to the development and test sets. As development sets, we used all the test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and wit"
W14-3319,W13-2235,0,0.294214,"he LMs, monolingual corpora and the target side of the parallel corpora are first used individually to train models. Then the individual models are interpolated according to perplexity minimisation on the development sets. To train the TMs, first a baseline is built using the News Commentary parallel corpus. Second, each remaining parallel corpus is processed individually using bilingual cross-entropy difference (Axelrod et al., 2011) in order to separate pseudo in-domain and out-of-domain sentence pairs, and filtering the pseudo out-ofdomain instances with the vocabulary saturation approach (Lewis and Eetemadi, 2013). Third, synthetic translation rules are automatically extracted from the development set and used to train another translation model following a novel approach (S´anchez-Cartagena et al., 2014). Finally, we interpolate the four translation models (baseline, in-domain, filtered out-of-domain and rules) by minimising the perplexity obtained on the development sets and investigate the best tuning and decoding parameters. Introduction The reminder of this paper is organised as follows: the datasets and tools used in our experiments are described in Section 2. Then, details about the LMs and TMs a"
W14-3319,P10-2041,0,0.0671112,"ces and (ii) reduce the total amount of out-of-domain data. Second, a novel approach for the automatic extraction of translation rules and their use to enrich the phrase table is detailed. 10 Parallel Data Filtering and Vocabulary Saturation Bilingual Cross-Entropy Difference 4.1 Amongst the parallel corpora provided by the shared task organisers, only News Commentary can be considered as in-domain regarding the development and test sets. We use this training corpus to build our baseline SMT system. The other parallel corpora are individually filtered using bilingual cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011). This data filtering method relies on four LMs, two in the source and two in the target language, which aim to model particular features of in and out-ofdomain sentences. We build the in-domain LMs using the source and target sides of the News Commentary parallel corpus. Out-of-domain LMs are trained on a vocabulary-constrained subset of each remaining parallel corpus individually using the SRILM toolkit, which leads to eight models (four in the source language and four in the target language).3 8 6 4 2 0 Common Crawl Europarl 10^9 UN -2 -4 0 2k 4k 6k Sentence Pairs 8k"
W14-3319,J03-1002,0,0.01001,"ts, we used all the test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use K EN LM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use M OSES (Koehn et al., 2007) version 2.1 with MGIZA ++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich"
W14-3319,P03-1021,0,0.0469427,"rter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use K EN LM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use M OSES (Koehn et al., 2007) version 2.1 with MGIZA ++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over se"
W14-3319,P02-1040,0,0.0911366,"d the in-domain LMs using the source and target sides of the News Commentary parallel corpus. Out-of-domain LMs are trained on a vocabulary-constrained subset of each remaining parallel corpus individually using the SRILM toolkit, which leads to eight models (four in the source language and four in the target language).3 8 6 4 2 0 Common Crawl Europarl 10^9 UN -2 -4 0 2k 4k 6k Sentence Pairs 8k 10k Figure 1: Sample of ranked sentence-pairs (10k) of each of the out-of-domain parallel corpora with bilingual cross-entropy difference The results obtained using the pseudo indomain data show B LEU (Papineni et al., 2002) scores superior or equal to the baseline score. Only the Europarl subset is slightly lower than the baseline, while the subset taken from the 109 corpus reaches the highest B LEU compared to the other systems (30.29). This is mainly due to the 3 The subsets contain the same number of sentences and the same vocabulary as News Commentary. 173 size of this subset which is ten times larger than the one taken from Europarl. The last row of Table 3 shows the B LEU score obtained after interpolating the four pseudo in-domain translation models. This system outperforms the best pseudo indomain one by"
W14-3319,2011.mtsummit-papers.64,1,0.889124,"Missing"
W14-3319,E12-1055,0,0.0325549,"Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over sequences of operations, implemented in M OSES Sentences (k) Words (M) Monolingual Data – English Europarl v7 2,218.2 News Commentary v8 304.2 News Shuffled 2007 3,782.5 News Shuffled 2008 12,954.5 News Shuffled 2009 14,680.0 News Shuffled 2010 6,797.2 News Shuffled 2011 15,437.7 News Shuffled 2012 14,869.7 News Shuffled 2013 21,688.4 LDC afp 7,184.9 LDC apw 8,829.4 LDC cna 618.4 L"
W14-3319,2006.amta-papers.25,0,0.0323304,"r detruecasing and de-tokenising using the scripts distributed with M OSES. This setup allowed us to compare our results with the participants of the translation shared task last year. We pick the decoding parameters leading to the best results in terms of B LEU and decode the official test set of WMT14 newstest2014. The results are reported in Table 7. Results on newstest2013 show that the decoding parameters investigation leads to an overall improvement of 0.1 B LEU absolute. The results on newstest2014 show that adding synthetic rules did not help improving B LEU and degraded slightly TER (Snover et al., 2006) scores. In addition to our English→French submission, we submitted a French→English translation. Our French→English MT system is built on the alignments obtained from the English→French direction. The training processes between the two sys27.76 28.06 Table 5: BLEU scores reported by MERT on English–French newstest2013 for the baseline SMT system standalone and with automatically extracted translation rules. 5 27.76 31.93 31.90 32.21 32.10 Table 6: B LEU scores reported by MERT on English–French newstest2013 development set. BLEUdev Baseline Baseline+Rules BLEUdev Tuning and Decoding We presen"
W14-3319,D07-1080,0,0.0658355,"experiments. Adding the synthetic translation rules degrades B LEU (as indicated by the last row in the Table), thus we decide to submit two systems to the shared task: one without and one with synthetic rules. By submitting a system without synthetic rules, we also ensure that our SMT system is constrained according to the shared task guidelines. System Baseline + pseudo in + pseudo out + OSM + MERT 200-best + Rules As MERT is not suitable when a large number of features are used (our system uses 19 fetures), we switch to the Margin Infused Relaxed Algorithm (MIRA) for our submitted systems (Watanabe et al., 2007). The development set used is newstest2012, as we aim to select the best decoding parameters according to the scores obtained when decoding the newstest2013 corpus, after detruecasing and de-tokenising using the scripts distributed with M OSES. This setup allowed us to compare our results with the participants of the translation shared task last year. We pick the decoding parameters leading to the best results in terms of B LEU and decode the official test set of WMT14 newstest2014. The results are reported in Table 7. Results on newstest2013 show that the decoding parameters investigation lea"
W14-3319,W14-3320,1,\N,Missing
W15-0714,C88-1016,0,0.18011,"this paper we explore the feasibility of applying the current state-of-the-art MT technology to literary texts, what might be considered to be the last bastion of human translation. The perceived wisdom is that MT is of no use for the translation of literature. We challenge that view, despite the fact that – to the best of our knowledge – the applicability of MT to literature has to date been only partially studied from an empirical point of view. Introduction The field of Machine Translation (MT) has evolved very rapidly since the emergence of statistical approaches almost three decades ago (Brown et al., 1988; Brown et al., 1990). MT is nowadays a growing reality throughout the industry, which continues to adopt this technology as it results in demonstrable improvements in translation productivity, at least In this paper we aim to measure the translatability of literary text. Our empirical methodology relies on the fact that the applicability of MT to a given type of text can be assessed by analysing parallel corpora of that particular type and measuring (i) the degree of freedom of the translations (how literal the translations are), and (ii) the narrowness of the domain (how specific or general"
W15-0714,J90-2002,0,0.813076,"e the feasibility of applying the current state-of-the-art MT technology to literary texts, what might be considered to be the last bastion of human translation. The perceived wisdom is that MT is of no use for the translation of literature. We challenge that view, despite the fact that – to the best of our knowledge – the applicability of MT to literature has to date been only partially studied from an empirical point of view. Introduction The field of Machine Translation (MT) has evolved very rapidly since the emergence of statistical approaches almost three decades ago (Brown et al., 1988; Brown et al., 1990). MT is nowadays a growing reality throughout the industry, which continues to adopt this technology as it results in demonstrable improvements in translation productivity, at least In this paper we aim to measure the translatability of literary text. Our empirical methodology relies on the fact that the applicability of MT to a given type of text can be assessed by analysing parallel corpora of that particular type and measuring (i) the degree of freedom of the translations (how literal the translations are), and (ii) the narrowness of the domain (how specific or general that text is). Hence,"
W15-0714,P96-1041,0,0.179931,"apertium/files/apertium-es-ca/1.2.1/ 10 http://sourceforge.net/projects/ apertium/files/apertium-en-es/0.8.0/ 11 Manual evaluation for English, French and Greek concluded that 0.4 was an adequate threshold for Hunalign’s confidence score (Pecina et al., 2012). Narrowness of the Domain As previously mentioned, we use LM perplexity as a proxy to measure the narrowness of the domain. We take two random samples without replacement for the Spanish side of each dataset, to be used for training (200,000 tokens) and testing (20,000 tokens). We train an LM of order 3 and improved Kneser-Ney smoothing (Chen and Goodman, 1996) with IRSTLM (Federico et al., 2008). For each LM we report the perplexity on the testset built from the same dataset in Figure 1. The two novels considered (perplexities in the range [230.61, 254.49]) fall somewhere between news ([359.73, 560.62]) and technical domain ([127.30, 228.38]). Our intuition is that novels cover a narrow domain, like technical texts, but the vocabulary and language used in novels is richer, thus leading to higher perplexity than technical texts. News, on the contrary, covers a large variety of topics. Hence, despite novels possibly using more complex linguistic cons"
W15-0714,W14-3348,0,0.132007,"Missing"
W15-0714,D10-1016,0,0.650978,"Missing"
W15-0714,E14-1047,0,0.0425261,"Missing"
W15-0714,D10-1051,0,0.583547,"Missing"
W15-0714,C08-1048,0,0.0175521,"d the author concludes that such a workflow can be a useful low-cost alternative for translating literary works, albeit at the expense of sacrificing translation quality. According to the opinion of a professional translator, the main errors had to do with using English syntactic structures and expressions instead of their French equivalents and not taking into account certain cultural references. Finally, there are some works that use MT techniques in literary text, but for generation rather than for translation. He et al. (2012) used SMT to generate poems in Chinese given a set of keywords. Jiang and Zhou (2008) used SMT to generate the second line of Chinese couplets given the first line. In a similar fashion, Wu et al. (2013) used transduction grammars to generate rhyming responses in hip-hop given the original challenges. This paper contributes to the current state-of-theart in two dimensions. On the one hand, we conduct a comparative analysis on the translatability of literary text according to narrowness of the domain and freedom of translation. This can be seen as a more general and complementary analysis to the one conducted by Voigt and Jurafsky (2012). On the other hand, and related to Besac"
W15-0714,W04-3250,0,0.286519,"Missing"
W15-0714,2005.mtsummit-papers.11,0,0.053497,"to as news1) for Spanish–Catalan, and newscommentary v84 (referred to as news2) for Spanish– English. For technical documentation we use four datasets: DOGC,5 a corpus from the official journal of the Catalan Goverment, for Spanish–Catalan; EMEA,6 a corpus from the European Medicines Agency, for Spanish–English; JRC-Acquis (henceforth referred as JRC) (Steinberger et al., 2006), made of legislative text of the European Union, for Spanish– English; and KDE4,7 a corpus of localisation files of the KDE desktop environment, for the two language pairs. Finally, we consider the Europarl corpus v7 (Koehn, 2005), given it is widely used in the MT community, for Spanish–English. All the datasets are pre-processed as follows. First they are tokenised and truecased with Moses’ (Koehn et al., 2007) scripts. Truecasing is carried out with a model trained on the caWaC corpus for Catalan (Ljubeˇsi´c and Toral, 2014) and News Crawl 20128 both for English and Spanish. Parallel datasets not available in a sentence-split format (novel1 and novel2) are sentence-split using Freeling (Padr´o and Stanilovsky, 2012). All parallel datasets are then sentence aligned. We use Hunalign (Varga et al., 2005) and keep only"
W15-0714,ljubesic-toral-2014-cawac,1,0.882326,"Missing"
W15-0714,J03-1002,0,0.00673519,"Equation 1, as a proxy to measure the degree of translation freedom. Word alignment perplexity gives an indication of how well the model fits the data. log2 P P = − X s log2 p(es |fs ) (1) The assumption is that the freer the translations are for a given parallel corpus, the higher the perplexity of the word alignment model learnt from such dataset, as the word alignment algorithms would have more difficulty to find suitable alignments. For each parallel dataset, we randomly select a set of sentence pairs whose overall size accounts for 500,000 tokens. We then run word alignment with GIZA++ (Och and Ney, 2003) in both directions, with the default parameters used in Moses. For each dataset and language pair, we report in Figure 2 the perplexity of the word alignment after the last iteration for each direction. The most important discriminating variable appears to be the level of relatedness of the languages involved, i.e. all the perplexities for Spanish–Catalan are below 10 while all the perplexities for Spanish–English are well above this number. 50 45 40 Perplexity 35 30 es-ca ca-es es-en en-es 25 20 15 10 5 0 novel1 novel2 news1 news2 DOGC Acquis KDE4 EMEA ep Dataset Figure 2: Word alignment per"
W15-0714,P03-1021,0,0.0109664,"domain-adapted systems. The domain adaptation is carried out by using two previous novels from the same author that were translated by the same translator (cf. the dataset novel1 in Section 3.1). We explore their use for tuning (+inDev), LM (concatenated +inLM and interpolated +IinLM) and TM (concatenated +inTM and interpolated +IinTM). The testset is made of a set of randomly selected sentence pairs from The Prisoner of Heaven. Table 1 provides an overview of the datasets used for MT. We train phrase-based SMT systems with Moses v2.1 using default parameters. Tuning is carried out with MERT (Och, 2003). LMs are linearly interpolated with SRILM (Stolcke et al., 2011) by means of perplexity minimisation on the development set from the novel1 dataset. Similarly, TMs are linearly interpolated, also by means of perplexity minimisation (Sennrich, 2012). 4.1 Automatic Evaluation Our systems are evaluated with a set of state-ofthe-art automatic metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR 1.5 (Denkowski and Lavie, 2014). Table 2 shows the results obtained by each of the systems built. For each domain-adapted system System baseline +inDev +inDev+inLM +inDev+IinLM +inDe"
W15-0714,padro-stanilovsky-2012-freeling,0,0.0885312,"Missing"
W15-0714,P02-1040,0,0.0933569,"Missing"
W15-0714,2012.eamt-1.38,1,0.904327,"Missing"
W15-0714,2014.eamt-1.39,0,0.0137405,"predictable domains such as news (cf. WMT translation task series).2 We propose to study the applicability of SMT to literary text by comparing the degree of freedom and narrowness of parallel corpora for literature to other domains widely studied in the area of MT (technical documentation and news). Such a corpus study can be carried out by using a set of automatic measures. The perplexity of the word alignment can be used as a proxy to measure the degree of freedom of the translation. The narrowness of the domain can be assessed by measuring perplexity with respect to a language model (LM) (Ruiz and Federico, 2014). Therefore, in order to assess the translatability of literary text with MT, we contextualise the problem by comparing it to the translatability of other widely studied types of text. Instead of considering the 2 http://www.statmt.org/wmt14/ translation-task.html translatability of literature as a whole, we root the study along two axes: • Relatedness of the language pair: from pairs of languages that belong to the same family (e.g. Romance languages), through languages that belong to the same group (e.g. Romance and Germanic languages of the Indo-European group) to unrelated languages (e.g."
W15-0714,E12-1055,0,0.042822,"Missing"
W15-0714,2006.amta-papers.25,0,0.213719,"Missing"
W15-0714,W12-2503,0,0.293951,"real terms. Their system was trained and evaluated with WMT-09 data1 for French–English. Greene et al. (2010) also translated poetry, choosing target realisations that conform to the desired rhythmic patterns. Specifically, they translated Dante’s Divine Comedy from Italian sonnets into English iambic pentameter. Instead of constraining the SMT system, they passed its output lattice through a FST that maps words to sequences of stressed and unstressed syllables. These sequences are finally filtered with a iambic pentameter acceptor. Their output translations are evaluated qualitatively only. Voigt and Jurafsky (2012) examined how referential cohesion is expressed in literary and nonliterary texts, and how this cohesion affects trans1 http://www.statmt.org/wmt09/ translation-task.html 124 lation. They found that literary texts have more dense reference chains and conclude that incorporating discourse features beyond the level of the sentence is an important direction for applying MT to literary texts. Jones and Irvine (2013) used existing MT systems to translate samples of French literature (prose and poetry) into English. They then used qualitative analysis grounded in translation theory on the MT output"
W15-0714,2013.mtsummit-papers.14,0,0.0183302,"the expense of sacrificing translation quality. According to the opinion of a professional translator, the main errors had to do with using English syntactic structures and expressions instead of their French equivalents and not taking into account certain cultural references. Finally, there are some works that use MT techniques in literary text, but for generation rather than for translation. He et al. (2012) used SMT to generate poems in Chinese given a set of keywords. Jiang and Zhou (2008) used SMT to generate the second line of Chinese couplets given the first line. In a similar fashion, Wu et al. (2013) used transduction grammars to generate rhyming responses in hip-hop given the original challenges. This paper contributes to the current state-of-theart in two dimensions. On the one hand, we conduct a comparative analysis on the translatability of literary text according to narrowness of the domain and freedom of translation. This can be seen as a more general and complementary analysis to the one conducted by Voigt and Jurafsky (2012). On the other hand, and related to Besacier (2014), we evaluate MT output for literary text. There are two differences though; first, they translated a short"
W15-0714,2012.amta-wptp.10,0,0.0567945,"Missing"
W15-0714,steinberger-etal-2006-jrc,0,\N,Missing
W15-0714,P07-2045,0,\N,Missing
W15-3022,D11-1033,0,0.0118991,"used to train the translation models, after pre-processing. lation models are presented in Table 1. Figure 1 shows how different segmentation methods affect the vocabulary size; given that linguistic segmentation have larger vocabularies as statistical their contribution to translation models may be at least partially complementary. The two unconstrained parallel datasets are split into three subsets: pseudo in-domain, pseudo outof-domain top and pseudo out-of-domain bottom, henceforth in, outt and outb. We rank the sentence pairs according to bilingual cross-entropy difference on the devset (Axelrod et al., 2011) and calculate the perplexity on the devset of LMs trained on different portions of the top ranked sentences (the top 1/64, 1/32 and so on). The subset for which we obtain the lowest perplexities is kept as in (this was 1/4 for fienwac (403.89 and 3610.95 for English and Finnish, respectively), and 1/16 for osubs (702.45 and 7032.2). The remaining part of each dataset is split in two sequential parts in ranking order of same number of lines, which are kept as outt and outb. The out-of-domain part of osubs is further processed with vocabulary saturation (Lewis and Eetemadi, 2013) in order to ha"
W15-3022,P05-1033,0,0.0336499,"ipt omorfi-factorise.py http://opus.lingfil.uu.se/ 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 words flatcat hfst comp news shuffled 2014 hfst morph Corpora morfessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same parallel data for each model, as well as the provided development set to tune the systems. The phrase-based SMT system is augmented with additional features: an Ope"
W15-3022,P14-1129,0,0.147789,"Missing"
W15-3022,P11-1105,0,0.0620347,"d model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same parallel data for each model, as well as the provided development set to tune the systems. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014), both trained on the parallel data used to learn the phrase-table. All the translation systems also benefit from two additional reordering models, namely a phrase-based model with three different orientations (monotone, swap and discontinuous) and a hierarchical model with four orientations (non merged discontinuous left and right orientations), both trained in a bidirectional way (Koehn et al., 2005; Galley and Manning, 2008). Our constrained systems are trained on the data available for the shared task, while unconstrained"
W15-3022,espla-gomis-etal-2014-comparing,1,0.815616,"Missing"
W15-3022,D08-1089,0,0.0261127,"s. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014), both trained on the parallel data used to learn the phrase-table. All the translation systems also benefit from two additional reordering models, namely a phrase-based model with three different orientations (monotone, swap and discontinuous) and a hierarchical model with four orientations (non merged discontinuous left and right orientations), both trained in a bidirectional way (Koehn et al., 2005; Galley and Manning, 2008). Our constrained systems are trained on the data available for the shared task, while unconstrained systems are trained with two additional sets of parallel data, the F I E N WAC crawled dataset (cf. Section 2.2) and Open Subtitles, henceforth OSUBS.12 The details about the corpora used to train the trans11 Unique Tokens (M) language and translation models. We rely on the scripts included in the M OSES toolkit (Koehn et al., 2007) and perform the following operations: punctuation normalisation, tokenisation, true-casing and escaping of problematic characters. The truecaser is lexicon-based, t"
W15-3022,W08-0509,0,0.135098,"fst comp news shuffled 2014 hfst morph Corpora morfessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same parallel data for each model, as well as the provided development set to tune the systems. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014"
W15-3022,D07-1091,0,0.013807,"ters of the word alignment, phrase extraction and decoding algorithms have not been modified to take into account the nature of the segmented data. 4.1 12 using the script omorfi-factorise.py http://opus.lingfil.uu.se/ 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 words flatcat hfst comp news shuffled 2014 hfst morph Corpora morfessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same paral"
W15-3022,2005.iwslt-1.8,0,0.121773,"t to tune the systems. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014), both trained on the parallel data used to learn the phrase-table. All the translation systems also benefit from two additional reordering models, namely a phrase-based model with three different orientations (monotone, swap and discontinuous) and a hierarchical model with four orientations (non merged discontinuous left and right orientations), both trained in a bidirectional way (Koehn et al., 2005; Galley and Manning, 2008). Our constrained systems are trained on the data available for the shared task, while unconstrained systems are trained with two additional sets of parallel data, the F I E N WAC crawled dataset (cf. Section 2.2) and Open Subtitles, henceforth OSUBS.12 The details about the corpora used to train the trans11 Unique Tokens (M) language and translation models. We rely on the scripts included in the M OSES toolkit (Koehn et al., 2007) and perform the following operations: punctuation normalisation, tokenisation, true-casing and escaping of problematic characters. The tr"
W15-3022,P07-2045,0,0.00899697,"d a hierarchical model with four orientations (non merged discontinuous left and right orientations), both trained in a bidirectional way (Koehn et al., 2005; Galley and Manning, 2008). Our constrained systems are trained on the data available for the shared task, while unconstrained systems are trained with two additional sets of parallel data, the F I E N WAC crawled dataset (cf. Section 2.2) and Open Subtitles, henceforth OSUBS.12 The details about the corpora used to train the trans11 Unique Tokens (M) language and translation models. We rely on the scripts included in the M OSES toolkit (Koehn et al., 2007) and perform the following operations: punctuation normalisation, tokenisation, true-casing and escaping of problematic characters. The truecaser is lexicon-based, trained on all the monolingual and parallel data. In addition, we remove sentence pairs from the parallel corpora where either side is longer than 80 tokens. Corpus Europarl v8 fienwac.in fienwac.outt fienwac.outb osubs.in osubs.outt osubs.outb Sentences (k) Words (M) Finnish English Constrained System 1,901.1 36.5 Unconstrained System 640.1 9.2 838.9 12.5 838.9 13.9 492.2 3.6 1,169.6 8.8 1,169.6 7.8 50.9 13.6 18.1 18.1 5.6 14.4 13."
W15-3022,2005.mtsummit-papers.11,0,0.134429,"ne translation (SMT) systems submitted by the Abu-MaTran project for the WMT 2015 translation task. The language pair concerned is Finnish–English with a strong focus on the English-to-Finnish direction. The Finnish language is newly introduced this year as a particular translation challenge due to its rich morphology and to the lack of resources available, compared to e.g. English or French. Morphologically rich languages, and especially Finnish, are known to be difficult to translate using phrase-based SMT systems mainly because of the large diversity of word forms leading to data scarcity (Koehn, 2005). We assume that data acquisition and morphological segmentation should contribute to decrease the out-of-vocabulary rate and thus improve the performance of SMT. To gather additional data, we decide to build on previous work conducted in the Abu-MaTran project and crawl the Web looking for monolingual and parallel corpora (Toral et al., 2014). In addition, morphological segmentation of Finnish is used in our systems as pre- and post-processing steps. Four segmentation methods are proposed in this paper, two unsupervised and two rule-based. Both constrained and unconstrained translation system"
W15-3022,W13-2235,0,0.0133834,"nce on the devset (Axelrod et al., 2011) and calculate the perplexity on the devset of LMs trained on different portions of the top ranked sentences (the top 1/64, 1/32 and so on). The subset for which we obtain the lowest perplexities is kept as in (this was 1/4 for fienwac (403.89 and 3610.95 for English and Finnish, respectively), and 1/16 for osubs (702.45 and 7032.2). The remaining part of each dataset is split in two sequential parts in ranking order of same number of lines, which are kept as outt and outb. The out-of-domain part of osubs is further processed with vocabulary saturation (Lewis and Eetemadi, 2013) in order to have a more efficient and compact system (Rubino et al., 2014). We traverse the sentence pairs in the order they are ranked 187 Corpus Europarl v8 News Commentary v10 News Shuffled 2007 2008 2009 2010 2011 2012 2013 2014 Gigaword 5th Sentences (k) Words (M) 2,218.2 344.9 59.9 8.6 3 782.5 12 954.5 14 680.0 6 797.2 15 437.7 14 869.7 21 688.4 28 221.3 28,178.1 90.2 308.1 347.0 157.8 358.1 345.5 495.2 636.6 4,831.5 Corpus Constrained System News Shuffle 2014 1,378.8 Unconstrained System FiWaC 146,557.4 System and filter out those for which we have seen already each 1-gram at least 10"
W15-3022,W14-0405,1,0.869044,"Missing"
W15-3022,J04-4002,0,0.0271016,"n form of automake scriptlets at http:// github.com/flammie/autostuff-moses-smt/. 10 The parameters of the word alignment, phrase extraction and decoding algorithms have not been modified to take into account the nature of the segmented data. 4.1 12 using the script omorfi-factorise.py http://opus.lingfil.uu.se/ 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 words flatcat hfst comp news shuffled 2014 hfst morph Corpora morfessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Ha"
W15-3022,C14-1111,0,0.168889,"Missing"
W15-3022,W13-2506,1,0.831045,"precisionoriented. In this first step, a large amount of potentially parallel data is obtained by post-processing data collected with a TLD crawl, which is not primarily aimed at finding parallel data. To make use of this resource in a more efficient way, we re-crawl some of the most promising web sites (we call them multilingual hotspots) with the ILSP-FC crawler specialised in locating parallel documents during crawling. According to Espl`a-Gomis et al. (2014), B ITEXTOR and ILSP-FC have shown to be complementary, and combining both tools leads to a larger amount of parallel data. ILSP-FC (Papavassiliou et al., 2013) is a modular crawling system allowing to easily acquire domain-specific and generic corpora from the Web.5 This crawler includes a de-duplicator which checks all documents in a pairwise manner to identify near-duplicates. This is achieved by comparing the quantised word frequencies and the paragraphs of each pair of candidate duplicate documents. A document-pair detector also examines each document in the same manner and identifies pairs of documents that could be considered parallel. The main methods used by the pair detector are URL similarity, co-occurrences of images with the same filenam"
W15-3022,P02-1040,0,0.096443,"47 0.830 0.828 0.819 0.864 0.849 Combination 14.61 0.786 13.54 0.801 Table 4: Results obtained on the development and test sets for the constrained English-to-Finnish translation task. Best individual system in bold. 2010) using default settings, except for the beam size (set to 1, 500) and radius (5 for Finnish and 7 for English), following empirical results obtained on the development set. 5.1 Constrained Results Individual systems trained on the provided data are evaluated before being combined. The results obtained for the English-to-Finnish direction are presented in Table 4.13 The BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores obtained by the system trained on compoundsegmented data (HFST Comp) show a positive impact of this method on SMT according to the development set, compared to the other individual systems. The unsupervised segmentation methods do not improve over phrase-based SMT, while the hierarchical model shows an interesting reduction of the TER score compared to a classic phrasebased approach. On the test set, the use of inflectional morph segments as well as compounds (HFST Morph) leads to the best results for the individual systems on both evaluation metrics. The"
W15-3022,W15-1844,1,0.795965,"section: morfessor produces 1-best segmentation: and ‘Kun→←ta→←liito→←ksen selvitt¨a→←misess¨a’ and flatcat ‘Kun→←tali→←itoksen selvitt¨amis→←ess¨a’ 3.1 3.3 3.2 Rule-based Segmentation Rule-based morphological segmentation is based on linguistically motivated computational descriptions of the morphology by dividing the word-forms into morphs (minimal segments carrying semantic or syntactic meaning). The rule-based approach to morphological segmentation uses a morphological dictionary of words and an implementation of the morphological grammar to analyse word-forms. In our case, we use OMORFI (Pirinen, 2015), an open-source implementation of the Finnish morphology.8 OMORFI’s segmentation produces named segment boundaries: stem, inflection, derivation, compound-word and other etymological. The two variants of rule-based segmentation we use are based on selection of the boundary points: compound segmentation uses compound segments and discards the rest (referred in tables and figures to as HFST Comp), and morph segmentation uses compound and Unsupervised Segmentation Segments in the SMT Pipeline The segmented data is used exactly as the wordform-based data during training, tuning and testing of the"
W15-3022,P13-2121,0,0.0253168,"Missing"
W15-3022,W14-3319,1,0.888215,"Missing"
W15-3022,E12-1055,0,0.0141743,".2 636.6 4,831.5 Corpus Constrained System News Shuffle 2014 1,378.8 Unconstrained System FiWaC 146,557.4 System and filter out those for which we have seen already each 1-gram at least 10 times. This results in a reduction of 3.2x on the number of sentence pairs (from 7.3M to 2.3M ) and 2.6x on the number of words (from 114M to 44M ). The resulting parallel datasets (7 in total: Europarl and 3 sets for each fienwac and osubs) are used individually to train translation and reordering models before being combined by linear interpolation based on perplexity minimisation on the development set. (Sennrich, 2012) Language Models All the Language Models (LM) used in our experiments are 5-grams modified Kneser-Ney smoothed LMs trained using KenLM (Heafield et al., 2013). For the constrained setup, the Finnish and the English LMs are trained following two different approaches. The English LM is trained on the concatenation of all available corpora while the Finnish LM is obtained by linearly interpolating individually trained LMs based on each corpus. The weights given to each individual LM is calculated by minimising the perplexity obtained on the development set. For the unconstrained setup, the Finnis"
W15-3022,2006.amta-papers.25,0,0.0135668,"Combination 14.61 0.786 13.54 0.801 Table 4: Results obtained on the development and test sets for the constrained English-to-Finnish translation task. Best individual system in bold. 2010) using default settings, except for the beam size (set to 1, 500) and radius (5 for Finnish and 7 for English), following empirical results obtained on the development set. 5.1 Constrained Results Individual systems trained on the provided data are evaluated before being combined. The results obtained for the English-to-Finnish direction are presented in Table 4.13 The BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores obtained by the system trained on compoundsegmented data (HFST Comp) show a positive impact of this method on SMT according to the development set, compared to the other individual systems. The unsupervised segmentation methods do not improve over phrase-based SMT, while the hierarchical model shows an interesting reduction of the TER score compared to a classic phrasebased approach. On the test set, the use of inflectional morph segments as well as compounds (HFST Morph) leads to the best results for the individual systems on both evaluation metrics. The combination of these 7 systems"
W15-3022,2014.eamt-1.45,1,0.822092,"Missing"
W15-3022,D07-1080,0,0.0247453,"fessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same parallel data for each model, as well as the provided development set to tune the systems. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014), both trained on the parallel data used to learn t"
W15-4906,P11-1103,0,0.0204333,"a large-scale Engish-to-Farsi translation task. The rest of this paper is organised as follows. Section 2 reviews the related work and contextualises our work. Section 3 outlines the main reordering issues due to syntactic differences between English and Farsi. Section 4 presents our reordering model, which is then evaluated in Section 5. Finally, Section 6 concludes the paper and outlines avenues of future work. 2 Related Work Phrase-based systems can perform local (short distance) reordering inside the phrases but they are inherently weak at non-local (medium and long distance) reordering (Birch and Osborne, 2011). Previous work to address reordering in PB-SMT can generally be categorised into two groups. Approaches in the ﬁrst group perform reordering in a pre-processing step (i.e. before decoding) by applying some reordering rules to the source sentences to make them in order more similar to that of the target language (Xia and McCord, 2004; Collins et al., 2005; Genzel, 2010). Although all these approaches have reported improvements, there is a fundamental problem with separating the reordering task into a pre-processing component as every faulty decision in the pre-processing step will be passed al"
W15-4906,W09-0434,0,0.0219166,"re long- and even medium-distance reorderings, since they try to ﬁnd suitable reorderings only between adjacent phrases. The ﬁrst limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this lin"
W15-4906,D14-1082,0,0.0446672,"Missing"
W15-4906,P05-1033,0,0.19675,"f lexical models, they have two important limitations (Birch, 2011). First, since these models are conditioned on actual phrases, they have no ability to be generalised to unseen phrases. Second, these models still fail to capture long- and even medium-distance reorderings, since they try to ﬁnd suitable reorderings only between adjacent phrases. The ﬁrst limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our w"
W15-4906,P05-1066,0,0.347083,"Missing"
W15-4906,W14-3348,0,0.0422701,"irs and features examined (cf. Table 6) and two additional systems that model the reordering for both types of constituent pairs (rows all) with (ws) and without (wos) surface forms. We compare our systems to two baselines, a standard HPB-SMT system (HPB) and a HPBSMT system with added swap glue grammar rule (HPB sgg) as in Equation 4. The swap glue rule allows adjacent phrases to be reversed. X → (X1 X2 , X2 X1 ) (4) Table 7 shows the results obtained by each of the MT systems according to four automatic evaluation metrics: BLEU, NIST (Doddington, 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2014). For each system and evaluation metric we show its relative improvement over the baseline HPB (columns diff). The scores obtained by systems that implement our novel reordering between pairs of dependents (columns dd) are better than those of the baseline, both with (ws) and without (wos) surface forms, accross all the four evaluation metrics. The same is true for models that implement reordering between both pairs of constituent types (columns all), except for the system all wos according to BLEU. The results for systems that perform reordering between pairs of head and dependent offer a mix"
W15-4906,D08-1089,0,0.0302952,"Despite the satisfactory performance of lexical models, they have two important limitations (Birch, 2011). First, since these models are conditioned on actual phrases, they have no ability to be generalised to unseen phrases. Second, these models still fail to capture long- and even medium-distance reorderings, since they try to ﬁnd suitable reorderings only between adjacent phrases. The ﬁrst limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanne"
W15-4906,D11-1079,0,0.16187,"rs from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pair so far, Chinese-to-English. This paper contributes to this research line on two dimensions. First, we extend the work of (Gao et al., 2011), who studied reordering of headdependent pairs (i.e. parent and child elements in the dependency tree), and consider also the reordering of pairs of dependents (i.e. sibling elements in the dependency tree). Second, this is the ﬁrst paper in this line of work to be applied to a language pair ot"
W15-4906,C10-1043,0,0.0213204,"d outlines avenues of future work. 2 Related Work Phrase-based systems can perform local (short distance) reordering inside the phrases but they are inherently weak at non-local (medium and long distance) reordering (Birch and Osborne, 2011). Previous work to address reordering in PB-SMT can generally be categorised into two groups. Approaches in the ﬁrst group perform reordering in a pre-processing step (i.e. before decoding) by applying some reordering rules to the source sentences to make them in order more similar to that of the target language (Xia and McCord, 2004; Collins et al., 2005; Genzel, 2010). Although all these approaches have reported improvements, there is a fundamental problem with separating the reordering task into a pre-processing component as every faulty decision in the pre-processing step will be passed along as a hard decision to the translation system. This also violates the main principle behind statistical modelling in SMT, i.e. to avoid any hard choices and having the ability to reverse early faulty choices. Approaches in the second group try to handle reordering in the decoding step, as a part of the translation process. They implement a probabilistic reordering mo"
W15-4906,J03-1002,0,0.0143311,"g. Table 4 presents the details about this dataset. We parsed the source side (English) of the corpus using the Stanford dependency parser (Chen 1 tribes wandered http://dadegan.ir/catalog/mizan 47 wife wandered unit English Farsi sentences 1,016,758 1,016,758 Train words 13,919,071 14,043,499 sentences 3,000 3,000 Tune words 40,831 41,670 sentences 1,000 1,000 Test words 13,165 13,444 Table 4: Mizan parallel corpus statistics and Manning, 2014) and used the “collapsed representation” of the parser output to obtain direct dependencies between the words in the source sentences. We used GIZA++ (Och and Ney, 2003) to align the words in the corpus. Then we extracted 6,391,255 head−dependent pairs and 5,247,137 dependent−dependent pairs from train dataset and determined the orientation for each pair based on Equation 1. In order to measure the impact of different features on the accuracy of our reordering model (as will be described in Section 5.2), we used the Naive Bayes classiﬁer with standard settings from the Weka machine learning toolkit (Hall et al., 2009). We trained the classiﬁer separately for head−dependent and dependent−dependent pairs. Our baseline MT system was the Moses implementation of H"
W15-4906,P03-1021,0,0.00762563,"on the accuracy of our reordering model (as will be described in Section 5.2), we used the Naive Bayes classiﬁer with standard settings from the Weka machine learning toolkit (Hall et al., 2009). We trained the classiﬁer separately for head−dependent and dependent−dependent pairs. Our baseline MT system was the Moses implementation of HPM model with default settings (Hoang et al., 2009). We used a 5-gram target language model trained on the Farsi side of the training data. In all experiments, the weights of our reordering feature-function and the builtin feature-functions was tuned with MERT (Och, 2003). 5.2 Impact of different features Since the proposed reordering model has to classify the head−dependent and dependent−dependent pairs into their correct monotone or swap orientation classes, its task can be seen as a binary classiﬁcation task. We used the Naive Bayes algorithm to build such an orientation classiﬁer. We then used different feature sets in each classiﬁcation experiment to determine their impact on the accuracy of the model. The features that were examined in this paper are shown in Table 5. All of these features are entirely based on the source sentence and source dependency p"
W15-4906,P05-1034,0,0.185607,"lement pairs presented by Dryer (1992). Dryer has shown that these pairs can be used to distinguish SOV and SVO languages. 4 Dependency-based Reordering Model Our reordering model is based on the source dependency tree, an example of which is shown in Figure 1. The dependency tree of a sentence shows the grammatical relations between the head and dependent words of that sentence. For example in Figure 1, the arrow from “he” to “bought” with label “nsubj”, expresses that the 45 dependent word “he” is the subject of the head word “bought”. Under the assumption that constituents move as a whole (Quirk et al., 2005), our proposed reordering model aims to predict the orientation of each dependent word with respect to its head (head−dependent), and also with respect to the other dependents of that head (dependent−dependent orientation). For example, for the sentence in Figure 1 we try to predict the appropriate orientations between the headdependent and dependent-dependent pairs shown in Tables 2 and 3, respectively. Our motivation for using dependency structure as the basis of our reordering model is based on the assumption that, if it is the case that a reordering pattern is employed for one English–Fars"
W15-4906,2006.amta-papers.25,0,0.0607029,"r according to the constituent pairs and features examined (cf. Table 6) and two additional systems that model the reordering for both types of constituent pairs (rows all) with (ws) and without (wos) surface forms. We compare our systems to two baselines, a standard HPB-SMT system (HPB) and a HPBSMT system with added swap glue grammar rule (HPB sgg) as in Equation 4. The swap glue rule allows adjacent phrases to be reversed. X → (X1 X2 , X2 X1 ) (4) Table 7 shows the results obtained by each of the MT systems according to four automatic evaluation metrics: BLEU, NIST (Doddington, 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2014). For each system and evaluation metric we show its relative improvement over the baseline HPB (columns diff). The scores obtained by systems that implement our novel reordering between pairs of dependents (columns dd) are better than those of the baseline, both with (ws) and without (wos) surface forms, accross all the four evaluation metrics. The same is true for models that implement reordering between both pairs of constituent types (columns all), except for the system all wos according to BLEU. The results for systems that perform reordering between"
W15-4906,N04-4026,0,0.16978,"pre-processing step will be passed along as a hard decision to the translation system. This also violates the main principle behind statistical modelling in SMT, i.e. to avoid any hard choices and having the ability to reverse early faulty choices. Approaches in the second group try to handle reordering in the decoding step, as a part of the translation process. They implement a probabilistic reordering model that can be used in combination with the other models in SMT to ﬁnd the best translation. These approaches range from distortion models (Koehn et al., 2003) to lexical reordering models (Tillmann, 2004). 44 Distortion models generally prefer monotone translation which, while may work for related languages, is not a realistic assumption for translating between languages with different grammatical structure. On top of this limitation, these models do not take the content into consideration, and thus they do not generalise well. Lexical reordering models take content into account and condition reordering on actual phrases. They try to learn local orientations for each adjacent phrase from training data. Despite the satisfactory performance of lexical models, they have two important limitations"
W15-4906,N13-1029,0,0.0134009,"2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pai"
W15-4906,J10-2004,0,0.0160499,"ccess to the necessary structural information to perform long-distance reordering. However, due to the complexity of the decoding algorithm, they have very low performance on large-scale translations. In order to overcome some of these deﬁciencies, we propose a dependency-based reordering model for HPB-SMT. Our model uses the dependency structure of the source sentence to capture the medium- and long-distance reorderings between the dependent parts of the sentence. Unlike the syntax-based models that impose harsh syntactic limits on rule extraction and require serious efforts to be optimised (Wang et al., 2010), we use syntactic information only in the reordering model and augment the HPB model with soft dependency constraints. We report experimental results on a large-scale Engish-to-Farsi translation task. The rest of this paper is organised as follows. Section 2 reviews the related work and contextualises our work. Section 3 outlines the main reordering issues due to syntactic differences between English and Farsi. Section 4 presents our reordering model, which is then evaluated in Section 5. Finally, Section 6 concludes the paper and outlines avenues of future work. 2 Related Work Phrase-based s"
W15-4906,2009.iwslt-papers.4,0,0.100382,"s. Then we extracted 6,391,255 head−dependent pairs and 5,247,137 dependent−dependent pairs from train dataset and determined the orientation for each pair based on Equation 1. In order to measure the impact of different features on the accuracy of our reordering model (as will be described in Section 5.2), we used the Naive Bayes classiﬁer with standard settings from the Weka machine learning toolkit (Hall et al., 2009). We trained the classiﬁer separately for head−dependent and dependent−dependent pairs. Our baseline MT system was the Moses implementation of HPM model with default settings (Hoang et al., 2009). We used a 5-gram target language model trained on the Farsi side of the training data. In all experiments, the weights of our reordering feature-function and the builtin feature-functions was tuned with MERT (Och, 2003). 5.2 Impact of different features Since the proposed reordering model has to classify the head−dependent and dependent−dependent pairs into their correct monotone or swap orientation classes, its task can be seen as a binary classiﬁcation task. We used the Naive Bayes algorithm to build such an orientation classiﬁer. We then used different feature sets in each classiﬁcation e"
W15-4906,D13-1053,0,0.0230185,"anslation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pair so far, Chinese-to-English. This paper contributes to this research line on two dimensions. First, we extend the work of (Gao et al., 2011), who studied reordering of headdependent pairs (i.e. parent and child elements in the dependency tree), and consider also the reordering of pairs of dependents (i.e. sibling elements in the dependency tree). Second, this is the ﬁ"
W15-4906,N03-1017,0,0.0445723,"cessing component as every faulty decision in the pre-processing step will be passed along as a hard decision to the translation system. This also violates the main principle behind statistical modelling in SMT, i.e. to avoid any hard choices and having the ability to reverse early faulty choices. Approaches in the second group try to handle reordering in the decoding step, as a part of the translation process. They implement a probabilistic reordering model that can be used in combination with the other models in SMT to ﬁnd the best translation. These approaches range from distortion models (Koehn et al., 2003) to lexical reordering models (Tillmann, 2004). 44 Distortion models generally prefer monotone translation which, while may work for related languages, is not a realistic assumption for translating between languages with different grammatical structure. On top of this limitation, these models do not take the content into consideration, and thus they do not generalise well. Lexical reordering models take content into account and condition reordering on actual phrases. They try to learn local orientations for each adjacent phrase from training data. Despite the satisfactory performance of lexica"
W15-4906,W04-3250,0,0.203099,"Missing"
W15-4906,P06-1066,0,0.0259177,"dering on actual phrases. They try to learn local orientations for each adjacent phrase from training data. Despite the satisfactory performance of lexical models, they have two important limitations (Birch, 2011). First, since these models are conditioned on actual phrases, they have no ability to be generalised to unseen phrases. Second, these models still fail to capture long- and even medium-distance reorderings, since they try to ﬁnd suitable reorderings only between adjacent phrases. The ﬁrst limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre"
W15-4906,P12-1095,0,0.0196735,"ssing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pair so far, Chinese-to-English. This paper contributes to this research line on two dimensions. First, we extend the work of (Gao et al., 2011), who studied reordering of headdependent pairs (i.e. parent and child elements in the dependency tree), and consider also the reordering of pairs of dependents (i.e. sibling elements in the dependency tree). Second, this is the ﬁrst paper in this line of work to be applied to a language pair other than Chinese-to-English. Our language pair, Englis"
W15-4906,N09-1028,0,0.0188487,"only between adjacent phrases. The ﬁrst limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source el"
W15-4906,C04-1073,0,\N,Missing
W15-4906,N13-1060,0,\N,Missing
W15-4944,E06-1032,0,\N,Missing
W15-4944,W10-1751,0,\N,Missing
W15-4944,W14-3301,0,\N,Missing
W15-4944,P02-1040,0,\N,Missing
W15-4944,W14-3319,1,\N,Missing
W15-4944,P11-1105,0,\N,Missing
W15-4944,P10-2041,0,\N,Missing
W15-4944,W05-0909,0,\N,Missing
W15-4944,P07-2045,0,\N,Missing
W15-4944,W07-0718,0,\N,Missing
W15-4944,C14-1111,0,\N,Missing
W15-4944,P12-3005,0,\N,Missing
W15-4944,2012.eamt-1.67,1,\N,Missing
W15-4944,2014.eamt-1.4,1,\N,Missing
W15-4944,W14-3320,0,\N,Missing
W15-4944,2005.mtsummit-papers.11,0,\N,Missing
W15-4944,ljubesic-etal-2014-tweetcat,1,\N,Missing
W15-4944,W15-3036,1,\N,Missing
W15-4944,rubino-etal-2014-quality,1,\N,Missing
W15-4944,W15-3022,1,\N,Missing
W15-4944,W15-4903,1,\N,Missing
W15-4944,2015.eamt-1.4,1,\N,Missing
W15-4944,espla-gomis-etal-2014-comparing,1,\N,Missing
W15-4944,W15-3001,0,\N,Missing
W15-4944,ljubesic-toral-2014-cawac,1,\N,Missing
W15-4944,W14-0405,1,\N,Missing
W15-4944,2005.iwslt-1.8,0,\N,Missing
W15-4944,W16-3421,1,\N,Missing
W15-4944,D07-1078,0,\N,Missing
W15-4944,W08-0509,0,\N,Missing
W15-4944,W11-2123,0,\N,Missing
W15-4944,P14-1129,0,\N,Missing
W15-4944,W16-2347,0,\N,Missing
W15-4944,W16-2375,1,\N,Missing
W15-4944,W16-2367,1,\N,Missing
W15-4944,W16-3423,1,\N,Missing
W16-2322,P16-1160,0,0.0980628,"corpora. We used an additional synthetic parallel corpus to train our NMT system, which was obtained by backtranslating the Finnish News Crawl corpora into English with an SMT system (see Section 3).2 The monolingual corpora used for training its LM are listed in Table 2. Throughout the paper we evaluate the systems we build in terms on three automatic evaluation metrics: BLEU (Papineni et al., 2002), 3 Neural Machine Translation NMT systems have been reported to outperform SMT systems for different language pairs (Sennrich et al., 2015a; Luong et al., 2015; Costa-Juss`a and Fonollosa, 2016; Chung et al., 2016a). Unlike SMT, in which different models are trained independently and their weights are tuned jointly, in NMT all the components are jointly trained to maximise translation quality. NMT systems have a strong generalisation power because they encode words as real-valued vectors (similar words are close to each other in that vector space) and they are able to model long-distance phenomena thanks to the use of LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Chung et al., 2014) units. We followed the encoder-decoder architecture with attention proposed by Bahdanau et al. (2015).3 NMT models are"
W16-2322,P16-2058,0,0.0478889,"Missing"
W16-2322,P14-1129,0,0.0435652,"the evaluation on newstest2015. The best score for each metric is shown in bold. An arrow pointing upwards (↑) means that the corresponding system outperforms the system without segmentation by a statistically significant margin, while an arrow pointing downwards (↓) means the opposite: the system without segmentation wins. System BLEU TER chrF1 best individual model 0.1568 0.7714 49.52 (most probable SL word) ensemble 0.1819 ↑ 0.7409 ↑ 52.21 ↑ (most probable SL word) ensemble 0.1830 ↑ 0.7411 52.43 ↑ (preserve named entities) Model (Durrani et al., 2011) and a Bilingual Neural Language Model (Devlin et al., 2014), as well as three reordering models: word- and phrasebased and hierarchical (Koehn et al., 2005; Galley and Manning, 2008). This year’s SMT systems used the same models and datasets, except for the LMs, which this time were log-linearly interpolated and used the additional corpus available (Common Crawl, cf. Table 1). We built three SMT systems, which share the same models and data, with the only difference being the segmentation used in the Finnish data: Table 6: Results of the evaluation on newstest2016 of our NMT submission (in bold), the simpler strategy for translating unknown words by J"
W16-2322,2005.iwslt-1.8,0,0.161518,"Missing"
W16-2322,N13-1073,0,0.0482312,"Missing"
W16-2322,D08-1089,0,0.0148591,"hat the corresponding system outperforms the system without segmentation by a statistically significant margin, while an arrow pointing downwards (↓) means the opposite: the system without segmentation wins. System BLEU TER chrF1 best individual model 0.1568 0.7714 49.52 (most probable SL word) ensemble 0.1819 ↑ 0.7409 ↑ 52.21 ↑ (most probable SL word) ensemble 0.1830 ↑ 0.7411 52.43 ↑ (preserve named entities) Model (Durrani et al., 2011) and a Bilingual Neural Language Model (Devlin et al., 2014), as well as three reordering models: word- and phrasebased and hierarchical (Koehn et al., 2005; Galley and Manning, 2008). This year’s SMT systems used the same models and datasets, except for the LMs, which this time were log-linearly interpolated and used the additional corpus available (Common Crawl, cf. Table 1). We built three SMT systems, which share the same models and data, with the only difference being the segmentation used in the Finnish data: Table 6: Results of the evaluation on newstest2016 of our NMT submission (in bold), the simpler strategy for translating unknown words by Jean et al. (2015, Sec. 3.3) (labelled as most probable SL word) and our best individual NMT model. The best score for each"
W16-2322,C14-1111,0,0.0977044,"Missing"
W16-2322,D15-1166,0,0.0366615,"nd Table 3 shows the same information for the parallel corpora. We used an additional synthetic parallel corpus to train our NMT system, which was obtained by backtranslating the Finnish News Crawl corpora into English with an SMT system (see Section 3).2 The monolingual corpora used for training its LM are listed in Table 2. Throughout the paper we evaluate the systems we build in terms on three automatic evaluation metrics: BLEU (Papineni et al., 2002), 3 Neural Machine Translation NMT systems have been reported to outperform SMT systems for different language pairs (Sennrich et al., 2015a; Luong et al., 2015; Costa-Juss`a and Fonollosa, 2016; Chung et al., 2016a). Unlike SMT, in which different models are trained independently and their weights are tuned jointly, in NMT all the components are jointly trained to maximise translation quality. NMT systems have a strong generalisation power because they encode words as real-valued vectors (similar words are close to each other in that vector space) and they are able to model long-distance phenomena thanks to the use of LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Chung et al., 2014) units. We followed the encoder-decoder architecture with attentio"
W16-2322,P02-1040,0,0.0967831,"Missing"
W16-2322,P15-1001,0,0.43727,"match because, due to time constraints, we did not backtranslate a few tens of thousands of sentences. We used the code available at: https: //github.com/sebastien-j/LV_groundhog/ tree/master/experiments/nmt 363 followed the set-up of the rule-based morphologically segmented system from our last year’s constrained submission (Rubino et al., 2015). It was trained on Europarl and the concatenation of the English monolingual corpora listed in Table 2. Most of the NMT architectures in the literature can only operate with a fixed TL vocabulary (that ranges from 30 000 to 80 000 words, according to Jean et al. (2015)), since training and decoding computational complexity grows with its size. Although Jean et al. (2015) proposed an to reduce that complexity and hence use larger vocabularies, Sennrich et al. (2015b) showed that segmenting words into smaller units can also reduce complexity, increase effective vocabulary size and even improve translation quality. We followed the latter strategy. The evaluation of character-based NMT approaches (Ling et al., 2015; Costa-Juss`a and Fonollosa, 2016; Chung et al., 2016b) was left as future work. In the remainder of this section, we present the segmentation appro"
W16-2322,W15-1844,0,0.0836262,"ion task. We participated in the English-to-Finnish constrained task. English-to-Finnish is a particularly challenging language pair for corpus-based MT because of the lack of in-domain parallel data (the only available parallel corpus in the shared task is Europarl) and the complex morphology of Finnish. The fact that the same root can be inflected in many different ways and that nouns can be joined together in order to build compound words exacerbates the aforementioned lack of parallel data problem. As in our last year’s submission (Rubino et al., 2015), we used morphological segmentation (Pirinen, 2015) on the Finnish side in order to deal with data scarcity and reduce the size of the Finnish vocabulary. We also used character-level evaluation 2 Datasets and Tools We preprocessed the training corpora with scripts included in the Moses toolkit (Koehn et al., 2007). We performed the following operations: punctuation normalisation, tokenisation, true-casing and escaping of problematic characters. The truecaser is lexicon-based and it was trained on all the monolingual data. In addition, we removed sentence pairs from the parallel corpora where either side is longer than 80 tokens. 1 For instanc"
W16-2322,W04-3250,0,0.0831678,"Missing"
W16-2322,W15-3049,0,0.131385,"Missing"
W16-2322,2006.amta-papers.25,0,0.0339497,"guistics Corpus Europarl v8 Common Crawl News Crawl 2014–15 Sentences (k) 2 121 113 995 6 741 Words (M) Corpus Sentences (k) English Finnish Europarl v8 1 901 50.9 36.6 backtranslated News Crawl 2014–15 6 674 106.6 82.3 (only for NMT) Words (M) 39.5 2 416.7 83.1 Table 1: Finnish monolingual data, after preprocessing, used to train the LMs of our SMT submission. Corpus Europarl v7 News Commentary v11 News Crawl 2007–15 News Discussions Sentences (k) 2 218 391 117 446 57 804 Table 3: Parallel data, after preprocessing, used to train our SMT and NMT systems. Words (M) 59.9 9.8 2 713.2 983.2 TER (Snover et al., 2006) and chrF1 (Popovi´c, 2015). As the performance obtained in the development (newsdev2015) and validation (newstest2015) sets guides our decisions, we believe it is sensible to use three metrics with different underlying methodologies and that work on different elements (words and characters). Statistical significance of the difference between systems is computed with paired bootstrap resampling (Koehn, 2004) (p ≤ 0.05, 1 000 iterations). Table 2: English monolingual data, after preprocessing, used to train the LM of the Finnish-toEnglish SMT system we used to backtranslate the Finnish News Cra"
W16-2322,W15-3031,0,0.0921985,"Missing"
W16-2322,P07-2045,0,\N,Missing
W16-2322,W15-3022,1,\N,Missing
W16-2322,P16-1162,0,\N,Missing
W16-2322,P16-1009,0,\N,Missing
W16-3423,agic-ljubesic-2014-setimes,0,0.0575829,"Missing"
W16-3423,D11-1033,0,0.0612128,"Missing"
W16-3423,N12-1047,0,0.0331989,"d by professional and amateur translators. 2. Compare the use of three reordering models (word-, phrase-based and hierarchical). 3. Measure the impact of using additional models recently introduced in the SMT pipeline. Specifically, the operation sequence model (OSM) and bilingual neural language models (BiNLM). 4. Assess the impact of different ways to select and combine data sets. 5. Compare our best systems to widely-used commercial systems. 2 Experimental Setting 2.1 MT Systems SMT systems are trained with Moses 3.0,4 using default settings unless mentioned otherwise, and tuned with MIRA (Cherry and Foster, 2012). Language models are of order 5 with Kneser-Ney modified smoothing. The following publicly available parallel corpora are used for training: HrEnWaC 2.0,5 the DGT Translation Memory,6 the JRC Acquis,7 SET IMES(Agi´c and Ljubeˇsi´c, 2014), T ED talks,8 OpenSubtitles 2013 cleaned (Espl`a-Gomis et al., 2014) and SrEnWaC.9 The last one is a parallel corpus for Serbian–English. The Serbian side is translated to Croatian with a rule-based system in order to get more English-Croatian parallel text.10 Language models are trained on the hrWaC corpus11 for Croatian, and on all the available English dat"
W16-3423,P14-1129,0,0.187282,"Missing"
W16-3423,P11-1105,0,0.265402,"Missing"
W16-3423,D08-1089,0,0.0386896,"human evaluation. This may indicate that the impact of the development set in our setup is too small to be of importance. 3.2 Reordering Models We compare using a word-based reordering model solely (the default in the Moses MT toolkit) to adding two additional models: 1) phrase-based (with the same three orientations as the word-based model: monotone, swap and discontinuous) and 2) hierarchical (with four orientations: non merged, discontinuous, left and right). This combination has been shown to yield the best performance in terms of automatic metrics for English–Chinese and English–Arabic (Galley and Manning, 2008). Here we evaluate it for another language and not only automatically but also manually. Results are shown in Table 2. Table 2. Results using different reordering models. Best results shown in bold. English→Croatian Croatian→English BLEU TER Human Range BLEU TER Human Range Word-based 0.2363 0.6303 -0.117 1-2 0.3392 0.5211 0.168 1 Three 0.2355 0.6336 0.117 1-2 0.3404 0.5202 -0.168 2 System The results are mixed. In terms of automatic metrics, the differences are very small and not significant. According to the human evaluation, the word-based model alone leads to significantly better results t"
W16-3423,W13-2235,0,0.0453153,"Missing"
W16-3423,P10-2041,0,0.0839258,"Missing"
W16-3423,P02-1040,0,0.110767,"Missing"
W16-3423,W14-3301,0,0.209135,"Missing"
W16-3423,E12-1055,0,0.0483699,"Missing"
W16-3423,2006.amta-papers.25,0,0.0504835,"lly (except the one in Section 3.4). We used the widely used automatic metrics BLEU (Papineni et al., 2002) and 4 5 6 7 8 9 10 11 12 https://github.com/moses-smt/mosesdecoder/tree/RELEASE-3.0 http://hdl.handle.net/11356/1058 https://ec.europa.eu/jrc/en/language-technologies/dgt-translation-memory http://tinyurl.com/CroatianAcquis http://nlp.ffzg.hr/resources/corpora/ted-talks/ http://hdl.handle.net/11356/1059 https://svn.code.sf.net/p/apertium/svn/staging/apertium-hbs HR-hbs SR/ http://nlp.ffzg.hr/resources/corpora/hrwac/ http://www.statmt.org/wmt15/translation-task.html 370 Toral et al. TER (Snover et al., 2006). Statistical significance is calculated on BLEU scores with paired bootstrap resampling (1,000 iterations and p = 0.95). The human evaluation consists of ranking MT outputs with Appraise.13 For each experiment 100 randomly selected segments were ranked. All the annotations were carried out by 2 native Croatian speakers with an advanced level of English. The following guidelines were provided to the annotators: Given translations by more than two MT systems, the task is to rank them: - Rank system A higher (rank1) than B (rank2), if the output of the first is better than the output of the seco"
W16-3423,N13-1069,0,0.136687,".14 Namely, we run 1,000 iterations of rankings followed by clustering (p = 0.95). If two systems are placed in different clusters (column “range” in results’ tables) then the one with lower range is considered significantly better. 3 Experiments 3.1 Development Sets In this experiment we aim to assess the impact of using a development set obtained by professional versus amateur translators. While professional translations should lead to a higher quality data set, its cost is in our case close to an order of magnitude (both in terms of price and time) higher than crowdsourcing. In this sense, Zbib et al. (2013) built MT systems for Arabic–English using development sets that were professionally translated and crowdsourced. They compared tuning with one reference (either professional or crowdsourced) and using both together as multiple references. The latter setup led to the best results. In our experiments we aim to corroborate these results for English-to-Croatian15 and also compare the use of professional and crowdsourced translations for tuning. Results are shown in Table 1. Following Zbib et al. (2013), we would expect the combination of both professional and crowdsourced translations to perform"
W18-6312,D16-1025,0,0.0446923,"nal translators against those of non-experts and discover that those of the experts result in higher inter-annotator agreement and better discrimination between human and machine translations. In addition, we analyse the human translations of the test set and identify important translation issues. Finally, based on these findings, we provide a set of recommendations for future human evaluations of MT. 1 Introduction Neural machine translation (NMT) has revolutionised the field of MT by overcoming many of the weaknesses of the previous state-of-the-art phrase-based machine translation (PBSMT) (Bentivogli et al., 2016; Toral and S´anchez-Cartagena, 2017). In only a few years since the first working models, this approach has led to a substantial improvement in translation quality, reported in terms of automatic metrics (Bojar et al., 2016, 2017; Sennrich et al., 2016). This has ignited higher levels of expectation, fuelled in part by hyperbolic claims from large MT developers. First we saw in Wu et al. (2016) that Google NMT was “bridging the gap between human and machine translation [quality]”. This was amplified 1 https://www.sdl.com/about/news-media/press/2018/sdlcracks-russian-to-english-neural-machine-"
W18-6312,D17-1301,1,0.828365,"we evaluate sentences in consecutive order (rather than randomly). This can be accommodated in ranking as we can show all three translations for each source sentence together with the previous and next source sentences Context Hassan et al. (2018) evaluated the sentences in the testset in randomised order, meaning that sentences were evaluated in isolation. However, documents such as the news stories that make up the test set contain relations that go beyond the sentence level. To translate them correctly one needs to take this inter-sentential context into account (Voigt and Jurafsky, 2012; Wang et al., 2017a). The MT system by Hassan et al. (2018) translates sentences in isolation while humans naturally consider the wider context when conducting translation. Our hypothesis is that referential relations that go beyond the sentence-level were ignored in the evaluation as its setup considered sentences in isolation (randomised). This probably resulted in the evaluation missing some errors by the MT system that might have been caused by its lack of inter-sentential contextual knowledge. In contrast, our revised human evaluation takes intersentential context into account. Sentences are not randomised"
W18-6312,W17-4742,0,0.0316018,"we evaluate sentences in consecutive order (rather than randomly). This can be accommodated in ranking as we can show all three translations for each source sentence together with the previous and next source sentences Context Hassan et al. (2018) evaluated the sentences in the testset in randomised order, meaning that sentences were evaluated in isolation. However, documents such as the news stories that make up the test set contain relations that go beyond the sentence level. To translate them correctly one needs to take this inter-sentential context into account (Voigt and Jurafsky, 2012; Wang et al., 2017a). The MT system by Hassan et al. (2018) translates sentences in isolation while humans naturally consider the wider context when conducting translation. Our hypothesis is that referential relations that go beyond the sentence-level were ignored in the evaluation as its setup considered sentences in isolation (randomised). This probably resulted in the evaluation missing some errors by the MT system that might have been caused by its lack of inter-sentential contextual knowledge. In contrast, our revised human evaluation takes intersentential context into account. Sentences are not randomised"
W18-6312,N15-1124,0,0.0458269,"sentences, they see them in order. We randomised the documents in the test set (169) and prepared one evaluation task per document, for the first 49 documents (503 sentences). Of these 49 documents, 41 were originally written in ZH (amounting to 299 sentences, with each document containing 7.3 sentences on average) and the remaining 8 were originally written in EN (204 sentences, average of 25.5 sentences per document). Evaluators were asked to annotate all the sentences of each document in one go, so that they can take intersentential context into account. Rather than direct assessment (DA) (Graham et al., 2015), as in Hassan et al. (2018), we conduct a relative ranking evaluation. While DA has some advantages over ranking and has replaced the latter at the WMT shared task since 2017 (Bojar et al., 2017), ranking is more appropriate for our evaluation due to the fact that we evaluate sentences in consecutive order (rather than randomly). This can be accommodated in ranking as we can show all three translations for each source sentence together with the previous and next source sentences Context Hassan et al. (2018) evaluated the sentences in the testset in randomised order, meaning that sentences wer"
W18-6312,D18-1512,0,0.0886803,"Missing"
W18-6312,W16-2323,0,0.0468048,"and identify important translation issues. Finally, based on these findings, we provide a set of recommendations for future human evaluations of MT. 1 Introduction Neural machine translation (NMT) has revolutionised the field of MT by overcoming many of the weaknesses of the previous state-of-the-art phrase-based machine translation (PBSMT) (Bentivogli et al., 2016; Toral and S´anchez-Cartagena, 2017). In only a few years since the first working models, this approach has led to a substantial improvement in translation quality, reported in terms of automatic metrics (Bojar et al., 2016, 2017; Sennrich et al., 2016). This has ignited higher levels of expectation, fuelled in part by hyperbolic claims from large MT developers. First we saw in Wu et al. (2016) that Google NMT was “bridging the gap between human and machine translation [quality]”. This was amplified 1 https://www.sdl.com/about/news-media/press/2018/sdlcracks-russian-to-english-neural-machine-translation.html 2 http://aka.ms/Translator-HumanParityData 113 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 113–123 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computati"
W18-6312,E17-1100,1,0.881329,"Missing"
W18-6312,W12-2503,0,0.0411092,"ation due to the fact that we evaluate sentences in consecutive order (rather than randomly). This can be accommodated in ranking as we can show all three translations for each source sentence together with the previous and next source sentences Context Hassan et al. (2018) evaluated the sentences in the testset in randomised order, meaning that sentences were evaluated in isolation. However, documents such as the news stories that make up the test set contain relations that go beyond the sentence level. To translate them correctly one needs to take this inter-sentential context into account (Voigt and Jurafsky, 2012; Wang et al., 2017a). The MT system by Hassan et al. (2018) translates sentences in isolation while humans naturally consider the wider context when conducting translation. Our hypothesis is that referential relations that go beyond the sentence-level were ignored in the evaluation as its setup considered sentences in isolation (randomised). This probably resulted in the evaluation missing some errors by the MT system that might have been caused by its lack of inter-sentential contextual knowledge. In contrast, our revised human evaluation takes intersentential context into account. Sentences"
W19-0504,E17-2039,1,0.867206,"Missing"
W19-0504,W17-6901,1,0.701943,"ijn, 1972). In a post-processing step, the original clause structured is restored.1 To include morphological and syntactic information, we apply a lemmatizer, POS-tagger and dependency parser using Stanford CoreNLP (Manning et al., 2014), similar to Sennrich and Haddow (2016) for machine translation. The lemmas and POS-tags are added as a token after each word. For the dependency parse, we add the incoming arc for each word. We also apply the easyCCG parser of Lewis and Steedman (2014), using the supertags.2 Finally, we exploit semantic information by using semantic tags (Bjerva et al., 2016; Abzianidze and Bos, 2017). Semantic tags are language-neutral semantic categories, which get assigned to a word in a similar fashion as part-of-speech tags. Semantic tags are able to express important semantic distinctions, such as negation, modals and types of quantification. We train a semantic tagger with the TnT tagger (Brants, 2000) on the gold and silver standard data in the PMB release. Examples of the input to the model for each source of information are shown in Table 1. 1 2 See Van Noord, Abzianidze, Toral, and Bos (2018) for a more detailed overview of the representation used. We segment the supertags, e.g."
W19-0504,P17-2021,0,0.0279904,"al., 2017; Dong and Lapata, 2018; Liu et al., 2018; Van Noord, Abzianidze, Toral, and Bos, 2018). This architecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a multi-source approach has proved particularly successful for adding syntax (Currey and Heafield, 2018). The current approaches in neural semantic parsing either include (some) linguistic information in a single encoder (POS-tags in Van Noord and Bos 2017a,b, lemmas in Liu et al. 2018), or use multiple encoders to represent multiple languages rather than linguistic knowledge (Duong et al., 2017; Susanto and Lu, 2017). To our knowledge, we are the first to investigate the potential of exploiting linguistic information in a multi-encoder setup for (neural) semantic parsing. Specifically, t"
W19-0504,C16-1333,1,0.810388,"Bruijn index (de Bruijn, 1972). In a post-processing step, the original clause structured is restored.1 To include morphological and syntactic information, we apply a lemmatizer, POS-tagger and dependency parser using Stanford CoreNLP (Manning et al., 2014), similar to Sennrich and Haddow (2016) for machine translation. The lemmas and POS-tags are added as a token after each word. For the dependency parse, we add the incoming arc for each word. We also apply the easyCCG parser of Lewis and Steedman (2014), using the supertags.2 Finally, we exploit semantic information by using semantic tags (Bjerva et al., 2016; Abzianidze and Bos, 2017). Semantic tags are language-neutral semantic categories, which get assigned to a word in a similar fashion as part-of-speech tags. Semantic tags are able to express important semantic distinctions, such as negation, modals and types of quantification. We train a semantic tagger with the TnT tagger (Brants, 2000) on the gold and silver standard data in the PMB release. Examples of the input to the model for each source of information are shown in Table 1. 1 2 See Van Noord, Abzianidze, Toral, and Bos (2018) for a more detailed overview of the representation used. We"
W19-0504,W08-2222,1,0.464715,"similar to only using gold data, with the exception of the semantic tags, which even hurt the performance now. Interestingly, when stacking the linguistic features, there is no improvement over only using the lemma of the source words. We now compare our best models to previous parsers4 (Bos, 2015; Van Noord, Abzianidze, Toral, and Bos, 2018) and two baseline systems, SPAR and SIM - SPAR. As previously indicated, Van Noord, Abzianidze, Toral, and Bos (2018) used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features. Boxer (Bos, 2008, 2015) is a DRS parser that uses a statistical CCG parser for syntactic analysis and a compositional semantics based on λcalculus, followed by pronoun and presupposition resolution. SPAR is a baseline system that outputs the same DRS for each test instance5 , while SIM - SPAR outputs the DRS of the most similar sentence in the training set, based on a simple word embedding metric.6 The results are shown in Table 5. Our model clearly outperforms the previous systems, even when only using gold standard data. When compared to Van Noord, Abzianidze, Toral, and Bos (2018), retrained with the same"
W19-0504,W15-1841,1,0.921824,"observe an improvement for each addition, resulting in a final 2.7 point F-score increase over the baseline. If we also employ silver data, we again observe that the multi-encoder setup is preferable over a single encoder, for both isolating and stacking the linguistic features. On isolation, the results are similar to only using gold data, with the exception of the semantic tags, which even hurt the performance now. Interestingly, when stacking the linguistic features, there is no improvement over only using the lemma of the source words. We now compare our best models to previous parsers4 (Bos, 2015; Van Noord, Abzianidze, Toral, and Bos, 2018) and two baseline systems, SPAR and SIM - SPAR. As previously indicated, Van Noord, Abzianidze, Toral, and Bos (2018) used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features. Boxer (Bos, 2008, 2015) is a DRS parser that uses a statistical CCG parser for syntactic analysis and a compositional semantics based on λcalculus, followed by pronoun and presupposition resolution. SPAR is a baseline system that outputs the same DRS for each test instance5 , while SIM - SPAR outputs the"
W19-0504,A00-1031,0,0.0448471,"are added as a token after each word. For the dependency parse, we add the incoming arc for each word. We also apply the easyCCG parser of Lewis and Steedman (2014), using the supertags.2 Finally, we exploit semantic information by using semantic tags (Bjerva et al., 2016; Abzianidze and Bos, 2017). Semantic tags are language-neutral semantic categories, which get assigned to a word in a similar fashion as part-of-speech tags. Semantic tags are able to express important semantic distinctions, such as negation, modals and types of quantification. We train a semantic tagger with the TnT tagger (Brants, 2000) on the gold and silver standard data in the PMB release. Examples of the input to the model for each source of information are shown in Table 1. 1 2 See Van Noord, Abzianidze, Toral, and Bos (2018) for a more detailed overview of the representation used. We segment the supertags, e.g. (SNP)(SNP) is represented as ( S  NP )  ( S  NP ) Table 1: Example representations for each source of input information. Source Representation Sentence Lemma POS-tags Dependency parse Semantic tags CCG supertags I am not working for Tom . I be not work for Tom . PRP VBP RB VBG IN NNP . nsubj aux neg ROOT c"
W19-0504,P13-2131,0,0.0233128,"300 15 Dropout RNN Dropout src/tgt Batch size Optimization crit Vocab size src/tgt Optimizer 0.2 0.0 12 ce-mean 80/150 adam Learning rate (LR) LR decay LR decay strategy LR decay start Clip normalization 0.002 0.8 epoch 9 3 Beam size Length normalization Label smoothing Skip connections Layer normalization 10 0.9 0.1 True True 2.4 Evaluation Procedure Produced DRSs are compared with the gold standard representations by using COUNTER (Van Noord, Abzianidze, Haagsma, and Bos, 2018). This is a tool that calculates micro precision, recall and F-score over matching clauses, similar to the SMATCH (Cai and Knight, 2013) evaluation tool for AMR parsing. All clauses have the same weight in matching, except for REF clauses, which are ignored. An example of the matching procedure is shown in Figure 1. The produced DRSs go through a strict syntactic and semantic validation process, as described in Van Noord, Abzianidze, Toral, and Bos (2018). If a produced DRS is invalid, it is replaced by a dummy DRS, which gets an F-score of 0.0. We check whether two systems differ significantly by performing approximate randomization (Noreen, 1989), with α = 0.05, R = 1000 and F (model1 ) > F (model2 ) as test statistic for ea"
W19-0504,D18-1327,0,0.076743,"ecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a multi-source approach has proved particularly successful for adding syntax (Currey and Heafield, 2018). The current approaches in neural semantic parsing either include (some) linguistic information in a single encoder (POS-tags in Van Noord and Bos 2017a,b, lemmas in Liu et al. 2018), or use multiple encoders to represent multiple languages rather than linguistic knowledge (Duong et al., 2017; Susanto and Lu, 2017). To our knowledge, we are the first to investigate the potential of exploiting linguistic information in a multi-encoder setup for (neural) semantic parsing. Specifically, the aims of this paper are to investigate (i) whether exploiting linguistic information can improve semantic p"
W19-0504,P18-1068,0,0.0211209,"achine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders. 1 Introduction Sequence-to-sequence neural networks have shown remarkable performance in semantic parsing (Ling et al., 2016; Jia and Liang, 2016; Konstas et al., 2017; Dong and Lapata, 2018; Liu et al., 2018; Van Noord, Abzianidze, Toral, and Bos, 2018). This architecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a"
W19-0504,K17-1038,0,0.0139763,"els can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a multi-source approach has proved particularly successful for adding syntax (Currey and Heafield, 2018). The current approaches in neural semantic parsing either include (some) linguistic information in a single encoder (POS-tags in Van Noord and Bos 2017a,b, lemmas in Liu et al. 2018), or use multiple encoders to represent multiple languages rather than linguistic knowledge (Duong et al., 2017; Susanto and Lu, 2017). To our knowledge, we are the first to investigate the potential of exploiting linguistic information in a multi-encoder setup for (neural) semantic parsing. Specifically, the aims of this paper are to investigate (i) whether exploiting linguistic information can improve semantic parsing and (ii) whether it is better to include this linguistic information in the same encoder or in an additional one. We take as baseline the neural semantic parser for Discourse Representation Structures (DRS, Kamp and Reyle, 1993; Van Noord, Abzianidze, Haagsma, and Bos, 2018) developed b"
W19-0504,N16-1101,0,0.0338903,"ncy parse Semantic tags CCG supertags I am not working for Tom . I be not work for Tom . PRP VBP RB VBG IN NNP . nsubj aux neg ROOT case nmod punct PRO NOW NOT EXG REL PER NIL NP (S[dcl]NP)/(S[ng]NP) (SNP)(SNP) (S[ng]NP)/PP PP/NP N . There are two ways to add the linguistic information; (1) merging all the information (i.e., input text and linguistic information) in a single encoder, or (2) using multiple encoders (i.e., encoding separately the input text and the linguistic information). Multi-source encoders were initially introduced for multilingual translation (Zoph and Knight, 2016; Firat et al., 2016; Libovick´y and Helcl, 2017), but recently were used to introduce syntactic information to the model (Currey and Heafield, 2018). Table 2 shows examples of how the input is structured for using one or more encoders. Table 2: Example representation when using one or two encoders, for either a single source of information (POS) or multiple sources (POS + Sem) for the sentence I am not working for Tom. For readability purposes we show the word-level instead of character-level representation of the source words here. Source Encoder Representation POS - 1 enc Enc 1 I PRP am VBP not RB working VBG"
W19-0504,P16-1002,0,0.0423357,"formance even further. Research in neural machine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders. 1 Introduction Sequence-to-sequence neural networks have shown remarkable performance in semantic parsing (Ling et al., 2016; Jia and Liang, 2016; Konstas et al., 2017; Dong and Lapata, 2018; Liu et al., 2018; Van Noord, Abzianidze, Toral, and Bos, 2018). This architecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Ha"
W19-0504,P18-4020,0,0.0339798,"NIL Experiments showed that using more than two encoders drastically decreased performance. Therefore, we merge all the linguistic information in a single encoder (see last row of Table 2). 2.3 Neural Architecture We employ a recurrent sequence-to-sequence neural network with attention (Bahdanau et al., 2014) and two bi-LSTM layers, similar to the one used by Van Noord, Abzianidze, Toral, and Bos (2018). However, their model was trained with OpenNMT (Klein et al., 2017), which does not support multiple encoders. Therefore, we switch to the sequence-to-sequence framework implemented in Marian (Junczys-Dowmunt et al., 2018). We use model-type s2s (for a single encoder) or multi-s2s (for multiple encoders). For the latter, this means that the multiple inputs are encoded separately by an identical RNN (without sharing parameters). The encoders share a single decoder, in which the resulting context vectors are concatenated. An attention layer3 is then applied to selectively give more attention to certain parts of the vector (i.e. it can learn that the words themselves are more important than just the POS-tags). A detailed overview of our parameter settings, found after a search on the dev set, can be found in Table"
W19-0504,P17-4012,0,0.0484199,"VBG EXG for IN REL Tom NNP PER . . NIL POS + Sem - 2 enc Enc 1 Enc 2 I am not working for Tom . PRP PRO VBP NOW RB NOT VBG EXG IN REL NNP PER . NIL Experiments showed that using more than two encoders drastically decreased performance. Therefore, we merge all the linguistic information in a single encoder (see last row of Table 2). 2.3 Neural Architecture We employ a recurrent sequence-to-sequence neural network with attention (Bahdanau et al., 2014) and two bi-LSTM layers, similar to the one used by Van Noord, Abzianidze, Toral, and Bos (2018). However, their model was trained with OpenNMT (Klein et al., 2017), which does not support multiple encoders. Therefore, we switch to the sequence-to-sequence framework implemented in Marian (Junczys-Dowmunt et al., 2018). We use model-type s2s (for a single encoder) or multi-s2s (for multiple encoders). For the latter, this means that the multiple inputs are encoded separately by an identical RNN (without sharing parameters). The encoders share a single decoder, in which the resulting context vectors are concatenated. An attention layer3 is then applied to selectively give more attention to certain parts of the vector (i.e. it can learn that the words thems"
W19-0504,P17-1014,0,0.0454551,". Research in neural machine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders. 1 Introduction Sequence-to-sequence neural networks have shown remarkable performance in semantic parsing (Ling et al., 2016; Jia and Liang, 2016; Konstas et al., 2017; Dong and Lapata, 2018; Liu et al., 2018; Van Noord, Abzianidze, Toral, and Bos, 2018). This architecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni an"
W19-0504,D14-1107,0,0.0285442,"iable names itself are meaningless, the DRS variables are rewritten to a more general representation, using the De Bruijn index (de Bruijn, 1972). In a post-processing step, the original clause structured is restored.1 To include morphological and syntactic information, we apply a lemmatizer, POS-tagger and dependency parser using Stanford CoreNLP (Manning et al., 2014), similar to Sennrich and Haddow (2016) for machine translation. The lemmas and POS-tags are added as a token after each word. For the dependency parse, we add the incoming arc for each word. We also apply the easyCCG parser of Lewis and Steedman (2014), using the supertags.2 Finally, we exploit semantic information by using semantic tags (Bjerva et al., 2016; Abzianidze and Bos, 2017). Semantic tags are language-neutral semantic categories, which get assigned to a word in a similar fashion as part-of-speech tags. Semantic tags are able to express important semantic distinctions, such as negation, modals and types of quantification. We train a semantic tagger with the TnT tagger (Brants, 2000) on the gold and silver standard data in the PMB release. Examples of the input to the model for each source of information are shown in Table 1. 1 2 S"
W19-0504,P17-2031,0,0.0536274,"Missing"
W19-0504,P16-1057,0,0.0359713,"Missing"
W19-0504,P18-1040,0,0.251877,"shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders. 1 Introduction Sequence-to-sequence neural networks have shown remarkable performance in semantic parsing (Ling et al., 2016; Jia and Liang, 2016; Konstas et al., 2017; Dong and Lapata, 2018; Liu et al., 2018; Van Noord, Abzianidze, Toral, and Bos, 2018). This architecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a multi-source appr"
W19-0504,P14-5010,0,0.00241439,"cters. The target DRS is also represented as a sequence of characters, with the exception of DRS operators, thematic roles and DRS variables, which are represented as super characters (Van Noord and Bos, 2017b), i.e. individual tokens. Since the variable names itself are meaningless, the DRS variables are rewritten to a more general representation, using the De Bruijn index (de Bruijn, 1972). In a post-processing step, the original clause structured is restored.1 To include morphological and syntactic information, we apply a lemmatizer, POS-tagger and dependency parser using Stanford CoreNLP (Manning et al., 2014), similar to Sennrich and Haddow (2016) for machine translation. The lemmas and POS-tags are added as a token after each word. For the dependency parse, we add the incoming arc for each word. We also apply the easyCCG parser of Lewis and Steedman (2014), using the supertags.2 Finally, we exploit semantic information by using semantic tags (Bjerva et al., 2016; Abzianidze and Bos, 2017). Semantic tags are language-neutral semantic categories, which get assigned to a word in a similar fashion as part-of-speech tags. Semantic tags are able to express important semantic distinctions, such as negat"
W19-0504,W16-2209,0,0.091739,"and Liang, 2016; Konstas et al., 2017; Dong and Lapata, 2018; Liu et al., 2018; Van Noord, Abzianidze, Toral, and Bos, 2018). This architecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a multi-source approach has proved particularly successful for adding syntax (Currey and Heafield, 2018). The current approaches in neural semantic parsing either include (some) linguistic information in a single encoder (POS-tags in Van Noord and Bos 2017a,b, lemmas in Liu et al. 2018), or use multiple encoders to represent multiple languages rather than linguistic knowledge (Duong et al., 2017; Susanto and Lu, 2017). To our knowledge, we are the first to investigate the potential of exploiting linguistic information in a multi-encoder setup for (neural) sema"
W19-0504,P17-2007,0,0.0309244,"n relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a multi-source approach has proved particularly successful for adding syntax (Currey and Heafield, 2018). The current approaches in neural semantic parsing either include (some) linguistic information in a single encoder (POS-tags in Van Noord and Bos 2017a,b, lemmas in Liu et al. 2018), or use multiple encoders to represent multiple languages rather than linguistic knowledge (Duong et al., 2017; Susanto and Lu, 2017). To our knowledge, we are the first to investigate the potential of exploiting linguistic information in a multi-encoder setup for (neural) semantic parsing. Specifically, the aims of this paper are to investigate (i) whether exploiting linguistic information can improve semantic parsing and (ii) whether it is better to include this linguistic information in the same encoder or in an additional one. We take as baseline the neural semantic parser for Discourse Representation Structures (DRS, Kamp and Reyle, 1993; Van Noord, Abzianidze, Haagsma, and Bos, 2018) developed by Van Noord, Abzianidze"
W19-0504,L18-1267,1,0.669753,"Missing"
W19-0504,Q18-1043,1,0.801546,"Missing"
W19-0504,W17-7306,1,0.922095,"Missing"
W19-0504,N16-1004,0,0.0191766,"Lemma POS-tags Dependency parse Semantic tags CCG supertags I am not working for Tom . I be not work for Tom . PRP VBP RB VBG IN NNP . nsubj aux neg ROOT case nmod punct PRO NOW NOT EXG REL PER NIL NP (S[dcl]NP)/(S[ng]NP) (SNP)(SNP) (S[ng]NP)/PP PP/NP N . There are two ways to add the linguistic information; (1) merging all the information (i.e., input text and linguistic information) in a single encoder, or (2) using multiple encoders (i.e., encoding separately the input text and the linguistic information). Multi-source encoders were initially introduced for multilingual translation (Zoph and Knight, 2016; Firat et al., 2016; Libovick´y and Helcl, 2017), but recently were used to introduce syntactic information to the model (Currey and Heafield, 2018). Table 2 shows examples of how the input is structured for using one or more encoders. Table 2: Example representation when using one or two encoders, for either a single source of information (POS) or multiple sources (POS + Sem) for the sentence I am not working for Tom. For readability purposes we show the word-level instead of character-level representation of the source words here. Source Encoder Representation POS - 1 enc Enc 1 I PRP am VBP"
W19-5208,P17-1050,0,0.0557252,"Missing"
W19-5208,E17-2002,0,0.0355595,"Missing"
W19-5208,W17-0230,0,0.0524671,"sh (original source and translationese target, henceforth referred to as O→T) outperformed systems trained on human translations in the opposite direction (i.e. translationese source and original target, henceforth referred to as T→O). These findings were corroborated by Lembersky (2013), who also adapted phrase tables to translationese, which resulted in further improvements. Lembersky et al. (2012) focused on the monolingual data used to train the language model of a statistical MT system and found that using translated texts led to better translation quality than relying on original texts. Stymne (2017) investigated the effect of translationese on tuning for statistical MT, using data from the WMT 2008–2013 (Bojar et al., 2013) for three language pairs. The results using O→T and T→O tuning texts were compared; the former led to a better length ratio and a better translation, in terms of automatic evaluation metrics. Finally, Toral et al. (2018) investigated the effect of translationese on the Chinese→English (ZH→EN) test set from WMT’s 2017 news shared task. They hypothesized that the sentences originally written in EN are easier to translate than those originally written in ZH, due to the s"
W19-5208,W13-2305,0,0.033984,"nal universal principles of translation, explicitation and normalisation, would also indicate that a ZH text originally written in EN would be easier to translate. In fact, they looked at a human translation and the translation by an MT system (Hassan et al., 2018) and observed that the human translation outperforms the MT system when the input text is written in the original language Data Sets We use the test data from WMT16, WMT17, and WMT18 news translation tasks (newstest2016, newstest2017, and newstest2018) exclusively, because they provide results using the direct assessment (DA) score (Graham et al., 2013, 2014, 2017), which is the metric we will use in our experiments. DA is a crowd-sourced human evaluation metric to determine MT quality. To elaborate, after participants submit their translations produced by their MT systems, a human evaluation campaign is run. This is to assess the translation quality of the systems, and to rank them accordingly. Human evaluation scores are provided via crowdsourcing and/or by participants, using Appraise (Federmann, 2012). Human assessors are asked to rate a given candidate translation by how adequately it expresses the meaning of the corresponding referenc"
W19-5208,W18-6312,1,0.729222,"ich resulted in further improvements. Lembersky et al. (2012) focused on the monolingual data used to train the language model of a statistical MT system and found that using translated texts led to better translation quality than relying on original texts. Stymne (2017) investigated the effect of translationese on tuning for statistical MT, using data from the WMT 2008–2013 (Bojar et al., 2013) for three language pairs. The results using O→T and T→O tuning texts were compared; the former led to a better length ratio and a better translation, in terms of automatic evaluation metrics. Finally, Toral et al. (2018) investigated the effect of translationese on the Chinese→English (ZH→EN) test set from WMT’s 2017 news shared task. They hypothesized that the sentences originally written in EN are easier to translate than those originally written in ZH, due to the simplification principle of translationese, namely that translated sentences tend to be simpler than their original counterparts (Laviosa-Braithwaite, 1998). Two additional universal principles of translation, explicitation and normalisation, would also indicate that a ZH text originally written in EN would be easier to translate. In fact, they lo"
W19-5208,E14-1047,0,0.048513,"Missing"
W19-5208,2009.mtsummit-papers.9,0,0.782775,". Therefore, they concluded that the use of translationese as the source language in test sets distorts the results in favour of MT systems. our research. This is followed by Section 4, Section 5 and Section 6, where we conduct the experiments for RQ1, RQ2 and RQ3, respectively. Finally, Section 7 outlines our conclusions and lines of future work. 2 Related Work 3 There is previous research in the field of MT that has looked at the impact of translationese, mostly on training data, but there are works that have focused also on tuning and testing data sets. The pioneering work on this topic by Kurokawa et al. (2009) showed that French-to-English statistical MT systems trained on human translations from French to English (original source and translationese target, henceforth referred to as O→T) outperformed systems trained on human translations in the opposite direction (i.e. translationese source and original target, henceforth referred to as T→O). These findings were corroborated by Lembersky (2013), who also adapted phrase tables to translationese, which resulted in further improvements. Lembersky et al. (2012) focused on the monolingual data used to train the language model of a statistical MT system"
W19-5208,J12-4004,0,0.0686425,"that have focused also on tuning and testing data sets. The pioneering work on this topic by Kurokawa et al. (2009) showed that French-to-English statistical MT systems trained on human translations from French to English (original source and translationese target, henceforth referred to as O→T) outperformed systems trained on human translations in the opposite direction (i.e. translationese source and original target, henceforth referred to as T→O). These findings were corroborated by Lembersky (2013), who also adapted phrase tables to translationese, which resulted in further improvements. Lembersky et al. (2012) focused on the monolingual data used to train the language model of a statistical MT system and found that using translated texts led to better translation quality than relying on original texts. Stymne (2017) investigated the effect of translationese on tuning for statistical MT, using data from the WMT 2008–2013 (Bojar et al., 2013) for three language pairs. The results using O→T and T→O tuning texts were compared; the former led to a better length ratio and a better translation, in terms of automatic evaluation metrics. Finally, Toral et al. (2018) investigated the effect of translationese"
W19-5208,D11-1034,0,\N,Missing
W19-5208,W13-2201,0,\N,Missing
W19-5208,W17-4717,0,\N,Missing
W19-5208,W18-6401,0,\N,Missing
W19-5343,E17-2068,0,0.0171412,"Missing"
W19-5343,P18-4020,0,0.0354066,"Missing"
W19-5343,P07-2045,0,0.0197716,"Missing"
W19-5343,washington-etal-2014-finite,0,0.0690062,"Missing"
W19-5343,D18-1103,0,0.0541941,"Missing"
W19-5343,P02-1040,0,0.105862,"Missing"
W19-5343,W15-3049,0,0.110562,"Missing"
W19-5343,W16-2322,1,0.871219,"Missing"
W19-5343,P16-1009,0,0.0573485,"Missing"
W19-5343,P16-1162,0,0.322619,"stems submitted by the University of Groningen to the WMT 2019 news translation task.1 We participated in the English↔Kazakh (henceforth referred to as EN↔KK) constrained tasks. Because of the inherent characteristics of this language pair and the current state-of-the-art of related techniques, we focused on two main research questions (RQs): • RQ1. Does morphological segmentation help? Recent research in NMT for agglutinative languages found that morphological segmentation outperforms the most widely used segmentation technique, byte-pair encoding (BPE, using character sequence frequencies) (Sennrich et al., 2016). Rule-based segmentation improved English-to-Finnish translation (Sánchez-Cartagena and Toral, The rest of the paper is organized as follows. Section 2 describes the datasets and tools used. Then Section 3 details our experiments. Finally, Section 4 outlines our conclusions and plans for future work. 2 Datasets and Tools We preprocessed all the corpora used (training, validation and test sets) with scripts from the 1 http://www.statmt.org/wmt19/ translation-task.html 386 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 386–392 c Flo"
W19-5343,W15-3031,0,0.058712,"Missing"
W19-6627,avramidis-etal-2014-taraxu,0,0.160434,"(the most frequent ones) and that this is the case also, to some extent, in PEs. HTs and PEs were also compared in terms of number of errors, which were found to be comparable, corroborating the findings of the literature covered at the beginning of this section. Our contribution falls into this research line, to which we contribute a computational study whose analyses are chosen to align to translation universals and laws of translation and that covers multiple languages and domains. 3 part-of-speech sequences (Section 3.5). 3.1 We make use of three datasets in all our experiments: Tarax¨u (Avramidis et al., 2014), IWSLT (Cettolo et al., 2015; Mauro et al., 2016) and Microsoft “Human Parity” (Hassan et al., 2018), henceforth referred to as MS. These datasets cover five different translation directions that involve five languages:7 English↔German, English→French, Spanish→German and Chinese→English. In addition, this choice of datasets allows us to include a longitudinal aspect into the analyses since there are state-of-the-art MT systems from almost one decade ago (in Tarax¨u), from three and four years ago (IWSLT) and from just one year ago (MS). Table 1 shows detailed information about each dataset, n"
W19-6627,D16-1025,0,0.0973866,"xplained in Section 3.1, that the MT systems in the MT and PE conditions are different in this dataset, with the one in the MT condition being substantially better (Hassan et al., 2018). Overall, we interpret these results as MT being the translation type that contains the most interference in terms of PoS sequences, followed by PE. We now look at the PE and MT results under different MT paradigms. Comparing SMT and NMT, the results indicate that the latter has less interference, both in PE and MT conditions. This corroborates earlier research that compared SMT and NMT in terms of reordering (Bentivogli et al., 2016). We do not find clear trends when comparing SMT and RBMT though. 4 Conclusions and Future Work We have carried out a set of computational analyses on three datasets that contain five translation directions with the aim of finding out whether postedited translations (PEs) exhibit different phenomena than human translations from scratch (HTs). In other words, whether there is evidence of the Proceedings of MT Summit XVII, volume 1 existence of post-editese. The analyses conducted measure aspects related to translation universals and laws of translation, namely simplification, normalisation and"
W19-6627,2015.iwslt-evaluation.1,0,0.0645759,"Missing"
W19-6627,K17-3009,0,0.0226961,"Missing"
W19-6627,L16-1680,0,0.0230842,"Missing"
W19-6627,E17-1100,1,0.903818,"Missing"
W19-6627,W18-6312,1,0.910742,"Missing"
W19-6627,W16-3401,0,0.198058,"Missing"
W19-6627,2009.mtsummit-btm.7,0,\N,Missing
