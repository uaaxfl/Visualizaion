2020.acl-main.501,P17-1171,0,0.0646925,"multi-instance learning (Surdeanu et al., 2012), assuming at least one supporting evidence (Hoffmann et al., 2011), integration of label-specific priors (Ritter et al., 2013), and adaption to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-supervised QA setting due to the noise in the heuristics-based span labels (Chen et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017). One line of work jointly learns RC and evidence ranking using either a pipeline system (Wang et al., 2018a; Lee et al., 2018; Kratzwald and Feuerriegel, 2018) or an end-to-end model (Lee et al., 2019). Another line of work focuses on improving distantly-supervised RC models by developing learning methods and model architectures that can better use noisy labels. Clark and Gardner (2018) propose a paragraph-pair ranking objective, which has components of both our H2-P and H3-D position-based formulations. They don’t explore multiple"
2020.acl-main.501,P18-1078,0,0.317345,"ons perform better. We therefore study exhaustive combinations of probability space, distant supervision assumption, and training and inference methods. 5 Experiments 5.1 Data and Implementation Two datasets are used in this paper: TriviaQA (Joshi et al., 2017) in its Wikipedia formulation, and NarrativeQA (summaries setting) (Koˇcisk´y et al., 2018). Using the same preprocessing as 5 P For inference with marginal ( ) scoring, we use an approximate scheme where we only aggregate probabilities of candidates strings generated from a 20-best list of begin/end answer positions for each paragraph. Clark and Gardner (2018) for TriviaQA-Wiki6 , we only keep the top 8 ranked paragraphs up to 400 tokens for each document-question pair for both training and evaluation. Following Min et al. (2019), for NarrativeQA we define the possible answer string sets A using Rouge-L (Lin, 2004) similarity with crouwdsourced abstractive answer strings. We use identical data preprocessing and the evaluation script provided by the authors. In this work, we use the BERT-base model for text encoding and train our model with the default configuration as described in (Devlin et al., 2019), fine-tuning all parameters. We fine-tune for"
2020.acl-main.501,N19-1423,1,0.560444,"ons. While distant supervision reduces the annotation cost, increased coverage often comes with increased noise (e.g., expanding entity answer strings with aliases improves coverage but also increases noise). Even for fixed document-level distant supervision in the form of a set of answers A, different interpretations of the partial supervision lead to different points in the coverage/noise space and their relative performance is not well understood. This work systematically studies methods for learning and inference with document-level distantly supervised extractive QA models. Using a BERT (Devlin et al., 2019) joint question-passage encoder, we study the compound impact of: • Probability space (§2): ways to define the model’s probability space based on independent paragraphs or whole documents. • Distant supervision assumptions (§3): ways to translate the supervision from possible strings A to possible locations of answer mentions in the document. • Optimization and inference (§4): ways to define corresponding training objectives (e.g. Hard EM as in Min et al. (2019) vs. Maximum Marginal Likelihood) and make answer string predictions during inference (Viterbi or marginal inference). We show that th"
2020.acl-main.501,P16-1086,0,0.0752964,"Missing"
2020.acl-main.501,Q18-1023,0,0.0895334,"Missing"
2020.acl-main.501,D18-1055,0,0.040217,"Missing"
2020.acl-main.501,D18-1053,0,0.0249577,"to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-supervised QA setting due to the noise in the heuristics-based span labels (Chen et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017). One line of work jointly learns RC and evidence ranking using either a pipeline system (Wang et al., 2018a; Lee et al., 2018; Kratzwald and Feuerriegel, 2018) or an end-to-end model (Lee et al., 2019). Another line of work focuses on improving distantly-supervised RC models by developing learning methods and model architectures that can better use noisy labels. Clark and Gardner (2018) propose a paragraph-pair ranking objective, which has components of both our H2-P and H3-D position-based formulations. They don’t explore multiple inference methods or combinations of objectives and use less powerful representations. In (Lin et al., 2018), a coarse-to-fine model is proposed to handle label noise by aggregating infor"
2020.acl-main.501,P19-1612,1,0.942839,"s consider each paragraph in the document independently, whereas document models integrate some dependencies among paragraphs. To define the model, we need to specify the probability space, consisting of a set of possible outcomes and a way to assign probabilities to individual outcomes. For extractive QA, the probability space outcomes consist of token positions of answer mention spans. The overall model architecture is shown in Fig. 2. We use BERT (Devlin et al., 2019) to derive representations of document tokens. As is standard in state-of-the-art extractive QA models (Devlin et al., 2019; Lee et al., 2019; Min et al., 2019), the BERT model is used to encode a pair of a given question with one paragraph from a given document into neural text representations. These representations are then used to 2 The code is available at https://github.com/ hao-cheng/ds_doc_qa 5658 define scores/probabilities of possible answer begin and end positions, which are in turn used to define probabilities over possible answer spans. Then the answer string probabilities can be defined as the aggregation over all possible answer spans/mentions. In the following, we show that paragraph-level and document-level models d"
2020.acl-main.501,W04-1013,0,0.0436926,"edia formulation, and NarrativeQA (summaries setting) (Koˇcisk´y et al., 2018). Using the same preprocessing as 5 P For inference with marginal ( ) scoring, we use an approximate scheme where we only aggregate probabilities of candidates strings generated from a 20-best list of begin/end answer positions for each paragraph. Clark and Gardner (2018) for TriviaQA-Wiki6 , we only keep the top 8 ranked paragraphs up to 400 tokens for each document-question pair for both training and evaluation. Following Min et al. (2019), for NarrativeQA we define the possible answer string sets A using Rouge-L (Lin, 2004) similarity with crouwdsourced abstractive answer strings. We use identical data preprocessing and the evaluation script provided by the authors. In this work, we use the BERT-base model for text encoding and train our model with the default configuration as described in (Devlin et al., 2019), fine-tuning all parameters. We fine-tune for 3 epochs on TriviaQA and 2 epochs on NarrativeQA. 5.2 Optimization and Inference for Latent Variable Models Here we look at the cross product of optimization (HardEM vs MML) and inference (Max vs Sum) for all distant supervision assumptions that result in mode"
2020.acl-main.501,P18-1161,0,0.0509609,"rns RC and evidence ranking using either a pipeline system (Wang et al., 2018a; Lee et al., 2018; Kratzwald and Feuerriegel, 2018) or an end-to-end model (Lee et al., 2019). Another line of work focuses on improving distantly-supervised RC models by developing learning methods and model architectures that can better use noisy labels. Clark and Gardner (2018) propose a paragraph-pair ranking objective, which has components of both our H2-P and H3-D position-based formulations. They don’t explore multiple inference methods or combinations of objectives and use less powerful representations. In (Lin et al., 2018), a coarse-to-fine model is proposed to handle label noise by aggregating information from relevant paragraphs and then extracting answers from selected ones. Min et al. (2019) propose a hard EM learning scheme which we included in our experimental evaluation. Our work focuses on examining probabilistic assumptions for document-level extractive QA. We provide a unified view of multiple methods in terms of their probability space and distant supervision assumptions and evaluate the impact of their components in combination with optimization and inference methods. To the best of our knowledge, t"
2020.acl-main.501,P11-1055,0,0.0509056,"e Qll likely contains more incorrect answer strings (false aliases). We can observe that the improvement is more significant for these noisier subsets, suggesting document-level modeling is crucial for handling both types of label noise. 6 Related Work Distant supervision has been successfully used for decades for information extraction tasks such as entity tagging and relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009). Several ways have been proposed to learn with DS, e.g., multi-label multi-instance learning (Surdeanu et al., 2012), assuming at least one supporting evidence (Hoffmann et al., 2011), integration of label-specific priors (Ritter et al., 2013), and adaption to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-supervised QA setting due to the noise in the heuristics-based span labels (Chen et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017). One line of work jointly learns R"
2020.acl-main.501,D19-1284,0,0.061792,"ematically studies methods for learning and inference with document-level distantly supervised extractive QA models. Using a BERT (Devlin et al., 2019) joint question-passage encoder, we study the compound impact of: • Probability space (§2): ways to define the model’s probability space based on independent paragraphs or whole documents. • Distant supervision assumptions (§3): ways to translate the supervision from possible strings A to possible locations of answer mentions in the document. • Optimization and inference (§4): ways to define corresponding training objectives (e.g. Hard EM as in Min et al. (2019) vs. Maximum Marginal Likelihood) and make answer string predictions during inference (Viterbi or marginal inference). We show that the choice of probability space puts constraints on the distant supervision assumptions that can be captured, and that all three choices interact, leading to large differences in performance. Specifically, we provide a framework for understanding different distant supervision assumptions and the corresponding trade-off among the coverage, quality and strength of distant supervision signal. The best configuration depends on the properties of the possible annotation"
2020.acl-main.501,P17-1147,0,0.611651,"lves? Answer: in the spring at mount helicon mount helicon : { in the spring at mount helicon, mount helicon } P1: The play begins with three pages … P2: The courtiers … She sentences them to make reparation and to purify themselves by bathing in the spring at mount helicon. The figure of Actaeon in the play may represent ... Figure 1: TriviaQA and NarrativeQA examples. In the TrivIntroduction Distant supervision assumptions have enabled the creation of large-scale datasets that can be used to train fine-grained extractive short answer question answering (QA) systems. One example is TriviaQA (Joshi et al., 2017). There the authors utilized a pre-existing set of Trivia questionanswer string pairs and coupled them with relevant documents, such that, with high likelihood, the documents support answering the questions (see Fig. 1 for an illustration). Another example is the NarrativeQA dataset (Koˇcisk´y et al., 2018), where crowd-sourced abstractive answer strings were used to weakly supervise answer mentions in the text of movie scripts or their summaries. In this work, we focus on the setting of documentlevel extractive QA, where distant supervision is specified as a set A of answer strings for an inp"
2020.acl-main.501,P17-2081,0,0.0255796,"me parameters for the two objectives and the multiobjective formulation does not have more parameters and is no less efficient than the individual models. Second, we use external clean supervision from SQUAD 2.0 (Rajpurkar et al., 2018) to train the BERT-based QA model for 2 epochs. This model matches the P probability space and is able to detect both NULL and extractive answer spans. The resulting network is used to initialize the models for TriviaQA and NarrativeQA. The results are shown in Table 4. It is not surprising that using external clean supervision improves model performance (e.g. (Min et al., 2017)). We note that, interestingly, this external supervision narrows the performance gap between paragraph-level and document-level models, and reduces the difference between the two inference methods. Compared with their single-objective components, multi-objective formulations improve performance on both TriviaQA and NarrativeQA. 5.5 F1 NarrativeQA Summary Multi-objective Par + Doc Verified Test Set Evaluation Table 5 reports test set results on TriviaQA and NarrativeQA for our best models, in comparison to recent state-of-art (SOTA) models. For TriviaQA, we report F1 and EM scores on the full"
2020.acl-main.501,P09-1113,0,0.102905,"nce separately on each subset, as shown in Table 6. In general, we expect Qsl and Qll to be noisier due to the larger I, where Qsl potentially includes many irrelevant mentions while Qll likely contains more incorrect answer strings (false aliases). We can observe that the improvement is more significant for these noisier subsets, suggesting document-level modeling is crucial for handling both types of label noise. 6 Related Work Distant supervision has been successfully used for decades for information extraction tasks such as entity tagging and relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009). Several ways have been proposed to learn with DS, e.g., multi-label multi-instance learning (Surdeanu et al., 2012), assuming at least one supporting evidence (Hoffmann et al., 2011), integration of label-specific priors (Ritter et al., 2013), and adaption to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-superv"
2020.acl-main.501,P19-1220,0,0.0147623,"paragraph-level and document-level models, and reduces the difference between the two inference methods. Compared with their single-objective components, multi-objective formulations improve performance on both TriviaQA and NarrativeQA. 5.5 F1 NarrativeQA Summary Multi-objective Par + Doc Verified Test Set Evaluation Table 5 reports test set results on TriviaQA and NarrativeQA for our best models, in comparison to recent state-of-art (SOTA) models. For TriviaQA, we report F1 and EM scores on the full test set and the verified subset. For NarrativeQA, RougeOurs (H2-P+H2-D) w/o SQUAD 62.9 60.5 (Nishida et al., 2019) w/o external data 59.9 54.7 (Min et al., 2019) 58.8 Table 5: Test set results on TriviaQA Wiki and NarrativeQA Summaries. “w/o SQUAD” refers to our best model without pretraining on SQUAD 2.0. “w/o external data” refers to the model from (Nishida et al., 2019) without using MS MARCO data (Bajaj et al., 2018). L scores are reported. Compared to recent TriviaQA SOTA (Wang et al., 2018b), our best models achieve 4.9 F1 and 5.5 EM improvement on the full test set, and 6.8 F1 and 7.4 EM improvement on the verified subset. On the NarrativeQA test set, we improve Rouge-L by 3.0 over (Nishida et al.,"
2020.acl-main.501,P18-2124,0,0.190125,"obabilities (?? , ?? ) … (“Joan Rivers” |?? ) BERT BERT ? ?? … … … … Contextualized Representation … ? ?? … … Figure 2: The document-level QA model as used for test-time inference. The lower part is a BERT-based paragraph-level answer scoring component, and the upper part illustrates the probability aggregation across answer spans sharing the same answer string. Ξ refers to either a sum or a max operator. In the given example, “John Rivers” is derived from two paragraphs. tasks. Results are further strengthened by transfer learning from fully labeled short-answer extraction data in SQuAD 2.0 (Rajpurkar et al., 2018), leading to a final state-of-the-art performance of 76.3 F1 on TriviaQA-Wiki and 62.9 on the NarrativeQA summaries task.2 2 Probability Space Here, we first formalize both paragraph-level and document-level models, which have been previously used for document-level extractive QA. Typically, paragraph-level models consider each paragraph in the document independently, whereas document models integrate some dependencies among paragraphs. To define the model, we need to specify the probability space, consisting of a set of possible outcomes and a way to assign probabilities to individual outcome"
2020.acl-main.501,Q13-1030,0,0.0175032,"ases). We can observe that the improvement is more significant for these noisier subsets, suggesting document-level modeling is crucial for handling both types of label noise. 6 Related Work Distant supervision has been successfully used for decades for information extraction tasks such as entity tagging and relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009). Several ways have been proposed to learn with DS, e.g., multi-label multi-instance learning (Surdeanu et al., 2012), assuming at least one supporting evidence (Hoffmann et al., 2011), integration of label-specific priors (Ritter et al., 2013), and adaption to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-supervised QA setting due to the noise in the heuristics-based span labels (Chen et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017). One line of work jointly learns RC and evidence ranking using either a pipeline system (Wang"
2020.acl-main.501,D12-1042,0,0.0302617,"ger I, where Qsl potentially includes many irrelevant mentions while Qll likely contains more incorrect answer strings (false aliases). We can observe that the improvement is more significant for these noisier subsets, suggesting document-level modeling is crucial for handling both types of label noise. 6 Related Work Distant supervision has been successfully used for decades for information extraction tasks such as entity tagging and relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009). Several ways have been proposed to learn with DS, e.g., multi-label multi-instance learning (Surdeanu et al., 2012), assuming at least one supporting evidence (Hoffmann et al., 2011), integration of label-specific priors (Ritter et al., 2013), and adaption to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-supervised QA setting due to the noise in the heuristics-based span labels (Chen et al., 2017; Joshi et al., 2017; Dunn et"
2020.acl-main.501,P18-1158,0,0.220499,"hods to further improve weakly supervised QA models. First, we combine two distant supervision objectives in a multitask manner, i.e. H2-P and H3-D for TriviaQA, and H2-P and H2-D for NarrativeQA, chosen based on the results in §5.3. H2 objectives have higher coverage than H3 while being more susceptible 5663 Objective Clean Infer TriviaQA F1 NarrativeQA EM Rouge-L TriviaQA Wiki Full Single-objective X Max Sum 71.9 73.0 67.7 69.0 59.2 57.8 X Max Sum 74.2 74.9 70.1 70.9 61.7 61.7 X Max Sum 75.1 75.3 70.6 70.8 60.1 59.9 X Max Sum 75.5 75.5 70.8 70.9 62.8 62.9 Par Doc Ours (H2-P+H3-D) w/o SQUAD (Wang et al., 2018b) (Clark and Gardner, 2018) (Min et al., 2019) X Max Sum 75.6 75.9 71.2 71.6 60.5 60.5 X Max Sum 75.8 76.2 71.2 71.7 63.0 63.1 EM F1 EM 76.3 75.7 71.4 68.9 67.1 72.1 71.6 66.6 64.0 – 85.5 83.6 78.7 72.9 – 82.2 79.6 74.8 68.0 – Rouge-L Table 4: Dev set results comparing multi-objectives and clean supervison. X indicates the QA model is pre-trained on SQUAD. to noise. Paragraph-level models have the advantage of learning to score irrelevant paragraphs (via NULL outcomes). Note that we use the same parameters for the two objectives and the multiobjective formulation does not have more parameters"
2020.acl-main.501,D19-1397,0,0.01441,"these noisier subsets, suggesting document-level modeling is crucial for handling both types of label noise. 6 Related Work Distant supervision has been successfully used for decades for information extraction tasks such as entity tagging and relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009). Several ways have been proposed to learn with DS, e.g., multi-label multi-instance learning (Surdeanu et al., 2012), assuming at least one supporting evidence (Hoffmann et al., 2011), integration of label-specific priors (Ritter et al., 2013), and adaption to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-supervised QA setting due to the noise in the heuristics-based span labels (Chen et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017). One line of work jointly learns RC and evidence ranking using either a pipeline system (Wang et al., 2018a; Lee et al., 2018; Kratzwald and Feuerriegel, 201"
2020.acl-main.742,D11-1039,0,0.0194607,"arsing is code generation (Oda et al., 2015; Ling et al., 2016; Yin et al., 2018; Lin et al., 2018; Iyer et al., 2018). While our experiments are performed on English-langauge data, a limited amount of existing work has explored semantic parsing in languages besides English (Wong and Mooney, 2006; Min et al., 2019). Annotating SQL queries for new domains can be expensive. Several prior works present approaches to reduce this cost, for example by having crowdworkers paraphrase generated examples (Wang et al., 2015; Zhong et al., 2017), give feedback (Iyer et al., 2017), interact with a system (Artzi and Zettlemoyer, 2011; Thomason et al., 2015; Labutov et al., 2018), or a combination (Herzig and Berant, 2019). Research in SSP and code generation has led to innovations including constrained decoding and grammar-based decoding (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Lin et al., 2019). SSP has also been studied alongside additional generalization challenges, including to new compositional structures (Finegan-Dollak et al., 2018) and with additional context (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018). Recent works evaluating in the XSP setting have explore"
2020.acl-main.742,Q13-1005,0,0.0365863,"to database contents at model inference time (Yu et al., 2018). Preprocessing steps that perform database look-ups are unavailable at inference time. Instead, the model only has access to the database schema for each evaluation example. This setting requires additional generalization, where the model must be able to map unfamiliar entities to columns in domains unseen during training. Other Related Work Semantic parsing has been widely studied for tasks including sentence understanding (Zettlemoyer and Collins, 2005, 2007; Banarescu et al., 2013), instruction following (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Long et al., 2016; Givoli and Reichart, 2019), and knowledge base querying (Popescu et al., 2004; Poon, 2013; Iyer et al., 2017). Related to the task of semantic parsing is code generation (Oda et al., 2015; Ling et al., 2016; Yin et al., 2018; Lin et al., 2018; Iyer et al., 2018). While our experiments are performed on English-langauge data, a limited amount of existing work has explored semantic parsing in languages besides English (Wong and Mooney, 2006; Min et al., 2019). Annotating SQL queries for new domains can be expensive. Several prior works present approaches to reduce this cost,"
2020.acl-main.742,W13-2322,0,0.0101242,"like SSP, prior work in XSP does not assume that the system has access to database contents at model inference time (Yu et al., 2018). Preprocessing steps that perform database look-ups are unavailable at inference time. Instead, the model only has access to the database schema for each evaluation example. This setting requires additional generalization, where the model must be able to map unfamiliar entities to columns in domains unseen during training. Other Related Work Semantic parsing has been widely studied for tasks including sentence understanding (Zettlemoyer and Collins, 2005, 2007; Banarescu et al., 2013), instruction following (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Long et al., 2016; Givoli and Reichart, 2019), and knowledge base querying (Popescu et al., 2004; Poon, 2013; Iyer et al., 2017). Related to the task of semantic parsing is code generation (Oda et al., 2015; Ling et al., 2016; Yin et al., 2018; Lin et al., 2018; Iyer et al., 2018). While our experiments are performed on English-langauge data, a limited amount of existing work has explored semantic parsing in languages besides English (Wong and Mooney, 2006; Min et al., 2019). Annotating SQL queries for new domains can"
2020.acl-main.742,P16-1004,0,0.0158555,"}N l=1 grounded in database D. The evaluation data contains M unseen pairs of utterances and SQL queries {x(l) , y (l) }M l=1 , also grounded in D. SSP has been studied using a number of datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994) and GeoQuery (Zelle and Mooney, 1996). Many prior approaches in SSP assume access to database contents at inference time. At test time, this allows the system to resolve the columns containing novel entities by performing a database look-up; for example, by labeling entity mentions in the input utterance with the columns in which they appear (Dong and Lapata, 2016; Iyer et al., 2017; Suhr et al., 2018). Cross-database Semantic Parsing (XSP) In the XSP setting, examples from the evaluation databases are not seen at training time (Yu et al., 2018, 2019b,a). Previously, the cross-domain semantic parsing task focused mostly on databases consisting of a single table (Pasupat and Liang, 2015; Iyyer et al., 2017; Zhong et al., 2017). However, the cross-database setting requires generaliz8373 ing to unseen domains and novel database schemas. In XSP, the N training examples are (l) {x(l) , y (l) , Di }N l=1 and the M evaluation ex(l) (l) amples are {x , y (l) ,"
2020.acl-main.742,P18-1033,0,0.484593,"mple by having crowdworkers paraphrase generated examples (Wang et al., 2015; Zhong et al., 2017), give feedback (Iyer et al., 2017), interact with a system (Artzi and Zettlemoyer, 2011; Thomason et al., 2015; Labutov et al., 2018), or a combination (Herzig and Berant, 2019). Research in SSP and code generation has led to innovations including constrained decoding and grammar-based decoding (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Lin et al., 2019). SSP has also been studied alongside additional generalization challenges, including to new compositional structures (Finegan-Dollak et al., 2018) and with additional context (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018). Recent works evaluating in the XSP setting have explored methods of jointly embedding an utterance and the database schema (Shaw et al., 2019; Bogin et al., 2019a), interactive learning (Yao et al., 2019), and using intermediate output representations and new inference methods (Herzig and Berant, 2018; Guo et al., 2019; Zhang et al., 2019; Bogin et al., 2019b; Lin et al., 2019). We incorporate several such methods proposed into our proposed system. 3 Evaluating on Re-purposed Data We propose t"
2020.acl-main.742,P19-1438,0,0.016202,"et al., 2018). Preprocessing steps that perform database look-ups are unavailable at inference time. Instead, the model only has access to the database schema for each evaluation example. This setting requires additional generalization, where the model must be able to map unfamiliar entities to columns in domains unseen during training. Other Related Work Semantic parsing has been widely studied for tasks including sentence understanding (Zettlemoyer and Collins, 2005, 2007; Banarescu et al., 2013), instruction following (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Long et al., 2016; Givoli and Reichart, 2019), and knowledge base querying (Popescu et al., 2004; Poon, 2013; Iyer et al., 2017). Related to the task of semantic parsing is code generation (Oda et al., 2015; Ling et al., 2016; Yin et al., 2018; Lin et al., 2018; Iyer et al., 2018). While our experiments are performed on English-langauge data, a limited amount of existing work has explored semantic parsing in languages besides English (Wong and Mooney, 2006; Min et al., 2019). Annotating SQL queries for new domains can be expensive. Several prior works present approaches to reduce this cost, for example by having crowdworkers paraphrase g"
2020.acl-main.742,P16-1014,0,0.0166567,"the highest-probability, syntactically correct prediction. We impose a maximum execution time of 45 seconds for predictions. More details on the model, learning, and evaluation setup are available in Appendix B. 5.1 Generalization Strategies While using pre-trained language models can help encode natural language text, we need other strategies to reason jointly about the language and the database schema in completely unseen domains. We focus on generalizing to domain-specific language and novel database structures. Value Copying Similar to previous work (Jia and Liang, 2016; Gu et al., 2016; Gulcehre et al., 2016; See et al., 2017), we use a copy mechanism in the decoder. At each output step, the decoder generates a distribution over possible actions, including selecting a symbol from the output vocabulary, and copying a token from the input x. We only allow copying of certain token types, and mask out invalid copying actions, including independent wordpieces from u and TAB and column-type tokens. For table and column tokens, the name of 7 To discourage over-fitting to an arbitrary ordering of schema elements, we duplicate each Spider training example seven times with randomly permuted orderings. Dupl"
2020.acl-main.742,H94-1010,0,0.876146,"ied in the setting where in-domain training data is available, and instead use them as additional evaluation data for XSP systems instead. We build a system that performs well on Spider, and find that it struggles to generalize to our re-purposed set. Our setup uncovers several generalization challenges for cross-database semantic parsing, demonstrating the need to use and develop diverse training and evaluation datasets. 1 GeoQuery (Zelle and Mooney, 1996) NL: How many people live in mississippi? SQL: select population from state where state name = ‘mississippi’; ATIS (Hemphill et al., 1990; Dahl et al., 1994) NL: Flights from Phoenix to Milwaukee SQL: select distinct T1.flight id from airport service as T2, airport service as T3, city as T4, city as T5, flight as T1 where T4.city code = T2.city code and T4.city name = ‘Phoenix’ and T5.city code = T3.city code and T5.city name = ‘Milwaukee’ and T1.from airport = T2.airport code and T1.to airport = T3.airport code; Spider (Yu et al., 2018) NL: List the emails of the professionals who live in the state of Hawaii or the state of Wisconsin. SQL: select email address from professionals where state = ‘Hawaii’ or state = ‘Wisconsin’; Introduction Semantic"
2020.acl-main.742,N19-1423,1,0.286286,"name. Each ci,j is a serialization of a column Ci,j , where ci,j = CTi,j + C i,j . CTi,j is a token denoting the type of the column’s contents as provided by the database schema, for example numerical or text. C i,j is the tokenization of the column’s name. The ordering of table schemas in s and table columns in each ti is arbitrary.7 The input to the encoder is the concatenation of the query wordpieces and the serialized schema, represented as the sequence of tokens x = hCLSi + u + hSEPi + s. The inputs to the encoder are embedded and passed to a pretrained Transformer encoder such as BERT (Devlin et al., 2019). The decoder is an autoregressive Transformer decoder (Vaswani et al., 2017) that attends over the outputs of the encoder and the generated prefix. We use a training set {x(l) , y (l) , S (l) }N l=1 consisting of pairs of natural language utterances, gold SQL queries, and database schemas. We train the encoder and decoder end-to-end, minimizing the token-level cross-entropy loss of the gold query y (l) . We update the parameters of the pre-trained encoder during training. For training data we use training sets developed for XSP. Importantly, to ensure we are evaluating the cross-database sett"
2020.acl-main.742,P19-1444,0,0.406567,"; Krishnamurthy et al., 2017; Lin et al., 2019). SSP has also been studied alongside additional generalization challenges, including to new compositional structures (Finegan-Dollak et al., 2018) and with additional context (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018). Recent works evaluating in the XSP setting have explored methods of jointly embedding an utterance and the database schema (Shaw et al., 2019; Bogin et al., 2019a), interactive learning (Yao et al., 2019), and using intermediate output representations and new inference methods (Herzig and Berant, 2018; Guo et al., 2019; Zhang et al., 2019; Bogin et al., 2019b; Lin et al., 2019). We incorporate several such methods proposed into our proposed system. 3 Evaluating on Re-purposed Data We propose to study the task of XSP by training on datasets designed for XSP, and evaluating on datasets originally designed for SSP. In our full model, we use both the Spider1 (Yu et al., 2018) and WikiSQL (Zhong et al., 2017) datasets for training. For evaluation, in addition to the Spider development set,2 we use eight English-language SSP datasets curated by Finegan-Dollak et al. (2018) covering a variety of domains, for examp"
2020.acl-main.742,H90-1021,0,0.906107,"Missing"
2020.acl-main.742,D18-1190,0,0.0295821,"016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Lin et al., 2019). SSP has also been studied alongside additional generalization challenges, including to new compositional structures (Finegan-Dollak et al., 2018) and with additional context (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018). Recent works evaluating in the XSP setting have explored methods of jointly embedding an utterance and the database schema (Shaw et al., 2019; Bogin et al., 2019a), interactive learning (Yao et al., 2019), and using intermediate output representations and new inference methods (Herzig and Berant, 2018; Guo et al., 2019; Zhang et al., 2019; Bogin et al., 2019b; Lin et al., 2019). We incorporate several such methods proposed into our proposed system. 3 Evaluating on Re-purposed Data We propose to study the task of XSP by training on datasets designed for XSP, and evaluating on datasets originally designed for SSP. In our full model, we use both the Spider1 (Yu et al., 2018) and WikiSQL (Zhong et al., 2017) datasets for training. For evaluation, in addition to the Spider development set,2 we use eight English-language SSP datasets curated by Finegan-Dollak et al. (2018) covering a variety of"
2020.acl-main.742,D19-1394,0,0.0130239,"2018; Iyer et al., 2018). While our experiments are performed on English-langauge data, a limited amount of existing work has explored semantic parsing in languages besides English (Wong and Mooney, 2006; Min et al., 2019). Annotating SQL queries for new domains can be expensive. Several prior works present approaches to reduce this cost, for example by having crowdworkers paraphrase generated examples (Wang et al., 2015; Zhong et al., 2017), give feedback (Iyer et al., 2017), interact with a system (Artzi and Zettlemoyer, 2011; Thomason et al., 2015; Labutov et al., 2018), or a combination (Herzig and Berant, 2019). Research in SSP and code generation has led to innovations including constrained decoding and grammar-based decoding (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Lin et al., 2019). SSP has also been studied alongside additional generalization challenges, including to new compositional structures (Finegan-Dollak et al., 2018) and with additional context (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018). Recent works evaluating in the XSP setting have explored methods of jointly embedding an utterance and the database schema (Shaw et al., 2019; Bo"
2020.acl-main.742,P17-1089,0,0.467568,"abase D. The evaluation data contains M unseen pairs of utterances and SQL queries {x(l) , y (l) }M l=1 , also grounded in D. SSP has been studied using a number of datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994) and GeoQuery (Zelle and Mooney, 1996). Many prior approaches in SSP assume access to database contents at inference time. At test time, this allows the system to resolve the columns containing novel entities by performing a database look-up; for example, by labeling entity mentions in the input utterance with the columns in which they appear (Dong and Lapata, 2016; Iyer et al., 2017; Suhr et al., 2018). Cross-database Semantic Parsing (XSP) In the XSP setting, examples from the evaluation databases are not seen at training time (Yu et al., 2018, 2019b,a). Previously, the cross-domain semantic parsing task focused mostly on databases consisting of a single table (Pasupat and Liang, 2015; Iyyer et al., 2017; Zhong et al., 2017). However, the cross-database setting requires generaliz8373 ing to unseen domains and novel database schemas. In XSP, the N training examples are (l) {x(l) , y (l) , Di }N l=1 and the M evaluation ex(l) (l) amples are {x , y (l) , Dj }N l=1 , where"
2020.acl-main.742,D18-1192,0,0.013727,"the model must be able to map unfamiliar entities to columns in domains unseen during training. Other Related Work Semantic parsing has been widely studied for tasks including sentence understanding (Zettlemoyer and Collins, 2005, 2007; Banarescu et al., 2013), instruction following (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Long et al., 2016; Givoli and Reichart, 2019), and knowledge base querying (Popescu et al., 2004; Poon, 2013; Iyer et al., 2017). Related to the task of semantic parsing is code generation (Oda et al., 2015; Ling et al., 2016; Yin et al., 2018; Lin et al., 2018; Iyer et al., 2018). While our experiments are performed on English-langauge data, a limited amount of existing work has explored semantic parsing in languages besides English (Wong and Mooney, 2006; Min et al., 2019). Annotating SQL queries for new domains can be expensive. Several prior works present approaches to reduce this cost, for example by having crowdworkers paraphrase generated examples (Wang et al., 2015; Zhong et al., 2017), give feedback (Iyer et al., 2017), interact with a system (Artzi and Zettlemoyer, 2011; Thomason et al., 2015; Labutov et al., 2018), or a combination (Herzig and Berant, 2019)."
2020.acl-main.742,P17-1167,1,0.901526,"Missing"
2020.acl-main.742,P16-1002,0,0.0207748,"erence, we use beam search and execute the highest-probability, syntactically correct prediction. We impose a maximum execution time of 45 seconds for predictions. More details on the model, learning, and evaluation setup are available in Appendix B. 5.1 Generalization Strategies While using pre-trained language models can help encode natural language text, we need other strategies to reason jointly about the language and the database schema in completely unseen domains. We focus on generalizing to domain-specific language and novel database structures. Value Copying Similar to previous work (Jia and Liang, 2016; Gu et al., 2016; Gulcehre et al., 2016; See et al., 2017), we use a copy mechanism in the decoder. At each output step, the decoder generates a distribution over possible actions, including selecting a symbol from the output vocabulary, and copying a token from the input x. We only allow copying of certain token types, and mask out invalid copying actions, including independent wordpieces from u and TAB and column-type tokens. For table and column tokens, the name of 7 To discourage over-fitting to an arbitrary ordering of schema elements, we duplicate each Spider training example seven time"
2020.acl-main.742,P16-1057,0,0.0548744,"Missing"
2020.acl-main.742,2021.ccl-1.108,0,0.086761,"Missing"
2020.acl-main.742,P16-1138,0,0.0253619,"inference time (Yu et al., 2018). Preprocessing steps that perform database look-ups are unavailable at inference time. Instead, the model only has access to the database schema for each evaluation example. This setting requires additional generalization, where the model must be able to map unfamiliar entities to columns in domains unseen during training. Other Related Work Semantic parsing has been widely studied for tasks including sentence understanding (Zettlemoyer and Collins, 2005, 2007; Banarescu et al., 2013), instruction following (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Long et al., 2016; Givoli and Reichart, 2019), and knowledge base querying (Popescu et al., 2004; Poon, 2013; Iyer et al., 2017). Related to the task of semantic parsing is code generation (Oda et al., 2015; Ling et al., 2016; Yin et al., 2018; Lin et al., 2018; Iyer et al., 2018). While our experiments are performed on English-langauge data, a limited amount of existing work has explored semantic parsing in languages besides English (Wong and Mooney, 2006; Min et al., 2019). Annotating SQL queries for new domains can be expensive. Several prior works present approaches to reduce this cost, for example by havi"
2020.acl-main.742,P13-1092,0,0.0265097,"ble at inference time. Instead, the model only has access to the database schema for each evaluation example. This setting requires additional generalization, where the model must be able to map unfamiliar entities to columns in domains unseen during training. Other Related Work Semantic parsing has been widely studied for tasks including sentence understanding (Zettlemoyer and Collins, 2005, 2007; Banarescu et al., 2013), instruction following (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Long et al., 2016; Givoli and Reichart, 2019), and knowledge base querying (Popescu et al., 2004; Poon, 2013; Iyer et al., 2017). Related to the task of semantic parsing is code generation (Oda et al., 2015; Ling et al., 2016; Yin et al., 2018; Lin et al., 2018; Iyer et al., 2018). While our experiments are performed on English-langauge data, a limited amount of existing work has explored semantic parsing in languages besides English (Wong and Mooney, 2006; Min et al., 2019). Annotating SQL queries for new domains can be expensive. Several prior works present approaches to reduce this cost, for example by having crowdworkers paraphrase generated examples (Wang et al., 2015; Zhong et al., 2017), give"
2020.acl-main.742,P15-1129,0,0.152915,"Missing"
2020.acl-main.742,C04-1021,0,0.146793,"look-ups are unavailable at inference time. Instead, the model only has access to the database schema for each evaluation example. This setting requires additional generalization, where the model must be able to map unfamiliar entities to columns in domains unseen during training. Other Related Work Semantic parsing has been widely studied for tasks including sentence understanding (Zettlemoyer and Collins, 2005, 2007; Banarescu et al., 2013), instruction following (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Long et al., 2016; Givoli and Reichart, 2019), and knowledge base querying (Popescu et al., 2004; Poon, 2013; Iyer et al., 2017). Related to the task of semantic parsing is code generation (Oda et al., 2015; Ling et al., 2016; Yin et al., 2018; Lin et al., 2018; Iyer et al., 2018). While our experiments are performed on English-langauge data, a limited amount of existing work has explored semantic parsing in languages besides English (Wong and Mooney, 2006; Min et al., 2019). Annotating SQL queries for new domains can be expensive. Several prior works present approaches to reduce this cost, for example by having crowdworkers paraphrase generated examples (Wang et al., 2015; Zhong et al.,"
2020.acl-main.742,P17-1099,0,0.0160971,"y, syntactically correct prediction. We impose a maximum execution time of 45 seconds for predictions. More details on the model, learning, and evaluation setup are available in Appendix B. 5.1 Generalization Strategies While using pre-trained language models can help encode natural language text, we need other strategies to reason jointly about the language and the database schema in completely unseen domains. We focus on generalizing to domain-specific language and novel database structures. Value Copying Similar to previous work (Jia and Liang, 2016; Gu et al., 2016; Gulcehre et al., 2016; See et al., 2017), we use a copy mechanism in the decoder. At each output step, the decoder generates a distribution over possible actions, including selecting a symbol from the output vocabulary, and copying a token from the input x. We only allow copying of certain token types, and mask out invalid copying actions, including independent wordpieces from u and TAB and column-type tokens. For table and column tokens, the name of 7 To discourage over-fitting to an arbitrary ordering of schema elements, we duplicate each Spider training example seven times with randomly permuted orderings. Duplicating seven times"
2020.acl-main.742,P19-1010,1,0.820841,"zig and Berant, 2019). Research in SSP and code generation has led to innovations including constrained decoding and grammar-based decoding (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Lin et al., 2019). SSP has also been studied alongside additional generalization challenges, including to new compositional structures (Finegan-Dollak et al., 2018) and with additional context (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018). Recent works evaluating in the XSP setting have explored methods of jointly embedding an utterance and the database schema (Shaw et al., 2019; Bogin et al., 2019a), interactive learning (Yao et al., 2019), and using intermediate output representations and new inference methods (Herzig and Berant, 2018; Guo et al., 2019; Zhang et al., 2019; Bogin et al., 2019b; Lin et al., 2019). We incorporate several such methods proposed into our proposed system. 3 Evaluating on Re-purposed Data We propose to study the task of XSP by training on datasets designed for XSP, and evaluating on datasets originally designed for SSP. In our full model, we use both the Spider1 (Yu et al., 2018) and WikiSQL (Zhong et al., 2017) datasets for training. For"
2020.acl-main.742,N18-1203,1,0.847379,"Missing"
2020.acl-main.742,W00-1317,0,0.827899,"Missing"
2020.acl-main.742,P16-1127,0,0.0305464,"parsing in languages besides English (Wong and Mooney, 2006; Min et al., 2019). Annotating SQL queries for new domains can be expensive. Several prior works present approaches to reduce this cost, for example by having crowdworkers paraphrase generated examples (Wang et al., 2015; Zhong et al., 2017), give feedback (Iyer et al., 2017), interact with a system (Artzi and Zettlemoyer, 2011; Thomason et al., 2015; Labutov et al., 2018), or a combination (Herzig and Berant, 2019). Research in SSP and code generation has led to innovations including constrained decoding and grammar-based decoding (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Lin et al., 2019). SSP has also been studied alongside additional generalization challenges, including to new compositional structures (Finegan-Dollak et al., 2018) and with additional context (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018). Recent works evaluating in the XSP setting have explored methods of jointly embedding an utterance and the database schema (Shaw et al., 2019; Bogin et al., 2019a), interactive learning (Yao et al., 2019), and using intermediate output representations and new inference methods (Her"
2020.acl-main.742,P19-1443,0,0.192416,"Missing"
2020.acl-main.742,D07-1071,0,0.133107,"Missing"
2020.emnlp-main.705,P19-1620,0,0.0436022,"Missing"
2020.emnlp-main.705,N03-1020,0,0.0602103,"user populations. 2. We demonstrate that our information-needdriven model can generate much higher quality captions on this task than those of state-of-theart traditional generic captioning systems. 3. We propose a novel synthetic pre-training routine that greatly improves the performance of reinforcement learning under this new paradigm. 8756 2 Related Work Since the early days of the field, human-written references have been used for the supervised training and evaluation of text generation systems, including image captioning, summarization, and other related applications (Edmundson, 1969; Lin and Hovy, 2003; Ordonez et al., 2011; Vinyals et al., 2015). Recently, researchers have begun to consider a multitude of different objectives for reference comparison (B¨ohm et al., 2019; Gao et al., 2019), or even parametric regressions trained on human judgements (Louis and Nenkova, 2013; Peyrard and Gurevych, 2018). Though diverse in approach, each ultimately relies on designing a robust general-purpose metric. In practice, engineering such a metric is challenging—if at all possible (Sp¨arck Jones, 1994, 1999). Here we take a more empirical approach by relying on the information need expressed by users’"
2020.emnlp-main.705,W02-0109,0,0.332097,"Missing"
2020.emnlp-main.705,J13-2002,0,0.0133966,"ves the performance of reinforcement learning under this new paradigm. 8756 2 Related Work Since the early days of the field, human-written references have been used for the supervised training and evaluation of text generation systems, including image captioning, summarization, and other related applications (Edmundson, 1969; Lin and Hovy, 2003; Ordonez et al., 2011; Vinyals et al., 2015). Recently, researchers have begun to consider a multitude of different objectives for reference comparison (B¨ohm et al., 2019; Gao et al., 2019), or even parametric regressions trained on human judgements (Louis and Nenkova, 2013; Peyrard and Gurevych, 2018). Though diverse in approach, each ultimately relies on designing a robust general-purpose metric. In practice, engineering such a metric is challenging—if at all possible (Sp¨arck Jones, 1994, 1999). Here we take a more empirical approach by relying on the information need expressed by users’ questions. Many studies have observed that reference-trained captioning models suffer from systematic usability issues—including being rigid, neglecting relevant image aspects, and regurgitating frequent phrases (Wang et al., 2017; Dai et al., 2017). As a result, much effort"
2020.emnlp-main.705,N18-1158,0,0.0540072,"Missing"
2020.emnlp-main.705,P19-1101,0,0.016233,"e generation model at test time. Information Need: We assume that the QA data from D is derived by the following process: 1. an image x is drawn from distribution p(x); 2. a question-answer pair (q, a) targeting an informative detail of x perceived as important to a user in D is drawn from distribution p(q, a|x). The operating assumption is that the marginal distribution over (q, a) pairs represents the visual interests of the typical user. In other words, answers to common questions represent the type of information that is often considered important. This is comparable to content selection (Peyrard, 2019). Question Anticipation: We do not assume the existence of a “gold” caption. Rather, the caption y is assumed to be a latent variable, and Gθ (y|x) is a 8757 Caption: A bus is boarding passengers at a stop. Captioning Model REINFORCE Question: Why is the bus stopped? Question Answering Model Answer: boarding passengers Figure 2: Overview of our proposed approach to the C AP WAP task. The captioning model Gθ (y|x) is learned using supervision from question-answer-image triples. Generated text that can be used to answer the question correctly, according to an extractive question answering model,"
2020.emnlp-main.705,N18-2103,0,0.0157446,"inforcement learning under this new paradigm. 8756 2 Related Work Since the early days of the field, human-written references have been used for the supervised training and evaluation of text generation systems, including image captioning, summarization, and other related applications (Edmundson, 1969; Lin and Hovy, 2003; Ordonez et al., 2011; Vinyals et al., 2015). Recently, researchers have begun to consider a multitude of different objectives for reference comparison (B¨ohm et al., 2019; Gao et al., 2019), or even parametric regressions trained on human judgements (Louis and Nenkova, 2013; Peyrard and Gurevych, 2018). Though diverse in approach, each ultimately relies on designing a robust general-purpose metric. In practice, engineering such a metric is challenging—if at all possible (Sp¨arck Jones, 1994, 1999). Here we take a more empirical approach by relying on the information need expressed by users’ questions. Many studies have observed that reference-trained captioning models suffer from systematic usability issues—including being rigid, neglecting relevant image aspects, and regurgitating frequent phrases (Wang et al., 2017; Dai et al., 2017). As a result, much effort has been focused on developin"
2020.emnlp-main.705,P18-2124,0,0.0295367,"eter settings and optimization choices. 8758 Initialization: We initialize Gθ (y|x) using maximum likelihood estimation (MLE) on a corpus of ˜ ), as common out-of-domain generic captions (˜ x, y practice (Ranzato et al., 2016). This warm-starts our policy with an initial set of grounded image concepts, albeit not necessarily the ones we ultimately care about. Given the generic reference ˜ = (˜ y y1 , . . . , y˜n ), we minimize the cross-entropy: LXE (θ) = − n X ˜ , y˜j<i ) log Gθ (˜ yi |x (2) i=1 QA Model: We implement the QA model M using a BERTLARGE extractive model fine-tuned on SQuAD 2.0 (Rajpurkar et al., 2018)—which contains unanswerable questions. As an extractive model, M predicts a span yi...j . Important for our use-case, M is both able to be accurate when predicting the answer a when a is present in y, and also able to abstain from answering when a is not logically entailed (i.e., predict “no answer”). QA Reward: We take R(y, q, a) from Eq. 1 as the F1 score of the predicted answer with the gold answer. We control for reward noise with a confidence threshold for predicting “no answer.” Policy Gradient: We use REINFORCE with a baseline (Williams, 1992) to compute the policy gradient ∇θ LQA (θ)"
2020.emnlp-main.705,D16-1264,0,0.126522,"Missing"
2020.emnlp-main.705,D19-1320,0,0.0177595,"g relevant image aspects, and regurgitating frequent phrases (Wang et al., 2017; Dai et al., 2017). As a result, much effort has been focused on developing secondary, corrective objectives—for instance, “discriminability” losses encouraging captions to be unique (Dai and Lin, 2017; Liu et al., 2018; Luo et al., 2018). While these measures provide some fixes, they do not necessarily reflect user information needs—a central concept in C AP WAP. The idea of using QA for assessing information quality has been proposed in recent work for text summarization (Arumae and Liu, 2019; Eyal et al., 2019; Scialom et al., 2019). The primary distinctions with our work are both the domain (images) and how questions are obtained—both of which impact the task objective and learning procedure. In this prior work, questions are generated programmatically (e.g., following Hermann et al., 2015). Such “questions” may not necessarily reflect real user preferences. Our work focuses on QA not as just another method to improve standard referencebased metrics, but as a key, flexible way of formulating user information need—and as such we focus on challenging, real QA datasets. Furthermore, we train on this signal, rather than rel"
2020.emnlp-main.705,H94-1018,0,0.131943,"ms, including image captioning, summarization, and other related applications (Edmundson, 1969; Lin and Hovy, 2003; Ordonez et al., 2011; Vinyals et al., 2015). Recently, researchers have begun to consider a multitude of different objectives for reference comparison (B¨ohm et al., 2019; Gao et al., 2019), or even parametric regressions trained on human judgements (Louis and Nenkova, 2013; Peyrard and Gurevych, 2018). Though diverse in approach, each ultimately relies on designing a robust general-purpose metric. In practice, engineering such a metric is challenging—if at all possible (Sp¨arck Jones, 1994, 1999). Here we take a more empirical approach by relying on the information need expressed by users’ questions. Many studies have observed that reference-trained captioning models suffer from systematic usability issues—including being rigid, neglecting relevant image aspects, and regurgitating frequent phrases (Wang et al., 2017; Dai et al., 2017). As a result, much effort has been focused on developing secondary, corrective objectives—for instance, “discriminability” losses encouraging captions to be unique (Dai and Lin, 2017; Liu et al., 2018; Luo et al., 2018). While these measures provi"
2020.emnlp-main.705,D19-1514,0,0.0210884,"earning information need while maintaining fluency. Table 7 shows how synthetic pre-training regularizes the model to stay closer to human-level production patterns. Similarly, Table 8 shows how using the QA model to provide rewards (as opposed to a simple keyword search) helps the model avoid spurious rewards. Future Work: The C AP WAP paradigm introduces new challenges for learning effective systems, some of which our approach solves, and others which it still leaves open (e.g., maintaining fluency and fidelty). While some may be addressed by large-scale multi-modal models (Li et al., 2019; Tan and Bansal, 2019), it is still unclear whether they would fully cover the diversity of information that real users are interested in (e.g., OCR). 7 Conclusion We defined and studied the C AP WAP task, where question-answer pairs provided by users are used as a source of supervision for learning their visual information needs. Our results indicate that measuring caption content by its ability to logically support the answers to typical QA pairs from a target audience is (1) not only feasible, but also (2) a good proxy for uncovering information need. We hope this work will motivate the image captioning field to"
2020.emnlp-main.705,2020.acl-main.450,0,0.020024,"our work are both the domain (images) and how questions are obtained—both of which impact the task objective and learning procedure. In this prior work, questions are generated programmatically (e.g., following Hermann et al., 2015). Such “questions” may not necessarily reflect real user preferences. Our work focuses on QA not as just another method to improve standard referencebased metrics, but as a key, flexible way of formulating user information need—and as such we focus on challenging, real QA datasets. Furthermore, we train on this signal, rather than rely on it solely for evaluation (Wang et al., 2020). Efforts to leverage VQA resources to drive image captioning, and vice-versa, via variations of transfer learning, have also received extensive interest in recent years (Li et al., 2018; Wu et al., 2019; Yang and Xu, 2019). As opposed to optimizing metrics for specific VQA or supervised captioning benchmarks, the primary focus in C AP WAP is on modeling the target user population in order to anticipate the correct information-need. In a similar vein, VQA and textual QA resources have also been leveraged for active learning (Shen et al., 2019; Li et al., 2017), where the model learns to query"
2020.emnlp-main.705,P19-1348,0,0.0210925,"owing Hermann et al., 2015). Such “questions” may not necessarily reflect real user preferences. Our work focuses on QA not as just another method to improve standard referencebased metrics, but as a key, flexible way of formulating user information need—and as such we focus on challenging, real QA datasets. Furthermore, we train on this signal, rather than rely on it solely for evaluation (Wang et al., 2020). Efforts to leverage VQA resources to drive image captioning, and vice-versa, via variations of transfer learning, have also received extensive interest in recent years (Li et al., 2018; Wu et al., 2019; Yang and Xu, 2019). As opposed to optimizing metrics for specific VQA or supervised captioning benchmarks, the primary focus in C AP WAP is on modeling the target user population in order to anticipate the correct information-need. In a similar vein, VQA and textual QA resources have also been leveraged for active learning (Shen et al., 2019; Li et al., 2017), where the model learns to query its environment for information it is uncertain about to help improve its performance on the given task. The key distinction with our work is the directionality of the questions. In C AP WAP, the model u"
2021.acl-long.75,2020.acl-main.676,0,0.366849,"on-synthetic datasets by focusing on the template splits proposed by Finegan-Dollak et al. (2018), demonstrating improvements over standard seq2seq models. The effect of large-scale pre-training on compositional generalization ability has also been studied. Furrer et al. (2020) finds that pre-training alone cannot solve several compositional generalization challenges, despite its effectiveness across NLP tasks such as question answering (Raffel et al., 2020). While our work focuses on modeling approaches, compositional data augmentation techniques have also been proposed (Jia and Liang, 2016; Andreas, 2020). NQG-T5 outperforms previously reported results for these methods, but more in-depth analysis is needed. 3 Target Maximum Compound Divergence (TMCD) Splits The existing evaluations targeting compositional generalization for non-synthetic tasks are template splits and length splits. Here we propose an additional method which expands the set of available evaluations by generating data splits that maximize compound divergence over non-synthetic datasets, termed Target Maximum Compound Divergence (TMCD) splits. As we show in § 6, it results in a generalization problem with different characteristi"
2021.acl-long.75,P13-2009,0,0.191301,"our induced grammar G.5 Our grammar G contains a single non-terminal symbol, N T . We restrict source rules to ones containing at most 2 non-terminal symbols, and do not allow unary productions as source rules. This enables efficient parsing using an algorithm similar to CKY (Cocke, 1969; Kasami, 1965; Younger, 1967) that does not require binarization of the grammar. NQG Component NQG is inspired by more traditional approaches to semantic parsing based on grammar formalisms such as CCG (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2013) and SCFG (Wong and Mooney, 2006, 2007; Andreas et al., 2013; Li 925 Induction Procedure To induce G from the training data, we propose a QCFG induction algorithm that does not rely on task-specific heuristics or pre-computed word alignments. Notably, our approach makes no explicit assumptions about the source or target languages, beyond those implicit in the QCFG formalism. Table 1 shows examples of induced rules. Our grammar induction algorithm is guided by the principle of Occam’s razor, which leads us to 5 See Appendix A.1 for additional background on QCFGs. is the set of derivations that yield source string x and any target string. The constants l"
2021.acl-long.75,D15-1170,0,0.0504146,"Missing"
2021.acl-long.75,P07-1121,0,0.0298222,"Missing"
2021.acl-long.75,N06-1056,0,0.377494,"2006), or QCFGs, to refer to our induced grammar G.5 Our grammar G contains a single non-terminal symbol, N T . We restrict source rules to ones containing at most 2 non-terminal symbols, and do not allow unary productions as source rules. This enables efficient parsing using an algorithm similar to CKY (Cocke, 1969; Kasami, 1965; Younger, 1967) that does not require binarization of the grammar. NQG Component NQG is inspired by more traditional approaches to semantic parsing based on grammar formalisms such as CCG (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2013) and SCFG (Wong and Mooney, 2006, 2007; Andreas et al., 2013; Li 925 Induction Procedure To induce G from the training data, we propose a QCFG induction algorithm that does not rely on task-specific heuristics or pre-computed word alignments. Notably, our approach makes no explicit assumptions about the source or target languages, beyond those implicit in the QCFG formalism. Table 1 shows examples of induced rules. Our grammar induction algorithm is guided by the principle of Occam’s razor, which leads us to 5 See Appendix A.1 for additional background on QCFGs. is the set of derivations that yield source string x and any ta"
2021.acl-long.75,D18-1425,0,0.564354,"-trained seq2seq models that perform well on in-distribution evaluations do not address most of the compositional generalization challenges proposed in SCAN (Furrer et al., 2020). Add Primitive (Lake and Baroni, 2018) Length TMCD Template (Finegan-Dollak et al., 2018) Length Random SYNTHETIC NON - SYNTHETIC NATURAL LANGUAGE VARIATION Figure 2: We evaluate semantic parsing approaches across a diverse set of evaluations focused on natural language variation, compositional generalization, or both. We add TMCD splits to complement existing evaluations. Ordering within each cell is arbitrary. DER (Yu et al., 2018). Our results indicate that NQG-T5 is a strong baseline for our challenge of developing approaches that perform well across a diverse set of evaluations focusing on either natural language variation, compositional generalization, or both. Comparing five approaches across eight evaluations on SCAN and G EO Q UERY, its average rank is 1, with the rank of the best previous approach (T5) being 2.9; performance is also competitive across several evaluations on S PIDER. While still far from affirmatively answering our research question, our study highlights the importance of a diverse set of evaluat"
2021.acl-long.75,D07-1071,0,0.0339813,"Missing"
2021.emnlp-main.560,D13-1160,0,0.0653188,"tive is the sum of −logP (p|q, B) of the passages including any valid answer to q. At inference, I NDEP PR outputs the top k passages based the logit values of the passage indices. We compare mainly to I N DEP PR because it is the strict non-autoregressive version of JPR, and is empirically better than or comparable to Nogueira et al. (2020) (Section 5.1). We train and evaluate on three datasets that provide a set of distinct answers for each question. Statistics of each dataset are provided in Table 2. W EB QSP (Yih et al., 2016) consists of questions from Google Suggest API, originally from Berant et al. (2013). The answer is a set of distinct entities in Freebase; we recast this problem as textual question answering based on Wikipedia. A MBIG QA (Min et al., 2020) consists of questions mined from Google search queries, originally from NQ (Kwiatkowski et al., 2019). Each question is paired with an annotated set of distinct answers 4.4 Implementation Details that are equally valid based on Wikipedia. TREC (Baudiš and Šediv`y, 2015) contains ques- We use the English Wikipedia from 12/20/2018 tions curated from TREC QA tracks, along with as the retrieval corpus C, where each article is regular expressi"
2021.emnlp-main.560,P17-1171,0,0.0188345,"ropose JPR, a joint passage retrieval model that integrates dependencies among selected passages, along with new training and decoding algorithms. 3. On three multi-answer QA datasets, JPR significantly outperforms a range of baselines with independent scoring of passages, both in retrieval recall and answer accuracy. 2 2.1 Background Review: Single-Answer Retrieval In a typical single-answer retrieval problem, a model is given a natural language question q and retrieves k passages {p1 ...pk } from a large text corpus C (Voorhees et al., 1999; Ramos et al., 2003; Robertson and Zaragoza, 2009; Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020; Luan et al., 2020). The goal is to retrieve at least one passage that contains the answer to q. During training, question-answer pairs (q, a) are given to the model. Task Single-answer Retrieval Multi-answer Retrieval Train Data (q, a) (q, {a1 ...an }) Inference q → {p1 ...pk } q → {p1 ...pk } Evaluation R ECALL( a, {p1 ...pk }) MR ECALL( {a1 ...an }, {p1 ...pk }) Appropriate Model P (pi |q) P (p1 ...pk |q) Table 1: A comparison of single-answer and multianswer retrieval tasks. Previous work has used independent ranking models P (pi |q) for multi-ans"
2021.emnlp-main.560,2021.eacl-main.74,0,0.444028,"Inference q → {p1 ...pk } q → {p1 ...pk } Evaluation R ECALL( a, {p1 ...pk }) MR ECALL( {a1 ...an }, {p1 ...pk }) Appropriate Model P (pi |q) P (p1 ...pk |q) Table 1: A comparison of single-answer and multianswer retrieval tasks. Previous work has used independent ranking models P (pi |q) for multi-answer retrieval because the inference-time inputs and outputs are the same. We propose JPR as an instance of P (p1 ...pk |q). trieval successful if the answer a is included in {p1 ...pk }. Extrinsic evaluation uses the retrieved passages as input to an answer generation model such as the model in Izacard and Grave (2021) and evaluates final question answering performance. Reranking Much prior work (Liu, 2011; Asadi and Lin, 2013; Nogueira et al., 2020) found an effective strategy in using a two-step approach of (1) retrieving a set of candidate passages B from the corpus C (k &lt; |B | |C|) and (2) using another model to rerank the passages, obtaining a final top k. A reranker could be more expressive than the first-stage model (e.g. by using cross-attention), as it needs to process much fewer candidates. Most prior work in reranking, including the current stateof-the-art (Nogueira et al., 2020), scores each pa"
2021.emnlp-main.560,2020.emnlp-main.550,1,0.91278,"model that integrates dependencies among selected passages, along with new training and decoding algorithms. 3. On three multi-answer QA datasets, JPR significantly outperforms a range of baselines with independent scoring of passages, both in retrieval recall and answer accuracy. 2 2.1 Background Review: Single-Answer Retrieval In a typical single-answer retrieval problem, a model is given a natural language question q and retrieves k passages {p1 ...pk } from a large text corpus C (Voorhees et al., 1999; Ramos et al., 2003; Robertson and Zaragoza, 2009; Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020; Luan et al., 2020). The goal is to retrieve at least one passage that contains the answer to q. During training, question-answer pairs (q, a) are given to the model. Task Single-answer Retrieval Multi-answer Retrieval Train Data (q, a) (q, {a1 ...an }) Inference q → {p1 ...pk } q → {p1 ...pk } Evaluation R ECALL( a, {p1 ...pk }) MR ECALL( {a1 ...an }, {p1 ...pk }) Appropriate Model P (pi |q) P (p1 ...pk |q) Table 1: A comparison of single-answer and multianswer retrieval tasks. Previous work has used independent ranking models P (pi |q) for multi-answer retrieval because the inference-time i"
2021.emnlp-main.560,Q19-1026,1,0.820022,"ve version of JPR, and is empirically better than or comparable to Nogueira et al. (2020) (Section 5.1). We train and evaluate on three datasets that provide a set of distinct answers for each question. Statistics of each dataset are provided in Table 2. W EB QSP (Yih et al., 2016) consists of questions from Google Suggest API, originally from Berant et al. (2013). The answer is a set of distinct entities in Freebase; we recast this problem as textual question answering based on Wikipedia. A MBIG QA (Min et al., 2020) consists of questions mined from Google search queries, originally from NQ (Kwiatkowski et al., 2019). Each question is paired with an annotated set of distinct answers 4.4 Implementation Details that are equally valid based on Wikipedia. TREC (Baudiš and Šediv`y, 2015) contains ques- We use the English Wikipedia from 12/20/2018 tions curated from TREC QA tracks, along with as the retrieval corpus C, where each article is regular expressions as answers. Prior work uses split into passages with up to 288 wordpieces, All this data as a task of finding a single answer (where rerankers are based on T5 (Raffel et al., 2020), retrieving any of the correct answers is sufficient), a pretrained encode"
2021.emnlp-main.560,P19-1612,1,0.924355,"e reof passages relevant to a natural language question call, and keeping passages relevant to the question. from a large text corpus. Most prior work focuses In this work, we introduce Joint Passage Reon single-answer retrieval, which scores passages trieval (JPR), a new model that addresses these independently from each other according to their challenges. To jointly score passages, JPR emrelevance to the given question, assuming there is ploys an encoder-decoder reranker and autoregresa single answer (Voorhees et al., 1999; Chen et al., sively generates passage references by modeling 2017; Lee et al., 2019). However, questions posed the probability of each passage as a function of by humans are often open-ended and ambiguous, previously retrieved passages. Since there is no leading to multiple valid answers (Min et al., 2020). ground truth ordering of passages, we employ a For example, for the question in Figure 1, “What new training method that dynamically forms suwas Eli Whitney’s job?”, an ideal retrieval system pervision to drive the model to prefer passages should provide passages covering all professions of with answers not already covered by previously seEli Whitney. This introduces the p"
2021.emnlp-main.560,2020.emnlp-main.466,1,0.870763,"ieval, which scores passages trieval (JPR), a new model that addresses these independently from each other according to their challenges. To jointly score passages, JPR emrelevance to the given question, assuming there is ploys an encoder-decoder reranker and autoregresa single answer (Voorhees et al., 1999; Chen et al., sively generates passage references by modeling 2017; Lee et al., 2019). However, questions posed the probability of each passage as a function of by humans are often open-ended and ambiguous, previously retrieved passages. Since there is no leading to multiple valid answers (Min et al., 2020). ground truth ordering of passages, we employ a For example, for the question in Figure 1, “What new training method that dynamically forms suwas Eli Whitney’s job?”, an ideal retrieval system pervision to drive the model to prefer passages should provide passages covering all professions of with answers not already covered by previously seEli Whitney. This introduces the problem of multilected passages. Furthermore, we introduce a new answer retrieval—retrieval of multiple passages tree-decoding algorithm to allow flexibility in the ∗ Work done while interning at Google. degree of diversity."
2021.emnlp-main.560,P19-1416,1,0.828576,"ncreasing the distances between retrieved passages does not help.7 Second, multi-answer retrieval uses a clear notion of “answers”; “sub-topics” in diverse IR are more subjective and hard to enumerate fully. Multi-hop passage retrieval Recent work studies multi-hop passage retrieval, where a passage containing the answer is the destination of a chain of multiple hops (Asai et al., 2020; Xiong et al., 2021; Khattab et al., 2021). This is a difficult problem as passages in a chain are dissimilar to each other, but existing datasets often suffer from annotation artifacts (Chen and Durrett, 2019; Min et al., 2019), resulting in strong lexical cues for each hop. We study an orthogonal problem of finding multiple answers, where the challenge is in controlling the trade-off between relevance and diversity. 7 Conclusion We introduce JPR, an autoregressive passage reranker designed to address the multi-answer retrieval problem. On three multi-answer datasets, JPR significantly outperforms a range of baselines 7 Diverse retrieval Studies on diverse retrieval in In our preliminary experiment, we tried increasing diversity based on Maximal Marginal Relevance (Carbonell and the context of information retrieval"
2021.emnlp-main.560,2020.findings-emnlp.63,0,0.293556,"pi |q) P (p1 ...pk |q) Table 1: A comparison of single-answer and multianswer retrieval tasks. Previous work has used independent ranking models P (pi |q) for multi-answer retrieval because the inference-time inputs and outputs are the same. We propose JPR as an instance of P (p1 ...pk |q). trieval successful if the answer a is included in {p1 ...pk }. Extrinsic evaluation uses the retrieved passages as input to an answer generation model such as the model in Izacard and Grave (2021) and evaluates final question answering performance. Reranking Much prior work (Liu, 2011; Asadi and Lin, 2013; Nogueira et al., 2020) found an effective strategy in using a two-step approach of (1) retrieving a set of candidate passages B from the corpus C (k &lt; |B | |C|) and (2) using another model to rerank the passages, obtaining a final top k. A reranker could be more expressive than the first-stage model (e.g. by using cross-attention), as it needs to process much fewer candidates. Most prior work in reranking, including the current stateof-the-art (Nogueira et al., 2020), scores each passage independently, modeling P (p|q). 2.2 Multi-Answer Retrieval We now formally define the task of multi-answer retrieval. A model i"
2021.emnlp-main.560,P16-2033,1,0.83115,"rained to output a single token i (1 ≤ i ≤ |B|) rather than a sequence. The objective is the sum of −logP (p|q, B) of the passages including any valid answer to q. At inference, I NDEP PR outputs the top k passages based the logit values of the passage indices. We compare mainly to I N DEP PR because it is the strict non-autoregressive version of JPR, and is empirically better than or comparable to Nogueira et al. (2020) (Section 5.1). We train and evaluate on three datasets that provide a set of distinct answers for each question. Statistics of each dataset are provided in Table 2. W EB QSP (Yih et al., 2016) consists of questions from Google Suggest API, originally from Berant et al. (2013). The answer is a set of distinct entities in Freebase; we recast this problem as textual question answering based on Wikipedia. A MBIG QA (Min et al., 2020) consists of questions mined from Google search queries, originally from NQ (Kwiatkowski et al., 2019). Each question is paired with an annotated set of distinct answers 4.4 Implementation Details that are equally valid based on Wikipedia. TREC (Baudiš and Šediv`y, 2015) contains ques- We use the English Wikipedia from 12/20/2018 tions curated from TREC QA"
D10-1075,W06-1615,0,0.402207,"also show that applying an adaptation algorithm that finds shared representation between domains often impacts the choice in adaptation algorithm that makes use of target labeled data. 1 Focuses on P (X) This type of adaptation algorithm attempts to resolve the difference between the feature space statistics of two domains. While many different techniques have been proposed, the common goal of these algorithms is to find a better shared representation that brings the source domain and the target domain closer. Often these algorithms do not use labeled examples in the target domain. The works (Blitzer et al., 2006; Huang and Yates, 2009) all belong to this category. Introduction While recent advances in statistical modeling for natural language processing are exciting, the problem of domain adaptation remains a big challenge. It is widely known that a classifier trained on one domain (e.g. news domain) usually performs poorly on a different domain (e.g. medical domain) (Jiang and Focuses on P (Y |X) These adaptation algorithms assume that there exists a small amount of labeled data for the target domain. Instead of training two weight vectors independently (one for source and the other for the target d"
D10-1075,W04-3237,0,0.115617,"he problem of domain adaptation remains a big challenge. It is widely known that a classifier trained on one domain (e.g. news domain) usually performs poorly on a different domain (e.g. medical domain) (Jiang and Focuses on P (Y |X) These adaptation algorithms assume that there exists a small amount of labeled data for the target domain. Instead of training two weight vectors independently (one for source and the other for the target domain), these algorithms try to relate the source and target weight vectors. This is often achieved by using a special designed regularization term. The works (Chelba and Acero, 2004; Daum´e III, 2007; Finkel and Manning, 2009) belong to this category. 767 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 767–777, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics It is important to give the definition of an adaptation framework. An adaptation framework is specified by the data/resources used and a specific learning algorithm. For example, a framework that used only source labeled examples and one that used both source and target labeled examples should be considered as two different fra"
D10-1075,D09-1047,0,0.0154504,"d word-cluster information. Notice that because of combining two classes of adaption algorithms, our approach is significantly better than these two systems10 . Preposition Sense Disambiguation We also test the combination of unlabeled and labeled adaption on the task of Preposition Sense Disambiguation. Here the data contains multiple prepositions where each preposition has many different senses. The goal is to predict the right sense for a given preposition in the testing data. The source domain is the SemEval 2007 preposition WSD Task and the target domain is from the dataset annotated in (Dahlmeier et al., 2009). Our feature design mainly comes from (Tratz and Hovy, 2009) (who do not evaluate their system on our target data). As our un10 The work (Ratinov and Roth, 2009) also combines their system with several document-level features. While it is possible to add these features in our system, we do not include any global features for the sake of simplicity. Note that our system is competitive to (Ratinov and Roth, 2009) even though our system does not use global features. Systems Our NER FM09 RR09 RR09 + global Cluster? y n y y TGT? y y n n P.F1 84.1 79.98 N/A N/A T.F1 86.5 N/A 83.2 86.2 Table 3: Comp"
D10-1075,P07-1033,0,0.443142,"Missing"
D10-1075,N09-1068,0,0.461514,"big challenge. It is widely known that a classifier trained on one domain (e.g. news domain) usually performs poorly on a different domain (e.g. medical domain) (Jiang and Focuses on P (Y |X) These adaptation algorithms assume that there exists a small amount of labeled data for the target domain. Instead of training two weight vectors independently (one for source and the other for the target domain), these algorithms try to relate the source and target weight vectors. This is often achieved by using a special designed regularization term. The works (Chelba and Acero, 2004; Daum´e III, 2007; Finkel and Manning, 2009) belong to this category. 767 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 767–777, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics It is important to give the definition of an adaptation framework. An adaptation framework is specified by the data/resources used and a specific learning algorithm. For example, a framework that used only source labeled examples and one that used both source and target labeled examples should be considered as two different frameworks, even though they might use exactly t"
D10-1075,P09-1056,0,0.0714695,"g an adaptation algorithm that finds shared representation between domains often impacts the choice in adaptation algorithm that makes use of target labeled data. 1 Focuses on P (X) This type of adaptation algorithm attempts to resolve the difference between the feature space statistics of two domains. While many different techniques have been proposed, the common goal of these algorithms is to find a better shared representation that brings the source domain and the target domain closer. Often these algorithms do not use labeled examples in the target domain. The works (Blitzer et al., 2006; Huang and Yates, 2009) all belong to this category. Introduction While recent advances in statistical modeling for natural language processing are exciting, the problem of domain adaptation remains a big challenge. It is widely known that a classifier trained on one domain (e.g. news domain) usually performs poorly on a different domain (e.g. medical domain) (Jiang and Focuses on P (Y |X) These adaptation algorithms assume that there exists a small amount of labeled data for the target domain. Instead of training two weight vectors independently (one for source and the other for the target domain), these algorithms"
D10-1075,P07-1034,0,0.0802003,"urce and target labeled data to work. We denote n as the total number of features3 and m is the number of the “domains”, where one of the domains is the target domain. The FE framework creates a global weight vector in Rn(m+1) , an extended space for all domains. The representation x of the t-th domain is mapped by Φt (x) ∈ Rn(m+1) . In the extended space, the first n features consist of the “shared” block, which is always active across all tasks. The (t+1)-th block (the (nt+1)-th to the (nt+n)-th features) is a “specific” block, and is only active when 2 Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. However, little analysis on the interaction of the two aspects is discussed in that paper 3 We assume that the number of features in each domain is equal. 769 extracting examples from the task t. More formally, 2 6 Φt (x) = 4 |{z} x shared blocks z } |{ 0...0 3 blocks z } |{ 7 0 . . . 0 5 . (1) (m−t) (t−1) x |{z} specific A single weight vector w ¯ is obtained by training on the modified labeled data {yit , Φt (xti )}m t=1 . Given that this framework only extends the feature space, in this paper, we also call it"
D10-1075,P08-1068,0,0.0295145,"ngle model on the pooled and unextended source and target training data. Unlabeled adaptation: Adding cluster-like features Recall that unlabeled adaptation frameworks find the features that “work” across domain. In this paper, we find such features in two steps. First, we use word clusters generated from unlabeled text and/or third party resources that spans domains. Then, for every feature template that contains a word, we append another feature template that uses the word’s cluster instead of the word itself. This technique is used in many recent works including dependency parsing and NER (Koo et al., 2008; Ratinov and Roth, 2009). Note that the unlabeled text need not come from the source or target domain. In fact, in this paper, we use clusters generated with the Reuters 1996 dataset, a superset of the CoNLL03 NER dataset (Koo et al., 2008; Liang, 2005). We adopt the Brown cluster algorithm to find the word cluster (Brown et al., 1992; Liang, 2005). We can use other resources to create clusters as well. For example, in the NER domain, we also include gazetteers4 as an unlabeled cluster resource, which can bring the domains together quite effectively. 4 Our gazetteers comes from (Ratinov and R"
D10-1075,W09-1119,1,0.819377,"pooled and unextended source and target training data. Unlabeled adaptation: Adding cluster-like features Recall that unlabeled adaptation frameworks find the features that “work” across domain. In this paper, we find such features in two steps. First, we use word clusters generated from unlabeled text and/or third party resources that spans domains. Then, for every feature template that contains a word, we append another feature template that uses the word’s cluster instead of the word itself. This technique is used in many recent works including dependency parsing and NER (Koo et al., 2008; Ratinov and Roth, 2009). Note that the unlabeled text need not come from the source or target domain. In fact, in this paper, we use clusters generated with the Reuters 1996 dataset, a superset of the CoNLL03 NER dataset (Koo et al., 2008; Liang, 2005). We adopt the Brown cluster algorithm to find the word cluster (Brown et al., 1992; Liang, 2005). We can use other resources to create clusters as well. For example, in the NER domain, we also include gazetteers4 as an unlabeled cluster resource, which can bring the domains together quite effectively. 4 Our gazetteers comes from (Ratinov and Roth, 2009). Framework Unl"
D10-1075,N09-3017,0,0.0459385,"Missing"
D10-1075,J92-4003,0,\N,Missing
D10-1075,W03-0419,0,\N,Missing
D16-1029,Q13-1005,0,0.0781941,"ems. We discuss this in detail in Section 4. 1 The new resource and the dataset we used for training is available soon on https://aka.ms/dataimplicit and https://aka.ms/datadraw 298 Solving automatic algebra word problems can be viewed as a semantic parsing task. In the semantic parsing community, the technique of learning from implicit supervision signals has been applied (under the name response-driven learning (Clarke et al., 2010)) to knowledge base question answering tasks such as Geoquery (Zelle and Mooney, 1996) and WebQuestions (Berant et al., 2013) or mapping instructions to actions (Artzi and Zettlemoyer, 2013). In these tasks, researchers have shown that it is possible to train a semantic parser only from questionanswer pairs, such as “What is the largest state bordering Texas?” and “New Mexico” (Clarke et al., 2010; Liang et al., 2013; Yih et al., 2015). One key reason that such implicit supervision is effective is because the correct semantic parses of the questions can often be found using the answers and the knowledge base alone, with the help of heuristics developed for the specific domain. For instance, when the question is relatively simple and does not have complex compositional structure,"
D16-1029,D13-1160,0,0.0446335,"ly the solutions with little or no annoated equation systems. We discuss this in detail in Section 4. 1 The new resource and the dataset we used for training is available soon on https://aka.ms/dataimplicit and https://aka.ms/datadraw 298 Solving automatic algebra word problems can be viewed as a semantic parsing task. In the semantic parsing community, the technique of learning from implicit supervision signals has been applied (under the name response-driven learning (Clarke et al., 2010)) to knowledge base question answering tasks such as Geoquery (Zelle and Mooney, 1996) and WebQuestions (Berant et al., 2013) or mapping instructions to actions (Artzi and Zettlemoyer, 2013). In these tasks, researchers have shown that it is possible to train a semantic parser only from questionanswer pairs, such as “What is the largest state bordering Texas?” and “New Mexico” (Clarke et al., 2010; Liang et al., 2013; Yih et al., 2015). One key reason that such implicit supervision is effective is because the correct semantic parses of the questions can often be found using the answers and the knowledge base alone, with the help of heuristics developed for the specific domain. For instance, when the question is rela"
D16-1029,D13-1057,1,0.141976,"word problems. Compared to the knowledge base question answering problems, one key difference is that a large number (potentially infinitely many) of different equation systems could end up having the same solutions. Without a database or special rules for combining variables and coefficients, the number of candidate equation systems cannot be trimmed effectively, given only the solutions. From the algorithmic point of view, our proposed learning framework is related to several lines of work. Similar efforts have been made to develop latent structured prediction models (Yu and Joachims, 2009; Chang et al., 2013; Zettlemoyer and Collins, 2007) to find latent semantic structures which best explain the answer given the question. Our algorithm is also influenced by the discriminative reranking algorithms (Collins, 2000; Ge and Mooney, 2006; Charniak and Johnson, 2005) and models for learning from intractable supervision (Steinhardt and Liang, 2015). Recently, Huang et al. (2016) collected a large number of noisily annotated word problems from online forums. While they collected a large-scale dataset, unlike our work, they did not demonstrate how to utilize the newly crawled dataset to improve existing s"
D16-1029,P05-1022,0,0.0221719,"les for combining variables and coefficients, the number of candidate equation systems cannot be trimmed effectively, given only the solutions. From the algorithmic point of view, our proposed learning framework is related to several lines of work. Similar efforts have been made to develop latent structured prediction models (Yu and Joachims, 2009; Chang et al., 2013; Zettlemoyer and Collins, 2007) to find latent semantic structures which best explain the answer given the question. Our algorithm is also influenced by the discriminative reranking algorithms (Collins, 2000; Ge and Mooney, 2006; Charniak and Johnson, 2005) and models for learning from intractable supervision (Steinhardt and Liang, 2015). Recently, Huang et al. (2016) collected a large number of noisily annotated word problems from online forums. While they collected a large-scale dataset, unlike our work, they did not demonstrate how to utilize the newly crawled dataset to improve existing systems. It will be interesting to see if our proposed algorithm can make further improvements using their newly collected dataset.2 3 Problem Definition Table 1 lists all the symbols representing the components in the process. The input algebra word problem"
D16-1029,W10-2903,1,0.937121,"ushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015), where the authors train the algebra solvers using only the solutions with little or no annoated equation systems. We discuss this in detail in Section 4. 1 The new resource and the dataset we used for training is available soon on https://aka.ms/dataimplicit and https://aka.ms/datadraw 298 Solving automatic algebra word problems can be viewed as a semantic parsing task. In the semantic parsing community, the technique of learning from implicit supervision signals has been applied (under the name response-driven learning (Clarke et al., 2010)) to knowledge base question answering tasks such as Geoquery (Zelle and Mooney, 1996) and WebQuestions (Berant et al., 2013) or mapping instructions to actions (Artzi and Zettlemoyer, 2013). In these tasks, researchers have shown that it is possible to train a semantic parser only from questionanswer pairs, such as “What is the largest state bordering Texas?” and “New Mexico” (Clarke et al., 2010; Liang et al., 2013; Yih et al., 2015). One key reason that such implicit supervision is effective is because the correct semantic parses of the questions can often be found using the answers and the"
D16-1029,W02-1001,0,0.118847,"used in this paper to formally describe the problem of mapping algebra word problems to equations. 4 Learning from Mixed Supervision We assume that we have two sets: De = {(xe , ye )} and Dm = {(xm , zm )}. De contains the fully annotated equation system ye for each algebra word problem xe , whereas in Dm , we have access to the numerical solution zm to each problem, but not the equation system (ym = ∅). We refer to De as the explicit set and Dm as the implicit set. For the sake of simplicity, we explain our approach by modifying the training procedure of the structured Perceptron algorithm (Collins, 2002).4 As discussed in Section 3, the key challenge of learning from implicit supervision is that the mapping E(y) is one-directional. Therefore, the correct equation system cannot be easily derived from the numerical solution. Intuitively, for data with only implicit supervision, we can explore the structure ˜∈Y space Y and find the best possible derivation y according to the current model. If E(˜ y) matches z, ˜ . Following then we can update the model based on y this intuition, we propose MixedSP (Algorithm 1). For each example, we use an approximate search algorithm to collect top scoring cand"
D16-1029,P06-2034,0,0.019982,"atabase or special rules for combining variables and coefficients, the number of candidate equation systems cannot be trimmed effectively, given only the solutions. From the algorithmic point of view, our proposed learning framework is related to several lines of work. Similar efforts have been made to develop latent structured prediction models (Yu and Joachims, 2009; Chang et al., 2013; Zettlemoyer and Collins, 2007) to find latent semantic structures which best explain the answer given the question. Our algorithm is also influenced by the discriminative reranking algorithms (Collins, 2000; Ge and Mooney, 2006; Charniak and Johnson, 2005) and models for learning from intractable supervision (Steinhardt and Liang, 2015). Recently, Huang et al. (2016) collected a large number of noisily annotated word problems from online forums. While they collected a large-scale dataset, unlike our work, they did not demonstrate how to utilize the newly crawled dataset to improve existing systems. It will be interesting to see if our proposed algorithm can make further improvements using their newly collected dataset.2 3 Problem Definition Table 1 lists all the symbols representing the components in the process. Th"
D16-1029,D14-1058,0,0.44745,"Missing"
D16-1029,P16-1084,0,0.323797,"en only the solutions. From the algorithmic point of view, our proposed learning framework is related to several lines of work. Similar efforts have been made to develop latent structured prediction models (Yu and Joachims, 2009; Chang et al., 2013; Zettlemoyer and Collins, 2007) to find latent semantic structures which best explain the answer given the question. Our algorithm is also influenced by the discriminative reranking algorithms (Collins, 2000; Ge and Mooney, 2006; Charniak and Johnson, 2005) and models for learning from intractable supervision (Steinhardt and Liang, 2015). Recently, Huang et al. (2016) collected a large number of noisily annotated word problems from online forums. While they collected a large-scale dataset, unlike our work, they did not demonstrate how to utilize the newly crawled dataset to improve existing systems. It will be interesting to see if our proposed algorithm can make further improvements using their newly collected dataset.2 3 Problem Definition Table 1 lists all the symbols representing the components in the process. The input algebra word problem is denoted by x, and the output y = (T, A) is called a derivation, which consists of an equation system template"
D16-1029,P14-1026,0,0.346527,"a textual number (e.g., four) in a word problem. Let Q(x) be all the textual numbers in the problem x, and C(T ) be the coefficients to be determined in the template T . An alignment is a set of tuples A = {(q, c) |q ∈ Q(x), c ∈ C(T ) ∪ {}}, where the tuple (q, ) indicates that the number q is not relevant to the final equation system. By specifying the value of each coefficient, it identifies an equation system belonging to the family represented by template T . Together, T and A generate a complete equation system, and the solution z can be derived by the mathematical engine E. Following (Kushman et al., 2014; Zhou et al., 2015), our strategy of mapping a word problem to an equation system is to first choose a template that consists of variables and coefficients, and then align each coefficient to a textual number mentioned in the problem. We formulate the mapping between an algebra word problem and an equation system as a structured learning problem. The output space is the set of all possible derivations using templates that are observed in the training data. Our model maps x to y = (T, A) by a linear scoring function wT Φ(x, y), where w is the model parameters and Φ is the feature functions. At"
D16-1029,J13-2005,0,0.0262744,"semantic parsing task. In the semantic parsing community, the technique of learning from implicit supervision signals has been applied (under the name response-driven learning (Clarke et al., 2010)) to knowledge base question answering tasks such as Geoquery (Zelle and Mooney, 1996) and WebQuestions (Berant et al., 2013) or mapping instructions to actions (Artzi and Zettlemoyer, 2013). In these tasks, researchers have shown that it is possible to train a semantic parser only from questionanswer pairs, such as “What is the largest state bordering Texas?” and “New Mexico” (Clarke et al., 2010; Liang et al., 2013; Yih et al., 2015). One key reason that such implicit supervision is effective is because the correct semantic parses of the questions can often be found using the answers and the knowledge base alone, with the help of heuristics developed for the specific domain. For instance, when the question is relatively simple and does not have complex compositional structure, paths in the knowledge graph that connect the answers and the entities in the narrative can be interpreted as legitimate semantic parses. However, as we will show in our experiments, learning from implicit supervision alone is not"
D16-1029,P14-5010,0,0.0039774,"Missing"
D16-1029,D15-1202,0,0.350821,"Missing"
D16-1029,D15-1135,0,0.350252,"elated Work Automatically solving mathematical reasoning problems expressed in natural language has been a long-studied problem (Bobrow, 1964; Newell et al., 1959; Mukherjee and Garain, 2008). Recently, Kushman et al. (2014) created a template-base search procedure to map word problems into equations. Then, several following papers studied different aspects of the task: Hosseini et al. (2014) focused on improving the generalization ability of the solvers by leveraging extra annotations; Roy and Roth (2015) focused on how to solve arithmetic problems without using any pre-defined template. In (Shi et al., 2015), the authors focused on number word problems and proposed a system that is created using semi-automatically generated rules. In Zhou et al. (2015), the authors simplified the inference procedure and pushed the state-of-the-art benchmark accuracy. The idea of learning from implicit supervision is discussed in (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015), where the authors train the algebra solvers using only the solutions with little or no annoated equation systems. We discuss this in detail in Section 4. 1 The new resource and the dataset we used for training is a"
D16-1029,P15-1128,1,0.261412,"sk. In the semantic parsing community, the technique of learning from implicit supervision signals has been applied (under the name response-driven learning (Clarke et al., 2010)) to knowledge base question answering tasks such as Geoquery (Zelle and Mooney, 1996) and WebQuestions (Berant et al., 2013) or mapping instructions to actions (Artzi and Zettlemoyer, 2013). In these tasks, researchers have shown that it is possible to train a semantic parser only from questionanswer pairs, such as “What is the largest state bordering Texas?” and “New Mexico” (Clarke et al., 2010; Liang et al., 2013; Yih et al., 2015). One key reason that such implicit supervision is effective is because the correct semantic parses of the questions can often be found using the answers and the knowledge base alone, with the help of heuristics developed for the specific domain. For instance, when the question is relatively simple and does not have complex compositional structure, paths in the knowledge graph that connect the answers and the entities in the narrative can be interpreted as legitimate semantic parses. However, as we will show in our experiments, learning from implicit supervision alone is not a viable strategy"
D16-1029,D07-1071,0,0.220524,"red to the knowledge base question answering problems, one key difference is that a large number (potentially infinitely many) of different equation systems could end up having the same solutions. Without a database or special rules for combining variables and coefficients, the number of candidate equation systems cannot be trimmed effectively, given only the solutions. From the algorithmic point of view, our proposed learning framework is related to several lines of work. Similar efforts have been made to develop latent structured prediction models (Yu and Joachims, 2009; Chang et al., 2013; Zettlemoyer and Collins, 2007) to find latent semantic structures which best explain the answer given the question. Our algorithm is also influenced by the discriminative reranking algorithms (Collins, 2000; Ge and Mooney, 2006; Charniak and Johnson, 2005) and models for learning from intractable supervision (Steinhardt and Liang, 2015). Recently, Huang et al. (2016) collected a large number of noisily annotated word problems from online forums. While they collected a large-scale dataset, unlike our work, they did not demonstrate how to utilize the newly crawled dataset to improve existing systems. It will be interesting t"
D16-1029,D15-1096,0,0.678971,"ell et al., 1959; Mukherjee and Garain, 2008). Recently, Kushman et al. (2014) created a template-base search procedure to map word problems into equations. Then, several following papers studied different aspects of the task: Hosseini et al. (2014) focused on improving the generalization ability of the solvers by leveraging extra annotations; Roy and Roth (2015) focused on how to solve arithmetic problems without using any pre-defined template. In (Shi et al., 2015), the authors focused on number word problems and proposed a system that is created using semi-automatically generated rules. In Zhou et al. (2015), the authors simplified the inference procedure and pushed the state-of-the-art benchmark accuracy. The idea of learning from implicit supervision is discussed in (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015), where the authors train the algebra solvers using only the solutions with little or no annoated equation systems. We discuss this in detail in Section 4. 1 The new resource and the dataset we used for training is available soon on https://aka.ms/dataimplicit and https://aka.ms/datadraw 298 Solving automatic algebra word problems can be viewed as a semantic pa"
D16-1029,Q15-1042,0,\N,Missing
D16-1152,E06-1002,0,0.068515,"disambiguates entities for mentions such as ‘Sox’ (Boston Red Sox vs. Chicago White Sox), ‘Sanders’ (Bernie Sanders vs. Barry Sanders), and ‘Memphis’ (Memphis Grizzlies vs. Memphis, Tennessee), which are mistakenly linked to the other entities or Nil by the mentionentity model. Another example is that the social network information helps the system correctly link ‘Kim’ to Lil’ Kim instead of Kim Kardashian, despite that the latter entity’s wikipedia page is considerably more popular. 6 Related Work Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambigua1459 tion is usually performed by maximizing the global topical coherence between entities. However, these approaches often yield unsatisfactory performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities throug"
D16-1152,D07-1074,0,0.0153479,"or mentions such as ‘Sox’ (Boston Red Sox vs. Chicago White Sox), ‘Sanders’ (Bernie Sanders vs. Barry Sanders), and ‘Memphis’ (Memphis Grizzlies vs. Memphis, Tennessee), which are mistakenly linked to the other entities or Nil by the mentionentity model. Another example is that the social network information helps the system correctly link ‘Kim’ to Lil’ Kim instead of Kim Kardashian, despite that the latter entity’s wikipedia page is considerably more popular. 6 Related Work Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambigua1459 tion is usually performed by maximizing the global topical coherence between entities. However, these approaches often yield unsatisfactory performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities through label propagat"
D16-1152,Q14-1021,1,0.880186,"performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities through label propagation. Shen et al. (2013) employ Twitter user account information to improve entity linking, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Fang and Chang (2014) demonstrate that spatial and temporal signals are critical for the task, and they advance the performance by associating entity prior distributions with different timestamps and locations. Our work overcomes the difficulty by leveraging social relations — socially connected individuals are assumed to share similar interests on entities. As the Twitter post information is often sparse for some users, our assumption enables the utilization of more relevant information that helps to improve the task. NLP with social relations Most previous work on incorporating social relations for NLP problems"
D16-1152,N13-1122,1,0.94033,"ce on embedding information networks (Tang et al., 2015), which induces low-dimensional representations for author nodes based on the network structure. By learning the semantic interactions between the author embeddings and the pre-trained Freebase entity embeddings, the entity linking system can incorporate more disambiguating context from the social network. We also consider lowdimensional representations of mentions, another source of related information for entity linking, with the intuition that semantically related mentions can refer to similar entities. Previously proposed approaches (Guo et al., 2013a; Yang and Chang, 2015) are based on hand-crafted features and off-the-shelf machine learning algorithms. Our preliminary study suggests that simply augmenting the traditional surface features with the distributed representations barely improves the performance of these entity linking systems. Therefore, we propose N TEL, a Neural model for Tweet Entity Linking, to leverage the distributed representations of authors, mentions, and entities. N TEL can not only make efficient use of statistical surface features built from a knowledge base, but also learn the interactions between these distribut"
D16-1152,D13-1085,0,0.619753,"ce on embedding information networks (Tang et al., 2015), which induces low-dimensional representations for author nodes based on the network structure. By learning the semantic interactions between the author embeddings and the pre-trained Freebase entity embeddings, the entity linking system can incorporate more disambiguating context from the social network. We also consider lowdimensional representations of mentions, another source of related information for entity linking, with the intuition that semantically related mentions can refer to similar entities. Previously proposed approaches (Guo et al., 2013a; Yang and Chang, 2015) are based on hand-crafted features and off-the-shelf machine learning algorithms. Our preliminary study suggests that simply augmenting the traditional surface features with the distributed representations barely improves the performance of these entity linking systems. Therefore, we propose N TEL, a Neural model for Tweet Entity Linking, to leverage the distributed representations of authors, mentions, and entities. N TEL can not only make efficient use of statistical surface features built from a knowledge base, but also learn the interactions between these distribut"
D16-1152,P15-1073,0,0.0584107,"mpossible to disambiguate between these entities solely based on the individual text message. We propose to overcome the difficulty and improve the entity disambiguation capability of the entity linking system by employing social network structures. The sociological theory of homophily asserts that socially connected individuals are more likely to have similar behaviors or share similar interests (McPherson et al., 2001). This property has been used to improve many natural language processing tasks such as sentiment analysis (Tan et al., 2011; Yang and Eisenstein, 2015), topic classification (Hovy, 2015) and user attribute inference (Li et al., 2015). We assume Twitter users will have similar interests in real world entities to their near neighbors — an assumption of entity homophily — which 1452 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1452–1461, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics is demonstrated in Figure 1. The social relation between users u1 and u2 may lead to more coherent topics in tweets t1 and t2 . Therefore, by successfully linking the less ambiguous mention ‘Red Sox’ in tweet t2 to"
D16-1152,P14-1036,0,0.01724,"age is considerably more popular. 6 Related Work Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambigua1459 tion is usually performed by maximizing the global topical coherence between entities. However, these approaches often yield unsatisfactory performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities through label propagation. Shen et al. (2013) employ Twitter user account information to improve entity linking, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Fang and Chang (2014) demonstrate that spatial and temporal signals are critical for the task, and they advance the performance by associating entity prior distributions with different timestamps and locations. Our work overcomes the difficul"
D16-1152,N15-1142,0,0.0125348,"mes that Twitter users sharing many neighbors are close to each other in the embedding space. According to the original paper, the second-order proximity yields slightly better performances than the firstorder proximity, which assumes connecting users are close to each other, on a variety of downstream tasks. be written as: Mention embeddings The representation of a mention is the average of embeddings of words it contains. As each mention is typically one to three words, the simple representations often perform surprisingly well (Socher et al., 2013). We adopt the structured skip-gram model (Ling et al., 2015) to learn the word embeddings E(w) on a Twitter corpus with 52 million tweets (Owoputi et al., 2013). The mention vector of the t-th mention candidate can be written as: X 1 (m) (w) vt = (w) vw , (4) |xt | (w) where W(u,e) and W(m,e) are D(u) × D(e) and D(w) × D(e) bilinear transformation matrices. Similar bilinear formulation has been used in the literature of knowledge base completion and inference (Socher et al., 2013; Yang et al., 2014). The parameters of the composition model are Θ2 = {W(u,e) , W(m,e) , E(u) , E(w) , E(e) }. w∈xt (w) where xt is the set of words in the mention. Entity emb"
D16-1152,N13-1039,0,0.0370358,"Missing"
D16-1152,W10-0510,1,0.916876,"Missing"
D16-1152,D11-1141,0,0.0551077,"onger social network ties than directed links (Kwak et al., 2010; Wu et al., 2011). The numbers of social relations for the networks are 1,604, 379 and 342 respectively. 1 We are able to obtain at most 3,200 tweets for each Twitter user, due to the Twitter API limits. Network F OLLOWER M ENTION R ETWEET sim(i ↔ j) sim(i ↔ / j) 0.128 0.121 0.173 0.025 0.025 0.025 Table 2: The average entity-driven similarity results for the networks. Metrics We propose to use the entity-driven similarity between authors to test the hypothesis of entity homophily. For a user ui , we employ a Twitter NER system (Ritter et al., 2011) to detect entity mentions in the timeline, which we use to construct (ent) (ent) a user entity vector ui , so that ui,j = 1 iff user i has mentioned entity j.2 The entity-driven similarity between two users ui and uj is defined as the cosine similarity score between the vectors (ent) (ent) ui and uj . We evaluate the three networks by calculating the average entity-driven similarity of the connected user pairs and that of the disconnected user pairs, which we name as sim(i ↔ j) and sim(i ↔ / j). Results The entity-driven similarity results of these networks are presented in Table 2. As shown,"
D16-1152,W11-2207,0,0.106461,"work overcomes the difficulty by leveraging social relations — socially connected individuals are assumed to share similar interests on entities. As the Twitter post information is often sparse for some users, our assumption enables the utilization of more relevant information that helps to improve the task. NLP with social relations Most previous work on incorporating social relations for NLP problems focuses on Twitter sentiment analysis, where the existence of social relations between users is considered as a clue that the sentiment polarities of messages from the users should be similar. Speriosu et al. (2011) construct a heterogeneous network with tweets, users, and n-grams as nodes, and the sentiment label distributions associated with the nodes are refined by performing label propagation over social relations. Tan et al. (2011) and Hu et al. (2013) leverage social relations for sentiment analysis by exploiting a factor graph model and the graph Laplacian technique respectively, so that the tweets belonging to social connected users share similar label distributions. We work on entity linking in Twitter messages, where the label space is much larger than that of sentiment classification. The soci"
D16-1152,P15-1049,1,0.811948,"ormation networks (Tang et al., 2015), which induces low-dimensional representations for author nodes based on the network structure. By learning the semantic interactions between the author embeddings and the pre-trained Freebase entity embeddings, the entity linking system can incorporate more disambiguating context from the social network. We also consider lowdimensional representations of mentions, another source of related information for entity linking, with the intuition that semantically related mentions can refer to similar entities. Previously proposed approaches (Guo et al., 2013a; Yang and Chang, 2015) are based on hand-crafted features and off-the-shelf machine learning algorithms. Our preliminary study suggests that simply augmenting the traditional surface features with the distributed representations barely improves the performance of these entity linking systems. Therefore, we propose N TEL, a Neural model for Tweet Entity Linking, to leverage the distributed representations of authors, mentions, and entities. N TEL can not only make efficient use of statistical surface features built from a knowledge base, but also learn the interactions between these distributed representations. Our"
D16-1152,P15-1128,1,0.786133,"Missing"
D17-1252,Q13-1005,0,0.0274295,"parse is fully constructed. Early model update before the search of a full semantic parse is complete is generally infeasible.1 It is also not clear how to leverage implicit and explicit signals integrally during learning when both kinds of labels are present. In this work, we propose Maximum Margin Reward Networks (MMRN), which is a general neural network-based framework that is able to learn from both implicit and explicit supervision signals. By casting structured-output learning as a search problem, the key insight in MMRN is the 1 Existing weakly supervised methods (Clarke et al., 2010; Artzi and Zettlemoyer, 2013) often leverage domain-specific heuristics, which are not always available. 2368 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2368–2378 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics special mechanism of rewards. Rewards can be viewed as the training signals that drive the model to explore the search space and to find the correct structure. The explicit supervision signals can be viewed as a source of immediate rewards, as we can often instantly know the correctness of the current action. On the other"
D17-1252,D13-1160,0,0.33632,"on 1” (Y (s)). By comparing Y (s) to the answer set A, the precision is 16 and the recall is 1. Therefore, the F1 score used for the reward is 27 . action a to state s0 . Let Y (s) be the set of predicted answers generated from state s, and Y (s) = {} when s is not a legitimate semantic parse. The reward function R(s0 , a) can be defined by comparing Y (s) and the labeled answers, A, to the input question. While a set similarity function like the Jaccard coefficient can be used as the reward function, we chose the F1 score in this work as it was used as the evaluation metric in previous work (Berant et al., 2013). Figure 3 shows an example of this reward function. 4.2 Max-Margin Loss & Learning Algorithm The MMRN learning algorithm can be viewed as an extension of M3 N (Taskar et al., 2004) and Structured SVM (Joachims et al., 2009; Yu and Joachims, 2009). The learning algorithm takes three steps, where the first two involve two different search procedures. The final step is to update the models with respect to the inference results. Finding the best path The first search step is to find the best path h∗ by solving the following optimization problem: h∗ = arg max R(h; y) + fθ (h). h∈E(x) (1) The firs"
D17-1252,W10-2903,1,0.538838,"ledge base) after the parse is fully constructed. Early model update before the search of a full semantic parse is complete is generally infeasible.1 It is also not clear how to leverage implicit and explicit signals integrally during learning when both kinds of labels are present. In this work, we propose Maximum Margin Reward Networks (MMRN), which is a general neural network-based framework that is able to learn from both implicit and explicit supervision signals. By casting structured-output learning as a search problem, the key insight in MMRN is the 1 Existing weakly supervised methods (Clarke et al., 2010; Artzi and Zettlemoyer, 2013) often leverage domain-specific heuristics, which are not always available. 2368 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2368–2378 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics special mechanism of rewards. Rewards can be viewed as the training signals that drive the model to explore the search space and to find the correct structure. The explicit supervision signals can be viewed as a source of immediate rewards, as we can often instantly know the correctness of th"
D17-1252,W02-1001,0,0.0461079,"start the description of our method from the search formulation and the state–action spaces in our targeted tasks in Sec. 3, followed by the reward and learning algorithm in Sec. 4 and the detailed neural model design in Sec. 5. Sec. 6 reports the experimental results and Sec. 8 concludes the paper. 2 Related Work Structured output prediction tasks have been studied extensively in the field of natural language processing (NLP). Many supervised structured learning algorithms has been proposed for capturing the relationships between output variables. These models include structured perceptron (Collins, 2002; Collins and Roark, 2004), conditional random fields (Lafferty et al., 2001), and structured SVM (Taskar et al., 2004; Joachims et al., 2009). Later, the learning to search framework is proposed (Daum´e and Marcu, 2005; Daum´e et al., 2009), which casts the structured prediction task as a general search problem. Most recently, recurrent neural networks such as LSTM models (Hochreiter and Schmidhuber, 1997) have been used as a general tool for structured output models (Vinyals et al., 2015). Latent structured learning algorithms address the problem of learning from incomplete labeled data (Yu"
D17-1252,P04-1015,0,0.0198179,"ription of our method from the search formulation and the state–action spaces in our targeted tasks in Sec. 3, followed by the reward and learning algorithm in Sec. 4 and the detailed neural model design in Sec. 5. Sec. 6 reports the experimental results and Sec. 8 concludes the paper. 2 Related Work Structured output prediction tasks have been studied extensively in the field of natural language processing (NLP). Many supervised structured learning algorithms has been proposed for capturing the relationships between output variables. These models include structured perceptron (Collins, 2002; Collins and Roark, 2004), conditional random fields (Lafferty et al., 2001), and structured SVM (Taskar et al., 2004; Joachims et al., 2009). Later, the learning to search framework is proposed (Daum´e and Marcu, 2005; Daum´e et al., 2009), which casts the structured prediction task as a general search problem. Most recently, recurrent neural networks such as LSTM models (Hochreiter and Schmidhuber, 1997) have been used as a general tool for structured output models (Vinyals et al., 2015). Latent structured learning algorithms address the problem of learning from incomplete labeled data (Yu and Joachims, 2009; Quatto"
D17-1252,P16-1004,0,0.0327797,"there are no gold parses, a model needs to explore different parses, where their quality can only be indirectly verified by comparing retrieved answers and the labeled answers. Introduction Structured-output prediction problems, where the goal is to determine values of a set of interdependent variables, are ubiquitous in NLP. Structures of such problems can range from simple sequences like part-of-speech tagging (Ling et al., 2015) and named entity recognition (Lample et al., 2016), to complex syntactic or semantic analysis such as dependency parsing (Dyer et al., 2015) and semantic parsing (Dong and Lapata, 2016). Stateof-the-art methods of these tasks are often neural network models trained using fully annotated structures, which can be costly or time-consuming to obtain. Weakly supervised learning settings, where the algorithm assumes only the existence of implicit signals on whether a prediction is correct, are thus more appealing in many scenarios. For example, Figure 1 shows a weakly supervised setting of learning semantic parsers using only question–answer pairs. When the system generates a candidate semantic parse during training, the quality needs to be indirectly measured by comparing the der"
D17-1252,P15-1033,0,0.0085205,"ervision signals (labeled answers). Since there are no gold parses, a model needs to explore different parses, where their quality can only be indirectly verified by comparing retrieved answers and the labeled answers. Introduction Structured-output prediction problems, where the goal is to determine values of a set of interdependent variables, are ubiquitous in NLP. Structures of such problems can range from simple sequences like part-of-speech tagging (Ling et al., 2015) and named entity recognition (Lample et al., 2016), to complex syntactic or semantic analysis such as dependency parsing (Dyer et al., 2015) and semantic parsing (Dong and Lapata, 2016). Stateof-the-art methods of these tasks are often neural network models trained using fully annotated structures, which can be costly or time-consuming to obtain. Weakly supervised learning settings, where the algorithm assumes only the existence of implicit signals on whether a prediction is correct, are thus more appealing in many scenarios. For example, Figure 1 shows a weakly supervised setting of learning semantic parsers using only question–answer pairs. When the system generates a candidate semantic parse during training, the quality needs t"
D17-1252,Q14-1021,1,0.761333,"Missing"
D17-1252,D13-1085,0,0.0200214,"am search, and the reward function is simply the number of correct tag assignments to the words. The results are shown in Table 1, compared with recently proposed systems based on neural models. When the beam size is set to 20, MMRN achieves 91.4, which is the best published result so far (without using any gazetteers). Notice that when beam size is 5, the performance drops to 90.03. This demonstrates the importance of search quality when applying MMRN. 6.2 Entity linking For entity linking, we adopt two publicly available datasets for tweet entity linking: NEEL (Cano et al., 2014)4 and TACL (Guo et al., 2013; Fang 3 Available at http://nlp.stanford.edu/projects/glove/ 4 NEEL dataset was originally created for an entity linking competition: http://microposts2016.seas. upenn.edu/challenge.html and Chang, 2014; Yang and Chang, 2015; Yang et al., 2016). We follow prior works (Guo et al., 2013; Yang and Chang, 2015) and perform the standard evaluation for an end-to-end entity linking system by computing precision, recall, and F1 scores, according to the entity references and the system output. An output entity is considered correct if it matches the gold entity and the mention boundary overlaps with t"
D17-1252,P17-1167,1,0.0701783,"RN can be more efficient than REINFORCE, as MMRN can use the reward signals of multiple paths more effectively. In addition, MMRN is not a probabilistic model, so it does not need to handle normalization issues, which often causes large variance in estimating the gradient direction when optimizing the expected reward. Semantic Parsing MMRN can be applied for many semantic parsing tasks. One key step is to design the right approximated reward for a given task to guide the beam search to nd the reference parses in MMRN, given that the actual reward is often very sparse. In our companion paper, (Iyyer et al., 2017), we used a simple form of approximated reward to get feedback as early as possible during search. In other words, the semantic parse will be executed as soon as the parse is executable (even if the parse is still not completed) during search. The execution results will be used to calculate the Jaccard coefficient with respect to the labeled answers as the approximated rewards. The use of approximated reward has been proven to be effective in (Iyyer et al., 2017). An important research direction for semantic parsing is to reduce the supervision cost. In (Yih et al., 2016), the authors demonstr"
D17-1252,N16-1030,0,0.00488354,"ner, Alec Sulkin KB A: Lacey Chabert Figure 1: Learning a semantic parser using implicit supervision signals (labeled answers). Since there are no gold parses, a model needs to explore different parses, where their quality can only be indirectly verified by comparing retrieved answers and the labeled answers. Introduction Structured-output prediction problems, where the goal is to determine values of a set of interdependent variables, are ubiquitous in NLP. Structures of such problems can range from simple sequences like part-of-speech tagging (Ling et al., 2015) and named entity recognition (Lample et al., 2016), to complex syntactic or semantic analysis such as dependency parsing (Dyer et al., 2015) and semantic parsing (Dong and Lapata, 2016). Stateof-the-art methods of these tasks are often neural network models trained using fully annotated structures, which can be costly or time-consuming to obtain. Weakly supervised learning settings, where the algorithm assumes only the existence of implicit signals on whether a prediction is correct, are thus more appealing in many scenarios. For example, Figure 1 shows a weakly supervised setting of learning semantic parsers using only question–answer pairs."
D17-1252,W16-3920,0,0.0145837,"n scoring model for EL. The model of the action scoring function fθ (s, a) is depicted in Figure 4, which is basically the dot product of the action embedding and state embedding. The action embedding is initialized randomly for each action, but can be fine-tuned during training (i.e. back-propagate the error through the network and update the word/entity type embeddings). The state embedding is the concatenation of bi-LSTM word embeddings of the current word, the character-based word embeddings, and the embedding of the previous action. We also include the orthographic embeddings proposed by Limsopatham and Collier (2016). Entity Linking An action in entity linking is to determine whether a mention should be linked to a particular entity (cf. Sec. 3.2). As shown in Figure 5, we design the scoring function as a feed-forward neural network that takes as input three different input vectors: (1) surface features from hand-crafted mention-entity statistics that are similar to the ones used in (Yang and Chang, 2015); (2) mention context embeddings from a bidirectional LSTM module; (3) entity embeddings constructed from entity type embeddings. All these embeddings, except the feature vectors, are fine-tuned during tr"
D17-1252,P16-1101,0,0.0198869,"Missing"
D17-1252,D16-1261,0,0.00878913,"d function L(h, h∗ ) is our optimization goal, where we want can be thought of as estimating whether there exto update the model by fixing the biggest violation. ists a high-reward state that is reachable from the Note that the associated constraint is only violated ∗ current state. The effectiveness of this strategy has when L(h, h ) is positive. To find the path h in been demonstrated successfully by several recent this step that maximizes the violation is equivalent efforts (Mnih et al., 2013; Krishnamurthy et al., to maximizing fθ (h) − R(h), given that the rest 2015; Silver et al., 2016; Narasimhan et al., 2016). of the terms are constant with respect to h. When there exist only explicit supervision sig5 Neural Architectures nals, our objective function reduces to the one for optimizing structured SVM without regularWhile the learning algorithm of MMRN described in ization. For implicit signals, we find h∗ approxiSec. 4 is general, the exact model design is taskmately before we optimize the margin loss. In this dependent. In this section, we describe in detail case, the search is not exact as the reward signals the neural network architectures of the three tarare delayed. Nevertheless, we found the m"
D17-1252,D14-1162,0,0.103477,".59 90.10 90.77 90.88 90.94 91.21 MMRN-NER Beam = 5 MMRN-NER Beam = 20 90.03 91.39 Table 1: Explicit Supervision: Named Entity Recognition. Our MMRN with beam size 20 outperforms current best systems, which are based on neural networks. NEEL-Test F1 S-MART 77.7 77.9 NTEL MMRN-EL 78.5 MMRN-EL - Entity 77.4 76.6 MMRN-EL - LSTM TACL F1 63.6 68.1 67.5 66.5 66.0 Table 2: Explicit Supervision: Entity Linking. Our system trained with MMRN is comparable to the state-of-art NTEL system. metric is the F1 score. The pre-trained word embeddings are 100-dimension GloVe vectors trained on 6 billion tokens (Pennington et al., 2014)3 . The search procedure is conducted using beam search, and the reward function is simply the number of correct tag assignments to the words. The results are shown in Table 1, compared with recently proposed systems based on neural models. When the beam size is set to 20, MMRN achieves 91.4, which is the best published result so far (without using any gazetteers). Notice that when beam size is 5, the performance drops to 90.03. This demonstrates the importance of search quality when applying MMRN. 6.2 Entity linking For entity linking, we adopt two publicly available datasets for tweet entity"
D17-1252,W09-1119,0,0.010576,"Missing"
D17-1252,D16-1029,1,0.850528,"h framework is proposed (Daum´e and Marcu, 2005; Daum´e et al., 2009), which casts the structured prediction task as a general search problem. Most recently, recurrent neural networks such as LSTM models (Hochreiter and Schmidhuber, 1997) have been used as a general tool for structured output models (Vinyals et al., 2015). Latent structured learning algorithms address the problem of learning from incomplete labeled data (Yu and Joachims, 2009; Quattoni et al., 2007). The main difference compared to our framework is the existence of the external environment when learning from implicit signals. Upadhyay et al. (2016) first proposed the idea of learning from implicit supervision, and is the most related paper to our work. Compared to their linear algorithm, our framework is more principled and general as we integrate the concept of margin in our method. Furthermore, we also extend the framework using neural models. 3 Search-based Inference In our framework, predicting the best structured output, inference, is formulated as a state/action search problem. Our search space can be described as follows. The initial state, s0 , is the starting point of the search process. We define γ(s) as the set of all feasibl"
D17-1252,P15-1049,1,0.265692,"ncatenation of bi-LSTM word embeddings of the current word, the character-based word embeddings, and the embedding of the previous action. We also include the orthographic embeddings proposed by Limsopatham and Collier (2016). Entity Linking An action in entity linking is to determine whether a mention should be linked to a particular entity (cf. Sec. 3.2). As shown in Figure 5, we design the scoring function as a feed-forward neural network that takes as input three different input vectors: (1) surface features from hand-crafted mention-entity statistics that are similar to the ones used in (Yang and Chang, 2015); (2) mention context embeddings from a bidirectional LSTM module; (3) entity embeddings constructed from entity type embeddings. All these embeddings, except the feature vectors, are fine-tuned during training. Some unique properties of our entity linking model are worth noticing. First, we add mention context embeddings from a bidirectional LSTM module as additional input. While using LSTMs is a common practice for sequence labeling, it is not usually used for short-text entity linking. For each mention, we only extract the output from the bi-LSTM module at the start and end tokens of the me"
D17-1252,D16-1152,1,0.83286,"1.4, which is the best published result so far (without using any gazetteers). Notice that when beam size is 5, the performance drops to 90.03. This demonstrates the importance of search quality when applying MMRN. 6.2 Entity linking For entity linking, we adopt two publicly available datasets for tweet entity linking: NEEL (Cano et al., 2014)4 and TACL (Guo et al., 2013; Fang 3 Available at http://nlp.stanford.edu/projects/glove/ 4 NEEL dataset was originally created for an entity linking competition: http://microposts2016.seas. upenn.edu/challenge.html and Chang, 2014; Yang and Chang, 2015; Yang et al., 2016). We follow prior works (Guo et al., 2013; Yang and Chang, 2015) and perform the standard evaluation for an end-to-end entity linking system by computing precision, recall, and F1 scores, according to the entity references and the system output. An output entity is considered correct if it matches the gold entity and the mention boundary overlaps with the gold mention boundary. Interested readers can refer to (Carmel et al., 2014) for more detail. We initialize the word embeddings from pretrained GloVe vectors trained on the twitter corpus, and type embeddings from the pre-trained skip-gram mo"
D17-1252,P15-1128,1,0.669068,"ample question “who played meg in season 1 of family guy”, assuming the knowledge base is Freebase (Bollacker et al., 2008). An entity linking component plays an important role by mapping “meg” to MegGriffin and “season 1 of family guy” to FamilyGuySeason1. Predicates like cast, actor and character are also from the knowledge base that define the relationships between these entities and the answer. Together the semantic parse in λ-calculus is shown in the top of Figure 2. Equivalently, the semantic parse can be represented as a query graph (Figure 2 bottom), which is used in the STAGG system (Yih et al., 2015). The nodes are either grounded entities or variables, where x is the answer entity. The edges denote the relationship between two entities. Regardless of the choice of the formal language, the process of constructing the semantic parse is typically formulated as a search problem. A state is essentially a partial or complete semantic parse, and an action is to extend the current semantic parse by adding a new relation or constraint. Different from previous systems which treat entity linking as a static component, our search space consists of the search space of both entity linking and semantic"
D17-1252,P16-2033,1,0.39892,"thm. Results of entity linking experiments are presented in Table 2, which are compared with those of S-MART (Yang and Chang, 2015)6 and NTEL (Yang et al., 2016)7 , two state-of-the-art entity linking systems for short texts. Our MMRN-EL is comparable to the best system. We also conducted two ablation studies by removing the entity type vectors (MMRN-EL - Entity), and by removing the LSTM vectors (MMRN-EL - LSTM). Both show significant performance drops, which validates the importance of these two additional input vectors. 6.3 Semantic parsing For semantic parsing, we use the dataset WebQSP8 (Yih et al., 2016) in our experiments. This dataset is a clean and enhanced version of the widely used WebQuestions dataset (Berant et al., 2013), which consists of pairs of questions and answers found in Freebase. Compared to WebQuestions, WebQSP excludes questions with ambiguous intent, and provides verified answers and full semantic parses to the remaining 4,737 questions. We follow the implicit supervision setting in (Yih et al., 2016), using 3, 098 question–answer pairs for training, and 1, 639 for testing. A subset of 620 pairs from the training set is used for hyperparameter tuning. Because there can be"
D18-1266,Q13-1005,0,0.294332,"er: England Figure 1: An example of semantic parsing from denotations. Given the table environment, map the question to an executable program that evaluates to the answer. Introduction Semantic parsing from denotations (SpFD) is the problem of mapping text to executable formal representations (or program) in a situated environment and executing them to generate denotations (or answer), in the absence of access to correct representations. Several problems have been handled within this framework, including question answering (Berant et al., 2013; Iyyer et al., 2017) and instructions for robots (Artzi and Zettlemoyer, 2013; Misra et al., 2015). Consider the example in Figure 1. Given the question and a table environment, a semantic parser maps the question to an executable program, in this case a SQL query, and then executes the query on the environment to generate the answer England. In the SpFD setting, the training data does not contain the correct programs. Thus, the existing learning approaches for SpFD perform two steps for every training example, a search step that explores the space of programs and finds suitable candidates, and an update step that uses these programs to update the model. Figure 2 shows"
D18-1266,D13-1160,0,0.690977,"s Wales 25 5 5 8.8 Program: Select Nation Where Points is Maximum Answer: England Figure 1: An example of semantic parsing from denotations. Given the table environment, map the question to an executable program that evaluates to the answer. Introduction Semantic parsing from denotations (SpFD) is the problem of mapping text to executable formal representations (or program) in a situated environment and executing them to generate denotations (or answer), in the absence of access to correct representations. Several problems have been handled within this framework, including question answering (Berant et al., 2013; Iyyer et al., 2017) and instructions for robots (Artzi and Zettlemoyer, 2013; Misra et al., 2015). Consider the example in Figure 1. Given the question and a table environment, a semantic parser maps the question to an executable program, in this case a SQL query, and then executes the query on the environment to generate the answer England. In the SpFD setting, the training data does not contain the correct programs. Thus, the existing learning approaches for SpFD perform two steps for every training example, a search step that explores the space of programs and finds suitable candidates, a"
D18-1266,D16-1245,0,0.0689966,"Missing"
D18-1266,W10-2903,1,0.86718,"eralization in model shaping. 6 Related Work Semantic Parsing from Denotation Mapping natural language text to formal meaning representation was first studied by Montague (1970). Early work on learning semantic parsers rely on labeled formal representations as the supervision signals (Zettlemoyer and Collins, 2005, 2007; Zelle and Mooney, 1993). However, because getting access to gold formal representation generally requires expensive annotations by an expert, distant supervision approaches, where semantic parsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017). Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates. Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man2449 Question “of these teams, which had m"
D18-1266,P17-1097,0,0.502504,"s approach can suffer from a divergence phenomenon whereby the score of spurious programs picked up by the search in the first epoch increases, making it more likely for the search to pick them up in the future. Such divergence issues are common with latent-variable learning and often require careful initialization to overcome (Rose, 1998). Unfortunately such initialization schemes are not applicable for deep neural networks which form the model of most successful semantic parsers today (Jia and Liang, 2016; Misra and Artzi, 2016; Iyyer et al., 2017). Prior work, such as ✏-greedy exploration (Guu et al., 2017), has reduced the severity of this problem by introducing random noise in the search procedure to avoid saturating the search on high-scoring spurious programs. However, random noise need not bias the search towards the correct program(s). In this paper, we introduce a simple policy-shaping method to guide the search. This approach allows incorporating prior knowledge in the exploration policy and can bias the search away from spurious programs. 2444 1 This transformation preserves the answer of the question. Algorithm 1 Learning a semantic parser from denotation using generalized updates. Inp"
D18-1266,N12-1015,0,0.0607653,"Missing"
D18-1266,P17-1167,1,0.824952,"Missing"
D18-1266,P16-1002,0,0.0489753,"policy which is based on the score function, for example b✓ (y|x, t, z) / exp{score✓ (y, t)}. However, this approach can suffer from a divergence phenomenon whereby the score of spurious programs picked up by the search in the first epoch increases, making it more likely for the search to pick them up in the future. Such divergence issues are common with latent-variable learning and often require careful initialization to overcome (Rose, 1998). Unfortunately such initialization schemes are not applicable for deep neural networks which form the model of most successful semantic parsers today (Jia and Liang, 2016; Misra and Artzi, 2016; Iyyer et al., 2017). Prior work, such as ✏-greedy exploration (Guu et al., 2017), has reduced the severity of this problem by introducing random noise in the search procedure to avoid saturating the search on high-scoring spurious programs. However, random noise need not bias the search towards the correct program(s). In this paper, we introduce a simple policy-shaping method to guide the search. This approach allows incorporating prior knowledge in the exploration policy and can bias the search away from spurious programs. 2444 1 This transformation preserves the answ"
D18-1266,D17-1160,0,0.302332,"n frequently co-occurring pairs of tokens in the program and instruction. For example, the token most is highly likely to co-occur with a correct program containing the keyword Max. This happens for the example in Figure 2. Similarly the token not may co-occur with the keyword NotEqual. We assume access to a lexicon ⇤ = {(wj , !j )}kj=1 containing (3) Addressing Update Strategy Selection: Generalized Update Equation Given the set of programs generated by the search step, one can use many objectives to update the parameters. For example, previous work have utilized maximum marginal likelihood (Krishnamurthy et al., 2017; Guu et al., 2017), reinforcement learning (Zhong et al., 2017; Guu et al., 2017) and margin based methods (Iyyer et al., 2017). It could be difficult to choose the suitable algorithm from these options. In this section, we propose a principle and general update equation such that previous update algorithms can be considered as special cases to this equation. Having a general update is important for the following reasons. First, it allows us to understand existing algorithms better by examining their basic properties. Second, the generalized update equation also makes it easy to implement and"
D18-1266,D15-1032,0,0.0173733,"derperform maximum marginal likelihood approaches. Our result on the SQA dataset supports their observation. However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods. Margin-based Learning Margin-based methods have been considered in the context of SVM learning. In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the most violated example. Latent Variable Modeling Learning semantic pars"
D18-1266,D16-1262,0,0.0554985,"sed learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the most violated example. Latent Variable Modeling Learning semantic parsers from denotation can be viewed as a latent variable modeling problem, where the program is the latent variable. Probabilistic latent variable models have been studied using EM-algorithm and its variant (Dempster et al., 1977). The graphical model literature has studied latent variable learning on margin-based methods (Yu and Joachims, 2009) and probabilistic m"
D18-1266,D16-1127,0,0.0263699,"expensive annotations by an expert, distant supervision approaches, where semantic parsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017). Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates. Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man2449 Question “of these teams, which had more than 21 losses?&quot; “of the remaining, which earned the most bronze medals?&quot; “of those competitors from germany, which was not paul sievert?&quot; without policy shaping SELECT Club WHERE Losses = ROW 15 SELECT Nation WHERE Rank = ROW 1 SELECT Name WHERE Time (hand) = ROW 3 with policy shaping SELECT Club WHERE Losses > 21 FollowUp WHERE Bronze is Max FollowUp WHERE Name != ROW 5 Table 4: Training examples and the highest ran"
D18-1266,P11-1060,0,0.4172,"Learning Learning a semantic parser is equivalent to learning the parameters ✓ in the scoring function, which is a structured learning problem, due to the large, structured output space Y. Structured learning algorithms generally consist of two major components: search and update. When the gold programs are available during training, the search procedure finds a set of high-scoring incorrect programs. These programs are used by the update step to derive loss for updating parameters. For example, these programs are used for approximating the partition-function in maximum-likelihood objective (Liang et al., 2011) and finding set of programs causing margin violation in margin based methods (Daumé III and Marcu, 2005). Depending on the exact algorithm being used, these two components are not necessarily separated into isolated steps. For instance, parameters can be updated in the middle of search (e.g., Huang et al., 2012). For learning semantic parsers from denotations, where we assume only answers are available in a training set {(xi , ti , zi )}N i=1 of N examples, the basic construction of the learning algorithms remains the same. However, the problems that search needs to handle in SpFD is more cha"
D18-1266,P05-1012,0,0.147574,"s. ning, 2016), semantic parsing (Guu et al., 2017) and instruction following (Misra et al., 2017). Guu et al. (2017) show that policy gradient methods underperform maximum marginal likelihood approaches. Our result on the SQA dataset supports their observation. However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods. Margin-based Learning Margin-based methods have been considered in the context of SVM learning. In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms"
D18-1266,D17-1106,1,0.8804,"Missing"
D18-1266,D16-1183,1,0.853215,"d on the score function, for example b✓ (y|x, t, z) / exp{score✓ (y, t)}. However, this approach can suffer from a divergence phenomenon whereby the score of spurious programs picked up by the search in the first epoch increases, making it more likely for the search to pick them up in the future. Such divergence issues are common with latent-variable learning and often require careful initialization to overcome (Rose, 1998). Unfortunately such initialization schemes are not applicable for deep neural networks which form the model of most successful semantic parsers today (Jia and Liang, 2016; Misra and Artzi, 2016; Iyyer et al., 2017). Prior work, such as ✏-greedy exploration (Guu et al., 2017), has reduced the severity of this problem by introducing random noise in the search procedure to avoid saturating the search on high-scoring spurious programs. However, random noise need not bias the search towards the correct program(s). In this paper, we introduce a simple policy-shaping method to guide the search. This approach allows incorporating prior knowledge in the exploration policy and can bias the search away from spurious programs. 2444 1 This transformation preserves the answer of the question. Alg"
D18-1266,P15-1096,1,0.853345,"ple of semantic parsing from denotations. Given the table environment, map the question to an executable program that evaluates to the answer. Introduction Semantic parsing from denotations (SpFD) is the problem of mapping text to executable formal representations (or program) in a situated environment and executing them to generate denotations (or answer), in the absence of access to correct representations. Several problems have been handled within this framework, including question answering (Berant et al., 2013; Iyyer et al., 2017) and instructions for robots (Artzi and Zettlemoyer, 2013; Misra et al., 2015). Consider the example in Figure 1. Given the question and a table environment, a semantic parser maps the question to an executable program, in this case a SQL query, and then executes the query on the environment to generate the answer England. In the SpFD setting, the training data does not contain the correct programs. Thus, the existing learning approaches for SpFD perform two steps for every training example, a search step that explores the space of programs and finds suitable candidates, and an update step that uses these programs to update the model. Figure 2 shows the two step trainin"
D18-1266,D15-1001,0,0.0445552,"distant supervision approaches, where semantic parsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017). Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates. Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man2449 Question “of these teams, which had more than 21 losses?&quot; “of the remaining, which earned the most bronze medals?&quot; “of those competitors from germany, which was not paul sievert?&quot; without policy shaping SELECT Club WHERE Losses = ROW 15 SELECT Nation WHERE Rank = ROW 1 SELECT Name WHERE Time (hand) = ROW 3 with policy shaping SELECT Club WHERE Losses > 21 FollowUp WHERE Bronze is Max FollowUp WHERE Name != ROW 5 Table 4: Training examples and the highest ranked program in the beam search, scored accor"
D18-1266,D16-1261,0,0.0160404,"rsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017). Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates. Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man2449 Question “of these teams, which had more than 21 losses?&quot; “of the remaining, which earned the most bronze medals?&quot; “of those competitors from germany, which was not paul sievert?&quot; without policy shaping SELECT Club WHERE Losses = ROW 15 SELECT Nation WHERE Rank = ROW 1 SELECT Name WHERE Time (hand) = ROW 3 with policy shaping SELECT Club WHERE Losses > 21 FollowUp WHERE Bronze is Max FollowUp WHERE Name != ROW 5 Table 4: Training examples and the highest ranked program in the beam search, scored according to the shaped policy, after training with MAV"
D18-1266,N12-1087,1,0.808266,"of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the most violated example. Latent Variable Modeling Learning semantic parsers from denotation can be viewed as a latent variable modeling problem, where the program is the latent variable. Probabilistic latent variable models have been studied using EM-algorithm and its variant (Dempster et al., 1977). The graphical model literature has studied latent variable learning on margin-based methods (Yu and Joachims, 2009) and probabilistic models (Quattoni et al., 2007). Samdani et al. (2012) studied various variants of EM algorithm and showed that all of them are special cases of a unified framework. Our generalized update framework is similar in spirit. 7 Conclusion In this paper, we propose a general update equation from semantic parsing from denotation and propose a policy shaping method for addressing the spurious program challenge. For the future, we plan to apply the proposed learning framework to more semantic parsing tasks and consider new methods for policy shaping. 8 Acknowledgements We thank Ryan Benmalek, Alane Suhr, Yoav Artzi, Claire Cardie, Chris Quirk, Michel Gall"
D18-1266,W04-3201,0,0.0982967,"tep for these examples. ning, 2016), semantic parsing (Guu et al., 2017) and instruction following (Misra et al., 2017). Guu et al. (2017) show that policy gradient methods underperform maximum marginal likelihood approaches. Our result on the SQA dataset supports their observation. However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods. Margin-based Learning Margin-based methods have been considered in the context of SVM learning. In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning,"
D18-1266,D07-1080,0,0.0177084,"al., 2017). Guu et al. (2017) show that policy gradient methods underperform maximum marginal likelihood approaches. Our result on the SQA dataset supports their observation. However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods. Margin-based Learning Margin-based methods have been considered in the context of SVM learning. In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the mo"
D18-1266,D07-1071,0,0.197773,"Missing"
E17-1047,D13-1160,0,0.0444415,"Missing"
E17-1047,P09-1011,0,0.0107587,"relevant to solving the word problem. As we only annotate relevant numbers in our annotations, our datasets can provide high quality supervision for such classifiers. The datasets can also be used in evaluation test-beds, like the one proposed in (Koncel-Kedziorski et al., 2016). We hope our datasets will open new possibilities for the community to simulate new ideas and applications for automatic problem solvers. Labeling Semantic Parses Similar to our work, efforts have been made to annotate semantic parses for other tasks, although primarily for providing supervision. Prior to the works of Liang et al. (2009) and Clarke et al. (2010), semantic parsers were trained using annotated logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007, inter alia), which were expensive to annotate. Recently, Yih et al. (2016) showed that labeled semantic parses for the knowledge based question answering task can be obtained at a cost comparable to obtaining answers. They showed significant improvements in performance of a questionanswering system using the labeled parses instead of answers for training. More recently, by treating word problems as a semantic parsing task, Upadhya"
E17-1047,W10-2903,1,0.672157,"word problem. As we only annotate relevant numbers in our annotations, our datasets can provide high quality supervision for such classifiers. The datasets can also be used in evaluation test-beds, like the one proposed in (Koncel-Kedziorski et al., 2016). We hope our datasets will open new possibilities for the community to simulate new ideas and applications for automatic problem solvers. Labeling Semantic Parses Similar to our work, efforts have been made to annotate semantic parses for other tasks, although primarily for providing supervision. Prior to the works of Liang et al. (2009) and Clarke et al. (2010), semantic parsers were trained using annotated logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007, inter alia), which were expensive to annotate. Recently, Yih et al. (2016) showed that labeled semantic parses for the knowledge based question answering task can be obtained at a cost comparable to obtaining answers. They showed significant improvements in performance of a questionanswering system using the labeled parses instead of answers for training. More recently, by treating word problems as a semantic parsing task, Upadhyay et al. (2016) found tha"
E17-1047,P11-1060,0,0.0287889,"formance of a questionanswering system using the labeled parses instead of answers for training. More recently, by treating word problems as a semantic parsing task, Upadhyay et al. (2016) found that joint learning using both explicit (derivation as labeled semantic parses) and implicit supervision signals (solution as responses) can significantly outperform models trained using only one type of supervision signal. Other Semantic Parsing Tasks We demonstrated that response-based evaluation, which is quite popular for most semantic parsing problems (Zelle and Mooney, 1996; Berant et al., 2013; Liang et al., 2011, inter alia) can overlook reasoning errors for algebra problems. A reason for this is that in algebra word problems there can be several semantic parses (i.e., derivations, both correct and incorrect) that can lead to the correct solution using the input (i.e., textual number in word problem). This is not the case for semantic parsing problems like knowledge based question answering, as correct semantic parse can often be identified given the question and the answer. For instance, paths in the knowledge base (KB), that connect the answer and the entities in the question can be interpreted as"
E17-1047,W02-1001,0,0.135657,"Only the (x, y) pairs are provided as supervision. Similar to (Kushman et al., 2014; Zhou et al., 2015), the solver finds a derivation which agrees with the equation system and the solution, and trains on it. Note that the derivation found by the solver may be incorrect. ˆ identified by and scored. The equation system y ˆ is output as the prehighest scoring derivation z diction. Following (Zhou et al., 2015), we do not model the alignment of nouns phrases to variables, allowing for tractable inference when scoring the generated derivations. The solver is trained using a structured perceptron (Collins, 2002). We extract the following features for a (x, z) pair, TD (T RAIN ON D ERIVATION ) (x, z) pairs obtained by the derivation annotation are used as supervision. This setting trains the solver on humanlabeled derivations. Clearly, the TD setting is a more informative supervision strategy than the TE setting. TD provides the correct template and correct alignment (i.e. labeled derivation) as supervision and is expected to perform better than TE, which only provides the question-equation pair. We first present the main results comparing different evaluation metrics on solvers trained using the two"
E17-1047,P14-5010,0,0.00283642,"e marked in the EquivTNum field in the annotation. If two textual numbers q, q 0 ∈ EquivTNum, then we can align a coefficient slot to either q or q 0 , and generate a equivalent alignment. An alignment A and a template T together identify a derivation z = (T, A) of an equation system. Note that there may be multiple valid derivations, using one of the equivalent alignments. We assume there exists a routine Solve(y) that find the solution of an equation system. We use a Gaussian elimination solver for our Solve routine. We use hand-written rules and the quantity normalizer in Stanford CoreNLP (Manning et al., 2014) to identify textual numbers. Input: Predicted (Tp , Ap ) and gold (Tg , Ag ) derivation Output: 1 if predicted derivation is correct, 0 otherwise 1: if |C(Tp ) |6= |C(Tg ) |then . different # of coeff. slots 2: return 0 3: end if 4: Γ ← T EMPL E QUIV(Tp ,Tg ) 5: if Γ = ∅ then . not equivalent templates 6: return 0 7: end if 8: if A LIGN E QUIV(Γ, Ap , Ag ) then . Check alignments 9: return 1 10: end if 11: return 0 12: 13: procedure T EMPL E QUIV(T1 , T2 ) 14: . Note that here |C(T1 ) |= |C(T2 ) |holds 15: Γ←∅ 16: for each 1-to-1 mapping γ : C(T1 ) → C(T2 ) do 17: match ← True 18: for t = 1 ·"
E17-1047,D14-1058,0,0.546885,"the solutions are less informative, as they do not explain which span of text aligns to the coefficients in the equations. While the derivation is clearly the most informative structure, surprisingly, no prior work evaluates automatic solvers using derivations directly. To the best of our knowledge, none of the current datasets contain human-annotated derivations, possibly due to the belief that the current evaluation metrics are sufficient and the benefit of evaluating on derivations is minor. Currently, the most popular evaluation strategy is to use solution accuracy (Kushman et al., 2014; Hosseini et al., 2014; Shi et al., 2015; Koncel-Kedziorski et Introduction Automatically solving math reasoning problems is a long-pursued goal of AI (Newell et al., 1959; Bobrow, 1964). Recent work (Kushman et al., 2014; Shi et al., 2015; Koncel-Kedziorski et al., 2015) has focused on developing solvers for algebra word problems, such as the one shown in Figure 1. Developing a solver for word problems can open several new avenues, especially for online education and intelligent tutoring systems (Kang et al., 2016). In addition, as solving word problems requires the ability to understand and analyze natural langua"
E17-1047,P16-1084,0,0.334074,"tutoring systems (Kang et al., 2016). In addition, as solving word problems requires the ability to understand and analyze natural language, it serves as a good test-bed for evaluating progress towards goals of artificial intelligence (Clark and Etzioni, 2016). 1 Also referred to as a template. We use these two terms interchangeably. 494 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 494–504, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics al., 2015; Zhou et al., 2015; Huang et al., 2016), which computes whether the solution was correct or not, as this is an easy-to-implement metric. Another evaluation strategy was proposed in (Kushman et al., 2014), which finds an approximate derivation from the gold equation system and uses it to compare against a predicted derivation. We follow (Kushman et al., 2014) and call this evaluation strategy the equation accuracy. 2 In this work, we argue that evaluating solvers against human labeled derivation is important. Existing evaluation metrics, like solution accuracy are often quite generous — for example, an incorrect equation system, suc"
E17-1047,D15-1202,0,0.0881874,"ata is essential for correct evaluation. The value of such annotations for evaluation becomes more immediate for online education scenarios, where such word solvers are likely to be used. Indeed, in these cases, merely arriving at the correct solution, by using incorrect reasoning may prove detrimental for teaching purposes. We believe derivation based evaluation closely mirrors how humans are evaluated in schools (by forcing solvers to show “their work”). Our datasets with the derivation annotations have applications beyond accurate evaluation. For instance, certain solvers, like the one in (Roy and Roth, 2015), train a relevance classifier to identify which textual numbers are relevant to solving the word problem. As we only annotate relevant numbers in our annotations, our datasets can provide high quality supervision for such classifiers. The datasets can also be used in evaluation test-beds, like the one proposed in (Koncel-Kedziorski et al., 2016). We hope our datasets will open new possibilities for the community to simulate new ideas and applications for automatic problem solvers. Labeling Semantic Parses Similar to our work, efforts have been made to annotate semantic parses for other tasks,"
E17-1047,D15-1135,0,0.597216,"informative, as they do not explain which span of text aligns to the coefficients in the equations. While the derivation is clearly the most informative structure, surprisingly, no prior work evaluates automatic solvers using derivations directly. To the best of our knowledge, none of the current datasets contain human-annotated derivations, possibly due to the belief that the current evaluation metrics are sufficient and the benefit of evaluating on derivations is minor. Currently, the most popular evaluation strategy is to use solution accuracy (Kushman et al., 2014; Hosseini et al., 2014; Shi et al., 2015; Koncel-Kedziorski et Introduction Automatically solving math reasoning problems is a long-pursued goal of AI (Newell et al., 1959; Bobrow, 1964). Recent work (Kushman et al., 2014; Shi et al., 2015; Koncel-Kedziorski et al., 2015) has focused on developing solvers for algebra word problems, such as the one shown in Figure 1. Developing a solver for word problems can open several new avenues, especially for online education and intelligent tutoring systems (Kang et al., 2016). In addition, as solving word problems requires the ability to understand and analyze natural language, it serves as a"
E17-1047,Q15-1042,0,0.584237,"ers using derivations directly. To the best of our knowledge, none of the current datasets contain human-annotated derivations, possibly due to the belief that the current evaluation metrics are sufficient and the benefit of evaluating on derivations is minor. Currently, the most popular evaluation strategy is to use solution accuracy (Kushman et al., 2014; Hosseini et al., 2014; Shi et al., 2015; Koncel-Kedziorski et Introduction Automatically solving math reasoning problems is a long-pursued goal of AI (Newell et al., 1959; Bobrow, 1964). Recent work (Kushman et al., 2014; Shi et al., 2015; Koncel-Kedziorski et al., 2015) has focused on developing solvers for algebra word problems, such as the one shown in Figure 1. Developing a solver for word problems can open several new avenues, especially for online education and intelligent tutoring systems (Kang et al., 2016). In addition, as solving word problems requires the ability to understand and analyze natural language, it serves as a good test-bed for evaluating progress towards goals of artificial intelligence (Clark and Etzioni, 2016). 1 Also referred to as a template. We use these two terms interchangeably. 494 Proceedings of the 15th Conference of the Europ"
E17-1047,D16-1029,1,0.857585,"(2009) and Clarke et al. (2010), semantic parsers were trained using annotated logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007, inter alia), which were expensive to annotate. Recently, Yih et al. (2016) showed that labeled semantic parses for the knowledge based question answering task can be obtained at a cost comparable to obtaining answers. They showed significant improvements in performance of a questionanswering system using the labeled parses instead of answers for training. More recently, by treating word problems as a semantic parsing task, Upadhyay et al. (2016) found that joint learning using both explicit (derivation as labeled semantic parses) and implicit supervision signals (solution as responses) can significantly outperform models trained using only one type of supervision signal. Other Semantic Parsing Tasks We demonstrated that response-based evaluation, which is quite popular for most semantic parsing problems (Zelle and Mooney, 1996; Berant et al., 2013; Liang et al., 2011, inter alia) can overlook reasoning errors for algebra problems. A reason for this is that in algebra word problems there can be several semantic parses (i.e., derivatio"
E17-1047,N16-1136,0,0.129334,"e derivation based evaluation closely mirrors how humans are evaluated in schools (by forcing solvers to show “their work”). Our datasets with the derivation annotations have applications beyond accurate evaluation. For instance, certain solvers, like the one in (Roy and Roth, 2015), train a relevance classifier to identify which textual numbers are relevant to solving the word problem. As we only annotate relevant numbers in our annotations, our datasets can provide high quality supervision for such classifiers. The datasets can also be used in evaluation test-beds, like the one proposed in (Koncel-Kedziorski et al., 2016). We hope our datasets will open new possibilities for the community to simulate new ideas and applications for automatic problem solvers. Labeling Semantic Parses Similar to our work, efforts have been made to annotate semantic parses for other tasks, although primarily for providing supervision. Prior to the works of Liang et al. (2009) and Clarke et al. (2010), semantic parsers were trained using annotated logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007, inter alia), which were expensive to annotate. Recently, Yih et al. (2016) showed that labeled"
E17-1047,P07-1121,0,0.00877881,"ers. The datasets can also be used in evaluation test-beds, like the one proposed in (Koncel-Kedziorski et al., 2016). We hope our datasets will open new possibilities for the community to simulate new ideas and applications for automatic problem solvers. Labeling Semantic Parses Similar to our work, efforts have been made to annotate semantic parses for other tasks, although primarily for providing supervision. Prior to the works of Liang et al. (2009) and Clarke et al. (2010), semantic parsers were trained using annotated logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007, inter alia), which were expensive to annotate. Recently, Yih et al. (2016) showed that labeled semantic parses for the knowledge based question answering task can be obtained at a cost comparable to obtaining answers. They showed significant improvements in performance of a questionanswering system using the labeled parses instead of answers for training. More recently, by treating word problems as a semantic parsing task, Upadhyay et al. (2016) found that joint learning using both explicit (derivation as labeled semantic parses) and implicit supervision signals (solution as responses) can s"
E17-1047,P16-2033,1,0.819982,"d in (Koncel-Kedziorski et al., 2016). We hope our datasets will open new possibilities for the community to simulate new ideas and applications for automatic problem solvers. Labeling Semantic Parses Similar to our work, efforts have been made to annotate semantic parses for other tasks, although primarily for providing supervision. Prior to the works of Liang et al. (2009) and Clarke et al. (2010), semantic parsers were trained using annotated logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007, inter alia), which were expensive to annotate. Recently, Yih et al. (2016) showed that labeled semantic parses for the knowledge based question answering task can be obtained at a cost comparable to obtaining answers. They showed significant improvements in performance of a questionanswering system using the labeled parses instead of answers for training. More recently, by treating word problems as a semantic parsing task, Upadhyay et al. (2016) found that joint learning using both explicit (derivation as labeled semantic parses) and implicit supervision signals (solution as responses) can significantly outperform models trained using only one type of supervision si"
E17-1047,D15-1096,0,0.247966,"Missing"
N09-1034,P07-1083,0,0.224595,"performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in a bilingual corpora avoid the OOV problem by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions about the source and target la"
N09-1034,P07-1036,1,0.868408,"r supervised settings (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b), or weakly supervised settings with additional temporal information (Sproat et al., 2006; Klementiev and Roth, 2006a). Our work differs from these works in that it is completely unsupervised and makes no assumptions about the training data. Incorporating knowledge encoded as constraints into learning problems has attracted a lot of attention in the NLP community recently. This has been shown both in supervised settings (Roth and Yih, 2004; Riedel and Clarke, 2006) and unsupervised settings (Haghighi and Klein, 2006; Chang et al., 2007) in which constraints are used to bootstrap the model. (Chang et al., 2007) describes an unsupervised training of a Constrained Conditional Model (CCM), a general framework for combining statistical models with declarative constraints. We extend this work to include constraints over possible assignments to latent variables which, in turn, define the underlying representation for the learning problem. In the transliteration community there are several works (Ristad and Yianilos, 1998; Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b) that show how the feature representation of a word pair"
N09-1034,P08-2014,1,0.932369,"hes and achieves comparable performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in a bilingual corpora avoid the OOV problem by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions ab"
N09-1034,D08-1037,1,0.91608,"hes and achieves comparable performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in a bilingual corpora avoid the OOV problem by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions ab"
N09-1034,N06-1041,0,0.0335088,"del is typically done under supervised settings (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b), or weakly supervised settings with additional temporal information (Sproat et al., 2006; Klementiev and Roth, 2006a). Our work differs from these works in that it is completely unsupervised and makes no assumptions about the training data. Incorporating knowledge encoded as constraints into learning problems has attracted a lot of attention in the NLP community recently. This has been shown both in supervised settings (Roth and Yih, 2004; Riedel and Clarke, 2006) and unsupervised settings (Haghighi and Klein, 2006; Chang et al., 2007) in which constraints are used to bootstrap the model. (Chang et al., 2007) describes an unsupervised training of a Constrained Conditional Model (CCM), a general framework for combining statistical models with declarative constraints. We extend this work to include constraints over possible assignments to latent variables which, in turn, define the underlying representation for the learning problem. In the transliteration community there are several works (Ristad and Yianilos, 1998; Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b) that show how the feature represent"
N09-1034,P08-1045,0,0.0379901,"Missing"
N09-1034,C00-1056,0,0.0366664,"esource in conjunction with constraints provided us with a robust transliteration system which significantly outperforms existing unsupervised approaches and achieves comparable performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in"
N09-1034,N06-1011,1,0.682814,"uages - Chinese, Russian and Hebrew. Our experiments show that constraint driven learning can significantly outperform existing unsupervised models and achieve competitive results to existing supervised models. 1 Introduction Named entity (NE) transliteration is the process of transcribing a NE from a source language to some target language while preserving its pronunciation in the original language. Automatic NE transliteration is an important component in many cross-language applications, such as Cross-Lingual Information Retrieval (CLIR) and Machine Translation(MT) (Hermjakob et al., 2008; Klementiev and Roth, 2006a; Meng et al., 2001; Knight and Graehl, 1998). It might initially seem that transliteration is an easy task, requiring only finding a phonetic mapping between character sets. However simply matching every source language character to its target language counterpart is not likely to work well as in practice this mapping depends on the context the 299 characters appear in and on transliteration conventions which may change across domains. As a result, current approaches employ machine learning methods which, given enough labeled training data learn how to determine whether a pair of words const"
N09-1034,P06-1103,1,0.68478,"uages - Chinese, Russian and Hebrew. Our experiments show that constraint driven learning can significantly outperform existing unsupervised models and achieve competitive results to existing supervised models. 1 Introduction Named entity (NE) transliteration is the process of transcribing a NE from a source language to some target language while preserving its pronunciation in the original language. Automatic NE transliteration is an important component in many cross-language applications, such as Cross-Lingual Information Retrieval (CLIR) and Machine Translation(MT) (Hermjakob et al., 2008; Klementiev and Roth, 2006a; Meng et al., 2001; Knight and Graehl, 1998). It might initially seem that transliteration is an easy task, requiring only finding a phonetic mapping between character sets. However simply matching every source language character to its target language counterpart is not likely to work well as in practice this mapping depends on the context the 299 characters appear in and on transliteration conventions which may change across domains. As a result, current approaches employ machine learning methods which, given enough labeled training data learn how to determine whether a pair of words const"
N09-1034,P04-1021,0,0.051568,"ing this simple resource in conjunction with constraints provided us with a robust transliteration system which significantly outperforms existing unsupervised approaches and achieves comparable performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for"
N09-1034,W06-1616,0,0.0203426,"ss data to converge. Training the transliteration model is typically done under supervised settings (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b), or weakly supervised settings with additional temporal information (Sproat et al., 2006; Klementiev and Roth, 2006a). Our work differs from these works in that it is completely unsupervised and makes no assumptions about the training data. Incorporating knowledge encoded as constraints into learning problems has attracted a lot of attention in the NLP community recently. This has been shown both in supervised settings (Roth and Yih, 2004; Riedel and Clarke, 2006) and unsupervised settings (Haghighi and Klein, 2006; Chang et al., 2007) in which constraints are used to bootstrap the model. (Chang et al., 2007) describes an unsupervised training of a Constrained Conditional Model (CCM), a general framework for combining statistical models with declarative constraints. We extend this work to include constraints over possible assignments to latent variables which, in turn, define the underlying representation for the learning problem. In the transliteration community there are several works (Ristad and Yianilos, 1998; Bergsma and Kondrak, 2007; Goldwasser"
N09-1034,P06-1010,0,0.166253,"ethods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in a bilingual corpora avoid the OOV problem by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions about the source and target languages and require c"
N09-1034,W06-1630,0,0.155505,"f the temporal information. Our best model, the unsupervised learning with all constraints, outperforms both models in (Klementiev and Roth, 2006b), even though we do not use any temporal information. guages: Russian, Chinese, and Hebrew, and compared our results to previously published results. 5.1 Experimental Settings In our experiments the system is evaluated on its ability to correctly identify the gold transliteration for each source word. We evaluated the system’s performance using two measures adopted in many transliteration works. The first one is Mean Reciprocal Rank (MRR), used in (Tao et al., 2006; Sproat et al., 2006), which is the average of the multiplicative inverse of the rank of the correct answer. Formally, Let n be the number of source NEs. Let GoldRank(i) be the rank the algorithm assigns to the correct transliteration. Then, MRR is defined by: n MRR = 1 1X . n goldRank(i) i=1 Another measure is Accuracy (ACC) used in (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008a), which is the percentage of the top rank candidates being the gold transliteration. In our implementation we used the support vector machine (SVM) learning algorithm with linear kernel as our underlying lea"
N09-1034,P07-1015,0,0.0372154,"e we have in these resources - the romanization table is a noisy mapping covering the character set and is therefore better suited as a feature. Constraints, represented by pervasive, correct character mapping, indicate the sound mapping tendency between source and target languages. For example, certain n-gram phonemic mappings, such as r → l 303 to Chinese transliteration (see Sec. 3.2 for more details). Constraints in boldface apply to all positions, the rest apply only to characters appearing in initial position. These patterns have been used by other systems as features or pseudofeatures (Yoon et al., 2007). However, in our system these language specific ruleof-thumbs are systematically used as constraints to exclude impossible alignments and therefore generate better features for learning. We listed in Table 1 all 20 language specific constraints we used for Chinese. There is a total of 24 constraints for Hebrew and 17 for Russian. The constraints in Table 1 indicate a systematic sound mapping between English and Chinese unigram character mappings. Arranged by manners of articulation each row of the table indicates the sound change tendency among vowels, nasals, approximants (retroflex and glid"
N09-1034,W04-2401,1,\N,Missing
N09-1034,J98-4003,0,\N,Missing
N10-1066,D08-1031,1,0.109195,"idden variable types from Section 4 – wordmapping, word-deletion and edge-mapping, along with the associated constraints as defined earlier. Since the text is typically much longer than the hypothesis, we create word-deletion latent variables (and features) only for the hypothesis. The second column of Table 2 lists the resources used to generate features corresponding to each hidden variable type. For word-mapping variables, the features include a WordNet based metric (WNSim), indicators for the POS tags and negation identifiers. We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger3 ) to generate features. For edge 3 http://L2R.cs.uiuc.edu/˜cogcomp/software.php Hidden Variable word-mapping word-deletion edge-mapping RTE features WordNet, POS, Coref, Neg POS NODE-INFO edge-deletion N/A Paraphrase features WordNet, POS, NE, ED POS, NE NODE-INFO, DEP DEP Paraphrase System Acc Experiments using (Dolan et al., 2004) (Qiu et al., 2006) 72.00 (Das and Smith, 2009) 73.86 (Wan et al., 2006) 75.6"
N10-1066,P07-1083,0,0.0229497,"operties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning ta"
N10-1066,N09-1034,1,0.852715,"hus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation. Our Learning Constr"
N10-1066,2008.amta-papers.4,0,0.0547444,"atent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and provides a general platform for a wide range of NLP tasks. The optimization procedure in this work and (Felzenszwalb et al., 2009) are quite different. We use the coordinate descent and cutting-plane methods ensuring we have fewer parameters and the inference procedure can be easily parallelized. Our procedure also allows different loss functions. (Cherry and Quirk, 2008) adopts the Latent SVM algorithm to define a language model. Unfortunately, their implementation is not guaranteed to converge. In CRF-like models with latent variables (McCal436 lum et al., 2005), the decision function marginalizes over the all hidden states when presented with an input example. Unfortunately, the computational cost of applying their framework is prohibitive with constrained latent representations. In contrast, our framework requires only the best hidden representation instead of marginalizing over all possible representations, thus reducing the computational effort. 7 Conclu"
N10-1066,P09-1053,0,0.174568,"ing into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation. Our Learning Constrained Latent Represent"
N10-1066,C04-1051,0,0.692767,"intermediate representation. We evaluate our algorithm on three different NLP tasks – transliteration, paraphrase identification and textual entailment – and show that our joint method significantly improves performance. 1 Introduction Many NLP tasks can be phrased as decision problems over complex linguistic structures. Successful learning depends on correctly encoding these (often latent) structures as features for the learning system. Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al., 2006) and paraphrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1 , and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by definin"
N10-1066,P08-2014,1,0.934205,"termediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation"
N10-1066,D08-1037,1,0.861012,"termediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation"
N10-1066,H05-1049,0,0.0443099,"a and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approach in the NLP community (Haghighi et al., 2005; McCallum et al., 2005; Goldwasser and Roth, 2008b; Das and Smith, 2009); however, the intermediate representation and the inference for constructing it are closely coupled with the application task. In contrast, LCLR provides a general formulation that allows the use of expressive constraints, making it applicable to many NLP tasks. Unlike other latent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and"
N10-1066,D08-1084,0,0.222611,"n et al., 2006) and paraphrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1 , and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning in"
N10-1066,J08-2005,1,0.216571,"b). 5.2 Textual Entailment Recognizing Textual Entailment (RTE) is an important textual inference task of predicting if a given text snippet, entails the meaning of another (the hypothesis). In many current RTE systems, the entailment decision depends on successfully aligning the constituents of the text and hypothesis, accounting for the internal linguistic structure of the input. The raw input – the text and hypothesis – are represented as directed acyclic graphs, where vertices correspond to words. Directed edges link verbs to the head words of semantic role labeling arguments produced by (Punyakanok et al., 2008). All other words are connected by dependency edges. The intermediate representation is an alignment between the nodes and edges of the graphs. We used three hidden variable types from Section 4 – wordmapping, word-deletion and edge-mapping, along with the associated constraints as defined earlier. Since the text is typically much longer than the hypothesis, we create word-deletion latent variables (and features) only for the hypothesis. The second column of Table 2 lists the resources used to generate features corresponding to each hidden variable type. For word-mapping variables, the feature"
N10-1066,W06-1603,0,0.101349,"e learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show"
N10-1066,W04-3219,0,0.0114581,"ence formulation, which makes it easy to define the intermediate representation and to inject knowledge in the form of constraints. While ILP has been applied to structured output learning, to the best of our knowledge, this is the first work that makes use of ILP in formalizing the general problem of learning intermediate representations. 2 Preliminaries We introduce notation using the Paraphrase Identification task as a running example. This is the bi430 nary classification task of identifying whether one sentence is a paraphrase of another. A paraphrase pair from the MSR Paraphrase corpus (Quirk et al., 2004) is shown in Figure 1. In order to identify that the sentences paraphrase each other , we need to align constituents of these sentences. One possible alignment is shown in the figure, in which the dotted edges correspond to the aligned constituents. An alignment can be specified using binary variables corresponding to every edge between constituents, indicating whether the edge is included in the alignment. Different activations of these variables induce the space of intermediate representations. The notification was first reported Friday by MSNBC. MSNBC.com first reported the CIA request on F"
N10-1066,P09-2015,1,0.892648,"hrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1 , and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – spec"
N10-1066,U06-1019,0,0.169702,"ngtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger3 ) to generate features. For edge 3 http://L2R.cs.uiuc.edu/˜cogcomp/software.php Hidden Variable word-mapping word-deletion edge-mapping RTE features WordNet, POS, Coref, Neg POS NODE-INFO edge-deletion N/A Paraphrase features WordNet, POS, NE, ED POS, NE NODE-INFO, DEP DEP Paraphrase System Acc Experiments using (Dolan et al., 2004) (Qiu et al., 2006) 72.00 (Das and Smith, 2009) 73.86 (Wan et al., 2006) 75.60 Alignment + Learning 76.23 LCLR 76.41 Experiments using Extended data set Alignment + Learning 72.00 LCLR 72.75 Table 2: Summary of latent variables and feature resources for the entailment and paraphrase identification tasks. See Section 4 for an explanation of the hidden variable types. The linguistic resources used to generate features are abbreviated as follows – POS: Part of speech, Coref: Canonical coreferent entities; NE: Named Entity, ED: Edit distance, Neg: Negation markers, DEP: Dependency labels, NODE-INFO: corresponding node alignment resources, N/A: Hidden variable not used"
N10-1066,P06-1051,0,0.0183547,"hat the similarity in performance between the joint LCLR algorithm and the two stage 4 Previous work (Das and Smith, 2009) has shown that combining the results of several systems improves performance. (Alignment + Learning) systems is due to the limited intermediate representation space for input pairs in this dataset. We evaluated these systems on the more difficult Extended dataset. Results indeed show that the margin between the two systems increases as the inference problem becomes harder. 6 Related Work Recent NLP research has largely focused on twostage approaches. Examples include RTE (Zanzotto and Moschitti, 2006; MacCartney et al., 2008; Roth et al., 2009); string matching (Bergsma and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There a"
N10-1066,W07-1401,0,\N,Missing
N12-1087,N10-1083,0,0.0705101,"Missing"
N12-1087,J93-2003,0,0.0221784,", information extraction, and word-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn’t available earlier, exhibiting the benefits of the UEM framework. 1 Dan Roth University of Illinois danr@illinois.edu Introduction Expectation Maximization (EM) (Dempster et al., 1977) is inarguably the most widely used algorithm for unsupervised and semi-supervised learning. Many successful applications of unsupervised and semi-supervised learning in NLP use EM including text classification (McCallum et al., 1998; Nigam et al., 2000), machine translation (Brown et al., 1993), and parsing (Klein and Manning, 2004). Recently, EM algorithms which incorporate constraints on structured output spaces have been proposed (Chang et al., 2007; Ganchev et al., 2010). Several variations of EM (e.g. hard EM) exist in the literature and choosing a suitable variation is of1. We propose a general framework called Unified Expectation Maximization (UEM) that presents a continuous spectrum of EM algorithms parameterized by a simple temperaturelike tuning parameter. The framework covers both constrained and unconstrained EM algorithms. UEM thus connects EM, hard EM, PR, and CoDL so"
N12-1087,P07-1036,1,0.62282,"Unified EM (UEM), that covers many EM variations including the constrained cases along with a continuum of new ones. UEM allows us to compare and investigate the properties of EM in a systematic way and helps find better alternatives. The contributions of this paper are as follows: We present a general framework containing a graded spectrum of Expectation Maximization (EM) algorithms called Unified Expectation Maximization (UEM.) UEM is parameterized by a single parameter and covers existing algorithms like standard EM and hard EM, constrained versions of EM such as ConstraintDriven Learning (Chang et al., 2007) and Posterior Regularization (Ganchev et al., 2010), along with a range of new EM algorithms. For the constrained inference step in UEM we present an efficient dual projected gradient ascent algorithm which generalizes several dual decomposition and Lagrange relaxation algorithms popularized recently in the NLP literature (Ganchev et al., 2008; Koo et al., 2010; Rush and Collins, 2011). UEM is as efficient and easy to implement as standard EM. Furthermore, experiments on POS tagging, information extraction, and word-alignment show that often the best performing algorithm in the UEM family is"
N12-1087,P06-2019,0,0.032328,"re are a few different styles of expressing EM, following the style of (Neal and Hinton, 1998), we define F (θ, q) = L(θ) − KL(q, Pθ (h|x)), (2) where q is a posterior distribution over H(x) and KL(p1 , p2 ) is the KL divergence between two distributions p1 and p2 . Given this formulation, EM can 689 2.2 Constraints in EM It has become a common practice in the NLP community to use constraints on output variables to guide inference. Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata, 2006), and agreement constraints between wordalignment directions (Ganchev et al., 2008) or various parsing models (Koo et al., 2010). In the context of EM, constraints can be imposed on the posterior probabilities, q, to guide the learning procedure (Chang et al., 2007; Ganchev et al., 2010). In this paper, we focus on linear constraints over h (potentially non-linear over x.) This is a very general formulation as it is known that all Boolean constraints can be transformed into sets of linear constraints over binary variables (Roth and Yih, 2007). Assume that we have m linear constraints on output"
N12-1087,P08-1112,0,0.334219,"Expectation Maximization (EM) algorithms called Unified Expectation Maximization (UEM.) UEM is parameterized by a single parameter and covers existing algorithms like standard EM and hard EM, constrained versions of EM such as ConstraintDriven Learning (Chang et al., 2007) and Posterior Regularization (Ganchev et al., 2010), along with a range of new EM algorithms. For the constrained inference step in UEM we present an efficient dual projected gradient ascent algorithm which generalizes several dual decomposition and Lagrange relaxation algorithms popularized recently in the NLP literature (Ganchev et al., 2008; Koo et al., 2010; Rush and Collins, 2011). UEM is as efficient and easy to implement as standard EM. Furthermore, experiments on POS tagging, information extraction, and word-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn’t available earlier, exhibiting the benefits of the UEM framework. 1 Dan Roth University of Illinois danr@illinois.edu Introduction Expectation Maximization (EM) (Dempster et al., 1977) is inarguably the most widely used algorithm for unsupervised and semi-supervised learning. Many successful applications of unsupervis"
N12-1087,P04-1061,0,0.0173967,"-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn’t available earlier, exhibiting the benefits of the UEM framework. 1 Dan Roth University of Illinois danr@illinois.edu Introduction Expectation Maximization (EM) (Dempster et al., 1977) is inarguably the most widely used algorithm for unsupervised and semi-supervised learning. Many successful applications of unsupervised and semi-supervised learning in NLP use EM including text classification (McCallum et al., 1998; Nigam et al., 2000), machine translation (Brown et al., 1993), and parsing (Klein and Manning, 2004). Recently, EM algorithms which incorporate constraints on structured output spaces have been proposed (Chang et al., 2007; Ganchev et al., 2010). Several variations of EM (e.g. hard EM) exist in the literature and choosing a suitable variation is of1. We propose a general framework called Unified Expectation Maximization (UEM) that presents a continuous spectrum of EM algorithms parameterized by a simple temperaturelike tuning parameter. The framework covers both constrained and unconstrained EM algorithms. UEM thus connects EM, hard EM, PR, and CoDL so that the relation between different alg"
N12-1087,D10-1125,0,0.280133,"ion (EM) algorithms called Unified Expectation Maximization (UEM.) UEM is parameterized by a single parameter and covers existing algorithms like standard EM and hard EM, constrained versions of EM such as ConstraintDriven Learning (Chang et al., 2007) and Posterior Regularization (Ganchev et al., 2010), along with a range of new EM algorithms. For the constrained inference step in UEM we present an efficient dual projected gradient ascent algorithm which generalizes several dual decomposition and Lagrange relaxation algorithms popularized recently in the NLP literature (Ganchev et al., 2008; Koo et al., 2010; Rush and Collins, 2011). UEM is as efficient and easy to implement as standard EM. Furthermore, experiments on POS tagging, information extraction, and word-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn’t available earlier, exhibiting the benefits of the UEM framework. 1 Dan Roth University of Illinois danr@illinois.edu Introduction Expectation Maximization (EM) (Dempster et al., 1977) is inarguably the most widely used algorithm for unsupervised and semi-supervised learning. Many successful applications of unsupervised and semi-superv"
N12-1087,P09-1039,0,0.00600942,"ns and their associated values γ. This paper focuses on values of γ between 0 and 1 for the following reasons. First, the Estep (8) is non-convex for γ &lt; 0 and hence computationally expensive; e.g., hard EM (i.e. γ = −∞) requires ILP inference. For γ ≥ 0, (8) is a convex optimization problem which can be solved exactly and efficiently. Second, for γ = 0, the E-step solves P max (10) h∈H(x) q(h) log Pθ (h|x) q s.t. Eq [Uh] ≤ b, q(h) ≥ 0, ∀h ∈ H(x), P h∈H(x) q(h) = 1 , which is an LP-relaxation of hard EM (Eq. (4) and (9)). LP relaxations often provide a decent proxy to ILP (Roth and Yih, 2004; Martins et al., 2009). Third, γ ∈ [0, 1] covers standard EM/PR. KL(q, Pθ (y|x)) + (1 − γ)H(q) — UEM (6) minimizes the former during the E-step, while Standard EM (3) minimizes the latter. The additional term (1 − γ)H(q) is essentially an entropic prior on the posterior distribution q which can be used to regularize the entropy as desired. For γ &lt; 1, the regularization term penalizes the entropy of the posterior thus reducing the probability mass on the tail of the distribution. This is significant, for instance, in unsupervised structured prediction where the tail can carry a substantial amount of probability mass"
N12-1087,P06-1065,0,0.0265801,"Missing"
N12-1087,P00-1056,0,0.168354,"Missing"
N12-1087,J03-1002,0,0.00643672,".94 24.46 23.78 PR CoDL Fr-En 10.71 14.68 8.40 10.09 8.09 8.93 Es-En 22.00 28.13 20.08 23.01 19.70 21.60 UEM 9.21 7.40 6.87 20.83 18.95 18.64 Table 2: AER (Alignment Error Rate) comparisons for French-English (above) and Spanish-English (below) alignment for various data sizes. For French-English setting, tuned γ for all data-sizes is either 0.5 or 0.6. For Spanish-English, tuned γ for all data-sizes is 0.7. threshold, tuned over the development set. Results We compare UEM with EM, PR, and CoDL on the basis of Alignment Error Rate (AER) for different sizes of unlabeled data (See Tab. 2.) See (Och and Ney, 2003) for the definition of AER. UEM consistently outperforms EM, PR, and CoDL with a wide margin. 6 Conclusion We proposed a continuum of EM algorithms parameterized by a single parameter. Our framework naturally incorporates constraints on output variables and generalizes existing constrained and unconstrained EM algorithms like standard and hard EM, PR, and CoDL. We provided an efficient Lagrange relaxation algorithm for inference with constraints in the E-step and empirically showed how important it is to choose the right EM version. Our technique is amenable to be combined with many existing v"
N12-1087,P09-1057,0,0.0215675,"(Spitkovsky et al., 2010) by exploring the space of EM algorithms in a “continuous” way. Furthermore, we also study the relation between quality of model initialization and the value of γ in the case of POS tagging. This is inspired by a general “research wisdom” that hard EM is a better choice than EM with a good initialization point whereas the opposite is true with an “uninformed” initialization. Unsupervised POS Tagging We conduct experiments on unsupervised POS learning experiment with the tagging dictionary assumption. We use a standard subset of Penn Treebank containing 24,115 tokens (Ravi and Knight, 2009) with the tagging dictionary derived from the entire Penn Treebank. We run UEM with a first order (bigram) HMM model5 . We consider initialization points of varying quality and observe the performance for γ ∈ [0, 1]. Different initialization points are constructed as follows. The “posterior uniform” initialization is created by spreading the probability uniformly over all possible tags for each token. Our EM model on 5 (Ravi and Knight, 2009) showed that a first order HMM model performs much better than a second order HMM model on unsupervised POS tagging 694 Relative performance to EM (Gamma="
N12-1087,W09-1110,1,0.833401,"Missing"
N12-1087,W04-2401,1,0.377695,"ommon technique for learning θ, which maximizes a tight lower bound on L(θ). While there are a few different styles of expressing EM, following the style of (Neal and Hinton, 1998), we define F (θ, q) = L(θ) − KL(q, Pθ (h|x)), (2) where q is a posterior distribution over H(x) and KL(p1 , p2 ) is the KL divergence between two distributions p1 and p2 . Given this formulation, EM can 689 2.2 Constraints in EM It has become a common practice in the NLP community to use constraints on output variables to guide inference. Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata, 2006), and agreement constraints between wordalignment directions (Ganchev et al., 2008) or various parsing models (Koo et al., 2010). In the context of EM, constraints can be imposed on the posterior probabilities, q, to guide the learning procedure (Chang et al., 2007; Ganchev et al., 2010). In this paper, we focus on linear constraints over h (potentially non-linear over x.) This is a very general formulation as it is known that all Boolean constraints can be transformed into sets of linear constraints ove"
N12-1087,P11-1008,0,0.148874,"s called Unified Expectation Maximization (UEM.) UEM is parameterized by a single parameter and covers existing algorithms like standard EM and hard EM, constrained versions of EM such as ConstraintDriven Learning (Chang et al., 2007) and Posterior Regularization (Ganchev et al., 2010), along with a range of new EM algorithms. For the constrained inference step in UEM we present an efficient dual projected gradient ascent algorithm which generalizes several dual decomposition and Lagrange relaxation algorithms popularized recently in the NLP literature (Ganchev et al., 2008; Koo et al., 2010; Rush and Collins, 2011). UEM is as efficient and easy to implement as standard EM. Furthermore, experiments on POS tagging, information extraction, and word-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn’t available earlier, exhibiting the benefits of the UEM framework. 1 Dan Roth University of Illinois danr@illinois.edu Introduction Expectation Maximization (EM) (Dempster et al., 1977) is inarguably the most widely used algorithm for unsupervised and semi-supervised learning. Many successful applications of unsupervised and semi-supervised learning in NLP use"
N12-1087,D10-1001,0,0.0886964,"or which it is a convex optimization problem, and use a Lagrange relaxation algorithm (Bertsekas, 1999). Our contributions are two fold: • We describe an algorithm for UEM with constraints that is as easy to implement as PR or CoDL. Existing code for constrained EM (PR or CoDL) can be easily extended to run UEM. • We solve the E-step (8) using a Lagrangian dual-based algorithm which performs projected subgradient-ascent on dual variables. Our algorithm covers Lagrange relaxation and dual decomposition techniques (Bertsekas, 1999) which were recently popularized in NLP (Rush and Collins, 2011; Rush et al., 2010; Koo et al., 2010). Not only do we extend the algorithmic framework to a continuum of algorithms, we also allow, unlike the aforementioned works, general inequality constraints over the output variables. Furthermore, we establish new and 692 We combine both the ideas by setting q(h) = G(h, Pθt (·|x), λT U, γ) where G(h, P, v, γ) =      1 P (h) γ e − vh γ 1 P 0 γ h0 P (h ) e − vh γ 0  0    δ(h= arg max P (h0 )e−vh ) γ&gt;0 , γ=0 . h0 ∈H(x) (14) Alg. 2 shows the overall optimization scheme. The dual variables for inequality constraints are restricted to be positive and hence after a gradi"
N12-1087,P04-1062,0,0.15651,"ter, we 3.1 Relationship between UEM and Other EM Algorithms show that UEM naturally includes and generalizes both PR and CoDL. The relation between unconstrained versions of EM 3 Unified Expectation Maximization We now present the Unified Expectation Maximization (UEM) framework which captures a continuum of (constrained and unconstrained) EM algorithms 2 Note that this set is a finite set of discrete variables not to be confused with a polytope. Polytopes are also specified as {z|Az ≤ d} but are over real variables whereas h is discrete. 690 has been mentioned before (Ueda and Nakano, 1998; Smith and Eisner, 2004). We show that the relationship takes novel aspects in the presence of constraints. In order to better understand different UEM variations, we write the UEM E-step (6) explicitly as an optimization problem: 3 The term ‘metric’ is used very loosely. KL(·, ·; γ) does not satisfy the mathematical properties of a metric. Framework Constrained γ = −∞ Hard EM γ=0 Hard EM γ ∈ (0, 1) (NEW) UEMγ γ=1 Standard EM Unconstrained CoDL (Chang et al., 2007) (NEW) EM with Lin. Prog. (NEW) constrained UEMγ PR (Ganchev et al., 2010) γ=∞→1 Deterministic Annealing EM Table 1: Summary of different UEM algorithms. T"
N12-1087,W10-2902,0,0.0809394,"M (6) minimizes the former during the E-step, while Standard EM (3) minimizes the latter. The additional term (1 − γ)H(q) is essentially an entropic prior on the posterior distribution q which can be used to regularize the entropy as desired. For γ &lt; 1, the regularization term penalizes the entropy of the posterior thus reducing the probability mass on the tail of the distribution. This is significant, for instance, in unsupervised structured prediction where the tail can carry a substantial amount of probability mass as the output space is massive. This notion aligns with the observation of (Spitkovsky et al., 2010) who criticize EM for frittering away too much probability mass on unimportant outputs while showing that hard EM does much better in PCFG parsing. In particular, they empirically show that when initialized with a “good” set of parameters obtained by supervised learning, EM drifts away (thus losing accuracy) much farther than hard-EM. 4 interesting connections between existing constrained inference techniques. 4.1 Projected Subgradient Ascent with Lagrangian Dual We provide below a high-level view of our algorithm, omitting the technical derivations due to lack of space. To solve the E-step (8"
N12-1087,H05-1010,0,0.032537,"Missing"
N12-1087,C96-2141,0,0.184257,"Missing"
N12-1087,J94-2001,0,\N,Missing
N13-1122,D07-1074,0,0.622449,"ystem. In contrast, our study jointly identifies and disambiguates entity mentions within tweets (short text fragments). A subset of existing literature targets end-to-end linking (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Sun, 2011; Meij et al., 2012), but there are quite a few differences between our work and each of these systems. Some systems (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Sun, 2011) heavily depend on Wikipedia text and might not work well in short and noisy tweets. Many systems (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Ferragina and Scaiella, 2010) treat mention detection and entity disambiguation as two different problems. (Meij et al., 2012) is the most related to our paper. While their system also considers mention detection and entity disambiguation together, they do not consider entityto-entity relationships and do not incorporate contextual words from tweets. An area of work closely related to the mention detection problem is the Named Entity Recognition (NER) problem, the identification of textual phrases which belong to core categories (Person, Location, Organization). It is"
N13-1122,P12-1086,0,0.016681,"Missing"
N13-1122,P11-1095,0,0.147886,"Missing"
N13-1122,P11-1138,0,0.298713,"Missing"
N13-1122,D11-1141,0,0.228388,"Missing"
N13-1122,D12-1011,0,0.0322482,"Missing"
N15-1054,D13-1160,0,0.0084334,"Missing"
N15-1054,D14-1067,0,0.0161244,"Missing"
N15-1054,D14-1165,0,0.0214649,"test snapshot is taken from a later period, and a KBC algorithm is evaluated by its ability of recovering newly added knowledge in the test snapshot. This enables the methods to be directly evaluated on facts that are missing in a KB snapshot. Note that the facts that are added to the test snapshot, in general, are more subtle than the facts that they already contain and predicting the newly added facts could be harder. Hence, our approach enables a more realistic and challenging evaluation setting than previous work. We use manually constructed Freebase as the KB in our experiments. Notably, Chang et al. (2014) use a two-snapshot strategy for constructing a dataset for relation extraction using automatically constructed NELL as their KB. The new facts that are added to a KB by an automatic method may not have all the characteristics that make the two snapshot strategy more advantageous. We construct our train snapshot Λ0 by taking the Freebase snapshot on 3rd September, 2013 and consider entities that have a link to their Wikipedia page. KBC algorithms are evaluated by their ability to predict facts that were added to the 1st June, 2014 snapshot of Freebase Λ. To get negative data, we make a closed"
N15-1054,W99-0612,0,0.0224608,"ise. GAP is the average precision of this transformed problem which can measure the ability of the methods to rank predictions both within and across entity types. Prior to us, Bordes et al. (2013) use mean reciprocal rank as a global evaluation metric for a KBC task. We use average precision instead of mean reciprocal rank since MRR could be biased to the top predictions of the method (West et al., 2014) While GAP captures global ordering, it would be beneficial to measure the quality of the top k predictions of the model for bootstrapping and active learning scenarios (Lewis and Gale, 1994; Cucerzan and Yarowsky, 1999). We report G@k, GAP measured on the top k predictions (similarly to Precision@k and Hits@k). This metric can be reliably used to measure the overall quality of the top k predictions. 4 Global Objective for Knowledge Base Completion We describe our approach for predicting missing entity types in a KB in this section. While we focus on recovering entity types in this paper, the methods we develop can be easily extended to other KB completion tasks. 4.1 Global Objective Framework During training, only positive examples are observed in KB completion tasks. Similar to previous work (Mintz et al.,"
N15-1054,Q14-1021,1,0.6652,"Missing"
N15-1054,N13-1122,1,0.787811,"Missing"
N15-1054,D13-1029,0,0.0400788,"Missing"
N15-1054,D07-1073,0,0.0951497,"Missing"
N15-1054,D13-1161,0,0.0308063,"Missing"
N15-1054,D11-1049,0,0.0143929,"e false positive predictions are caused by the use of shallow linguistic features. For example, an entity who has acted in a movie and composes music only for television shows is wrongly tagged with the type / film/composer since words like ”movie”, ”composer” and ”music” occur frequently in the Wikipedia article of the entity (http://en.wikipedia. org/wiki/J._J._Abrams). 6 Knowledge Base Completion Much of precious work in KB completion has focused on the problem of relation extraction. Majority of the methods infer missing relation facts using information within the KB (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) while methods such as Mintz et al. (2009) use information in text documents. Riedel et al. (2013) use both information within and outside the KB to complete the KB. Linear Embedding Model Weston et al. (2011) is one of first work that developed a supervised linear embedding model and applied it to image retrieval. We apply this model to entity type prediction but we train using a different objective function which is more suited for our task. 7 Conclusion and Future Work We propose an evaluation framework comprising of methods for dataset constructio"
N15-1054,P09-1113,0,0.782439,"inguistics Figure 1: Freebase description of Jean Metellus can be used to infer that the entity has the type /book/author. This missing fact is found by our algorithm and is still missing in the latest version of Freebase when the paper is written. on their ability to predict missing facts. To overcome these drawbacks we construct the train and test data using two snapshots of the KB and evaluate the methods on predicting facts that are added to the more recent snapshot, enabling a more realistic and challenging evaluation. Standard evaluation metrics for KBC methods are generally type-based (Mintz et al., 2009; Riedel et al., 2013), measuring the quality of the predictions by aggregating scores computed within a type. This is not ideal because: (1) it treats every entity type equally not considering the distribution of types, (2) it does not measure the ability of the methods to rank predictions across types. Therefore, we additionally use a global evaluation metric, where the quality of predictions is measured within and across types, and also accounts for the high variance in type distribution. In our experiments, we show that models trained with negative examples from the entity side perform bet"
N15-1054,P12-1059,0,0.0212606,"Missing"
N15-1054,D12-1113,0,0.051415,"Missing"
N15-1054,N13-1008,0,0.633673,"Freebase description of Jean Metellus can be used to infer that the entity has the type /book/author. This missing fact is found by our algorithm and is still missing in the latest version of Freebase when the paper is written. on their ability to predict missing facts. To overcome these drawbacks we construct the train and test data using two snapshots of the KB and evaluate the methods on predicting facts that are added to the more recent snapshot, enabling a more realistic and challenging evaluation. Standard evaluation metrics for KBC methods are generally type-based (Mintz et al., 2009; Riedel et al., 2013), measuring the quality of the predictions by aggregating scores computed within a type. This is not ideal because: (1) it treats every entity type equally not considering the distribution of types, (2) it does not measure the ability of the methods to rank predictions across types. Therefore, we additionally use a global evaluation metric, where the quality of predictions is measured within and across types, and also accounts for the high variance in type distribution. In our experiments, we show that models trained with negative examples from the entity side perform better on type-based metr"
N15-1054,W06-2809,0,0.058942,"Missing"
N15-1054,P14-1090,0,0.00682657,"Missing"
N19-1300,Q19-1026,1,0.809429,"s a result, they can be used to construct highly inferential reading comprehension datasets that have the added benefit of being directly related to the practical end-task of answering user yes/no questions. Yes/No questions do appear as a subset of some existing datasets (Reddy et al., 2018; Choi et al., 2018; Yang et al., 2018). However, these datasets are primarily intended to test other aspects of question answering (QA), such as conversational QA or multi-step reasoning, and do not contain naturally occurring questions. We follow the data collection method used by Natural Questions (NQ) (Kwiatkowski et al., 2019) to gather 16,000 naturally occurring yes/no questions into a dataset we call BoolQ (for Boolean Questions). Each question is paired with a paragraph from Wikipedia that an independent annotator has marked as containing the answer. The task is then to take a question and passage as input, and to return “yes” or “no” as output. Figure 1 contains some examples, and Appendix A.1 contains additional randomly selected examples. Following recent work (Wang et al., 2018), we focus on using transfer learning to establish baselines for our dataset. Yes/No QA is closely related to many other NLP tasks,"
N19-1300,D15-1075,0,0.265278,"Missing"
N19-1300,P17-1152,0,0.0881736,"Missing"
N19-1300,D18-1241,0,0.0608786,"Missing"
N19-1300,L18-1269,0,0.0405572,"Missing"
N19-1300,N18-2017,0,0.0305137,"consists of a question (Q), an excerpt from a passage (P), and an answer (A) with an explanation added for clarity. Introduction 1 Has the UK been hit by a hurricane? The Great Storm of 1987 was a violent extratropical cyclone which caused casualties in England, France and the Channel Islands . . . Yes. [An example event is given.] ference (NLI) proposed the task of labeling candidate statements as being entailed or contradicted by a given passage. However, in practice, generating candidate statements that test for complex inferential abilities is challenging. For instance, evidence suggests (Gururangan et al., 2018; Jia and Liang, 2017; McCoy et al., 2019) that simply asking human annotators to write candidate statements will result in examples that typically only require surface-level reasoning. In this paper we propose an alternative: we test models on their ability to answer naturally occurring yes/no questions. That is, questions that were authored by people who were not prompted to write particular kinds of questions, including even being required to write yes/no questions, and who did not know the answer to the question they were asking. Figure 1 contains some examples from our dataset. We find su"
N19-1300,D17-1215,0,0.0358614,"), an excerpt from a passage (P), and an answer (A) with an explanation added for clarity. Introduction 1 Has the UK been hit by a hurricane? The Great Storm of 1987 was a violent extratropical cyclone which caused casualties in England, France and the Channel Islands . . . Yes. [An example event is given.] ference (NLI) proposed the task of labeling candidate statements as being entailed or contradicted by a given passage. However, in practice, generating candidate statements that test for complex inferential abilities is challenging. For instance, evidence suggests (Gururangan et al., 2018; Jia and Liang, 2017; McCoy et al., 2019) that simply asking human annotators to write candidate statements will result in examples that typically only require surface-level reasoning. In this paper we propose an alternative: we test models on their ability to answer naturally occurring yes/no questions. That is, questions that were authored by people who were not prompted to write particular kinds of questions, including even being required to write yes/no questions, and who did not know the answer to the question they were asking. Figure 1 contains some examples from our dataset. We find such questions often qu"
N19-1300,P17-1147,0,0.100114,"Missing"
N19-1300,P19-1334,0,0.0296322,"Missing"
N19-1300,D18-1260,0,0.0376092,"Missing"
N19-1300,L18-1008,0,0.0292594,"Missing"
N19-1300,D16-1244,0,0.0948182,"Missing"
N19-1300,N18-1202,1,0.559304,"e detail: Our Recurrent model follows a standard recurrent plus attention architecture for text-pair classification (Wang et al., 2018). It embeds the premise/hypothesis text using fasttext word vectors (Mikolov et al., 2018) and learned character vectors, applies a shared bidirectional LSTM to both parts, applies co-attention (Parikh et al., 2016) to share information between the two parts, applies another bi-LSTM to both parts, pools the result, and uses the pooled representation to predict the final class. See Appendix A.2 for details. Our Recurrent +ELMo model uses the language model from Peters et al. (2018) to provide contextualized embeddings to the baseline model outlined above, as recommended by the authors. Our OpenAI GPT model fine-tunes the 12 layer 768 dimensional uni-directional transformer from Radford et al. (2018), which has been pretrained as a language model on the Books corpus (Zhu et al., 2015). Our BERTL model fine-tunes the 24 layer 1024 dimensional transformer from Devlin et al. (2018), which has been trained on next-sentence-selection and masked language modelling on the Book Corpus and Wikipedia. We fine-tune the BERTL and the OpenAI GPT models using the optimizers recommende"
N19-1300,W18-5441,0,0.0742579,"Missing"
N19-1300,P18-2124,0,0.099408,"Missing"
N19-1300,D18-1233,0,0.064836,"Missing"
N19-1300,W18-5446,0,0.39751,"ning, and do not contain naturally occurring questions. We follow the data collection method used by Natural Questions (NQ) (Kwiatkowski et al., 2019) to gather 16,000 naturally occurring yes/no questions into a dataset we call BoolQ (for Boolean Questions). Each question is paired with a paragraph from Wikipedia that an independent annotator has marked as containing the answer. The task is then to take a question and passage as input, and to return “yes” or “no” as output. Figure 1 contains some examples, and Appendix A.1 contains additional randomly selected examples. Following recent work (Wang et al., 2018), we focus on using transfer learning to establish baselines for our dataset. Yes/No QA is closely related to many other NLP tasks, including other forms of question answering, entailment, and paraphrasing. Therefore, it is not clear what the best data sources to transfer from are, or if it will be sufficient to just transfer from powerful pretrained language models such as BERT (Devlin et al., 2018) or ELMo (Peters et al., 2018). We experiment with state-of-the-art unsupervised approaches, using existing entailment datasets, three methods of leveraging extractive QA data, and using a few othe"
N19-1300,Q18-1021,0,0.0822727,"Missing"
N19-1300,N18-1101,0,0.16728,"Missing"
N19-1300,D18-1259,0,0.0909819,"Missing"
N19-1300,D18-1009,0,0.09126,"Missing"
N19-1423,C18-1139,0,0.153493,"a, June 2 - June 7, 2019. 2019 Association for Computational Linguistics word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a “next sentence prediction” task that jointly pretrains text-pair representations. The contributions of our paper are as follows: • We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. • We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-spec"
P06-2009,C96-1058,0,0.045379,"y competitive parsing results. A somewhat similar approach was used in (Nivre and Scholz, 2004) to develop a hybrid bottom-up/topdown approach; there, the edges are also labeled with semantic types, yielding lower accuracy than the works mentioned above. The overall goal of dependency parsing (DP) learning is to infer a tree structure. A common way to do that is to predict with respect to each potential edge (i, j) in the tree, and then choose a global structure that (1) is a tree and that (2) maximizes some score. In the context of DPs, this “edge based factorization method” was proposed by (Eisner, 1996). In other contexts, this is similar to the approach of (Roth and Yih, 2004) in that scoring each edge depends only on the raw data observed and not on the classifications of other edges, and that global considerations can be used to overwrite the local (edge-based) decisions. On the other hand, the key in a pipeline model is that making a decision with respect to the edge (i, j) may gain from taking into account decisions already made with respect to neighboring edges. However, given that these decisions are noisy, there is a need to devise policies for reducing the number of predictions in o"
P06-2009,H05-1049,0,0.0342671,"decisions we made, guided by these, principles lead to a significant improvement in the accuracy of the resulting parse tree. 1.1 Dependency Parsing and Pipeline Models Dependency trees provide a syntactic reresentation that encodes functional relationships between words; it is relatively independent of the grammar theory and can be used to represent the structure of sentences in different languages. Dependency structures are more efficient to parse (Eisner, 1996) and are believed to be easier to learn, yet they still capture much of the predicate-argument information needed in applications (Haghighi et al., 2005), which is one reason for the recent interest in learning these structures (Eisner, 1996; McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). Eisner’s work – O(n3 ) parsing time generative algorithm – embarked the interest in this area. His model, however, seems to be limited when dealing with complex and long sentences. (McDonald et al., 2005) build on this work, and use a global discriminative training approach to improve the edges’ scores, along with Eisner’s algorithm, to yield the expected improvement. A different approach was studied by (Yamada and Matsumoto, 2003"
P06-2009,W05-0618,0,0.0172521,"rocessing for good reasons. It is based on the assumption that some decisions might be easier or more reliable than others, and their outcomes, therefore, can be counted on when making further decisions. Nevertheless, it is clear that it results in error accumulation and suffers from its inability to correct mistakes in previous stages. Researchers have recently started to address some of the disadvantages of this model. E.g., (Roth and Yih, 2004) suggests a model in which global constraints are taken into account in a later stage to fix mistakes due to the pipeline. (Punyakanok et al., 2005; Marciniak and Strube, 2005) also address some aspects of this problem. However, these solutions rely on the fact that all decisions are made with respect to the same input; specifically, all classifiers considered use the same examples as their input. In addition, the pipelines they study are shallow. Pipeline computation, in which a task is decomposed into several stages that are solved sequentially, is a common computational strategy in natural language processing. The key problem of this model is that it results in error accumulation and suffers from its inability to correct mistakes in previous stages. We develop a"
P06-2009,J93-2004,0,0.0269474,"ithm considers the pair (a, b), a &lt; b, we call the word a the current focus point. Next we describe several policies for determining the focus point of the algorithm following an action. We note that, with a few exceptions, determining the focus point does not affect the correctness of the algorithm. It is easy to show that for (almost) any focus point chosen, if the correct Policy Start over Stay Step back #Shift 156545 117819 43374 #Left 26351 26351 26351 #Right 27918 27918 27918 Table 1: The number of actions required to build all the trees for the sentences in section 23 of Penn Treebank (Marcus et al., 1993) as a function of the focus point placement policy. The statistics are taken with the correct (gold-standard) actions. It is clear from Table 1 that the policies result 1 Note that (Yamada and Matsumoto, 2003) mention that they move the focus point back after R, but do not state what they do after executing L actions, and why. (Yamada, 2006) indicates that they also move focus point back after L. 67 Algorithm 2 Pseudo Code of the dependency parsing algorithm. getFeatures extracts the features describing the word pair currently considered; getAction determines the appropriate action for the pai"
P06-2009,P05-1012,0,0.0195453,"ng parse tree. 1.1 Dependency Parsing and Pipeline Models Dependency trees provide a syntactic reresentation that encodes functional relationships between words; it is relatively independent of the grammar theory and can be used to represent the structure of sentences in different languages. Dependency structures are more efficient to parse (Eisner, 1996) and are believed to be easier to learn, yet they still capture much of the predicate-argument information needed in applications (Haghighi et al., 2005), which is one reason for the recent interest in learning these structures (Eisner, 1996; McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). Eisner’s work – O(n3 ) parsing time generative algorithm – embarked the interest in this area. His model, however, seems to be limited when dealing with complex and long sentences. (McDonald et al., 2005) build on this work, and use a global discriminative training approach to improve the edges’ scores, along with Eisner’s algorithm, to yield the expected improvement. A different approach was studied by (Yamada and Matsumoto, 2003), that develop a bottom-up approach and learn the parsing decisions between consecutive words in the sentence."
P06-2009,C04-1010,0,0.331122,"h sentence may be viewed as a pipeline computation as it processes a token and, potentially, makes use of this result when processing the token to the right. 65 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 65–72, c Sydney, July 2006. 2006 Association for Computational Linguistics rather than on the overall quality of the parser, and chained to yield the global structure. Clearly, it suffers from the limitations of pipeline processing, such as accumulation of errors, but nevertheless, yields very competitive parsing results. A somewhat similar approach was used in (Nivre and Scholz, 2004) to develop a hybrid bottom-up/topdown approach; there, the edges are also labeled with semantic types, yielding lower accuracy than the works mentioned above. The overall goal of dependency parsing (DP) learning is to infer a tree structure. A common way to do that is to predict with respect to each potential edge (i, j) in the tree, and then choose a global structure that (1) is a tree and that (2) maximizes some score. In the context of DPs, this “edge based factorization method” was proposed by (Eisner, 1996). In other contexts, this is similar to the approach of (Roth and Yih, 2004) in th"
P06-2009,W03-3017,0,0.070011,"Missing"
P06-2009,W05-0639,0,0.354463,"ion in natural language processing for good reasons. It is based on the assumption that some decisions might be easier or more reliable than others, and their outcomes, therefore, can be counted on when making further decisions. Nevertheless, it is clear that it results in error accumulation and suffers from its inability to correct mistakes in previous stages. Researchers have recently started to address some of the disadvantages of this model. E.g., (Roth and Yih, 2004) suggests a model in which global constraints are taken into account in a later stage to fix mistakes due to the pipeline. (Punyakanok et al., 2005; Marciniak and Strube, 2005) also address some aspects of this problem. However, these solutions rely on the fact that all decisions are made with respect to the same input; specifically, all classifiers considered use the same examples as their input. In addition, the pipelines they study are shallow. Pipeline computation, in which a task is decomposed into several stages that are solved sequentially, is a common computational strategy in natural language processing. The key problem of this model is that it results in error accumulation and suffers from its inability to correct mistakes in p"
P06-2009,W97-0301,0,0.0399474,"n dealing with complex and long sentences. (McDonald et al., 2005) build on this work, and use a global discriminative training approach to improve the edges’ scores, along with Eisner’s algorithm, to yield the expected improvement. A different approach was studied by (Yamada and Matsumoto, 2003), that develop a bottom-up approach and learn the parsing decisions between consecutive words in the sentence. Local actions are used to generate a dependency tree using a shift-reduce parsing approach (Aho et al., 1986). This is a true pipeline approach, as was done in other successful parsers, e.g. (Ratnaparkhi, 1997), in that the classifiers are trained on individual decisions 2 Efficient Dependency Parsing This section describes our DP algorithm and justifies its advantages as a pipeline model. We pro66 action is selected for the corresponding edge, the algorithm will eventually yield the correct tree (but may require multiple cycles through the sentence). In practice, the actions selected are noisy, and a wasteful focus point policy will result in a large number of actions, and thus in error accumulation. To minimize the number of actions taken, we want to find a good focus point placement policy. After"
P06-2009,W04-2401,1,0.901718,"f Illinois at Urbana-Champaign Urbana, IL 61801 {mchang21, quangdo2, danr}@uiuc.edu Abstract The pipeline model is a standard model of computation in natural language processing for good reasons. It is based on the assumption that some decisions might be easier or more reliable than others, and their outcomes, therefore, can be counted on when making further decisions. Nevertheless, it is clear that it results in error accumulation and suffers from its inability to correct mistakes in previous stages. Researchers have recently started to address some of the disadvantages of this model. E.g., (Roth and Yih, 2004) suggests a model in which global constraints are taken into account in a later stage to fix mistakes due to the pipeline. (Punyakanok et al., 2005; Marciniak and Strube, 2005) also address some aspects of this problem. However, these solutions rely on the fact that all decisions are made with respect to the same input; specifically, all classifiers considered use the same examples as their input. In addition, the pipelines they study are shallow. Pipeline computation, in which a task is decomposed into several stages that are solved sequentially, is a common computational strategy in natural"
P06-2009,N03-1033,0,0.0163304,"Missing"
P06-2009,W03-3023,0,0.250114,"and Yih, 2004) in that scoring each edge depends only on the raw data observed and not on the classifications of other edges, and that global considerations can be used to overwrite the local (edge-based) decisions. On the other hand, the key in a pipeline model is that making a decision with respect to the edge (i, j) may gain from taking into account decisions already made with respect to neighboring edges. However, given that these decisions are noisy, there is a need to devise policies for reducing the number of predictions in order to make the parser more robust. This is exemplified in (Yamada and Matsumoto, 2003) – a bottom-up approach, that is most related to the work presented here. Their model is a “traditional” pipeline model – a classifier suggests a decision that, once taken, determines the next action to be taken (as well as the input the next action observes). In the rest of this paper, we propose and justify a framework for improving pipeline processing based on the principles mentioned above: (i) make local decisions as reliably as possible, and (ii) reduce the number of decisions made. We use the proposed principles to examine the (Yamada and Matsumoto, 2003) parsing algorithm and show that"
P07-1036,W99-0613,0,0.927384,"ng, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks. 1 Introduction knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets. However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000). In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002). On the other hand, in the supervised setting, it has been shown that incorporating domain and problem specific structured information can result in substantial improvements (Toutanova et al., 2005; Roth and Yih, 2005). This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning. We develop a formalism for constraints-based learning that unifies several kinds of constraints: unary, dictionary based and n-ary constraints, which encode structural information and interdependencies among possible labels."
P07-1036,W02-1001,0,0.037614,"× Y → R to assign scores to each possible input/output pair. Given an input x, a desired function f will assign the correct output y the highest score among all the possible outputs. The global scoring function is often decomposed as a weighted sum of feature functions, f (x, y) = M X λi fi (x, y) = λ · F (x, y). i=1 This decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs, in which case the linear sum corresponds to log likelihood assigned to the input/output pair by the model (for details see (Roth, 1999) for the classification case and (Collins, 2002) for the structured case). Even when not dictated by the model, the feature functions fi (x, y) used are local to allow inference tractability. Local feature function can capture some context for each input or output variable, yet it is very limited to allow dynamic programming decoding during inference. Now, consider a scenario where we have a set of constraints C1 , . . . , CK . We define a constraint C : X × Y → {0, 1} as a function that indicates whether the input/output sequence violates some desired properties. When the constraints are hard, the solution is given by argmax λ · F (x, y),"
P07-1036,P05-1046,0,0.570757,"process. The core of our approach, (1), is described in Section 5. The task is described in Section 3 and the Experimental study in Section 6. It is shown there that the improvement on the training examples via the constraints indeed boosts the learned model and the proposed method significantly outperforms the traditional semi-supervised framework. 2 Related Work In the semi-supervised domain there are two main approaches for injecting domain specific knowledge. One is using the prior knowledge to accurately tailor the generative model so that it captures the domain structure. For example, (Grenager et al., 2005) proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labels. This is done by constraining the HMM transition matrix, which can be done also for other models, such as CRF. However (Roth and Yih, 2005) showed that reasoning with more expressive, non-sequential constraints can improve the performance for the supervised protocol. A second approach has been to use a small highaccuracy set of labeled tokens as a way to seed and bootstrap the semi-supervised learning. This was used, for example, by (Thelen and Riloff, 2002; Collins and Singe"
P07-1036,N06-1041,0,0.734055,"k allows for high performance learning with significantly less training data than was possible before on these tasks. 1 Introduction knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets. However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000). In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002). On the other hand, in the supervised setting, it has been shown that incorporating domain and problem specific structured information can result in substantial improvements (Toutanova et al., 2005; Roth and Yih, 2005). This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning. We develop a formalism for constraints-based learning that unifies several kinds of constraints: unary, dictionary based and n-ary constraints, which encode structural information and interdependencies among possible labels. One advantage of our forma"
P07-1036,N06-1020,0,0.0154554,"is that when iteratively estimating the parameters with EM, we disallow the parameters to drift too far from the supervised model. The parameter re-estimation in line 9, uses a similar intuition, but instead of weighting data instances, we introduced a smoothing parameter γ which controls the convex combination of models induced by the labeled and the unlabeled data. Unlike the technique mentioned above which focuses on naive Bayes, our method allows us to weight linear models generated by different learning algorithms. Another way to look the algorithm is from the self-training perspective (McClosky et al., 2006). Similarly to self-training, we use the current model to generate new training examples from the unla284 Input: Cycles: learning cycles T r = {x, y}: labeled training set. U : unlabeled dataset F : set of feature functions. {ρi }: set of penalties. {Ci }: set of constraints. γ: balancing parameter with the supervised model. learn(T r, F ): supervised learning algorithm Top-K-Inference: returns top-K labeled scored by the cost function (1) CODL: 1. Initialize λ0 = learn(T r, F ). 2. λ = λ0 . 3. For Cycles iterations do: 4. T =φ 5. For each x ∈ U 6. {(x, y 1 ), . . . , (x, y K )} = 7. Top-K-Inf"
P07-1036,W02-1028,0,\N,Missing
P07-1036,P05-1073,0,\N,Missing
P07-1036,P05-1044,0,\N,Missing
P13-1171,N09-1003,0,0.0569711,"Missing"
P13-1171,P11-1059,0,0.00851492,"ge of the Is-A relation (Song et al., 2011). For instance, when a word refers to a named entity, the particular sense and meaning is often not encoded. As a result, relations such as “Apple” is-a “company” and “Jaguar” is-a “car” cannot be found in WordNet. Similar to the case in synonymy, the Is-A relation defined in WordNet does not provide a native, real-valued degree of the relation, which can only be roughly approximated using the number of links on the taxonomy path connecting two 3 Mapping two antonyms may be desired if one of them is in the scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do not attempt to resolve the negation scope in this work. concepts (Resnik, 1995). In order to remedy these issues, we augment WordNet with the Is-A relations found in Probase (Wu et al., 2012). Probase is a knowledge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences"
P13-1171,J06-1003,0,0.00993252,"2009; Reisinger and Mooney, 2010; Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney and Pantel, 2010) are often used as the core component. Inspired by (Yih and Qazvinian, 2012), which argues the importance of incorporating heterogeneous vector space models for measuring word similarity, we leverage three different VSMs in this work: Wiki term-vectors, recurrent neural 4 Instead of making the distinction, word similarity here refers to the larger set of relations commonly covered by word relatedness (Budanitsky and Hirst, 2006). 1747 network language model (RNNLM) and a concept vector space model learned from click-through data. Semantic word similarity is estimated using the cosine score of the corresponding word vectors in these VSMs. Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). Following the setting suggested by Yih and Qazvinian (2012), we create termvectors representing about 1 million words by aggregating terms within a window of [−10, 10] of each occurrence of the target word. The vectors are further refined by"
P13-1171,N10-1066,1,0.703494,"question/sentence pairs, such as the one below. Q: Which was the first movie that James Dean was in? A: James Dean, who began as an actor on TV dramas, didn’t make his screen debut until 1951’s “Fixed Bayonet.” While this sentence correctly answers the question, the fact that James Dean began as a TV actor is unrelated to the question. As a result, an “ideal” word alignment structure should not link words in this clause to those in the question. In order to leverage the latent structured information, we adapt a recently proposed framework of learning constrained latent representations (LCLR) (Chang et al., 2010). LCLR can be viewed as a variant of Latent-SVM (Felzenszwalb et al., 2009) with different learning formulations and a general inference framework. The idea of LCLR is to replace the decision function of a standard linear model θT φ(x) with arg max θT φ(x, h), (3) h where θ represents the weight vector and h represents the latent variables. In this answer selection task, x = (q, s) represents a pair of question q and candidate sentence s. As described in Sec. 3, h refers to the latent alignment between q and s. The intuition behinds Eq. (3) is: candidate sentence s correctly answers question q"
P13-1171,C04-1051,0,0.0404812,"ons. First, although we focus on improving TREC-style open-domain question answering in this work, we would like to apply the proposed technology to other QA scenarios, such as community-based QA (CQA). For instance, the sentence matching technique can help map a given question to some questions in an existing CQA database (e.g., Yahoo! Answers). Moreover, the answer sentence selection scheme could also be useful in extracting the most related sentences from the answer text to form a summary answer. Second, because the task of answer sentence selection is very similar to paraphrase detection (Dolan et al., 2004) and recognizing textual entailment (Dagan et al., 2006), we would like to investigate whether systems for these tasks can be improved by incorporating enhanced lexical semantic knowledge as well. Finally, we would like to improve our system for the answer sentence selection task and for question answering in general. In addition to following the directions suggested by the error analysis presented in Sec. 6.4, we plan to use logic-like semantic representations of questions and sentences, and explore the role of lexical semantics for handling questions that require inference. Acknowledgments W"
P13-1171,P03-1003,0,0.0407351,"e exact answer, along with the document that supports the answer. In contrast, the Jeopardy! TV quiz show provides another open-domain question answering setting, in which IBM’s Watson system famously beat the two highest ranked players (Ferrucci, 2012). Questions in this game are presented in a statement form and the system needs to identify the true question and to give the exact answer. A short sentence or paragraph to justify the answer is not required in either TREC-QA or Jeopardy! As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. Limiting the scope of an answer to a sentence is first highlighted by Wang et al. (2007), who argued that it was more informative to present the whole sentence instead of a short answer to users. Observing the limitations of the bag-of-words models, Wang et al. (2007) proposed a syntaxdriven approach, where each pair of question and sentence are matched by their dependency trees. The mapping is learned by a generative probabilistic model based on a Quasi-synchronous Grammar formulation (Smith and Eisner, 2006). This approach was later improve"
P13-1171,C92-2082,0,0.0790644,"the relation, which can only be roughly approximated using the number of links on the taxonomy path connecting two 3 Mapping two antonyms may be desired if one of them is in the scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do not attempt to resolve the negation scope in this work. concepts (Resnik, 1995). In order to remedy these issues, we augment WordNet with the Is-A relations found in Probase (Wu et al., 2012). Probase is a knowledge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having"
P13-1171,N10-1145,0,0.836138,"measure how close the question and sentence can be matched semantically. Due to the variety of word choices and inherent ambiguities in natural languages, bag-ofwords approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting both the syntactic and semantic structure in questions/sentences. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007) and on tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasisynchronous grammar (Wang et al., 2007) that match the dependency parse trees of questions and sentences. However, such approaches often require more computational resources. In addition to applying a syntactic or semantic parser during run-time, finding the best matching between structured representations of sentences is not trivial. For example, the computational complexity of tree matching is O(V 2 L4 ), where V is the number of nodes and L is the maximum depth (Tai, 1979). Instead of focusing on the high-level semantic representation, we turn our attention in this work to improvi"
P13-1171,S12-1047,0,0.00933608,"edge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having a class-inclusion relation. Probase’s prediction correlates well with the human annotations and achieves a high Spearman’s rank correlation coefficient score, ρ = 0.619. In comparison, the previous best system (Rink and Harabagiu, 2012) in the task only reaches ρ = 0.233. These appealing qualities make Probase a robust lexical semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similari"
P13-1171,S12-1035,0,0.00801338,"and the lack of the coverage of the Is-A relation (Song et al., 2011). For instance, when a word refers to a named entity, the particular sense and meaning is often not encoded. As a result, relations such as “Apple” is-a “company” and “Jaguar” is-a “car” cannot be found in WordNet. Similar to the case in synonymy, the Is-A relation defined in WordNet does not provide a native, real-valued degree of the relation, which can only be roughly approximated using the number of links on the taxonomy path connecting two 3 Mapping two antonyms may be desired if one of them is in the scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do not attempt to resolve the negation scope in this work. concepts (Resnik, 1995). In order to remedy these issues, we augment WordNet with the Is-A relations found in Probase (Wu et al., 2012). Probase is a knowledge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the freq"
P13-1171,N10-1013,0,0.015552,"0.233. These appealing qualities make Probase a robust lexical semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similarity. Unlike synonymy and hyponymy, word similarity is only loosely defined when two words can be associated by some implicit relation.4 The general word similarity model can be viewed as a “back-off” solution when the exact lexical relation (e.g., partwhole and attribute) is not available or cannot be accurately detected. Among various word similarity models (Agirre et al., 2009; Reisinger and Mooney, 2010; Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney and Pantel, 2010) are often used as the core component. Inspired by (Yih and Qazvinian, 2012), which argues the importance of incorporating heterogeneous vector space models for measuring word similarity, we leverage three different VSMs in this work: Wiki term-vectors, recurrent neural 4 Instead of making the distinction, word similarity here refers to the larger set of relations commonly covered by word relatedness (Budanitsky and Hirst, 2006). 174"
P13-1171,S12-1055,0,0.0136018,"y of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having a class-inclusion relation. Probase’s prediction correlates well with the human annotations and achieves a high Spearman’s rank correlation coefficient score, ρ = 0.619. In comparison, the previous best system (Rink and Harabagiu, 2012) in the task only reaches ρ = 0.233. These appealing qualities make Probase a robust lexical semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similarity. Unlike synonymy and hyponymy, word similarity is only loosely defined when two words can be associated by some implicit relation.4 The general word similarity model can be viewed as a “back-off” solution when the exact lexical relation (e.g., partwhole and attribute) is not available or cannot be accurately detected. Among various word similarity"
P13-1171,D07-1002,0,0.0629302,"ction can be naturally reduced to a semantic text matching problem. Conceptually, we would like to measure how close the question and sentence can be matched semantically. Due to the variety of word choices and inherent ambiguities in natural languages, bag-ofwords approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting both the syntactic and semantic structure in questions/sentences. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007) and on tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasisynchronous grammar (Wang et al., 2007) that match the dependency parse trees of questions and sentences. However, such approaches often require more computational resources. In addition to applying a syntactic or semantic parser during run-time, finding the best matching between structured representations of sentences is not trivial. For example, the computational complexity of tree matching is O(V 2 L4 ), where V is the number of nodes and L is the maximum depth (Tai, 1979). Instead"
P13-1171,W06-3104,0,0.00568154,"ents, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. Limiting the scope of an answer to a sentence is first highlighted by Wang et al. (2007), who argued that it was more informative to present the whole sentence instead of a short answer to users. Observing the limitations of the bag-of-words models, Wang et al. (2007) proposed a syntaxdriven approach, where each pair of question and sentence are matched by their dependency trees. The mapping is learned by a generative probabilistic model based on a Quasi-synchronous Grammar formulation (Smith and Eisner, 2006). This approach was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. In contrast, general tree matching methods based on tree-edit distance have been first proposed by Punyakanok et al. (2004) for a similar answer selection task. Heilman and Smith (2010) proposed a discriminative approach that first computes a tree kernel function between the dependency trees of the question and candidate sentence, and then learns a classifier based on the tree-edit features extracted. Although lexical semantic information derived from WordNet has"
P13-1171,C10-1131,0,0.537165,"answer selection problem is clearly critical. Limiting the scope of an answer to a sentence is first highlighted by Wang et al. (2007), who argued that it was more informative to present the whole sentence instead of a short answer to users. Observing the limitations of the bag-of-words models, Wang et al. (2007) proposed a syntaxdriven approach, where each pair of question and sentence are matched by their dependency trees. The mapping is learned by a generative probabilistic model based on a Quasi-synchronous Grammar formulation (Smith and Eisner, 2006). This approach was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. In contrast, general tree matching methods based on tree-edit distance have been first proposed by Punyakanok et al. (2004) for a similar answer selection task. Heilman and Smith (2010) proposed a discriminative approach that first computes a tree kernel function between the dependency trees of the question and candidate sentence, and then learns a classifier based on the tree-edit features extracted. Although lexical semantic information derived from WordNet has been used in some of these approaches, the research has main"
P13-1171,D07-1003,0,0.732536,"ched semantically. Due to the variety of word choices and inherent ambiguities in natural languages, bag-ofwords approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting both the syntactic and semantic structure in questions/sentences. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007) and on tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasisynchronous grammar (Wang et al., 2007) that match the dependency parse trees of questions and sentences. However, such approaches often require more computational resources. In addition to applying a syntactic or semantic parser during run-time, finding the best matching between structured representations of sentences is not trivial. For example, the computational complexity of tree matching is O(V 2 L4 ), where V is the number of nodes and L is the maximum depth (Tai, 1979). Instead of focusing on the high-level semantic representation, we turn our attention in this work to improving the shallow semantic compo1744 Proceedings of"
P13-1171,N12-1077,1,0.578782,"hyponymy, word similarity is only loosely defined when two words can be associated by some implicit relation.4 The general word similarity model can be viewed as a “back-off” solution when the exact lexical relation (e.g., partwhole and attribute) is not available or cannot be accurately detected. Among various word similarity models (Agirre et al., 2009; Reisinger and Mooney, 2010; Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney and Pantel, 2010) are often used as the core component. Inspired by (Yih and Qazvinian, 2012), which argues the importance of incorporating heterogeneous vector space models for measuring word similarity, we leverage three different VSMs in this work: Wiki term-vectors, recurrent neural 4 Instead of making the distinction, word similarity here refers to the larger set of relations commonly covered by word relatedness (Budanitsky and Hirst, 2006). 1747 network language model (RNNLM) and a concept vector space model learned from click-through data. Semantic word similarity is estimated using the cosine score of the corresponding word vectors in these VSMs. Contextual term-vectors create"
P13-1171,W11-0329,1,0.440917,"ke a 640-dimensional version of RNNLM vectors, which is trained using the Broadcast News corpus of 320M words.5 The final word relatedness model is a projection model learned from the click-through data of a commercial search engine (Gao et al., 2011). Unlike the previous two models, which are created or trained using a text corpus, the input for this model is pairs of aggregated queries and titles of pages users click. This parallel data is used to train a projection matrix for creating the mapping between words in queries and documents based on user feedback, using a Siamese neural network (Yih et al., 2011). Each row vector of this matrix is the dense vector representation of the corresponding word in the vocabulary. Perhaps due to its unique information source, we found this particular word embedding seems to complement the other two VSMs and tends to improve the word similarity measure in general. 5 Learning QA Matching Models In this section, we investigate the effectiveness of various learning models for matching questions and sentences, including the bag-of-words setting 5 http://www.fit.vutbr.cz/˜imikolov/ rnnlm/ and the framework of learning latent structures. 5.1 Bag-of-Words Model The b"
P13-1171,D12-1111,1,0.864508,"nd needs to be handled reliably. Although sets of synonyms can be easily found in thesauri or WordNet synsets, such resources typically cover only strict synonyms. When comparing two words, it is more useful to estimate the degree of synonymy as well. For instance, ship and boat are not strict synonyms because a ship is usually viewed as a large boat. Knowing that two words are somewhat synonymous could be valuable in determining whether they should be mapped. In order to estimate the degree of synonymy, we leverage a recently proposed polarity-inducing latent semantic analysis (PILSA) model (Yih et al., 2012). Given a thesaurus, the model first constructs a signed d-by-n co-occurrence matrix W , where d is the number of word groups and n is the size of the vocabulary. Each row consists of a 2 Proposed by an anonymous reviewer, one justification of this word-alignment approach, where syntactic analysis plays a less important role, is that there are often few sensible combinations of words. For instance, knowing only the set of words {”car”, ”fastest”, ”world”}, one may still guess correctly the question “What is the fastest car in the world?” 1746 group of synonyms and antonyms of a particular sens"
P13-1171,N13-1120,1,0.636053,"en 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having a class-inclusion relation. Probase’s prediction correlates well with the human annotations and achieves a high Spearman’s rank correlation coefficient score, ρ = 0.619. In comparison, the previous best system (Rink and Harabagiu, 2012) in the task only reaches ρ = 0.233. These appealing qualities make Probase a robust lexical semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similarity. Unlike synonymy and hyponymy, word sim"
P13-1171,S12-1032,0,\N,Missing
P13-1171,N13-1106,0,\N,Missing
P14-6004,C10-1032,0,\N,Missing
P14-6004,E06-1002,0,\N,Missing
P14-6004,C10-1150,0,\N,Missing
P14-6004,D12-1010,0,\N,Missing
P14-6004,W11-2213,0,\N,Missing
P14-6004,C10-1145,0,\N,Missing
P14-6004,D11-1074,0,\N,Missing
P14-6004,mcnamee-etal-2010-evaluation,0,\N,Missing
P14-6004,D09-1025,0,\N,Missing
P14-6004,D12-1011,0,\N,Missing
P14-6004,D12-1082,0,\N,Missing
P14-6004,P11-1138,1,\N,Missing
P14-6004,P12-1086,0,\N,Missing
P14-6004,P13-2006,0,\N,Missing
P14-6004,P11-1095,0,\N,Missing
P14-6004,D13-1041,0,\N,Missing
P14-6004,I11-1029,0,\N,Missing
P14-6004,C12-1028,1,\N,Missing
P14-6004,D13-1184,1,\N,Missing
P14-6004,P13-1128,0,\N,Missing
P14-6004,D07-1074,0,\N,Missing
P14-6004,N10-1072,0,\N,Missing
P14-6004,W12-1014,0,\N,Missing
P14-6004,D11-1011,0,\N,Missing
P14-6004,P13-1107,1,\N,Missing
P14-6004,D11-1071,1,\N,Missing
P14-6004,I11-1113,0,\N,Missing
P14-6004,D12-1113,1,\N,Missing
P15-1049,E06-1002,0,0.0203941,"As Guo et al. (2013) shows that most mentions in tweets should be linked to the most popular entities, IE setting actually pays more attention on mention detection sub-problem. In contrast to IE setting, IR setting focuses on entity disambiguation, since we only need to decide whether the tweet is relevant to the query entity. Therefore, we believe that both evaluation policies are needed for tweet entity linking. Balance Precision and Recall Figure 2 shows the results of tuning the bias term for balancing 511 References Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to p"
P15-1049,W02-1001,0,0.0978265,"statistical features such as the probability of the surface to be used as anchor text in Wikipedia. We also add additional Entity Type features correspond to the following entity types: Character, Event, Product and Brand. Finally, we include several NER features to indicate each mention candidate belongs to one the following NER types: Twitter user, Twitter hashtag, Person, Location, Organization, Product, Event and Date. Algorithms Table 2 summarizes all the algorithms that are compared in our experiments. First, we consider two linear structured learning algorithms: Structured Perceptron (Collins, 2002) and Linear Structured SVM (SSVM) (Tsochantaridis et al., 2004). For non-linear models, we consider polynomial SSVM, which employs polynomial kernel inside the structured SVM algorithm. We also include LambdaRank (Quoc and Le, 2007), a neuralbased learning to rank algorithm, which is widely used in the information retrieval literature. We further compare with MART, which is designed for performing multiclass classification using log loss without considering the structured information. Finally, we have our proposed log-loss SMART algorithm, as described in Section 3. 9 Note that our baseline sy"
P15-1049,P14-1001,0,0.0180591,"ed by popular entities (e.g. new york in Figure 1). 5 Related Work Linear structured learning methods have been proposed and widely used in the literature. Popular models include Structured Perceptron (Collins, 2002), Conditional Random Field (Lafferty et al., 2001) and Structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005). Recently, many structured learning models based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only work that explores tree-"
P15-1049,D07-1074,0,0.190164,"Missing"
P15-1049,Q14-1021,1,0.906986,"Γ(t−i )| • Do structured entity linking models perform better than non-structured ones? • How can we best capture the relationships between entities? 4.1 Evaluation Methodology and Data We evaluate each entity linking system using two evaluation policies: Information Extraction (IE) driven evaluation and Information Retrieval (IR) driven evaluation. For both evaluation settings, precision, recall and F1 scores are reported. Our data is constructed from two publicly available sources: Named Entity Extraction & Linking (NEEL) Challenge (Cano et al., 2014) datasets, and the datasets released by Fang and Chang (2014). Note that we gather two datasets from Fang and Chang (2014) and they are used in two different evaluation settings. We refer to these two datasets as TACL-IE and TACL-IR, respectively. We perform some data cleaning and unification on Beyond S- MART: Modeling entity-entity relationships It is important for entity linking systems to take advantage of the entity-to-entity information while making local decisions. For instance, the identification of entity “eli manning” leads to a strong clue for linking “new york giants” to the NFL team. Instead of defining a more complicated structure and lear"
P15-1049,D11-1141,0,0.0471981,"Missing"
P15-1049,N13-1122,1,0.943744,"ion candidate has different own entity sets. 5 Sorting helps the algorithms find non-overlapping candidates. 507 sion by two-stage approach as the solution for modeling entity-entity relationships after we found that SMART achieves high precision and reasonable recall. Specifically, in the first stage, the system identifies all possible entities with basic features, which enables the extraction of entity-entity features. In the second stage, we re-train S- MART on a union of basic features and entity-entity features. We define entity-entity features based on the Jaccard distance introduced by Guo et al. (2013). Let Γ(ei ) denotes the set of Wikipedia pages that contain a hyperlink to an entity ei and Γ(t−i ) denotes the set of pages that contain a hyperlink to any identified entity ej of the tweet t in the first stage excluding ei . The Jaccard distance between ei and t is β(uK , K) =1 X exp(F (x, yk+Q = uk+Q )) β(uk , k) = uk+Q · Q−1 Y exp(F (x, yk+q = Nil)) q=1 · β(uk+Q , k + Q) (5) where k + Q is the index of the next nonoverlapping mention candidate. Note that the third terms of equation (4) or (5) will vanish if there are no corresponding non-overlapping mention candidates. Given the potential"
P15-1049,P11-1115,0,0.0237247,"the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to promote entity linking research (Ji et al., 2010; Ji and Grishman, 2011; Cano and others, 2014; Carmel et al., 2014). Building an end-to-end entity linking system involves in solving two interrelated sub-problems: mention detection and entity disambiguation. Earlier research on entity linking has been largely focused on the entity disambiguation problem, including most work on entity linking for wellwritten documents such as news and encyclopedia articles (Cucerzan, 2007) and also few for tweets (Liu et al., 2013). Recently, people have focused on building systems that consider mention detection and entity disambiguation jointly. For example, Cucerzan (2012) dela"
P15-1049,D13-1170,0,0.00140779,"ect is that S- MART can easily eliminate some common errors caused by popular entities (e.g. new york in Figure 1). 5 Related Work Linear structured learning methods have been proposed and widely used in the literature. Popular models include Structured Perceptron (Collins, 2002), Conditional Random Field (Lafferty et al., 2001) and Structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005). Recently, many structured learning models based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCR"
P15-1049,P13-1128,0,0.0263518,"Missing"
P15-1128,P14-1091,0,0.396713,"gely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA. For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB (Kwiatkowski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions. Each state is a candidate parse in the query graph representation and each action defines a way to grow the graph. The representation power of the semantic parse is thus controlled by t"
P15-1128,D14-1002,1,0.473482,"etworks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similarity is then computed using some distance function, such as cosine. This continuous-space representation approach has been proposed recently for semantic parsing and question answering (Bordes et al., 2014a; Yih et al., 2014) and has shown better results compared to lexical matching approaches (e.g., word-alignment models). In this work, we adapt a convolutional neural network (CNN) framework (Shen et al., 2014b; Shen et al., 2014a; Gao et al., 2014) to this matching problem. The network architecture is illustrated in Fig. 6. The CNN model first applies a word hashing technique (Huang et al., 2013) that breaks a word into a vector of letter-trigrams (xt → ft in Fig. 6). For example, the bag of letter-trigrams of the word “who” are #-w-h, w-h-o, h-o-# after adding the actor x Meg Griffin Convolution matrix: Wc Word hashing layer: ft x y x Figure 7: Extending an inferential chain with constraints and aggregation functions. word boundary symbol #. Then, it uses a convolutional layer to project the letter-trigram vectors of words within a con"
P15-1128,P14-1133,0,0.16292,"wever, most traditional approaches for semantic parsing are largely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA. For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB (Kwiatkowski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions. Each state is a candidate parse in the query graph representation and each action defines a way to grow the graph. The represent"
P15-1128,D13-1160,0,0.888293,"et al., 2014), but with some key differences. The nodes and edges in our query graph closely resemble the exact entities and predicates from the knowledge base. As a result, the graph can be straightforwardly translated to a logical form query that is directly executable. In contrast, the query graph in (Reddy et al., 2014) is mapped from the CCG parse of the question, and needs further transformations before mapping to subgraphs 2 y should be grounded to a CVT entity in this case. of the target knowledge base to retrieve answers. Semantically, our query graph is more related to simple λ-DCS (Berant et al., 2013; Liang, 2013), which is a syntactic simplification of λ-calculus when applied to graph databases. A query graph can be viewed as the tree-like graph pattern of a logical form in λ-DCS. For instance, the path from the answer node to an entity node can be described using a series of join operations in λ-DCS. Different paths of the tree graph are combined via the intersection operators. 3 Staged Query Graph Generation We focus on generating query graphs with the following properties. First, the tree graph consists of one entity node as the root, referred as the topic entity. Second, there exists"
P15-1128,D13-1161,0,0.0148159,"eved simply by executing the query. The semantic parse also provides a deeper understanding of the question, which can be used to justify the answer to users, as well as to provide easily interpretable information to developers for error analysis. However, most traditional approaches for semantic parsing are largely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA. For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB (Kwiatkowski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Sem"
P15-1128,J13-2005,0,0.417175,"ski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions. Each state is a candidate parse in the query graph representation and each action defines a way to grow the graph. The representation power of the semantic parse is thus controlled by the set of legitimate actions applicable to each state. In particular, we stage the actions into three main steps: locating the topic entity in the question, finding the main relationship between the answer and the topic entity, and expanding the query graph with additional constraints that"
P15-1128,D14-1067,0,0.894208,"ce, one of our constructions maps the question to a pattern by replacing the entity mention with a generic symbol <e&gt; and then compares it with a candidate chain, such as “who first voiced meg on <e&gt;” vs. cast-actor. The model consists of two neural networks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similarity is then computed using some distance function, such as cosine. This continuous-space representation approach has been proposed recently for semantic parsing and question answering (Bordes et al., 2014a; Yih et al., 2014) and has shown better results compared to lexical matching approaches (e.g., word-alignment models). In this work, we adapt a convolutional neural network (CNN) framework (Shen et al., 2014b; Shen et al., 2014a; Gao et al., 2014) to this matching problem. The network architecture is illustrated in Fig. 6. The CNN model first applies a word hashing technique (Huang et al., 2013) that breaks a word into a vector of letter-trigrams (xt → ft in Fig. 6). For example, the bag of letter-trigrams of the word “who” are #-w-h, w-h-o, h-o-# after adding the actor x Meg Griffin Convolu"
P15-1128,P13-1042,0,0.321803,"Missing"
P15-1128,Q14-1030,0,0.0642215,"to map entities retrieved by the query. The diamond node arg min constrains that the answer needs to be the earliest actor for this role. Equivalently, the logical form query in λ-calculus without the aggregation function is: λx.∃y.cast(FamilyGuy, y) ∧ actor(y, x) ∧ character(y, MegGriffin) Running this query graph against K as in Fig. 1 will match both LaceyChabert and MilaKunis before applying the aggregation function, but only LaceyChabert is the correct answer as she started this role earlier (by checking the from property of the grounded CVT node). Our query graph design is inspired by (Reddy et al., 2014), but with some key differences. The nodes and edges in our query graph closely resemble the exact entities and predicates from the knowledge base. As a result, the graph can be straightforwardly translated to a logical form query that is directly executable. In contrast, the query graph in (Reddy et al., 2014) is mapped from the CCG parse of the question, and needs further transformations before mapping to subgraphs 2 y should be grounded to a CVT entity in this case. of the target knowledge base to retrieve answers. Semantically, our query graph is more related to simple λ-DCS (Berant et al."
P15-1128,P15-1049,1,0.237944,"iority queue, which is formally defined in Appendix A. In the following subsections, we use a running example of finding the semantic parse of question qex = “Who first voiced Meg of Family Guy?” to describe the sequence of actions. 3.1 Family Guy Linking Topic Entity Starting from the initial state s0 , the valid actions are to create a single-node graph that corresponds to the topic entity found in the given question. For instance, possible topic entities in qex can either be FamilyGuy or MegGriffin, shown in Fig. 4. We use an entity linking system that is designed for short and noisy text (Yang and Chang, 2015). For each entity e in the knowledge base, the system first prepares a surface-form lexicon that lists all possible ways that e can be mentioned in text. This lexicon is created using various data sources, such as names and aliases of the entities, the anchor text in Web documents and the Wikipedia redirect table. Given a question, it considers all the y Family Guy cast Family Guy writer y Family Guy genre x Family Guy actor start x x Figure 5: Candidate core inferential chains start from the entity FamilyGuy. consecutive word sequences that have occurred in the lexicon as possible mentions, p"
P15-1128,D14-1071,0,0.379928,"t nodes indicating that certain conditions cannot be satisfied. Our graph generation method is inspired by (Yao and Van Durme, 2014; Bao et al., 2014). Unlike traditional semantic parsing approaches, it uses the knowledge base to help prune the search space when forming the parse. Similar ideas have also been explored in (Poon, 2013). Empirically, our results suggest that it is crucial to identify the core inferential chain, which matches the relationship between the topic entity in the question and the answer. Our CNN models can be analogous to the embedding approaches (Bordes et al., 2014a; Yang et al., 2014), but are more sophisticated. By allowing parameter sharing among different question-pattern and KB predicate pairs, the matching score of a rare or even unseen pair in the training data can still be predicted precisely. This is due to the fact that the prediction is based on the shared model parameters (i.e., projection matrices) that are estimated using all training pairs. 6 Conclusion In this paper, we present a semantic parsing framework for question answering using a knowledge base. We define a query graph as the meaning representation that can be directly mapped to a logical form. Semant"
P15-1128,P14-1090,0,0.722517,"Missing"
P15-1128,P14-2105,1,0.725471,"ctions maps the question to a pattern by replacing the entity mention with a generic symbol <e&gt; and then compares it with a candidate chain, such as “who first voiced meg on <e&gt;” vs. cast-actor. The model consists of two neural networks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similarity is then computed using some distance function, such as cosine. This continuous-space representation approach has been proposed recently for semantic parsing and question answering (Bordes et al., 2014a; Yih et al., 2014) and has shown better results compared to lexical matching approaches (e.g., word-alignment models). In this work, we adapt a convolutional neural network (CNN) framework (Shen et al., 2014b; Shen et al., 2014a; Gao et al., 2014) to this matching problem. The network architecture is illustrated in Fig. 6. The CNN model first applies a word hashing technique (Huang et al., 2013) that breaks a word into a vector of letter-trigrams (xt → ft in Fig. 6). For example, the bag of letter-trigrams of the word “who” are #-w-h, w-h-o, h-o-# after adding the actor x Meg Griffin Convolution matrix: Wc Word"
P15-1128,P13-1092,0,\N,Missing
P16-2033,P14-1133,0,0.0172645,"cted entity will be shown, along with the objects (either properties or entities). Suppose the labeler chooses Meg Griffin as the topic entity. He should then pick actor as the main relationship, meaning the answer should be the persons who have played this role. To accurately describe the question, the labeler should add additional filters like the TV series is Family Guy and the performance type is voice in the final stage2 . The design of our UI is inspired by recent work on semantic parsing that has been applied to the W EB Q UESTIONS dataset (Bast and Haussmann, 2015; Reddy et al., 2014; Berant and Liang, 2014; Yih et al., 2015), as these approaches use a simpler and yet more restricted semantic representation than first-order logic expressions. Following the notion of query graph in (Yih et al., 2015), the semantic parse is anchored to one of the entities in the question as the topic entity and the core component is to represent the relation between the entity and the answer, referred as the inferential chain. Constraints, such as properties of the answer or additional conditions the relation needs to hold, are captured as well. Figure 1 shows an example of these annotated semantic parse component"
P16-2033,D13-1160,0,0.713298,"o an absolute 5-point increase in average F1 . Our work demonstrates that semantic parse labels can provide additional value over answer labels while, with the right labeling tools, being comparable in cost to collect. Besides accuracy gains, semantic parses also have further benefits in yielding answers that are more accurate and consistent, as well as being updatable if the knowledge base changes (for example, as facts are added or revised). 2 Collecting Semantic Parses In order to verify the benefits of having labeled semantic parses, we completely re-annotated the W EB Q UESTIONS dataset (Berant et al., 2013) such that it contains both semantic parses and the derived answers. We chose to annotate the questions with the full semantic parses in SPARQL, based on the schema and data of the latest and last version of Freebase (2015-08-09). Labeling interface Writing SPARQL queries for natural language questions using a text editor is obviously not an efficient way to provide semantic parses even for experts. Therefore, we designed a staged, dialog-like user interface (UI) to improve the labeling efficiency. Our UI breaks the potentially complicated structured-labeling task into separate, but inter-depe"
P16-2033,P13-1042,0,0.0413412,"Missing"
P16-2033,W10-2903,1,0.309285,"ins, 2005; Wong and Mooney, 2007). Recent work on semantic parsing for knowledge base questionanswering (KBQA) has called into question the value of collecting such semantic parse labels, with most recent KBQA semantic parsing systems being trained using only question-answer pairs instead of question-parse pairs. In fact, there is evidence that using only question-answer pairs can yield improved performance as compared with approaches based on semantic parse labels (Liang et al., 2013). It is also widely believed that collecting semantic parse labels can be a “difficult, time consuming task” (Clarke et al., 2010) even for domain experts. Furthermore, recent focus has been more on the final task-specific performance of a 1 Available at http://aka.ms/WebQSP. 201 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 201–206, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics further constrain the answers. One key advantage of our UI design is that the annotator only needs to focus on one particular sub-task during each stage. All of the choices made by the labeler are used to automatically construct a coherent semantic parse. Note"
P16-2033,D15-1076,0,0.0113682,"when we limit to semantically valid paths. However, in domains where the number of potential paths between candidate entities and answers is small, the value of collecting semantic parse labels might also be small. complements existing datasets in these individual tasks, as they tend to target at normal corpora of regular sentences. While our labeling interface design was aimed at supporting labeling experts, it would be valuable to enable crowdsourcing workers to provide semantic parse labels. One promising approach is to use a more dialog-driven interface using natural language (similar to (He et al., 2015)). Such UI design is also crucial for extending our work to handling more complicated questions. For instance, allowing users to traverse longer paths in a sequential manner will increase the expressiveness of the output parses, both in the core relation and constraints. Displaying a small knowledge graph centered at the selected entities and relations may help users explore alternative relations more effectively as well. Semantic parsing labels provide additional benefits. For example, collecting semantic parse labels relative to a knowledge base can ensure that the answers are more faithful"
P16-2033,D13-1161,0,0.260095,"Missing"
P16-2033,J13-2005,0,0.0486608,"ntic parsers made use of datasets of questions and their associated semantic parses (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Recent work on semantic parsing for knowledge base questionanswering (KBQA) has called into question the value of collecting such semantic parse labels, with most recent KBQA semantic parsing systems being trained using only question-answer pairs instead of question-parse pairs. In fact, there is evidence that using only question-answer pairs can yield improved performance as compared with approaches based on semantic parse labels (Liang et al., 2013). It is also widely believed that collecting semantic parse labels can be a “difficult, time consuming task” (Clarke et al., 2010) even for domain experts. Furthermore, recent focus has been more on the final task-specific performance of a 1 Available at http://aka.ms/WebQSP. 201 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 201–206, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics further constrain the answers. One key advantage of our UI design is that the annotator only needs to focus on one particular sub-"
P16-2033,Q14-1030,0,0.0423834,"edicates of the selected entity will be shown, along with the objects (either properties or entities). Suppose the labeler chooses Meg Griffin as the topic entity. He should then pick actor as the main relationship, meaning the answer should be the persons who have played this role. To accurately describe the question, the labeler should add additional filters like the TV series is Family Guy and the performance type is voice in the final stage2 . The design of our UI is inspired by recent work on semantic parsing that has been applied to the W EB Q UESTIONS dataset (Bast and Haussmann, 2015; Reddy et al., 2014; Berant and Liang, 2014; Yih et al., 2015), as these approaches use a simpler and yet more restricted semantic representation than first-order logic expressions. Following the notion of query graph in (Yih et al., 2015), the semantic parse is anchored to one of the entities in the question as the topic entity and the core component is to represent the relation between the entity and the answer, referred as the inferential chain. Constraints, such as properties of the answer or additional conditions the relation needs to hold, are captured as well. Figure 1 shows an example of these annotated"
P16-2033,P07-1121,0,0.012919,"compared to learning from answers, (2) we show that with an appropriate user interface, one can obtain semantic parses with high accuracy and at a cost comparable or lower than obtaining just answers, and (3) we have created and shared the largest semantic-parse labeled dataset to date in order to advance research in question answering. 1 Introduction Semantic parsing is the mapping of text to a meaning representation. Early work on learning to build semantic parsers made use of datasets of questions and their associated semantic parses (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Recent work on semantic parsing for knowledge base questionanswering (KBQA) has called into question the value of collecting such semantic parse labels, with most recent KBQA semantic parsing systems being trained using only question-answer pairs instead of question-parse pairs. In fact, there is evidence that using only question-answer pairs can yield improved performance as compared with approaches based on semantic parse labels (Liang et al., 2013). It is also widely believed that collecting semantic parse labels can be a “difficult, time consuming task” (Clarke et al., 2010) even for dom"
P16-2033,P15-1049,1,0.352876,"ic parses in SPARQL, based on the schema and data of the latest and last version of Freebase (2015-08-09). Labeling interface Writing SPARQL queries for natural language questions using a text editor is obviously not an efficient way to provide semantic parses even for experts. Therefore, we designed a staged, dialog-like user interface (UI) to improve the labeling efficiency. Our UI breaks the potentially complicated structured-labeling task into separate, but inter-dependent sub-tasks. Given a question, the UI first presents entities detected in the questions using an entity linking system (Yang and Chang, 2015), and asks the user to pick an entity in the question as the topic entity that could lead to the answers. The user can also suggest a new entity if none of the candidates returned by the entity linking system is correct. Once the entity is selected, the system then requests the user to pick the Freebase predicate that represents the relationship between the answers and this topic entity. Finally, additional filters can be added to Labeling process In order to ensure the data quality, we recruit five annotators who are familiar with design of Freebase. Our goal is to provide 2 Screenshots are i"
P16-2033,P15-1128,1,0.82746,"n, along with the objects (either properties or entities). Suppose the labeler chooses Meg Griffin as the topic entity. He should then pick actor as the main relationship, meaning the answer should be the persons who have played this role. To accurately describe the question, the labeler should add additional filters like the TV series is Family Guy and the performance type is voice in the final stage2 . The design of our UI is inspired by recent work on semantic parsing that has been applied to the W EB Q UESTIONS dataset (Bast and Haussmann, 2015; Reddy et al., 2014; Berant and Liang, 2014; Yih et al., 2015), as these approaches use a simpler and yet more restricted semantic representation than first-order logic expressions. Following the notion of query graph in (Yih et al., 2015), the semantic parse is anchored to one of the entities in the question as the topic entity and the core component is to represent the relation between the entity and the answer, referred as the inferential chain. Constraints, such as properties of the answer or additional conditions the relation needs to hold, are captured as well. Figure 1 shows an example of these annotated semantic parse components and the correspon"
P16-2033,P11-1060,0,\N,Missing
P17-1167,N16-1181,0,0.0369064,"ring function. In our design, the score of a state is determined by the scores of actions taken from the initial state to the target state, which are predicted by different neural network modules based on action type. By leveraging a margin-based objective function, the model learning procedure resembles several structured-output learning algorithms such as structured SVMs (Tsochantaridis et al., 2005), but can take either strong or weak supervision seamlessly. DynSP is inspired by STAGG, a search-based semantic parser (Yih et al., 2015), as well as the dynamic neural module network (DNMN) of Andreas et al. (2016). Much like STAGG, DynSP chains together different modules as search progresses; however, these modules are implemented as neural networks, which enables end-to-end training as in DNMN. The key difference between DynSP and DNMN is that in DynSP the network structure of an example is not predetermined. Instead, different network structures are constructed dynamically as our learning procedure explores the state space. It is straightforward to answer sequential questions using our framework: we allow the model to take the previous question and its answers as input, with a slightly modified actio"
P17-1167,D11-1039,0,0.0126661,"y implemented in our language due to concerns with the search space size. Increasing the number of complex actions requires designing smarter optimization procedures, which we leave to future work. 5 Related Work Previous work on conversational QA has focused on small, single-domain datasets. Perhaps most related to our task is the context-dependent sentence analysis described in (Zettlemoyer and Collins, 2009), where conversations between customers and travel agents are mapped to logical forms after resolving referential expressions. Another dataset of travel booking conversations is used by Artzi and Zettlemoyer (2011) to learn a semantic parser for complicated queries given user clarifications. More recently, Long et al. (2016) collect three contextual semantic parsing datasets (from synthetic domains) that contain coreferences to entities and MAX SUBSEQUENT 1 SUBSEQUENT 2 Figure 3: Parses computed by DynSP for three test sequences (actions in blue boxes, values from table in white boxes). Top: all three questions are parsed correctly. Middle: semantic matching errors cause the model to select incorrect columns and conditions. Bottom: The final question is unanswerable due to limitations of our parse langu"
P17-1167,D14-1179,0,0.00414407,"Missing"
P17-1167,P05-1026,0,0.0994602,"rse language. actions. We differentiate ourselves from these prior works in two significant ways: first, our dataset is not restricted to a particular domain, and second, a major goal of our work is to analyze the different types of sequence progressions people create when they are trying to express a complicated intent. Complex, interactive QA tasks have also been proposed in the information retrieval community, where the data source is a corpus of newswire text (Kelly and Lin, 2007). We also build on aspects of some existing interactive question-answering systems. For example, the system of Harabagiu et al. (2005) includes a module that predicts what a user will ask next given their current question. Other than FP and NP, the work of Neural Symbolic Machines (NSM) (Liang et al., 2017) is perhaps the closest to ours. NSM aims to generate formal semantic parses of questions that can be executed on Freebase to retrieve answers, and is trained using the REINFORCE algorithm (Williams, 1992) augmented with approximate gold parses found in a separate curriculum learning stage. In comparison, finding reference parses is an integral part of our algorithm. Our non1828 probabilistic, margin-based objective functi"
P17-1167,P17-1003,0,0.117723,"goal of our work is to analyze the different types of sequence progressions people create when they are trying to express a complicated intent. Complex, interactive QA tasks have also been proposed in the information retrieval community, where the data source is a corpus of newswire text (Kelly and Lin, 2007). We also build on aspects of some existing interactive question-answering systems. For example, the system of Harabagiu et al. (2005) includes a module that predicts what a user will ask next given their current question. Other than FP and NP, the work of Neural Symbolic Machines (NSM) (Liang et al., 2017) is perhaps the closest to ours. NSM aims to generate formal semantic parses of questions that can be executed on Freebase to retrieve answers, and is trained using the REINFORCE algorithm (Williams, 1992) augmented with approximate gold parses found in a separate curriculum learning stage. In comparison, finding reference parses is an integral part of our algorithm. Our non1828 probabilistic, margin-based objective function also helps avoid the need for empirical tricks to handle normalization and proper sampling, which are crucial when applying REINFORCE in practice. 6 Conclusion & Future Wo"
P17-1167,P16-1138,0,0.279417,"s designing smarter optimization procedures, which we leave to future work. 5 Related Work Previous work on conversational QA has focused on small, single-domain datasets. Perhaps most related to our task is the context-dependent sentence analysis described in (Zettlemoyer and Collins, 2009), where conversations between customers and travel agents are mapped to logical forms after resolving referential expressions. Another dataset of travel booking conversations is used by Artzi and Zettlemoyer (2011) to learn a semantic parser for complicated queries given user clarifications. More recently, Long et al. (2016) collect three contextual semantic parsing datasets (from synthetic domains) that contain coreferences to entities and MAX SUBSEQUENT 1 SUBSEQUENT 2 Figure 3: Parses computed by DynSP for three test sequences (actions in blue boxes, values from table in white boxes). Top: all three questions are parsed correctly. Middle: semantic matching errors cause the model to select incorrect columns and conditions. Bottom: The final question is unanswerable due to limitations of our parse language. actions. We differentiate ourselves from these prior works in two significant ways: first, our dataset is n"
P17-1167,P09-1110,0,0.0506913,"e surface, the final question in the bottom sequence of Figure 3 is one such example; the correct semantic parse requires access to the answers of both the first and second question, actions that we have not currently implemented in our language due to concerns with the search space size. Increasing the number of complex actions requires designing smarter optimization procedures, which we leave to future work. 5 Related Work Previous work on conversational QA has focused on small, single-domain datasets. Perhaps most related to our task is the context-dependent sentence analysis described in (Zettlemoyer and Collins, 2009), where conversations between customers and travel agents are mapped to logical forms after resolving referential expressions. Another dataset of travel booking conversations is used by Artzi and Zettlemoyer (2011) to learn a semantic parser for complicated queries given user clarifications. More recently, Long et al. (2016) collect three contextual semantic parsing datasets (from synthetic domains) that contain coreferences to entities and MAX SUBSEQUENT 1 SUBSEQUENT 2 Figure 3: Parses computed by DynSP for three test sequences (actions in blue boxes, values from table in white boxes). Top: a"
P17-1167,D17-1252,1,0.0819152,"time to converge. In this work, we propose a conceptually simple learning algorithm for weakly supervised training that sidesteps the inefficient learning problem. Our key insight is to conduct inference using a beam search procedure guided by an approximate reward function. The search procedure is executed twice for each training example, one for finding the best possible reference semantic parse and the other for finding the predicted semantic parse to update the model. Our framework is suitable for learning from either implicit or explicit supervision, and is detailed in a companion paper (Peng et al., 2017). Below we describe how we adapt it to the semantic parsing problem in this work. Approximate reward Let A(s) be the answers retrieved by executing the semantic parse represented by state s, and let A∗ be the set of gold answers of a given question. We define the reward R(s; A∗ ) = 1[A(s) = A∗ ], or the accuracy of the retrieved answers. We use R(s) as the abbreviation for R(s; A∗ ). A state s with R(s) = 1 is called a goal state. Directly using this reward function in search of goal states can be difficult, as rewards of most states are 0. However, even when the answers from a semantic parse"
P17-1167,D14-1162,0,0.11017,"nal constraints. 4.1 DynSP implementation details Unlike previous dynamic neural network frameworks (Andreas et al., 2016; Looks et al., 2017), where each example can have different but predetermined structure, DynSP needs to dynamically explores and constructs different neural network structures for each question. Therefore, we choose DyNet (Neubig et al., 2017) as our implementation platform for its flexibility in composing computation graphs. We optimize our model parameters using standard stochastic gradient descent. The word embeddings are initialized with 100-d pretrained GloVe vectors (Pennington et al., 2014) and fine-tuned during training with dropout rate 0.5. For follow-up questions, we choose uniformly at random to use either gold answers to the previous question or the model’s previous predictions.8 We constrain the maximum length of actions to 3 for computational efficiency and set the beam size to 15 in our reported models, as accuracy gains are negligible with larger beam sizes. We train our model for 30 epochs, although the best model on the validation set is usually found within the first 20 epochs. Only CPU is used in model training, and each epoch in the beam size 15 setting takes abou"
P17-1167,P15-1128,1,0.52009,"se. The quality of the induced semantic parse obviously depends on the scoring function. In our design, the score of a state is determined by the scores of actions taken from the initial state to the target state, which are predicted by different neural network modules based on action type. By leveraging a margin-based objective function, the model learning procedure resembles several structured-output learning algorithms such as structured SVMs (Tsochantaridis et al., 2005), but can take either strong or weak supervision seamlessly. DynSP is inspired by STAGG, a search-based semantic parser (Yih et al., 2015), as well as the dynamic neural module network (DNMN) of Andreas et al. (2016). Much like STAGG, DynSP chains together different modules as search progresses; however, these modules are implemented as neural networks, which enables end-to-end training as in DNMN. The key difference between DynSP and DNMN is that in DynSP the network structure of an example is not predetermined. Instead, different network structures are constructed dynamically as our learning procedure explores the state space. It is straightforward to answer sequential questions using our framework: we allow the model to take"
P17-1167,P16-2033,1,0.0431227,"cores of actions in the sequence, the goal of model optimization is to learn the parameters in the neural networks behind the policy function. Let θ be the collection of all the model parameters. Then thePstate value function can be written as: Vθ (st ) = ti=1 πθ (si−1 , ai ). In a fully supervised setting where the correct semantic parse of each question is available, learning the policy function can be reduced to a sequence prediction problem. However, while having full supervision leads to a better semantic parser, collecting the correct parses requires a much more sophisticated UI design (Yih et al., 2016). In many scenarios, such as the one in the SQA dataset, it is often the case that only the answers to the questions are available. Adapting a learning algorithm to this weakly supervised setting is thus critical. Generally speaking, weakly supervised semantic parsers operate on one assumption — a candidate semantic parse is treated as a correct one if it results in answers that are identical to the gold answers. Therefore, a straightforward modification of existing structured learning algorithms in our setting is to use any semantic parse found to evaluate to the correct answers during beam s"
P17-1167,P15-1142,0,\N,Missing
P19-1335,N19-1423,1,0.727081,"t al. (2011); Chen et al. (2012); Yang and Eisenstein (2015), inter alia, pre-trained on the source and target domain unlabeled data jointly with the goal of discovering features that generalize across domains. After pre-training, the model is fine-tuned on the source-domain labeled data.6 Open-corpus pre-training Instead of explicitly adapting to a target domain, this approach simply applies unsupervised pre-training to large corpora before fine-tuning on the source-domain labeled data. Examples of this approach include ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). 6 In many works, the learned representations are kept fixed and only higher layers are updated. 3453 Intuitively, the target-domain distribution is likely to be partially captured by pre-training if the open corpus is sufficiently large and diverse. Indeed, open-corpus pre-training has been shown to benefit out-of-domain performance far more than indomain performance (He et al., 2018). Domain-adaptive pre-training In addition to pre-training stages from other approaches, we propose to insert a penultimate domain adaptive pre-training (DAP) stage, where the model is pre-trained only on the ta"
P19-1335,D17-1277,0,0.200861,"Missing"
P19-1335,D18-1498,0,0.048978,"Missing"
P19-1335,D17-1284,0,0.326376,"etween EL datasets (Bunescu and Pasca, 2006; Ling et al., 2015), most focus on a standard setting where mentions from a comprehensive test entity dictionary (often Wikipedia) are seen during training, and rich statistics and meta-data can be utilized (Roth et al., 2014). Labeled in-domain documents with mentions are also assumed to be available. Cross-Domain EL Recent work has also generalized to a cross-domain setting, linking entity mentions in different types of text, such as blogposts and news articles to the Wikipedia KB, while only using labeled mentions in Wikipedia for training (e.g., Gupta et al. (2017); Le and Titov (2018), inter alia). Linking to Any DB Sil et al. (2012) proposed a task setup very similar to ours, and later work (Wang et al., 2015) has followed a similar setting. The main difference between zero-shot EL and these works is that they assumed either a highcoverage alias table or high-precision token overlap heuristics to reduce the size of the entity candidate set (i.e., to less than four in Sil et al. (2012)) and relied on structured data to help disambiguation. By compiling and releasing a multi-world dataset focused on learning from textual information, we hope to help dri"
P19-1335,P18-2058,1,0.846885,"es unsupervised pre-training to large corpora before fine-tuning on the source-domain labeled data. Examples of this approach include ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). 6 In many works, the learned representations are kept fixed and only higher layers are updated. 3453 Intuitively, the target-domain distribution is likely to be partially captured by pre-training if the open corpus is sufficiently large and diverse. Indeed, open-corpus pre-training has been shown to benefit out-of-domain performance far more than indomain performance (He et al., 2018). Domain-adaptive pre-training In addition to pre-training stages from other approaches, we propose to insert a penultimate domain adaptive pre-training (DAP) stage, where the model is pre-trained only on the target-domain data. As usual, DAP is followed by a final fine-tuning stage on the source-domain labeled data. The intuition for DAP is that representational capacity is limited, so models should prioritize the quality of target domain representations above all else. We introduce notation to describe various ways in which pre-training stages can be composed. • Usrc denotes text segments fr"
P19-1335,P18-1031,0,0.049145,"Missing"
P19-1335,P18-1148,0,0.0731455,"and Pasca, 2006; Ling et al., 2015), most focus on a standard setting where mentions from a comprehensive test entity dictionary (often Wikipedia) are seen during training, and rich statistics and meta-data can be utilized (Roth et al., 2014). Labeled in-domain documents with mentions are also assumed to be available. Cross-Domain EL Recent work has also generalized to a cross-domain setting, linking entity mentions in different types of text, such as blogposts and news articles to the Wikipedia KB, while only using labeled mentions in Wikipedia for training (e.g., Gupta et al. (2017); Le and Titov (2018), inter alia). Linking to Any DB Sil et al. (2012) proposed a task setup very similar to ours, and later work (Wang et al., 2015) has followed a similar setting. The main difference between zero-shot EL and these works is that they assumed either a highcoverage alias table or high-precision token overlap heuristics to reduce the size of the entity candidate set (i.e., to less than four in Sil et al. (2012)) and relied on structured data to help disambiguation. By compiling and releasing a multi-world dataset focused on learning from textual information, we hope to help drive progress in linkin"
P19-1335,P18-1010,0,0.0448191,"respectively the normalized and unnormalized accuracies. 7 Related Work We discussed prior entity linking task definitions and compared them to our task in section 2. Here, we briefly overview related entity linking models and unsupervised domain adaptation methods. Entity linking models Entity linking given mention boundaries as input can be broken into the tasks of candidate generation and candidate ranking. When frequency information or alias tables are unavailable, prior work has used measures of similarity of the mention string to entity names for candidate generation (Sil et al., 2012; Murty et al., 2018). For candidate ranking, recent work employed distributed representations of mentions in context and entity candidates and neural models to score their compatibility. Mentions in context have been represented using e.g., CNN (Murty et al., 2018), LSTM (Gupta et al., 2017), or bag-of-word embeddings (Ganea and Hofmann, 2017). Entity descriptions have been represented using similar architectures. To the best of our knowledge, while some models allow for crossattention between single-vector entity embeddings and mention-in-context token representations, no prior works have used full cross-attenti"
P19-1335,N18-1202,1,0.490024,"to the two existing approaches. Task-adaptive pre-training Glorot et al. (2011); Chen et al. (2012); Yang and Eisenstein (2015), inter alia, pre-trained on the source and target domain unlabeled data jointly with the goal of discovering features that generalize across domains. After pre-training, the model is fine-tuned on the source-domain labeled data.6 Open-corpus pre-training Instead of explicitly adapting to a target domain, this approach simply applies unsupervised pre-training to large corpora before fine-tuning on the source-domain labeled data. Examples of this approach include ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). 6 In many works, the learned representations are kept fixed and only higher layers are updated. 3453 Intuitively, the target-domain distribution is likely to be partially captured by pre-training if the open corpus is sufficiently large and diverse. Indeed, open-corpus pre-training has been shown to benefit out-of-domain performance far more than indomain performance (He et al., 2018). Domain-adaptive pre-training In addition to pre-training stages from other approaches, we propose to insert a penultimate domain adaptive pre-"
P19-1335,D15-1081,0,0.407235,"dictionaries such as legal cases, company project descriptions, the set of characters in a novel, or a terminology glossary. Unfortunately, labeled data are not readily available and are often expensive to obtain for these specialized entity dictionaries. Therefore, we need to develop entity linking systems that can generalize to unseen specialized entities. Without frequency statistics and meta-data, the task becomes substantially more challenging. Some prior works have pointed out the importance of building entity linking systems that can generalize to unseen entity sets (Sil et al., 2012; Wang et al., 2015), but adopt an additional set of assumptions. 3449 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3449–3460 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics In this work, we propose a new zero-shot entity linking task, and construct a new dataset for it.2 The target dictionary is simply defined as a set of entities, each with a text description (from a canonical entity page, for example). We do not constrain mentions to named entities, unlike some prior work, which makes the task harder due to large numb"
P19-1335,N15-1069,0,0.0314569,"similar to Pool-Transformer and Cand-Pool-Transformer respectively but with different neural architectures for encoding. 5 Adapting to the Target World We focus on using unsupervised pre-training to ensure that downstream models are robust to target domain data. There exist two general strategies for pre-training: (1) task-adaptive pre-training, and (2) open-corpus pre-training. We describe these below, and also propose a new strategy: domainadaptive pre-training (DAP), which is complementary to the two existing approaches. Task-adaptive pre-training Glorot et al. (2011); Chen et al. (2012); Yang and Eisenstein (2015), inter alia, pre-trained on the source and target domain unlabeled data jointly with the goal of discovering features that generalize across domains. After pre-training, the model is fine-tuned on the source-domain labeled data.6 Open-corpus pre-training Instead of explicitly adapting to a target domain, this approach simply applies unsupervised pre-training to large corpora before fine-tuning on the source-domain labeled data. Examples of this approach include ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). 6 In many works, the learned represent"
P19-1335,P14-6004,1,0.925416,"the zero-shot entity linking task and discuss its relationship to prior work. 2 Existing datasets are either unsuitable or would have to be artificially partitioned to construct a dataset for this task. 2.1 Review: Entity linking Entity linking (EL) is the task of grounding entity mentions by linking them to entries in a given database or dictionary of entities. Formally, given a mention m and its context, an entity linking system links m to the corresponding entity in an entity set E = {ei }i=1,...,K , where K is the number of entities. The standard definition of EL (Bunescu and Pasca, 2006; Roth et al., 2014; Sil et al., 2018) assumes that mention boundaries are provided by users or a mention detection system. The entity set E can contain tens of thousands or even millions of entities, making this a challenging task. In practice, many entity linking systems rely on the following resources or assumptions: Single entity set This assumes that there is a single comprehensive set of entities E shared between training and test examples. Alias table An alias table contains entity candidates for a given mention string and limits the possibilities to a relatively small set. Such tables are often compiled"
P19-1335,P18-5008,0,0.0129955,"y linking task and discuss its relationship to prior work. 2 Existing datasets are either unsuitable or would have to be artificially partitioned to construct a dataset for this task. 2.1 Review: Entity linking Entity linking (EL) is the task of grounding entity mentions by linking them to entries in a given database or dictionary of entities. Formally, given a mention m and its context, an entity linking system links m to the corresponding entity in an entity set E = {ei }i=1,...,K , where K is the number of entities. The standard definition of EL (Bunescu and Pasca, 2006; Roth et al., 2014; Sil et al., 2018) assumes that mention boundaries are provided by users or a mention detection system. The entity set E can contain tens of thousands or even millions of entities, making this a challenging task. In practice, many entity linking systems rely on the following resources or assumptions: Single entity set This assumes that there is a single comprehensive set of entities E shared between training and test examples. Alias table An alias table contains entity candidates for a given mention string and limits the possibilities to a relatively small set. Such tables are often compiled from a labeled trai"
P19-1335,D12-1011,0,0.496517,"Missing"
P19-1335,E06-1002,0,\N,Missing
P19-1335,Q15-1023,0,\N,Missing
P19-1483,W00-1401,0,0.21157,"e an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information which no system can be expected to produce given only the associated tab"
P19-1483,H05-1042,0,0.0465407,"Missing"
P19-1483,E06-1040,0,0.0647752,"ema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours i"
P19-1483,W07-0718,0,0.171507,"Missing"
P19-1483,E06-1032,0,0.111485,"al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show an"
P19-1483,W17-3209,0,0.0568619,"Missing"
P19-1483,D16-1128,0,0.611103,"the information extraction based evaluation proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has lar"
P19-1483,P18-1060,0,0.0664973,"Missing"
P19-1483,W14-3346,0,0.0160021,"f the longest common subsequence between x and y. The LCS function, borrowed from ROUGE, ensures that entity names in r¯k appear in the same order in the text as the table. Higher values of Er (T i ) denote that more records are likely to be mentioned in Gi . The entailed precision and recall are combined into an F-score to give the PARENT metric for one instance. The system-level PARENT score for a Smoothing & Multiple References. The danger with geometric averages is that if any of the components being averaged become 0, the average will also be 0. Hence, we adopt a smoothing technique from Chen and Cherry (2014) that assigns a small positive value  to any of Epn , Ern (Ri ) and Er (T i ) which are 0. When multiple references are available for a table, we compute PARENT against each reference and take the maximum as its overall score, similar to METEOR (Denkowski and Lavie, 2014). Choosing λ and . To set the value of λ we can tune it to maximize the correlation of the metric with human judgments, when such data is available. When such data is not available, we can use the recall of the reference against the table, using Eq. 6, as the value of 1 − λ. The intuition here is that if the recall of the re"
P19-1483,P09-1011,0,0.345045,"Missing"
P19-1483,W04-1013,0,0.218157,"et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information which no system can be expected to produce given only the associated table. We call such reference texts divergent from the table. We show that existing automatic metrics, including BLEU, correlate"
P19-1483,W14-3348,0,0.355805,"led precision and recall are combined into an F-score to give the PARENT metric for one instance. The system-level PARENT score for a Smoothing & Multiple References. The danger with geometric averages is that if any of the components being averaged become 0, the average will also be 0. Hence, we adopt a smoothing technique from Chen and Cherry (2014) that assigns a small positive value  to any of Epn , Ern (Ri ) and Er (T i ) which are 0. When multiple references are available for a table, we compute PARENT against each reference and take the maximum as its overall score, similar to METEOR (Denkowski and Lavie, 2014). Choosing λ and . To set the value of λ we can tune it to maximize the correlation of the metric with human judgments, when such data is available. When such data is not available, we can use the recall of the reference against the table, using Eq. 6, as the value of 1 − λ. The intuition here is that if the recall of the reference against the table is high, it already covers most of the information, and we can assign it a high weight in Eq. 4. This leads to a separate value of λ automatically set for each instance.7  is set to 10−5 for all experiments. 4 Evaluation via Information Extractio"
P19-1483,P17-1017,0,0.371266,"al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). Th"
P19-1483,W05-1208,0,0.0215623,"recision Epn for n-grams of order n is given by: Epn = P g∈Gin P   / Rni )w(g) #Gin (g) P r(g ∈ Rni ) + P r(g ∈ P , g∈Gin #Gin (g) g∈Gin = #Gin (g)w(g) + #Gin ,Rni (g)[1 − w(g)] P . g∈Gin #Gin (g) (2) In words, an n-gram receives a reward of 1 if it appears in the reference, with probability P r(g ∈ Rni ), and otherwise it receives a reward of w(g). Both numerator and denominator are weighted by the count of the n-gram in Gin . P r(g ∈ Rni ) rewards an n-gram for appearing as many times as it appears in the reference, not more. We combine precisions for n-gram orders 1-4 using a geometric 5 Glickman and Dagan (2005) used a product instead of geometric mean. Here we use a geometric mean to ensure that n-grams of different lengths have comparable probabilities of being entailed. 6 It is unlikely that an automated system produces the same extra n-gram as present in the reference, thus a match with the reference n-gram is considered positive. For example, in Figure 1, it is highly unlikely that a system would produce “Silkworm” when it is not present in the table. 4886 model M is the average of instance level PARENT scores across the evaluation set: average, similar to BLEU: Ep = exp 4 X 1 n=1 4 ! log Epn N"
P19-1483,D14-1020,0,0.0226453,"an idea of how the correlation of each metric varies with the underlying data. In each iteration, we sample with replacement, tables along with their references and all the generated texts for that table. Then we compute aggregated human evaluation and metric scores for each of the models and compute the correlation between the two. We report the average correlation across all bootstrap samples for each metric in Table 2. The distribution of correlations for the best performing metrics are shown in Figure 3. Table 2 also indicates whether PARENT is significantly better than a baseline metric. Graham and Baldwin (2014) suggest using the William’s test for this purpose, but since we are computing correlations between only 4/13 systems at a time, this test has very weak power in our case. Hence, we use the bootstrap samples to obtain a 1 − α confidence interval of the difference in correlation 4889 1.0 WikiBio-Systems 0.8 1.0 WikiBio-Hyperparams 0.8 0.8 0.6 0.6 0.4 0.6 0.2 0.0 U -T -F C BLE BLEU RG PRT- PRT-WPRT*-W 0.0 U -T -F C BLE BLEU RG PRT- PRT-WPRT*-W Figure 3: Distribution of metric correlations across 500 bootstrap samples. PRT = PARENT. between PARENT and any other metric and check whether this is ab"
P19-1483,D17-1274,0,0.041252,"Missing"
P19-1483,P83-1022,0,0.360705,"gh a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system."
P19-1483,D10-1090,0,0.0335412,"Bio, the recall of the references against the table is generally low, and hence the recall of the generated text relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often 1.0), and hence 12 We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work. 4891 While iBLEU penalizes texts which match the source, here we reward such texts since our task values accuracy of generated text more than the need for paraphrasing the tabular content (Liu et al., 2010). Similar to SARI for text simplification (Xu et al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic met"
P19-1483,D16-1230,0,0.0372707,"nder the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours is the first study which checks t"
P19-1483,D18-1429,0,0.0249933,"relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often 1.0), and hence 12 We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work. 4891 While iBLEU penalizes texts which match the source, here we reward such texts since our task values accuracy of generated text more than the need for paraphrasing the tabular content (Liu et al., 2010). Similar to SARI for text simplification (Xu et al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006"
P19-1483,D17-1238,0,0.267569,"proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) a"
P19-1483,W17-5525,0,0.311156,"proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) a"
P19-1483,P02-1040,0,0.106059,"7; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information which no system can be expected to produce given only the associated table. We call such reference texts divergent from the table. We show that existing automatic metrics, inc"
P19-1483,D14-1162,0,0.0825959,"Missing"
P19-1483,J18-3002,0,0.0906513,"PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours is the first st"
P19-1483,D18-1437,0,0.0608232,"curring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours is the first study which checks the quality of metrics when tableto-text references are divergent. We show that in this case even system level correlations can be unreliable. Hallucination (Rohrbach et al., 2018; Lee et al., 2018) refers to when an NLG system generates text which mentions extra information than what is present in the source from which it is generated. Divergence can be viewed as hallucination in the reference text itself. PARENT deals with hallucination by discounting n-grams which do not overlap with either the reference or the table. PARENT draws inspiration from iBLEU (Sun and Zhou, 2012), a metric for evaluating paraphrase generation, which compares the generated text to both the source text and the reference. Conclusions We study the automatic evaluation of table-to-text systems"
P19-1483,P12-2008,0,0.369925,"erences were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio. 5.3 Compared Metrics Text only: We compare BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014), CIDEr and CIDErD (Vedantam et al., 2015) using their publicly available implementations. Information Extraction based: We compare the CS, RG and RG-F metrics discussed in §4. Text & Table: We compare a variant of BLEU, denoted as BLEU-T, where the values from the table are used as additional references. BLEUT draws inspiration from iBLEU (Sun and Zhou, 2012) but instead rewards n-grams which match the table rather than penalizing them. For PARENT, we compare both the word-overlap model (PARENT-W) and the co-occurrence model (PARENT-C) for determining entailment. We also compare versions where a single λ is tuned on the entire dataset to maximize correlation with human judgments, denoted as PARENT*-W/C. WikiBio Systems WikiBio Hyperparams Avg ROUGE CIDEr CIDEr-D METEOR BLEU 0.518±0.07C,W 0.674±0.06C,W 0.646±0.06C,W 0.697±0.06C,W 0.548±0.07C,W -0.585±0.15C,W -0.516±0.15C,W -0.372±0.16C,W -0.079±0.24C,W 0.407±0.15C,W -0.034 0.079 0.137 0.309 0.478 C"
P19-1483,N18-1136,0,0.0306942,"Missing"
P19-1483,D17-1239,0,0.530722,"red data. We show that metrics which rely solely on the reference texts, such as BLEU and ROUGE, show poor correlation with human judgments when those references diverge. We propose a new metric, PARENT, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. Through a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et"
P19-1483,Q16-1029,0,0.0311333,"ally low, and hence the recall of the generated text relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often 1.0), and hence 12 We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work. 4891 While iBLEU penalizes texts which match the source, here we reward such texts since our task values accuracy of generated text more than the need for paraphrasing the tabular content (Liu et al., 2010). Similar to SARI for text simplification (Xu et al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Bu"
P19-1483,E17-1019,0,\N,Missing
P19-1483,P17-1099,0,\N,Missing
P19-1612,Q13-1005,0,0.0263304,"concepts that sparse representations can cleanly separate. These errors indicate that a hybrid approach would be promising future work. 10 Related Work Recent progress has been made towards improving evidence retrieval (Wang et al., 2018; Kratzwald and Feuerriegel, 2018; Lee et al., 2018; Das et al., 2019) by learning to aggregate from multiple retrieval steps. They re-rank evidence candidates from a closed set, and we aim to integrate these complementary approaches in future work. Our approach is also reminiscent of weakly supervised semantic parsing (Clarke et al., 2010; Liang et al., 2013; Artzi and Zettlemoyer, 2013; Fader et al., 2014; Berant et al., 2013; Kwiatkowski et al., 2013), with which we share similar challenges—(1) inference and learning are tightly coupled, (2) latent derivations must be discovered, and (3) strong inductive biases are needed to find positive learning signal while avoiding spurious ambiguities. While we motivate ICT from first principles as an unsupervised proxy for evidence retrieval, it is closely related to existing representation learning literature. ICT can be considered a generalization of the skip-gram objective (Mikolov et al., 2013), with a coarser granularity, deep a"
P19-1612,P17-1147,0,0.580471,"put. This presents a more realistic scenario for practical applications. Current approaches require a blackbox information retrieval (IR) system to do much of the heavy lifting, even though it cannot be fine-tuned on the downstream task. In the strongly supervised setting popularized by DrQA (Chen et al., 2017), they also assume a reading comprehension model trained on question-answer-evidence triples, such as SQuAD (Rajpurkar et al., 2016). The IR system is used at test time to generate evidence candidates in place of the gold evidence. In the weakly supervised setting, proposed by TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), and Quasar (Dhingra et al., 2017), the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence. These approaches rely on the IR system to massively reduce the search space and/or reduce spurious ambiguity. However, QA is fundamentally different from IR (Singh, 2012). Whereas IR is concerned with lexical and semantic matching, questions are by definition under-specified and require more language understanding, since users are explicitly looking for unknown information. Instead of being subject to the recall ceiling"
P19-1612,D18-1055,0,0.150956,"Missing"
P19-1612,D13-1161,0,0.0262235,"s indicate that a hybrid approach would be promising future work. 10 Related Work Recent progress has been made towards improving evidence retrieval (Wang et al., 2018; Kratzwald and Feuerriegel, 2018; Lee et al., 2018; Das et al., 2019) by learning to aggregate from multiple retrieval steps. They re-rank evidence candidates from a closed set, and we aim to integrate these complementary approaches in future work. Our approach is also reminiscent of weakly supervised semantic parsing (Clarke et al., 2010; Liang et al., 2013; Artzi and Zettlemoyer, 2013; Fader et al., 2014; Berant et al., 2013; Kwiatkowski et al., 2013), with which we share similar challenges—(1) inference and learning are tightly coupled, (2) latent derivations must be discovered, and (3) strong inductive biases are needed to find positive learning signal while avoiding spurious ambiguities. While we motivate ICT from first principles as an unsupervised proxy for evidence retrieval, it is closely related to existing representation learning literature. ICT can be considered a generalization of the skip-gram objective (Mikolov et al., 2013), with a coarser granularity, deep architecture, and in-batch negative sampling from Logeswaran and Lee"
P19-1612,D13-1160,0,0.941992,"rs are observed during training, and evidence retrieval is learned in a completely end-to-end manner. ing the marginal log-likelihood of correct answers that were found. We evaluate ORQA on open versions of five existing QA datasets. On datasets where the question writers already know the answer—SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017)—the retrieval problem resembles traditional IR, and BM25 (Robertson et al., 2009) provides state-of-the-art retrieval. On datasets where question writers do not know the answer— Natural Questions (Kwiatkowski et al., 2019), WebQuestions (Berant et al., 2013), and CuratedTrec (Baudis and Sediv´y, 2015)—we show that learned retrieval is crucial, providing improvements of 6 to 19 points in exact match over BM25. 2 During inference, the model outputs the answer string of the highest scoring derivation: a∗ = TEXT(argmax S(b, s, q)) Task In open domain question answering, the input q is a question string, and the output a is an answer string. Unlike reading comprehension, the source of evidence is a modeling choice rather than a part of the task definition. We compare the assumptions made by variants of reading comprehension and question answering task"
P19-1612,P17-1171,0,0.122006,"val is crucial, outperforming BM25 by up to 19 points in exact match. 1 Introduction Due to recent advances in reading comprehension systems, there has been a revival of interest in open domain question answering (QA), where the evidence must be retrieved from an open corpus, rather than being given as input. This presents a more realistic scenario for practical applications. Current approaches require a blackbox information retrieval (IR) system to do much of the heavy lifting, even though it cannot be fine-tuned on the downstream task. In the strongly supervised setting popularized by DrQA (Chen et al., 2017), they also assume a reading comprehension model trained on question-answer-evidence triples, such as SQuAD (Rajpurkar et al., 2016). The IR system is used at test time to generate evidence candidates in place of the gold evidence. In the weakly supervised setting, proposed by TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), and Quasar (Dhingra et al., 2017), the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence. These approaches rely on the IR system to massively reduce the search space and/or reduce spurious ambiguity. However"
P19-1612,W10-2903,1,0.667621,"recisely representing extremely specific concepts that sparse representations can cleanly separate. These errors indicate that a hybrid approach would be promising future work. 10 Related Work Recent progress has been made towards improving evidence retrieval (Wang et al., 2018; Kratzwald and Feuerriegel, 2018; Lee et al., 2018; Das et al., 2019) by learning to aggregate from multiple retrieval steps. They re-rank evidence candidates from a closed set, and we aim to integrate these complementary approaches in future work. Our approach is also reminiscent of weakly supervised semantic parsing (Clarke et al., 2010; Liang et al., 2013; Artzi and Zettlemoyer, 2013; Fader et al., 2014; Berant et al., 2013; Kwiatkowski et al., 2013), with which we share similar challenges—(1) inference and learning are tightly coupled, (2) latent derivations must be discovered, and (3) strong inductive biases are needed to find positive learning signal while avoiding spurious ambiguities. While we motivate ICT from first principles as an unsupervised proxy for evidence retrieval, it is closely related to existing representation learning literature. ICT can be considered a generalization of the skip-gram objective (Mikolov"
P19-1612,D18-1053,0,0.517125,"by definition under-specified and require more language understanding, since users are explicitly looking for unknown information. Instead of being subject to the recall ceiling from blackbox IR systems, we should directly learn to retrieve using question-answering data. In this work, we introduce the first OpenRetrieval Question Answering system (ORQA). ORQA learns to retrieve evidence from an open corpus, and is supervised only by questionanswer string pairs. While recent work on improving evidence retrieval has made significant progress (Wang et al., 2018; Kratzwald and Feuerriegel, 2018; Lee et al., 2018; Das et al., 2019), they still only rerank a closed evidence set. The main challenge to fully end-to-end learning is that retrieval over the open corpus must be considered a latent variable that would be impractical to train from scratch. IR systems offer a reasonable but potentially suboptimal starting point. The key insight of this work is that end-toend learning is possible if we pre-train the retriever with an unsupervised Inverse Cloze Task (ICT). In ICT, a sentence is treated as a pseudoquestion, and its context is treated as pseudoevidence. Given a pseudo-question, ICT requires selecti"
P19-1612,J13-2005,0,0.0275901,"extremely specific concepts that sparse representations can cleanly separate. These errors indicate that a hybrid approach would be promising future work. 10 Related Work Recent progress has been made towards improving evidence retrieval (Wang et al., 2018; Kratzwald and Feuerriegel, 2018; Lee et al., 2018; Das et al., 2019) by learning to aggregate from multiple retrieval steps. They re-rank evidence candidates from a closed set, and we aim to integrate these complementary approaches in future work. Our approach is also reminiscent of weakly supervised semantic parsing (Clarke et al., 2010; Liang et al., 2013; Artzi and Zettlemoyer, 2013; Fader et al., 2014; Berant et al., 2013; Kwiatkowski et al., 2013), with which we share similar challenges—(1) inference and learning are tightly coupled, (2) latent derivations must be discovered, and (3) strong inductive biases are needed to find positive learning signal while avoiding spurious ambiguities. While we motivate ICT from first principles as an unsupervised proxy for evidence retrieval, it is closely related to existing representation learning literature. ICT can be considered a generalization of the skip-gram objective (Mikolov et al., 2013), with"
P19-1612,D16-1261,0,0.0229849,"be discovered, and (3) strong inductive biases are needed to find positive learning signal while avoiding spurious ambiguities. While we motivate ICT from first principles as an unsupervised proxy for evidence retrieval, it is closely related to existing representation learning literature. ICT can be considered a generalization of the skip-gram objective (Mikolov et al., 2013), with a coarser granularity, deep architecture, and in-batch negative sampling from Logeswaran and Lee (2018). Consulting external evidence sources with latent retrieval has also been explored in information extraction (Narasimhan et al., 2016). In comparison, we are able to learn a much more expressive retriever due to the strong inductive biases from ICT pre-training. 11 Conclusion We presented ORQA, the first open domain question answering system where the retriever and reader are jointly learned end-to-end using only question-answer pairs and without any IR system. This is made possible by pre-training the retriever using an Inverse Cloze Task (ICT). Experiments show that learning to retrieve is crucial when the questions reflect an information need, i.e. the question writers do not already know the answer. Acknowledgements We t"
P19-1612,N18-1202,1,0.417885,"ementation is based on Lucene.3 Language Models While unsupervised neural retrieval is notoriously difficult to improve over traditional IR (Lin, 2019), we include them as baselines for comparison. We experiment with unsupervised pooled representations from neural language models (LM), which has been shown to be state-of-the-art unsupervised representations (Perone et al., 2018). We compare with two widely-used 128-dimensional representations: (1) NNLM, context-independent embeddings from a feed-forward LMs (Bengio et al., 2003),4 and (2) ELM O (small), a context-dependent bidirectional LSTM (Peters et al., 2018).5 As with ICT, we use the alternate encoders to pre-compute the encoded evidence blocks hb and to initialize the question encoding hq , which is fine-tuned. Based on existing IR literature and the intuition that LMs do not explicitly optimize for retrieval, we do not expect these to be strong baselines, but they demonstrate the difficulty of encoding blocks of text into 128 dimensions. 8.2 Results The main results are show in Table 5. The first result to note is that BM25 is a powerful retrieval system. Word matching is important, and 3 https://lucene.apache.org/ https://tfhub.dev/google/nnlm"
P19-1612,D16-1264,0,0.80349,"nsion systems, there has been a revival of interest in open domain question answering (QA), where the evidence must be retrieved from an open corpus, rather than being given as input. This presents a more realistic scenario for practical applications. Current approaches require a blackbox information retrieval (IR) system to do much of the heavy lifting, even though it cannot be fine-tuned on the downstream task. In the strongly supervised setting popularized by DrQA (Chen et al., 2017), they also assume a reading comprehension model trained on question-answer-evidence triples, such as SQuAD (Rajpurkar et al., 2016). The IR system is used at test time to generate evidence candidates in place of the gold evidence. In the weakly supervised setting, proposed by TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), and Quasar (Dhingra et al., 2017), the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence. These approaches rely on the IR system to massively reduce the search space and/or reduce spurious ambiguity. However, QA is fundamentally different from IR (Singh, 2012). Whereas IR is concerned with lexical and semantic matching, questions are by"
P19-1612,D12-1116,0,0.0330258,"n model trained on question-answer-evidence triples, such as SQuAD (Rajpurkar et al., 2016). The IR system is used at test time to generate evidence candidates in place of the gold evidence. In the weakly supervised setting, proposed by TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), and Quasar (Dhingra et al., 2017), the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence. These approaches rely on the IR system to massively reduce the search space and/or reduce spurious ambiguity. However, QA is fundamentally different from IR (Singh, 2012). Whereas IR is concerned with lexical and semantic matching, questions are by definition under-specified and require more language understanding, since users are explicitly looking for unknown information. Instead of being subject to the recall ceiling from blackbox IR systems, we should directly learn to retrieve using question-answering data. In this work, we introduce the first OpenRetrieval Question Answering system (ORQA). ORQA learns to retrieve evidence from an open corpus, and is supervised only by questionanswer string pairs. While recent work on improving evidence retrieval has made"
P19-1612,N19-4013,0,0.226803,"Missing"
P19-1612,P11-1060,0,\N,Missing
P19-1612,Q19-1026,1,\N,Missing
Q13-1017,W02-1001,0,0.938558,"ed on four benchmark NLP datasets for part-ofspeech tagging, named-entity recognition and dependency parsing. 1 Introduction Complex natural language processing tasks are inherently structured. From sequence labeling problems like part-of-speech tagging and named entity recognition to tree construction tasks like syntactic parsing, strong dependencies exist among the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the struct"
Q13-1017,D07-1015,0,0.0488286,"Missing"
Q13-1017,P09-1058,0,0.0222853,"echnique has been shown to improve the generalization ability of the model. MIRA The Margin Infused Relaxed Algorithm (MIRA), which was introduced by Crammer et al. (2005), explicitly uses the notion of margin to update the weight vector. The MIRA updates the weight vector by calculating the step size using 1 min kw − w0 k2 w 2 T S . T. w Φyi ,y (xi ) ≥ ∆(y, yi ), ∀y ∈ Hk , where Hk is a set containing the best-k structures according to the weight vector w0 . MIRA is a very popular method in the NLP community and has been applied to NLP tasks like word segmentation and part-of-speech tagging (Kruengkrai et al., 2009), NER and chunking (Mejer and Crammer, 2010) and dependency parsing (McDonald et al., 2005). 2.2 Structural SVM Structural SVM (SSVM) is a maximum margin model for the structured output prediction setting. Training SSVM is equivalent to solving the following global optimization problem: l arg max wT Φ(xi , y). (1) y∈Y(xi ) 208 min w X kwk2 +C L(xi , yi , w), 2 i=1 (2) where l is the number of labeled examples and L1-Loss SSVM (Ratliff et al., 2007). Taskar et   al. (2004a) proposed a structured SMO algorithm.   L(xi , yi , w) = ` max ∆(yi , y) − wT Φyx i,y (xi ) Because the algorithm solve"
Q13-1017,J93-2004,0,0.0478868,"f entities. We followed the settings in (Ratinov and Roth, 2009) and consider three main entities categories: PER, LOC and ORG. We evaluated the results using phrase-level F1 . retic analysis significantly. Also note that Theorem 2 shows that if we put all possible structures in the working sets (i.e., F (α) = FS (α)), then the DCD algorithms can obtain -optimal solution in O(log( 1 )) iterations. 213 POS-WSJ The standard set for evaluating the performance of a part-of-speech tagger. The training, development and test sets consist of sections 0-18, 19-21 and 22-24 of the Penn Treebank data (Marcus et al., 1993), respectively. We evaluated the results by token-level accuracy. DP-WSJ We took sections 02-21 of Penn Treebank as the training set, section 00 as the development set and section 23 as the test set. We implement a simple version of hash kernel to speed up of training procedure for this task (Bohnet, 2010). We reported the unlabeled attachment accuracy for this task (McDonald et al., 2005). 80 86 79 84 78 Test Acc Test F1 Test F1 97 82 96 DCD-SSVM FW-Struct SVM-Struct 77 80 76 0 10 20 30 0 20 Training Time (seconds) 40 60 80 100 120 Training Time (seconds) 95 0 200 400 600 Training Time (secon"
Q13-1017,P05-1012,0,0.615635,"hmark NLP datasets for part-ofspeech tagging, named-entity recognition and dependency parsing. 1 Introduction Complex natural language processing tasks are inherently structured. From sequence labeling problems like part-of-speech tagging and named entity recognition to tree construction tasks like syntactic parsing, strong dependencies exist among the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron (Collins"
Q13-1017,D10-1095,0,0.344884,"alization ability of the model. MIRA The Margin Infused Relaxed Algorithm (MIRA), which was introduced by Crammer et al. (2005), explicitly uses the notion of margin to update the weight vector. The MIRA updates the weight vector by calculating the step size using 1 min kw − w0 k2 w 2 T S . T. w Φyi ,y (xi ) ≥ ∆(y, yi ), ∀y ∈ Hk , where Hk is a set containing the best-k structures according to the weight vector w0 . MIRA is a very popular method in the NLP community and has been applied to NLP tasks like word segmentation and part-of-speech tagging (Kruengkrai et al., 2009), NER and chunking (Mejer and Crammer, 2010) and dependency parsing (McDonald et al., 2005). 2.2 Structural SVM Structural SVM (SSVM) is a maximum margin model for the structured output prediction setting. Training SSVM is equivalent to solving the following global optimization problem: l arg max wT Φ(xi , y). (1) y∈Y(xi ) 208 min w X kwk2 +C L(xi , yi , w), 2 i=1 (2) where l is the number of labeled examples and L1-Loss SSVM (Ratliff et al., 2007). Taskar et   al. (2004a) proposed a structured SMO algorithm.   L(xi , yi , w) = ` max ∆(yi , y) − wT Φyx i,y (xi ) Because the algorithm solves the dual formulation y of the L1-Loss SSVM"
Q13-1017,W09-1119,0,0.420529,"2003 shared task (T. K. Sang and De Meulder, 2003). The data set labels sentences from the Reuters Corpus, Volume 1 (Lewis et al., 2004) with four different entity types: PER, LOC, ORG and MISC. We evaluated the results using phrase-level F1 . 4.1 Tasks and Data We evaluated our method and existing structured output learning approaches on named entity recognition (NER), part-of-speech tagging (POS) and dependency parsing (DP) on four benchmark datasets. NER-MUC7 MUC-7 data contains a subset of North American News Text Corpora annotated with many types of entities. We followed the settings in (Ratinov and Roth, 2009) and consider three main entities categories: PER, LOC and ORG. We evaluated the results using phrase-level F1 . retic analysis significantly. Also note that Theorem 2 shows that if we put all possible structures in the working sets (i.e., F (α) = FS (α)), then the DCD algorithms can obtain -optimal solution in O(log( 1 )) iterations. 213 POS-WSJ The standard set for evaluating the performance of a part-of-speech tagger. The training, development and test sets consist of sections 0-18, 19-21 and 22-24 of the Penn Treebank data (Marcus et al., 1993), respectively. We evaluated the results by"
Q13-1017,W03-0419,0,0.23167,"Missing"
Q13-1017,W04-3201,0,0.179695,"ong the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron (Collins, 2002). This may be due to the fact that current SSVM implementations often suffer from several practical issues. First, state-of-the-art implementations of SSVM such as cutting plane methods (Joachims et al., 2009) are typically complicated.1 Second, while methods like stochastic gradient descent are simple to implement, tuning learning rat"
Q13-1017,P10-1040,0,0.0111844,"= 1]  X    [yi = 1][yi−1 = 2]  , Φ(x, y) =    i=l  ... [yi = k][yi−1 = k] where Φemi is the feature vector dedicated to the i-th token (or, the emission features), N represents the number of tokens in this sequence, yi represents the i-th token in the sequence y, [yi = 1] is the indictor variable and k is the number of tags. The inference problems are solved by the Viterbi algorithm. The emission features used in both POS and NER are the standard ones, including word features, word-shape features, etc. For NER, we used additional simple gazetteer features7 and word cluster features (Turian et al., 2010) For dependency parsing, we followed the setting described in (McDonald et al., 2005) and used similar features. The decoding algorithm is the firstorder Eisner’s algorithm (Eisner, 1997). 4.3 Algorithms and Implementation Detail algorithms. For NER-MUC7, NER-CoNLL and POS-WSJ, we ran the online algorithms and DCDSSVM for 25 iterations. For DP-WSJ, we only let the algorithms run for 10 iterations as the inference procedure is very expensive computationally. The algorithms in the experiments are: DCD Our dual coordinate descent method on the L2-Loss SSVM. For DCD-SSVM, r is set to be 5. For bot"
Q13-1017,C96-1058,0,\N,Missing
Q13-1017,P06-1065,1,\N,Missing
Q14-1021,P11-1040,0,0.0140078,"rns in social media. While we believe that entity linking is the first step towards intelligence gathering, many existing studies filter or cluster tweets based on merely keywords. On the one hand, manual selection of keywords (Sakaki et al., 2010; Tumasjan et al., 2010) requires significant labor, and thus is not scalable to the vast number of entities. On the other hand, automatic approaches (Li et al., 2013) only identify coarse-grained topics (e.g., crime or sports), falling short of recognizing specific entities. Lastly, there is a line of research on record extraction from social media (Benson et al., 2011; Ritter et al., 2012). Although the problem is different from entity linking, they present an interesting insight into social media. They observe that the same record is often referenced by multiple messages, and exploit this redundancy to help with extraction. The redundant nature of social media can be potentially leveraged to improve entity linking as well. 3 base type information. In total, there are 2.7 million entities in our database. A lexicon is a dictionary that maps a candidate anchor a (i.e., a surface form) to its possible entity set E(a). Similar to existing studies (Cucerzan, 2"
Q14-1021,E06-1002,0,0.112677,"for a given entity (e.g., a product or an organization), a straightforward approach is to formulate a keyword query. However, simple keyword matching is largely ineffective, since keywords are often ambiguous. For instance, “spurs” can refer to two distinct sports teams (San Antonio Spurs, which is a basketball team in the US, and Tottenham Hotspur F.C., which is a soccer team in the UK), besides being a non-entity verb or noun. Thus, retrieving tweets via keyword queries inevitably mixes different entities. Given the ambiguity of keywords, in this paper, we study the task of entity linking (Bunescu and Pasca, 2006) on microblogs. As input, we are given a short message (e.g., a tweet) and an entity database (e.g., Wikipedia where each article is an entity). As output, we map every surface form (e.g., “spurs”) in the message to an entity (e.g., San Antonio Spurs) or to ∅ (i.e., a non-entity). This task is particularly challenging for microblogs due to their short, noisy and colloquial nature. Fortunately, we also observe two new opportunities, which are often missing in traditional data. First, microblogs usually embed rich meta-data, most notably spatiotemporal (i.e., spatial and temporal) signals. Speci"
Q14-1021,W02-1001,0,0.05298,"rmation from disambiguation pages, redirect pages and anchor texts in Wikipedia3 . To handle the case where a is not an entity, we add a special symbol ∅ to every E(a). We have 6 million anchors in total. Given a tweet, the system generates a set of candidate anchors based on the lexicon. Specifically, each tweet is tokenized into individual words, keeping each punctuation as a separate token. To identify if any sequence of tokens matches an anchor in the lexicon, we use exact matching, but allow for case-insensitivity. Furthermore, we employ an NER system trained using structural perceptron (Collins, 2002) to filter the anchors of type person, loc or org, such that only the anchors recognized by the NER are retained. 3.2 Incorporating Spatiotemporal Signals Consider a tweet m with timestamp t and location l. Given anchor a in tweet m, our system links a to the candidate entity e∗ as follows: e∗ = arg maxe∈E(a) P (e|m, a, t, l) = arg maxe∈E(a) P (e, m, a, t, l) Spatiotemporal Entity Linking Our spatiotemporal framework for entity linking requires an input tweet m, as well as its associated timestamp t and location l. For each tweet m, the goal is to predict an output set {e1 , e2 , . . .} of ent"
Q14-1021,D07-1074,0,0.685885,"al signals for entity linking. • We demonstrate the effectiveness of our framework through extensive quantitative experiments. In particular, we improve F1 by more than 10 points over the existing state of the art. • We point out that entity linking should be evaluated for both information extraction and retrieval needs. In the former, we evaluate the extraction of entities from tweets, while in the latter we evaluate the retrieval of tweets for a query entity. A qualitative study is also presented for the latter. 260 2 Related Work Earlier research on entity linking (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008) has been focused on well-written documents such as news and encyclopedia articles. The TAC KBP track (Ji et al., 2010; Ji and Grishman, 2011) also includes an entity linking task with a slightly different setting—to link a given mention based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coheren"
Q14-1021,D10-1124,0,0.108549,"Missing"
Q14-1021,N13-1122,1,0.617127,"ncludes an entity linking task with a slightly different setting—to link a given mention based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coherence. These techniques have also been adapted and tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011; Li et al., 2012a). However, given the short and noisy nature of microblogs, these approaches, which largely depend on textual features, often result in unsatisfactory performance. Fortunately, additional non-textual meta-data in microblogs can often help. A recent study (Shen et al., 2013) improves entity linking by utilizing user account information, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore"
Q14-1021,D11-1072,0,0.180853,"Missing"
Q14-1021,P11-1115,0,0.0148193,"more than 10 points over the existing state of the art. • We point out that entity linking should be evaluated for both information extraction and retrieval needs. In the former, we evaluate the extraction of entities from tweets, while in the latter we evaluate the retrieval of tweets for a query entity. A qualitative study is also presented for the latter. 260 2 Related Work Earlier research on entity linking (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008) has been focused on well-written documents such as news and encyclopedia articles. The TAC KBP track (Ji et al., 2010; Ji and Grishman, 2011) also includes an entity linking task with a slightly different setting—to link a given mention based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coherence. These techniques have also been adapted and tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al.,"
Q14-1021,P11-1138,0,0.0748186,"ach algorithm. Two-tail paired t-test is then applied to determine if the F1 scores of two algorithms over the 10 splits are significantly different. 6.3 Results and Discussion We present the empirical findings for the following research questions. Q1: Q2: Q3: Q4: How do our base systems perform? Are spatiotemporal signals indeed useful? Does the graph-based smoothing help? What causes the errors? How to recover them? Base system comparison (Q1). To show that our base systems, in particular E2E, already outperform other systems, we compare with Wikiminer (Milne and Witten, 2008) and Illinois (Ratinov et al., 2011) systems.8 As existing systems are more geared for the IE scenario, we report in Table 2 the IE-drive F1 on the test set. 8 We use the authors’ implementations. AIDA (Hoffart et al., 2011) is not compared, as it mostly links to person, org and loc only. TAGME (Ferragina and Scaiella, 2010) and Cucerzan (Cucerzan, 2007) were already compared in an earlier paper (Guo et al., 2013) which E2E is largely based on. To be fair, we discard non-core entities linked by Wikiminer or Illinois. ? Winter solstice ´ Les Miserables (2012 film) Django unchained (2012 film) Table 1: Query entities for IR-driven"
Q14-1021,D11-1141,0,0.206777,"Missing"
Q14-1021,P11-1096,0,0.0946895,"uition that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore the spatiotemporal signals associated with tweets. Although the spatiotemporal aspect of social media has been studied in many papers, including temporal cycle tracking (Leskovec et al., 2009), spatial object matching (Dalvi et al., 2012), mining emerging topics (Mei et al., 2006; Cataldi et al., 2010; Yin et al., 2011), event monitoring (Sakaki et al., 2010; Xu et al., 2012), and identifying geographical linguistic variations (Eisenstein et al., 2010; Wing and Baldridge, 2011), none of them addresses the problem of entity linking. In this paper, we propose a novel spatiotemporal framework for entity linking, building upon some of the previously observed patterns in social media. While we believe that entity linking is the first step towards intelligence gathering, many existing studies filter or cluster tweets based on merely keywords. On the one hand, manual selection of keywords (Sakaki et al., 2010; Tumasjan et al., 2010) requires significant labor, and thus is not scalable to the vast number of entities. On the other hand, automatic approaches (Li et al., 2013)"
Q19-1026,D15-1075,0,0.0508562,"ent set also appear in the training set, we implement two ‘‘copying’’ baselines. The first of these simply selects the most frequent annotation applied to a given page in the training set. The second selects the annotation given to the training set question closest to the eval set question according to TFIDF weighted word overlap. These three baselines are reported as First paragraph, Most frequent, and Closest question in Table 3, respectively. 6.3 Custom Pipeline (DecAtt + DocReader) One view of the long answer selection task is that it is more closely related to natural language inference (Bowman et al., 2015; Williams et al., 2018) than short answer extraction. A valid long answer must contain all of the information required to infer the answer. Short answers do not need to contain this information—they need to be surrounded by it. Motivated by this intuition, we implement a pipelined approach that uses a model drawn from the natural language interference literature to select long answers. Then short answers are selected from these using a model drawn from the short answer extraction literature. 6.2 Document-QA We adapt the reference implementation12 of Document-QA (Clark and Gardner, 2018) for t"
Q19-1026,P17-1171,0,0.275889,"Missing"
Q19-1026,D18-1241,0,0.0611339,"choice of supporting facts is somewhat subjective. They set high human upper bounds by selecting, for each example, the score maximizing partition of four annotations into one prediction and three references. The reference labels chosen by this maximization are not representative of the reference labels in HotpotQA’s evaluation set, and it is not clear that the upper bounds are achievable. A more robust approach is to keep the evaluation distribution fixed, and calculate an acheivable upper bound by approximating the expectation over annotations—as we have done for NQ in Section 5. The QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018) data sets contain dialogues between a questioner, who is trying to learn about a text, and an answerer. QuAC also prevents the questioner from seeing the evidence text. Conversational QA is an exciting new area, but it is significantly different from the single turn QA task in NQ. In both QuAC and CoQA, conversations tend to explore evidence texts incrementally, progressing from the start to the end of the text. 3 Task Definition and Data Collection Natural Questions contains (question, wikipedia page, long answer, short answer) quadruples where: the question see"
Q19-1026,P18-1078,0,0.217148,"(P), recall (R), and the harmonic mean of these (F1) of all baselines, a single annotator, and the super-annotator upper bound. The human performances marked with † are evaluated on a sample of five annotations from the 25-way annotated data introduced in Section 5. To address (ii), we tried adding special NULL passages to represent the lack of answer. However, we achieved better performance by training on the subset of questions with answers and then only predicting those answers whose scores exceed a threshold. With these two modifications, we are able to apply Document-QA to NQ. We follow Clark and Gardner (2018) in pruning documents down to the set of passages that have highest TFIDF similarity with the question. Under this approach, we consider the top 16 passages as long answers. We consider short answers containing up to 17 words. We train Document-QA for 30 epochs with batches containing 15 examples. The post hoc score threshold is set to 3.0. All of these values were chosen on the basis of development set performance. 6.1 Untrained Baselines NQ’s long answer selection task admits several untrained baselines. The first paragraph of a Wikipedia page commonly acts as a summary of the most important"
Q19-1026,D17-1082,0,0.109935,"annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should n"
Q19-1026,W18-2605,0,0.0359237,"ions. MS Marco contains 100,000 questions with freeform answers. For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages. Free-form text answers allow more flexibility in providing abstractive answers, but lead to difficulties in evaluation (BLEU score [Papineni et al., 2002] is used). MS Marco’s authors do not discuss issues of variability or report quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and ans"
Q19-1026,D18-1260,0,0.0463566,"variability or report quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the ins"
Q19-1026,C92-2082,0,0.0914034,"elines and tooling divide the annotation task into three conceptual stages, where all three stages are completed by a single annotator in succession. The decision flow through these is illustrated in Figure 2 and the instructions given to annotators are summarized below. Question Identification: Contributors determine whether the given question is good or bad. A good question is a fact-seeking question that can be answered with an entity or explanation. A bad question is ambigous, incomprehensible, 3 We pre-define the set of categorical noun phrases used in 4 and 5 by running Hearst patterns (Hearst, 1992) to find a broad set of hypernyms. Part of speech tags and entities are identified using Google’s Cloud NLP API: https://cloud. google.com/natural-language. 456 2) k -way annotations (with k = 25) on a subset of the data. Post hoc evaluation of non-null answers leads directly to a measure of annotation precision. As is common in information-retrieval style problems such as long-answer identification, measuring recall is more challenging. However, we describe how 25-way annotated data provide useful insights into recall, particularly when combined with expert judgments. dependent on clear false"
Q19-1026,D16-1241,0,0.0190295,"nts to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should not change the answer, and SQuAD 2.0 introduces questions that are designed to be unanswerable. However, we argue that questions written to be unanswerable can be identified as such with little reasoning, in contrast to NQ’s task of deciding whether a pa"
Q19-1026,P16-1144,0,0.031038,"to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should not change the answer, and SQuAD 2.0 introduces questions that are designed to be unanswerable. However, we argue that questions written to be unanswerable can be identified as such with little reasoning, in contrast to NQ’s task of"
Q19-1026,D17-1215,0,0.0458559,"questions from the query stream. Thus the questions are ‘‘natural’’ in that they represent real queries from people seeking information. 2 Related Work The SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Kocisky et al., 2018), and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer. The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as test data. We justify the use of 5-way annotation for evaluation in Section 5. 454 This contrasts with NQ, where individual questions often require reasoning over large bodies of text. The WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine. WikiQA contains only 3,047 questions. MS Marco contains 100,000 questions with free"
Q19-1026,P17-1147,0,0.26169,"ssues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should not change the answer, and SQuAD 2.0 introduc"
Q19-1026,P02-1040,0,0.108627,"asoning over large bodies of text. The WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine. WikiQA contains only 3,047 questions. MS Marco contains 100,000 questions with freeform answers. For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages. Free-form text answers allow more flexibility in providing abstractive answers, but lead to difficulties in evaluation (BLEU score [Papineni et al., 2002] is used). MS Marco’s authors do not discuss issues of variability or report quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading co"
Q19-1026,N18-1101,0,0.0325746,"n the training set, we implement two ‘‘copying’’ baselines. The first of these simply selects the most frequent annotation applied to a given page in the training set. The second selects the annotation given to the training set question closest to the eval set question according to TFIDF weighted word overlap. These three baselines are reported as First paragraph, Most frequent, and Closest question in Table 3, respectively. 6.3 Custom Pipeline (DecAtt + DocReader) One view of the long answer selection task is that it is more closely related to natural language inference (Bowman et al., 2015; Williams et al., 2018) than short answer extraction. A valid long answer must contain all of the information required to infer the answer. Short answers do not need to contain this information—they need to be surrounded by it. Motivated by this intuition, we implement a pipelined approach that uses a model drawn from the natural language interference literature to select long answers. Then short answers are selected from these using a model drawn from the short answer extraction literature. 6.2 Document-QA We adapt the reference implementation12 of Document-QA (Clark and Gardner, 2018) for the NQ task. This system"
Q19-1026,P18-2124,0,0.322524,"answer (s) can be a span or set of spans (typically entities) within l that answer the question, a boolean yes or no answer, or NULL. If l = NULL then s = NULL, necessarily. Figure 1 shows examples. Natural Questions has the following properties: Source of questions The questions consist of real anonymized, aggregated queries issued to the Google search engine. Simple heuristics are used to filter questions from the query stream. Thus the questions are ‘‘natural’’ in that they represent real queries from people seeking information. 2 Related Work The SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Kocisky et al., 2018), and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer. The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as"
Q19-1026,D16-1264,0,0.822388,"chine translation, speech recognition, and image recognition. One major factor in these successes has been the development of neural methods that far exceed the performance of previous approaches. A second major factor has been the existence of large quantities of training data for these systems. Open-domain question answering (QA) is a benchmark task in natural language understanding (NLU), which has significant utility to users, and in addition is potentially a challenge task that can drive the development of methods for NLU. Several pieces of recent work have introduced QA data sets (e.g., Rajpurkar et al., 2016; Reddy et al., 2018). However, in contrast to tasks where it is relatively easy to gather naturally occurring examples,1 the definition of a suitable QA task, and the development of a methodology for annotation and evaluation, is challenging. Key issues include the methods and sources used to obtain questions; the methods used to annotate and collect answers; the methods used to measure and ensure annotation quality; and the metrics used for evaluation. For more discussion of the limitations of previous work with respect to these issues, see Section 2 of this paper. This paper introduces Natu"
Q19-1026,D15-1237,0,0.10531,"D data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as test data. We justify the use of 5-way annotation for evaluation in Section 5. 454 This contrasts with NQ, where individual questions often require reasoning over large bodies of text. The WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine. WikiQA contains only 3,047 questions. MS Marco contains 100,000 questions with freeform answers. For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages. Free-form text answers allow more flexibility in providing abstractive answers, but lead to difficulties in evaluation (BLEU score [Papineni et al., 2002] is used). MS Marco’s authors do not discus"
Q19-1026,D18-1259,0,0.167869,"answer the question, a boolean yes or no answer, or NULL. If l = NULL then s = NULL, necessarily. Figure 1 shows examples. Natural Questions has the following properties: Source of questions The questions consist of real anonymized, aggregated queries issued to the Google search engine. Simple heuristics are used to filter questions from the query stream. Thus the questions are ‘‘natural’’ in that they represent real queries from people seeking information. 2 Related Work The SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Kocisky et al., 2018), and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer. The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as test data. We justify the use of 5-way annotation for evaluation in Se"
Q19-1026,D13-1020,0,0.136104,"quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sent"
Q19-1026,D16-1244,1,\N,Missing
Q19-1026,Q18-1023,0,\N,Missing
W06-2926,P05-1013,0,0.0843985,"sentences. This is a multiclass classification task. The number of the dependency types for each language can be found in the organizer’s introduction paper of the shared task of CoNLL-X. In the phase of learning dependency types, the parent of the tokens, which was labeled in the first phase, will be used as features. The predicted actions can help us to make accurate predictions for dependency types. 1.3 Dealing with Crossing Edges The algorithm described in previous section is primarily designed for projective languages. To deal with non-projective languages, we use a similar approach of (Nivre and Nilsson, 2005) to map nonprojective trees to projective trees. Any single rooted projective dependency tree can be mapped into a projective tree by the Lift operation. The definition of Lift is as follows: Lift(wj → wk ) = parent(wj ) → wk , where a → b means that a is the parent of b, and parent is a function which returns the parent word of the given word. The procedure is as follows. First, the mapping algorithm examines if there is a crossing edge in the current tree. If there is a crossing edge, it will perform Lift and replace the edge until the tree becomes projective. 2 Local Search The advantage of"
W06-2926,W03-3023,0,0.712134,"ad local search that serves to make the local predictions more robust. As shown, the performance of the first generation of this algorithm is promising. 1 System Description 1.1 Parsing as a Pipeline Pipeline computation is a common computational strategy in natural language processing, where a task is decomposed into several stages that are solved sequentially. For example, a semantic role labeling program may start by using a part-of-speech tagger, than apply a shallow parser to chunk the sentence into phrases, and continue by identifying predicates and arguments and then classifying them. (Yamada and Matsumoto, 2003) proposed a bottom-up dependency parsing algorithm, where the local actions, chosen from among Shift, Left, Right, are used to generate a dependency tree using a shift-reduce parsing approach. Moreover, they used SVMs to learn the parsing decisions between pairs of consecutive words in the sentences 1 . This is a true pipeline approach in that the classifiers are trained on individual decisions rather than on the overall quality of the parser, and chained to yield the 1 A pair of words may become consecutive after the words between them become the children of these two words global structure."
W06-2926,dzeroski-etal-2006-towards,0,\N,Missing
W06-2926,W03-2405,0,\N,Missing
W06-2926,afonso-etal-2002-floresta,0,\N,Missing
W10-2903,P09-1010,0,0.283359,"and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation. These works operate in settings different to ours as they rely on an external world state that is directly referenced by the input text. Although our framework can also be applied in these settings we do not assume that the text can be grounded in a world state. In our experiments the input text consists of generalized statements which describe some information need that does not correspond directly to a Acknowledgements We are grateful to Rohit Kate and Raymond Mooney for their help with the Geoquery"
W10-2903,N10-1066,1,0.218067,"aining data and at a faster rate than D IRECT. Note that the performance of the D I RECT approach drops at the first iteration. We hypothesize this is due to imbalances in the binary feedback dataset (too many negative examples) in the first iteration. 7 It is relatively difficult to compare different approaches in the Geoquery domain given that many existing papers do not use the same data split. 25 grounded world state. Our learning framework closely follows recent work on learning from indirect supervision. The direct approach resembles learning a binary classifier over a latent structure (Chang et al., 2010a); while the aggressive approach has similarities with work that uses labeled structures and a binary signal indicating the existence of good structures to improve structured prediction (Chang et al., 2010b). 90 Accuracy on Response 250 80 70 60 50 40 30 Initialization 20 Direct Approach 10 0 0 Aggressive Approach 1 2 3 4 5 6 7 Learning Iterations 8 Conclusions Figure 2: Accuracy on training set as number of learning In this paper we tackle one of the key bottlenecks in semantic parsing — providing sufficient supervision to train a semantic parser. Our solution is two fold, first we present a"
W10-2903,N06-1056,0,0.865919,"gical symbol candidates per word (on average 13 logical symbols per word). 1. Is it possible to learn a semantic parser without annotated logical forms? 24 Algorithm N O L EARN D IRECT AGGRESSIVE S UPERVISED R250 22.2 75.2 82.4 87.6 Q250 — 69.2 73.2 80.4 To answer the second question, we compare a supervised version of our model to existing semantic parsers. The results are in Table 2. Although the numbers are not directly comparable due to different splits in the data7 , we can see that with a similar number of logical forms for training our S UPERVISED approach outperforms existing systems (Wong and Mooney, 2006; Wong and Mooney, 2007), while the AGGRESSIVE approach remains competitive without using any logical forms. Our S UPERVISED model is still very competitive with other approaches (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007), which used considerably more annotated logical forms in the training phase. Table 1: Accuracy of learned models on R250 data and Q250 (testing) data. N O L EARN: using initialized weight vector, D IRECT : using feedback with the direct approach, AGGRESSIVE : using feedback with the aggressive approach, S UPERVISED: using gold 250 logical forms for training. Note"
W10-2903,P07-1121,0,0.92159,"the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limited to syntactic patterns identified in the training data, the learning algorithm requires considerable amoun"
W10-2903,W05-0602,0,0.64828,"reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limited to syntactic patterns identified in the training d"
W10-2903,P06-1115,0,0.848912,"can outperform fully supervised systems. iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an"
W10-2903,D07-1071,0,0.658715,"ntic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limited to syntactic patterns identified in the training data, the learning algorithm req"
W10-2903,W08-2105,0,0.095047,"aning Representation Sentence 2: “What is Texas’ capital?” Following previous work, we capture the semantics of the Geoquery domain using a subset of first-order logic consisting of typed constants and functions. There are two types: entities E in the domain and numeric values N . Functions describe a functional relationship over types (e.g., population : E → N ). A complete logical form is constructed through functional composition; in our formalism this is perThe ability to adapt to unseen inputs is one of the key challenges in semantic parsing. Several works (Zettlemoyer and Collins, 2007; Kate, 2008) have addressed this issue explicitly by manually defining syntactic transformation rules that can help the learned parser generalize better. Unfortunately these are only partial solutions as a 3 Mistake driven algorithms that do not enforce margin constraints may not be able to generalize using this protocol since they will repeat the same prediction at training time and therefore will not update the model. 4 This is true for all meaning representations designed to be executed by a computer system. 22 stituent c). formed by the substitution operator. For example, given the function next to(x)"
W10-2903,P09-1110,0,0.436956,"ng to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation. These works operat"
W10-2903,P09-1011,0,0.131626,"s. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation. These works operate in settings different to ours as they rely on an external world state that is directly referenced by the input text. Although our framework can also be applied in these settings we do not assume that the text can be grounded in a world state. In our experiments the input text consists of generalized statements which describe some information need that does not correspond directly to a Acknowledgements We are grateful to Rohit Kate and Raymond Mooney for thei"
W10-2903,P06-2080,0,0.0713532,"better and reduce the required amount of supervision. We demonstrate the effectiveness of our training paradigm and interpretation model over the Geoquery domain, and show that our model can outperform fully supervised systems. iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty o"
W10-2903,W00-1317,0,0.0100761,"ntic interpretation that does not rely on NL syntactic parsing rules, but rather uses the syntactic information to bias the interpretation process. This approach allows the model to generalize better and reduce the required amount of supervision. We demonstrate the effectiveness of our training paradigm and interpretation model over the Geoquery domain, and show that our model can outperform fully supervised systems. iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then def"
W17-2608,D13-1160,0,0.0245633,"Missing"
W17-2608,P15-1067,0,0.0406751,"POSITION) 1. Forward-center 2. Swingman 3. Cabinet of the United States 5 get entity. Hence, the whole inference process can be thought as the model iteratively reformulates the representations in order to minimize its distance to the target entity in neural space. Related Work Link Prediction and Knowledge Base Completion Given that R is a relation, h is the head entity, and t is the tail entity, most of the embedding models for link prediction focus on finding the scoring function fr (h, t) that represents the implausibility of a triple. (Bordes et al., 2011, 2014, 2013; Wang et al., 2014; Ji et al., 2015; Nguyen et al., 2016). In many studies, the scoring function fr (h, t) is linear or bi-linear. For example, in TransE (Bordes et al., 2013), the function is implemented as fr (h, t) = kh + r − tk, where h, r and t are the corresponding vector representations. Recently, different studies (Guu et al., 2015; Lin et al., 2015a; Neelakantan et al., 2015; Das et al., 2016; Toutanova et al., 2016) demonstrate the importance for models to also learn from multi-step relations. Learning from multi-step relations injects the structured relationships between triples into the model. However, this also pos"
W17-2608,D15-1082,0,0.433431,"us, the knowledge base completion (KBC) task has emerged an important open research problem (Nickel et al., 2011). Neural-network based methods have been very popular for solving the KBC task. Following Bordes et al. (2013), one of the most popular approaches for KBC is to learn vector-space representations of entities and relations during training, and then apply linear or bi-linear operations to infer the missing relations at test time. However, several recent papers demonstrate limitations of prior approaches relying upon vector-space models alone (Guu et al., 2015; Toutanova et al., 2016; Lin et al., 2015a). By themselves, there is no straightforward way to capture the structured relationships between multiple triples adequately. For example, assume that we want to fill in the missing relation for the triple (Obama, NATIONALITY, ?), a multi-step search procedure might be needed to discover the evidence in the observed triples such as (Obama, B ORN I N, Hawaii) and (Hawaii, PART O F, U.S.A). To address this issue, Guu et al. (2015); Toutanova et al. (2016); Neelakantan et al. (2015); Das et al. (2016); Lin et al. (2015a) propose different approaches of injecting structured information based on"
W17-2608,D16-1147,0,0.0100396,"Missing"
W17-2608,P09-1113,0,0.0126757,"Missing"
W17-2608,P15-1016,0,0.0372826,"a relation, h is the head entity, and t is the tail entity, most of the embedding models for link prediction focus on finding the scoring function fr (h, t) that represents the implausibility of a triple. (Bordes et al., 2011, 2014, 2013; Wang et al., 2014; Ji et al., 2015; Nguyen et al., 2016). In many studies, the scoring function fr (h, t) is linear or bi-linear. For example, in TransE (Bordes et al., 2013), the function is implemented as fr (h, t) = kh + r − tk, where h, r and t are the corresponding vector representations. Recently, different studies (Guu et al., 2015; Lin et al., 2015a; Neelakantan et al., 2015; Das et al., 2016; Toutanova et al., 2016) demonstrate the importance for models to also learn from multi-step relations. Learning from multi-step relations injects the structured relationships between triples into the model. However, this also poses a technical challenge of considering exponential numbers of multi-step relationships. Prior approaches address this issue by designing path-mining algorithms (Lin et al., 2015a) or considering all possible paths using a dynamic programming algorithm with the restriction of using linear or bi-linear models only (Toutanova et al., 2016). Neelakanta"
W17-2608,N16-1054,0,0.312463,"controller. We set the maximum inference step Tmax of the IRN to 5. We randomly initialize all model parameters, and use SGD as the training algorithm with mini-batch size of 64. We set the learning rate to a constant number, 0.01. To prevent the model from learning a trivial solution by increasing entity embeddings norms, we follow Bordes et al. (2013) to enforce the L2 -norm of the entity embeddings as 1. We use hits@10 as the validation metric for the IRN. Following the work (Lin et al., 2015a), we add reverse relations into the training triplet set to increase the training data. Following Nguyen et al. (2016), we divide the results of previous work into two groups. The first group contains the models that directly optimize a scoring function for the triples in a knowledge base without using extra information. The second group of models make uses of additional information from multi-step relations. For example, RTransE (García-Durán et al., 2015) and PTransE Experimental Results In this section, we evaluate the performance of our model on the benchmark FB15k and WN18 datasets for KBC (Bordes et al., 2013). These datasets contain multi-relations between head and tail entities. Given a head entity an"
W17-2608,N16-3020,0,0.00355877,"Missing"
W17-2608,D16-1145,0,0.289258,"Missing"
W17-2608,N13-1008,0,0.0189883,"ference depending on the complexity of the instance. MemNN and NTM explicitly store inputs (such as graph definition, supporting facts) in the memory. In contrast, in IRNs, we do not explicitly store all the observed inputs in the shared memory. Instead, we directly operate on the shared memory, which modeling the structured relationships implicitly. During training, we randomly initialize the memory and update the memory jointly with the controller with respect to task-specific objectives via back-propagation, instead of explicitly defining memory write operations as in NTM. Studies such as (Riedel et al., 2013) show that incorporating textual information can further improve the KBC tasks. It would be interesting to incorporate the information outside the knowledge bases in our model in the future. 6 Neural Frameworks Sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) have shown to be successful in many applications such as machine translation and conversation modeling (Sordoni et al., 2015). While sequence-tosequence models are powerful, recent work has shown the necessity of incorporating an external memory to perform inference in simple algorithmic tasks (Graves et al., 2014, 2"
W17-2608,D10-1106,0,0.0300331,"Missing"
W17-2608,P15-1128,1,0.0252863,"Missing"
W17-2608,N15-1020,1,0.0245097,"Missing"
W17-2608,W15-4007,0,0.0843574,"ses disease/symptoms “sports” sports-team-roster/team basketball-roster-position/player basketball-roster-position/player baseball-player/position-s appointment/appointed-by batting-statistics/team basketball-player-stats/team person/profession “tv program” tv-producer-term/program tv-producer-term/producer-type tv-guest-role/episodes-appeared-in tv-program/languages tv-guest-role/actor tv-program/spin-offs award-honor/honored-for tv-program/country-of-origin (2015) and Das et al. (2016) use an RNN to model the multi-step relationships over a set of random walk paths on the observed triplets. Toutanova and Chen (2015) shows the effectiveness of using simple node and link features that encode structured information on FB15k and WN18. In our work, the IRN outperforms prior results and shows that similar information can be captured by the model without explicitly designing inference procedures on the observed triplets. Our model can be regarded as a recursive function that iteratively update the representation in such a way that its distance to the target entity in the neural space is minimized, i.e., kfIRN (h, r) − tk. biggest difference between our model and the existing frameworks is the controller and the"
W17-2608,D15-1174,0,0.0202694,"Missing"
W17-2608,P16-1136,0,0.406423,"tities and relations. Thus, the knowledge base completion (KBC) task has emerged an important open research problem (Nickel et al., 2011). Neural-network based methods have been very popular for solving the KBC task. Following Bordes et al. (2013), one of the most popular approaches for KBC is to learn vector-space representations of entities and relations during training, and then apply linear or bi-linear operations to infer the missing relations at test time. However, several recent papers demonstrate limitations of prior approaches relying upon vector-space models alone (Guu et al., 2015; Toutanova et al., 2016; Lin et al., 2015a). By themselves, there is no straightforward way to capture the structured relationships between multiple triples adequately. For example, assume that we want to fill in the missing relation for the triple (Obama, NATIONALITY, ?), a multi-step search procedure might be needed to discover the evidence in the observed triples such as (Obama, B ORN I N, Hawaii) and (Hawaii, PART O F, U.S.A). To address this issue, Guu et al. (2015); Toutanova et al. (2016); Neelakantan et al. (2015); Das et al. (2016); Lin et al. (2015a) propose different approaches of injecting structured inf"
W17-2608,D15-1034,0,\N,Missing
