2003.mtsummit-systems.14,W93-0423,1,0.817927,"Missing"
2003.mtsummit-systems.14,1997.tmi-1.12,1,0.883655,"Missing"
2003.mtsummit-systems.14,hein-etal-2002-scaling,1,\N,Missing
2003.mtsummit-systems.14,tiedemann-2002-matslex,1,\N,Missing
2009.eamt-1.16,P07-2045,0,0.00823727,"on-line engine. 2.3 PSMT for Question Translation Phrase-based statistical machine translation (PSMT) is currently extremely popular and can be seen as one of the state-of-the-art approaches in today’s machine translation research. Its popularity is also due to the availability of tools for building statistical models (word aligners and phrase extractors) and for the actual translation (decoders). The techniques are becoming so well-known that we omit the general introduction of the (P)SMT approach and just refer to standard literature (see for example (Brown et al., 1993; Och and Ney, 2003; Koehn et al., 2007; Koehn and Hoang, 2007)). It might come as a surprise to see PSMT as one of the approaches applied here after the introduction of our training data. SMT usually requires large amounts of training data (for instance, more than 1,000,000 sentences of parallel data). However, for our task-specific approach we only have a tiny amount of translated questions available. On the other hand, we know that questions (especially factoid questions) follow very regular patterns. They are often very short and usually do not include embedded clauses or other complex structures. The general question we want t"
2009.eamt-1.16,2005.mtsummit-papers.11,0,0.0144336,"et al., 2003a), and the Multi-six corpus, a collection of 200 questions in six languages collected from CLEF QA-2003 (Magnini et al., 2003b). Altogether, this amounts to 1349 questions with English and Dutch translations. Additionally, we also have one source of Dutch questions coming from the popular Winkler-Prins game (a Dutch quiz game similar to Trivial Pursuit), which we have used previously for training our disambiguation module when parsing questions (Bouma et al., 2005). This monolingual corpus contains 4509 questions. Another resource that we use is the multilingual Europarl corpus (Koehn, 2005) with its more than 1,000,000 parallel sentences. From this corpus we also extracted 31,506 questions by simply searching for lines ending with question marks in both, English and Dutch translations. Certainly, these are not the typical questions to be expected as input for a QA system. However, they are still useful as they represent the specific syntactic structures of questions. Finally, we also collected multilingual term databases from Wikipedia and Geonames.org. From the latter, we simply extracted all pairs of Dutch and English place names giving us 55,381 entries. From Wikipedia we mad"
2009.eamt-1.16,J93-2003,0,0.00876234,"s to be solved in later versions of the on-line engine. 2.3 PSMT for Question Translation Phrase-based statistical machine translation (PSMT) is currently extremely popular and can be seen as one of the state-of-the-art approaches in today’s machine translation research. Its popularity is also due to the availability of tools for building statistical models (word aligners and phrase extractors) and for the actual translation (decoders). The techniques are becoming so well-known that we omit the general introduction of the (P)SMT approach and just refer to standard literature (see for example (Brown et al., 1993; Och and Ney, 2003; Koehn et al., 2007; Koehn and Hoang, 2007)). It might come as a surprise to see PSMT as one of the approaches applied here after the introduction of our training data. SMT usually requires large amounts of training data (for instance, more than 1,000,000 sentences of parallel data). However, for our task-specific approach we only have a tiny amount of translated questions available. On the other hand, we know that questions (especially factoid questions) follow very regular patterns. They are often very short and usually do not include embedded clauses or other complex str"
2009.eamt-1.16,P05-1066,0,0.0856195,"Missing"
2009.eamt-1.16,P07-2009,0,0.0242827,"Missing"
2009.eamt-1.16,D07-1091,0,0.071409,"PSMT for Question Translation Phrase-based statistical machine translation (PSMT) is currently extremely popular and can be seen as one of the state-of-the-art approaches in today’s machine translation research. Its popularity is also due to the availability of tools for building statistical models (word aligners and phrase extractors) and for the actual translation (decoders). The techniques are becoming so well-known that we omit the general introduction of the (P)SMT approach and just refer to standard literature (see for example (Brown et al., 1993; Och and Ney, 2003; Koehn et al., 2007; Koehn and Hoang, 2007)). It might come as a surprise to see PSMT as one of the approaches applied here after the introduction of our training data. SMT usually requires large amounts of training data (for instance, more than 1,000,000 sentences of parallel data). However, for our task-specific approach we only have a tiny amount of translated questions available. On the other hand, we know that questions (especially factoid questions) follow very regular patterns. They are often very short and usually do not include embedded clauses or other complex structures. The general question we want to ask here is: Can a sma"
2009.eamt-1.16,J04-2003,0,0.0245698,"other but because there are apparently many questions about people’s deaths in our training data. The preference for links at similar positions causes the word aligner to select “did” as the alignment of “stierf”, which in the end causes the error described above. When did Shapour Bakhtiar die ? Wanneer stierf Shapour Bakhtiar ? At what age did Fernando Rey die ? Op welke leeftijd stierf Fernando Rey ? The success of “pre-ordering” the source language has already been shown in earlier studies (Collins et al., 2005) and also moving the main verb in questions has been applied in other studies (Nießen and Ney, 2004). We therefore parsed our data with the Stanford parser obtaining not only phrasestructure trees but also dependency relations. We then moved the infinitive next to the auxiliary if they are in a (corresponding) direct relation to each other: original: How did Jimi Hendrix die ? reordered: How did die Jimi Hendrix ? original: What language do the Berbers speak ? reordered: What language do speak the Berbers ? This is done for the CLEF questions before estimating the MT models and before translating questions from the test set. Results using this approach are shown in table 7. language model CL"
2009.eamt-1.16,J03-1002,0,0.0102735,"ter versions of the on-line engine. 2.3 PSMT for Question Translation Phrase-based statistical machine translation (PSMT) is currently extremely popular and can be seen as one of the state-of-the-art approaches in today’s machine translation research. Its popularity is also due to the availability of tools for building statistical models (word aligners and phrase extractors) and for the actual translation (decoders). The techniques are becoming so well-known that we omit the general introduction of the (P)SMT approach and just refer to standard literature (see for example (Brown et al., 1993; Och and Ney, 2003; Koehn et al., 2007; Koehn and Hoang, 2007)). It might come as a surprise to see PSMT as one of the approaches applied here after the introduction of our training data. SMT usually requires large amounts of training data (for instance, more than 1,000,000 sentences of parallel data). However, for our task-specific approach we only have a tiny amount of translated questions available. On the other hand, we know that questions (especially factoid questions) follow very regular patterns. They are often very short and usually do not include embedded clauses or other complex structures. The genera"
2009.eamt-1.16,2006.jeptalnrecital-invite.2,0,0.0215303,"Missing"
2009.eamt-1.3,P02-1051,0,0.00866771,"uite acceptable performance and they concluded c 2009 European Association for Machine Translation. Proceedings of the 13th Annual Conference of the EAMT, pages 12–19, Barcelona, May 2009 12 that this technique is especially useful when training material is scarce. They also demonstrate a possible combination of letter-based and wordbased models and obtained modest improvements in terms of BLEU scores. Other solutions for the translation of special types of unknown words have been described in various articles. For example, the translation of named entities is discussed in (Chen et al., 1998; Al-onaizan and Knight, 2002). The treatment of compound words is discussed in (Koehn and Knight, 2003). Another idea for translating unknown words using analogical learning has been proposed by (Langlais and Patry, 2007). In their approach proportional analogies between strings are used to solve analogical equations to retrieve translations of previously unseen terms. The use of phrase-based statistical machine translation on the character level has already been described in (Matthews, 2007). In their work, these models are applied to the task of machine transliteration of Chinese-English and Arabic-English. Similar tech"
2009.eamt-1.3,P98-1036,0,0.028318,"d system showed a quite acceptable performance and they concluded c 2009 European Association for Machine Translation. Proceedings of the 13th Annual Conference of the EAMT, pages 12–19, Barcelona, May 2009 12 that this technique is especially useful when training material is scarce. They also demonstrate a possible combination of letter-based and wordbased models and obtained modest improvements in terms of BLEU scores. Other solutions for the translation of special types of unknown words have been described in various articles. For example, the translation of named entities is discussed in (Chen et al., 1998; Al-onaizan and Knight, 2002). The treatment of compound words is discussed in (Koehn and Knight, 2003). Another idea for translating unknown words using analogical learning has been proposed by (Langlais and Patry, 2007). In their approach proportional analogies between strings are used to solve analogical equations to retrieve translations of previously unseen terms. The use of phrase-based statistical machine translation on the character level has already been described in (Matthews, 2007). In their work, these models are applied to the task of machine transliteration of Chinese-English an"
2009.eamt-1.3,E03-1076,0,0.0172051,"Machine Translation. Proceedings of the 13th Annual Conference of the EAMT, pages 12–19, Barcelona, May 2009 12 that this technique is especially useful when training material is scarce. They also demonstrate a possible combination of letter-based and wordbased models and obtained modest improvements in terms of BLEU scores. Other solutions for the translation of special types of unknown words have been described in various articles. For example, the translation of named entities is discussed in (Chen et al., 1998; Al-onaizan and Knight, 2002). The treatment of compound words is discussed in (Koehn and Knight, 2003). Another idea for translating unknown words using analogical learning has been proposed by (Langlais and Patry, 2007). In their approach proportional analogies between strings are used to solve analogical equations to retrieve translations of previously unseen terms. The use of phrase-based statistical machine translation on the character level has already been described in (Matthews, 2007). In their work, these models are applied to the task of machine transliteration of Chinese-English and Arabic-English. Similar techniques can also be applied to languages using the same writing system in o"
2009.eamt-1.3,2005.mtsummit-papers.11,0,0.00807942,"cturally very close related with each other. Hence, a character-based SMT model will probably not be powerful enough on its own (not even with very long phrases that correspond to words and word n-grams) to take care of these phenomena without a decent reordering model on the word/phrase level. Therefore, we focus on the combination of both, a word-level and a characterlevel model. In our experiments, the language models (for both, word LM and character LM) are simply trained on the target language side of our parallel data. We also add more out-of-domain data coming from the Europarl corpus (Koehn, 2005) to test a larger language model also on the character level. The data for training, tuning and testing our approach is taken from the OpenSubtitle corpus, which is part of the OPUS collection (Tiedemann, 2008). The corpus contains a fair amount of Norwegian-Swedish aligned movie subtitles – still very little with respect to the requirements of statistical MT. Here are some statistics of the data used in our experiments: training data : two different sets: word model: 142,654 sentence pairs 1,015,844 Norwegian tokens 990,431 Swedish tokens character model: (<= 40 char/sentence) 108,380 sentenc"
2009.eamt-1.3,D07-1092,0,0.015187,"this technique is especially useful when training material is scarce. They also demonstrate a possible combination of letter-based and wordbased models and obtained modest improvements in terms of BLEU scores. Other solutions for the translation of special types of unknown words have been described in various articles. For example, the translation of named entities is discussed in (Chen et al., 1998; Al-onaizan and Knight, 2002). The treatment of compound words is discussed in (Koehn and Knight, 2003). Another idea for translating unknown words using analogical learning has been proposed by (Langlais and Patry, 2007). In their approach proportional analogies between strings are used to solve analogical equations to retrieve translations of previously unseen terms. The use of phrase-based statistical machine translation on the character level has already been described in (Matthews, 2007). In their work, these models are applied to the task of machine transliteration of Chinese-English and Arabic-English. Similar techniques can also be applied to languages using the same writing system in order to cover spelling differences of names even across related languages (Tiedemann and Nabende, submitted). 3 corres"
2009.eamt-1.3,tiedemann-2008-synchronizing,1,0.814858,"take care of these phenomena without a decent reordering model on the word/phrase level. Therefore, we focus on the combination of both, a word-level and a characterlevel model. In our experiments, the language models (for both, word LM and character LM) are simply trained on the target language side of our parallel data. We also add more out-of-domain data coming from the Europarl corpus (Koehn, 2005) to test a larger language model also on the character level. The data for training, tuning and testing our approach is taken from the OpenSubtitle corpus, which is part of the OPUS collection (Tiedemann, 2008). The corpus contains a fair amount of Norwegian-Swedish aligned movie subtitles – still very little with respect to the requirements of statistical MT. Here are some statistics of the data used in our experiments: training data : two different sets: word model: 142,654 sentence pairs 1,015,844 Norwegian tokens 990,431 Swedish tokens character model: (<= 40 char/sentence) 108,380 sentence pairs 601,100 Norwegian tokens 595,208 Swedish tokens development set: 500 alignment units evaluation set: 500 alignment units 4.2 Note, that the aligned sentences/sentence fragments are rather short which is"
2009.eamt-1.3,W07-0705,0,0.75891,"n words probably more than, for example, the official EU languages for which sufficient amounts of training data is available. However, many of them (not only names) will actually be very similar to their translations. In this paper, we investigate the use of character-based PSMT models to translate such unknown words in order to improve the coverage of the MT system. In this way, we take Weaver’s decoding idea to the extreme – translating foreign words as sequences of encoded characters. This approach has already been applied to another pair of closely related languages, Spanish and Catalan (Vilar et al., 2007). Our work mainly follows their approach. However, we use different settings and techniques for training our character-based model and also compare the various setups and their impact on translation quality. The paper is organized as follows: First we will briefly mention related work. Thereafter, we describe the character-based model we will apply in the experiments discussed in the subsequent section. Finally we will summarize our study with some discussion and conclusions. Translating unknown words between related languages using a character-based statistical machine translation model can b"
2009.eamt-1.3,P07-2045,0,\N,Missing
2009.eamt-1.3,C98-1036,0,\N,Missing
2009.eamt-1.3,J03-1002,0,\N,Missing
2010.eamt-1.21,P03-1021,0,0.0106946,"angla in Bangladesh) and the BN IN (Bangla in WestBengal/India) domains. The KDE4 corpus has been taken from the OPUS corpus (Tiedemann and Nygard, 2004) which is already aligned. This KDE4 system message corpus contains 221,409 words and 33,365 sentence pairs with UTF-8 encoding. We used the monolingual corpus from the EMILLE project and the Prothom-Alo corpus developed by BRAC University, Bangladesh. The EMILLE monolingual corpus contains 1,867,452 words and the ProthomAlo corpus contains 19,496,884 words. We have used freely available tools, such as MOSES, GIZA++ (Och and Ney, 2003), MERT (Och, 2003) and SRILM (Stolcke, 2002) to build the system. To evaluate a MT system a single metric is not enough. Therefore, we have used more than one metric to evaluate our translation and transliteration system. BLEU (Papineni et al. , 2001), NIST (Doddington, 2002) and TER (Snover et al. , 2006) have been used for translation evaluation and ACC (accuracy in Top-1), Top5, Top-20 as well as Mean F–score for transliteration evaluation. ACC represents the correctness of candidate transliteration; Top-5 and Top-20 represent the percentage of correct transliteration in top 5 and top 20 candidates; Mean F–s"
2010.eamt-1.21,J03-1002,0,0.00678993,"stem architecture BN BD (Bangla in Bangladesh) and the BN IN (Bangla in WestBengal/India) domains. The KDE4 corpus has been taken from the OPUS corpus (Tiedemann and Nygard, 2004) which is already aligned. This KDE4 system message corpus contains 221,409 words and 33,365 sentence pairs with UTF-8 encoding. We used the monolingual corpus from the EMILLE project and the Prothom-Alo corpus developed by BRAC University, Bangladesh. The EMILLE monolingual corpus contains 1,867,452 words and the ProthomAlo corpus contains 19,496,884 words. We have used freely available tools, such as MOSES, GIZA++ (Och and Ney, 2003), MERT (Och, 2003) and SRILM (Stolcke, 2002) to build the system. To evaluate a MT system a single metric is not enough. Therefore, we have used more than one metric to evaluate our translation and transliteration system. BLEU (Papineni et al. , 2001), NIST (Doddington, 2002) and TER (Snover et al. , 2006) have been used for translation evaluation and ACC (accuracy in Top-1), Top5, Top-20 as well as Mean F–score for transliteration evaluation. ACC represents the correctness of candidate transliteration; Top-5 and Top-20 represent the percentage of correct transliteration in top 5 and top 20 ca"
2010.eamt-1.21,2001.mtsummit-papers.68,0,0.0864571,"Missing"
2010.eamt-1.21,2006.amta-papers.25,0,0.0259341,"Missing"
2010.eamt-1.21,W09-3521,0,0.0591252,"Missing"
2010.eamt-1.21,W96-0201,0,0.0171321,"ন ায ব বসা dti publications orderline office - ন ায ব বসা বাধ তামূলকভােব একিট নতুন শতাংশই সাধারণ িমঃ john vickers অে াবর উিনশশ এই নয় িহসােব কিঠন এইিট িহেসেব িন এবং করেত threat সটা যায়িন যেথ পােরন। mahmoud ahmadinejad হেয়েছ ত াখ াত য holocaust, এটা ”myth” ban ki-moon the closeness of candidate transliteration with the reference transliteration. Table 3: Evaluation of basline system Test corpus BLEU NIST TER EMILLE 1.20 1.65 0.90 KDE4 14.00 4.19 1.02 Combined 5.10 2.70 0.89 4.3 Baseline System We constructed a parallel training corpus of 10,850 sentence pairs using the GMA sentence aligner (Dan Melamed, 1996). The English corpus contains 199,973 words and the Bangla corpus contains 189,495 words. Each side of the KDE4 corpus contains 35,366 sentence pairs. The KDE4 Bangla corpus contains 221,409 words while the English corpus contains 157,392 words. We separated 500 sentence pairs from the EMILLE corpus and 1,000 sentence pairs from the KDE4 corpus for development sets. We also separated the same number of sentences from both corpus as a test set. The 5-gram language model was built from the EMILLE monolingual corpus, Prothom-Alo corpus and the training data, which together contain more than 21 mi"
2010.eamt-1.21,tiedemann-2006-isa,1,0.784739,"sh words in the target Bangla translation. Figure 1 shows the combined system architecture. Table 2 shows that the output contains some English words. The reason for this is the inclusion of many English words in the Bangla side of training, development and test sets, in some cases even entirely in English. This occurs for both the EMILLE and the KDE4 corpus. Therefore, we decided to clean the corpus in order to get better translation. First of all, we wanted to improve the automatic sentence alignment of the EMILLE corpus. For this we experimented with the Interactive Sentence Aligner (ISA) (Tiedemann, 2006) tool. ISA is an interactive tool with web interface for semi-automatic sentence alignment of parallel XML documents. It uses a standard length-based approach to align sentences and allows to manually add or remove segment boundaries to correct existing alignments in order to improve the overall quality of the parallel corpus. However, the ISA corpus alignment tool was not enough to clean the EMILLE corpus. Most of the files vary substantially. For example, the English text about child education has a total of Table 4: Irregularities in Bangla corpus English Corpus We analysed relevant data an"
2010.eamt-1.21,2005.mtsummit-posters.8,0,\N,Missing
2010.eamt-1.21,W06-2113,0,\N,Missing
2010.eamt-1.21,P02-1040,0,\N,Missing
2010.eamt-1.21,P07-2045,0,\N,Missing
2010.eamt-1.21,tiedemann-nygaard-2004-opus,1,\N,Missing
2011.mtsummit-systems.5,2009.mtsummit-posters.6,0,0.034449,"e improved open source SMT toolkit Moses developed by the University of Edinburgh. The Moses SMT toolkit is a complete statistical translation system distributed under the Lesser General Public License (LGPL). Moses includes all the components needed to pre-process data and to train language and translation models (Koehn et al. 2007). Moses is widely used in the research community and has also reached the commercial sector. While the use of the software is not closely monitored (there is no need to sign a license agreement), Moses is known to be in commercial use by companies such as Systran (Dugast et al. 2009), Asia Online2, Autodesk (Plitt and Masselot, 2010), Matrixware 3 , Adobe, Pangeanic, Logrus (Joscelyne 2010). The LetsMT! project coordinator Tilde bases its free online Latvian MT system on the Moses platform. LetsMT! uses Moses as a language independent SMT solution and integrates it as a cloud-based service into the LetsMT! online platform. One of 2 Asia Online. Wikipedia. 2011-08-22. http://en.wikipedia.org/wiki/Asia_Online. (Archived by WebCite® at http://www.webcitation.org/617phHvgD) 3 Machine Translation at Matrixware. 2011-08-22. http://irfacility.net/downloads/mxw_factsheet_smt_2009"
2011.mtsummit-systems.5,P07-2045,0,0.0108938,"ing of SMT systems from specified collections of training data; Custom building of MT engines from selected pools of training data; Custom building of MT engines from proprietary non-public data; MT evaluation facilities. 3 Architecture overview Figure 1 illustrates the general architecture of the LetsMT! platform. Its components for SMT training, parallel data collection and data processing are described further down in this paper. The development of the system was particularly facilitated by the open-source alignment tool GIZA++ (Och et al. 2002) and the MT training and decoding tool Moses (Koehn et al 2007). LetsMT! translation services can be used in several ways: through the web portal, through a widget provided for free inclusion in a web-page, through browser plug-ins, and through integration in computer-assisted translation (CAT) tools and different online and offline applications. Localisation and translation businesses as well as other professional translation can use the LetsMT! platform for uploading their parallel corpora in the LetsMT! website, building custom SMT solutions from the specified collections of training data, and accessing these solutions in their productivity environment"
2011.mtsummit-systems.5,D09-1079,0,0.0644016,"Missing"
2011.mtsummit-systems.5,J03-1002,0,0.00402392,"Missing"
2020.acl-demos.20,W19-6721,0,0.0277801,"Missing"
2020.acl-demos.20,W18-6317,0,0.042732,"Missing"
2020.acl-demos.20,W18-2709,0,0.0115905,"the quality of alignment between source and target language needs to checked. The aligned translations drive the mapping from input to the output language as a strong supervision during the training steps, and the amount of noise will have a decisive impact on the adequacy of the translations. The effect is especially severe for low resource settings, in which little data is available, and each mistake might directly influence the end result. The interest in automatic bitext (i.e. bilingual parallel corpora) filtering is constantly growing pushed by the advances in neural machine translation. Khayrallah and Koehn (2018) show that noisy training data is often more harmful for neural translation models than statistical translation models. As a consequence, international evaluation campaigns like the ones organised by WMT now feature shared tasks on data cleaning and ranking (Koehn et al., 2018, 2019). Various approaches have been proposed based on such challenges and directly benefit the development of MT engines in low-resource settings. This paper presents a framework for bitext cleaning, OpusFilter, focusing on processing data collected in OPUS (Tiedemann, 2012), the world’s largest resource of openly avail"
2020.acl-demos.20,P17-4012,0,0.0146132,"score values used for training classifiers in the Finnish-English noise filtering. CharacterScoreFilters have been excluded from histograms as their values are almost always one. training sets. Next, we reorder the data with our toolkit and again create new data sets by removing data with the same proportions as previously. We then apply data provided for the WMT news translation task6 for validation and testing. In particular, we use newstest2018 as the development set and newstest2019 as our test set for both language directions. The translation models are trained with the OpenNMT toolkit (Klein et al., 2017) using RNN encoders and decoders with LSTM gates. All training sets are tokenized with the tokenizer from the mosesdecoder toolkit (Koehn et al., 2007) and segmented with BPE (Sennrich et al., 2016) using subword-nmt7 before feeding them to OpenNMT. 3.1 Ranking Following V´azquez et al. (2019), we first produce an initial filtering of the ParaCrawl corpus. For this, we use the following heuristic filters from the OpusFilter toolbox: bly because similar filters are already applied in bicleaner when preparing the original data set. Nevertheless, these steps are useful for creating data to train"
2020.acl-demos.20,W19-5404,0,0.0243301,"Missing"
2020.acl-demos.20,P07-2045,0,0.00833198,"alues are almost always one. training sets. Next, we reorder the data with our toolkit and again create new data sets by removing data with the same proportions as previously. We then apply data provided for the WMT news translation task6 for validation and testing. In particular, we use newstest2018 as the development set and newstest2019 as our test set for both language directions. The translation models are trained with the OpenNMT toolkit (Klein et al., 2017) using RNN encoders and decoders with LSTM gates. All training sets are tokenized with the tokenizer from the mosesdecoder toolkit (Koehn et al., 2007) and segmented with BPE (Sennrich et al., 2016) using subword-nmt7 before feeding them to OpenNMT. 3.1 Ranking Following V´azquez et al. (2019), we first produce an initial filtering of the ParaCrawl corpus. For this, we use the following heuristic filters from the OpusFilter toolbox: bly because similar filters are already applied in bicleaner when preparing the original data set. Nevertheless, these steps are useful for creating data to train models used in the later filtering methods. First, we train word alignment priors for the ¨ model 3 of the eflomal tool8 (Ostling and Tiedemann, 2016)"
2020.acl-demos.20,W18-6453,0,0.0163569,"lations. The effect is especially severe for low resource settings, in which little data is available, and each mistake might directly influence the end result. The interest in automatic bitext (i.e. bilingual parallel corpora) filtering is constantly growing pushed by the advances in neural machine translation. Khayrallah and Koehn (2018) show that noisy training data is often more harmful for neural translation models than statistical translation models. As a consequence, international evaluation campaigns like the ones organised by WMT now feature shared tasks on data cleaning and ranking (Koehn et al., 2018, 2019). Various approaches have been proposed based on such challenges and directly benefit the development of MT engines in low-resource settings. This paper presents a framework for bitext cleaning, OpusFilter, focusing on processing data collected in OPUS (Tiedemann, 2012), the world’s largest resource of openly available parallel corpora. In contrast to tools such as bicleaner (S´anchez-Cartagena et al., 2018) and Zipporah (Xu and Koehn, 2017), that implement a single method for parallel corpus filtering, OpusFilter is designed as a toolbox that is useful for testing and using many differ"
2020.acl-demos.20,W18-6488,0,0.0249667,"Missing"
2020.acl-demos.20,P16-1162,0,0.00973415,"Next, we reorder the data with our toolkit and again create new data sets by removing data with the same proportions as previously. We then apply data provided for the WMT news translation task6 for validation and testing. In particular, we use newstest2018 as the development set and newstest2019 as our test set for both language directions. The translation models are trained with the OpenNMT toolkit (Klein et al., 2017) using RNN encoders and decoders with LSTM gates. All training sets are tokenized with the tokenizer from the mosesdecoder toolkit (Koehn et al., 2007) and segmented with BPE (Sennrich et al., 2016) using subword-nmt7 before feeding them to OpenNMT. 3.1 Ranking Following V´azquez et al. (2019), we first produce an initial filtering of the ParaCrawl corpus. For this, we use the following heuristic filters from the OpusFilter toolbox: bly because similar filters are already applied in bicleaner when preparing the original data set. Nevertheless, these steps are useful for creating data to train models used in the later filtering methods. First, we train word alignment priors for the ¨ model 3 of the eflomal tool8 (Ostling and Tiedemann, 2016) and variable-length character n-gram models for"
2020.acl-demos.20,tiedemann-2012-parallel,1,0.803308,"ces in neural machine translation. Khayrallah and Koehn (2018) show that noisy training data is often more harmful for neural translation models than statistical translation models. As a consequence, international evaluation campaigns like the ones organised by WMT now feature shared tasks on data cleaning and ranking (Koehn et al., 2018, 2019). Various approaches have been proposed based on such challenges and directly benefit the development of MT engines in low-resource settings. This paper presents a framework for bitext cleaning, OpusFilter, focusing on processing data collected in OPUS (Tiedemann, 2012), the world’s largest resource of openly available parallel corpora. In contrast to tools such as bicleaner (S´anchez-Cartagena et al., 2018) and Zipporah (Xu and Koehn, 2017), that implement a single method for parallel corpus filtering, OpusFilter is designed as a toolbox that is useful for testing and using many different approaches. Below we describe the design of OpusFilter and present its application in the test case of filtering Finnish-English parallel data included in ParaCrawl. 2 OpusFilter Toolbox The OpusFilter toolbox is implemented in Python 3 and is available at https://github.c"
2020.acl-demos.20,W19-5441,1,0.817299,"Missing"
2020.acl-demos.20,D17-1319,0,0.0183955,"n models. As a consequence, international evaluation campaigns like the ones organised by WMT now feature shared tasks on data cleaning and ranking (Koehn et al., 2018, 2019). Various approaches have been proposed based on such challenges and directly benefit the development of MT engines in low-resource settings. This paper presents a framework for bitext cleaning, OpusFilter, focusing on processing data collected in OPUS (Tiedemann, 2012), the world’s largest resource of openly available parallel corpora. In contrast to tools such as bicleaner (S´anchez-Cartagena et al., 2018) and Zipporah (Xu and Koehn, 2017), that implement a single method for parallel corpus filtering, OpusFilter is designed as a toolbox that is useful for testing and using many different approaches. Below we describe the design of OpusFilter and present its application in the test case of filtering Finnish-English parallel data included in ParaCrawl. 2 OpusFilter Toolbox The OpusFilter toolbox is implemented in Python 3 and is available at https://github.com/ Helsinki-NLP/OpusFilter under the permissive MIT open-source license. The main script provided by the package is opusfilter, which takes a configuration file as an input."
2020.amta-pemdt.6,R11-1014,0,0.0761151,"Missing"
2020.amta-pemdt.6,etchegoyhen-etal-2014-machine,0,0.0559791,"Missing"
2020.amta-pemdt.6,J93-1004,0,0.355482,". In contrast to standard bitext alignment we are now interested in 1-to-n alignments only in which each existing subtitle frame needs to be filled with one or more segments coming from the automatically generated translations. For the target language segmentation we consider simple heuristics for splitting sentences into clauses by breaking strings that are separated by punctuation plus space characters. Resulting sequences that exceed a certain length threshold are further split on space characters closest to the center of the string. After that, we apply the famous Gale & Church algorithm (Gale and Church, 1993) to optimise the global alignment between source segments (original subtitle frame data) and target segments with adjusted parameters referring to our specific task: (1) We apply a uniform prior over alignment types as there is no strong preference for frame-to-clause alignment in our case. (2) We define alignment types to include one-to-x units only with x ranging from one to four. (3) We introduce extra costs to discourage frame boundaries within running sentences and assignments that violate length constraints. Figure 1 shows an example outcome of the procedure. Finally, we also apply simpl"
2020.amta-pemdt.6,W19-5321,0,0.0115787,"op on Post-Editing in Modern-Day Translation Page 81 network in both the encoder and decoder with 8 self-attention heads per layer. Recommended features like label smoothing and dropout are enabled and we use tied embeddings and a shared vocabulary. SentencePiece (Kudo and Richardson, 2018) is used for tokenisation and subword segmentation with models independently trained for source and target language. The shared vocabulary is set to a size of 65,000 with equal proportions in each language. The document-level models refer to concatenative models proposed by Tiedemann and Scherrer (2017) and Junczys-Dowmunt (2019) using units of a maximum length of 100 tokens and special tokens for marking sentence boundaries. We observed that 100 tokens typically covers a substantial amount of contextual information in subtitles where sentences and sentence fragments are often very short. About 3.3 million pseudo-documents are created in a sequential way without overlaps for Finnish↔Swedish and 4.7 million pseudo-documents for Finnish↔English, corresponding to roughly 9 sentences per document on average. The same kind of chunking needs to be done during test time. Sentence-level models are translated in the usual way."
2020.amta-pemdt.6,P18-4020,0,0.0255224,"Missing"
2020.amta-pemdt.6,2020.iwslt-1.26,0,0.491729,"2017). In a study comparing task time for translation from scratch and MTPE, Bywood et al. (2017) report that MTPE increased the translators productivity; however, the results varied for different translators, language pairs and content types. More recently, Matusov et al. (2019) tested an NMT system customised for subtitles using parallel subtitle corpora from OpenSubtitles, GlobalVoices and TED talks and reported productivity increases for MTPE in a study involving two translators. So far, work has focused on the use of intralingual subtitles as the source text for MT, but a recent paper by Karakanta et al. (2020a) explores an end-to-end spoken language translation system for subtitling. No user evaluation of the system is reported, although Karakanta et al. (2020a) note that based on automatic evaluation against “gold standard” human subtitles the MT quality appears satisfactory. Karakanta et al. (2020b) also investigate annotating subtitle corpora for segment breaks and propose an approach for segmenting sentences into subtitles conforming to length constraints. 2.2 Studies on user experience/feedback from translators Subjective feedback is invaluable for providing insight into tools and workflows t"
2020.amta-pemdt.6,2020.lrec-1.460,0,0.199127,"2017). In a study comparing task time for translation from scratch and MTPE, Bywood et al. (2017) report that MTPE increased the translators productivity; however, the results varied for different translators, language pairs and content types. More recently, Matusov et al. (2019) tested an NMT system customised for subtitles using parallel subtitle corpora from OpenSubtitles, GlobalVoices and TED talks and reported productivity increases for MTPE in a study involving two translators. So far, work has focused on the use of intralingual subtitles as the source text for MT, but a recent paper by Karakanta et al. (2020a) explores an end-to-end spoken language translation system for subtitling. No user evaluation of the system is reported, although Karakanta et al. (2020a) note that based on automatic evaluation against “gold standard” human subtitles the MT quality appears satisfactory. Karakanta et al. (2020b) also investigate annotating subtitle corpora for segment breaks and propose an approach for segmenting sentences into subtitles conforming to length constraints. 2.2 Studies on user experience/feedback from translators Subjective feedback is invaluable for providing insight into tools and workflows t"
2020.amta-pemdt.6,2020.eamt-1.13,1,0.472584,"increased cognitive load of dealing with MT output is a significant part of productivity. Although a detailed discussion of the process data is not within the scope of this paper, some parallels can be seen in our productivity measurements. On average, task times for MTPE were slightly faster than for translation from scratch, although considerable variation was observed between different files and participants. Five out of twelve participants were in fact slower when post-editing, concurring with the translators’ mixed experience. For a more detailed analysis of the productivity metrics, see Koponen et al. (2020). Some care is needed when interpreting the results. Firstly, it is important to note that the participants in this study did not have prior experience with MT for subtitling (only two had previously tested it). Their responses may therefore be affected by the unfamiliarity of the task, which some participants mentioned in the interviews (see also similar observations by Bywood et al., 2017). Secondly, the participants are used to doing first translation from audio instead of working with subtitle templates. Since the MT outputs were created using intralingual subtitles as the source text, rat"
2020.amta-pemdt.6,D18-2012,0,0.0336141,"bible-uedin, DGT, EMEA, EUbookshop, EUconst, Europarl, Finlex, fiskmo, GNOME, infopankki, JRC-Acquis, KDE4, MultiParaCrawl, OpenSubtitles, PHP, QED, Tatoeba, TildeMODEL, Ubuntu, wikimedia 2 http://opus.nlpl.eu Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, 1st Workshop on Post-Editing in Modern-Day Translation Page 81 network in both the encoder and decoder with 8 self-attention heads per layer. Recommended features like label smoothing and dropout are enabled and we use tied embeddings and a shared vocabulary. SentencePiece (Kudo and Richardson, 2018) is used for tokenisation and subword segmentation with models independently trained for source and target language. The shared vocabulary is set to a size of 65,000 with equal proportions in each language. The document-level models refer to concatenative models proposed by Tiedemann and Scherrer (2017) and Junczys-Dowmunt (2019) using units of a maximum length of 100 tokens and special tokens for marking sentence boundaries. We observed that 100 tokens typically covers a substantial amount of contextual information in subtitles where sentences and sentence fragments are often very short. Abou"
2020.amta-pemdt.6,W19-5209,0,0.217033,"Missing"
2020.amta-pemdt.6,2006.tc-1.10,0,0.153427,"ntralingual subtitles in the source language or sometimes interlingual subtitles in a pivot language (often English) with set subtitle segmentation and timing (Nikoli´c, 2015). To date, the use of MT and PE for subtitling has been less common in AVT than other translation fields. Explanations for this may include the characteristics of subtitle translation, which pose challenges for MT, and also the difficulty of integrating current NMT systems to subtitle translation workflows (Matusov et al., 2019). MT for movie and TV subtitling has been tested in some language pairs since the early 2000s (Melero et al., 2006; Volk et al., 2010; de Sousa et al., 2011) with suggestions that PE may increase productivity also in this context. A subtitle-oriented statistical MT system and PE platform was developed by the SUMAT project, and tested in a user evaluation involving several language pairs and 19 professional subtitle translators (Etchegoyhen et al., 2014; Bywood et al., 2017). In a study comparing task time for translation from scratch and MTPE, Bywood et al. (2017) report that MTPE increased the translators productivity; however, the results varied for different translators, language pairs and content type"
2020.amta-pemdt.6,tiedemann-2008-synchronizing,1,0.692383,"e Americas October 6 - 9, 2020, 1st Workshop on Post-Editing in Modern-Day Translation Page 82 3.2 Subtitle frame alignment One of the crucial steps in subtitle translation is the assignment to appropriate time slots. Our approach is to map translations back into the frames defined in the original source language subtitles assuming that they can fit in a similar way as the source language text was segmented. Those subtitle frames may include multiple sentences and sentences may stretch over several frames. Sentence extraction from the original subtitles is done with the techniques proposed by Tiedemann (2008). Time allocation of the translated sentences is implemented as yet another alignment algorithm. Subtitles converted to sentence-level segments in XML: Mapped back to subtitle frames after translation: &lt;s id=""13""&gt; &lt;time id=""T16S"" value=""00:01:05,960"" /&gt; We have to make readmission agreements with other countries, &lt;time id=""T16E"" value=""00:01:12,360"" /&gt; &lt;time id=""T17S"" value=""00:01:12,440"" /&gt; so that they would be willing. &lt;/s&gt; &lt;s id=""14""&gt; We have to cooperate closely. &lt;time id=""T17E"" value=""00:01:17,440"" /&gt; &lt;/s&gt; 16 00:01:05,960 --&gt; 00:01:12,360 Meid¨ an on teht¨ av¨ a takaisinottosopimuksia mu"
2020.amta-pemdt.6,W17-4811,1,0.834859,"as October 6 - 9, 2020, 1st Workshop on Post-Editing in Modern-Day Translation Page 81 network in both the encoder and decoder with 8 self-attention heads per layer. Recommended features like label smoothing and dropout are enabled and we use tied embeddings and a shared vocabulary. SentencePiece (Kudo and Richardson, 2018) is used for tokenisation and subword segmentation with models independently trained for source and target language. The shared vocabulary is set to a size of 65,000 with equal proportions in each language. The document-level models refer to concatenative models proposed by Tiedemann and Scherrer (2017) and Junczys-Dowmunt (2019) using units of a maximum length of 100 tokens and special tokens for marking sentence boundaries. We observed that 100 tokens typically covers a substantial amount of contextual information in subtitles where sentences and sentence fragments are often very short. About 3.3 million pseudo-documents are created in a sequential way without overlaps for Finnish↔Swedish and 4.7 million pseudo-documents for Finnish↔English, corresponding to roughly 9 sentences per document on average. The same kind of chunking needs to be done during test time. Sentence-level models are t"
2020.amta-pemdt.6,2010.jec-1.7,0,0.0830119,"Missing"
2020.blackboxnlp-1.13,marelli-etal-2014-sick,0,0.0607422,"16; Hewitt and Manning, 2019; Rogers et al., 2020; Tenney et al., 2019) as well as variation in their context of use. We propose to explore the impact of context variation on word representations. We specifically address representations generated by the BERT model (Devlin et al., 2019), trained using a language modeling objective, and translation models involving one or more language pairs (Artetxe and Schwenk, 2019; V´azquez et al., 2020). We run a series of controlled experiments using sentences illustrating both meaning preserving and meaning altering transformations from the SICK dataset (Marelli et al., 2014b), and examples automatically generated using a template-based method (Prasad et al., 2019). We explore the impact of specific alternations on the representations, namely passivization and negation. Examples in our datasets consist of sentences that only differ in terms of the specific alternation addressed. In order to detect the imprint of these transformations on the representations, we employ methodology inspired by work on linguistic bias detection in embedding representations (Bolukbasi et al., 2016; Lauscher et al., 2019; Ravfogel et al., 2020). Furthermore, we investigate the impact o"
2020.blackboxnlp-1.13,2020.cl-2.5,1,0.86502,"Missing"
2020.blackboxnlp-1.13,D14-1162,0,0.0953117,"Missing"
2020.blackboxnlp-1.13,N18-1202,0,0.0526859,"er agreement (Linzen et al., 2016; Hewitt and Manning, 2019; Hewitt and Liang, 2019), or semantic phenomena such as semantic role labeling and coreference (Tenney et al., 2019; Kovaleva et al., 2019). In our work, we shift the focus from interpreting the knowledge about language encoded in the representations, to exploring the imprint of two specific transformations, passivization and negation, on word representations. The majority of the above mentioned works address representations generated by models trained with a language modeling objective, such as LSTM RNNs (Linzen et al., 2016), ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Voita et al. (2019a) propose to study the representations obtained from models trained with a different objective. We take the same stance and investigate the impact of context on representations generated by BERT, and by the encoder of neural machine translation (NMT) models involving one or more language pairs. In order to detect the information related to the two studied transformations that is encoded in the representations, we employ methodology initially proposed for identifying and removing linguistic and other kinds of biases from representations. Such"
2020.blackboxnlp-1.13,K19-1007,0,0.0615277,"Missing"
2020.blackboxnlp-1.13,2020.acl-demos.14,0,0.0902401,"ing and illustrates lexical, syntactic and semantic phenomena that compositional distributional semantic models are expected to account for. PAS is one of the meaning preserving alternations in SICK, where a sentence S2 results from the passivization of an active sentence S1. We use all the 276 sentence 4 Our code and data are available at https://github. com/Helsinki-NLP/Syntactic_Debiasing 138 5 The code is available at https://github.com/ grushaprasad/RNN-Priming. 6 The complexity of the sentences also resulted in numerous syntactic analysis errors when we tried to parse them using Stanza (Qi et al., 2020). 7 The dataset was used in SemEval 2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment (Marelli et al., 2014a). zi cannot be predicted from g(xi ). The method is based on iteratively (1) training a linear classifier to predict zi from xi , followed by (2) projecting xi on the null-space of the classifier, using a projection matrix PN (W ) such that W (PN (W ) x) = 0 ∀x, where W is the weight matrix of the classifier, and N (W ) is its null-space. Through the projection step in each iteration, the informa"
2020.blackboxnlp-1.13,D18-1521,0,0.0187822,"lation (NMT) models involving one or more language pairs. In order to detect the information related to the two studied transformations that is encoded in the representations, we employ methodology initially proposed for identifying and removing linguistic and other kinds of biases from representations. Such methods fall in two main paradigms: projection and adversarial methods. Projection methods identify specific directions in word embedding space that correspond to the protected attribute, and remove them. Bolukbasi et al. (2016) identify a gender subspace by exploring gendered word lists. Zhao et al. (2018) propose to train debiased word embeddings from scratch by altering the loss of the GloVe model (Pennington et al., 2014) to concentrate specific information (e.g., about gender) in a dedicated coordinate of each vector. Dev and Phillips (2019) propose a simple linear projection method to reduce the bias in word embed137 dings. Lauscher et al. (2019) develop a variation of this method that introduces more flexibility in the formation of the debiasing vector used in the projection. Adversial methods extend the main task objective with a component that competes with the encoder trying to extract"
2020.blackboxnlp-1.13,D19-1448,0,0.0373628,"Missing"
2020.blackboxnlp-1.13,P19-1580,0,0.0294334,"heme in direct object position. In , 2 the semantic relationship of the mafia and the millionaire to the kidnapping event is the same but their syntactic roles have changed. 3 These two transformations were preferred on the basis that they do not change the words in the sentence, as opposed to other possible translations, which involve reformulations, eg. “a sewing machine” vs. “a machine made for sewing”. Related Work The analysis and interpretation of the linguistic knowledge present in contextualized representations has recently been the focus of a large amount of work (Clark et al., 2019; Voita et al., 2019b; Tenney et al., 2019; Talmor et al., 2019). The bulk of this interpretation work relies on probing tasks which serve to predict linguistic properties from the representations generated by the models (Linzen, 2018; Rogers et al., 2020). These might involve structural aspects of language, such as syntax, word order, or number agreement (Linzen et al., 2016; Hewitt and Manning, 2019; Hewitt and Liang, 2019), or semantic phenomena such as semantic role labeling and coreference (Tenney et al., 2019; Kovaleva et al., 2019). In our work, we shift the focus from interpreting the knowledge about lang"
2020.cl-2.5,S15-2045,0,0.0602347,"Missing"
2020.cl-2.5,S14-2010,0,0.0854519,"Missing"
2020.cl-2.5,S16-1081,0,0.0610018,"Missing"
2020.cl-2.5,S13-1004,0,0.0433475,"Missing"
2020.cl-2.5,S12-1051,0,0.0329705,"Missing"
2020.cl-2.5,Q19-1038,0,0.02664,"hat seq2seq models are better at encoding the meaning of sentences into vector spaces than the bag-of-words model. Recent work includes that of Schwenk and Douze (2017), who used multiple encoders and decoders that are connected through a shared layer, albeit with a different purpose than performing translations; Platanios et al. (2018) showed an intermediate representation that can be decoded to any target language while describing a parameter generation method for universal NMT; Britz, Guan, and Luong (2017) made a computational efficiency analysis for MT using a fixed-size attention layer; Artetxe and Schwenk (2019) used a shared LSTM with max pooling to learn sentence embeddings on 93 translation directions; Cífka and Bojar (2018) introduced an architecture with a self-attentive layer to extract sentence meaning representations of fixed size. Here we use a similar architecture but in a multilingual setting. Our work on multilingual MT and sentence representations is closely related to the study from Lu et al. (2018). There, the authors attempt to build a neural interlingua by using language-independent encoders and decoders which share an attentive LSTM layer. Our approach differs on the choice of the c"
2020.cl-2.5,P17-1080,0,0.0292268,"n language pairs, so-called zero-shot translation (Ha, Niehues, and Waibel 2016; Johnson et al. 2017; Gu et al. 2018a). The effective computation of sentence representations using the translation task as an auxiliary semantic signal has also drawn interest to MT models (Hill, Cho, and Korhonen 2016; McCann et al. 2017; Schwenk and Douze 2017; Subramanian et al. 2018). Indeed, recent work makes use of machine translation models to capture syntactic and semantic properties of the input sentences, later to be used for learning generalpurpose sentence representations (Shi, Padhi, and Knight 2016; Belinkov et al. 2017; Dalvi et al. 2017; Poliak et al. 2018; Bau et al. 2019). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the effective reduction to a fixed-sized vector; it enables functionality, at the expense of hampering the performance in the MT task (Britz, Guan, and Luong 2017; Cífka and Bojar 2018). However, it is not fully clear how the properties of the fixed-sized vector influence the tradeoff between the performance of the model in MT and the information it encodes as a meaning representation vector. Recent studies either focus on th"
2020.cl-2.5,C18-1263,0,0.0252538,"Missing"
2020.cl-2.5,Q17-1010,0,0.0571462,"izes the results obtained for different settings. We rely on our multilingual attention bridge model trained in a many-to-many fashion. For comparison, we include representations obtained from large pretrained word embeddings. Note that those embeddings are trained on vastly more data than our model, which is trained on the parallel Europarl corpus. In particular, we use the multilingual word embeddings from the fastText (Grave et al. 2018) and the MUSE (Conneau et al. 2018b) libraries. The fastText algorithm is based on word2vec and produces word embeddings compounded from character n-grams (Bojanowski et al. 2017), which is to be preferred for morphologically richer languages in a multilingual setting. The fastText word vectors are pretrained in CommonCrawl and Wikipedia using CBOW with position weights, whereas MUSE word embeddings are Wikipedia fastText vectors from 30 languages aligned in a supervised way into a single vector space. Because fastText vectors are not aligned into the same space we only present the accuracies on the relevant languages for each case. To obtain sentence representations, we compute the average of the individual word vectors. We use the XNLI corpus to train multilingual cl"
2020.cl-2.5,D15-1075,0,0.309405,"le of focusing on k different components of the sentence (Lin et al. 2017; Chen, Ling, and Zhu 2018; Cífka and Bojar 2018), using self-attention as follows: A = softmax W2 ReLU(W1 HT )  M = AH (8) (9) where W1 ∈ Rdw ×dh and W2 ∈ Rk×dw are weight matrices, with dw a hyperparameter set arbitrarily, and k the number of attention heads in the attention bridge. Note that each column of M, mi , is a component focusing on a portion of the sentence, so all of them together should reflect the overall semantics of the sentence. Each decoder follows a common attention mechanism in NMT (Luong, Pham, and Manning 2015), with an initial state computed by mean pooling over M, and using M instead of the hidden states of the encoder for computing the context vector. Formally, we only need to compute Equations (6) and (7) using the columns of M instead of the encoder states hi . (2) Language-specific encoders and decoders: To deal with additional language pairs, we incorporate an encoder for each input language and an attentive decoder for each 2 Note that the attention bridge is independent of the underlying encoder and decoder (Lu et al. 2018). Although we use a BiLSTM, it could be replaced with a GRU (Cho et"
2020.cl-2.5,D17-1040,0,0.0432274,"Missing"
2020.cl-2.5,S17-2001,0,0.0545359,"Missing"
2020.cl-2.5,C18-1154,0,0.0420691,"Missing"
2020.cl-2.5,P17-1176,0,0.0184683,"n interesting question is whether we can see the same trend with less related languages included in our data and whether we can force specialization using certain constraints or augmented loss functions during training. We also want to explore the effect of adding other tasks that can be modeled with the same architecture including speech recognition, sequence labeling, or parsing. 6. Related Work Before concluding the paper, we briefly summarize related work. Multilingual NMT has been widely studied and developed in different pathways during recent years (Dong et al. 2015; Luong et al. 2016; Chen et al. 2017; Johnson et al. 2017). Work has been done with networks that use language specific encoders and decoders, such as Dong et al. (2015), who used a separate attention mechanism for each decoder on one-to-many translation. Zoph and Knight (2016) exploited a multi-way parallel corpus in a many-to-one multilingual scenario, while Firat, Cho, and Bengio (2016) used language-specific encoders and decoders that share a traditional attention mechanism in a many-to-many scheme. Another approach is the use of universal encoder–decoder networks that share embedding spaces to improve the performance of the"
2020.cl-2.5,P18-1126,0,0.0585152,"n et al. 2018). Indeed, recent work makes use of machine translation models to capture syntactic and semantic properties of the input sentences, later to be used for learning generalpurpose sentence representations (Shi, Padhi, and Knight 2016; Belinkov et al. 2017; Dalvi et al. 2017; Poliak et al. 2018; Bau et al. 2019). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the effective reduction to a fixed-sized vector; it enables functionality, at the expense of hampering the performance in the MT task (Britz, Guan, and Luong 2017; Cífka and Bojar 2018). However, it is not fully clear how the properties of the fixed-sized vector influence the tradeoff between the performance of the model in MT and the information it encodes as a meaning representation vector. Recent studies either focus on the usage of such MT-based vector representations in other tasks (Schwenk 2018), on translation quality (Lu et al. 2018), on speed comparison (Britz, Guan, and Luong 2017), or only explore a bilingual scenario (Cífka and Bojar 2018). For this study, we focus on exploring a crosslingual intermediate shared layer in an MT model. We apply an architecture base"
2020.cl-2.5,L18-1269,0,0.0557379,"enable multilingual training we augment the network with language-specific encoders and decoders trainable with a language-rotating scheduler (Dong et al. 2015; Schwenk and Douze 2017). We also incorporate an intermediate inner-attention layer, which summarizes the encoder information in a fixed-size vector representation, to serve as a language-agnostic layer (Cífka and Bojar 2018; Lu et al. 2018). Because of the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is a multilingual adaptation from the model proposed by Cífka and Bojar (2018). The overall architecture is illustrated in Figure 1. 2.1 Background: Attention Mechanism Given an input X = (x1 , . . . , xn ), a sequence of embedded tokens into the vector space Rdx , our goal is to generate a translation Y = (y1 , . . . , ym ). For the sake of clarity, we assume a recurrent encoder in the following even though the mechanism is not restricted to this particular type of encoder. A recurrent neural network (RNN)-based encoder reads each element in X to generate a context vector c. Generally, for each token the 389 Computational Linguistics Volume 46, Number 2 Figure 1 Archit"
2020.cl-2.5,D17-1070,0,0.0247302,"many set-up produces better scores compared with the similar set-up with many-to-English models. Some additional studies might reveal further insights, which we will leave for future work. 4.3 SentEval Similarity Tasks The next evaluation refers to the similarity tasks of SentEval—that is, English data only. Table 8 summarizes the results using Pearson’s correlation coefficient as well as the average on all tasks. As comparison we include the bag-of-vectors baseline (GloVeBOW) as in the earlier SentEval classification tasks, the best model from Cífka and Bojar (2018), and the InferSent model (Conneau et al. 2017) as a state-of-the-art model that is pretrained on a natural language inference (NLI) task. As discussed earlier, note that the SICK and STSB benchmarks provide training data where a classifier learns to predict the probability distribution of the relatedness scores (Tai, Socher, and Manning 2015). Two different trends become visible: i) On the unsupervised textual similarity tasks, having fewer attention heads is beneficial. Contrary to the results in the classification tasks, the best overall model is Table 8 Results from seven similarity tasks, measured using Pearson’s correlation coefficie"
2020.cl-2.5,P18-1198,0,0.0609641,"nstream tasks with representations learned from limited resources, using the Multi30K data set, to further demonstrate that useful representations can be picked up even from tiny data sets. This is in contrast to related work where huge amounts of training data are typically applied to obtain reasonable performance. Our assumption is that multilinguality contributes to a higher level of semantic abstraction that can be learned from the translation objective. To test this claim, we apply standard benchmarks collected in the SentEval toolkit (Conneau and Kiela 2018), the XNLI evaluation corpus (Conneau et al. 2018c), as well as the Yelp challenge data set.8 The SentEval toolkit contains three benchmark types: classification, similarity, and linguistic probing tasks. In the classification tasks, a classifier is trained on top of a sentence embedding involving various data sets: CR—product reviews (Hu and Liu 2004), MR—movie reviews (Pang and Lee 2005), MPQA—opinion polarity (Wiebe, Wilson, and Cardie 2005), SUBJ—subjectivity/objectivity status (Pang and Lee 2004), SST—binary and fine-grained sentiment analysis (Socher et al. 2013), TREC—questiontype classification (Voorhees and Tice 2000), MRPC—paraphra"
2020.cl-2.5,D18-1269,0,0.0500244,"Missing"
2020.cl-2.5,I17-1015,0,0.0202705,"alled zero-shot translation (Ha, Niehues, and Waibel 2016; Johnson et al. 2017; Gu et al. 2018a). The effective computation of sentence representations using the translation task as an auxiliary semantic signal has also drawn interest to MT models (Hill, Cho, and Korhonen 2016; McCann et al. 2017; Schwenk and Douze 2017; Subramanian et al. 2018). Indeed, recent work makes use of machine translation models to capture syntactic and semantic properties of the input sentences, later to be used for learning generalpurpose sentence representations (Shi, Padhi, and Knight 2016; Belinkov et al. 2017; Dalvi et al. 2017; Poliak et al. 2018; Bau et al. 2019). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the effective reduction to a fixed-sized vector; it enables functionality, at the expense of hampering the performance in the MT task (Britz, Guan, and Luong 2017; Cífka and Bojar 2018). However, it is not fully clear how the properties of the fixed-sized vector influence the tradeoff between the performance of the model in MT and the information it encodes as a meaning representation vector. Recent studies either focus on the usage of such MT-"
2020.cl-2.5,N19-1423,0,0.0324914,"er look at individual classification tasks to get a more detailed picture of the performance in the various settings. Tables 4 and 5 show the performance of our models on the different downstream tasks. We report the accuracy on each individual test set, including the following comparison scores: a baseline of the most frequent class; a bag-of-vectors baseline obtained by averaging GloVe word embeddings (Pennington, Socher, and Manning 2014); an average of word embeddings as well as the CLS fixed-size sentence vector representation obtained from the large-scale pretrained language model BERT (Devlin et al. 2019; Reimers and Gurevych 2019); a state of the art general-purpose model that exploits large-scale multitask learning on different tasks including machine translation (Subramanian et al. 2018); and the performance from other MT systems by Hill, Cho, and Korhonen (2016) and Conneau et al. (2018a).9 The experiments reveal two important findings: 1. In contrast with the results from Cífka and Bojar (2018), our scores demonstrate that an increasing number of attention heads is beneficial for classification-based downstream tasks. All models perform best with more than one attention head and the gene"
2020.cl-2.5,C04-1051,0,0.198548,"Missing"
2020.cl-2.5,P15-1166,0,0.609206,"e translation (Firat et al. 2016; Lakew, Cettolo, and Federico 2018; Wang et al. 2018). Inasmuch as MT is described as the task of translating a sentence from one language to another, at the recent conferences on MT (WMT18 and WMT19)1 much interest was put on multilingualism, where a sub-track on multilingual systems was introduced with the aim of exploiting a third language to improve a bilingual model. Multilingual neural machine translation comes in many flavors with different architectures and ways of sharing parameters (Luong et al. 2016; Zoph and Knight 2016; Lee, Cho, and Hofmann 2017; Dong et al. 2015; Firat, Cho, and Bengio 2016; Lu et al. 2018; Blackwood, Ballesteros, and Ward 2018). The main motivation of multilingual models is the effect of transfer learning that enables machine translation systems to benefit from relationships between languages and training signals that come from different data sets. Common techniques explore multisource encoders, multitarget decoders, or combinations of both. Multilingual models can push the translation performance of low-resource language pairs but also enable the translation between unseen language pairs, so-called zero-shot translation (Ha, Niehue"
2020.cl-2.5,D18-1457,0,0.035809,"Missing"
2020.cl-2.5,W16-3210,0,0.0730558,"Missing"
2020.cl-2.5,N16-1101,0,0.0175494,"encode linguistic properties. We carefully analyze the information that is captured by individual attention heads and identify interesting patterns that explain the performance of specific settings in linguistic probing tasks. 1. Introduction Neural machine translation (NMT) has rapidly become the new machine translation (MT) standard, significantly improving over the traditional statistical machine translation model (Bojar et al. 2018). In only about four years, several architectures and approaches have been proposed, with increasing research efforts toward multilingual machine translation (Firat et al. 2016; Lakew, Cettolo, and Federico 2018; Wang et al. 2018). Inasmuch as MT is described as the task of translating a sentence from one language to another, at the recent conferences on MT (WMT18 and WMT19)1 much interest was put on multilingualism, where a sub-track on multilingual systems was introduced with the aim of exploiting a third language to improve a bilingual model. Multilingual neural machine translation comes in many flavors with different architectures and ways of sharing parameters (Luong et al. 2016; Zoph and Knight 2016; Lee, Cho, and Hofmann 2017; Dong et al. 2015; Firat, Cho, an"
2020.cl-2.5,D16-1026,0,0.0190263,"encode linguistic properties. We carefully analyze the information that is captured by individual attention heads and identify interesting patterns that explain the performance of specific settings in linguistic probing tasks. 1. Introduction Neural machine translation (NMT) has rapidly become the new machine translation (MT) standard, significantly improving over the traditional statistical machine translation model (Bojar et al. 2018). In only about four years, several architectures and approaches have been proposed, with increasing research efforts toward multilingual machine translation (Firat et al. 2016; Lakew, Cettolo, and Federico 2018; Wang et al. 2018). Inasmuch as MT is described as the task of translating a sentence from one language to another, at the recent conferences on MT (WMT18 and WMT19)1 much interest was put on multilingualism, where a sub-track on multilingual systems was introduced with the aim of exploiting a third language to improve a bilingual model. Multilingual neural machine translation comes in many flavors with different architectures and ways of sharing parameters (Luong et al. 2016; Zoph and Knight 2016; Lee, Cho, and Hofmann 2017; Dong et al. 2015; Firat, Cho, an"
2020.cl-2.5,L18-1550,0,0.0210836,"her one. In order to make this work, one essentially needs to produce crosslingual sentence representations that are useful for the task in all test languages. Table 6 summarizes the results obtained for different settings. We rely on our multilingual attention bridge model trained in a many-to-many fashion. For comparison, we include representations obtained from large pretrained word embeddings. Note that those embeddings are trained on vastly more data than our model, which is trained on the parallel Europarl corpus. In particular, we use the multilingual word embeddings from the fastText (Grave et al. 2018) and the MUSE (Conneau et al. 2018b) libraries. The fastText algorithm is based on word2vec and produces word embeddings compounded from character n-grams (Bojanowski et al. 2017), which is to be preferred for morphologically richer languages in a multilingual setting. The fastText word vectors are pretrained in CommonCrawl and Wikipedia using CBOW with position weights, whereas MUSE word embeddings are Wikipedia fastText vectors from 30 languages aligned in a supervised way into a single vector space. Because fastText vectors are not aligned into the same space we only present the accuracies"
2020.cl-2.5,N18-1032,0,0.362159,"2018; Blackwood, Ballesteros, and Ward 2018). The main motivation of multilingual models is the effect of transfer learning that enables machine translation systems to benefit from relationships between languages and training signals that come from different data sets. Common techniques explore multisource encoders, multitarget decoders, or combinations of both. Multilingual models can push the translation performance of low-resource language pairs but also enable the translation between unseen language pairs, so-called zero-shot translation (Ha, Niehues, and Waibel 2016; Johnson et al. 2017; Gu et al. 2018a). The effective computation of sentence representations using the translation task as an auxiliary semantic signal has also drawn interest to MT models (Hill, Cho, and Korhonen 2016; McCann et al. 2017; Schwenk and Douze 2017; Subramanian et al. 2018). Indeed, recent work makes use of machine translation models to capture syntactic and semantic properties of the input sentences, later to be used for learning generalpurpose sentence representations (Shi, Padhi, and Knight 2016; Belinkov et al. 2017; Dalvi et al. 2017; Poliak et al. 2018; Bau et al. 2019). An important feature that enables an"
2020.cl-2.5,N16-1162,0,0.0548009,"Missing"
2020.cl-2.5,E17-2068,0,0.0323529,"Missing"
2020.cl-2.5,P17-4012,0,0.0416879,".92 30.27 57.87 40.17 – 19.50 26.46 37.30 22.13 – 22.80 50.41 25.96 20.09 – M ↔ M + MONOLINGUAL EN DE CS FR – 41.82 39.58 50.94 38.48 – 31.51 35.25 30.47 26.90 – 28.80 57.35 41.49 40.87 – model input and output, the word embeddings have dimension dx = dy = 512. We use an attention bridge layer with k = 10 attention heads with dw = 1, 024, dimensions of W1 and W2 from Equation (8). We use a stochastic gradient descent optimizer with a learning rate of 1.0 and batch size 64, and for each experiment, we select the best model on the development set. We implement our model on top of an OpenNMT-py (Klein et al. 2017) fork, which we make available for reproducibility purposes.4 3.1.1 Baselines. The first experiment we conduct is to corroborate that the proposed architecture works correctly, and we assess performance in a bilingual setting. We expect that the models slightly drop in performance when the fixed-size attention bridge is introduced, because there are no direct crosslingual attention links between the source and target languages. However, we want to see whether the architecture is robust enough to carry over the essential information needed for translation with the inclusion of the additional in"
2020.cl-2.5,2005.mtsummit-papers.11,0,0.0739544,"ngual and multilingual scenario as we will see in our experiments in Sections 3.2 and 4. 3. Translation Quality Before applying and analyzing sentence representations that can be learned with the proposed architecture from the previous section, we ought to verify that the model is indeed capable of learning multilingual translation—the original training objective. For this, we apply the model in two scenarios: a low-resource scenario with a multilingual image caption translation task (Elliott et al. 2016) and the application to considerably larger data sets based on experiments with Europarl (Koehn 2005) and news translation tasks (Callison-Burch et al. 2007). In the following we will first discuss multilingual transfer learning in the low-resource scenario before we analyze the effect of the attention bridge size on translation quality in the large-data setting. 3.1 Multilingual Translation of Image Captions Multi30K (Elliott et al. 2016) is a parallel data set containing 29k image captions for training and 1k sentences for validation in four European languages; Czech (cs), German (de), French (fr), and English (en). We test the trained model with the flickr 2016 test data of the same data s"
2020.cl-2.5,P15-1094,0,0.0243121,"ir sentence representation, whereas we use the attention bridge layer. The model by Cífka and Bojar (2018) is based on a very similar architecture as ours, but they train on bilingual data, 57 million English–Czech sentence pairs. We train on a considerably smaller, but multilingual, data set (3 times 2 million sentence pairs of EN–FR, EN–DE, and EN-ES). Yet our system outperforms theirs in six out of nine tasks listed in Tables 4 and 5. This again demonstrates the power of multilingual models. In futher comparisons, we can see that our model outperforms the competitive baseline of GloVe-BOW (Kruszewski et al. 2015; Arora, Liang, and Ma 2017; Adi et al. 2017) in five tasks out of ten. However, Conneau et al. (2018a) and Subramanian et al. (2018) perform better than us in all the classification and NLI tasks. We believe that the strong performance of the latter models is explained by orders of magnitudes of more training data. GloVe-BOW and the Conneau et al. (2018a) model are based on word embeddings, which have been pretrained on several billions of words of text. The large vocabularies of the pretrained embeddings provide better representations for low-frequency as well as out-of-vocabulary words. Sub"
2020.cl-2.5,Q17-1026,0,0.116955,"Missing"
2020.cl-2.5,W18-6309,0,0.0866066,"o, and Federico 2018; Wang et al. 2018). Inasmuch as MT is described as the task of translating a sentence from one language to another, at the recent conferences on MT (WMT18 and WMT19)1 much interest was put on multilingualism, where a sub-track on multilingual systems was introduced with the aim of exploiting a third language to improve a bilingual model. Multilingual neural machine translation comes in many flavors with different architectures and ways of sharing parameters (Luong et al. 2016; Zoph and Knight 2016; Lee, Cho, and Hofmann 2017; Dong et al. 2015; Firat, Cho, and Bengio 2016; Lu et al. 2018; Blackwood, Ballesteros, and Ward 2018). The main motivation of multilingual models is the effect of transfer learning that enables machine translation systems to benefit from relationships between languages and training signals that come from different data sets. Common techniques explore multisource encoders, multitarget decoders, or combinations of both. Multilingual models can push the translation performance of low-resource language pairs but also enable the translation between unseen language pairs, so-called zero-shot translation (Ha, Niehues, and Waibel 2016; Johnson et al. 2017; Gu e"
2020.cl-2.5,D15-1166,0,0.162509,"Missing"
2020.cl-2.5,marelli-etal-2014-sick,0,0.0255895,"ilarity, and linguistic probing tasks. In the classification tasks, a classifier is trained on top of a sentence embedding involving various data sets: CR—product reviews (Hu and Liu 2004), MR—movie reviews (Pang and Lee 2005), MPQA—opinion polarity (Wiebe, Wilson, and Cardie 2005), SUBJ—subjectivity/objectivity status (Pang and Lee 2004), SST—binary and fine-grained sentiment analysis (Socher et al. 2013), TREC—questiontype classification (Voorhees and Tice 2000), MRPC—paraphrase detection (Dolan, Quirk, and Brockett 2004), and SICK and SNLI—textual entailment and natural language inference (Marelli et al. 2014; Bowman et al. 2015). In contrast to the classification tasks mentioned above, the similarity tasks do not involve any training and, instead, correlate the cosine distance between two sentence representations with a human labeled score using Pearson and Spearman coefficients. The data sets come from the SemEval Semantic Textual Similarity (STS) task series, from 2012 to 2016 (Agirre et al. 2012, 2013, 2014, 2015, 2016). The only exceptions are the SICK and STSB data set (Marelli et al. 2014; Cer et al. 2017), where training data are provided. In addition, the SentEval toolkit contains probing"
2020.cl-2.5,P04-1035,0,0.0241996,"ive. To test this claim, we apply standard benchmarks collected in the SentEval toolkit (Conneau and Kiela 2018), the XNLI evaluation corpus (Conneau et al. 2018c), as well as the Yelp challenge data set.8 The SentEval toolkit contains three benchmark types: classification, similarity, and linguistic probing tasks. In the classification tasks, a classifier is trained on top of a sentence embedding involving various data sets: CR—product reviews (Hu and Liu 2004), MR—movie reviews (Pang and Lee 2005), MPQA—opinion polarity (Wiebe, Wilson, and Cardie 2005), SUBJ—subjectivity/objectivity status (Pang and Lee 2004), SST—binary and fine-grained sentiment analysis (Socher et al. 2013), TREC—questiontype classification (Voorhees and Tice 2000), MRPC—paraphrase detection (Dolan, Quirk, and Brockett 2004), and SICK and SNLI—textual entailment and natural language inference (Marelli et al. 2014; Bowman et al. 2015). In contrast to the classification tasks mentioned above, the similarity tasks do not involve any training and, instead, correlate the cosine distance between two sentence representations with a human labeled score using Pearson and Spearman coefficients. The data sets come from the SemEval Semanti"
2020.cl-2.5,P05-1015,0,0.110226,"tilinguality contributes to a higher level of semantic abstraction that can be learned from the translation objective. To test this claim, we apply standard benchmarks collected in the SentEval toolkit (Conneau and Kiela 2018), the XNLI evaluation corpus (Conneau et al. 2018c), as well as the Yelp challenge data set.8 The SentEval toolkit contains three benchmark types: classification, similarity, and linguistic probing tasks. In the classification tasks, a classifier is trained on top of a sentence embedding involving various data sets: CR—product reviews (Hu and Liu 2004), MR—movie reviews (Pang and Lee 2005), MPQA—opinion polarity (Wiebe, Wilson, and Cardie 2005), SUBJ—subjectivity/objectivity status (Pang and Lee 2004), SST—binary and fine-grained sentiment analysis (Socher et al. 2013), TREC—questiontype classification (Voorhees and Tice 2000), MRPC—paraphrase detection (Dolan, Quirk, and Brockett 2004), and SICK and SNLI—textual entailment and natural language inference (Marelli et al. 2014; Bowman et al. 2015). In contrast to the classification tasks mentioned above, the similarity tasks do not involve any training and, instead, correlate the cosine distance between two sentence representatio"
2020.cl-2.5,D14-1162,0,0.090548,"Missing"
2020.cl-2.5,D18-1039,0,0.0178285,"e, it does not provide a straightforward meaning representation that can be used for downstream tasks. Sentence meaning representation has also been vastly studied under NMT settings. When introducing the encoder–decoder architectures for MT, Sutskever, Vinyals, and Le (2014) showed that seq2seq models are better at encoding the meaning of sentences into vector spaces than the bag-of-words model. Recent work includes that of Schwenk and Douze (2017), who used multiple encoders and decoders that are connected through a shared layer, albeit with a different purpose than performing translations; Platanios et al. (2018) showed an intermediate representation that can be decoded to any target language while describing a parameter generation method for universal NMT; Britz, Guan, and Luong (2017) made a computational efficiency analysis for MT using a fixed-size attention layer; Artetxe and Schwenk (2019) used a shared LSTM with max pooling to learn sentence embeddings on 93 translation directions; Cífka and Bojar (2018) introduced an architecture with a self-attentive layer to extract sentence meaning representations of fixed size. Here we use a similar architecture but in a multilingual setting. Our work on m"
2020.cl-2.5,N18-2082,0,0.0505223,"Missing"
2020.cl-2.5,W18-6319,0,0.0158067,"). In the following we will first discuss multilingual transfer learning in the low-resource scenario before we analyze the effect of the attention bridge size on translation quality in the large-data setting. 3.1 Multilingual Translation of Image Captions Multi30K (Elliott et al. 2016) is a parallel data set containing 29k image captions for training and 1k sentences for validation in four European languages; Czech (cs), German (de), French (fr), and English (en). We test the trained model with the flickr 2016 test data of the same data set and obtain BLEU scores using the sacreBLEU script3 (Post 2018). The preprocessing pipeline consists of lowercasing, normalizing, and tokenizing using the scripts provided in the Moses decoder (Koehn et al. 2007), together with learning and applying a 10k operations byte-pair-encoding (BPE) model per language (Sennrich, Haddow, and Birch 2016). Each encoder consists of two stacked BiLSTMs of size dh = 512 (i.e., the hidden states per direction are of size 256). Each decoder is composed of two stacked unidirectional LSTMs with hidden states of size 512. For the 3 With signature BLEU+case.lc+numrefs.1+smooth.exp+tok.13a+version.1.2.11. 392 Vázquez et al. A"
2020.cl-2.5,D19-1410,0,0.0268331,"Missing"
2020.cl-2.5,P18-2037,0,0.0196177,"portant feature that enables an immediate use of the MT-based representations in other downstream tasks is the effective reduction to a fixed-sized vector; it enables functionality, at the expense of hampering the performance in the MT task (Britz, Guan, and Luong 2017; Cífka and Bojar 2018). However, it is not fully clear how the properties of the fixed-sized vector influence the tradeoff between the performance of the model in MT and the information it encodes as a meaning representation vector. Recent studies either focus on the usage of such MT-based vector representations in other tasks (Schwenk 2018), on translation quality (Lu et al. 2018), on speed comparison (Britz, Guan, and Luong 2017), or only explore a bilingual scenario (Cífka and Bojar 2018). For this study, we focus on exploring a crosslingual intermediate shared layer in an MT model. We apply an architecture based on shared inner-attention with 1 http://www.statmt.org/wmt18/translation-task.html. http://www.statmt.org/wmt19/translation-task.html. 388 Vázquez et al. A Systematic Study of Inner-Attention-Based Sentence Representations in NMT language-specific encoders and decoders that can easily scale to a large number of langua"
2020.cl-2.5,W17-2619,0,0.340354,"aining signals that come from different data sets. Common techniques explore multisource encoders, multitarget decoders, or combinations of both. Multilingual models can push the translation performance of low-resource language pairs but also enable the translation between unseen language pairs, so-called zero-shot translation (Ha, Niehues, and Waibel 2016; Johnson et al. 2017; Gu et al. 2018a). The effective computation of sentence representations using the translation task as an auxiliary semantic signal has also drawn interest to MT models (Hill, Cho, and Korhonen 2016; McCann et al. 2017; Schwenk and Douze 2017; Subramanian et al. 2018). Indeed, recent work makes use of machine translation models to capture syntactic and semantic properties of the input sentences, later to be used for learning generalpurpose sentence representations (Shi, Padhi, and Knight 2016; Belinkov et al. 2017; Dalvi et al. 2017; Poliak et al. 2018; Bau et al. 2019). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the effective reduction to a fixed-sized vector; it enables functionality, at the expense of hampering the performance in the MT task (Britz, Guan, and"
2020.cl-2.5,P16-1162,0,0.146448,"Missing"
2020.cl-2.5,D16-1159,0,0.0504823,"Missing"
2020.cl-2.5,D13-1170,0,0.0155322,"Missing"
2020.cl-2.5,P15-1150,0,0.140731,"Missing"
2020.cl-2.5,Q17-1007,0,0.0251889,"and the quality of the translations produced. We could also see that the attention bridge model is capable of translating with a similar performance even though it creates an additional bottleneck of fixed-size representations. Nevertheless, the performance drops slightly and, in this section, we would like to investigate the reasons for that drop by looking at the effect on different subsets of the test data. One of the main motivations for having more attention heads lies in the better support of longer sentences. To study the effect, following previous work (Bahdanau, Cho, and Bengio 2015; Tu et al. 2017; Dou et al. 2018), we group sentences of similar length and compute the BLEU score for each group. As we can see from Figure 3, a larger number of attention heads has, indeed, a positive impact when translating longer sentences. Long sentences do require a bigger attention bridge, and it affects both bilingual and multilingual models. Interestingly enough, on sentences with up to 45 words, there is no real gap between the results of the baseline model and our bridge models with a high number of attention heads. It looks like the performance drop of the attention bridge models is entirely due"
2020.cl-2.5,D18-1326,0,0.0329228,"Missing"
2020.cl-2.5,N16-1004,0,0.146658,"easing research efforts toward multilingual machine translation (Firat et al. 2016; Lakew, Cettolo, and Federico 2018; Wang et al. 2018). Inasmuch as MT is described as the task of translating a sentence from one language to another, at the recent conferences on MT (WMT18 and WMT19)1 much interest was put on multilingualism, where a sub-track on multilingual systems was introduced with the aim of exploiting a third language to improve a bilingual model. Multilingual neural machine translation comes in many flavors with different architectures and ways of sharing parameters (Luong et al. 2016; Zoph and Knight 2016; Lee, Cho, and Hofmann 2017; Dong et al. 2015; Firat, Cho, and Bengio 2016; Lu et al. 2018; Blackwood, Ballesteros, and Ward 2018). The main motivation of multilingual models is the effect of transfer learning that enables machine translation systems to benefit from relationships between languages and training signals that come from different data sets. Common techniques explore multisource encoders, multitarget decoders, or combinations of both. Multilingual models can push the translation performance of low-resource language pairs but also enable the translation between unseen language pair"
2020.cl-2.5,Q17-1024,0,\N,Missing
2020.coling-main.575,P17-1067,0,0.326772,"iew data such as, e.g. Amazon product reviews, or movie reviews (Blitzer et al., 2007; Maas et al., 2011; Turney, 2002). Many, if not most, emotion datasets on the other hand use Twitter as a source and individual tweets as 1 https://github.com/Helsinki-NLP/XED This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 6542 Proceedings of the 28th International Conference on Computational Linguistics, pages 6542–6552 Barcelona, Spain (Online), December 8-13, 2020 level of granularity (Schuff et al., 2017; Abdul-Mageed and Ungar, 2017; Mohammad et al., 2018). In the case of emotion datasets, the emotion taxonomies used are often based on Ekman (1971) and Plutchik (1980) (which is partially based on Ekman). 2.1 Existing Emotion Datasets Bostan and Klinger (2018) analyze 14 existing emotion datasets of which only two are multilabel. These are AffectiveText (Strapparava and Mihalcea, 2007) and SSEC (Schuff et al., 2017). Nearly all of these datasets use an annotation scheme based on Ekman (Ekman, 1971; Ekman, 1992) with many adding a few labels often following Plutchik’s theory of emotions (Plutchik, 1980). A typical emotion"
2020.coling-main.575,Q16-1022,0,0.0351925,"Missing"
2020.coling-main.575,H05-1073,0,0.349937,"Missing"
2020.coling-main.575,2020.lrec-1.467,1,0.720878,"(FI), French (FR), German (DE), Greek (EL), Hebrew (HE), Hungarian 6546 Finnish data. We have made all 32 datasets available on GitHub plus the raw data for all 43 languages including the 11 datasets that had fewer than 950 lines. IT 10,582 FI 11,128 FR 11,503 CS 11,885 PT 12,559 PL 12,836 SR 14,831 TR 15,712 EL 15,713 RO 16,217 ES 16,608 PT BR 22,194 Table 4: Languages (ISO code) with over 10k parallel sentences with our annotated English data. To test how well our data is suited for emotion projection, we projected the English annotations onto our Finnish unannotated data using OPUS tools (Aulamo et al., 2020). We chose Finnish as our main test language as we also have some annotated data for it to use as a test set. The manually annotated Finnish data consists of nearly 20k individual annotations and almost 15k unique annotated sentences plus an additional 7,536 sentences annotated as neutral 6 . The criteria for the inclusion of an annotation was the same as for English. The distribution of the number of labels and the labels themselves are quite similar to that of the English data. Relatively speaking there is a little less anticipation in the Finnish data, but anger is the biggest category in b"
2020.coling-main.575,P07-1056,0,0.203924,"s and emotion detection such as offensive language identification. The data is open source1 licensed under a Creative Commons Attribution 4.0 International License (CC-BY). In the following sections we discuss related work and describe our datasets. The datasets are then evaluated and the results discussed in the discussion section. 2 Background & Previous Work Datasets created for sentiment analysis have been available for researchers since at least the early 2000s (M¨antyl¨a et al., 2018). Such datasets generally use a binary or ternary annotation scheme (positive, negative + neutral) (e.g. Blitzer et al. (2007)) and have traditionally been based on review data such as, e.g. Amazon product reviews, or movie reviews (Blitzer et al., 2007; Maas et al., 2011; Turney, 2002). Many, if not most, emotion datasets on the other hand use Twitter as a source and individual tweets as 1 https://github.com/Helsinki-NLP/XED This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 6542 Proceedings of the 28th International Conference on Computational Linguistics, pages 6542–6552 Barcelona, Spain (Online), December 8-13, 2020 l"
2020.coling-main.575,C18-1179,0,0.0178986,"github.com/Helsinki-NLP/XED This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 6542 Proceedings of the 28th International Conference on Computational Linguistics, pages 6542–6552 Barcelona, Spain (Online), December 8-13, 2020 level of granularity (Schuff et al., 2017; Abdul-Mageed and Ungar, 2017; Mohammad et al., 2018). In the case of emotion datasets, the emotion taxonomies used are often based on Ekman (1971) and Plutchik (1980) (which is partially based on Ekman). 2.1 Existing Emotion Datasets Bostan and Klinger (2018) analyze 14 existing emotion datasets of which only two are multilabel. These are AffectiveText (Strapparava and Mihalcea, 2007) and SSEC (Schuff et al., 2017). Nearly all of these datasets use an annotation scheme based on Ekman (Ekman, 1971; Ekman, 1992) with many adding a few labels often following Plutchik’s theory of emotions (Plutchik, 1980). A typical emotion dataset consists of 6-8 categories. The exception Bostan and Klinger (2018) mention is CrowdFlower2 with 14 categories, and those not mentioned in Bostan et al. are e.g. the SemEval 2018 task 1 subtask c dataset (Mohammad et al., 2"
2020.coling-main.575,L18-1218,0,0.165712,"in a multitude of situations similar to many social media platforms. Because OPUS open subtitles is a parallel corpus we are able to evaluate our annotated datasets across languages and at identical levels of granularity. Although the subtitles might be translated using different translation philosophies (favoring e.g. meaning, mood, or idiomatic language as the prime objective) (Carl et al., 2011), we expect the translations to have aimed at capturing the sentiments and emotions originally expressed in the film based on previous studies (e.g. Cowen et al. (2019), Scherer and Wallbott (1994), Creutz (2018), Scherrer (2020) and Kajava et al. (2020)). 3.2 Data Annotation The vast majority of the dataset was annotated by university students learning about sentiment analysis ¨ with some annotations provided by expert annotators for reliability measurements (Ohman et al., 2018). The students’ annotation process was monitored and evaluated. They received only minimal instructions. These instructions included that they were to focus on the quality of annotations rather than quantity, and to annotate from the point of view of the speaker. We also asked for feedback on the annotation process to improve"
2020.coling-main.575,2020.acl-main.372,0,0.398779,"ion. We use Plutchik’s Wheel of Emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) (Plutchik, 1980) as our annotation scheme with the addition of neutral on movie subtitle data from OPUS (Lison and Tiedemann, 2016). We perform evaluations with fine-tuned cased multilingual and language specific BERT (Bidirectional Encoder Representations from Transformers) models (Devlin et al., 2019), as well as Suport Vector Machines (SVMs). Our evaluations show that the human-annotated datasets behave on par with comparable state-of-the-art datasets such as the GoEmotions dataset (Demszky et al., 2020). Furthermore, the projected datasets have accuracies that closely resemble human-annotated data with macro f1 scores of 0.51 for the human annotated Finnish data and 0.45 for the projected Finnish data when evaluating with FinBERT (Virtanen et al., 2019). The XED dataset can be used in emotion classification tasks and other applications that can benefit from sentiment analysis and emotion detection such as offensive language identification. The data is open source1 licensed under a Creative Commons Attribution 4.0 International License (CC-BY). In the following sections we discuss related wor"
2020.coling-main.575,N19-1423,0,0.0432074,"ntiment dataset. The dataset consists of parallel manually annotated data for English and Finnish, with additional parallel datasets of varying sizes for a total of 32 languages created by annotation projection. We use Plutchik’s Wheel of Emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) (Plutchik, 1980) as our annotation scheme with the addition of neutral on movie subtitle data from OPUS (Lison and Tiedemann, 2016). We perform evaluations with fine-tuned cased multilingual and language specific BERT (Bidirectional Encoder Representations from Transformers) models (Devlin et al., 2019), as well as Suport Vector Machines (SVMs). Our evaluations show that the human-annotated datasets behave on par with comparable state-of-the-art datasets such as the GoEmotions dataset (Demszky et al., 2020). Furthermore, the projected datasets have accuracies that closely resemble human-annotated data with macro f1 scores of 0.51 for the human annotated Finnish data and 0.45 for the projected Finnish data when evaluating with FinBERT (Virtanen et al., 2019). The XED dataset can be used in emotion classification tasks and other applications that can benefit from sentiment analysis and emotion"
2020.coling-main.575,P05-1045,0,0.0264114,"re extracted from the database, the data needed to be cleaned up. The different evaluations required different pre-processing steps. Most commonly, this included the removal of superfluous characters containing no information. We tried to keep as much of the original information as possible, including keeping offensive, racist, and sexist language as is. If such information is removed, the usefulness of the data is at risk of being reduced, particularly when used for e.g. offensive language detection (P`amies et al., 2020). For the English data we used Stanford NER (named entity recognition) (Finkel et al., 2005) to replace names and locations with the tags: [PERSON] and [LOCATION] respectively. We kept organization names as is because we felt that the emotions and sentiments towards some large well-known organizations differ too much (cf. IRS, FBI, WHO, EU, and MIT). For the Finnish data, we replaced names and locations using the Turku NER corpus (Luoma et al., 2020). Some minor text cleanup was also conducted, removing hyphens and quotations marks, and correcting erroneous renderings of characters (usually encoding issues) where possible. 3.4 English Dataset Description The final dataset contained 1"
2020.coling-main.575,L16-1147,1,0.92033,"d datasets for machine learning. This is true for English as well as other, often under-resourced, languages. We provide a cross-lingual fine-grained sentence-level emotion and sentiment dataset. The dataset consists of parallel manually annotated data for English and Finnish, with additional parallel datasets of varying sizes for a total of 32 languages created by annotation projection. We use Plutchik’s Wheel of Emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) (Plutchik, 1980) as our annotation scheme with the addition of neutral on movie subtitle data from OPUS (Lison and Tiedemann, 2016). We perform evaluations with fine-tuned cased multilingual and language specific BERT (Bidirectional Encoder Representations from Transformers) models (Devlin et al., 2019), as well as Suport Vector Machines (SVMs). Our evaluations show that the human-annotated datasets behave on par with comparable state-of-the-art datasets such as the GoEmotions dataset (Demszky et al., 2020). Furthermore, the projected datasets have accuracies that closely resemble human-annotated data with macro f1 scores of 0.51 for the human annotated Finnish data and 0.45 for the projected Finnish data when evaluating"
2020.coling-main.575,D19-1656,0,0.0329551,"Missing"
2020.coling-main.575,2020.lrec-1.567,0,0.181833,"Missing"
2020.coling-main.575,P11-1015,0,0.103718,"ional License (CC-BY). In the following sections we discuss related work and describe our datasets. The datasets are then evaluated and the results discussed in the discussion section. 2 Background & Previous Work Datasets created for sentiment analysis have been available for researchers since at least the early 2000s (M¨antyl¨a et al., 2018). Such datasets generally use a binary or ternary annotation scheme (positive, negative + neutral) (e.g. Blitzer et al. (2007)) and have traditionally been based on review data such as, e.g. Amazon product reviews, or movie reviews (Blitzer et al., 2007; Maas et al., 2011; Turney, 2002). Many, if not most, emotion datasets on the other hand use Twitter as a source and individual tweets as 1 https://github.com/Helsinki-NLP/XED This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 6542 Proceedings of the 28th International Conference on Computational Linguistics, pages 6542–6552 Barcelona, Spain (Online), December 8-13, 2020 level of granularity (Schuff et al., 2017; Abdul-Mageed and Ungar, 2017; Mohammad et al., 2018). In the case of emotion datasets, the emotion taxon"
2020.coling-main.575,S18-1001,0,0.0305534,"product reviews, or movie reviews (Blitzer et al., 2007; Maas et al., 2011; Turney, 2002). Many, if not most, emotion datasets on the other hand use Twitter as a source and individual tweets as 1 https://github.com/Helsinki-NLP/XED This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 6542 Proceedings of the 28th International Conference on Computational Linguistics, pages 6542–6552 Barcelona, Spain (Online), December 8-13, 2020 level of granularity (Schuff et al., 2017; Abdul-Mageed and Ungar, 2017; Mohammad et al., 2018). In the case of emotion datasets, the emotion taxonomies used are often based on Ekman (1971) and Plutchik (1980) (which is partially based on Ekman). 2.1 Existing Emotion Datasets Bostan and Klinger (2018) analyze 14 existing emotion datasets of which only two are multilabel. These are AffectiveText (Strapparava and Mihalcea, 2007) and SSEC (Schuff et al., 2017). Nearly all of these datasets use an annotation scheme based on Ekman (Ekman, 1971; Ekman, 1992) with many adding a few labels often following Plutchik’s theory of emotions (Plutchik, 1980). A typical emotion dataset consists of 6-8"
2020.coling-main.575,W18-6205,1,0.832829,"he combined number of annotations (22,424) differ because once the dataset was saved as a Python dictionary, identical lines were merged as one (i.e. some common movie lines like ”All right then!” and ”I love you” appeared multiple times from different sources). 4 A sentence could have been annotated as containing 3 different emotions by one or more annotators. This would count as 3 annotations on one unique data point. 6544 We used Plutchik’s core emotions as our annotation scheme resulting in 8 distinct emotion categories ¨ ¨ plus neutral. The Sentimentator platform (Ohman and Kajava, 2018; Ohman et al., 2018) allows for the annotation of intensities resulting in what is essentially 30 emotions and sentiments, however, as the intensity score is not available for all annotations, the intensity scores were discarded. The granularity of our annotations roughly correspond to sentence-level annotations, although as our source data is movie subtitles, our shortest subtitle is ! and the longest subtitle consists of three separate sentences. Number of annotations: Number of unique data points: Number of emotions: Number of annotators: Number of labels per data point: 24,164 + 9,384 neutral 17,520 + 6,420 n"
2020.coling-main.575,2020.semeval-1.205,1,0.754847,"Missing"
2020.coling-main.575,2020.lrec-1.848,0,0.0398448,"of situations similar to many social media platforms. Because OPUS open subtitles is a parallel corpus we are able to evaluate our annotated datasets across languages and at identical levels of granularity. Although the subtitles might be translated using different translation philosophies (favoring e.g. meaning, mood, or idiomatic language as the prime objective) (Carl et al., 2011), we expect the translations to have aimed at capturing the sentiments and emotions originally expressed in the film based on previous studies (e.g. Cowen et al. (2019), Scherer and Wallbott (1994), Creutz (2018), Scherrer (2020) and Kajava et al. (2020)). 3.2 Data Annotation The vast majority of the dataset was annotated by university students learning about sentiment analysis ¨ with some annotations provided by expert annotators for reliability measurements (Ohman et al., 2018). The students’ annotation process was monitored and evaluated. They received only minimal instructions. These instructions included that they were to focus on the quality of annotations rather than quantity, and to annotate from the point of view of the speaker. We also asked for feedback on the annotation process to improve the user-friendli"
2020.coling-main.575,W17-5203,0,0.036229,"Missing"
2020.coling-main.575,S07-1013,0,0.45269,"details: http://creativecommons.org/licenses/by/4.0/. 6542 Proceedings of the 28th International Conference on Computational Linguistics, pages 6542–6552 Barcelona, Spain (Online), December 8-13, 2020 level of granularity (Schuff et al., 2017; Abdul-Mageed and Ungar, 2017; Mohammad et al., 2018). In the case of emotion datasets, the emotion taxonomies used are often based on Ekman (1971) and Plutchik (1980) (which is partially based on Ekman). 2.1 Existing Emotion Datasets Bostan and Klinger (2018) analyze 14 existing emotion datasets of which only two are multilabel. These are AffectiveText (Strapparava and Mihalcea, 2007) and SSEC (Schuff et al., 2017). Nearly all of these datasets use an annotation scheme based on Ekman (Ekman, 1971; Ekman, 1992) with many adding a few labels often following Plutchik’s theory of emotions (Plutchik, 1980). A typical emotion dataset consists of 6-8 categories. The exception Bostan and Klinger (2018) mention is CrowdFlower2 with 14 categories, and those not mentioned in Bostan et al. are e.g. the SemEval 2018 task 1 subtask c dataset (Mohammad et al., 2018) with 11 categories, EmoNet with 24 (Abdul-Mageed and Ungar, 2017), and the GoEmotions dataset (Demszky et al., 2020) with 2"
2020.coling-main.575,C08-1111,0,0.299507,"e of the most significant emotion datasets in relation to this study. The table lists the paper in which the dataset was released (study), what the source data that was used was (source), what model was used to obtain the best evaluation scores (model), the number of categories used for annotation (cat), whether the system was multilabel or not (multi), and the macro f1 scores and accuracy score as reported by the paper (macro f1 and accuracy respectively). Some papers only reported a micro f1 and no macro f1 score. These scores have been marked with a µ. study Strapparava and Mihalcea (2007) Tokuhisa et al. (2008) Schuff et al. (2017) Abdul-Mageed and Ungar (2017) Abdul-Mageed and Ungar (2017) Samy et al. (2018) Yu et al. (2018) Jabreel and Moreno (2019) Huang et al. (2019) Liu et al. (2019) Demszky et al. (2020) Demszky et al. (2020) XED (English) source news headlines JP web corpus Twitter Twitter Twitter Twitter Twitter Twitter Twitter books etc Reddit Reddit movie subtitles model ”CLaC system” k-NN BiLSTM GRNNs GRNNs C-GRU DATN BiRNN/GRU BiLSTM/ELMO BERT BERT BERT BERT cat 6+val 10 8 6 24 11+neu 11 11 11 8+neu 27 6 8+neu multi yes no yes no no yes yes yes yes no yes no yes macro f1 N/A N/A N/A N/A"
2020.coling-main.575,P02-1053,0,0.0175097,"Y). In the following sections we discuss related work and describe our datasets. The datasets are then evaluated and the results discussed in the discussion section. 2 Background & Previous Work Datasets created for sentiment analysis have been available for researchers since at least the early 2000s (M¨antyl¨a et al., 2018). Such datasets generally use a binary or ternary annotation scheme (positive, negative + neutral) (e.g. Blitzer et al. (2007)) and have traditionally been based on review data such as, e.g. Amazon product reviews, or movie reviews (Blitzer et al., 2007; Maas et al., 2011; Turney, 2002). Many, if not most, emotion datasets on the other hand use Twitter as a source and individual tweets as 1 https://github.com/Helsinki-NLP/XED This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 6542 Proceedings of the 28th International Conference on Computational Linguistics, pages 6542–6552 Barcelona, Spain (Online), December 8-13, 2020 level of granularity (Schuff et al., 2017; Abdul-Mageed and Ungar, 2017; Mohammad et al., 2018). In the case of emotion datasets, the emotion taxonomies used are"
2020.coling-main.575,H01-1035,0,0.0599703,"Missing"
2020.coling-main.575,D18-1137,0,0.04581,"Missing"
2020.eamt-1.13,R11-1014,0,0.435017,"Missing"
2020.eamt-1.13,P02-1040,0,0.109594,"Missing"
2020.eamt-1.13,J93-1004,0,0.12875,"hat there is a strong length correlation between the source- and targetlanguage subtitles. The difference to traditional sentence alignment is that we are now only interested in 1-to-n alignments, meaning that each existing subtitle frame in the original input should be filled with one or more segments from the translation. The segments on the target side that we consider are clauses from the generated sentences. For simplicity, we split on any punctuation in the output that is followed by space to approximate the structural segmentation. We then apply the traditional Gale & Church algorithm (Gale and Church, 1993) to optimise the global alignment between source segments (original subtitle frame data) and target segments. For this, we adjust the parameters of the algorithm in two ways: (i) we remove priors and apply a uniform distribution over possible alignment types, and (ii) we change the set of alignment types to include all possible mappings from one source segment to a maximum of four target segments. The mapping between source and target is then created using the original algorithm that ensures a globally optimal mapping according to the model (see Figure 1 for an example). Furthermore, we apply"
2020.eamt-1.13,W19-5321,0,0.0347014,"in the MarianNMT documentation. For text segmentation, we apply SentencePiece (Kudo and Richardson, 2018) with models that are trained independently for source and target languages for a vocabulary size of 32,000 in each language. We do not apply any further pre-processing to keep the setup as general as possible, apart from some basic normalisation of Unicode punctuation characters, and parallel corpus filtering using standard scripts from the Moses SMT package (Koehn et al., 2007). For the document-level models, we apply the concatenative models proposed by Tiedemann and Scherrer (2017) and Junczys-Dowmunt (2019) using units of a maximum length of 100 tokens. Note that sentences and sentence fragments in subtitles are typically very short, and 100 tokens typically cover substantial amounts of context beyond sentence boundaries. We mark sentence boundtles, PHP, QED, Tatoeba, TildeMODEL, Ubuntu, wikimedia 3 OPUS corpora used: bible-uedin, Books, DGT, ECB, EMEA, EUbookshop, EUconst, Europarl, GNOME, infopankki, JRC-Acquis, KDE4, OpenSubtitles, ParaCrawl, PHP, QED, Tatoeba, TildeMODEL, Ubuntu aries with special tokens, chunking the training and test data sequentially from the beginning to the end without"
2020.eamt-1.13,P18-4020,0,0.0358046,"Missing"
2020.eamt-1.13,P07-2045,0,0.00747155,"dropout, and use tied embeddings with a shared vocabulary, basically following the recommendations for training transformer models in the MarianNMT documentation. For text segmentation, we apply SentencePiece (Kudo and Richardson, 2018) with models that are trained independently for source and target languages for a vocabulary size of 32,000 in each language. We do not apply any further pre-processing to keep the setup as general as possible, apart from some basic normalisation of Unicode punctuation characters, and parallel corpus filtering using standard scripts from the Moses SMT package (Koehn et al., 2007). For the document-level models, we apply the concatenative models proposed by Tiedemann and Scherrer (2017) and Junczys-Dowmunt (2019) using units of a maximum length of 100 tokens. Note that sentences and sentence fragments in subtitles are typically very short, and 100 tokens typically cover substantial amounts of context beyond sentence boundaries. We mark sentence boundtles, PHP, QED, Tatoeba, TildeMODEL, Ubuntu, wikimedia 3 OPUS corpora used: bible-uedin, Books, DGT, ECB, EMEA, EUbookshop, EUconst, Europarl, GNOME, infopankki, JRC-Acquis, KDE4, OpenSubtitles, ParaCrawl, PHP, QED, Tatoeba"
2020.eamt-1.13,D18-2012,0,0.0346515,", the current state of the art in NMT. We apply the implementation from the MarianNMT toolkit (JunczysDowmunt et al., 2018), which offers fast training and decoding with the latest features of production-ready NMT. We use the common settings of a multi-layer transformer, with 6 layers on both the encoder and the decoder, and 8 attention heads in each layer. We enable label smoothing and dropout, and use tied embeddings with a shared vocabulary, basically following the recommendations for training transformer models in the MarianNMT documentation. For text segmentation, we apply SentencePiece (Kudo and Richardson, 2018) with models that are trained independently for source and target languages for a vocabulary size of 32,000 in each language. We do not apply any further pre-processing to keep the setup as general as possible, apart from some basic normalisation of Unicode punctuation characters, and parallel corpus filtering using standard scripts from the Moses SMT package (Koehn et al., 2007). For the document-level models, we apply the concatenative models proposed by Tiedemann and Scherrer (2017) and Junczys-Dowmunt (2019) using units of a maximum length of 100 tokens. Note that sentences and sentence fr"
2020.eamt-1.13,W19-5209,0,0.364228,"lator. The use of MT and PE has been found to increase productivity in various translation scenarios (e.g. Plitt and Masselot, 2010). However, this workflow appears less common in the field of audiovisual translation (AVT). For example, Bywood et al. (2017) note that while specialised subtitling software with various functionc 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. J¨org Tiedemann ∗ Yle {name.surname}@yle.fi alities are used, technologies like translation memory (TM) or MT have not been widely adopted in AVT. Matusov et al. (2019) suggest that a reason for the lower rate of MT adoption in the AVT field may be that current NMT systems are not suited for the particular features of subtitle translation. This paper presents a pilot study carried out in November 2019 examining how the use of MT and PE in the subtitling workflow affects the work and productivity of subtitlers. In the study, 12 professional subtitle translators worked on a series of tasks in four language pairs (Finnish→English, Finnish→Swedish, English→Finnish, and Swedish→Finnish). They created interlingual (translated) subtitles for short video clips both"
2020.eamt-1.13,2006.tc-1.10,0,0.597916,"Missing"
2020.eamt-1.13,2006.amta-papers.25,0,0.147561,"was pre-segmented and timed based on the intralingual subtitles used as source text for the MT (see Section 3.2). To assess productivity, the process logs were analysed using Inputlog’s analysis functions. The task time and the number of keystrokes logged were used as productivity measures. Using Inputlog filters, we focused only on task time and keystrokes in the subtitling software, excluding other activity such as internet searches for terminology or other information. Based on the final subtitles produced, edit rate between the MT output and the final versions were calculated using HTER (Snover et al., 2006) and characTER (Wang et al., 2016). As PE of the subtitles involved also changes to the segmentation, e.g. adding or deleting frames and moving words between frames, subtitle segmentation was ignored and edit rates were calculated as document-level scores to focus on edits affecting the textual content. These measures were then compared between the tasks of creating interlingual subtitles from scratch and MTPE, as well as between PE of the sentence-level and document-level MT outputs described in Section 3.1. 5 Figure 2: Average task times subtitling through post-editing and from scratch. The"
2020.eamt-1.13,tiedemann-2008-synchronizing,1,0.806103,"be at least partially related to the problem of segmentation and time frame alignment, which we introduce below. 3.2 Subtitle frame alignment In both sentence-level and document-level translation, we have to treat the results in a way that maps the translations back into the time slots allocated for the original subtitles. Those time slots may include more than one sentence, and sentences may stretch over multiple time slots. Because our translation models are trained on sentence-aligned data, we need to extract sentences first from subtitles, too. We do this using the techniques proposed by Tiedemann (2008), which were also applied to the OpenSubtitles corpus in our training data. Subtitles converted to sentence-level segments in XML: <s id=""13""&gt; <time id=""T16S"" value=""00:01:05,960"" /&gt; We have to make readmission agreements with other countries, <time id=""T16E"" value=""00:01:12,360"" /&gt; <time id=""T17S"" value=""00:01:12,440"" /&gt; so that they would be willing. </s&gt; <s id=""14""&gt; We have to cooperate closely. <time id=""T17E"" value=""00:01:17,440"" /&gt; </s&gt; Mapped back to subtitle frames after translation: 16 00:01:05,960 --&gt; 00:01:12,360 Meid¨ an on teht¨ av¨ a takaisinottosopimuksia muiden maiden kanssa, 1"
2020.eamt-1.13,W17-4811,1,0.836426,"s for training transformer models in the MarianNMT documentation. For text segmentation, we apply SentencePiece (Kudo and Richardson, 2018) with models that are trained independently for source and target languages for a vocabulary size of 32,000 in each language. We do not apply any further pre-processing to keep the setup as general as possible, apart from some basic normalisation of Unicode punctuation characters, and parallel corpus filtering using standard scripts from the Moses SMT package (Koehn et al., 2007). For the document-level models, we apply the concatenative models proposed by Tiedemann and Scherrer (2017) and Junczys-Dowmunt (2019) using units of a maximum length of 100 tokens. Note that sentences and sentence fragments in subtitles are typically very short, and 100 tokens typically cover substantial amounts of context beyond sentence boundaries. We mark sentence boundtles, PHP, QED, Tatoeba, TildeMODEL, Ubuntu, wikimedia 3 OPUS corpora used: bible-uedin, Books, DGT, ECB, EMEA, EUbookshop, EUconst, Europarl, GNOME, infopankki, JRC-Acquis, KDE4, OpenSubtitles, ParaCrawl, PHP, QED, Tatoeba, TildeMODEL, Ubuntu aries with special tokens, chunking the training and test data sequentially from the be"
2020.eamt-1.13,2010.jec-1.7,0,0.81234,"Missing"
2020.eamt-1.13,W16-2342,0,0.0165559,"n the intralingual subtitles used as source text for the MT (see Section 3.2). To assess productivity, the process logs were analysed using Inputlog’s analysis functions. The task time and the number of keystrokes logged were used as productivity measures. Using Inputlog filters, we focused only on task time and keystrokes in the subtitling software, excluding other activity such as internet searches for terminology or other information. Based on the final subtitles produced, edit rate between the MT output and the final versions were calculated using HTER (Snover et al., 2006) and characTER (Wang et al., 2016). As PE of the subtitles involved also changes to the segmentation, e.g. adding or deleting frames and moving words between frames, subtitle segmentation was ignored and edit rates were calculated as document-level scores to focus on edits affecting the textual content. These measures were then compared between the tasks of creating interlingual subtitles from scratch and MTPE, as well as between PE of the sentence-level and document-level MT outputs described in Section 3.1. 5 Figure 2: Average task times subtitling through post-editing and from scratch. The top three bars show averages for p"
2020.eamt-1.61,Q17-1024,0,0.195481,"Missing"
2020.eamt-1.61,P18-4020,0,0.0998447,"Missing"
2020.eamt-1.61,tiedemann-2012-parallel,1,0.790177,"of translation quality, language coverage and emphasizes specific test cases to study the applicability of the approach. More details about the implementation and current status of the project are given below. 2 OPUS-MT models The models that we train are based on state-of-theart transformer-based neural machine translation (NMT). We apply Marian-NMT2 in our framework, a stable production-ready NMT toolbox with efficient training and decoding capabilities (JunczysDowmunt et al., 2018). Our models are trained on freely available parallel corpora collected in the large bitext repository OPUS3 (Tiedemann, 2012). The architecture is based on a standard transformer setup with 6 self-attentive layers in both, the encoder and decoder network with 8 attention heads in each layer. The hyper-parameters follow the general recommendations given in the documentation of the software. All the details can be seen in the training procedures that we also release as open source in our GitHub repository.4 OPUS-MT supports both, bilingual as well as multilingual models. For the latter, we apply the language label approach proposed by (Johnson et al., 2017). Our package implements generic 2 https://marian-nmt.github.i"
2020.findings-emnlp.49,P19-1470,0,0.255107,"to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others contribute very little or nothing at all. Similarly, Michel et al. (2019) raise the question whether 16 attention heads are really necessary to obtain competitive performance. Finally, several recent works address the computational challenge of modeling very long sequences and modify the Transformer architecture with attention operations that reduce time complexity (Shen et al., 2018; Wu et al., 2019; Child et al., 2019; Sukhbaatar et al., 2019; Dai et al., 2019; Indurthi et al., 2019; Kitaev et al., 2020). This study falls into the third category and is motivated by the observation that most self-attention patterns learned by the Transformer architecture merely re"
2020.findings-emnlp.49,2013.iwslt-evaluation.1,0,0.0167238,"Missing"
2020.findings-emnlp.49,2014.iwslt-evaluation.1,0,0.019893,"Missing"
2020.findings-emnlp.49,D14-1179,0,0.0593278,"Missing"
2020.findings-emnlp.49,W19-4828,0,0.038033,"rt architecture for NMT, and more recently for language modeling (Radford et al., 2018) and other downstream tasks (Strubell et al., 2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation"
2020.findings-emnlp.49,D19-1223,0,0.0592143,"Missing"
2020.findings-emnlp.49,D19-5622,0,0.0709276,"uld be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For instance, Shaw et al. (2018) replace the sinusoidal position encoding in the Transformer with relative position encoding to improve the focus on local positional patterns. Several studies modify the attention formula to bias the attention weights towards local areas (Yang et al., 2018; Xu et al., 2019; Fonollosa et al., 2019). Wu et al. (2019) and Yang et al. (2019) use convolutional modules to replace parts of self-attention, making the overall networks computationally more efficient. Cui et al. (2019) mask out certain tokens when computing attention, which favors local attention patterns and prevents redun557 dancy in the different attention heads. All these contributions have shown the importance of localness, and the possibility to use lightweight convolutional networks to reduce the number of parameters while yielding competitive results (Wu et al., 2019). In this respect, our work is orthogonal to previous research: we focus only on the original Transformer architecture and investigate the replacement of learnable encoder self-attention by fixed, non-learnable attentive patterns. Recen"
2020.findings-emnlp.49,W19-5203,0,0.0370687,"dies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connections (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Brunner et al., 2020). A closely related research area attempts to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others contribute very little or nothing at all. Similarly, Michel et al. (2019) raise the question whether 16 attention heads are really necessary to obtain competitive performance. Finally, several recent works address the computational challenge of modeling very long sequences and modify the Transformer"
2020.findings-emnlp.49,P19-1285,0,0.0608932,"Missing"
2020.findings-emnlp.49,R19-1028,0,0.0494014,"r lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connections (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Brunner et al., 2020). A closely related research area attempts to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others contribute very little or nothing at all. Similarly, Michel et al. (2019) raise the question whether 16 attention heads are really necessary to obtain competitive performance. Finally, several recent works address the computational challenge of modeling very long sequences and modify the Transformer architecture with atten"
2020.findings-emnlp.49,N19-1423,0,0.256847,"d of each encoder layer with simple fixed – non-learnable – attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios. 1 Introduction Models based on the Transformer architecture (Vaswani et al., 2017) have led to tremendous performance increases in a wide range of downstream tasks (Devlin et al., 2019; Radford et al., 2019), including Machine Translation (MT) (Vaswani et al., 2017; Ott et al., 2018). One main component of the architecture is the multi-headed attention mechanism that allows the model to capture longrange contextual information. Despite these successes, the impact of the suggested parametrization choices, in particular the self-attention mechanism with its large number of attention heads distributed over several layers, has been the subject of many studies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in p"
2020.findings-emnlp.49,D19-1453,0,0.0283687,"self-attention mechanism with its large number of attention heads distributed over several layers, has been the subject of many studies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connections (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Brunner et al., 2020). A closely related research area attempts to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others contribute very little or nothing at all. Similarly, Michel et al. (2019) raise the question whether 16 attention heads are really necessary to obtain competitive"
2020.findings-emnlp.49,D19-1082,0,0.0180082,"2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For ins"
2020.findings-emnlp.49,D19-1424,0,0.0364636,"2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For ins"
2020.findings-emnlp.49,P19-1290,0,0.0201375,"s ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others contribute very little or nothing at all. Similarly, Michel et al. (2019) raise the question whether 16 attention heads are really necessary to obtain competitive performance. Finally, several recent works address the computational challenge of modeling very long sequences and modify the Transformer architecture with attention operations that reduce time complexity (Shen et al., 2018; Wu et al., 2019; Child et al., 2019; Sukhbaatar et al., 2019; Dai et al., 2019; Indurthi et al., 2019; Kitaev et al., 2020). This study falls into the third category and is motivated by the observation that most self-attention patterns learned by the Transformer architecture merely reflect positional encoding of contextual information (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Voita et al., 2019a; Correia et al., 2019). From this standpoint, we argue that most attentive connections in the encoder do not need to be learned at all, but can be replaced by simple predefined patterns. To this end, we design, analyze and experimentally compare intuitive and simple fixed attention pattern"
2020.findings-emnlp.49,P19-1356,0,0.0291859,"2018) and other downstream tasks (Strubell et al., 2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the trans"
2020.findings-emnlp.49,P17-4012,0,0.0365928,"omary in NMT to split words into subword units, and there is evidence that self-attention treats split words differently than non-split ones (Correia et al., 2019). Therefore, we propose a second variant of the predefined patterns that assigns the same attention values to all parts of the same word (see lower row of Figure 1). 4 We perform a series of experiments to evaluate the fixed attentive encoder patterns, starting with a standard German ↔ English translation setup (Section 4.1) and then extending the scope to lowresource and high-resource scenarios (Section 4.2). We use the OpenNMT-py (Klein et al., 2017) library for training, the base version of Transformer as hyper-parameters (Vaswani et al., 2017), and compare against the reference using sacreBLEU (Papineni et al., 2002; Post, 2018) .3 4.1 j=0 (j Results: Standard scenario To assess the general viability of the proposed approach and to quantify the effects of different numbers of encoder and decoder layers, we train models on a mid-sized dataset of 2.9M training sentences from the German ↔ English WMT19 news translation task (Barrault et al., 2019), using newstest2013 and newstest2014 as development and test data, respectively. We learn tru"
2020.findings-emnlp.49,W04-3250,0,0.0785459,"en +1L 7Fword +1L 1L 25.66 24.90* 25.03* 23.76* 27.28 27.01 26.72* 25.75* 27.88 26.84* 27.38* 26.96* 28.62 28.09* 27.78* 27.34* 28.71 28.43 27.82* 27.44* 29.31 28.61* 28.40* 27.56* 30.99 30.61 30.69 30.17* Table 1: BLEU scores for the German ↔ English (DE ↔ EN) standard scenario, for different configurations of learnable (L) and fixed (F) attention heads. Scores marked in gray with * are significantly lower than the respective 8L model scores, at p &lt; 0.05. Statistical significance is computed using the compare-mt tool (Neubig et al., 2019) with paired bootstrap resampling with 1000 resamples (Koehn, 2004). coding (BPE) segmentation (Sennrich et al., 2016) on the training corpus, using 35 000 merge operations. We train four Transformer models: • 8L: all 8 attention heads in each layer are learnable, • 7Ftoken +1L: 7 fixed token-based attention heads and 1 learnable head per encoder layer, • 7Fword +1L: 7 fixed word-based attention patterns and 1 learnable head per encoder layer, • 1L: a single learnable attention head per encoder layer. Each model is trained in 7 configurations: 6 encoder layers with 6 decoder layers, and 1 to 6 encoder layers coupled to 1 decoder layer. BLEU scores are shown i"
2020.findings-emnlp.49,D19-1445,0,0.154105,"titive performance. Finally, several recent works address the computational challenge of modeling very long sequences and modify the Transformer architecture with attention operations that reduce time complexity (Shen et al., 2018; Wu et al., 2019; Child et al., 2019; Sukhbaatar et al., 2019; Dai et al., 2019; Indurthi et al., 2019; Kitaev et al., 2020). This study falls into the third category and is motivated by the observation that most self-attention patterns learned by the Transformer architecture merely reflect positional encoding of contextual information (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Voita et al., 2019a; Correia et al., 2019). From this standpoint, we argue that most attentive connections in the encoder do not need to be learned at all, but can be replaced by simple predefined patterns. To this end, we design, analyze and experimentally compare intuitive and simple fixed attention patterns. The proposed patterns are solely based on positional information and do not require any learnable parameters nor external knowledge. The fixed patterns reflect the importance of locality and pose the question whether encoder self-attention needs to be learned 556 Findings of the Assoc"
2020.findings-emnlp.49,P18-1007,0,0.0409789,"Missing"
2020.findings-emnlp.49,W19-4825,0,0.0950801,"Missing"
2020.findings-emnlp.49,Q16-1037,0,0.0364869,"heads on the encoder side is negligible. Moreover, we also note that as we replace attention heads with non-learnable ones, our configurations reduce the number of parameters without degrading translation quality. 5 Analysis To further analyze the fixed attentive encoder patterns, we perform three targeted evaluations: i) on the sentence length, ii) on the subject-verb agreement task, and iii) on the Word Sense Disambiguation (WSD) task. The length analysis inspects the translation quality by sentence length. The subjectverb agreement task is commonly used to evaluate long-range dependencies (Linzen et al., 2016; Tran et al., 2018; Tang et al., 2018), while the WSD task addresses lexical ambiguity phenomena, i.e., words of the source language that have multiple translations in the target language representing different meanings (Marvin and Koehn, 2018; Liu 8L 7Ftoken +1L 7Ftoken (H8 disabled) &lt;10 [10,20)[20,30)[30,40)[40,50)[50,60) ≥60 Figure 2: BLEU scores for different ranges of sentence lengths. et al., 2018; Pu et al., 2018; Tang et al., 2019). For both tasks, we use contrastive test suites (Sennrich, 2017; Popovi´c and Castilho, 2019) that rely on the ability of NMT systems to score given transl"
2020.findings-emnlp.49,N18-1121,0,0.0527275,"Missing"
2020.findings-emnlp.49,D15-1166,0,0.0881284,"ay hamper the extraction of global semantic features beneficial for the word sense disambiguation capability of the MT model. Keeping one learnable head in the encoder compensates for degradations, but this trade-off needs to be carefully assessed. • Position-wise attentive patterns play a key role in low-resource scenarios, both for related (German ↔ English) and unrelated (Vietnamese ↔ English) languages. 2 Related work Attention mechanisms in Neural Machine Translation (NMT) were first introduced in combination with Recurrent Neural Networks (RNNs) (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), between the encoder and decoder. The Transformer architecture extended the mechanism by introducing the so-called self-attention to replace the RNNs in the encoder and decoder, and by using multiple attention heads (Vaswani et al., 2017). This architecture rapidly became the de facto state-of-the-art architecture for NMT, and more recently for language modeling (Radford et al., 2018) and other downstream tasks (Strubell et al., 2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intui"
2020.findings-emnlp.49,W19-4827,0,0.0866727,"Missing"
2020.findings-emnlp.49,W18-1812,0,0.0199672,"ze the fixed attentive encoder patterns, we perform three targeted evaluations: i) on the sentence length, ii) on the subject-verb agreement task, and iii) on the Word Sense Disambiguation (WSD) task. The length analysis inspects the translation quality by sentence length. The subjectverb agreement task is commonly used to evaluate long-range dependencies (Linzen et al., 2016; Tran et al., 2018; Tang et al., 2018), while the WSD task addresses lexical ambiguity phenomena, i.e., words of the source language that have multiple translations in the target language representing different meanings (Marvin and Koehn, 2018; Liu 8L 7Ftoken +1L 7Ftoken (H8 disabled) &lt;10 [10,20)[20,30)[30,40)[40,50)[50,60) ≥60 Figure 2: BLEU scores for different ranges of sentence lengths. et al., 2018; Pu et al., 2018; Tang et al., 2019). For both tasks, we use contrastive test suites (Sennrich, 2017; Popovi´c and Castilho, 2019) that rely on the ability of NMT systems to score given translations. Broadly speaking, a sentence containing the linguistic phenomenon of interest is paired with the correct reference translation and with a modified translation with a specific type of error. A contrast is considered successfully detected"
2020.findings-emnlp.49,N19-4007,0,0.0434251,"74 22.63* 23.36 23.07 22.88* 22.29* 25.02 24.63 24.85 23.87* DE–EN 8L 7Ftoken +1L 7Fword +1L 1L 25.66 24.90* 25.03* 23.76* 27.28 27.01 26.72* 25.75* 27.88 26.84* 27.38* 26.96* 28.62 28.09* 27.78* 27.34* 28.71 28.43 27.82* 27.44* 29.31 28.61* 28.40* 27.56* 30.99 30.61 30.69 30.17* Table 1: BLEU scores for the German ↔ English (DE ↔ EN) standard scenario, for different configurations of learnable (L) and fixed (F) attention heads. Scores marked in gray with * are significantly lower than the respective 8L model scores, at p &lt; 0.05. Statistical significance is computed using the compare-mt tool (Neubig et al., 2019) with paired bootstrap resampling with 1000 resamples (Koehn, 2004). coding (BPE) segmentation (Sennrich et al., 2016) on the training corpus, using 35 000 merge operations. We train four Transformer models: • 8L: all 8 attention heads in each layer are learnable, • 7Ftoken +1L: 7 fixed token-based attention heads and 1 learnable head per encoder layer, • 7Fword +1L: 7 fixed word-based attention patterns and 1 learnable head per encoder layer, • 1L: a single learnable attention head per encoder layer. Each model is trained in 7 configurations: 6 encoder layers with 6 decoder layers, and 1 to 6"
2020.findings-emnlp.49,W18-6301,0,0.0232455,"n position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios. 1 Introduction Models based on the Transformer architecture (Vaswani et al., 2017) have led to tremendous performance increases in a wide range of downstream tasks (Devlin et al., 2019; Radford et al., 2019), including Machine Translation (MT) (Vaswani et al., 2017; Ott et al., 2018). One main component of the architecture is the multi-headed attention mechanism that allows the model to capture longrange contextual information. Despite these successes, the impact of the suggested parametrization choices, in particular the self-attention mechanism with its large number of attention heads distributed over several layers, has been the subject of many studies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connec"
2020.findings-emnlp.49,P02-1040,0,0.107248,"efore, we propose a second variant of the predefined patterns that assigns the same attention values to all parts of the same word (see lower row of Figure 1). 4 We perform a series of experiments to evaluate the fixed attentive encoder patterns, starting with a standard German ↔ English translation setup (Section 4.1) and then extending the scope to lowresource and high-resource scenarios (Section 4.2). We use the OpenNMT-py (Klein et al., 2017) library for training, the base version of Transformer as hyper-parameters (Vaswani et al., 2017), and compare against the reference using sacreBLEU (Papineni et al., 2002; Post, 2018) .3 4.1 j=0 (j Results: Standard scenario To assess the general viability of the proposed approach and to quantify the effects of different numbers of encoder and decoder layers, we train models on a mid-sized dataset of 2.9M training sentences from the German ↔ English WMT19 news translation task (Barrault et al., 2019), using newstest2013 and newstest2014 as development and test data, respectively. We learn truecasers and Byte-Pair En1 Pattern 5 and 7 are flipped versions of pattern 4 and 6, respectively. 2 In Section 4.4, we present a contrastive system in which the eighth head"
2020.findings-emnlp.49,Y16-2002,0,0.0203337,"os since intuitive properties of self-attention are directly encoded within the model, which may be hard to learn from small training datasets. We empirically test this assumption on four translation tasks: • German → English (DE→EN), using the data from the IWSLT 2014 shared task (Cettolo et al., 2014). As prior work (Ranzato et al., 2016; Sennrich and Zhang, 2019), we report BLEU score on the concatenated dev sets: tst2010, tst2011, tst2012, dev2010, dev2012 (159 000 training sentences, 7 282 for development, and 6 750 for testing). • Korean → English (KO→EN), using the dataset described in Park et al. (2016) (90 000 training sentences, 1 000 for development, and 2 000 for testing).4 • Vietnamese ↔ English (VI↔EN), using the data from the IWSLT 2015 shared task (Cettolo et al., 2015), using tst2012 and tst2013 for development and testing, respectively (133 000 training sentences, 1 553 for development and 1 268 per testing). 560 4 https://github.com/jungyeul/ korean-parallel-corpora Disabled head Enc. heads DE–EN KO–EN EN–VI VI–EN 8L 7Ftoken +1L 7Fword +1L 1L Prior work 30.86 32.95 32.56 30.22 † 33.60 6.67 8.43 8.70 6.14 † 10.37 29.85 31.05 31.15 28.67 ] 27.71 1 Current word 2 Previous word 3 Next"
2020.findings-emnlp.49,W19-7602,0,0.0356148,"Missing"
2020.findings-emnlp.49,W18-6319,0,0.0208893,"ond variant of the predefined patterns that assigns the same attention values to all parts of the same word (see lower row of Figure 1). 4 We perform a series of experiments to evaluate the fixed attentive encoder patterns, starting with a standard German ↔ English translation setup (Section 4.1) and then extending the scope to lowresource and high-resource scenarios (Section 4.2). We use the OpenNMT-py (Klein et al., 2017) library for training, the base version of Transformer as hyper-parameters (Vaswani et al., 2017), and compare against the reference using sacreBLEU (Papineni et al., 2002; Post, 2018) .3 4.1 j=0 (j Results: Standard scenario To assess the general viability of the proposed approach and to quantify the effects of different numbers of encoder and decoder layers, we train models on a mid-sized dataset of 2.9M training sentences from the German ↔ English WMT19 news translation task (Barrault et al., 2019), using newstest2013 and newstest2014 as development and test data, respectively. We learn truecasers and Byte-Pair En1 Pattern 5 and 7 are flipped versions of pattern 4 and 6, respectively. 2 In Section 4.4, we present a contrastive system in which the eighth head is fixed as"
2020.findings-emnlp.49,Q18-1044,0,0.0129872,"n (WSD) task. The length analysis inspects the translation quality by sentence length. The subjectverb agreement task is commonly used to evaluate long-range dependencies (Linzen et al., 2016; Tran et al., 2018; Tang et al., 2018), while the WSD task addresses lexical ambiguity phenomena, i.e., words of the source language that have multiple translations in the target language representing different meanings (Marvin and Koehn, 2018; Liu 8L 7Ftoken +1L 7Ftoken (H8 disabled) &lt;10 [10,20)[20,30)[30,40)[40,50)[50,60) ≥60 Figure 2: BLEU scores for different ranges of sentence lengths. et al., 2018; Pu et al., 2018; Tang et al., 2019). For both tasks, we use contrastive test suites (Sennrich, 2017; Popovi´c and Castilho, 2019) that rely on the ability of NMT systems to score given translations. Broadly speaking, a sentence containing the linguistic phenomenon of interest is paired with the correct reference translation and with a modified translation with a specific type of error. A contrast is considered successfully detected if the reference translation obtains a higher score than the artificially modified translation. The evaluation metric corresponds to the accuracy over all decisions. We conduct th"
2020.findings-emnlp.49,W19-5354,1,0.845427,"learnable attention head (7Ftoken H8 disabled), performance drops consistently in both test suites, showing that the learnable head plays a key role for WSD, specializing in semantic feature extraction. Word sense disambiguation It has been shown that the encoder of Transformerbased MT models includes semantic information beneficial for WSD (Tang et al., 2018, 2019). In this respect, a model with predefined fixed patterns may struggle to encode global semantic features. To this end, we evaluate our models on two German–English WSD test suites, ContraWSD (Rios Gonzales et al., 2017) and MuCoW (Raganato et al., 2019).9 Table 6 shows the performance of our models on the WSD benchmarks. Overall, the model with 6 decoder layers and fixed attentive patterns 9 As MuCoW is automatically built using various parallel corpora, we discarded those ones included in our training. We only report the average result from the TED (Cettolo et al., 2013) and Tatoeba (Tiedemann, 2012) sources. 563 6 Conclusion In this work, we propose to simplify encoder selfattention of Transformer-based NMT models by replacing all but one attention heads with fixed positional attentive patterns that require neither training nor external kn"
2020.findings-emnlp.49,W18-5431,1,0.940897,"ain component of the architecture is the multi-headed attention mechanism that allows the model to capture longrange contextual information. Despite these successes, the impact of the suggested parametrization choices, in particular the self-attention mechanism with its large number of attention heads distributed over several layers, has been the subject of many studies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connections (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Brunner et al., 2020). A closely related research area attempts to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show t"
2020.findings-emnlp.49,W17-4702,0,0.0179277,"ingly enough, when we disable the learnable attention head (7Ftoken H8 disabled), performance drops consistently in both test suites, showing that the learnable head plays a key role for WSD, specializing in semantic feature extraction. Word sense disambiguation It has been shown that the encoder of Transformerbased MT models includes semantic information beneficial for WSD (Tang et al., 2018, 2019). In this respect, a model with predefined fixed patterns may struggle to encode global semantic features. To this end, we evaluate our models on two German–English WSD test suites, ContraWSD (Rios Gonzales et al., 2017) and MuCoW (Raganato et al., 2019).9 Table 6 shows the performance of our models on the WSD benchmarks. Overall, the model with 6 decoder layers and fixed attentive patterns 9 As MuCoW is automatically built using various parallel corpora, we discarded those ones included in our training. We only report the average result from the TED (Cettolo et al., 2013) and Tatoeba (Tiedemann, 2012) sources. 563 6 Conclusion In this work, we propose to simplify encoder selfattention of Transformer-based NMT models by replacing all but one attention heads with fixed positional attentive patterns that requir"
2020.findings-emnlp.49,2020.tacl-1.54,0,0.0195092,"., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For instance, Shaw et al. (201"
2020.findings-emnlp.49,D19-1592,0,0.0620283,"Missing"
2020.findings-emnlp.49,E17-2060,0,0.124097,". The subjectverb agreement task is commonly used to evaluate long-range dependencies (Linzen et al., 2016; Tran et al., 2018; Tang et al., 2018), while the WSD task addresses lexical ambiguity phenomena, i.e., words of the source language that have multiple translations in the target language representing different meanings (Marvin and Koehn, 2018; Liu 8L 7Ftoken +1L 7Ftoken (H8 disabled) &lt;10 [10,20)[20,30)[30,40)[40,50)[50,60) ≥60 Figure 2: BLEU scores for different ranges of sentence lengths. et al., 2018; Pu et al., 2018; Tang et al., 2019). For both tasks, we use contrastive test suites (Sennrich, 2017; Popovi´c and Castilho, 2019) that rely on the ability of NMT systems to score given translations. Broadly speaking, a sentence containing the linguistic phenomenon of interest is paired with the correct reference translation and with a modified translation with a specific type of error. A contrast is considered successfully detected if the reference translation obtains a higher score than the artificially modified translation. The evaluation metric corresponds to the accuracy over all decisions. We conduct the analyses using the DE–EN models from Section 4.1, i.e., 8L, 7Ftoken +1L, and 7Ftok"
2020.findings-emnlp.49,P16-1162,0,0.126617,"23.76* 27.28 27.01 26.72* 25.75* 27.88 26.84* 27.38* 26.96* 28.62 28.09* 27.78* 27.34* 28.71 28.43 27.82* 27.44* 29.31 28.61* 28.40* 27.56* 30.99 30.61 30.69 30.17* Table 1: BLEU scores for the German ↔ English (DE ↔ EN) standard scenario, for different configurations of learnable (L) and fixed (F) attention heads. Scores marked in gray with * are significantly lower than the respective 8L model scores, at p &lt; 0.05. Statistical significance is computed using the compare-mt tool (Neubig et al., 2019) with paired bootstrap resampling with 1000 resamples (Koehn, 2004). coding (BPE) segmentation (Sennrich et al., 2016) on the training corpus, using 35 000 merge operations. We train four Transformer models: • 8L: all 8 attention heads in each layer are learnable, • 7Ftoken +1L: 7 fixed token-based attention heads and 1 learnable head per encoder layer, • 7Fword +1L: 7 fixed word-based attention patterns and 1 learnable head per encoder layer, • 1L: a single learnable attention head per encoder layer. Each model is trained in 7 configurations: 6 encoder layers with 6 decoder layers, and 1 to 6 encoder layers coupled to 1 decoder layer. BLEU scores are shown in Table 1. Results for the most powerful model (6+6"
2020.findings-emnlp.49,P19-1021,0,0.123147,"orm the word-based one, but with higher numbers of decoder layers the two variants are statistically equivalent. 4.2 Results: Low-resource and high-resource scenarios We hypothesize that fixed attentive patterns are especially useful in low-resource scenarios since intuitive properties of self-attention are directly encoded within the model, which may be hard to learn from small training datasets. We empirically test this assumption on four translation tasks: • German → English (DE→EN), using the data from the IWSLT 2014 shared task (Cettolo et al., 2014). As prior work (Ranzato et al., 2016; Sennrich and Zhang, 2019), we report BLEU score on the concatenated dev sets: tst2010, tst2011, tst2012, dev2010, dev2012 (159 000 training sentences, 7 282 for development, and 6 750 for testing). • Korean → English (KO→EN), using the dataset described in Park et al. (2016) (90 000 training sentences, 1 000 for development, and 2 000 for testing).4 • Vietnamese ↔ English (VI↔EN), using the data from the IWSLT 2015 shared task (Cettolo et al., 2015), using tst2012 and tst2013 for development and testing, respectively (133 000 training sentences, 1 553 for development and 1 268 per testing). 560 4 https://github.com/ju"
2020.findings-emnlp.49,N18-2074,0,0.0327278,"rs et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For instance, Shaw et al. (2018) replace the sinusoidal position encoding in the Transformer with relative position encoding to improve the focus on local positional patterns. Several studies modify the attention formula to bias the attention weights towards local areas (Yang et al., 2018; Xu et al., 2019; Fonollosa et al., 2019). Wu et al. (2019) and Yang et al. (2019) use convolutional modules to replace parts of self-attention, making the overall networks computationally more efficient. Cui et al. (2019) mask out certain tokens when computing attention, which favors local attention patterns and prevents redun557 dancy in"
2020.findings-emnlp.49,D18-1548,0,0.0620527,"Missing"
2020.findings-emnlp.49,P19-1452,0,0.0242263,"for language modeling (Radford et al., 2018) and other downstream tasks (Strubell et al., 2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within"
2020.findings-emnlp.49,tiedemann-2012-parallel,1,0.656296,", 2019). In this respect, a model with predefined fixed patterns may struggle to encode global semantic features. To this end, we evaluate our models on two German–English WSD test suites, ContraWSD (Rios Gonzales et al., 2017) and MuCoW (Raganato et al., 2019).9 Table 6 shows the performance of our models on the WSD benchmarks. Overall, the model with 6 decoder layers and fixed attentive patterns 9 As MuCoW is automatically built using various parallel corpora, we discarded those ones included in our training. We only report the average result from the TED (Cettolo et al., 2013) and Tatoeba (Tiedemann, 2012) sources. 563 6 Conclusion In this work, we propose to simplify encoder selfattention of Transformer-based NMT models by replacing all but one attention heads with fixed positional attentive patterns that require neither training nor external knowledge. We train NMT models on different data sizes and language directions with the proposed fixed patterns, showing that the encoder self-attention can be simplified drastically, reducing parameter footprint at training time without degradation in translation quality. In low-resource scenarios, translation quality is even improved. Our extensive anal"
2020.findings-emnlp.49,D18-1503,0,0.0461458,"Missing"
2020.findings-emnlp.49,W19-4808,0,0.021559,"de facto state-of-the-art architecture for NMT, and more recently for language modeling (Radford et al., 2018) and other downstream tasks (Strubell et al., 2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the li"
2020.findings-emnlp.49,P19-1032,0,0.0245622,"nt information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others contribute very little or nothing at all. Similarly, Michel et al. (2019) raise the question whether 16 attention heads are really necessary to obtain competitive performance. Finally, several recent works address the computational challenge of modeling very long sequences and modify the Transformer architecture with attention operations that reduce time complexity (Shen et al., 2018; Wu et al., 2019; Child et al., 2019; Sukhbaatar et al., 2019; Dai et al., 2019; Indurthi et al., 2019; Kitaev et al., 2020). This study falls into the third category and is motivated by the observation that most self-attention patterns learned by the Transformer architecture merely reflect positional encoding of contextual information (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Voita et al., 2019a; Correia et al., 2019). From this standpoint, we argue that most attentive connections in the encoder do not need to be learned at all, but can be replaced by simple predefined patterns. To this end, we design, analyze and experimentally compare int"
2020.findings-emnlp.49,D19-1448,0,0.27398,"that allows the model to capture longrange contextual information. Despite these successes, the impact of the suggested parametrization choices, in particular the self-attention mechanism with its large number of attention heads distributed over several layers, has been the subject of many studies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connections (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Brunner et al., 2020). A closely related research area attempts to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others"
2020.findings-emnlp.49,D18-1458,0,0.031139,"Missing"
2020.findings-emnlp.49,P19-1580,0,0.478925,"that allows the model to capture longrange contextual information. Despite these successes, the impact of the suggested parametrization choices, in particular the self-attention mechanism with its large number of attention heads distributed over several layers, has been the subject of many studies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connections (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Brunner et al., 2020). A closely related research area attempts to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others"
2020.findings-emnlp.49,D19-1149,0,0.0338734,"Missing"
2020.findings-emnlp.49,P19-1295,0,0.0438248,"while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For instance, Shaw et al. (2018) replace the sinusoidal position encoding in the Transformer with relative position encoding to improve the focus on local positional patterns. Several studies modify the attention formula to bias the attention weights towards local areas (Yang et al., 2018; Xu et al., 2019; Fonollosa et al., 2019). Wu et al. (2019) and Yang et al. (2019) use convolutional modules to replace parts of self-attention, making the overall networks computationally more efficient. Cui et al. (2019) mask out certain tokens when computing attention, which favors local attention patterns and prevents redun557 dancy in the different attention heads. All these contributions have shown the importance of localness, and the possibility to use lightweight convolutional networks to reduce the number of parameters while yielding competitive results (Wu et al., 2019). In this respect, our work is"
2020.findings-emnlp.49,D18-1475,0,0.0238611,"lation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For instance, Shaw et al. (2018) replace the sinusoidal position encoding in the Transformer with relative position encoding to improve the focus on local positional patterns. Several studies modify the attention formula to bias the attention weights towards local areas (Yang et al., 2018; Xu et al., 2019; Fonollosa et al., 2019). Wu et al. (2019) and Yang et al. (2019) use convolutional modules to replace parts of self-attention, making the overall networks computationally more efficient. Cui et al. (2019) mask out certain tokens when computing attention, which favors local attention patterns and prevents redun557 dancy in the different attention heads. All these contributions have shown the importance of localness, and the possibility to use lightweight convolutional networks to reduce the number of parameters while yielding competitive results (Wu et al., 2019). In this res"
2020.findings-emnlp.49,N19-1407,0,0.0330641,"research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For instance, Shaw et al. (2018) replace the sinusoidal position encoding in the Transformer with relative position encoding to improve the focus on local positional patterns. Several studies modify the attention formula to bias the attention weights towards local areas (Yang et al., 2018; Xu et al., 2019; Fonollosa et al., 2019). Wu et al. (2019) and Yang et al. (2019) use convolutional modules to replace parts of self-attention, making the overall networks computationally more efficient. Cui et al. (2019) mask out certain tokens when computing attention, which favors local attention patterns and prevents redun557 dancy in the different attention heads. All these contributions have shown the importance of localness, and the possibility to use lightweight convolutional networks to reduce the number of parameters while yielding competitive results (Wu et al., 2019). In this respect, our work is orthogonal to previous research: we focus only on the original Tr"
2020.findings-emnlp.49,2020.acl-main.687,0,0.0709417,"pes of heads: positional heads point to an adjacent token, syntactic heads point to tokens in a specific syntactic relation, and rare word heads point to the least frequent tokens in a sentence. Correia et al. (2019) identify two additional types of heads: BPE-merging heads spread weight over adjacent tokens that are part of the same BPE cluster or hyphenated words, and interrogation heads point to question marks at the end of the sentence. In line with these findings, we design our fixed attention patterns and train NMT models without the need of learning them. In concurrent work, You et al. (2020) propose to replace learnable attention weights in Transformerbased NMT with hard-coded Gaussian distributions. This paper is complementary and differs in several respects: while You et al. (2020) consider three fixed patterns across the encoder-decoder architecture, we focus only on the encoder selfattention but present seven fixed patterns that cover additional known properties of self-attention. We study the relative impact of each of them and analyze their performance with respect to different numbers of encoder-decoder layers, and as semantic feature extractor for lexical ambiguity phenom"
2020.findings-emnlp.49,2015.iwslt-evaluation.1,0,\N,Missing
2020.iwslt-1.10,P18-1126,0,0.0375899,"Missing"
2020.iwslt-1.10,N19-1202,0,0.0527933,"Missing"
2020.iwslt-1.10,P15-1166,0,0.0389434,"ltitask fashion, which maps the internal representations of different encoders into a shared space before decoding. For the cascade approach, we use a pipeline of three stages: (i) automatic speech recognition (ASR), 2 Proposed Systems End-to-end SLT We use an inner-attention based architecture proposed by V´azquez et al. (2020). In a nutshell, it follows the conventional structure of an encoderdecoder model of MT (Bahdanau et al., 2015; Luong et al., 2016) enabled with multilingual training by incorporating language-specific encoders and decoders trainable with a language-rotating scheduler (Dong et al., 2015; Schwenk and Douze, 2017), and an intermediate shared inner-attention layer (C´ıfka and Bojar, 2018; Lu et al., 2018). We implement our model on top of an OpenNMT-py (Klein et al., 2017) fork, which we make available for reproducibility purposes.1 The text encoders and the decoders (always text output) are transformers (Vaswani et al., 2017). 1 https://github.com/Helsinki-NLP/ OpenNMT-py/tree/iwslt2020 95 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 95–102 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.186"
2020.iwslt-1.10,P18-4020,0,0.0239735,"Missing"
2020.iwslt-1.10,P17-4012,0,0.0291109,"matic speech recognition (ASR), 2 Proposed Systems End-to-end SLT We use an inner-attention based architecture proposed by V´azquez et al. (2020). In a nutshell, it follows the conventional structure of an encoderdecoder model of MT (Bahdanau et al., 2015; Luong et al., 2016) enabled with multilingual training by incorporating language-specific encoders and decoders trainable with a language-rotating scheduler (Dong et al., 2015; Schwenk and Douze, 2017), and an intermediate shared inner-attention layer (C´ıfka and Bojar, 2018; Lu et al., 2018). We implement our model on top of an OpenNMT-py (Klein et al., 2017) fork, which we make available for reproducibility purposes.1 The text encoders and the decoders (always text output) are transformers (Vaswani et al., 2017). 1 https://github.com/Helsinki-NLP/ OpenNMT-py/tree/iwslt2020 95 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 95–102 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 We implement the transformer-based audio encoders inspired by the SLT architecture with tied layer structure from Tu et al. (2019) and the RTransformer from Di Gangi et al. (201"
2020.iwslt-1.10,P07-2045,0,0.00615653,"” the output of our ASR model to remove segments containing musical note characters ( ), and repeating phrases that were consistently hallucinated during silence, applause, laughter or noise in the audio (e.g. in our case, “Shake. Fold.”), as well as parts of segments that designate the speaker (e.g. “Audience: ...”). Subsequently, we use the same preprocessing pipeline for the cleaned ASR output as we do for all of our text data. For this, we start by removing nonprinting characters, normalizing punctuation, and retokenizing the text using the corresponding utilities from the Moses toolkit (Koehn et al., 2007). Afterwards, we apply subword segmentation via SentencePiece (Kudo and Richardson, 2018), using a joint English–German BPE model with a vocabulary size of 32 000 for all of our translation models, Table 2: Text training data used for end-to-end systems. Audio for the cascade system. We have extracted 40-dimensional Filterbank features with speaker normalization for each sentence-like segment of the MuST-C, How2 (Sanabria et al., 2018) and Mozilla Common Voice (Ardila et al., 2019) corpora using XNMT (Neubig et al., 2018). After getting rid of audio files that were too short (less than 0.4 sec"
2020.iwslt-1.10,D18-2012,0,0.0270326,"s ( ), and repeating phrases that were consistently hallucinated during silence, applause, laughter or noise in the audio (e.g. in our case, “Shake. Fold.”), as well as parts of segments that designate the speaker (e.g. “Audience: ...”). Subsequently, we use the same preprocessing pipeline for the cleaned ASR output as we do for all of our text data. For this, we start by removing nonprinting characters, normalizing punctuation, and retokenizing the text using the corresponding utilities from the Moses toolkit (Koehn et al., 2007). Afterwards, we apply subword segmentation via SentencePiece (Kudo and Richardson, 2018), using a joint English–German BPE model with a vocabulary size of 32 000 for all of our translation models, Table 2: Text training data used for end-to-end systems. Audio for the cascade system. We have extracted 40-dimensional Filterbank features with speaker normalization for each sentence-like segment of the MuST-C, How2 (Sanabria et al., 2018) and Mozilla Common Voice (Ardila et al., 2019) corpora using XNMT (Neubig et al., 2018). After getting rid of audio files that were too short (less than 0.4 seconds), corrupted, or no longer available for download from YouTube, some 1.2M clean utter"
2020.iwslt-1.10,W18-6309,0,0.0140271,"the cascade approach, we use a pipeline of three stages: (i) automatic speech recognition (ASR), 2 Proposed Systems End-to-end SLT We use an inner-attention based architecture proposed by V´azquez et al. (2020). In a nutshell, it follows the conventional structure of an encoderdecoder model of MT (Bahdanau et al., 2015; Luong et al., 2016) enabled with multilingual training by incorporating language-specific encoders and decoders trainable with a language-rotating scheduler (Dong et al., 2015; Schwenk and Douze, 2017), and an intermediate shared inner-attention layer (C´ıfka and Bojar, 2018; Lu et al., 2018). We implement our model on top of an OpenNMT-py (Klein et al., 2017) fork, which we make available for reproducibility purposes.1 The text encoders and the decoders (always text output) are transformers (Vaswani et al., 2017). 1 https://github.com/Helsinki-NLP/ OpenNMT-py/tree/iwslt2020 95 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 95–102 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 We implement the transformer-based audio encoders inspired by the SLT architecture with tied layer structure"
2020.iwslt-1.10,W18-1818,0,0.0314185,"nizing the text using the corresponding utilities from the Moses toolkit (Koehn et al., 2007). Afterwards, we apply subword segmentation via SentencePiece (Kudo and Richardson, 2018), using a joint English–German BPE model with a vocabulary size of 32 000 for all of our translation models, Table 2: Text training data used for end-to-end systems. Audio for the cascade system. We have extracted 40-dimensional Filterbank features with speaker normalization for each sentence-like segment of the MuST-C, How2 (Sanabria et al., 2018) and Mozilla Common Voice (Ardila et al., 2019) corpora using XNMT (Neubig et al., 2018). After getting rid of audio files that were too short (less than 0.4 seconds), corrupted, or no longer available for download from YouTube, some 1.2M clean utterances remained for training the ASR system, and 30k for validation. On the target side, we use two contrastive preprocessing pipelines: 97 Figure 1: Configurations tested for multitask training. and an English unigram model with a vocabulary size of 24 000 for the restoration stage of our cascade SLT, both trained on all of the data used for the translation and restoration models combined. Before the training of the restoration model,"
2020.iwslt-1.10,W17-2619,0,0.0211349,"ich maps the internal representations of different encoders into a shared space before decoding. For the cascade approach, we use a pipeline of three stages: (i) automatic speech recognition (ASR), 2 Proposed Systems End-to-end SLT We use an inner-attention based architecture proposed by V´azquez et al. (2020). In a nutshell, it follows the conventional structure of an encoderdecoder model of MT (Bahdanau et al., 2015; Luong et al., 2016) enabled with multilingual training by incorporating language-specific encoders and decoders trainable with a language-rotating scheduler (Dong et al., 2015; Schwenk and Douze, 2017), and an intermediate shared inner-attention layer (C´ıfka and Bojar, 2018; Lu et al., 2018). We implement our model on top of an OpenNMT-py (Klein et al., 2017) fork, which we make available for reproducibility purposes.1 The text encoders and the decoders (always text output) are transformers (Vaswani et al., 2017). 1 https://github.com/Helsinki-NLP/ OpenNMT-py/tree/iwslt2020 95 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 95–102 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 We implement the"
2020.iwslt-1.10,2020.cl-2.5,1,0.878312,"Missing"
2020.lrec-1.452,Q15-1038,0,0.0127666,"TurkuNLP, and the OPUS collection of translated texts from the web. The test suite is available at http://github.com/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide speci"
2020.lrec-1.452,N19-1423,0,0.00878062,"bserved for that language pair. Figure 3 (right) shows that the expected effect holds for some language pairs (typically with English as target language), whereas the contrary effect is observed for other pairs (typically with English as source language). Experiments with a larger number of bins have shown similarly inconclusive outcomes. This outcome could hint at shortcomings of the sense embeddings used in this work (Mancini et al., 2017). In future work, we plan to evaluate more recent sense embedding approaches, for instance an approach based on big pre-trained language models like BERT (Devlin et al., 2019; Scarlini et al., 2020). Another way to assess sense distinctiveness is by including it directly in the evaluation metric, as proposed with the weighted precision score in Section 3. A comparison between the standard and the weighted precision scores is shown in Table 6. The weighted precision scores are generally higher, and the difference is proportional to the average similarity between senses (see Table 3). 5. Conclusion In this paper, we presented an extended version of M U C OW, an automatically built evaluation benchmark for measuring WSD capabilities of machine translation systems, av"
2020.lrec-1.452,P16-1191,0,0.0136942,"ated texts from the web. The test suite is available at http://github.com/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambi"
2020.lrec-1.452,K18-2013,0,0.0234719,"20 examples were found are discarded. Unambiguous source nouns, i.e., those associated with only one sense id, are discarded as well. We call each pairing of a source noun with a sense id sense cluster. 2.2. 2. Methodology: Building M U C OW The gist of our approach lies in the combination of different resources and tools: the wide-coverage multilingual sense inventory of BabelNet (Navigli and Ponzetto, 2012) and its associated sense embeddings (Mancini et al., 2017), the OPUS collection of translated texts from the web (Tiedemann, 2012), and the multilingual neural parsing pipeline TurkuNLP (Kanerva et al., 2018). In the following, we describe the three steps needed to create a M U C OW test suite. 2.1. It is known that lexical resources such as BabelNet tend to suffer from overly fine granularity of their sense inventory (Navigli, 2006; Palmer et al., 2007). We therefore introduce two additional merging steps: 1. We merge those sense clusters that share at least one common target word in BabelNet. 2. We merge sense clusters with similar meanings, as defined by their sense embeddings (Mancini et al., 2017). Concretely, following earlier work (Raganato et al., 2019), we merge senses whose cosine simila"
2020.lrec-1.452,P19-1568,0,0.0178262,"comprising training data with known sense distributions. Our approach for the construction of the benchmark builds upon the wide-coverage multilingual sense inventory of BabelNet, the multilingual neural parsing pipeline TurkuNLP, and the OPUS collection of translated texts from the web. The test suite is available at http://github.com/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific lingu"
2020.lrec-1.452,N18-1121,0,0.0207783,"oureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambiguity of nouns, to our knowledge, two major test suites exist: ContraWSD (Rios et al., 2017; Rios et al., 2018) and M U C OW (Raganato et al., 2019). Both test suites are available in two variants: scoring and translation. The first variant relies on the ability of neural machine translation systems"
2020.lrec-1.452,P19-1569,0,0.0154071,"ion for 10 language pairs, comprising training data with known sense distributions. Our approach for the construction of the benchmark builds upon the wide-coverage multilingual sense inventory of BabelNet, the multilingual neural parsing pipeline TurkuNLP, and the OPUS collection of translated texts from the web. The test suite is available at http://github.com/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To m"
2020.lrec-1.452,K17-1012,0,0.410241,"In the second case (Quelle), the first and second clusters are merged because their similarity is above the threshold. Sense ids for which less than 20 examples were found are discarded. Unambiguous source nouns, i.e., those associated with only one sense id, are discarded as well. We call each pairing of a source noun with a sense id sense cluster. 2.2. 2. Methodology: Building M U C OW The gist of our approach lies in the combination of different resources and tools: the wide-coverage multilingual sense inventory of BabelNet (Navigli and Ponzetto, 2012) and its associated sense embeddings (Mancini et al., 2017), the OPUS collection of translated texts from the web (Tiedemann, 2012), and the multilingual neural parsing pipeline TurkuNLP (Kanerva et al., 2018). In the following, we describe the three steps needed to create a M U C OW test suite. 2.1. It is known that lexical resources such as BabelNet tend to suffer from overly fine granularity of their sense inventory (Navigli, 2006; Palmer et al., 2007). We therefore introduce two additional merging steps: 1. We merge those sense clusters that share at least one common target word in BabelNet. 2. We merge sense clusters with similar meanings, as def"
2020.lrec-1.452,P06-1014,0,0.0818786,"C OW The gist of our approach lies in the combination of different resources and tools: the wide-coverage multilingual sense inventory of BabelNet (Navigli and Ponzetto, 2012) and its associated sense embeddings (Mancini et al., 2017), the OPUS collection of translated texts from the web (Tiedemann, 2012), and the multilingual neural parsing pipeline TurkuNLP (Kanerva et al., 2018). In the following, we describe the three steps needed to create a M U C OW test suite. 2.1. It is known that lexical resources such as BabelNet tend to suffer from overly fine granularity of their sense inventory (Navigli, 2006; Palmer et al., 2007). We therefore introduce two additional merging steps: 1. We merge those sense clusters that share at least one common target word in BabelNet. 2. We merge sense clusters with similar meanings, as defined by their sense embeddings (Mancini et al., 2017). Concretely, following earlier work (Raganato et al., 2019), we merge senses whose cosine similarity is higher than 0.3. Step 1: Identify ambiguous source words and their translations For each language pair, we determine a set of parallel text sources mainly from the OPUS collection (see Table 1).1 Based on our previous ex"
2020.lrec-1.452,P02-1040,0,0.108776,"Missing"
2020.lrec-1.452,P17-1170,0,0.022727,"test suite is available at http://github.com/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambiguity of nouns, to our"
2020.lrec-1.452,W19-7602,0,0.0872284,"Missing"
2020.lrec-1.452,W18-6319,0,0.0354383,"Missing"
2020.lrec-1.452,Q18-1044,0,0.0925728,"o et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambiguity of nouns, to our knowledge, two major test suites exist: ContraWSD (Rios et al., 2017; Rios et al., 2018) and M U C OW (Raganato et al., 2019). Both test suites are available in two variants: scoring and translation. The first variant relies on the ability of neural machine t"
2020.lrec-1.452,D17-1120,1,0.887901,"Missing"
2020.lrec-1.452,P16-1162,0,0.0418738,"0.4626 0.4651 0.5193 0.4142 0.4428 0.6752 0.7047 0.6977 0.7070 0.6982 0.7871 0.8383 0.7651 0.8008 0.8281 0.8054 0.8352 0.8315 0.8704 0.8811 0.9168 0.9311 0.8896 0.9433 0.9001 0.5437 0.4701 0.5444 0.3888 0.4947 0.7320 0.7120 0.7159 0.6968 0.7546 0.8202 0.8525 0.7984 0.7998 0.8516 0.8370 0.8465 0.8476 0.8869 0.8911 0.9296 0.9364 0.9099 0.9480 0.9239 Table 5: F1-scores of the baseline models, broken down by frequency bins. Bolded values indicate an improvement of at least 0.03 absolute compared to the other (Small or Big) model. pair. Sentences are encoded using Truecaser and Byte-Pair Encoding (Sennrich et al., 2016), with 32 000 merge operations for each language, learned on each training corpus separately. Note that these models are not specifically adapted towards good sense disambiguation performance. They merely show how well off-the-shelf NMT architectures perform on lexical ambiguities. It is expected that architectures specifically adapted to WSD (Pu et al., 2018, for instance) would show substantially higher scores. Table 4 summarizes the results of our experiments. The first column indicates general translation quality, as measured by BLEU7 score on the Newstest corpora.8 The Big training corpus"
2020.lrec-1.452,P19-1105,0,0.0178471,"/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambiguity of nouns, to our knowledge, two major test suites exist: Contr"
2020.lrec-1.452,W18-6304,0,0.0165359,"containing an ambiguous source word, an evaluation script checks whether any of the correct or incorrect target senses can be identified in the translation output. While both variants have different pros and cons, the translation one allows an evaluation directly on the output of a system, avoiding the need for a function for scoring a translation, which is typically not available for online systems or unsupervised MT approaches. Recent works suggest that the state-of-the-art Transformer architecture (Vaswani et al., 2017) for neural MT (NMT) is able to deal with lexical ambiguity quite well (Tang et al., 2018; Tang et al., 2019), learning to distinguish between senses during translation with high precision. Prior works have shown that NMT models based on Recurrent Neural Networks (RNNs) struggle when dealing with rare word senses (Rios et al., 2017), but it is not fully clear how well the more recent Transformer architecture performs under different sense frequencies, size of training corpora and across different language pairs. In earlier work (Raganato et al., 2019), we have presented M U C OW, a language-independent and fully automatic method for building a test suite for lexically ambiguous no"
2020.lrec-1.452,D19-1149,0,0.0136627,"uous source word, an evaluation script checks whether any of the correct or incorrect target senses can be identified in the translation output. While both variants have different pros and cons, the translation one allows an evaluation directly on the output of a system, avoiding the need for a function for scoring a translation, which is typically not available for online systems or unsupervised MT approaches. Recent works suggest that the state-of-the-art Transformer architecture (Vaswani et al., 2017) for neural MT (NMT) is able to deal with lexical ambiguity quite well (Tang et al., 2018; Tang et al., 2019), learning to distinguish between senses during translation with high precision. Prior works have shown that NMT models based on Recurrent Neural Networks (RNNs) struggle when dealing with rare word senses (Rios et al., 2017), but it is not fully clear how well the more recent Transformer architecture performs under different sense frequencies, size of training corpora and across different language pairs. In earlier work (Raganato et al., 2019), we have presented M U C OW, a language-independent and fully automatic method for building a test suite for lexically ambiguous nouns. Here, we report"
2020.lrec-1.452,P12-1029,0,0.0251856,"al neural parsing pipeline TurkuNLP, and the OPUS collection of translated texts from the web. The test suite is available at http://github.com/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic p"
2020.lrec-1.452,2013.iwslt-evaluation.1,0,0.0874185,"Missing"
2020.lrec-1.452,eisele-chen-2010-multiun,0,0.0194206,"Missing"
2020.lrec-1.452,2005.mtsummit-papers.11,0,0.396432,"Missing"
2020.lrec-1.452,W19-5354,1,0.938821,"T), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambiguity of nouns, to our knowledge, two major test suites exist: ContraWSD (Rios et al., 2017; Rios et al., 2018) and M U C OW (Raganato et al., 2019). Both test suites are available in two variants: scoring and translation. The first variant relies on the ability of neural machine translation systems to score given translations: a sentence containing an ambiguous source word is paired with the correct reference translation and with a modified translation in which the ambiguous word has been replaced by a word of a different sense. A contrast is considered successfully detected if the reference translation obtains a higher score than the artificially modified translation. The second variant relies directly on the translation produced by the"
2020.lrec-1.452,W17-4702,0,0.114354,"tion (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambiguity of nouns, to our knowledge, two major test suites exist: ContraWSD (Rios et al., 2017; Rios et al., 2018) and M U C OW (Raganato et al., 2019). Both test suites are available in two variants: scoring and translation. The first variant relies on the ability of"
2020.lrec-1.452,W18-6437,0,0.0323486,"Missing"
2020.lrec-1.452,tiedemann-2012-parallel,1,0.711047,"e their similarity is above the threshold. Sense ids for which less than 20 examples were found are discarded. Unambiguous source nouns, i.e., those associated with only one sense id, are discarded as well. We call each pairing of a source noun with a sense id sense cluster. 2.2. 2. Methodology: Building M U C OW The gist of our approach lies in the combination of different resources and tools: the wide-coverage multilingual sense inventory of BabelNet (Navigli and Ponzetto, 2012) and its associated sense embeddings (Mancini et al., 2017), the OPUS collection of translated texts from the web (Tiedemann, 2012), and the multilingual neural parsing pipeline TurkuNLP (Kanerva et al., 2018). In the following, we describe the three steps needed to create a M U C OW test suite. 2.1. It is known that lexical resources such as BabelNet tend to suffer from overly fine granularity of their sense inventory (Navigli, 2006; Palmer et al., 2007). We therefore introduce two additional merging steps: 1. We merge those sense clusters that share at least one common target word in BabelNet. 2. We merge sense clusters with similar meanings, as defined by their sense embeddings (Mancini et al., 2017). Concretely, follo"
2020.lrec-1.467,J96-1001,0,0.179916,"creating a new bitext A → B. Section 4. illustrates the use by the example of the creation of MultiParaCrawl. Finally, another alignment tool, opus-pt2dice, extracts rough probabilistic bilingual dictionaries from phrase-translation-tables created from word alignment and using SMT tools coming out of the Moses toolbox. Those dictionaries use some heuristics to filter the data and the tool also creates additional Dice scores as a symmetrised alignment value out of the conditional translation probabilities included in the original phrase tables, which is useful for bilingual lexicon extraction (Smadja et al., 1996). Other tools: The last tool category contains additional data processing tools such as opus-udpipe and opus-index. The former implements a wrapper around UDPipe (Straka and Strakov´a, 2017) to annotate OPUS data and to store the result in OPUS-conforming XML. OpusTools can use pre-trained models coming from LINDAT.7 Last but not least, opus-index is a tool for indexing OPUS corpora using the Corpus Work Bench (CWB) (Evert and Hardie, 2011). It creates all import files and runs the encoder if available to create multiparallel corpora to be queried using the CWB search engine. 4. ParaCrawl and"
2020.lrec-1.467,2005.mtsummit-papers.11,0,0.229261,"leased with different formats. Alignment tools: Sentence and word alignment can be used in various ways and these tools provide some convenient operations on top of aligned bitexts. Other processing tools: This category includes tools for annotation and indexing. In the first category, we have import tools such as moses2opus, tmx2opus and xml2opus. Export scripts include opus2moses, tmx2moses, opus2text and opus2multi. xml2opus is a simple script that adds sentence boundaries to arbitrary XML data. Sentence boundary detection is done using the tools released with the Europarl parallel corpus (Koehn, 2005) and packaged in the Perl module Lingua::Sentence. Additional tools based on UD treebank classifiers will be integrated in the future. Inline tags that add markup within sentences are not supported at the moment. moses2opus reads aligned plain text files as commonly used in machine translation with aligned sentences on the same line.6 The tool converts the data into simple standalone XML for the corpus data and the XCES Align format for standoff sentence alignment as it is used within OPUS. Currently, only bilingual input is supported. Plain text files do not contain sentence boundaries but st"
2020.lrec-1.467,tiedemann-2012-parallel,1,0.785192,"ls also includes tools for language identification and data filtering as well as tools for importing data from various sources into the OPUS format. We show the use of these tools in parallel corpus creation and data diagnostics. The latter is especially useful for the identification of potential problems and errors in the extensive data set. Using these tools, we can now monitor the validity of data sets and improve the overall quality and consistency of the data collection. Keywords: Corpus (Creation, Annotation, etc.); Machine Translation; Tools, Systems, Applications 1. Introduction OPUS (Tiedemann, 2012) is the biggest collection of openly available parallel corpora. The collection has been growing constantly over the years and is widely used in work on machine translation and cross-linguistic research. Currently it contains 57 released corpora covering over 700 languages and language variants creating more than 70,000 bitexts in the sense of aligned language pairs across all corpora in the collection. The size and popularity of OPUS makes it necessary to build an efficient infrastructure that enables the various users to obtain and access the data and this paper introduces two packages that"
2020.lrec-1.470,P19-1309,0,0.0701269,"guages. Further, typically only a small fraction of sentences can be assumed to have a translation counterpart in such data. These methods allow us to utilize also text sources inaccessible to methods that assume a prior document-level alignment. Bitext mining from incomparable corpora relies on crosslingual sentence embeddings constructed such that sentences from different languages are embedded into a single vector space, allowing their subsequent comparison using, for instance, the cosine similarity measure. Here we apply the LASER embeddings (Language Agnostic SEntence Representations) of Artetxe and Schwenk (2019b). This approach is based on an encoder-decoder architecture, with a shared sentence encoder, and a set of language-specific decoders. The encoder and decoders are trained using existing parallel data and the shared encoder can subsequently data set Web crawl ¨ crawl FISKMO YLE News Archive YLE News RSS Total gold 500K 85K 140K 25K 750K size silver mono (fi/sv) 2M 54M/90M 130K 4.3M/1.3M 300K 13.9M/4.5M 100K 1.4M/1.1M 2.5M 74M/97M Table 1: The number perfect or near-perfect (Gold) and partial translations or highly related (Silver) translation pairs extracted from different data collections. S"
2020.lrec-1.470,Q19-1038,0,0.101553,"guages. Further, typically only a small fraction of sentences can be assumed to have a translation counterpart in such data. These methods allow us to utilize also text sources inaccessible to methods that assume a prior document-level alignment. Bitext mining from incomparable corpora relies on crosslingual sentence embeddings constructed such that sentences from different languages are embedded into a single vector space, allowing their subsequent comparison using, for instance, the cosine similarity measure. Here we apply the LASER embeddings (Language Agnostic SEntence Representations) of Artetxe and Schwenk (2019b). This approach is based on an encoder-decoder architecture, with a shared sentence encoder, and a set of language-specific decoders. The encoder and decoders are trained using existing parallel data and the shared encoder can subsequently data set Web crawl ¨ crawl FISKMO YLE News Archive YLE News RSS Total gold 500K 85K 140K 25K 750K size silver mono (fi/sv) 2M 54M/90M 130K 4.3M/1.3M 300K 13.9M/4.5M 100K 1.4M/1.1M 2.5M 74M/97M Table 1: The number perfect or near-perfect (Gold) and partial translations or highly related (Silver) translation pairs extracted from different data collections. S"
2020.lrec-1.470,W19-6146,1,0.800955,"will return an aligned corpus in TMX format that can directly be useful in translation workflows and that we apply data pre-procesing and cleaning pipelines that help further data curation. We accept a variety of formats including websites, PDFs, Microsoft Word document to name a few. We also ask the general public to suggest sources like translated websites or to send translated documents through our public translation tool (see Figure 2). This is connected to a data crunching backend that runs data conversion, text extraction and alignment jobs. More details about that system are given in (Aulamo and Tiedemann, 2019). One common concern of potential data providers is related to privacy issues as discussed earlier. Some data is too sensitive to be published for general use. However, such data may still be used in model development and internal research. We offer different levels of privacy according to the choice of the provider. The data can be public, used internally in training public MT and language models or it can be used for training customized MT only for the provider itself. The main principles are illustrated in Figure 1. 2.2. Sentence-level Bitext Mining Recently, several methods have been propo"
2020.lrec-1.470,W18-6317,0,0.0116371,"ata is too sensitive to be published for general use. However, such data may still be used in model development and internal research. We offer different levels of privacy according to the choice of the provider. The data can be public, used internally in training public MT and language models or it can be used for training customized MT only for the provider itself. The main principles are illustrated in Figure 1. 2.2. Sentence-level Bitext Mining Recently, several methods have been proposed for bitext mining from incomparable, monolingual corpora (see for example (Espana-Bonet et al., 2017; Guo et al., 2018; Schwenk, 2018)). Such corpora, most typically resulting from web crawls, contain no metadata information that would allow direct linking of sentences, or at least documents across the languages. Further, typically only a small fraction of sentences can be assumed to have a translation counterpart in such data. These methods allow us to utilize also text sources inaccessible to methods that assume a prior document-level alignment. Bitext mining from incomparable corpora relies on crosslingual sentence embeddings constructed such that sentences from different languages are embedded into a sing"
2020.lrec-1.470,P18-4020,0,0.0267722,"Missing"
2020.lrec-1.470,2016.amta-researchers.9,0,0.0132079,"benefit is that the confidentiality and security of the MT system can be absolutely guaranteed. The risk posed by outages or discontinuation of external services is also eliminated. The deployment of the plugin is also simple, and can be performed by the end user. ¨ MT is also free to use, unlike most online MT FISKMO services intended for professional translation. From a technical point of view, the offline implementation ¨ plugin offers a promising basis for future of the FISKMO development of interactive MT capabilities that require low latency, such as interactive translation prediction (Knowles and Koehn, 2016). In an online MT system, these features are constrained by the latency of the two-way communication between the server and the client. Of course, an offline system is constrained by the available hardware, but the hardware constraint is not fixed like network latency, as it can be alleviated by optimization and design. A major motivation for creating the plugin was to generate interest in the data collection part of the project by offering potential partners a clear way to benefit from the ¨ project. This goal has been achieved, since the FISKMO MT plugin for SDL Trados Studio has already gen"
2020.lrec-1.470,P07-2045,0,0.00724838,"this, we apply the data we have collected in OPUS and the ¨ project and train state-of-the-art neural machine FISKMO translation (NMT) models using the popular transformer model (Vaswani et al., 2017) as implemented in MarianNMT (Junczys-Dowmunt et al., 2018). The training data comprises roughly 33 million training examples with about 1 billion tokens (counting both languages together). The data is derived from a wide mix of sources ranging from legislative texts to translated movie subtitles, software localization and general web content. We pre-process the data with the common Moses tools (Koehn et al., 2007) applying Unicode character normalization, corpus cleaning tools and the Moses tokenizer for Finnish and Swedish. Furthermore, we perform BPE-based subword segmentation using the Subword NMT package9 (Sennrich et al., 2016) with BPE models trained separately for each language and setting the number of merging operations to 32,000. For training the MT models we apply standard settings of a 6-layer transformer model (in both, encoder and decoder) with 8 self attention heads per layer, tied embed6 http://opus.nlpl.eu http://opus.nlpl.eu/bin/opuscqp.pl 8 https://version.helsinki.fi/Helsinki-NLP/fi"
2020.lrec-1.470,W15-2124,1,0.766044,"is computationally demanding, the highly optimized FAISS library of Johnson et al. (2019) is typically applied to carry the comparison out efficiently. Recently, this method was applied to Wikipedia, obtaining 135M parallel sentences for 85 languages (Schwenk et al., 2019a) and to CommonCrawl web crawl data, obtaining 3.5B parallel sentences for 38 languages (Schwenk et al., 2019b). Here, we use the LASER+FAISS method to extract Finnish-Swedish parallel data from several monolingual corpus pairs, whose sizes are summarized in Table 1. For web crawl data, we use the Finnish Internet Parsebank (Luotolahti et al., 2015), a large-scale dedicated Finnish web crawl corpus, and the Swedish section of the CoNLL-17 Shared Task raw data (Ginter et al., 2017), based on CommonCrawl. The news data are sourced from the Finnish national broadcast organization YLE 2011–2018 archive available through the Language Bank of Finland (YLE news archive) (Yleisradio, 2019b; Yleisradio, 2019a), and from the RSS feed of the Finnish and Swedish language YLE news (YLE News RSS), gathered during 2018–2019. Finally, we crawled web pages of various government organizations (FISKMO¨ crawl). For each corpus pair, we manually evaluated a"
2020.lrec-1.470,P02-1040,0,0.106958,"ems that support the translation between Finnish and Swedish, namely Google Translate15 and Presidency MT16 . The latter has been released in connection with the EU presidency of Finland and it has been heavily optimized for the translation from Finnish into English and Swedish. It is developed by Tilde, a Latvian language service provider, in collaboration with the Prime Minister’s Office of Finland using large data sets collected from European resources and data provided by the Finnish authorities. Both on-line systems have been accessed on October 29, 2019 and the results in terms of BLEU (Papineni et al., 2002) and chr-F2 (Popovi´c, 2015) scores are shown in Table 3. From the results, we can see that our system fairs quite well in comparison to both on-line translation engines. Note that it is not optimised for the data set in any way and trained without any fine-tuning for any specific domain. The advantage over Google Translate is astonishing showing that uncommon language pairs are still not well supported by general-purpose engines. In comparison to the Presidency MT engine, our system performs on a similar scale, slightly better for the translation into Finnish but worse in the other directions"
2020.lrec-1.470,W15-3049,0,0.0666906,"Missing"
2020.lrec-1.470,W18-6319,0,0.0340736,"Missing"
2020.lrec-1.470,P18-2037,0,0.0202527,"ve to be published for general use. However, such data may still be used in model development and internal research. We offer different levels of privacy according to the choice of the provider. The data can be public, used internally in training public MT and language models or it can be used for training customized MT only for the provider itself. The main principles are illustrated in Figure 1. 2.2. Sentence-level Bitext Mining Recently, several methods have been proposed for bitext mining from incomparable, monolingual corpora (see for example (Espana-Bonet et al., 2017; Guo et al., 2018; Schwenk, 2018)). Such corpora, most typically resulting from web crawls, contain no metadata information that would allow direct linking of sentences, or at least documents across the languages. Further, typically only a small fraction of sentences can be assumed to have a translation counterpart in such data. These methods allow us to utilize also text sources inaccessible to methods that assume a prior document-level alignment. Bitext mining from incomparable corpora relies on crosslingual sentence embeddings constructed such that sentences from different languages are embedded into a single vector space,"
2020.lrec-1.470,P16-1162,0,0.0120622,"MarianNMT (Junczys-Dowmunt et al., 2018). The training data comprises roughly 33 million training examples with about 1 billion tokens (counting both languages together). The data is derived from a wide mix of sources ranging from legislative texts to translated movie subtitles, software localization and general web content. We pre-process the data with the common Moses tools (Koehn et al., 2007) applying Unicode character normalization, corpus cleaning tools and the Moses tokenizer for Finnish and Swedish. Furthermore, we perform BPE-based subword segmentation using the Subword NMT package9 (Sennrich et al., 2016) with BPE models trained separately for each language and setting the number of merging operations to 32,000. For training the MT models we apply standard settings of a 6-layer transformer model (in both, encoder and decoder) with 8 self attention heads per layer, tied embed6 http://opus.nlpl.eu http://opus.nlpl.eu/bin/opuscqp.pl 8 https://version.helsinki.fi/Helsinki-NLP/fiskmo 9 https://github.com/rsennrich/subword-nmt 7 4 size 199K 322K 473K https://github.com/danielvarga/hunalign https://github.com/Helsinki-NLP/uplug 3811 ¨ translation interface. Figure 2: The public FISKMO dings and a sha"
2020.lrec-1.470,K17-3009,0,0.0645588,"Missing"
2020.semeval-1.205,2020.lrec-1.758,0,0.0138949,"C (Nikolov and Radivchev, 2019). For the Danish dataset (Sigurbergsson and Derczynski, 2020) we used Nordic BERT1 , which is pretrained on Danish Wikipedia texts, Danish text from Common Crawl, Danish OpenSubtitles, and text from popular Danish online forums. All in all, the training corpus consists of over 90M sentences and almost 20M unique tokens. For the other languages we used the standard BERT-Base models (Devlin et al., 2019) with no further pre-training and only the provided datasets for each language, i.e. Arabic (Mubarak et al., 2020), Greek (Pitenis et al., 2020), and Turkish (C¸o¨ ltekin, 2020). In sub-task C, since the language was English it was possible to use the Offensive Language Identification Dataset (OLID), provided by the organizers of OffensEval 2019. Despite consisting of different sub-tasks, all of them shared the same dataset that was annotated according to a three-level hierarchical model, so that each sub-task could use as dataset a subset of the previous sub-task’s dataset. First, all tweets were labeled as either offensive (OFF) or not offensive (NOT). Then, for sub-task B, all the offensive tweets were labeled as targeted (TIN) or untargeted insults (UNT). And fin"
2020.semeval-1.205,N19-1423,0,0.129796,"her an offensive tweet is targeted or untargeted. C. Offense Target Identification: whether a targeted offensive tweet is directed towards an individual, a group or otherwise. In this paper the system created by the LT@Helsinki team for sub-tasks A and C will be described. In sub-task A we participated in all the language tracks. For sub-task C the only language available was English. We qualified as second in sub-task C and our submission for sub-task A ranked first for Danish, seventh for Greek, eighteenth for Turkish, and forty-sixth for Arabic. In all submissions we used BERT-Base models (Devlin et al., 2019) fine-tuned on each dataset provided by the task organizers. We also experimented with random forest with TF-IDF and other kinds of features, but the results on the development set were not as good as with transfer learning techniques based on pre-trained language models. We discovered that, at least for this data, the language-specific model worked better than the multilingual. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 1569 Proceedings of the 14th International Workshop on Semantic Evalu"
2020.semeval-1.205,W17-3013,0,0.0651211,"Missing"
2020.semeval-1.205,W18-4401,0,0.0536926,"Missing"
2020.semeval-1.205,W18-4400,0,0.259607,"Missing"
2020.semeval-1.205,S19-2011,0,0.0184338,"aswani et al., 2017), which allows it to better understand a word’s context by looking at both its left and right neighbours. BERT provides out-of-the-box pre-trained monolingual and multilingual models that, after massive training on general corpora, can be fine-tuned with a small amount of task-specific data and still offer excellent performance. The results published in OffensEval’s previous edition (Zampieri et al., 2019b) proved that BERT is well suited for the offensive language detection task, since it was the chosen method of most of the top teams, including the winners of subtasks A (Liu et al., 2019) and C (Nikolov and Radivchev, 2019). For the Danish dataset (Sigurbergsson and Derczynski, 2020) we used Nordic BERT1 , which is pretrained on Danish Wikipedia texts, Danish text from Common Crawl, Danish OpenSubtitles, and text from popular Danish online forums. All in all, the training corpus consists of over 90M sentences and almost 20M unique tokens. For the other languages we used the standard BERT-Base models (Devlin et al., 2019) with no further pre-training and only the provided datasets for each language, i.e. Arabic (Mubarak et al., 2020), Greek (Pitenis et al., 2020), and Turkish ("
2020.semeval-1.205,S19-2123,0,0.0652083,"ch allows it to better understand a word’s context by looking at both its left and right neighbours. BERT provides out-of-the-box pre-trained monolingual and multilingual models that, after massive training on general corpora, can be fine-tuned with a small amount of task-specific data and still offer excellent performance. The results published in OffensEval’s previous edition (Zampieri et al., 2019b) proved that BERT is well suited for the offensive language detection task, since it was the chosen method of most of the top teams, including the winners of subtasks A (Liu et al., 2019) and C (Nikolov and Radivchev, 2019). For the Danish dataset (Sigurbergsson and Derczynski, 2020) we used Nordic BERT1 , which is pretrained on Danish Wikipedia texts, Danish text from Common Crawl, Danish OpenSubtitles, and text from popular Danish online forums. All in all, the training corpus consists of over 90M sentences and almost 20M unique tokens. For the other languages we used the standard BERT-Base models (Devlin et al., 2019) with no further pre-training and only the provided datasets for each language, i.e. Arabic (Mubarak et al., 2020), Greek (Pitenis et al., 2020), and Turkish (C¸o¨ ltekin, 2020). In sub-task C, s"
2020.semeval-1.205,P19-1493,0,0.0189548,"ults obtained on previously unseen data indicate that none of our models were heavily overfitted. 5 Conclusions Our scores on evaluation data show that models pre-trained on general corpora can obtain competitive results when there is very little data available, as was the case of the Danish sub-task where we obtained the best results. BERT performed well on both tasks, but there is still room for improvement. In the future, we intend to experiment with those languages that we could not focus on this time. It should be noted that multilingual BERT works best with languages similar to English (Pires et al., 2019) so it is very likely that the other languages would have benefited from the use of language-specific models even more than Danish as Danish is the language most similar to English in comparison to Greek, Turkish, and Arabic. A more thorough comparison between multilingual and language-specific BERT would yield more definitive answers as to whether language-specific is always best or whether that is language-dependent. Further examination and use of emotion-laden words in offensive texts could also help with the detection of offensive texts. Examining other baseline methods and combining them"
2020.semeval-1.205,2020.lrec-1.629,0,0.0538574,"ers of subtasks A (Liu et al., 2019) and C (Nikolov and Radivchev, 2019). For the Danish dataset (Sigurbergsson and Derczynski, 2020) we used Nordic BERT1 , which is pretrained on Danish Wikipedia texts, Danish text from Common Crawl, Danish OpenSubtitles, and text from popular Danish online forums. All in all, the training corpus consists of over 90M sentences and almost 20M unique tokens. For the other languages we used the standard BERT-Base models (Devlin et al., 2019) with no further pre-training and only the provided datasets for each language, i.e. Arabic (Mubarak et al., 2020), Greek (Pitenis et al., 2020), and Turkish (C¸o¨ ltekin, 2020). In sub-task C, since the language was English it was possible to use the Offensive Language Identification Dataset (OLID), provided by the organizers of OffensEval 2019. Despite consisting of different sub-tasks, all of them shared the same dataset that was annotated according to a three-level hierarchical model, so that each sub-task could use as dataset a subset of the previous sub-task’s dataset. First, all tweets were labeled as either offensive (OFF) or not offensive (NOT). Then, for sub-task B, all the offensive tweets were labeled as targeted (TIN) or"
2020.semeval-1.205,2020.lrec-1.430,0,0.0953532,"looking at both its left and right neighbours. BERT provides out-of-the-box pre-trained monolingual and multilingual models that, after massive training on general corpora, can be fine-tuned with a small amount of task-specific data and still offer excellent performance. The results published in OffensEval’s previous edition (Zampieri et al., 2019b) proved that BERT is well suited for the offensive language detection task, since it was the chosen method of most of the top teams, including the winners of subtasks A (Liu et al., 2019) and C (Nikolov and Radivchev, 2019). For the Danish dataset (Sigurbergsson and Derczynski, 2020) we used Nordic BERT1 , which is pretrained on Danish Wikipedia texts, Danish text from Common Crawl, Danish OpenSubtitles, and text from popular Danish online forums. All in all, the training corpus consists of over 90M sentences and almost 20M unique tokens. For the other languages we used the standard BERT-Base models (Devlin et al., 2019) with no further pre-training and only the provided datasets for each language, i.e. Arabic (Mubarak et al., 2020), Greek (Pitenis et al., 2020), and Turkish (C¸o¨ ltekin, 2020). In sub-task C, since the language was English it was possible to use the Offe"
2020.semeval-1.205,N19-1144,0,0.055659,"Missing"
2020.semeval-1.205,S19-2010,0,0.0632612,"Missing"
2020.semeval-1.205,W16-4315,1,0.867796,"Missing"
2020.vardial-1.3,I11-1062,0,0.0194345,"differ in both the way the language data is represented and their general architecture. 4.1 Approaches to automatic language identification There is a vast amount of literature on language identification and most approaches base their predictions on character n-gram statistics and language model features such as estimated token probabilities. A comprehensive overview of approaches is provided by Jauhiainen et al. (2018). There are also recent approaches based on neural models and representation learning. In our work we focus on two popular tools representing purely statistical models (langID, Lui and Baldwin (2011)) and neural models (fastText, Joulin et al. (2016)). While fastText is also used for language identification, it is designed as a general text classification model, which is used for tasks like sentiment analysis as well. This orientation towards general text classification presumably is the reason for choosing a language representation based on bag of words and bag of word n-grams. If Low Saxon had a unified orthography, one would expect such a model to be more useful for dialect identification, which in this case would have to rely more on differences in lexicon and syntax. The basic struct"
2020.vardial-1.3,tiedemann-2012-parallel,1,0.604228,"daily life (Kornai, 2013). This is especially true for unstandardised languages like Low Saxon, where the lack of a written norm poses challenges for the development of modern NLP applications, which typically rely on large amounts of, ideally, orthographically uniform data. While a reference corpus exists for Middle Low Saxon (ReN-Team, 2019), the few datasets of modern Low Saxon available so far tend to either be very restricted content-wise (e.g. the DSA data (Wrede et al., 1927–1956)) or only represent a fraction of the language area without indication of the dialect (e.g. the OPUS data (Tiedemann, 2012)). In addition, both the DSA data and most of the OPUS data consist of content translated into Low Saxon instead of original texts, which will affect the naturalness of the language. The aim of this dataset for Low Saxon is thus to provide open and for the most part original data in Low Saxon covering nearly the whole language area in order to foster research and facilitate the development of NLP tools. The composition of this dataset and testing the suitability of language recognition tools is a first step in our larger research project on processing Low Saxon data and modelling the historica"
2020.wmt-1.139,2012.eamt-1.60,0,0.0319634,"include sentences from Tatoeba and neither from the popular WMT testsets to allow a fair comparison to other models that have been evaluated using those data sets. Here, we propose an open challenge and the idea is to encourage people to develop machine translation in real-world cases for many languages. The 1 2 http://opus.nlpl.eu/ https://tatoeba.org/ most important point is to get away from artificial setups that only simulate low-resource scenarios or zero-shot translations. A lot of research is tested with multi-parallel data sets and high resource languages using data sets such as WIT3 (Cettolo et al., 2012) or Europarl (Koehn, 2005) simply reducing or taking away one language pair for arguing about the capabilities of learning translation with little or without explicit training data for the language pair in question (see, e.g., Firat et al. (2016a,b); Ha et al. (2016); Lakew et al. (2018)). Such a setup is, however, not realistic and most probably over-estimates the ability of transfer learning making claims that do not necessarily carry over towards real-world tasks. In the set we provide here we, instead, include all available data from the collection without removing anything. In this way, t"
2020.wmt-1.139,N16-1101,0,0.017856,"chine translation in real-world cases for many languages. The 1 2 http://opus.nlpl.eu/ https://tatoeba.org/ most important point is to get away from artificial setups that only simulate low-resource scenarios or zero-shot translations. A lot of research is tested with multi-parallel data sets and high resource languages using data sets such as WIT3 (Cettolo et al., 2012) or Europarl (Koehn, 2005) simply reducing or taking away one language pair for arguing about the capabilities of learning translation with little or without explicit training data for the language pair in question (see, e.g., Firat et al. (2016a,b); Ha et al. (2016); Lakew et al. (2018)). Such a setup is, however, not realistic and most probably over-estimates the ability of transfer learning making claims that do not necessarily carry over towards real-world tasks. In the set we provide here we, instead, include all available data from the collection without removing anything. In this way, the data refers to a diverse and skewed collection, which reflects the real situation we need to work with and many lowresource languages are only represented by noisy or very unrelated training data. Zero-shot scenarios are only tested if no dat"
2020.wmt-1.139,W18-6319,0,0.0206173,"ions of dropping validation perplexities. We use SentencePiece (Kudo and Richardson, 2018) for the segmentation into subword units and apply a shared vocabulary of a maximum of 65,000 items. Language label tokens in the spirit of Johnson et al. (2017) are used in case of multiple language variants or scripts in the target language. Models for over 400 language pairs are currently available and we refer the reader to the website with the latest results. For illustration, we provide some example scores below in Table 1 using automatic evaluation based on chrF2 and BLEU computed using sacrebleu (Post, 2018). The actual translations are also available for each model and the distribution comes along with the logfiles from the training process and all necessary data files such as the SentencePiece models and vocabularies. language pair aze-eng bel-eng cat-eng eng-epo eng-glg eng-hye eng-ilo eng-run chrF2 0.490 0.268 0.668 0.577 0.593 0.404 0.569 0.436 BLEU 31.9 10.0 50.2 35.6 37.8 16.6 30.8 10.4 https://marian-nmt.github.io https://github.com/marian-nmt/marianexamples/tree/master/transformer 13 https://github.com/Helsinki-NLP/OPUS-MTtrain/blob/master/doc/TatoebaChallenge.md 12 Multilingual Models O"
2020.wmt-1.139,D16-1026,0,0.0290261,"Missing"
2020.wmt-1.139,L16-1680,0,0.0619759,"Missing"
2020.wmt-1.139,tiedemann-2012-parallel,1,0.834684,"etups that are common when demonstrating zero-shot or few-shot learning. For the first time, this package provides a comprehensive collection of diverse data sets in hundreds of languages with systematic language and script annotation and data splits to extend the narrow coverage of existing benchmarks. Together with the data release, we also provide a growing number of pre-trained baseline models for individual language pairs and selected language groups. 1 Introduction The Tatoeba translation challenge includes shuffled training data taken from OPUS,1 an open collection of parallel corpora (Tiedemann, 2012), and test data from Tatoeba,2 a crowd-sourced collection of user-provided translations in a large number of languages. All data sets are labeled with ISO639-3 language codes using macro-languages in case when available. Naturally, training data do not include sentences from Tatoeba and neither from the popular WMT testsets to allow a fair comparison to other models that have been evaluated using those data sets. Here, we propose an open challenge and the idea is to encourage people to develop machine translation in real-world cases for many languages. The 1 2 http://opus.nlpl.eu/ https://tato"
2020.wmt-1.139,2020.eamt-1.61,1,0.813728,"hem. 6 Baseline Models Along with the data, we also release baseline models that we train with state-of-the-art trans1178 former models (Vaswani et al., 2017) using MarianNMT,11 a stable production-ready NMT toolbox with efficient training and decoding capabilities (Junczys-Dowmunt et al., 2018). We apply a common setup with 6 self-attentive layers in both, the encoder and decoder network using 8 attention heads in each layer. The hyper-parameters follow the general recommendations given in the documentation of the software.12 The training procedures follow the strategy implemented in OPUSMT (Tiedemann and Thottingal, 2020) and detailed instructions are available from github.13 We train a selection of models on v100 GPUs with early-stopping after 10 iterations of dropping validation perplexities. We use SentencePiece (Kudo and Richardson, 2018) for the segmentation into subword units and apply a shared vocabulary of a maximum of 65,000 items. Language label tokens in the spirit of Johnson et al. (2017) are used in case of multiple language variants or scripts in the target language. Models for over 400 language pairs are currently available and we refer the reader to the website with the latest results. For illu"
2020.wmt-1.139,2005.mtsummit-papers.11,0,0.134657,"neither from the popular WMT testsets to allow a fair comparison to other models that have been evaluated using those data sets. Here, we propose an open challenge and the idea is to encourage people to develop machine translation in real-world cases for many languages. The 1 2 http://opus.nlpl.eu/ https://tatoeba.org/ most important point is to get away from artificial setups that only simulate low-resource scenarios or zero-shot translations. A lot of research is tested with multi-parallel data sets and high resource languages using data sets such as WIT3 (Cettolo et al., 2012) or Europarl (Koehn, 2005) simply reducing or taking away one language pair for arguing about the capabilities of learning translation with little or without explicit training data for the language pair in question (see, e.g., Firat et al. (2016a,b); Ha et al. (2016); Lakew et al. (2018)). Such a setup is, however, not realistic and most probably over-estimates the ability of transfer learning making claims that do not necessarily carry over towards real-world tasks. In the set we provide here we, instead, include all available data from the collection without removing anything. In this way, the data refers to a divers"
2020.wmt-1.139,P07-2045,0,0.0111807,"C, Tanzil, TED2013, TedTalks, TEP, TildeMODEL, Ubuntu, UN, UNPC, wikimedia, Wikipedia, WikiSource, XhosaNavy. The data sets are compiled from the pre-aligned bitexts but further cleaned in various ways. First of all, we remove non-printable characters and strings that violate Unicode encoding principles using regular expressions and a recoding trick using the forced encoding mode of recode (v3.7), a popular character conversion tool.3 Furthermore, we also de-escape special characters (like ’&’ encoded as ’&amp;’) that may appear in some of the corpora. For that, we apply the tools from Moses (Koehn et al., 2007). Finally, we also apply automatic language identification to remove additional noise 3 from the data. We use the compact language detect library (CLD2) through its Python bindings4 and a Python library for converting between different ISO-639 standards.5 CLD2 supports 172 languages and we use the options for ”best effort” and apply the assumed language from the original data as the ”hint language code”. For unsupported languages, we remove all examples that are detected to be English as this is a common problem in some corpora where English texts appear in various places (e.g. untranslated te"
2020.wmt-1.139,D18-2012,0,0.0138275,"training and decoding capabilities (Junczys-Dowmunt et al., 2018). We apply a common setup with 6 self-attentive layers in both, the encoder and decoder network using 8 attention heads in each layer. The hyper-parameters follow the general recommendations given in the documentation of the software.12 The training procedures follow the strategy implemented in OPUSMT (Tiedemann and Thottingal, 2020) and detailed instructions are available from github.13 We train a selection of models on v100 GPUs with early-stopping after 10 iterations of dropping validation perplexities. We use SentencePiece (Kudo and Richardson, 2018) for the segmentation into subword units and apply a shared vocabulary of a maximum of 65,000 items. Language label tokens in the spirit of Johnson et al. (2017) are used in case of multiple language variants or scripts in the target language. Models for over 400 language pairs are currently available and we refer the reader to the website with the latest results. For illustration, we provide some example scores below in Table 1 using automatic evaluation based on chrF2 and BLEU computed using sacrebleu (Post, 2018). The actual translations are also available for each model and the distributio"
2020.wmt-1.40,D15-1124,0,0.0620164,"Missing"
2020.wmt-1.40,K17-1012,0,0.0195405,"mbiguous source words, we could not identify any substantial progress – at least to the extent that it is measurable by the M U C OW method – in that area over the last year. 1 The M U C OW test suite 1. Identify ambiguous source nouns and their translations, using word-aligned and tagged parallel corpora from the OPUS collection (Tiedemann, 2012). 2. Cluster the translations into senses. First, we query BabelNet (Navigli and Ponzetto, 2012), a wide-coverage multilingual encyclopedic dictionary, to assign senses (synsets) to words. Second, we refine the results with the SW2V sense embeddings (Mancini et al., 2017). Introduction At WMT 2019, we introduced the M U C OW (multilingual contrastive word sense disambiguation) test suite (Raganato et al., 2019) and evaluated the news task submissions of nine translation directions with it.1 We observed that systems generally performed quite well on word sense disambiguation, but found a big gap between indomain and out-of-domain disambiguation performance for some translation directions, in particular with constrained systems. For WMT 2020, we reuse the same test suite for the same language pairs. This gives us the opportunity to measure the advancement of mac"
2020.wmt-1.40,W19-5354,1,0.879826,"in that area over the last year. 1 The M U C OW test suite 1. Identify ambiguous source nouns and their translations, using word-aligned and tagged parallel corpora from the OPUS collection (Tiedemann, 2012). 2. Cluster the translations into senses. First, we query BabelNet (Navigli and Ponzetto, 2012), a wide-coverage multilingual encyclopedic dictionary, to assign senses (synsets) to words. Second, we refine the results with the SW2V sense embeddings (Mancini et al., 2017). Introduction At WMT 2019, we introduced the M U C OW (multilingual contrastive word sense disambiguation) test suite (Raganato et al., 2019) and evaluated the news task submissions of nine translation directions with it.1 We observed that systems generally performed quite well on word sense disambiguation, but found a big gap between indomain and out-of-domain disambiguation performance for some translation directions, in particular with constrained systems. For WMT 2020, we reuse the same test suite for the same language pairs. This gives us the opportunity to measure the advancement of machine translation within a year. We expect the larger training data sets and the model improvements to have a small but positive impact on tran"
2020.wmt-1.40,2020.lrec-1.452,1,0.774552,"st of incorrect target words (the incorrect target synset), and information about the domain of the synsets. The participants only see the source sentences, not the metadata. Table 1 shows a few example sentences taken from the English–German test suite. The main statistics of the test suites used for WMT 2020 are reported in Table 2. Language Source Target In-dom Out-dom Senpair words synsets synsets synsets tences EN–CS EN–DE DE–EN EN–RU 98 176 217 97 200 362 461 199 29 220 329 40 171 142 132 163 1843 3337 4268 1814 Table 2: Sizes of the M U C OW data sets compiled for WMT 2019 and 2020. In Raganato et al. (2020), we report on an extended version of M U C OW that covers the following aspects: • The selection of data sources is improved to reduce noise and domain effects. 3 • The sense inference process is streamlined and relies on lemmatization instead of word alignment, leading to better coverage especially for morphologically rich languages. Evaluation and Results The source language sentences were sent to the WMT participants as part of the test set, and we received the translations in the target language for evaluation. We then checked if any of the correct or incorrect target words listed in the"
2020.wmt-1.40,W18-6456,0,0.0460527,"Missing"
2020.wmt-1.40,tiedemann-2012-parallel,1,0.554765,"n process is also entirely automated. We evaluate all participating systems of the language pairs English → Czech, English ↔ German, and English → Russian and compare the results with those obtained at WMT 2019. While current NMT systems are fairly good at handling ambiguous source words, we could not identify any substantial progress – at least to the extent that it is measurable by the M U C OW method – in that area over the last year. 1 The M U C OW test suite 1. Identify ambiguous source nouns and their translations, using word-aligned and tagged parallel corpora from the OPUS collection (Tiedemann, 2012). 2. Cluster the translations into senses. First, we query BabelNet (Navigli and Ponzetto, 2012), a wide-coverage multilingual encyclopedic dictionary, to assign senses (synsets) to words. Second, we refine the results with the SW2V sense embeddings (Mancini et al., 2017). Introduction At WMT 2019, we introduced the M U C OW (multilingual contrastive word sense disambiguation) test suite (Raganato et al., 2019) and evaluated the news task submissions of nine translation directions with it.1 We observed that systems generally performed quite well on word sense disambiguation, but found a big ga"
2021.acl-srw.35,W19-4820,0,0.0187226,"Both metrics are corrected for anisotropy via subtracting the corresponding AvgSim, assuming AvgSim as a measure of anisotropy. Measures of Representational Similarity. We measure the similarities between pairs of layers of both models using Representational Similarity Analysis (RSA) (Laakso and Cottrell, 2000; Kriegeskorte et al., 2008) and Projection-Weighted Canonical Correlation Analysis (PWCCA) (Morcos et al., 2018) as task-agnostic measures. RSA, originally developed for neuroscience, and later adopted for quantifying the similarity between neural networks (Chrupała and Alishahi, 2019; Abnar et al., 2019) works by taking a set of input stimuli of size n, and running them through the models to be compared. For each model, the activations to each of the n stimuli points are pairwise compared to each other using a similarity metric to compute a an adjacency matrix of size [n × n] between the stimuli points obtained. These matrices are then contrasted against each other using the Pearson’s correlation coefficient, giving a measure of the ""representational similarity"". PWCCA is an extension over the SVCCA (Singular Value Canonical Correlation Analysis) distance measure (Raghu et al., 2017), which c"
2021.acl-srw.35,S16-1081,0,0.0127726,"oduced by BERT and the encoder of a Transformer trained on the MT task. BERT is composed of 12 layers, plus an initial input embedding layer, with a dimension of 768. The MT system we apply consists of an input embedding layer followed by 6 Transformer layers with a hidden dimension of 512. We use the pretrained bert-base-uncased model, as well as the pretrained English-German translation model opus-mt-en-de, both from the HuggingFace library (Wolf et al., 2019). Following Ethayarajh (2019), we extract embeddings using data from the SemEval Semantic Textual Similarity tasks from 2012 to 2016 (Agirre et al., 2016). Average similarity between random tokens. Figure 1 presents the layer-wise cosine similarity (top) and the Euclidean distance (bottom) distributions of randomly sampled words.The behavior of BERT in Figure 1(top) is consistent with the findings of Ethayarajh (2019). The level of anisotropy of the embedding representations throughout layers of BERT increases towards higher layers, with the exception of a slight drop at the last layer (L12), considering the average cosine similarity of the rep339 1 resentations as a proxy measure of anisotropy. Further, we notice Figure 1(bottom) that BERT emb"
2021.acl-srw.35,2020.lrec-1.467,1,0.670069,"mer encoders and serve as baselines: the MTbaseline model, a transformer-based MT model trained from scratch with the fine-tuning data (Table 2), and Huggingface en–de a state-ofthe-art, pretrained Transformer model. We compare the proposed alignment methods using M1, which uses only the explicit alignment transformation strategy, M2, which uses the implicit alignment via fine-tuning strategy, and the hybrid M3, which combines the two strategies. Data. We use data from the English-German sections of the MuST-C dataset (Di Gangi et al., 2019), Europarl (Koehn, 2005), extracted using OpusTools (Aulamo et al., 2020) and the development tarball from the WMT2019 news translation shared task (Bojar et al., 2019) in the proportions indicated in Table 2. We test using the MuST-C provided 342 test-split, newstest2014 (Bojar et al., 2014) and newstest2015 (Bojar et al., 2015), which were excluded from the train data. All of the data splits are attainable using our repository. We purposefully restrict the data amount used for training the alignments. Such aligned systems should be able to work under less intensive resource requirements. The size of the training data for both methods varies, because we try to kee"
2021.acl-srw.35,W15-3001,0,0.022153,"using M1, which uses only the explicit alignment transformation strategy, M2, which uses the implicit alignment via fine-tuning strategy, and the hybrid M3, which combines the two strategies. Data. We use data from the English-German sections of the MuST-C dataset (Di Gangi et al., 2019), Europarl (Koehn, 2005), extracted using OpusTools (Aulamo et al., 2020) and the development tarball from the WMT2019 news translation shared task (Bojar et al., 2019) in the proportions indicated in Table 2. We test using the MuST-C provided 342 test-split, newstest2014 (Bojar et al., 2014) and newstest2015 (Bojar et al., 2015), which were excluded from the train data. All of the data splits are attainable using our repository. We purposefully restrict the data amount used for training the alignments. Such aligned systems should be able to work under less intensive resource requirements. The size of the training data for both methods varies, because we try to keep the explicit alignment comparable to what was originally proposed for mBERT (Cao et al., 2020), whereas the implicit alignment via fine-tuning requires more data since the MT decoder is also to be fine-tuned. Results. Table 3 presents the BLEU scores for f"
2021.acl-srw.35,J93-2003,0,0.088407,"Missing"
2021.acl-srw.35,P19-1283,0,0.018992,"and the sentence mean vector. Both metrics are corrected for anisotropy via subtracting the corresponding AvgSim, assuming AvgSim as a measure of anisotropy. Measures of Representational Similarity. We measure the similarities between pairs of layers of both models using Representational Similarity Analysis (RSA) (Laakso and Cottrell, 2000; Kriegeskorte et al., 2008) and Projection-Weighted Canonical Correlation Analysis (PWCCA) (Morcos et al., 2018) as task-agnostic measures. RSA, originally developed for neuroscience, and later adopted for quantifying the similarity between neural networks (Chrupała and Alishahi, 2019; Abnar et al., 2019) works by taking a set of input stimuli of size n, and running them through the models to be compared. For each model, the activations to each of the n stimuli points are pairwise compared to each other using a similarity metric to compute a an adjacency matrix of size [n × n] between the stimuli points obtained. These matrices are then contrasted against each other using the Pearson’s correlation coefficient, giving a measure of the ""representational similarity"". PWCCA is an extension over the SVCCA (Singular Value Canonical Correlation Analysis) distance measure (Raghu e"
2021.acl-srw.35,D19-5611,0,0.0425608,"n particular BERT (Devlin et al., 2019), have ushered in a new era, allowing the separation of unsupervised pretraining of powerful representation spaces, from the supervised training of task-specific, comparatively shallow classifiers on top of these representations. BERT-based models have consistently shown stateof-the-art performance in a variety of tasks, which is largely attributed to the rich information captured by the representations. These capabilities and its Transformer-based architecture suggest that BERT could improve neural machine translation (NMT) as well. However, as shown by Clinchant et al. (2019), although useful, information encoded by BERT is not sufficient by itself for successful MT. The reason for this is still an open question. Some of the most widely accepted hypotheses to date argue that either there is a fundamental discrepancy between the masked language modeling training objective of BERT compared to the generative, left-to-right nature of the MT objective (Song et al., 2019; Lewis et al., 2020) ; or that catastrophic forgetting (Goodfellow et al., 2015) takes place when learning the MT objective on top of the pretrained LM (Merchant et al., 2020). The latter could be cause"
2021.acl-srw.35,L18-1269,0,0.0232308,"resentations more, and adjusting to the sentence context less. Layer-wise similarity analysis between models. Figures 3 and 4 show how the responses of M1/M2/M3 become significantly similar to that of the MT model post-alignment. Note that interestingly the explicit alignment method is particularly successful in achieving similarity to the MT model, in terms of similarities between responses to pairs Related Work Analysis of contextualized representations. While there has been huge efforts to analyze word representations, most of it has been conducted using probing tasks (McCann et al., 2017; Conneau and Kiela, 2018; Conneau et al., 2018; Hewitt and Manning, 2019). Similarly, Merchant et al. (2020) study the effects of fine-tuning BERT representations on a specific set of probing tasks and analyse the change in the contextual representations using similarity analysis. Mimno and Thompson (2017) quantitatively studied static word representations produced with skip-gram with negative sampling. Their work was extended by Ethayarajh (2019) for contextualized embeddings, in which they use word level measures of contextuality to contrast ELMo (Peters et al., 2018), GPT-2 (Radford et al., 2019) and BERT (Devlin"
2021.acl-srw.35,P18-1198,0,0.0226306,"justing to the sentence context less. Layer-wise similarity analysis between models. Figures 3 and 4 show how the responses of M1/M2/M3 become significantly similar to that of the MT model post-alignment. Note that interestingly the explicit alignment method is particularly successful in achieving similarity to the MT model, in terms of similarities between responses to pairs Related Work Analysis of contextualized representations. While there has been huge efforts to analyze word representations, most of it has been conducted using probing tasks (McCann et al., 2017; Conneau and Kiela, 2018; Conneau et al., 2018; Hewitt and Manning, 2019). Similarly, Merchant et al. (2020) study the effects of fine-tuning BERT representations on a specific set of probing tasks and analyse the change in the contextual representations using similarity analysis. Mimno and Thompson (2017) quantitatively studied static word representations produced with skip-gram with negative sampling. Their work was extended by Ethayarajh (2019) for contextualized embeddings, in which they use word level measures of contextuality to contrast ELMo (Peters et al., 2018), GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2019). Voita e"
2021.acl-srw.35,N19-1423,0,0.192344,"edding spaces they create, using average cosine similarity, contextuality metrics and measures for representational similarity for comparison, revealing that BERT and NMT encoder representations look significantly different from one another. In order to address this issue, we propose a supervised transformation from one into the other using explicit alignment and fine-tuning. Our results demonstrate the need for such a transformation to improve the applicability of BERT in MT. 1 Introduction Contextualized token representations produced by pretrained language models (LMs), in particular BERT (Devlin et al., 2019), have ushered in a new era, allowing the separation of unsupervised pretraining of powerful representation spaces, from the supervised training of task-specific, comparatively shallow classifiers on top of these representations. BERT-based models have consistently shown stateof-the-art performance in a variety of tasks, which is largely attributed to the rich information captured by the representations. These capabilities and its Transformer-based architecture suggest that BERT could improve neural machine translation (NMT) as well. However, as shown by Clinchant et al. (2019), although usefu"
2021.acl-srw.35,N19-1202,0,0.0424695,"Missing"
2021.acl-srw.35,D19-1006,0,0.281038,"estigate how the embedding spaces of BERT 337 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 337–347 August 5–6, 2021. ©2021 Association for Computational Linguistics and MT differ by making a layer-by-layer comparison of these spaces. First, we measure the level of isotropy of these spaces using the average cosine similarity (AvgSim) between the representations of uniformly randomly sampled words from different contexts (Ethayarajh, 2019). (An)isotropy corresponds to the degree of directional (non)uniformity in an embedding space, where perfect isotropy implies directional uniformity in the distribution word vectors. It is important to consider (an)isotropy when discussing contextuality since cosine similarity is relative to the directional uniformity of the sample space. Then, we also generalize AvgSim to using the Euclidean distance as our distance metric. Understanding how cosine similarity and the Euclidean distance interact allows for a more complete understanding of the space. We also make a layer-wise comparison using t"
2021.acl-srw.35,N19-1419,0,0.0250561,"e context less. Layer-wise similarity analysis between models. Figures 3 and 4 show how the responses of M1/M2/M3 become significantly similar to that of the MT model post-alignment. Note that interestingly the explicit alignment method is particularly successful in achieving similarity to the MT model, in terms of similarities between responses to pairs Related Work Analysis of contextualized representations. While there has been huge efforts to analyze word representations, most of it has been conducted using probing tasks (McCann et al., 2017; Conneau and Kiela, 2018; Conneau et al., 2018; Hewitt and Manning, 2019). Similarly, Merchant et al. (2020) study the effects of fine-tuning BERT representations on a specific set of probing tasks and analyse the change in the contextual representations using similarity analysis. Mimno and Thompson (2017) quantitatively studied static word representations produced with skip-gram with negative sampling. Their work was extended by Ethayarajh (2019) for contextualized embeddings, in which they use word level measures of contextuality to contrast ELMo (Peters et al., 2018), GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2019). Voita et al. (2019) present a comp"
2021.acl-srw.35,D19-5603,0,0.0172682,"ared to BERT. Pretrained LMs in NMT. Clinchant et al. (2019) present a systematic comparison of methods to integrate BERT into NMT models, including using BERT at the embedding level or for initializing an encoder. Zhu et al. (2020) propose a BERT-fused MT system that uses additional attention modules between the outputs of BERT and the encoder and decoder of the Transformer, increasing the model parameters by the number of parameters the chosen BERT flavour has. Yang et al. (2020) proposes a similar strategy, though using BERT outputs only in the encoder, and a three-fold training technique. Imamura and Sumita (2019) propose a simple yet effective two-stage optimization technique that first freezes BERT, and then fine-tunes 344 over the full model parameters set. We argue that this is similar to the align and fine-tune approach we propose for incorporating BERT into MT. Finally, a number of studies leverage pretraining techniques. MASS (Song et al., 2019) is partly inspired by BERT, but it is pretrained in NMT and is tailored to match the way prediction is done in NMT (leftto-right). Liu et al. (2020) enhance transformerbased MT systems performance by using a BART pretraining technique (Lewis et al., 2020"
2021.acl-srw.35,2005.mtsummit-papers.11,0,0.0210199,"le 1). Two of these use 6layered Transformer encoders and serve as baselines: the MTbaseline model, a transformer-based MT model trained from scratch with the fine-tuning data (Table 2), and Huggingface en–de a state-ofthe-art, pretrained Transformer model. We compare the proposed alignment methods using M1, which uses only the explicit alignment transformation strategy, M2, which uses the implicit alignment via fine-tuning strategy, and the hybrid M3, which combines the two strategies. Data. We use data from the English-German sections of the MuST-C dataset (Di Gangi et al., 2019), Europarl (Koehn, 2005), extracted using OpusTools (Aulamo et al., 2020) and the development tarball from the WMT2019 news translation shared task (Bojar et al., 2019) in the proportions indicated in Table 2. We test using the MuST-C provided 342 test-split, newstest2014 (Bojar et al., 2014) and newstest2015 (Bojar et al., 2015), which were excluded from the train data. All of the data splits are attainable using our repository. We purposefully restrict the data amount used for training the alignments. Such aligned systems should be able to work under less intensive resource requirements. The size of the training da"
2021.acl-srw.35,2020.acl-main.703,0,0.21958,"by the representations. These capabilities and its Transformer-based architecture suggest that BERT could improve neural machine translation (NMT) as well. However, as shown by Clinchant et al. (2019), although useful, information encoded by BERT is not sufficient by itself for successful MT. The reason for this is still an open question. Some of the most widely accepted hypotheses to date argue that either there is a fundamental discrepancy between the masked language modeling training objective of BERT compared to the generative, left-to-right nature of the MT objective (Song et al., 2019; Lewis et al., 2020) ; or that catastrophic forgetting (Goodfellow et al., 2015) takes place when learning the MT objective on top of the pretrained LM (Merchant et al., 2020). The latter could be caused by the large size of the training data typically used in MT,and by the high capacity decoder network used in MT because to fit the high-capacity model well on massive data requires a huge number of training steps. However, since on the one hand, the left-to-right constraint in MT is potentially more relevant for the decoders than the typically bidirectional encoder that has access to the entire input sequence, an"
2021.acl-srw.35,D19-1387,0,0.0197861,"tter could be caused by the large size of the training data typically used in MT,and by the high capacity decoder network used in MT because to fit the high-capacity model well on massive data requires a huge number of training steps. However, since on the one hand, the left-to-right constraint in MT is potentially more relevant for the decoders than the typically bidirectional encoder that has access to the entire input sequence, and on the other hand, BERT and other pre-trained LMs have been successfully used for other complex problems with large training data and high capacity classifiers (Liu and Lapata, 2019; Witteveen and Andrews, 2019; Huang et al., 2021), it is reasonable to assume there may be further reasons for these discrepancies. We take a complementary stance and analyze the differences between the representation spaces produced by BERT and those produced by the MT objective. We therefore attempt to align these spaces, and investigate whether such an explicit alignment would reshape the BERT representation space to enable its use as an NMT encoder. To the best of our knowledge, this is the first study to investigate the intrinsic differences of pretrained LM and MT spaces, as well as the"
2021.acl-srw.35,2020.tacl-1.47,0,0.0223065,"lar strategy, though using BERT outputs only in the encoder, and a three-fold training technique. Imamura and Sumita (2019) propose a simple yet effective two-stage optimization technique that first freezes BERT, and then fine-tunes 344 over the full model parameters set. We argue that this is similar to the align and fine-tune approach we propose for incorporating BERT into MT. Finally, a number of studies leverage pretraining techniques. MASS (Song et al., 2019) is partly inspired by BERT, but it is pretrained in NMT and is tailored to match the way prediction is done in NMT (leftto-right). Liu et al. (2020) enhance transformerbased MT systems performance by using a BART pretraining technique (Lewis et al., 2020) in a multilingual fashion to initialize an NMT system. Alignment. Numerous methods have been proposed for aligning (contextualized) word representations (Och and Ney, 2003; Ruder et al., 2019). Wang et al. (2019) learn an optimal linear transformation between embedding spaces. Schuster et al. (2019) propose a similar approach using the centroids of the instances of the same word in different contexts. Our work is closer to Cao et al. (2020), which use a resource-efficient algorithm that"
2021.acl-srw.35,2020.blackboxnlp-1.4,0,0.0658332,"ell. However, as shown by Clinchant et al. (2019), although useful, information encoded by BERT is not sufficient by itself for successful MT. The reason for this is still an open question. Some of the most widely accepted hypotheses to date argue that either there is a fundamental discrepancy between the masked language modeling training objective of BERT compared to the generative, left-to-right nature of the MT objective (Song et al., 2019; Lewis et al., 2020) ; or that catastrophic forgetting (Goodfellow et al., 2015) takes place when learning the MT objective on top of the pretrained LM (Merchant et al., 2020). The latter could be caused by the large size of the training data typically used in MT,and by the high capacity decoder network used in MT because to fit the high-capacity model well on massive data requires a huge number of training steps. However, since on the one hand, the left-to-right constraint in MT is potentially more relevant for the decoders than the typically bidirectional encoder that has access to the entire input sequence, and on the other hand, BERT and other pre-trained LMs have been successfully used for other complex problems with large training data and high capacity class"
2021.acl-srw.35,D17-1308,0,0.0283838,"od is particularly successful in achieving similarity to the MT model, in terms of similarities between responses to pairs Related Work Analysis of contextualized representations. While there has been huge efforts to analyze word representations, most of it has been conducted using probing tasks (McCann et al., 2017; Conneau and Kiela, 2018; Conneau et al., 2018; Hewitt and Manning, 2019). Similarly, Merchant et al. (2020) study the effects of fine-tuning BERT representations on a specific set of probing tasks and analyse the change in the contextual representations using similarity analysis. Mimno and Thompson (2017) quantitatively studied static word representations produced with skip-gram with negative sampling. Their work was extended by Ethayarajh (2019) for contextualized embeddings, in which they use word level measures of contextuality to contrast ELMo (Peters et al., 2018), GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2019). Voita et al. (2019) present a comparison of contextualized representations trained with different objectives, using CCA and and mutual information to study information flow across networks. They conclude that although MT-produced representations do get refined with co"
2021.acl-srw.35,J03-1002,0,0.0374929,"to better match those of the MT encoder, and (ii) an implicit alignment effect achieved by a fine-tuning process which uses translation as its objective. Explicit Alignment Transformation. We build upon Cao et al. (2020), maximizing the contextual alignment the model can achieve via the average accuracy on the contextual word retrieval task. This method presents several advantages that can be leveraged in our work. It is multilingual, it respects contextuality of the embeddings, and it makes use of rather reliable, widely used and not-memory intensive alignment algorithms (Brown et al., 1993; Och and Ney, 2003) The task, as originally posed by Cao et al. (2020) is as follows. Given a parallel pre-aligned corpus C of source-target pairs (s, t), and one word within a source sentence, the objective is to find the corresponding target word. Let each sentence pair (s, t) have word pairs, denoted a(s, t) = (i1 , j1 ), ..., (im , jm ), containing position tuples (i, j) such that the words si and tj are translations of each other. We use a regularized loss function Loss = L + λR so that L aligns the embeddings from one model, f1 (i, s), to the ones of the other model f2 (j, t): 338 Value 1.0 MT en-de BERT 0"
2021.acl-srw.35,N18-1202,0,0.0234245,"ed using probing tasks (McCann et al., 2017; Conneau and Kiela, 2018; Conneau et al., 2018; Hewitt and Manning, 2019). Similarly, Merchant et al. (2020) study the effects of fine-tuning BERT representations on a specific set of probing tasks and analyse the change in the contextual representations using similarity analysis. Mimno and Thompson (2017) quantitatively studied static word representations produced with skip-gram with negative sampling. Their work was extended by Ethayarajh (2019) for contextualized embeddings, in which they use word level measures of contextuality to contrast ELMo (Peters et al., 2018), GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2019). Voita et al. (2019) present a comparison of contextualized representations trained with different objectives, using CCA and and mutual information to study information flow across networks. They conclude that although MT-produced representations do get refined with context, the change in those is not as extreme as for masked LM-produced representations (BERT-like), which is in line with our observations of higher SelfSim and lower IntraSim (i.e. not ultra-contextualized embeddings) for MT and aligned models as compared to BERT. Pre"
2021.acl-srw.35,N19-1162,0,0.0248125,"studies leverage pretraining techniques. MASS (Song et al., 2019) is partly inspired by BERT, but it is pretrained in NMT and is tailored to match the way prediction is done in NMT (leftto-right). Liu et al. (2020) enhance transformerbased MT systems performance by using a BART pretraining technique (Lewis et al., 2020) in a multilingual fashion to initialize an NMT system. Alignment. Numerous methods have been proposed for aligning (contextualized) word representations (Och and Ney, 2003; Ruder et al., 2019). Wang et al. (2019) learn an optimal linear transformation between embedding spaces. Schuster et al. (2019) propose a similar approach using the centroids of the instances of the same word in different contexts. Our work is closer to Cao et al. (2020), which use a resource-efficient algorithm that takes into account the contextuality of embeddings. 6 Conclusion This paper provides an analysis of the intrinsic differences between BERT and machine translation encoders. We compare the representation spaces of both models and pinpoint discrepancies between them. We show that this mismatch can be remedied through an alignment strategy, which successfully reshapes BERT into an effective MT encoder. We al"
2021.acl-srw.35,tiedemann-2012-parallel,1,0.645004,"Missing"
2021.acl-srw.35,D19-1448,0,0.201528,"istent with the findings of Ethayarajh (2019). The level of anisotropy of the embedding representations throughout layers of BERT increases towards higher layers, with the exception of a slight drop at the last layer (L12), considering the average cosine similarity of the rep339 1 resentations as a proxy measure of anisotropy. Further, we notice Figure 1(bottom) that BERT embeddings follow an inverted U-shape. This, together with the AvgSim trend, means that the embedding space starts by stretching out and becoming narrower, later on to spread out shorter embeddings in layer 12, in line with (Voita et al., 2019). The MT-based representations, however, look significantly different. The cosine-based AvgSim follows an almost U-like trend: it starts from a relatively high level at layer 0, then immediately drops and stays low throughout the middle layers, before a sudden increase at the final layer (L6). In particular: 1. a high average similarity of the MT embeddings in layer 0 is striking since the representations are not yet that “contextualized” this early in the model, and 2. the gradual increase of average similarity in BERT layers, versus the very steep increase at the last layer of MT model. Beha"
2021.acl-srw.35,D19-1575,0,0.0325776,"Missing"
2021.acl-srw.35,D19-5623,0,0.0129363,"y the large size of the training data typically used in MT,and by the high capacity decoder network used in MT because to fit the high-capacity model well on massive data requires a huge number of training steps. However, since on the one hand, the left-to-right constraint in MT is potentially more relevant for the decoders than the typically bidirectional encoder that has access to the entire input sequence, and on the other hand, BERT and other pre-trained LMs have been successfully used for other complex problems with large training data and high capacity classifiers (Liu and Lapata, 2019; Witteveen and Andrews, 2019; Huang et al., 2021), it is reasonable to assume there may be further reasons for these discrepancies. We take a complementary stance and analyze the differences between the representation spaces produced by BERT and those produced by the MT objective. We therefore attempt to align these spaces, and investigate whether such an explicit alignment would reshape the BERT representation space to enable its use as an NMT encoder. To the best of our knowledge, this is the first study to investigate the intrinsic differences of pretrained LM and MT spaces, as well as the first attempt to explicitly"
2021.americasnlp-1.29,P19-1310,0,0.0459725,"Missing"
2021.americasnlp-1.29,2020.acl-demos.20,1,0.740639,"directed to finding shared task is aimed at developing machine transla- relevant corpora that could help with the translation tion (MT) systems for indigenous languages of the tasks, as well as to make the best out of the data Americas, all of them paired with Spanish (Mager provided by the organizers. In order to have an efet al., 2021). Needless to say, these language pairs ficient procedure to maintain and process the data pose big challenges since none of them benefits sets for all the ten languages, we utilized the Opusfrom large quantities of parallel data and there is Filter toolbox2 (Aulamo et al., 2020). It provides limited monolingual data. For our participation, both ready-made and extensible methods for comwe focused our efforts mainly on three aspects: (1) bining, cleaning, and filtering parallel and monogathering additional parallel and monolingual data lingual corpora. OpusFilter uses a configuration for each language, taking advantage in particular file that lists all the steps for processing the data; of the OPUS corpus collection (Tiedemann, 2012), in order to make quick changes and extensions prothe JHU Bible corpus (McCarthy et al., 2020) and grammatically, we generated the config"
2021.americasnlp-1.29,2020.lrec-1.356,0,0.0833266,"Missing"
2021.americasnlp-1.29,2020.lrec-1.320,0,0.232022,"Missing"
2021.americasnlp-1.29,2020.coling-main.351,0,0.459236,"Missing"
2021.americasnlp-1.29,2020.lrec-1.352,0,0.702383,"parallel data and there is Filter toolbox2 (Aulamo et al., 2020). It provides limited monolingual data. For our participation, both ready-made and extensible methods for comwe focused our efforts mainly on three aspects: (1) bining, cleaning, and filtering parallel and monogathering additional parallel and monolingual data lingual corpora. OpusFilter uses a configuration for each language, taking advantage in particular file that lists all the steps for processing the data; of the OPUS corpus collection (Tiedemann, 2012), in order to make quick changes and extensions prothe JHU Bible corpus (McCarthy et al., 2020) and grammatically, we generated the configuration file translations of political constitutions of various with a Python script. Latin American countries, (2) cleaning and filterFigure 1 shows a part of the applied OpusFiling the corpora to maximize their quality with the ter workflow for a single language pair, Spanish– OpusFilter toolbox (Aulamo et al., 2020), and (3) Raramuri, and restricted to the primary training contrasting different training techniques that could data. The provided training set and (concatenated) take advantage of the scarce data available. 1 https://github.com/Helsinki"
2021.americasnlp-1.29,tiedemann-2012-parallel,1,0.926026,"hem benefits sets for all the ten languages, we utilized the Opusfrom large quantities of parallel data and there is Filter toolbox2 (Aulamo et al., 2020). It provides limited monolingual data. For our participation, both ready-made and extensible methods for comwe focused our efforts mainly on three aspects: (1) bining, cleaning, and filtering parallel and monogathering additional parallel and monolingual data lingual corpora. OpusFilter uses a configuration for each language, taking advantage in particular file that lists all the steps for processing the data; of the OPUS corpus collection (Tiedemann, 2012), in order to make quick changes and extensions prothe JHU Bible corpus (McCarthy et al., 2020) and grammatically, we generated the configuration file translations of political constitutions of various with a Python script. Latin American countries, (2) cleaning and filterFigure 1 shows a part of the applied OpusFiling the corpora to maximize their quality with the ter workflow for a single language pair, Spanish– OpusFilter toolbox (Aulamo et al., 2020), and (3) Raramuri, and restricted to the primary training contrasting different training techniques that could data. The provided training se"
2021.americasnlp-1.29,W19-5441,1,0.88874,"Missing"
2021.americasnlp-1.29,W19-6804,0,0.422544,"Missing"
2021.americasnlp-1.29,2020.loresmt-1.1,0,0.313054,"Missing"
2021.americasnlp-1.29,L16-1144,0,0.334949,"Missing"
2021.americasnlp-1.29,2020.wmt-1.134,1,0.835483,"Missing"
2021.americasnlp-1.29,2020.wmt-1.139,1,0.845344,"Missing"
2021.bsnlp-1.8,2020.lrec-1.169,0,0.0482327,"Missing"
2021.bsnlp-1.8,D19-3009,0,0.0210535,", we used an architecture with 2 LSTM layers with hidden states of 500 and 500 hidden units. The dropout probability was set to 0.3. SGD was used as an optimizer, and global attention and input feeding were employed. We also employed the default learning rate of 1.0 with the decay of 0.7. The vocabulary size was set to 50000, which also happen to suit our needs, since, as can be seen from Table 3, the vocabularies of the original paragraphs only slightly exceed this number, and the adapted vocabularies are even smaller. For evaluation, we used the BLEU and SARI metrics from the EASSE library (Alva-Manchego et al., 2019) and the aforementioned FleschKincaid Grade Level score with constants optimized for Russian. We evaluated our models on both test sets that consisted of data from the same books and novels but which the models have not seen during training, and the small hand-crafted dataset used for alignment evaluation, which consists predominantly of excerpts from children’s literature. The test and development set sizes for the dataset aligned with Bleualign were both 1000 paragraphs, and for the data aligned with CATS 1500 paragraphs. The best results for each system on larger test sets can be seen in Ta"
2021.bsnlp-1.8,2020.ngt-1.6,0,0.0364363,"Dataset from Language Learner Data J¨org Tiedemann Anna Dmitrieva Department of Digital Humanities Department of Digital Humanities University of Helsinki University of Helsinki Helsinki, Finland Helsinki, Finland; jorg.tiedemann@helsinki.fi HSE University Moscow, Russia annadmitrieva252@gmail.com Abstract such example is the dataset for one of the Dialogue Evaluation tasks (RuSimpleSentEval)1 . However, this dataset consists mainly of texts from the WikiLarge corpus (Zhang and Lapata, 2017) that were translated into Russian automatically and parallel data from the Russian paraphrase corpus (Gudkov et al., 2020). Only the texts from the development and test set have been simplified by human experts. We will, instead, produce a corpus of Russian texts adapted by human experts for language learners, a unique resource for further work on Russian text simplification. Creating a new dataset for text simplification poses quite a few challenges. First of all, there needs to be a source of such data. Possible options include simplified Wikipedia articles like Simple English Wikipedia or Vikidia2 , a French website intended for young people with more accessible articles than in Wikipedia, both in terms of lan"
2021.bsnlp-1.8,P13-1151,0,0.0225193,"al. 1 Introduction Automatic text simplification is a task of natural language processing aimed at making texts more readable and accessible to a broader audience. Nowadays, this task is most often viewed as a neural machine translation problem (Xu et al., 2015). Much like cross-lingual machine translation, intralingual neural text simplification requires a substantial amount of data in order to be able to train and test appropriate models. Currently there are multiple datasets to choose from for English text simplification, such as Simple PPDB (Pavlick and Callison-Burch, 2016), Simple Wiki (Kauchak, 2013) and Newsela (Xu et al., 2015). However, not a lot of simplification-specific datasets are available for other languages. A few exceptions are, for example, the Spanish version of the Newsela dataset and the Alector parallel Simplified French corpus (Gala et al., 2020). For Russian, there are currently very limited options for publicly available simplification corpora. One 1 https://github.com/ dialogue-evaluation/RuSimpleSentEval 2 http://fr.vikidia.org 73 Proceedings of the 8th BSNLP Workshop on Balto-Slavic Natural Language Processing, pages 73–79 April 20, 2021. ©2021 Association for Compu"
2021.bsnlp-1.8,P17-4012,0,0.0122796,"ent alignment tools (see below). 6 BLEU 21.68 14.69 SARI 42.97 40.94 FKGL 2.82 2.82 Table 5: Simplification evaluation – larger automatically aligned test sets. Neural text simplification In order to evaluate the impact of alignment, we built neural text simplification models from the collected training data. We chose the architecture based on (Nisioi et al., 2017), which has proven to perform well on English-Simple English data. This architecture is openly available online10 and has some modifications that have further improved its performance (Cooper and Shardlow, 2020). We used OpenNMT-py (Klein et al., 2017) to build our models. Similar to the original paper, we used an architecture with 2 LSTM layers with hidden states of 500 and 500 hidden units. The dropout probability was set to 0.3. SGD was used as an optimizer, and global attention and input feeding were employed. We also employed the default learning rate of 1.0 with the decay of 0.7. The vocabulary size was set to 50000, which also happen to suit our needs, since, as can be seen from Table 3, the vocabularies of the original paragraphs only slightly exceed this number, and the adapted vocabularies are even smaller. For evaluation, we used"
2021.bsnlp-1.8,P17-2014,0,0.017383,"alignments, multiple original 8 https://github.com/neosyon/ SimpTextAlign 9 https://github.com/rsennrich/ Bleualign 76 Aligner Bleualign CATS downstream applications, we then decided to compare the performance on simplification models when using different alignment tools (see below). 6 BLEU 21.68 14.69 SARI 42.97 40.94 FKGL 2.82 2.82 Table 5: Simplification evaluation – larger automatically aligned test sets. Neural text simplification In order to evaluate the impact of alignment, we built neural text simplification models from the collected training data. We chose the architecture based on (Nisioi et al., 2017), which has proven to perform well on English-Simple English data. This architecture is openly available online10 and has some modifications that have further improved its performance (Cooper and Shardlow, 2020). We used OpenNMT-py (Klein et al., 2017) to build our models. Similar to the original paper, we used an architecture with 2 LSTM layers with hidden states of 500 and 500 hidden units. The dropout probability was set to 0.3. SGD was used as an optimizer, and global attention and input feeding were employed. We also employed the default learning rate of 1.0 with the decay of 0.7. The voc"
2021.bsnlp-1.8,W14-1206,0,0.0632251,"Missing"
2021.bsnlp-1.8,P16-2024,0,0.0159909,"ne of the first Simple Russian datasets in general. 1 Introduction Automatic text simplification is a task of natural language processing aimed at making texts more readable and accessible to a broader audience. Nowadays, this task is most often viewed as a neural machine translation problem (Xu et al., 2015). Much like cross-lingual machine translation, intralingual neural text simplification requires a substantial amount of data in order to be able to train and test appropriate models. Currently there are multiple datasets to choose from for English text simplification, such as Simple PPDB (Pavlick and Callison-Burch, 2016), Simple Wiki (Kauchak, 2013) and Newsela (Xu et al., 2015). However, not a lot of simplification-specific datasets are available for other languages. A few exceptions are, for example, the Spanish version of the Newsela dataset and the Alector parallel Simplified French corpus (Gala et al., 2020). For Russian, there are currently very limited options for publicly available simplification corpora. One 1 https://github.com/ dialogue-evaluation/RuSimpleSentEval 2 http://fr.vikidia.org 73 Proceedings of the 8th BSNLP Workshop on Balto-Slavic Natural Language Processing, pages 73–79 April 20, 2021"
2021.bsnlp-1.8,2020.lrec-1.686,0,0.0252412,"mance on simplification models when using different alignment tools (see below). 6 BLEU 21.68 14.69 SARI 42.97 40.94 FKGL 2.82 2.82 Table 5: Simplification evaluation – larger automatically aligned test sets. Neural text simplification In order to evaluate the impact of alignment, we built neural text simplification models from the collected training data. We chose the architecture based on (Nisioi et al., 2017), which has proven to perform well on English-Simple English data. This architecture is openly available online10 and has some modifications that have further improved its performance (Cooper and Shardlow, 2020). We used OpenNMT-py (Klein et al., 2017) to build our models. Similar to the original paper, we used an architecture with 2 LSTM layers with hidden states of 500 and 500 hidden units. The dropout probability was set to 0.3. SGD was used as an optimizer, and global attention and input feeding were employed. We also employed the default learning rate of 1.0 with the decay of 0.7. The vocabulary size was set to 50000, which also happen to suit our needs, since, as can be seen from Table 3, the vocabularies of the original paragraphs only slightly exceed this number, and the adapted vocabularies"
2021.bsnlp-1.8,2010.amta-papers.14,0,0.239307,". This is not a surprise as Hunalign (as most of the other algorithms) is developed for cross-lingual sentence alignment and not for linking texts in the same language but different levels of complexity. Nevertheless, the InterText interface provides a convenient tool that enabled a semi-automatic alignment of a smaller dataset that we can use for the evaluation of fully automatic alignment algorithms later on. Using this method we created a gold standard of 302 aligned segments taken from textbook exercises. We tested two methods to align the entire collection of books and novels, Bleualign (Sennrich and Volk, 2010) and CATS-Align: a tool for customised 7 https://github.com/infoculture/ plainrussian 75 Data Bleualign orig Bleualign adapt CATS orig CATS adapt Words 345779 254000 589715 268396 Vocab 52010 31831 55380 32746 Sents 32593 30553 50673 32382 Aligner Bleualign CATS Strict F1 0.90 0.98 Lax F1 0.90 0.98 Table 4: Alignment evaluation. Table 3: Alignment statistics. Vocab = vocabulary, sents = sentences, orig = original versions, adapt = adapted versions. paragraphs end up being aligned with more than one different adapted paragraph, and vice versa. Therefore, we only have 7671 unique original paragr"
2021.bsnlp-1.8,P17-2016,0,0.111864,"nments happen because the aligner takes some additional sentences from the next paragraphs. This seems to only happen with adapted paragraphs. One possible reason is the size mismatch between original and adapted texts. However, when aligning longer books and novels, this might be appropriate, since sometimes longer paragraphs from the original texts are split into multiple paragraphs during adaptation, so it can be good to grab additional adapted paragraphs during alignment to match a longer original one. To study the impact of different aligners on alignment of text simplification corpora8 (Stajner et al., 2017). Bleualign9 is designed for sentence-tosentence alignment of parallel texts used for training machine translation models. The alignment is performed based on the modified BLEU score between source sentences translated into the target language and the original target language sentences. The principle of matching sentences in the same language suits our needs very well and in the case of simplified language alignment we can, therefore, skip the translation step. CATS-Align is a tool developed specifically for aligning simplification datasets, particularly for Newsela. It offers multiple options"
2021.bsnlp-1.8,vondricka-2014-aligning,0,0.0762388,"Missing"
2021.bsnlp-1.8,Q15-1021,0,0.0198182,"sh and Simple English, but many other languages lack such data. In this paper we describe our work on creating an aligned Russian-Simple Russian dataset composed of Russian literature texts adapted for learners of Russian as a foreign language. This will be the first parallel dataset in this domain, and one of the first Simple Russian datasets in general. 1 Introduction Automatic text simplification is a task of natural language processing aimed at making texts more readable and accessible to a broader audience. Nowadays, this task is most often viewed as a neural machine translation problem (Xu et al., 2015). Much like cross-lingual machine translation, intralingual neural text simplification requires a substantial amount of data in order to be able to train and test appropriate models. Currently there are multiple datasets to choose from for English text simplification, such as Simple PPDB (Pavlick and Callison-Burch, 2016), Simple Wiki (Kauchak, 2013) and Newsela (Xu et al., 2015). However, not a lot of simplification-specific datasets are available for other languages. A few exceptions are, for example, the Spanish version of the Newsela dataset and the Alector parallel Simplified French corpu"
2021.bsnlp-1.8,D17-1062,0,0.0394793,"Missing"
2021.emnlp-main.664,Q17-1026,0,0.0239234,"tilingual MT benchmarks of different sizes, showing that simply supervising one cross attention head to focus both on word alignments and language labels reduces the bias towards translating into the wrong language, improving the zero-shot performance overall. Moreover, as an additional advantage, we find that our alignment supervision leads to more stable results across different training runs. 1 Introduction 2017). Many studies have investigated this feature, focusing on the impact of both, the model architecture design (Arivazhagan et al., 2019a; Pham et al., 2019) and data pre-processing (Lee et al., 2017; Wang et al., 2019; Rios et al., 2020; Wu et al., 2021). Broadly speaking, MNMT architectures are categorized according to their degree of parameter sharing, from fully shared (Johnson et al., 2017) to the use of language-specific components (Vázquez et al., 2020; Escolano et al., 2021; Zhang et al., 2021). The Johnson et al. (2017) MNMT model is widely used, due to its simplicity and good translation quality. It uses the fully shared parameters setting, and relies on appending an artificial language label to each input sentence to indicate the target language. While this method allows for ze"
2021.emnlp-main.664,2021.acl-long.25,0,0.0357294,"ention head to explicitly focus on the target language label. We show that alignment supervision mitigates the off-target translation issue in the zero-shot case. Our method improves the zero-shot translation performance and results in more stable results across different training runs. Multilingual Neural Machine Translation (MNMT) focuses on translation between multiple language pairs through a single optimized neural model, and has been explored from different angles witnessing a rapid progress in recent years (Arivazhagan et al., 2019b; Wang et al., 2020; Dabre et al., 2 Methodology 2020; Lin et al., 2021). Besides the great flexibility MNMT models offer, they are also highlighted Alignment Methods. Given a bitext Bsrc = by their so called zero-shot translation capabili- (s1 , ..., sj , ..., sN ) and Btrg = (t1 , ..., ti , ..., tM ) ties, i.e., translating between all combinations of where Bsrc is a sentence in the source language languages available in the training data, including and Btrg is its translation in the target language, an those with no parallel data seen at training time alignment A is a mapping of words between Bsrc (Ha et al., 2016; Firat et al., 2016; Johnson et al., and Btrg ("
2021.emnlp-main.664,W19-4827,0,0.0300435,"Missing"
2021.emnlp-main.664,J03-1002,0,0.0297246,"rat et al., 2016; Johnson et al., and Btrg (Tiedemann, 2011), formally defined as 8449 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8449–8456 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 1: English → German example sentence with different alignment methods. Alignments in (a) show word alignments between corresponding words in the two languages, (b) our introduced alignments between all target words and the input language label, and (c) the union of the two. a subset of the Cartesian product of the word positions (Och and Ney, 2003): A ⊆ {(j, i) : j = 1, ..., N ; i = 1, ..., M } (1) We study three different settings: (a) standard word alignment between corresponding words, (b) alignments between all target words and the language label in the input string, and (c) the union between the former two. Figure 1 shows an example of those approaches. To produce word alignments between parallel sentences, i.e., Figure 1 (a), we use the awesome-align tool (Dou and Neubig, 2021), a recent work that leverages multilingual BERT (Devlin et al., 2019) to extract the links.1 layer.2 Given the sparse nature of the alignments, we replace"
2021.emnlp-main.664,P02-1040,0,0.109805,"over 4 zero-shot language directions. We report average BLEU and accuracy scores, plus the standard deviation over 3 training runs with different random seeds. language pair. It provides supervised translation test data for 188 language pairs, and zero-shot evaluation data for 30 pairs. Following related work (Aharoni et al., 2019; Zhang et al., 2020), we apply joint Byte-Pair Encoding (BPE) segmentation (Sennrich et al., 2016; Kudo and Richardson, 2018), with a shared vocabulary size of 32K symbols for TED Talks and 64K for WMT-2018 and OPUS-100. As evaluation measure, we use tokenized BLEU (Papineni et al., 2002) to be comparable with Aharoni et al. (2019) for the TED Talks benchmark, and SACRE BLEU 5 (Post, 2018) for WMT-2018 and OPUS-100.6 As an additional evaluation, we report the target language identification accuracy score for the zeroshot cases (Zhang et al., 2020), called ACCzero . We use fasttext as a language identification tool (Joulin et al., 2017), counting how many times the translation language matches the reference target language. The Transformer models follow the base setting of Vaswani et al. (2017), with three different random seeds in each run. All of them are trained on the Many-"
2021.emnlp-main.664,P19-1146,0,0.0174727,"settings: (a) standard word alignment between corresponding words, (b) alignments between all target words and the language label in the input string, and (c) the union between the former two. Figure 1 shows an example of those approaches. To produce word alignments between parallel sentences, i.e., Figure 1 (a), we use the awesome-align tool (Dou and Neubig, 2021), a recent work that leverages multilingual BERT (Devlin et al., 2019) to extract the links.1 layer.2 Given the sparse nature of the alignments, we replace the softmax operator in the cross attention head with the α-entmax function (Peters et al., 2019; Correia et al., 2019). Entmax allows sparse attention weights for any α &gt; 1. Following Peters et al. (2019), we use α=1.5. 3 We use three highly multilingual MT benchmarks: • TED Talks (Qi et al., 2018). An Englishcentric parallel corpus with 10M training sentences across 116 translation directions. Following Aharoni et al. (2019), we evaluate on a total of 16 language directions, while as zeroshot test we evaluate on 4 language pairs. Models. To train Many-to-Many MNMT models, we use a 6-layer Transformer architecture (Vaswani et al., 2017), prepending a language label in the input to indic"
2021.emnlp-main.664,W19-5202,0,0.0410818,"Missing"
2021.emnlp-main.664,W18-6319,0,0.0117981,"3 training runs with different random seeds. language pair. It provides supervised translation test data for 188 language pairs, and zero-shot evaluation data for 30 pairs. Following related work (Aharoni et al., 2019; Zhang et al., 2020), we apply joint Byte-Pair Encoding (BPE) segmentation (Sennrich et al., 2016; Kudo and Richardson, 2018), with a shared vocabulary size of 32K symbols for TED Talks and 64K for WMT-2018 and OPUS-100. As evaluation measure, we use tokenized BLEU (Papineni et al., 2002) to be comparable with Aharoni et al. (2019) for the TED Talks benchmark, and SACRE BLEU 5 (Post, 2018) for WMT-2018 and OPUS-100.6 As an additional evaluation, we report the target language identification accuracy score for the zeroshot cases (Zhang et al., 2020), called ACCzero . We use fasttext as a language identification tool (Joulin et al., 2017), counting how many times the translation language matches the reference target language. The Transformer models follow the base setting of Vaswani et al. (2017), with three different random seeds in each run. All of them are trained on the Many-to-Many English-centric scenario, i.e., on the concatenation of the training data having English either"
2021.emnlp-main.664,N18-2084,0,0.0193691,"an example of those approaches. To produce word alignments between parallel sentences, i.e., Figure 1 (a), we use the awesome-align tool (Dou and Neubig, 2021), a recent work that leverages multilingual BERT (Devlin et al., 2019) to extract the links.1 layer.2 Given the sparse nature of the alignments, we replace the softmax operator in the cross attention head with the α-entmax function (Peters et al., 2019; Correia et al., 2019). Entmax allows sparse attention weights for any α &gt; 1. Following Peters et al. (2019), we use α=1.5. 3 We use three highly multilingual MT benchmarks: • TED Talks (Qi et al., 2018). An Englishcentric parallel corpus with 10M training sentences across 116 translation directions. Following Aharoni et al. (2019), we evaluate on a total of 16 language directions, while as zeroshot test we evaluate on 4 language pairs. Models. To train Many-to-Many MNMT models, we use a 6-layer Transformer architecture (Vaswani et al., 2017), prepending a language label in the input to indicate the target language (Johnson et al., 2017). Following Garg et al. (2019), given an alignment matrix AMM,N and an attention matrix computed by a cross attention head AHM,N , for each target word i, we"
2021.emnlp-main.664,2020.findings-emnlp.49,1,0.84444,"slation issue in the zero-shot scenario. We believe that our work will pave the way for designing new and better multilingual MT models to improve their generalization in zero-shot setups. As future work, we intend to analyze the quality of the learned alignments and their effect on the other attention weights in both supervised and zeroshot evaluation data (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019). Finally, we plan to explore other mechanisms to inject prior knowledge to better handle zero-shot translations (Deshpande and Narasimhan, 2020; Raganato et al., 2020; Song et al., 2020). Acknowledgments This work is part of the FoTran project, funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 771113). The authors gratefully acknowledge the support of the CSC – IT Center for Science, Finland, for computational resources. Finally, We would also like to acknowledge NVIDIA and their GPU grant. References Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapt"
2021.emnlp-main.664,W18-5431,1,0.850544,"variants shows that adding alignment supervision between corresponding words and the language label consistently improves the stability of the models, resulting in stable performance across different runs and mitigating the off-target translation issue in the zero-shot scenario. We believe that our work will pave the way for designing new and better multilingual MT models to improve their generalization in zero-shot setups. As future work, we intend to analyze the quality of the learned alignments and their effect on the other attention weights in both supervised and zeroshot evaluation data (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019). Finally, we plan to explore other mechanisms to inject prior knowledge to better handle zero-shot translations (Deshpande and Narasimhan, 2020; Raganato et al., 2020; Song et al., 2020). Acknowledgments This work is part of the FoTran project, funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 771113). The authors gratefully acknowledge the support of the CSC – IT Center for Science, Finland, for computational resources. Finally, We would a"
2021.emnlp-main.664,2020.wmt-1.64,0,0.141504,"sizes, showing that simply supervising one cross attention head to focus both on word alignments and language labels reduces the bias towards translating into the wrong language, improving the zero-shot performance overall. Moreover, as an additional advantage, we find that our alignment supervision leads to more stable results across different training runs. 1 Introduction 2017). Many studies have investigated this feature, focusing on the impact of both, the model architecture design (Arivazhagan et al., 2019a; Pham et al., 2019) and data pre-processing (Lee et al., 2017; Wang et al., 2019; Rios et al., 2020; Wu et al., 2021). Broadly speaking, MNMT architectures are categorized according to their degree of parameter sharing, from fully shared (Johnson et al., 2017) to the use of language-specific components (Vázquez et al., 2020; Escolano et al., 2021; Zhang et al., 2021). The Johnson et al. (2017) MNMT model is widely used, due to its simplicity and good translation quality. It uses the fully shared parameters setting, and relies on appending an artificial language label to each input sentence to indicate the target language. While this method allows for zeroshot translation, several works have"
2021.emnlp-main.664,P16-1162,0,0.0341881,"nglish languages and for Non-English languages to English on 16 language pairs respectively. “BLEUzero (4)” and “ACCzero (4)”: average BLEU scores and target language identification accuracy over 4 zero-shot language directions. We report average BLEU and accuracy scores, plus the standard deviation over 3 training runs with different random seeds. language pair. It provides supervised translation test data for 188 language pairs, and zero-shot evaluation data for 30 pairs. Following related work (Aharoni et al., 2019; Zhang et al., 2020), we apply joint Byte-Pair Encoding (BPE) segmentation (Sennrich et al., 2016; Kudo and Richardson, 2018), with a shared vocabulary size of 32K symbols for TED Talks and 64K for WMT-2018 and OPUS-100. As evaluation measure, we use tokenized BLEU (Papineni et al., 2002) to be comparable with Aharoni et al. (2019) for the TED Talks benchmark, and SACRE BLEU 5 (Post, 2018) for WMT-2018 and OPUS-100.6 As an additional evaluation, we report the target language identification accuracy score for the zeroshot cases (Zhang et al., 2020), called ACCzero . We use fasttext as a language identification tool (Joulin et al., 2017), counting how many times the translation language mat"
2021.emnlp-main.664,W18-6304,0,0.0214064,"gnment supervision between corresponding words and the language label consistently improves the stability of the models, resulting in stable performance across different runs and mitigating the off-target translation issue in the zero-shot scenario. We believe that our work will pave the way for designing new and better multilingual MT models to improve their generalization in zero-shot setups. As future work, we intend to analyze the quality of the learned alignments and their effect on the other attention weights in both supervised and zeroshot evaluation data (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019). Finally, we plan to explore other mechanisms to inject prior knowledge to better handle zero-shot translations (Deshpande and Narasimhan, 2020; Raganato et al., 2020; Song et al., 2020). Acknowledgments This work is part of the FoTran project, funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 771113). The authors gratefully acknowledge the support of the CSC – IT Center for Science, Finland, for computational resources. Finally, We would also like to acknowl"
2021.emnlp-main.664,tiedemann-2012-parallel,1,0.696855,"provided by the WMT-2018 shared task on news translation. We use all available language pairs, i.e. 14, up to 5M training sentences for each language pair. We evaluate the models on the test sets of the shared task, i.e. newstest2018. As there are no zero-shot test sets provided by the competition, we use the test portion from the Tatoeba-challenge (Tiedemann, 2020),4 in all possible language pair combinations included in the challenge. M N 1 XX AMi,j log(AHi,j ) M i=1 j=1 • OPUS-100 (Zhang et al., 2020). An Englishcentric multi-domain benchmark, built upon the OPUS parallel text collection (Tiedemann, 2012). It covers a total of 198 language directions, with up to 1M training sentence per (2) The overall loss L is: L = Lt + γLa (AH, AM ) (3) where Lt is the standard NLL translation loss, and γ is a hyperparameter. We use γ = 0.05, supervising only one cross attention head at the third last 1 We use the bert-base-multilingual-cased checkpoint, without fine-tuning, and with softmax as a extraction function. 2 As we use the OpenNMT-py (Klein et al., 2017) toolkit, it is recommended to supervise the third last layer. See https://github.com/OpenNMT/OpenNMT-py/ issues/1843. 3 http://data.statmt.org/wm"
2021.emnlp-main.664,2020.wmt-1.139,1,0.880692,"ross attention head AHM,N , for each target word i, we use the following cross-entropy loss La to minimize the Kullback-Leibler divergence between AH and AM : La (AH, AM ) = − Experimental Setup • WMT-2018 (Bojar et al., 2018).3 A parallel dataset provided by the WMT-2018 shared task on news translation. We use all available language pairs, i.e. 14, up to 5M training sentences for each language pair. We evaluate the models on the test sets of the shared task, i.e. newstest2018. As there are no zero-shot test sets provided by the competition, we use the test portion from the Tatoeba-challenge (Tiedemann, 2020),4 in all possible language pair combinations included in the challenge. M N 1 XX AMi,j log(AHi,j ) M i=1 j=1 • OPUS-100 (Zhang et al., 2020). An Englishcentric multi-domain benchmark, built upon the OPUS parallel text collection (Tiedemann, 2012). It covers a total of 198 language directions, with up to 1M training sentence per (2) The overall loss L is: L = Lt + γLa (AH, AM ) (3) where Lt is the standard NLL translation loss, and γ is a hyperparameter. We use γ = 0.05, supervising only one cross attention head at the third last 1 We use the bert-base-multilingual-cased checkpoint, without fi"
2021.emnlp-main.664,2020.cl-2.5,1,0.841489,"Moreover, as an additional advantage, we find that our alignment supervision leads to more stable results across different training runs. 1 Introduction 2017). Many studies have investigated this feature, focusing on the impact of both, the model architecture design (Arivazhagan et al., 2019a; Pham et al., 2019) and data pre-processing (Lee et al., 2017; Wang et al., 2019; Rios et al., 2020; Wu et al., 2021). Broadly speaking, MNMT architectures are categorized according to their degree of parameter sharing, from fully shared (Johnson et al., 2017) to the use of language-specific components (Vázquez et al., 2020; Escolano et al., 2021; Zhang et al., 2021). The Johnson et al. (2017) MNMT model is widely used, due to its simplicity and good translation quality. It uses the fully shared parameters setting, and relies on appending an artificial language label to each input sentence to indicate the target language. While this method allows for zeroshot translation, several works have highlighted two major flaws: i) its failure to reliably generalize to unseen language pairs, ending up with the so called off-target issue, where the language label is ignored and the wrong target language is produced as a re"
2021.emnlp-main.664,P19-1580,0,0.0181963,"ds and the language label consistently improves the stability of the models, resulting in stable performance across different runs and mitigating the off-target translation issue in the zero-shot scenario. We believe that our work will pave the way for designing new and better multilingual MT models to improve their generalization in zero-shot setups. As future work, we intend to analyze the quality of the learned alignments and their effect on the other attention weights in both supervised and zeroshot evaluation data (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019). Finally, we plan to explore other mechanisms to inject prior knowledge to better handle zero-shot translations (Deshpande and Narasimhan, 2020; Raganato et al., 2020; Song et al., 2020). Acknowledgments This work is part of the FoTran project, funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 771113). The authors gratefully acknowledge the support of the CSC – IT Center for Science, Finland, for computational resources. Finally, We would also like to acknowledge NVIDIA and their GPU grant. References Ro"
2021.emnlp-main.664,2020.emnlp-main.75,0,0.0287985,"al. (2017) setting, by jointly training one cross attention head to explicitly focus on the target language label. We show that alignment supervision mitigates the off-target translation issue in the zero-shot case. Our method improves the zero-shot translation performance and results in more stable results across different training runs. Multilingual Neural Machine Translation (MNMT) focuses on translation between multiple language pairs through a single optimized neural model, and has been explored from different angles witnessing a rapid progress in recent years (Arivazhagan et al., 2019b; Wang et al., 2020; Dabre et al., 2 Methodology 2020; Lin et al., 2021). Besides the great flexibility MNMT models offer, they are also highlighted Alignment Methods. Given a bitext Bsrc = by their so called zero-shot translation capabili- (s1 , ..., sj , ..., sN ) and Btrg = (t1 , ..., ti , ..., tM ) ties, i.e., translating between all combinations of where Bsrc is a sentence in the source language languages available in the training data, including and Btrg is its translation in the target language, an those with no parallel data seen at training time alignment A is a mapping of words between Bsrc (Ha et al.,"
2021.emnlp-main.664,2021.findings-acl.264,0,0.0229218,"simply supervising one cross attention head to focus both on word alignments and language labels reduces the bias towards translating into the wrong language, improving the zero-shot performance overall. Moreover, as an additional advantage, we find that our alignment supervision leads to more stable results across different training runs. 1 Introduction 2017). Many studies have investigated this feature, focusing on the impact of both, the model architecture design (Arivazhagan et al., 2019a; Pham et al., 2019) and data pre-processing (Lee et al., 2017; Wang et al., 2019; Rios et al., 2020; Wu et al., 2021). Broadly speaking, MNMT architectures are categorized according to their degree of parameter sharing, from fully shared (Johnson et al., 2017) to the use of language-specific components (Vázquez et al., 2020; Escolano et al., 2021; Zhang et al., 2021). The Johnson et al. (2017) MNMT model is widely used, due to its simplicity and good translation quality. It uses the fully shared parameters setting, and relies on appending an artificial language label to each input sentence to indicate the target language. While this method allows for zeroshot translation, several works have highlighted two m"
2021.emnlp-main.664,D19-1083,0,0.0372104,"Missing"
2021.emnlp-main.664,2020.acl-main.148,0,0.301177,"lano et al., 2021; Zhang et al., 2021). The Johnson et al. (2017) MNMT model is widely used, due to its simplicity and good translation quality. It uses the fully shared parameters setting, and relies on appending an artificial language label to each input sentence to indicate the target language. While this method allows for zeroshot translation, several works have highlighted two major flaws: i) its failure to reliably generalize to unseen language pairs, ending up with the so called off-target issue, where the language label is ignored and the wrong target language is produced as a result (Zhang et al., 2020), ii) its lack of stability in translation results between different training runs (Rios et al., 2020). In this work, we investigate the role of guided alignment in the Johnson et al. (2017) setting, by jointly training one cross attention head to explicitly focus on the target language label. We show that alignment supervision mitigates the off-target translation issue in the zero-shot case. Our method improves the zero-shot translation performance and results in more stable results across different training runs. Multilingual Neural Machine Translation (MNMT) focuses on translation between m"
2021.konvens-1.25,L18-1622,0,0.0631718,"Missing"
2021.konvens-1.25,2020.vardial-1.3,1,0.766582,"esent a dataset for Low Saxon,1 a Germanic minority language spoken by roughly five million people in Northern Central Europe (Moseley, 2010). Despite its relatively large number of speakers, there are hardly any annotated corpora for this language, hampering corpus-based research into more modern varieties and causing a lack of wellfunctioning NLP tools. The dataset is part of our research into the diachronic development of the internal variation in Low Saxon and builds upon the Reference Corpus Middle Low German/Low Rhenish (1200-1650) (ReN-Team, 2019) (henceforth ReN) and the LSDC dataset (Siewert et al., 2020) attempting to fill the gap between them. Therefore, it covers both historical and contemporary Low Saxon dialects from the Veluwe in the western corner of the language area to the Lower-Prussian dialects in the east. Our ultimate goal with this new dataset is to perform analyses of the internal variation within Low Saxon and its change over time. Questions 1 Also called Low German, referring here to the varieties protected under the European Charter for Regional and Minority Languages as Nedersaksisch in the Netherlands and Niederdeutsch in Germany as well as extinct eastern varieties. Figure"
2021.konvens-1.25,2021.findings-acl.433,0,0.0949783,"Missing"
2021.nodalida-main.28,P18-2103,0,0.0283125,"Missing"
2021.nodalida-main.28,N18-2017,0,0.037114,"Missing"
2021.nodalida-main.28,D19-1275,0,0.0269615,"ccuracy for models fine-tuned on the ANLI hypothesis-only dataset. Although there still seems to be space for improvement (accuracy is around 0.5, i.e. well above chance), the reported findings are promising. Specifically, the performance is lower than on the hypothesis-only SNLI/MNLI datasets, showing that the dataset contains less artefacts that can guide prediction. ANLI is thus a natural candidate to further test our hypotheses, as it claims to remedy for a number of the shortcomings of earlier NLI datasets. Lessons learnt from previous work on designing reliable linguistic probing tasks (Hewitt and Liang, 2019) and the overfitting problems of NLI models discussed above, demonstrate the importance of systematic sanity checks like the ones we propose in this paper. Our dedicated control tasks specifically allow to determine whether a dataset triggers the models’ reasoning capabilities or, instead, allows them to rely on statistical biases and annotation artefacts for prediction. We use the quality of the predictions made by models finetuned and tested on corrupted data as a proxy to evaluate data quality. 3 3.1 Datasets The Multi-Genre NLI (MNLI) Corpus We carry out our experiments on the MultiGenre N"
2021.nodalida-main.28,S14-2055,0,0.043277,"Missing"
2021.nodalida-main.28,2021.ccl-1.108,0,0.0865513,"Missing"
2021.nodalida-main.28,marelli-etal-2014-sick,0,0.0927551,"Missing"
2021.nodalida-main.28,S18-2023,0,0.0449508,"Missing"
2021.nodalida-main.28,W19-4810,1,0.877356,"Missing"
2021.nodalida-main.28,N18-1101,0,0.13604,"prediction (Pham et al., 2020). To the contrary, small tweaks or perturbations in the data, such as replacing words with mutually exclusive cohyponyms and antonyms (Glockner et al., 2018) or changing the order of the two sentences (Wang et al., 2019b), has been shown to hurt the performance of NLI models. Motivated by this situation, our goal is to contribute a new suite of diagnostic tests that can be used to assess the quality of an NLU benchmark. In particular, we conduct a series of controlled experiments where a set of data corruption transformations are applied to the widely used MNLI (Williams et al., 2018) and ANLI (Nie et al., 2020) datasets, and explore their impact on fine-tuned BERT and ROBERTa (Liu et al., 2019) model performance. The obtained results provide evidence that can reveal the quality of a dataset: Given that the transformations seriously affect the quality of NLI sentences, going as far as making them unintelligible (cf. examples in Table 1), a decrease in performance for models fine-tuned on the corrupted dataset would be expected. High performance would, instead, indicate the presence of biases and other artefacts in the dataset which guide models’ predictions. This situation"
2021.nodalida-main.28,2020.acl-main.773,0,0.0313274,"Missing"
2021.nodalida-main.37,2020.acl-demos.20,1,0.842361,"Missing"
2021.nodalida-main.37,W18-6315,0,0.0187219,"tion from Northern S´ami to Finnish (Pirinen et al., 2017) within the Apertium framework (Forcada et al., 2011). We also combine both methods to further augment the data. Our experiments demonstrate the positive effects of both strategies and the possibility of obtaining complementary information from different backtranslation engines. 2 Related work Using backtranslations from different sources as training data has been shown to be beneficial for improving machine translation quality. In addition to proposing training data augmentation methods that do not require reverse translation systems, Burlot and Yvon (2018) compare the effects of using statistical machine translation (SMT) and NMT based backtranslations for English→French and English→German translations. They show that both types of backtranslations improve translation quality, NMT slightly more than SMT. Poncelas et al. (2019) also produce backtranslations with SMT and NMT. They show that the translation quality of a German→English NMT system is improved when including either type of backtranslations in the training data. The greatest improvement is observed when both types of backtranslations are used. Augmenting training data with RBMT backtr"
2021.nodalida-main.37,D14-1179,0,0.0250003,"Missing"
2021.nodalida-main.37,W19-6908,0,0.0266878,"T based backtranslations for English→French and English→German translations. They show that both types of backtranslations improve translation quality, NMT slightly more than SMT. Poncelas et al. (2019) also produce backtranslations with SMT and NMT. They show that the translation quality of a German→English NMT system is improved when including either type of backtranslations in the training data. The greatest improvement is observed when both types of backtranslations are used. Augmenting training data with RBMT backtranslations has also proven to be useful for boosting translation quality. Dowling et al. (2019) use RBMT backtranslations to improve statistical machine translation performance for Scottish Gaelic→English translations. The authors show that backtranslations can be beneficial even in cases where the translation quality of the MT system used to produce the backtranslations is low. Soto et al. (2019) study the performance of NMT systems trained with augmented training data backtranslated using RBMT, SMT and NMT. They experiment with Basque→Spanish translations and show that the translation performance improves when using each type of augmented training data individually. Soto et al. (2020)"
2021.nodalida-main.37,2020.findings-emnlp.352,0,0.0350446,"as the backtranslation model. For Transformers, we use the example hyperparameters from MarianNMT 4 which replicate the setup 4 https://github.com/marian-nmt/ marian-examples/tree/master/transformer NMT RBMT UiT 19.4 12.3 YLE 4.5 10.0 Table 1: Reverse translation model (sme-fin) quality in BLEU points evaluated with the UiT test set and the YLE test set. from Vaswani et al. (2017). For subword segmentation, we use the SentencePiece tokenizer (Kudo and Richardson, 2018) with vocabulary size 8000, which has been shown to produce the best results with the data set sizes that we are dealing with (Gowda and May, 2020; Gr¨onroos et al., 2021). We train the models until the cross-entropy of the validation set does not improve for 10 consecutive validation steps. For the RBMT backtranslations, we use Apertium with the sme-fin model by Pirinen et al. (2017). This system implements a shallow transfer-based translation engine consisting of modules for morphological analysis, disambiguation and generation, modules for lexical translation based on context rules, and a module for syntactic transformation operations. Table 1 shows the quality of the sme-fin translation models used for backtranslations in BLEU point"
2021.nodalida-main.37,P18-4020,0,0.029041,"Missing"
2021.nodalida-main.37,D18-2012,0,0.0240889,"rection. All models using additional backtranslated training sets are trained with both RNNs and Transformers. All RNN models have the same architecture as the backtranslation model. For Transformers, we use the example hyperparameters from MarianNMT 4 which replicate the setup 4 https://github.com/marian-nmt/ marian-examples/tree/master/transformer NMT RBMT UiT 19.4 12.3 YLE 4.5 10.0 Table 1: Reverse translation model (sme-fin) quality in BLEU points evaluated with the UiT test set and the YLE test set. from Vaswani et al. (2017). For subword segmentation, we use the SentencePiece tokenizer (Kudo and Richardson, 2018) with vocabulary size 8000, which has been shown to produce the best results with the data set sizes that we are dealing with (Gowda and May, 2020; Gr¨onroos et al., 2021). We train the models until the cross-entropy of the validation set does not improve for 10 consecutive validation steps. For the RBMT backtranslations, we use Apertium with the sme-fin model by Pirinen et al. (2017). This system implements a shallow transfer-based translation engine consisting of modules for morphological analysis, disambiguation and generation, modules for lexical translation based on context rules, and a m"
2021.nodalida-main.37,P12-3005,0,0.0489923,"tences are empty or longer than 100 words, • The ratio of the sentence lengths in words is greater than 3, • The sentence pair contains words longer than 40 characters, • The sentence pair contains HTML elements, • The sentences have dissimilar numerals based on the “Non-zero numerals score” (V´azquez et al., 2019), • The sentences have dissimilar punctuation based on the “Terminal punctuation score” (V´azquez et al., 2019), • The sentence pair contains characters outside of the Latin script, • The sentences are not recognized to be their correct language by the langid.py language identifier (Lui and Baldwin, 2012). After filtering, 29,106 clean sentence pairs remain in the parallel data set. From this clean set, 2000 pairs are randomly selected to form a validation set and another 2000 pairs to form a test set, leaving 25,106 pairs for training. Note that all subsets are disjoint due to the initial deduplication. The additional test set consists of two news articles describing S´ami culture in Finland available in both Finnish and Northern S´ami on YLE News. It was extracted from the web and manually aligned to create a clean reference set. This test set 2 https://yle.fi/uutiset/osasto/sapmi/ https://g"
2021.nodalida-main.37,P02-1040,0,0.113675,"r¨onroos et al., 2021). We train the models until the cross-entropy of the validation set does not improve for 10 consecutive validation steps. For the RBMT backtranslations, we use Apertium with the sme-fin model by Pirinen et al. (2017). This system implements a shallow transfer-based translation engine consisting of modules for morphological analysis, disambiguation and generation, modules for lexical translation based on context rules, and a module for syntactic transformation operations. Table 1 shows the quality of the sme-fin translation models used for backtranslations in BLEU points (Papineni et al., 2002). The NMT model performs much better with UiT test data than with the YLE test data, which shows that the NMT system is strongly adapted to the UiT data, while the RBMT system has similar performance with both test sets. 4.1 Backtranslations All the 462,803 sentences of the cleaned monolingual data are translated with the sme-fin NMT and RBMT models. As the quality of the source side of the backtranslations is not as important as the quality of the target side (Sennrich et al., 2016), we keep an unfiltered version of both backtranslation data sets. To see the effect of filtering the augmented"
2021.nodalida-main.37,W17-0214,0,0.13371,"driven translation systems with automatically created synthetic data, e.g. backtranslation (Sennrich et al., 2016). In this paper, we combine both strategies in the context of neural machine translation (NMT) from Finnish to Northern S´ami. In particular, we investigate the impact of RBMT in data augmentation in comparison to standard NMT-based backtranslation. Northern S´ami is a Uralic minority language spoken in Norway, Sweden and Finland. Historically, most of the work on machine translation from and to S´ami languages is based on RBMT (Trosterud and Unhammer, 2012; Antonsen et al., 2017; Pirinen et al., 2017). Data-driven approaches such as NMT are generally more competitive, but require large amounts of training data in the form of parallel translated sentences. For minority languages, finding parallel data sets is usually more difficult than collecting monolingual data, which is also the case for Northern S´ami. A common way of leveraging monolingual data for NMT is the above mentioned backtranslation strategy, a method where monolingual data of the target language is translated automatically to the source language to create additional parallel training data. In this work, we use two reverse tra"
2021.nodalida-main.37,R19-1107,0,0.0350046,"Missing"
2021.nodalida-main.37,P16-1009,0,0.274972,"es the RBMT approach only for the in-domain test set. This suggests that the RBMT system provides general-domain knowledge that cannot be found from the relative small parallel training data. 1 Introduction Machine translation from and to minority languages is challenging because large parallel corpora are typically hard to obtain. Two strategies have proven most successful to eliminate this bottleneck: using rule-based machine translation (RBMT) systems that do not rely on large data, or training data-driven translation systems with automatically created synthetic data, e.g. backtranslation (Sennrich et al., 2016). In this paper, we combine both strategies in the context of neural machine translation (NMT) from Finnish to Northern S´ami. In particular, we investigate the impact of RBMT in data augmentation in comparison to standard NMT-based backtranslation. Northern S´ami is a Uralic minority language spoken in Norway, Sweden and Finland. Historically, most of the work on machine translation from and to S´ami languages is based on RBMT (Trosterud and Unhammer, 2012; Antonsen et al., 2017; Pirinen et al., 2017). Data-driven approaches such as NMT are generally more competitive, but require large amount"
2021.nodalida-main.37,W19-7102,0,0.0294514,"Missing"
2021.nodalida-main.37,2020.acl-main.359,0,0.0180743,"ling et al. (2019) use RBMT backtranslations to improve statistical machine translation performance for Scottish Gaelic→English translations. The authors show that backtranslations can be beneficial even in cases where the translation quality of the MT system used to produce the backtranslations is low. Soto et al. (2019) study the performance of NMT systems trained with augmented training data backtranslated using RBMT, SMT and NMT. They experiment with Basque→Spanish translations and show that the translation performance improves when using each type of augmented training data individually. Soto et al. (2020) also analyze the effects of using augmented training data backtranslated with the three different paradigms. They focus on two language pairs: a low-resource language pair, Basque→Spanish, and a high-resource language pair, German→English. In addition to showing similar results as Soto et al. (2019), they show further improvement in translation performance when all types of augmented training data are combined. 3 Data The UiT freecorpus1 contains a Finnish - Northern S´ami (fin-sme) parallel corpus with 110k sentence pairs and a distinct set of 868k monolingual Northern S´ami sentences. The U"
2021.nodalida-main.37,2012.freeopmt-1.3,0,0.116304,"Missing"
2021.nodalida-main.37,W19-5441,1,0.894614,"Missing"
ahrenberg-etal-2000-evaluation,J93-1004,0,\N,Missing
ahrenberg-etal-2000-evaluation,C00-2163,0,\N,Missing
ahrenberg-etal-2000-evaluation,E93-1015,0,\N,Missing
ahrenberg-etal-2000-evaluation,J93-2003,0,\N,Missing
ahrenberg-etal-2000-evaluation,J90-2002,0,\N,Missing
ahrenberg-etal-2000-evaluation,P91-1022,0,\N,Missing
ahrenberg-etal-2000-evaluation,P98-1004,1,\N,Missing
ahrenberg-etal-2000-evaluation,C98-1004,1,\N,Missing
ahrenberg-etal-2000-evaluation,P00-1056,0,\N,Missing
C04-1031,P98-1004,0,0.133487,"sh-Swedish bitext together with a handcrafted reference alignment used for evaluation. 1 Introduction Word alignment is the task of identifying translational relations between words in parallel corpora with the aim of re-using them in natural language processing. Typical applications that make use of word alignment techniques are machine translation and multi-lingual lexicography. Several approaches have been proposed for the automatic alignment of words and phrases using statistical techniques and alignment heuristics, e.g. (Brown et al., 1993; Vogel et al., 1996; Garc´ıa-Varea et al., 2002; Ahrenberg et al., 1998; Tiedemann, 1999; Tufis and Barbu, 2002; Melamed, 2000). Word alignment usually includes links between so-called multi-word units (MWUs) in cases where lexical items cannot be split into separated words with appropriate translations in another language. See for example the alignment between an English sentence and a Swedish sentence illustrated in figure 1. There are MWUs in both languages aligned to corresponding translations in the other language. The Swedish compound “mittplatsen” corresponds to three words in English (“the middle seat”) and the English verb “dislike” is translated into a"
C04-1031,ahrenberg-etal-2000-evaluation,1,0.881842,"gnment attempts. Gold standards can be re-used for additional test runs which is important when examining different parameter settings. However, recall and precision derived from information retrieval have to be adjusted for the task of word alignment. The main difficulty with these measures in connection with word alignment arises with links between MWUs that cause partially correct alignments. It is not straightforward how to judge such links in order to compute precision and recall. In order to account for partiality we use a slightly modified version of the partiality score Q proposed in (Ahrenberg et al., 2000)2 : Qprecision = x x ∩ corr x |+ |alg x ∩ corr x | |algsrc src trg trg x |+ |alg x | |algsrc trg x ∩ corr x |+ |alg x ∩ corr x | |algsrc src trg trg x |+ |corr x | |corrsrc trg x The set of algsrc includes all source language words of all proposed links if at least one of them is partially correct with respect to the reference link x from the gold standard. Similarly, x refers to all the proposed target language algtrg x and corr x refer to the sets of words. corrsrc trg source and target language words in link x of the gold standard. Using the partiality value Q, we can define the recall and"
C04-1031,J93-2003,0,0.0255421,"ng a word alignment approach based on association clues and an English-Swedish bitext together with a handcrafted reference alignment used for evaluation. 1 Introduction Word alignment is the task of identifying translational relations between words in parallel corpora with the aim of re-using them in natural language processing. Typical applications that make use of word alignment techniques are machine translation and multi-lingual lexicography. Several approaches have been proposed for the automatic alignment of words and phrases using statistical techniques and alignment heuristics, e.g. (Brown et al., 1993; Vogel et al., 1996; Garc´ıa-Varea et al., 2002; Ahrenberg et al., 1998; Tiedemann, 1999; Tufis and Barbu, 2002; Melamed, 2000). Word alignment usually includes links between so-called multi-word units (MWUs) in cases where lexical items cannot be split into separated words with appropriate translations in another language. See for example the alignment between an English sentence and a Swedish sentence illustrated in figure 1. There are MWUs in both languages aligned to corresponding translations in the other language. The Swedish compound “mittplatsen” corresponds to three words in English"
C04-1031,C02-1032,0,0.033857,"Missing"
C04-1031,W95-0115,0,0.0118602,"). tions between lexical items collected from different sources. Declarative clues can be taken from linguistic resources such as bilingual dictionaries. They may also include pre-defined relations between lexical items based on certain features such as parts of speech. Estimated clues are derived from the parallel data using, for example, measures of co-occurrence (e.g. the Dice coefficient (Smadja et al., 1996)), statistical alignment models (e.g. IBM models from statistical machine translation (Brown et al., 1993)), or string similarity measures (e.g. the longest common sub-sequence ratio (Melamed, 1995)). They can also be learned from previously aligned training data using linguistic and contextual features associated with aligned items. Relations between certain word classes with respect to the translational association of words belonging to these classes is one example of such clues that can be learned from aligned training data. In our experiments, for example, we will use clues that indicate relations between lexical items based on their part-of-speech tags and their positions in the sentence relative to each other. They are learned from automatically word-aligned training data. The clue"
C04-1031,1996.amta-1.13,0,0.0133935,"ket   t˚ alamod  no    one     one no is very • A new link is accepted if both items in the link are not yet aligned.  ingen   visar s¨ arskilt   mycket Using the inverse directional alignment strategy we would obtain the following links:  no    is ( ˆ∩ = L ) The intersection of links produces the following sets: LC R =    no one is   very patient ingen visar s¨ arskilt mycket ingen t˚ alamod            ingen visar s¨ arskilt mycket t˚ alamod      3.3 Competitive linking Another alignment approach is the competitive linking approach proposed by Melamed (Melamed, 1996). In this approach, one assumes that there are only one-to-one word links. The alignment is done in a greedy “bestfirst” search manner where links with the highest association scores are aligned first, and the aligned items are then immediately removed from the search space. This process is repeated until no more links can be found. In ˆ C ) for nonthis way, the optimal alignment (L overlapping one-to-one links is found. The number of possible links in an alignment is reduced to min(N, M ). Using competitive linking with our example we yield: ˆC = L  no    very is    one patient ingen s"
C04-1031,J00-2004,0,0.0277331,"t used for evaluation. 1 Introduction Word alignment is the task of identifying translational relations between words in parallel corpora with the aim of re-using them in natural language processing. Typical applications that make use of word alignment techniques are machine translation and multi-lingual lexicography. Several approaches have been proposed for the automatic alignment of words and phrases using statistical techniques and alignment heuristics, e.g. (Brown et al., 1993; Vogel et al., 1996; Garc´ıa-Varea et al., 2002; Ahrenberg et al., 1998; Tiedemann, 1999; Tufis and Barbu, 2002; Melamed, 2000). Word alignment usually includes links between so-called multi-word units (MWUs) in cases where lexical items cannot be split into separated words with appropriate translations in another language. See for example the alignment between an English sentence and a Swedish sentence illustrated in figure 1. There are MWUs in both languages aligned to corresponding translations in the other language. The Swedish compound “mittplatsen” corresponds to three words in English (“the middle seat”) and the English verb “dislike” is translated into a Swedish particle verb “tycker om” (English: like) that h"
C04-1031,W03-0301,0,0.0227993,"its Swedish translation (Bellow, 1977) (the Bellow corpus). word alignment as links between words in the source language and words in the target language as indicated by the arrows in figure 1. However, in cases like the English expression “I am not really put out” which corresponds to the Swedish expression “det g¨or mig inte s˚ a mycket” there is no proper way of connecting single words with each other in order to express this relation. In some approaches such relations are constructed in form of an exhaustive set of links between all word pairs included in both expressions (Melamed, 1998; Mihalcea and Pedersen, 2003). In other approaches complex expressions are identified in a pre-processing step in order to handle them as complex units in the same manner as single words in alignment (Smadja et al., 1996; Ahrenberg et al., 1998; Tiedemann, 1999). The one-to-one word linking approach seems to be very limited. However, single word links can be combined in order to describe links between multi-word units as illustrated in figure 1. In this paper we investigate different alignment strategies using this approach1 . For this we apply clue alignment introduced in the next section. 2 Word alignment with clues The"
C04-1031,C00-2163,0,0.0565518,"Missing"
C04-1031,P00-1056,0,0.0653969,"ctional alignment models applied to the example in figure 2:  no    one ˆD = L ingen ingen visar s¨ arskilt mycket is    very patient LC D =    no one is   very patient        ˆI = L ingen visar s¨ arskilt mycket t˚ alamod very    very one I LC =    no is   very one     is   very      very patient ( ∪ LC = no one is very patient ) The union and the intersection of links do not produce satisfactory results as seen in the example. Another alignment strategy is a refined ˆ R = {L ˆD ∩ L ˆI } ∪ combination of link sets (L R R {L1 , ..., Lr }) as suggested by (Och and Ney, 2000b). In this approach, the intersection of links is iteratively extended by additional links LR r which pass one of the following two constraints: • Mapped on a two-dimensional bitext space, the new link is either vertically or horizontally adjacent to an existing link and the new link does not cause any link to be adjacent to other links in both dimensions (horizontally and vertically). Applying this approach to the example, we get:    ˆR = L ingen ingen t˚ alamod visar s¨ arskilt mycket mycket                ingen t˚ alamod visar s¨ arskilt mycket  no      is very very"
C04-1031,J03-1002,0,0.0203764,"hrenberg et al., 1998; Tiedemann, 1999). The one-to-one word linking approach seems to be very limited. However, single word links can be combined in order to describe links between multi-word units as illustrated in figure 1. In this paper we investigate different alignment strategies using this approach1 . For this we apply clue alignment introduced in the next section. 2 Word alignment with clues The clue alignment approach has been presented in (Tiedemann, 2003). Alignment clues represent probabilistic indications of associa1 A similar study on statistical alignment models is included in (Och and Ney, 2003). tions between lexical items collected from different sources. Declarative clues can be taken from linguistic resources such as bilingual dictionaries. They may also include pre-defined relations between lexical items based on certain features such as parts of speech. Estimated clues are derived from the parallel data using, for example, measures of co-occurrence (e.g. the Dice coefficient (Smadja et al., 1996)), statistical alignment models (e.g. IBM models from statistical machine translation (Brown et al., 1993)), or string similarity measures (e.g. the longest common sub-sequence ratio (M"
C04-1031,J96-1001,0,0.0533534,"er, in cases like the English expression “I am not really put out” which corresponds to the Swedish expression “det g¨or mig inte s˚ a mycket” there is no proper way of connecting single words with each other in order to express this relation. In some approaches such relations are constructed in form of an exhaustive set of links between all word pairs included in both expressions (Melamed, 1998; Mihalcea and Pedersen, 2003). In other approaches complex expressions are identified in a pre-processing step in order to handle them as complex units in the same manner as single words in alignment (Smadja et al., 1996; Ahrenberg et al., 1998; Tiedemann, 1999). The one-to-one word linking approach seems to be very limited. However, single word links can be combined in order to describe links between multi-word units as illustrated in figure 1. In this paper we investigate different alignment strategies using this approach1 . For this we apply clue alignment introduced in the next section. 2 Word alignment with clues The clue alignment approach has been presented in (Tiedemann, 2003). Alignment clues represent probabilistic indications of associa1 A similar study on statistical alignment models is included i"
C04-1031,E03-1026,1,0.847433,"entified in a pre-processing step in order to handle them as complex units in the same manner as single words in alignment (Smadja et al., 1996; Ahrenberg et al., 1998; Tiedemann, 1999). The one-to-one word linking approach seems to be very limited. However, single word links can be combined in order to describe links between multi-word units as illustrated in figure 1. In this paper we investigate different alignment strategies using this approach1 . For this we apply clue alignment introduced in the next section. 2 Word alignment with clues The clue alignment approach has been presented in (Tiedemann, 2003). Alignment clues represent probabilistic indications of associa1 A similar study on statistical alignment models is included in (Och and Ney, 2003). tions between lexical items collected from different sources. Declarative clues can be taken from linguistic resources such as bilingual dictionaries. They may also include pre-defined relations between lexical items based on certain features such as parts of speech. Estimated clues are derived from the parallel data using, for example, measures of co-occurrence (e.g. the Dice coefficient (Smadja et al., 1996)), statistical alignment models (e.g."
C04-1031,tufis-barbu-2002-lexical,0,0.0154011,"fted reference alignment used for evaluation. 1 Introduction Word alignment is the task of identifying translational relations between words in parallel corpora with the aim of re-using them in natural language processing. Typical applications that make use of word alignment techniques are machine translation and multi-lingual lexicography. Several approaches have been proposed for the automatic alignment of words and phrases using statistical techniques and alignment heuristics, e.g. (Brown et al., 1993; Vogel et al., 1996; Garc´ıa-Varea et al., 2002; Ahrenberg et al., 1998; Tiedemann, 1999; Tufis and Barbu, 2002; Melamed, 2000). Word alignment usually includes links between so-called multi-word units (MWUs) in cases where lexical items cannot be split into separated words with appropriate translations in another language. See for example the alignment between an English sentence and a Swedish sentence illustrated in figure 1. There are MWUs in both languages aligned to corresponding translations in the other language. The Swedish compound “mittplatsen” corresponds to three words in English (“the middle seat”) and the English verb “dislike” is translated into a Swedish particle verb “tycker om” (Engli"
C04-1031,C96-2141,0,0.102195,"approach based on association clues and an English-Swedish bitext together with a handcrafted reference alignment used for evaluation. 1 Introduction Word alignment is the task of identifying translational relations between words in parallel corpora with the aim of re-using them in natural language processing. Typical applications that make use of word alignment techniques are machine translation and multi-lingual lexicography. Several approaches have been proposed for the automatic alignment of words and phrases using statistical techniques and alignment heuristics, e.g. (Brown et al., 1993; Vogel et al., 1996; Garc´ıa-Varea et al., 2002; Ahrenberg et al., 1998; Tiedemann, 1999; Tufis and Barbu, 2002; Melamed, 2000). Word alignment usually includes links between so-called multi-word units (MWUs) in cases where lexical items cannot be split into separated words with appropriate translations in another language. See for example the alignment between an English sentence and a Swedish sentence illustrated in figure 1. There are MWUs in both languages aligned to corresponding translations in the other language. The Swedish compound “mittplatsen” corresponds to three words in English (“the middle seat”)"
C04-1031,C98-1004,0,\N,Missing
C12-1160,I11-1062,0,0.0622133,"Missing"
C12-1160,P12-3005,0,0.107931,"Missing"
C12-1160,C00-2137,0,0.206395,"Missing"
C14-1175,E12-2012,0,0.039989,"endency parsing models. McDonald et al. (2013) propose delexicalized models as a simple baseline for model transfer and present encouraging labeled attachment scores (LAS) especially for closely related languages. As a reference, we have created similar baseline models using the same data set but a slightly different setup, which is compatible with the experiments we present later. Table 1 summarizes the scores in terms of LAS for all language pairs in the data set.1 In our setup, we apply MaltParser (Nivre et al., 2006) and optimize feature models and learning parameters using MaltOptimizer (Ballesteros and Nivre, 2012). For all cross-lingual experiments (columns represent target languages we test on), we always use the same feature model and parameters as we have found for the source language treebank. Contrasting our models with the scores from McDonald et al. (2013), we can see that they are comparable with some differences that are due to the tools and learning parameters they apply which are along the lines of Zhang and Nivre (2011). 4.2 Annotation Projection with Human Translations Our first batch of projection experiments considers parallel data taken from the well-known Europarl corpus, which is freq"
C14-1175,W06-2920,0,0.175777,"all language pairs in all translation directions using IBM model 4 Viterbi alignments. In contrast to Hwa et al. (2005), we then use symmetrization heuristics to combine forward and backward alignments, which is common practice in the SMT community. In particular, we apply the popular grow-diag-final-and heuristics as implemented in the Moses toolbox (Koehn et al., 2007). Let us first look at unlabeled attachment scores to compare results that can be achieved with harmonized annotation in contrast to the ones that we can see on the cross-lingually incompatible data from the CoNLL shared task (Buchholz and Marsi, 2006). Table 2 lists the scores that we obtain when applying our implementation of the direct projection algorithm.2 As expected, the performance on the CoNLL data is rather poor, which confirms the findings of Hwa et al. (2005) even though our scores are significantly above their results without post-correction. The scores on the Universal Treebank data, however, are up to about 20 UAS points higher than the corresponding results on CoNLL data but without any of the extensive post-processing transformations proposed by Hwa et al. (2005). 64 62 60 58 56 54 52 50 48 LAS on Universal Treebank data DE"
C14-1175,W12-3156,0,0.0137518,"y when the domain does not match the original training data. Starting with noisy source language annotations, the projection algorithm may transfer errors to the target language that can cause problems for the target language parsing model induced from that data. Using machine translation and the original source language treebanks, we avoid this kind of error propagation. Furthermore, we suspect that human translations are more difficult to align on the word level then machine translated data which are inherently based on word alignments and, therefore, tend to be more literal and consistent (Carpuat and Simard, 2012). Using statistical MT as our translation model, we can also obtain such alignment as a given output from the decoding process, which makes it unnecessary to run yet another error-prone process such as automatic word alignment. Furthermore, the treebank data is too small to be used alone with generative statistical alignment models. Concatenating the data with larger parallel data would help but domain mismatches may, again, negatively influence the alignment performance. In the following, we show the cross-lingual scores obtained by translating all treebanks in the Universal Treebank to all o"
C14-1175,D13-1205,0,0.0124591,"t only for a minority of the world’s languages. However, many NLP applications require robust tools and the development of language-specific resources is expensive and time consuming. Many of the common tools are based on data-driven techniques and they often require strong supervision to achieve reasonable results for real world applications. Fully unsupervised techniques are not a good alternative yet for tasks like data-driven syntactic parsing and, therefore, crosslingual learning has been proposed as a possible solution to quickly create initial tools for otherwise unsupported languages (Ganchev and Das, 2013). In syntactic parsing, two main strategies have been explored in cross-lingual learning: annotation projection and model transfer. The first strategy relies on parallel corpora and automatic word alignment that make it possible to map linguistics annotation from a source language to a new target language (Yarowsky et al., 2001; Hwa et al., 2005; T¨ackstr¨om et al., 2013a). The basic idea is that existing tools and models are used to process the source side of a parallel corpus and that projection heuristics guided by alignment can be used to transfer the automatic annotation to the target lan"
C14-1175,P07-2053,0,0.102429,"Missing"
C14-1175,P13-2121,0,0.0149543,"for that language. The translation models are trained on the entire Europarl corpus using a standard setup for phrase-based SMT and the Moses toolbox for training, tuning and decoding (Koehn et al., 2007). For tuning we use MERT (Och, 2003) and the newstest 2012 data provided by the annual workshop on statistical machine translation.4 and for language modeling, we use a combination of Europarl and News data provided from the same source. The language model is a standard 5-gram model estimated from the monolingual data using modified Kneser-Ney smoothing without pruning (applying KenLM tools (Heafield et al., 2013)). Table 3 summarizes the labeled attachment scores obtained with our projection approach on synthetic machine-translated data. The main observation we can make here is that this approach is very robust with respect to the noise introduced by the translation engine. Automatic translation is a difficult task on its own but we still achieve results that are similar to the ones from the projection approach on human translated data. Note that our training data is now much smaller5 compared to the data sizes used in Section 4.2 and, still, we outperform those models in several cases. This seems to"
C14-1175,D11-1006,0,0.331049,"ized POS tagset (Petrov et al., 2012), transfer models have been used for a variety of languages. The advantage over annotation projection approaches is that no parallel data is required (at least in the basic settings) and that training can be performed on gold standard annotation. However, it requires a common feature representation across languages (McDonald et al., 2013), which can be a strong bottleneck. There are also several extensions to improve the performance of transfer models. One idea is to use multiple source languages to increase the statistical ground for the learning process (McDonald et al., 2011; Naseem et al., 2012), a strategy that can also be used in the case of annotation projection. Another idea is to enhance models by cross-lingual word clusters (T¨ackstr¨om et al., 2012) and to use target language adaptation techniques with prior knowledge of language properties and their relatedness when using multiple sources in training (T¨ackstr¨om et al., 2013b). Based on the success of these techniques, model transfer has dominated recent research on cross-lingual learning. In this paper, we return to annotation projection as a powerful tool for porting syntactic parsers to new languages"
C14-1175,P12-1066,0,0.219165,"et al., 2012), transfer models have been used for a variety of languages. The advantage over annotation projection approaches is that no parallel data is required (at least in the basic settings) and that training can be performed on gold standard annotation. However, it requires a common feature representation across languages (McDonald et al., 2013), which can be a strong bottleneck. There are also several extensions to improve the performance of transfer models. One idea is to use multiple source languages to increase the statistical ground for the learning process (McDonald et al., 2011; Naseem et al., 2012), a strategy that can also be used in the case of annotation projection. Another idea is to enhance models by cross-lingual word clusters (T¨ackstr¨om et al., 2012) and to use target language adaptation techniques with prior knowledge of language properties and their relatedness when using multiple sources in training (T¨ackstr¨om et al., 2013b). Based on the success of these techniques, model transfer has dominated recent research on cross-lingual learning. In this paper, we return to annotation projection as a powerful tool for porting syntactic parsers to new languages. Building on the avai"
C14-1175,nivre-etal-2006-maltparser,0,0.0575224,"tion scheme across six languages. This data set enables cross-lingual learning of labeled dependency parsing models. McDonald et al. (2013) propose delexicalized models as a simple baseline for model transfer and present encouraging labeled attachment scores (LAS) especially for closely related languages. As a reference, we have created similar baseline models using the same data set but a slightly different setup, which is compatible with the experiments we present later. Table 1 summarizes the scores in terms of LAS for all language pairs in the data set.1 In our setup, we apply MaltParser (Nivre et al., 2006) and optimize feature models and learning parameters using MaltOptimizer (Ballesteros and Nivre, 2012). For all cross-lingual experiments (columns represent target languages we test on), we always use the same feature model and parameters as we have found for the source language treebank. Contrasting our models with the scores from McDonald et al. (2013), we can see that they are comparable with some differences that are due to the tools and learning parameters they apply which are along the lines of Zhang and Nivre (2011). 4.2 Annotation Projection with Human Translations Our first batch of p"
C14-1175,J03-1002,0,0.0108357,"-lingual parser induction. The corpus comes with automatic sentence alignments and is quite clean with respect to translation quality and sentence alignment accuracy. It is, therefore, well suited for our initial experiments with annotation projection even though the domain does not necessarily match the one included in the treebank test sets. Another important prerequisite for annotation projection is word alignment. Following the typical setup, we rely on automatic word alignment produced by models developed for statistical machine translation. Similar to Hwa et al. (2005), we apply GIZA++ (Och and Ney, 2003) to align the corpus for all language pairs in all translation directions using IBM model 4 Viterbi alignments. In contrast to Hwa et al. (2005), we then use symmetrization heuristics to combine forward and backward alignments, which is common practice in the SMT community. In particular, we apply the popular grow-diag-final-and heuristics as implemented in the Moses toolbox (Koehn et al., 2007). Let us first look at unlabeled attachment scores to compare results that can be achieved with harmonized annotation in contrast to the ones that we can see on the cross-lingually incompatible data fro"
C14-1175,P03-1021,0,0.005963,"ls. Concatenating the data with larger parallel data would help but domain mismatches may, again, negatively influence the alignment performance. In the following, we show the cross-lingual scores obtained by translating all treebanks in the Universal Treebank to all other languages. We leave out Korean here again, because no SMT training data is included in Europarl for that language. The translation models are trained on the entire Europarl corpus using a standard setup for phrase-based SMT and the Moses toolbox for training, tuning and decoding (Koehn et al., 2007). For tuning we use MERT (Och, 2003) and the newstest 2012 data provided by the annual workshop on statistical machine translation.4 and for language modeling, we use a combination of Europarl and News data provided from the same source. The language model is a standard 5-gram model estimated from the monolingual data using modified Kneser-Ney smoothing without pruning (applying KenLM tools (Heafield et al., 2013)). Table 3 summarizes the labeled attachment scores obtained with our projection approach on synthetic machine-translated data. The main observation we can make here is that this approach is very robust with respect to"
C14-1175,petrov-etal-2012-universal,0,0.124359,"bers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1854 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1854–1864, Dublin, Ireland, August 23-29 2014. The second strategy, model transfer instead relies on universal features and the transfer of model parameters from one language to another. The main idea is to reduce the need of language-specific information, e.g. using delexicalized parsers that ignore lexical information. Drawing from a harmonized POS tagset (Petrov et al., 2012), transfer models have been used for a variety of languages. The advantage over annotation projection approaches is that no parallel data is required (at least in the basic settings) and that training can be performed on gold standard annotation. However, it requires a common feature representation across languages (McDonald et al., 2013), which can be a strong bottleneck. There are also several extensions to improve the performance of transfer models. One idea is to use multiple source languages to increase the statistical ground for the learning process (McDonald et al., 2011; Naseem et al.,"
C14-1175,N12-1052,0,0.291447,"Missing"
C14-1175,N13-1126,0,0.397448,"Missing"
C14-1175,W14-1614,1,0.711329,"Missing"
C14-1175,H01-1035,0,0.742804,"d applications. Fully unsupervised techniques are not a good alternative yet for tasks like data-driven syntactic parsing and, therefore, crosslingual learning has been proposed as a possible solution to quickly create initial tools for otherwise unsupported languages (Ganchev and Das, 2013). In syntactic parsing, two main strategies have been explored in cross-lingual learning: annotation projection and model transfer. The first strategy relies on parallel corpora and automatic word alignment that make it possible to map linguistics annotation from a source language to a new target language (Yarowsky et al., 2001; Hwa et al., 2005; T¨ackstr¨om et al., 2013a). The basic idea is that existing tools and models are used to process the source side of a parallel corpus and that projection heuristics guided by alignment can be used to transfer the automatic annotation to the target language text. Using the projected annotation assuming that it is sufficiently correct, models can then be trained for the target language. However, directly projecting syntactic structure results in a rather poor performance when applied to resources that were developed separately for individual languages (Hwa et al., 2005). Exte"
C14-1175,P11-2033,0,0.058598,"or all language pairs in the data set.1 In our setup, we apply MaltParser (Nivre et al., 2006) and optimize feature models and learning parameters using MaltOptimizer (Ballesteros and Nivre, 2012). For all cross-lingual experiments (columns represent target languages we test on), we always use the same feature model and parameters as we have found for the source language treebank. Contrasting our models with the scores from McDonald et al. (2013), we can see that they are comparable with some differences that are due to the tools and learning parameters they apply which are along the lines of Zhang and Nivre (2011). 4.2 Annotation Projection with Human Translations Our first batch of projection experiments considers parallel data taken from the well-known Europarl corpus, which is frequently used in research on statistical machine translation (SMT). It contains large quantities of translated proceedings from the European Parliament for all but one language (namely 1 Note that we include punctuation in our evaluation. Ignoring punctuation leads to slightly higher scores but we do not report those numbers here. 1858 UAS on CoNLL data DE EN ES SV UAS on Universal Treebank data DE EN ES SV – 49.67 46.14 57."
D12-1108,W09-1114,0,0.0292868,"t promising step. Our main contribution with respect to the work by Langlais et al. (2007) is the introduction of the possibility of handling document-level models by lifting the assumption of sentence independence. As a consequence, enumerating the entire neighbourhood becomes too expensive, which is why we resort to a “first-choice” strategy that non-deterministically generates states and accepts the first one encountered that meets the acceptance criterion. More recently, Gibbs sampling was proposed as a way to generate samples from the posterior distribution of a phrase-based SMT decoder (Arun et al., 2009; Arun et al., 2010), a process that resembles local search in its use of a set of state-modifying operators to generate a sequence of decoder states. Where local search seeks for the best state attainable from a given initial state, Gibbs sampling produces a representative sample from the posterior. Like all work on SMT decoding that we know of, the Gibbs sampler presented by Arun et al. (2010) assumes independence of sentences and considers the complete neighbourhood of each state before taking a sample. 6 Conclusion In the last twenty years of SMT research, there has been a strong assumptio"
D12-1108,W11-1014,0,0.00662194,"Missing"
D12-1108,P01-1030,0,0.0117476,"d to realise higher gains from cross-sentence semantic information. They support our claim that crosssentence models should be examined more closely and that existing methods should be adapted to deal with them, a problem addressed by our main contribution, the local search document decoder. 5 Related Work Even though DP beam search (Koehn et al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based SMT, greedy hill-climbing techniques were advo1187 cated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al., 2008). That work anticipates many of the features found in our decoder, including the use of local search to refine an initial hypothesis produced by DP beam search. The possibility of using models that do not fit well into the beam search paradigm is mention"
D12-1108,N03-1010,0,0.0154231,"ins from cross-sentence semantic information. They support our claim that crosssentence models should be examined more closely and that existing methods should be adapted to deal with them, a problem addressed by our main contribution, the local search document decoder. 5 Related Work Even though DP beam search (Koehn et al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based SMT, greedy hill-climbing techniques were advo1187 cated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al., 2008). That work anticipates many of the features found in our decoder, including the use of local search to refine an initial hypothesis produced by DP beam search. The possibility of using models that do not fit well into the beam search paradigm is mentioned and illustra"
D12-1108,D11-1084,0,0.369773,"guage Learning, pages 1179–1190, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics any obvious way, especially if joint optimisation of a number of interdependent decisions over an entire document is required. Research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems. Consider, for instance, the work on cache-based language models by Tiedemann (2010) and Gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by Le Nagard and Koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and Hardmeier and Federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences. In this paper, we present a method for decoding complete documents in phrase-based SMT. Our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by the application of a s"
D12-1108,2010.iwslt-papers.10,1,0.837142,"ocument is required. Research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems. Consider, for instance, the work on cache-based language models by Tiedemann (2010) and Gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by Le Nagard and Koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and Hardmeier and Federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences. In this paper, we present a method for decoding complete documents in phrase-based SMT. Our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by the application of a series of operations using a hill climbing strategy to find a (local) maximum of the score function. This setup gives us complete freedom to define scoring functions over the entire document. Moreover, by optionally initialising the st"
D12-1108,W11-2123,0,0.0368737,"is important to keep 1180 The problem addressed by the decoder is the search for the state Sˆ with maximal score, such that Sˆ = arg max f (S). S (5) The feature functions implemented in our baseline system are identical to the ones found in the popular Moses SMT system (Koehn et al., 2007). In particular, our decoder has the following feature functions: 1. phrase translation scores provided by the phrase table including forward and backward conditional probabilities, lexical weights and a phrase penalty (Koehn et al., 2003), 2. n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3. a word penalty score, 4. a distortion model with geometric decay (Koehn et al., 2003), and 5. a feature indicating the number of times a given distortion limit is exceeded in the current state. In our experiments, the last feature is used with a fixed weight of negative infinity in order to limit the gaps between the coverage sets of adjacent anchored phrase pairs to a maximum value. In DP search, the distortion limit is usually enforced directly by the search algorithm and is not added as a feature. In our decoder, however, this restriction is not required to limit complexity, so we deci"
D12-1108,D07-1103,0,0.0127504,"left to right until the whole range [p; q] is covered. 4 goal of our experiments is to demonstrate the behaviour of the decoder and characterise its response to changes in the fundamental search parameters. The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al., 2011). The phrase table was trained on Europarl, news-commentary and UN data. To reduce the training data to a manageable size, singleton phrase pairs were removed before the phrase scoring step. Significance-based filtering (Johnson et al., 2007) was applied to the resulting phrase table. The language model was a 5gram model with Kneser-Ney smoothing trained on the monolingual News corpus with IRSTLM (Federico et al., 2008). Feature weights were trained with Minimum Error-Rate Training (MERT) (Och, 2003) on the news-test2008 development set using the DP beam search decoder and the MERT implementation of the Moses toolkit (Koehn et al., 2007). Experimental results are reported for the newstest2009 test set, a corpus of 111 newswire documents totalling 2,525 sentences or 65,595 English input tokens. Experimental Results In this section,"
D12-1108,P10-4006,0,0.0117936,"Missing"
D12-1108,N03-1017,0,0.283168,"tic phenomena such as pronominal anaphora cannot be translated correctly without referring to extra-sentential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it. DP beam search for phrase-based SMT was described by Koehn et al. (2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003). This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated. The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated. This fact is exploited by the search algorithm with a DP technique called hypothesis recombination (Och et al., 2001), which permits the elim"
D12-1108,P07-2045,0,0.0678837,"to a linear combination of K feature functions hk (S), each with a constant weight λk , so K f (S) = SMT Model (1) ∑ λk hk (S). (4) k=1 Our decoder is based on local search, so its state at any time is a representation of a complete translation of the entire document. Even though the decoder operates at the document level, it is important to keep 1180 The problem addressed by the decoder is the search for the state Sˆ with maximal score, such that Sˆ = arg max f (S). S (5) The feature functions implemented in our baseline system are identical to the ones found in the popular Moses SMT system (Koehn et al., 2007). In particular, our decoder has the following feature functions: 1. phrase translation scores provided by the phrase table including forward and backward conditional probabilities, lexical weights and a phrase penalty (Koehn et al., 2003), 2. n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3. a word penalty score, 4. a distortion model with geometric decay (Koehn et al., 2003), and 5. a feature indicating the number of times a given distortion limit is exceeded in the current state. In our experiments, the last feature is used with a fixed weight of negative"
D12-1108,2007.tmi-papers.13,0,0.889821,"oves are rejected in a row (rejection limit). Algorithm 1 Decoding algorithm Input: an initial document state S; search parameters maxsteps and maxrejected Output: a modified document state 1: nsteps ← 0 2: nrejected ← 0 3: while nsteps < maxsteps and nrejected < maxrejected do 4: S0 ← Neighbour(S) 5: if Accept( f (S0 ), f (S)) then 6: S ← S0 7: nrejected ← 0 8: else 9: nrejected ← nrejected + 1 10: end if 11: nsteps ← nsteps + 1 12: end while 13: return S A notable difference between this algorithm and other hill climbing algorithms that have been used for SMT decoding (Germann et al., 2004; Langlais et al., 2007) is its non-determinism. Previous work for sentence-level decoding employed a steepest ascent strategy which amounts to enumerating the complete neighbourhood of the current state as defined by the state operations and selecting the next state to be the best state found in the neighbourhood of the current one. Enumerating all neighbours of a given state, costly as it is, has the advantage that it makes it easy to prove local optimality of a state by recognising that all possible successor states have lower scores. It can be rather inefficient, since at every step only one modification will be"
D12-1108,2008.jeptalnrecital-long.12,0,0.0149019,"al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based SMT, greedy hill-climbing techniques were advo1187 cated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al., 2008). That work anticipates many of the features found in our decoder, including the use of local search to refine an initial hypothesis produced by DP beam search. The possibility of using models that do not fit well into the beam search paradigm is mentioned and illustrated with the example of a reversed n-gram language model, which the authors claim would be difficult to implement in a beam search decoder. Similarly to the work by Germann et al. (2001), their decoder is deterministic and explores the entire neighbourhood of a state in order to identify the most promising step. Our main contribu"
D12-1108,W10-1737,0,0.219847,"Missing"
D12-1108,W01-1408,0,0.0825319,"ring to extra-sentential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it. DP beam search for phrase-based SMT was described by Koehn et al. (2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003). This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated. The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated. This fact is exploited by the search algorithm with a DP technique called hypothesis recombination (Och et al., 2001), which permits the elimination of hypotheses from the search space if they coincide in a certain number of"
D12-1108,P03-1021,0,0.0172753,"e training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al., 2011). The phrase table was trained on Europarl, news-commentary and UN data. To reduce the training data to a manageable size, singleton phrase pairs were removed before the phrase scoring step. Significance-based filtering (Johnson et al., 2007) was applied to the resulting phrase table. The language model was a 5gram model with Kneser-Ney smoothing trained on the monolingual News corpus with IRSTLM (Federico et al., 2008). Feature weights were trained with Minimum Error-Rate Training (MERT) (Och, 2003) on the news-test2008 development set using the DP beam search decoder and the MERT implementation of the Moses toolkit (Koehn et al., 2007). Experimental results are reported for the newstest2009 test set, a corpus of 111 newswire documents totalling 2,525 sentences or 65,595 English input tokens. Experimental Results In this section, we present the results of a series of experiments with our document decoder. The 1184 Stability An important difference between our decoder and the classical DP decoder as well as previous work in SMT decoding with local search is that our decoder is inherently"
D12-1108,P02-1040,0,0.105323,"se in scores for all three test sets demonstrates that the hill climbing decoder manages to fix some of the search errors made by the DP search. The last row contains the scores obtained by adding in the semantic language model. Scores are presented for three publicly available test sets from recent WMT Machine Translation shared tasks, of which one (newstest2009) was used to monitor progress during development and select the final model. Adding the semantic language model results in a small increase in NIST scores (Doddington, 2002) for all three test sets as well as a small BLEU score gain (Papineni et al., 2002) for two out of three corpora. We note that the NIST score turned out to react more sensitively to improvements due to the semantic LM in all our experiments, which is reasonable because the model specifically targets content words, which benefit from the information weighting done by the NIST score. While the results we present do not constitute compelling evidence in favour of our semantic LM in its current form, they do suggest that this model could be improved to realise higher gains from cross-sentence semantic information. They support our claim that crosssentence models should be examin"
D12-1108,W10-1728,1,0.859272,"ational Natural c Language Learning, pages 1179–1190, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics any obvious way, especially if joint optimisation of a number of interdependent decisions over an entire document is required. Research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems. Consider, for instance, the work on cache-based language models by Tiedemann (2010) and Gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by Le Nagard and Koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and Hardmeier and Federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences. In this paper, we present a method for decoding complete documents in phrase-based SMT. Our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by"
D12-1108,J03-1005,0,0.0104688,"ential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it. DP beam search for phrase-based SMT was described by Koehn et al. (2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003). This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated. The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated. This fact is exploited by the search algorithm with a DP technique called hypothesis recombination (Och et al., 2001), which permits the elimination of hypotheses from the search space if they coincide in a certain number of final words with a bette"
D12-1108,P97-1037,0,0.0171941,"correctly without referring to extra-sentential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it. DP beam search for phrase-based SMT was described by Koehn et al. (2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003). This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated. The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated. This fact is exploited by the search algorithm with a DP technique called hypothesis recombination (Och et al., 2001), which permits the elimination of hypotheses from the search space if they coincide in a"
D12-1108,D07-1053,0,0.0691649,"Missing"
D13-1037,S10-1021,0,0.134997,"to interesting insights about anaphora resolution in a multi-lingual context. In particular, we show in this paper that the pronoun prediction task makes it possible to model the resolution of pronominal anaphora as a latent variable and opens up a way to solve a task relying on anaphora resolution without using any data annotated for anaphora. This is what we consider the main contribution of our present work. We start by modelling cross-lingual pronoun prediction as an independent machine learning task after doing anaphora resolution in the source language (English) using the BART software (Broscheit et al., 2010). We show that it is difficult to achieve satisfactory performance with standard maximum380 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 380–391, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics The latest version released in March is equipped with ... It is sold at ... La dernière version lancée en mars est dotée de ... • est vendue ... Figure 1: Task setup entropy classifiers especially for low-frequency pronouns such as the French feminine plural pronoun elles. We propose a neural network classifi"
D13-1037,2012.eamt-1.60,0,0.0357142,"vely. The target words are represented as one-hot vectors with the dimensionality of the target language vocabulary. These vectors are then averaged to yield a single vector per antecedent candidate. Finally, the vectors of all candidates for a given training example are weighted by the probabilities assigned to them by the anaphora resolver (p1 and p2 ) and summed to yield a single vector per training example. 3 Data Sets and External Tools We run experiments with two different test sets. The TED data set consists of around 2.6 million tokens of lecture subtitles released in the WIT3 corpus (Cettolo et al., 2012). The WIT3 training data yields 71,052 examples, which were randomly partitioned into a training set of 63,228 examples and a test set of 7,824 examples. The official WIT3 development and test sets were not used in our experiments. The news-commentary data set is version 6 of the parallel news-commentary corpus released as a part of the WMT 2011 training data1 . It contains around 2.8 million tokens of news text and yields 31,017 data points, 1 http://www.statmt.org/wmt11/translation-task. html (3 July 2013). 382 The feature setup of all our classifiers requires the detection of potential ante"
D13-1037,E09-1018,0,0.0150325,"ignments to project coreference annotations from one language to another with a view to training anaphora resolvers in the target language (Postolache et al., 2006; de Souza and Or˘asan, 2011). Rahman and Ng (2012) instead use machine translation to translate their test 389 data into a language for which they have an anaphora resolver and then project the annotations back to the original language. Completely unsupervised monolingual anaphora resolution has been approached using, e. g., Markov logic (Poon and Domingos, 2008) and the Expectation-Maximisation algorithm (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). To the best of our knowledge, the direct application of machine learning techniques to parallel data in a task related to anaphora resolution is novel in our work. Neural networks and deep learning techniques have recently gained some popularity in natural language processing. They have been applied to tasks such as language modelling (Bengio et al., 2003; Schwenk, 2007), translation modelling in statistical machine translation (Le et al., 2012), but also part-ofspeech tagging, chunking, named entity recognition and semantic role labelling (Collobert et al., 2011). In tasks related to anapho"
D13-1037,W05-0612,0,0.0253418,"ther work has used word alignments to project coreference annotations from one language to another with a view to training anaphora resolvers in the target language (Postolache et al., 2006; de Souza and Or˘asan, 2011). Rahman and Ng (2012) instead use machine translation to translate their test 389 data into a language for which they have an anaphora resolver and then project the annotations back to the original language. Completely unsupervised monolingual anaphora resolution has been approached using, e. g., Markov logic (Poon and Domingos, 2008) and the Expectation-Maximisation algorithm (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). To the best of our knowledge, the direct application of machine learning techniques to parallel data in a task related to anaphora resolution is novel in our work. Neural networks and deep learning techniques have recently gained some popularity in natural language processing. They have been applied to tasks such as language modelling (Bengio et al., 2003; Schwenk, 2007), translation modelling in statistical machine translation (Le et al., 2012), but also part-ofspeech tagging, chunking, named entity recognition and semantic role labelling (Collobert et al., 2011)"
D13-1037,E12-3001,0,0.373892,"with good results to project coreference annotations from one language into another by using word alignments (Postolache et al., 2006; Rahman and Ng, 2012). On the other hand, what is true in general need not be true for all types of linguistic elements. For instance, a substantial percentage of the English thirdperson subject pronouns he, she, it and they does not get realised as pronouns in French translations (Hardmeier, 2012). Moreover, it has been recognised by various authors in the statistical machine translation (SMT) community (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012) that pronoun translation is a difficult problem because, even when a pronoun does get translated as a pronoun, it may require choosing the correct word form based on agreement features that are not easily predictable from the source text. The work presented in this paper investigates the problem of cross-lingual pronoun prediction for English-French. Given an English pronoun and its discourse context as well as a French translation of the same discourse and word alignments between the two languages, we attempt to predict the French word aligned to the English pronoun. As far as we know, this"
D13-1037,2010.iwslt-papers.10,1,0.536436,"this fact has been exploited with good results to project coreference annotations from one language into another by using word alignments (Postolache et al., 2006; Rahman and Ng, 2012). On the other hand, what is true in general need not be true for all types of linguistic elements. For instance, a substantial percentage of the English thirdperson subject pronouns he, she, it and they does not get realised as pronouns in French translations (Hardmeier, 2012). Moreover, it has been recognised by various authors in the statistical machine translation (SMT) community (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012) that pronoun translation is a difficult problem because, even when a pronoun does get translated as a pronoun, it may require choosing the correct word form based on agreement features that are not easily predictable from the source text. The work presented in this paper investigates the problem of cross-lingual pronoun prediction for English-French. Given an English pronoun and its discourse context as well as a French translation of the same discourse and word alignments between the two languages, we attempt to predict the French word aligned to the English pronoun. As far a"
D13-1037,P03-1054,0,0.00907702,"e anaphora resolver BART to generate this information. BART (Broscheit et al., 2010) is an anaphora resolution toolkit consisting of a markable detection and feature extraction pipeline based on a variety of standard natural language processing (NLP) tools and a machine learning component to predict coreference links including both pronominal anaphora and noun-noun coreference. In our experiments, we always use BART’s markable detection and feature extraction machinery. Markable detection is based on the identification of noun phrases in constituency parses generated with the Stanford parser (Klein and Manning, 2003). The set of features extracted by BART is an extension of the widely used mention-pair anaphora resolution feature set by Soon et al. (2001) (see below, Section 6). In the experiments of the next two sections, we also use BART to predict anaphoric links for pronouns. The model used with BART is a maximum entropy ranker trained on the ACE02-npaper corpus (LDC2003T11). In order to obtain a probability distribution over antecedent candidates rather than onebest predictions or coreference sets, we modified the ranking component with which BART resolves pronouns to normalise and output the scores"
D13-1037,N12-1005,0,0.00779725,"been approached using, e. g., Markov logic (Poon and Domingos, 2008) and the Expectation-Maximisation algorithm (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). To the best of our knowledge, the direct application of machine learning techniques to parallel data in a task related to anaphora resolution is novel in our work. Neural networks and deep learning techniques have recently gained some popularity in natural language processing. They have been applied to tasks such as language modelling (Bengio et al., 2003; Schwenk, 2007), translation modelling in statistical machine translation (Le et al., 2012), but also part-ofspeech tagging, chunking, named entity recognition and semantic role labelling (Collobert et al., 2011). In tasks related to anaphora resolution, standard feedforward neural networks have been tested as a classifier in an anaphora resolution system (Stuckardt, 2007), but the network design presented in our work is novel. 9 Conclusion In this paper, we have introduced cross-lingual pronoun prediction as an independent natural language processing task. Even though it is not an end-to-end task, pronoun prediction is interesting for several reasons. It is related to the problem o"
D13-1037,W10-1737,0,0.450652,"Missing"
D13-1037,J03-1002,0,0.00316037,"ference resolution system (BART) to predict anaphoric links. Anaphora resolution is done by our neural network classifier and requires only some quantity of word-aligned parallel data for training, completely obviating the need for a coreference-annotated training set. 2 Task Setup The overall setup of the classification task we address in this paper is shown in Figure 1. We are given an English discourse containing a pronoun along with its French translation and word alignments between the two languages, which in our case were computed automatically using a standard SMT pipeline with GIZA++ (Och and Ney, 2003). We focus on the four English third-person subject pronouns he, she, it and they. The output of the classifier is a multinomial distribution over six classes: the four French subject pronouns il, elle, ils and elles, corresponding to masculine and feminine singular and plural, respectively; the impersonal pronoun ce/c’, which occurs in some very frequent constructions such as c’est (it is); and a sixth class OTHER, which indicates that none of these pronouns was used. In general, a pronoun may be aligned to multiple words; in this case, a training example is counted as a positive example for"
D13-1037,D08-1068,0,0.0258809,"to English-Czech data to resolve different uses of the pronoun it. Other work has used word alignments to project coreference annotations from one language to another with a view to training anaphora resolvers in the target language (Postolache et al., 2006; de Souza and Or˘asan, 2011). Rahman and Ng (2012) instead use machine translation to translate their test 389 data into a language for which they have an anaphora resolver and then project the annotations back to the original language. Completely unsupervised monolingual anaphora resolution has been approached using, e. g., Markov logic (Poon and Domingos, 2008) and the Expectation-Maximisation algorithm (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). To the best of our knowledge, the direct application of machine learning techniques to parallel data in a task related to anaphora resolution is novel in our work. Neural networks and deep learning techniques have recently gained some popularity in natural language processing. They have been applied to tasks such as language modelling (Bengio et al., 2003; Schwenk, 2007), translation modelling in statistical machine translation (Le et al., 2012), but also part-ofspeech tagging, chunking, named en"
D13-1037,postolache-etal-2006-transferring,0,0.0935963,"Missing"
D13-1037,N12-1090,0,0.203502,"tion When texts are translated from one language into another, the translation reconstructs the meaning or function of the source text with the means of the target language. Generally, this has the effect that the entities occurring in the translation and their mutual relations will display similar patterns as the entities in the source text. In particular, coreference patterns tend to be very similar in translations of a text, and this fact has been exploited with good results to project coreference annotations from one language into another by using word alignments (Postolache et al., 2006; Rahman and Ng, 2012). On the other hand, what is true in general need not be true for all types of linguistic elements. For instance, a substantial percentage of the English thirdperson subject pronouns he, she, it and they does not get realised as pronouns in French translations (Hardmeier, 2012). Moreover, it has been recognised by various authors in the statistical machine translation (SMT) community (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012) that pronoun translation is a difficult problem because, even when a pronoun does get translated as a pronoun, it may require choosing the c"
D13-1037,J01-4004,0,0.560755,"ection and feature extraction pipeline based on a variety of standard natural language processing (NLP) tools and a machine learning component to predict coreference links including both pronominal anaphora and noun-noun coreference. In our experiments, we always use BART’s markable detection and feature extraction machinery. Markable detection is based on the identification of noun phrases in constituency parses generated with the Stanford parser (Klein and Manning, 2003). The set of features extracted by BART is an extension of the widely used mention-pair anaphora resolution feature set by Soon et al. (2001) (see below, Section 6). In the experiments of the next two sections, we also use BART to predict anaphoric links for pronouns. The model used with BART is a maximum entropy ranker trained on the ACE02-npaper corpus (LDC2003T11). In order to obtain a probability distribution over antecedent candidates rather than onebest predictions or coreference sets, we modified the ranking component with which BART resolves pronouns to normalise and output the scores assigned by the ranker to all candidates instead of picking the highest-scoring candidate. 4 Baseline Classifiers In order to create a simple"
D13-1037,J03-4003,0,\N,Missing
D13-1037,sagot-etal-2006-lefff,0,\N,Missing
D19-6506,N18-1118,0,0.11533,"2013; • we provide a thorough analysis of two popular machine translation datasets in terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics recent work explores the feasability of extending NMT models to make them context-aware. A common approach is to use additional encoders for the context sentence(s) with a modified attention mechanism (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2018). Another technique (Miculicich et al., 2018; Maruf et al., 2019) explores the integration of context through a hierarchical architecture which models the contextual information in a structured manner using word-level and sentencelevel abstractions. The different models have been evaluated on different language pairs and different datasets. In this paper, we focus on a single language pair, English–German (in both directions), and on two textual domains: news translation and movie subtitles translation. For the news translation task (denoted as WMT) we rely on the establis"
D19-6506,W12-3156,0,0.0319356,"translation. Given the overall training data sizes, NMT models should thus be able to pick up this signal. Overall, the examined discourse-level features show consistent patterns across the training, validation and test sets. This was not necessarily expected for the WMT corpus, whose training set stems from a wide variety of sources.9 Three other discourse-level features could have been analyzed as well: We did not include verbal tenses, as we do not expect them to be particularly problematic for the German–English language pair. Likewise, we did not include measures for lexical consistency (Carpuat and Simard, 2012), as this was already reported to be handled well in SMT. Finally, we did not include ellipsis (Voita et al., 2019) as we found it difficult to detect and not very relevant for German. Negations: We establish a list of sentential and nominal negation words for both languages (cf. Table 7) and count the number of sentences that contain at least one negation word. We also count negation discrepancies, i.e. aligned sentence pairs where a negation was identified in one language but not in the other. While the overall frequencies of negations are similar in both corpora, there are significantly mor"
D19-6506,2012.eamt-1.60,0,0.0133131,"sCommentary v14, and Rapid2019 collections. We select the Newstest2015 and Newstest2016 corpora as our validation set and the Newstest2018 corpus as our test set. General properties of this dataset can be found in Table 1. Table 1 shows that the two datasets are comparable in terms of sentence numbers.4 However, the documents in OST are up to 50 times larger than those in WMT (cf. column Sents/Doc). On the other hand, WMT sentences are more than twice as long than OST sentences (cf. column Tokens/Sent), which is in line with our expectations. A third dataset based on transcripts of TED talks (Cettolo et al., 2012), has also been used for document-level translation (Agrawal et al., 2018). We do not consider this dataset for training due to its smaller size, but use the PROTEST test suite, which is based on this corpus, for evaluation (Guillou and Hardmeier, 2016; Guillou et al., 2018). 3.1 whereas pronouns are three to four times rarer in the WMT corpus.6 This divergence is to be expected, as OST consists mainly of dialogues. Not all pronouns are intrinsically hard to translate. Therefore, we also examine how many ambiguous pronouns occur in the corpora. To this end, the English and German corpora are w"
D19-6506,P11-2031,0,0.0483706,"-correlation and lexical matches using hunalign (Varga et al., 2005) to link the system output to the reference translations. The reported results from the fixed-size models are based on this approach. Evaluation Each system is evaluated on the respective test set using the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. In particular, we evaluate each of them on three variants of the test set: 5.1 Generic translation metrics We report BLEU and METEOR scores for all our experiments in Tables 4 and 5. The results and significance tests were computed using MultEval (Clark et al., 2011). By and large, the concatenation models are able to exploit contextual information: BLEU as well as METEOR scores decrease by statistically significant amounts if the context is inconsistent or absent. However, it is difficult to distinguish a winning configuration. In particular, the system that obtains the highest absolute scores is not necessarily the one that learns most from contextual information. The 1Prev+Curr → 1Prev+Curr system obtains the highest absolute scores among sliding window systems in all four tasks, but is not particularly affected by context inconsistencies. On the other"
D19-6506,W15-1003,0,0.0142582,"as been looked at (LapshinovaKoltunski and Hardmeier, 2017; Voita et al., 2018; Lapshinova-Koltunski et al., 2019). Other devices studied are verbal tenses (Gong et al., 2012; Lo´aiciga et al., 2014; Ramm and Fraser, 2016) and connectives (Meyer et al., 2012; Meyer and Popescu-Belis, 2012), although not using neural models. Motivated by approximating the ability of systems to grasp more abstract properties related to coherence, ambiguous words have also been targeted (Rios Gonzales et al., 2017; Bawden et al., 2018; Rios et al., 2018), as well as ellipsis (Voita et al., 2019). Last, negation (Fancellu and Webber, 2015) is a rather understudied phenomenon, but like pronouns and their antecedents, the scope of the negation can be in a different sentence. In this paper we investigate these features in the training data and assess translation using standard automatic metrics and a data scrambling strategy. 2.2 Context-aware NMT Tiedemann and Scherrer (2017) present a simple approach to context-aware NMT: instead of training the model on pairs of single source and target sentences, they add sentences from the left context to the sentence to be translated, either only on the source side or both on source and targ"
D19-6506,D12-1026,0,0.0235191,"titles (referred to as OST), we use data from the OpenSubtitles corpus released on OPUS3 with our own split into training, development and test data. More details about the data and our setup will be given in the following section. Guillou and Hardmeier, 2016; M¨uller et al., 2018; Guillou et al., 2018) have been largely at the center of attention, and more recently the translation of pronouns in the context of their coreferential chains has been looked at (LapshinovaKoltunski and Hardmeier, 2017; Voita et al., 2018; Lapshinova-Koltunski et al., 2019). Other devices studied are verbal tenses (Gong et al., 2012; Lo´aiciga et al., 2014; Ramm and Fraser, 2016) and connectives (Meyer et al., 2012; Meyer and Popescu-Belis, 2012), although not using neural models. Motivated by approximating the ability of systems to grasp more abstract properties related to coherence, ambiguous words have also been targeted (Rios Gonzales et al., 2017; Bawden et al., 2018; Rios et al., 2018), as well as ellipsis (Voita et al., 2019). Last, negation (Fancellu and Webber, 2015) is a rather understudied phenomenon, but like pronouns and their antecedents, the scope of the negation can be in a different sentence. In this pap"
D19-6506,W19-5321,0,0.244825,"r only on the source side or both on source and target sides. These models are evaluated on a German–English corpus extracted from OpenSubtitles, and the best results are obtained with two source sentences and one target sentence. Agrawal et al. (2018) extend these experiments by considering additional contexts. They evaluate their work on the IWSLT 2017 dataset for English–Italian, which consists of transcripts of TED talks. In 2019, the WMT conference featured for the first time a document-level translation task for English–German (Barrault et al., 2019). One of the best-performing systems (Junczys-Dowmunt, 2019) is based on a similar idea: all sentences of a document are concatenated and translated as a whole. Documents whose length exceeds the maximum sequence length defined by the model are simply split. The approaches outlined above, which we refer to as “concatenation models”, do not require any change to the NMT model architecture. Other 3 Two datasets for English–German document-level translation Different text genres and types exhibit different types of discourse-level properties. The choice of training corpus therefore determines what features a NMT model can potentially learn, and the choice"
D19-6506,E12-3001,0,0.029018,"paper, we investigate the discourse-related biases in data. Our contributions are twofold: 2 2.1 Related work Discourse Research about discourse and MT has shifted from explicitly enhancing systems with discourse knowledge to evaluating how much the systems have learned specific discourse features through different resources, test suites being a popular one (cf. Sim Smith, 2017; Popescu-Belis, 2019). Throughout, however, particular discourse phenomena are consistently targeted, as they are indeed indicators of globally good, cohesive and coherent texts. Pronouns (Hardmeier and Federico, 2010; Guillou, 2012; Hardmeier et al., 2013; • we provide a thorough analysis of two popular machine translation datasets in terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics recent work explores the feasability of extending NMT models to make them context-aware. A common approach is to use additional encoders for the context sentence(s) with a modified attention mechanism"
D19-6506,P18-4020,0,0.0671872,"Missing"
D19-6506,L16-1100,0,0.104266,"anguage pairs and different datasets. In this paper, we focus on a single language pair, English–German (in both directions), and on two textual domains: news translation and movie subtitles translation. For the news translation task (denoted as WMT) we rely on the established setup of WMT 20192 with the Newstest2018 data as our dedicated test set. For the movie subtitles (referred to as OST), we use data from the OpenSubtitles corpus released on OPUS3 with our own split into training, development and test data. More details about the data and our setup will be given in the following section. Guillou and Hardmeier, 2016; M¨uller et al., 2018; Guillou et al., 2018) have been largely at the center of attention, and more recently the translation of pronouns in the context of their coreferential chains has been looked at (LapshinovaKoltunski and Hardmeier, 2017; Voita et al., 2018; Lapshinova-Koltunski et al., 2019). Other devices studied are verbal tenses (Gong et al., 2012; Lo´aiciga et al., 2014; Ramm and Fraser, 2016) and connectives (Meyer et al., 2012; Meyer and Popescu-Belis, 2012), although not using neural models. Motivated by approximating the ability of systems to grasp more abstract properties relate"
D19-6506,W17-4810,0,0.0389785,"Missing"
D19-6506,W18-6435,1,0.893655,"Missing"
D19-6506,W19-2805,1,0.878249,"Missing"
D19-6506,P14-1065,0,0.0609459,"Missing"
D19-6506,L16-1147,1,0.83002,"nge to the NMT model architecture. Other 3 Two datasets for English–German document-level translation Different text genres and types exhibit different types of discourse-level properties. The choice of training corpus therefore determines what features a NMT model can potentially learn, and the choice of test corpus determines which features can be reliably evaluated. Our experiments are based on two datasets that cover the same language pair, but very different textual characteristics. The OST dataset is built from the English– German part of the publicly available OpenSubtitles2016 corpus (Lison and Tiedemann, 2016). Of the 16,910 movies and TV series in the collection, 16,510 are used for training, and 4 each are held out for development and testing purposes. Each movie is considered a single document. It corresponds to the dataset used in Tiedemann and Scherrer (2017). General properties of this dataset can be found in Table 1. The WMT dataset comprises the subset of corpora allowed at the WMT 2019 news translation 2 See http://www.statmt.org/wmt19/ translation-task.html. 3 http://opus.nlpl.eu/ OpenSubtitles2016.php 52 Corpus Documents Sentences Sents/Doc Tokens DE Tokens EN Tokens/Sent OST Train OST V"
D19-6506,2010.iwslt-papers.10,0,0.172,"Missing"
D19-6506,loaiciga-etal-2014-english,1,0.889712,"Missing"
D19-6506,W16-3418,0,0.0171114,"anaphor and the antecedent mismatch in features (they-singular, it/they group) are very poorly handled. Test suite metrics Discourse-specific metrics such as Guzm´an et al. (2014) would be welcome to assess the translation quality on specific discourse-level features such as those discussed in Section 3.1. However, they have the disadvantage of relying on a discourse parser, which we do not have for German. At 6 Conclusion We have presented two English–German document-level translation datasets and shown that they represent different text genres with 13 We used the provided tool described in Hardmeier and Guillou (2016). the differences are not significant. 58 different distributions of discourse-level features. The context-aware NMT models on these datasets show performance differences that are to some extent indicative of the underlying textual characteristics: the longer sentences in the news dataset make it harder to find differences between training configurations or evaluation setups. Fixed-window approaches show surprisingly good results on the movie subtitles dataset, but the impact of the realignment process remains to be investigated further. The general performance of a document-level MT system ca"
D19-6506,P14-5010,0,0.00242307,"ignificantly more discrepancies in the OST dataset. These can be ascribed to two factors: free translation (a negation can be paraphrased with expressions such as fail to, doubt if, etc.), and sentence alignment errors. Coreference chains: We assume that a large amount of pronouns, connectives and negations do not require access to large contexts for their correct translation, either because they are unambiguous or because the current sentence is sufficient for their disambiguation. To corroborate this assumption, we annotate the English corpora with the Stanford CoreNLP coreference resolver (Manning et al., 2014; Clark and Manning, 2016) and the German corpora with the CorZu coreference resolver (Tuggener, 2016).7 We first report the numbers of coreference chains identified by the resolvers. These numbers are hard to compare across languages due to different performance levels of the two resolvers, and translationese factors such as explicitation. However, they confirm the intuition that news text contains more referring entities than movie dialogues.8 4 Context-aware MT models In this paper, our main focus lies on concatenation models as one of the most straightforward and successful approaches to d"
D19-6506,D13-1037,1,0.817438,"tigate the discourse-related biases in data. Our contributions are twofold: 2 2.1 Related work Discourse Research about discourse and MT has shifted from explicitly enhancing systems with discourse knowledge to evaluating how much the systems have learned specific discourse features through different resources, test suites being a popular one (cf. Sim Smith, 2017; Popescu-Belis, 2019). Throughout, however, particular discourse phenomena are consistently targeted, as they are indeed indicators of globally good, cohesive and coherent texts. Pronouns (Hardmeier and Federico, 2010; Guillou, 2012; Hardmeier et al., 2013; • we provide a thorough analysis of two popular machine translation datasets in terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics recent work explores the feasability of extending NMT models to make them context-aware. A common approach is to use additional encoders for the context sentence(s) with a modified attention mechanism (Jean et al., 2017; Bawd"
D19-6506,N19-1313,0,0.192639,"terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics recent work explores the feasability of extending NMT models to make them context-aware. A common approach is to use additional encoders for the context sentence(s) with a modified attention mechanism (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2018). Another technique (Miculicich et al., 2018; Maruf et al., 2019) explores the integration of context through a hierarchical architecture which models the contextual information in a structured manner using word-level and sentencelevel abstractions. The different models have been evaluated on different language pairs and different datasets. In this paper, we focus on a single language pair, English–German (in both directions), and on two textual domains: news translation and movie subtitles translation. For the news translation task (denoted as WMT) we rely on the established setup of WMT 20192 with the Newstest2018 data as our dedicated test set. For the m"
D19-6506,W12-0117,0,0.0313976,"plit into training, development and test data. More details about the data and our setup will be given in the following section. Guillou and Hardmeier, 2016; M¨uller et al., 2018; Guillou et al., 2018) have been largely at the center of attention, and more recently the translation of pronouns in the context of their coreferential chains has been looked at (LapshinovaKoltunski and Hardmeier, 2017; Voita et al., 2018; Lapshinova-Koltunski et al., 2019). Other devices studied are verbal tenses (Gong et al., 2012; Lo´aiciga et al., 2014; Ramm and Fraser, 2016) and connectives (Meyer et al., 2012; Meyer and Popescu-Belis, 2012), although not using neural models. Motivated by approximating the ability of systems to grasp more abstract properties related to coherence, ambiguous words have also been targeted (Rios Gonzales et al., 2017; Bawden et al., 2018; Rios et al., 2018), as well as ellipsis (Voita et al., 2019). Last, negation (Fancellu and Webber, 2015) is a rather understudied phenomenon, but like pronouns and their antecedents, the scope of the negation can be in a different sentence. In this paper we investigate these features in the training data and assess translation using standard automatic metrics and a"
D19-6506,N16-1005,0,0.06311,"Missing"
D19-6506,2012.amta-papers.20,0,0.0252849,"OPUS3 with our own split into training, development and test data. More details about the data and our setup will be given in the following section. Guillou and Hardmeier, 2016; M¨uller et al., 2018; Guillou et al., 2018) have been largely at the center of attention, and more recently the translation of pronouns in the context of their coreferential chains has been looked at (LapshinovaKoltunski and Hardmeier, 2017; Voita et al., 2018; Lapshinova-Koltunski et al., 2019). Other devices studied are verbal tenses (Gong et al., 2012; Lo´aiciga et al., 2014; Ramm and Fraser, 2016) and connectives (Meyer et al., 2012; Meyer and Popescu-Belis, 2012), although not using neural models. Motivated by approximating the ability of systems to grasp more abstract properties related to coherence, ambiguous words have also been targeted (Rios Gonzales et al., 2017; Bawden et al., 2018; Rios et al., 2018), as well as ellipsis (Voita et al., 2019). Last, negation (Fancellu and Webber, 2015) is a rather understudied phenomenon, but like pronouns and their antecedents, the scope of the negation can be in a different sentence. In this paper we investigate these features in the training data and assess translation using s"
D19-6506,W17-4814,0,0.0187737,"analyzed. On one side, it is difficult to pinpoint particular contextual features that neural MT (NMT) models are picking up. On the other, it is difficult to judge good translations purely in terms of discourse features. In this paper, we investigate the discourse-related biases in data. Our contributions are twofold: 2 2.1 Related work Discourse Research about discourse and MT has shifted from explicitly enhancing systems with discourse knowledge to evaluating how much the systems have learned specific discourse features through different resources, test suites being a popular one (cf. Sim Smith, 2017; Popescu-Belis, 2019). Throughout, however, particular discourse phenomena are consistently targeted, as they are indeed indicators of globally good, cohesive and coherent texts. Pronouns (Hardmeier and Federico, 2010; Guillou, 2012; Hardmeier et al., 2013; • we provide a thorough analysis of two popular machine translation datasets in terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Associatio"
D19-6506,D18-1325,0,0.220364,"translation datasets in terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics recent work explores the feasability of extending NMT models to make them context-aware. A common approach is to use additional encoders for the context sentence(s) with a modified attention mechanism (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2018). Another technique (Miculicich et al., 2018; Maruf et al., 2019) explores the integration of context through a hierarchical architecture which models the contextual information in a structured manner using word-level and sentencelevel abstractions. The different models have been evaluated on different language pairs and different datasets. In this paper, we focus on a single language pair, English–German (in both directions), and on two textual domains: news translation and movie subtitles translation. For the news translation task (denoted as WMT) we rely on the established setup of WMT 20192 with the Newstest2018 data as our dedicate"
D19-6506,W17-4811,1,0.942765,"approximating the ability of systems to grasp more abstract properties related to coherence, ambiguous words have also been targeted (Rios Gonzales et al., 2017; Bawden et al., 2018; Rios et al., 2018), as well as ellipsis (Voita et al., 2019). Last, negation (Fancellu and Webber, 2015) is a rather understudied phenomenon, but like pronouns and their antecedents, the scope of the negation can be in a different sentence. In this paper we investigate these features in the training data and assess translation using standard automatic metrics and a data scrambling strategy. 2.2 Context-aware NMT Tiedemann and Scherrer (2017) present a simple approach to context-aware NMT: instead of training the model on pairs of single source and target sentences, they add sentences from the left context to the sentence to be translated, either only on the source side or both on source and target sides. These models are evaluated on a German–English corpus extracted from OpenSubtitles, and the best results are obtained with two source sentences and one target sentence. Agrawal et al. (2018) extend these experiments by considering additional contexts. They evaluate their work on the IWSLT 2017 dataset for English–Italian, which c"
D19-6506,W18-6307,0,0.0515987,"Missing"
D19-6506,P19-1116,0,0.121894,"Missing"
D19-6506,P02-1040,0,0.10417,"lized that they do not necessarily match with the segment boundaries in the reference data even though the original paper suggests that this should be rather stable (JunczysDowmunt, 2019). This is especially fatal if the number of segments does not match. Therefore, we apply standard sentence alignment based on length-correlation and lexical matches using hunalign (Varga et al., 2005) to link the system output to the reference translations. The reported results from the fixed-size models are based on this approach. Evaluation Each system is evaluated on the respective test set using the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. In particular, we evaluate each of them on three variants of the test set: 5.1 Generic translation metrics We report BLEU and METEOR scores for all our experiments in Tables 4 and 5. The results and significance tests were computed using MultEval (Clark et al., 2011). By and large, the concatenation models are able to exploit contextual information: BLEU as well as METEOR scores decrease by statistically significant amounts if the context is inconsistent or absent. However, it is difficult to distinguish a winning configuration. In particular, t"
D19-6506,P18-1117,0,0.0725072,"thorough analysis of two popular machine translation datasets in terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics recent work explores the feasability of extending NMT models to make them context-aware. A common approach is to use additional encoders for the context sentence(s) with a modified attention mechanism (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2018). Another technique (Miculicich et al., 2018; Maruf et al., 2019) explores the integration of context through a hierarchical architecture which models the contextual information in a structured manner using word-level and sentencelevel abstractions. The different models have been evaluated on different language pairs and different datasets. In this paper, we focus on a single language pair, English–German (in both directions), and on two textual domains: news translation and movie subtitles translation. For the news translation task (denoted as WMT) we rely on the established setup of WMT 2019"
D19-6506,W16-2203,0,0.0134477,"om the OpenSubtitles corpus released on OPUS3 with our own split into training, development and test data. More details about the data and our setup will be given in the following section. Guillou and Hardmeier, 2016; M¨uller et al., 2018; Guillou et al., 2018) have been largely at the center of attention, and more recently the translation of pronouns in the context of their coreferential chains has been looked at (LapshinovaKoltunski and Hardmeier, 2017; Voita et al., 2018; Lapshinova-Koltunski et al., 2019). Other devices studied are verbal tenses (Gong et al., 2012; Lo´aiciga et al., 2014; Ramm and Fraser, 2016) and connectives (Meyer et al., 2012; Meyer and Popescu-Belis, 2012), although not using neural models. Motivated by approximating the ability of systems to grasp more abstract properties related to coherence, ambiguous words have also been targeted (Rios Gonzales et al., 2017; Bawden et al., 2018; Rios et al., 2018), as well as ellipsis (Voita et al., 2019). Last, negation (Fancellu and Webber, 2015) is a rather understudied phenomenon, but like pronouns and their antecedents, the scope of the negation can be in a different sentence. In this paper we investigate these features in the training"
D19-6506,W18-6437,0,0.0598349,"Missing"
E03-1026,P98-1004,0,0.0945953,"s of translated texts is not a trivial task due to the diversity of natural languages. There are generally two approaches, the estimation approach which is used in, e.g., statistical machine translation, and the association approach which is used in, e.g., automatic extraction of bilingual terminology. In the estimation approach, alignment parameters are modeled as hidden parameters in a statistical translation model (Och and Ney, 2000). Association approaches base the alignment on similarity measures and association tests such as Dice scores (Smadj a et al., 1996; Tiedemann, 1999), t-scores (Ahrenberg et al., 1998) log-likelihood measures (Tufis and Barbu, 2002), and longest common subsequence ratios (Melamed, 1995). One of the main difficulties in all alignment strategies is the identification of appropriate units in the source and the target language to be aligned. This task is hard even for human experts as can be seen in the detailed guidelines which are required 339 for manual alignments (Merkel, 1999; Melamed, 1998). Many translation relations involve multiword units such as phrasal compounds, idiomatic expressions, and complex terms. Syntactic shifts can also require the consideration of a contex"
E03-1026,1999.mtsummit-1.31,0,0.0173841,"Missing"
E03-1026,ahrenberg-etal-2000-evaluation,1,0.873076,"Missing"
E03-1026,ahrenberg-etal-2002-system,0,0.0111088,"multiword units such as phrasal compounds, idiomatic expressions, and complex terms. Syntactic shifts can also require the consideration of a context larger than a single word. Some items are not translated at all. Splitting source and target language texts into appropriate units for alignment (henceforth: tokenization) is often not possible without considering the translation relations. In other words, initial tokenization borders may change when the translation relations are investigated. Human aligners frequently expand token units when aligning sentences manually depending on the context (Ahrenberg et al., 2002). Previous approaches use either iterative procedures to re-estimate alignment parameters (Smadja et al., 1996; Melamed, 1997; Vogel et al., 2000) or preprocessing steps for the identification of token Ngrams (Ahrenberg et al., 1998; Tiedemann, 1999). In our approach, we combine simple techniques for prior tokenization with dynamic techniques during the alignment phase. The second problem of traditional word alignment approaches is the fact that parameter estimations are usually based on plain text items only. Linguistic data, which could be used to identify associations between lexical items"
E03-1026,A00-1031,0,0.00308751,"procedure from section 3. 5 Clue Alignment Experiments 5.1 The Setup We applied the &quot;clue aligner&quot; to one of our parallel corpora from the PLUG project (Sägvall Hein, 1999), a novel by Saul Bellow &quot;To Jerusalem and back: a personal account&quot; with about 170,000 words in English and Swedish. The English portion of the corpus has been tagged automatically with PUS tags by the English maximum entropy tagger in the open-source software package Grok (Baldridge, 2002). The same package was used for shallow parsing of the English sentences. The Swedish portion was tagged by the Ngrambased TnT-tagger (Brants, 2000) which was trained for Swedish on the SUC corpus (Megyesi, 2001). Furthermore, we used a rule-based analyzer for syntactic parsing (Megyesi, 2002). Our basic alignment applies two association clues: the Dice coefficient and the longest common subsequence ratio (LCSR). Both clues have been weighted uniformly with a value of 0.5. The threshold for the Dice coefficient has been set to 0.3 and the minimal co-occurrence frequency to 2. The threshold of LCSR scores has been set to 0.4 and the minimal token length to 3 characters. 5 We certainly wanted to test the ability of finding phrasal links The"
E03-1026,W01-0519,0,0.0121801,"Setup We applied the &quot;clue aligner&quot; to one of our parallel corpora from the PLUG project (Sägvall Hein, 1999), a novel by Saul Bellow &quot;To Jerusalem and back: a personal account&quot; with about 170,000 words in English and Swedish. The English portion of the corpus has been tagged automatically with PUS tags by the English maximum entropy tagger in the open-source software package Grok (Baldridge, 2002). The same package was used for shallow parsing of the English sentences. The Swedish portion was tagged by the Ngrambased TnT-tagger (Brants, 2000) which was trained for Swedish on the SUC corpus (Megyesi, 2001). Furthermore, we used a rule-based analyzer for syntactic parsing (Megyesi, 2002). Our basic alignment applies two association clues: the Dice coefficient and the longest common subsequence ratio (LCSR). Both clues have been weighted uniformly with a value of 0.5. The threshold for the Dice coefficient has been set to 0.3 and the minimal co-occurrence frequency to 2. The threshold of LCSR scores has been set to 0.4 and the minimal token length to 3 characters. 5 We certainly wanted to test the ability of finding phrasal links Therefore, both association clues have been calculated for pairs of"
E03-1026,W95-0115,0,0.0179077,"pproaches, the estimation approach which is used in, e.g., statistical machine translation, and the association approach which is used in, e.g., automatic extraction of bilingual terminology. In the estimation approach, alignment parameters are modeled as hidden parameters in a statistical translation model (Och and Ney, 2000). Association approaches base the alignment on similarity measures and association tests such as Dice scores (Smadj a et al., 1996; Tiedemann, 1999), t-scores (Ahrenberg et al., 1998) log-likelihood measures (Tufis and Barbu, 2002), and longest common subsequence ratios (Melamed, 1995). One of the main difficulties in all alignment strategies is the identification of appropriate units in the source and the target language to be aligned. This task is hard even for human experts as can be seen in the detailed guidelines which are required 339 for manual alignments (Merkel, 1999; Melamed, 1998). Many translation relations involve multiword units such as phrasal compounds, idiomatic expressions, and complex terms. Syntactic shifts can also require the consideration of a context larger than a single word. Some items are not translated at all. Splitting source and target language"
E03-1026,W97-0311,0,0.0591939,"n of a context larger than a single word. Some items are not translated at all. Splitting source and target language texts into appropriate units for alignment (henceforth: tokenization) is often not possible without considering the translation relations. In other words, initial tokenization borders may change when the translation relations are investigated. Human aligners frequently expand token units when aligning sentences manually depending on the context (Ahrenberg et al., 2002). Previous approaches use either iterative procedures to re-estimate alignment parameters (Smadja et al., 1996; Melamed, 1997; Vogel et al., 2000) or preprocessing steps for the identification of token Ngrams (Ahrenberg et al., 1998; Tiedemann, 1999). In our approach, we combine simple techniques for prior tokenization with dynamic techniques during the alignment phase. The second problem of traditional word alignment approaches is the fact that parameter estimations are usually based on plain text items only. Linguistic data, which could be used to identify associations between lexical items are often ignored. Linguistic tools such as part-of-speech taggers, (shallow) parsers, named-entity recognizers become more a"
E03-1026,P00-1056,0,0.108009,"d the optimal alignment of words and phrases for a given translation according to the model M and its parameters (alignment recovery). Modeling the relations between lexical units of translated texts is not a trivial task due to the diversity of natural languages. There are generally two approaches, the estimation approach which is used in, e.g., statistical machine translation, and the association approach which is used in, e.g., automatic extraction of bilingual terminology. In the estimation approach, alignment parameters are modeled as hidden parameters in a statistical translation model (Och and Ney, 2000). Association approaches base the alignment on similarity measures and association tests such as Dice scores (Smadj a et al., 1996; Tiedemann, 1999), t-scores (Ahrenberg et al., 1998) log-likelihood measures (Tufis and Barbu, 2002), and longest common subsequence ratios (Melamed, 1995). One of the main difficulties in all alignment strategies is the identification of appropriate units in the source and the target language to be aligned. This task is hard even for human experts as can be seen in the detailed guidelines which are required 339 for manual alignments (Merkel, 1999; Melamed, 1998)."
E03-1026,J96-1001,0,0.171251,"uire the consideration of a context larger than a single word. Some items are not translated at all. Splitting source and target language texts into appropriate units for alignment (henceforth: tokenization) is often not possible without considering the translation relations. In other words, initial tokenization borders may change when the translation relations are investigated. Human aligners frequently expand token units when aligning sentences manually depending on the context (Ahrenberg et al., 2002). Previous approaches use either iterative procedures to re-estimate alignment parameters (Smadja et al., 1996; Melamed, 1997; Vogel et al., 2000) or preprocessing steps for the identification of token Ngrams (Ahrenberg et al., 1998; Tiedemann, 1999). In our approach, we combine simple techniques for prior tokenization with dynamic techniques during the alignment phase. The second problem of traditional word alignment approaches is the fact that parameter estimations are usually based on plain text items only. Linguistic data, which could be used to identify associations between lexical items are often ignored. Linguistic tools such as part-of-speech taggers, (shallow) parsers, named-entity recognizer"
E03-1026,tufis-barbu-2002-lexical,0,0.0887051,"o the diversity of natural languages. There are generally two approaches, the estimation approach which is used in, e.g., statistical machine translation, and the association approach which is used in, e.g., automatic extraction of bilingual terminology. In the estimation approach, alignment parameters are modeled as hidden parameters in a statistical translation model (Och and Ney, 2000). Association approaches base the alignment on similarity measures and association tests such as Dice scores (Smadj a et al., 1996; Tiedemann, 1999), t-scores (Ahrenberg et al., 1998) log-likelihood measures (Tufis and Barbu, 2002), and longest common subsequence ratios (Melamed, 1995). One of the main difficulties in all alignment strategies is the identification of appropriate units in the source and the target language to be aligned. This task is hard even for human experts as can be seen in the detailed guidelines which are required 339 for manual alignments (Merkel, 1999; Melamed, 1998). Many translation relations involve multiword units such as phrasal compounds, idiomatic expressions, and complex terms. Syntactic shifts can also require the consideration of a context larger than a single word. Some items are not"
E03-1026,C98-1004,0,\N,Missing
E12-1015,P05-1074,0,0.0672278,"Missing"
E12-1015,2008.iwslt-papers.1,0,0.0275845,"r. This can be useful as we do not consider the entire hypothesis space but only a small subset of N-best lists. In the simplest case, this weight is set to 0.5 making both models equally important. An alternative to fitting the interpolation weight would be to perform a global optimization procedure. However, a straightforward implementation of pivot-based MERT would be prohibitively slow due to the expensive two-step translation procedure over nbest lists. A general condition for the pivot approach is to assume independent training sets for both translation models as already pointed out by (Bertoldi et al., 2008). In contrast to research presented in related work (see, for example, (Koehn et al., 2009)) this condition is met in our setup in which all data sets represent different samples over the languages considered (see section 4).2 3 Character-Based SMT The basic idea behind character-based translation models is to take advantage of the strong lexical and syntactic similarities between closely related languages. Consider, for example, Figure 1. Related languages like Catalan and Spanish or Danish and Norwegian have common roots and, therefore, use similar concepts and express them in similar gramma"
E12-1015,P07-1092,0,0.0645261,"a resource-poor language (source or target) and the other one refers to a standard model with appropriate data resources. A condition is that we have at least some training data for the translation between pivot and the resource-poor language. However, for the original task (source-to-target translation) we do not require any data resources except for purposes of comparison. We will explore various models for the translation between the resource-poor language and the pivot language and most of them are not compatible with standard phrase-based translation models. Hence, triangulation methods (Cohn and Lapata, 2007) for combining phrase tables are not applicable in our case. Instead, we explore a cascaded approach (also called “transfer method” (Wu and Wang, 2009)) in which we translate the input text in two steps using a linear interpolation for rescoring N-best lists. Following the method described in (Utiyama and Isahara, 2007) and (Wu and Wang, 2009), we use the best n hypotheses from the translation of source sentences s to pivot sentences p and combine them with the top m hypotheses for translating these pivot sentences to target sentences t: tˆ ≈ argmax t L X k=1 sp pt pt αλsp k hk (s, p) + (1 − α"
E12-1015,C10-1027,0,0.0538602,"Missing"
E12-1015,fishel-kirik-2010-linguistically,0,0.0131008,"and this may cause serious confusion of the word alignment model that very much relies on context-independent lexical translation probabilities. Hence, for character alignment, the lexical evidence is much less reliable without their context. It is certainly possible to find a compromise between word-level and character-level models in order to generalize below word boundaries but avoiding alignment problems as discussed above. Morpheme-based translation models have been explored in several studies with similar motivations as in our approach, a better generalization from sparse training data (Fishel and Kirik, 2010; Luong et al., 2010). However, these approaches have the drawback that they require proper morphological analyses. Data-driven techniques exist even for morphology, but their use in SMT still needs to be shown (Fishel, 2009). The situation is comparable to the problems of integrating linguistically motivated phrases into phrasebased SMT (Koehn et al., 2003). Instead we opt for a more general approach to extend context to facilitate, especially, the alignment step. Figure 3 shows how we can transform texts into sequences of bigrams that can be aligned with standard approaches without making an"
E12-1015,N07-1047,0,0.0540975,"to divide sentence pairs into aligned segments of reasonable size and the characters are then aligned with the same algorithm. An alternative is to use models designed for transliteration or related character-level transformation tasks. Many approaches are based on transducer models that resemble string edit operations such as insertions, deletions and substitutions (Ristad and Yianilos, 1998). Weighted finite state transducers (WFST’s) can be trained on unaligned pairs of character sequences and have been shown to be very effective for transliteration tasks or letter-to-phoneme conversions (Jiampojamarn et al., 2007). The training procedure usually employs an expectation maximization (EM) procedure and the resulting transducer can be used to find the Viterbi alignment between characters according to the best sequence of edit operations applied to transform one string into the other. Extensions to this model are possible, for example the use of many-to-many alignments which have been shown to be very effective in letter-to-phoneme alignment tasks (Jiampojamarn et al., 2007). One advantage of the edit-distance-based transducer models is that the alignments they predict are strictly monotonic and cannot easi"
E12-1015,N03-1017,0,0.0257424,"word boundaries but avoiding alignment problems as discussed above. Morpheme-based translation models have been explored in several studies with similar motivations as in our approach, a better generalization from sparse training data (Fishel and Kirik, 2010; Luong et al., 2010). However, these approaches have the drawback that they require proper morphological analyses. Data-driven techniques exist even for morphology, but their use in SMT still needs to be shown (Fishel, 2009). The situation is comparable to the problems of integrating linguistically motivated phrases into phrasebased SMT (Koehn et al., 2003). Instead we opt for a more general approach to extend context to facilitate, especially, the alignment step. Figure 3 shows how we can transform texts into sequences of bigrams that can be aligned with standard approaches without making any assumptions about linguistically motivated segmentations. 143 cu ur rs so o c co on nf fi ir rm ma ad do o . . ¿ q qu u´e e´ e es s e es so o ? ? Figure 3: Two Spanish sentences as sequences of character bigrams with a final ’ ’ marking the end of a sentence. In this way we can construct a parallel corpus with slightly richer contextual information as inpu"
E12-1015,P07-2045,0,0.0207442,"test the approach on movie subtitles for three language pairs and legal texts for another language pair in a domain adaptation task. Our pivot translations outperform the baselines by a large margin. 1 Introduction Data-driven approaches have been extremely successful in most areas of natural language processing (NLP) and can be considered the main paradigm in application-oriented research and development. Research in machine translation is a typical example with the dominance of statistical models over the last decade. This is even enforced due to the availability of toolboxes such as Moses (Koehn et al., 2007) which make it possible to build translation engines within days or even hours for any language pair provided that appropriate training data is available. However, this reliance on training data is also the most severe limitation of statistical approaches. Resources in large quantities are only available for a few languages and domains. In the case of SMT, the dilemma is even more apparent as parallel corpora are rare and usually quite sparse. Some languages can be considered lucky, for example, because of political situations that lead to the production of freely available translated material"
E12-1015,2009.mtsummit-papers.7,0,0.0379035,"t of N-best lists. In the simplest case, this weight is set to 0.5 making both models equally important. An alternative to fitting the interpolation weight would be to perform a global optimization procedure. However, a straightforward implementation of pivot-based MERT would be prohibitively slow due to the expensive two-step translation procedure over nbest lists. A general condition for the pivot approach is to assume independent training sets for both translation models as already pointed out by (Bertoldi et al., 2008). In contrast to research presented in related work (see, for example, (Koehn et al., 2009)) this condition is met in our setup in which all data sets represent different samples over the languages considered (see section 4).2 3 Character-Based SMT The basic idea behind character-based translation models is to take advantage of the strong lexical and syntactic similarities between closely related languages. Consider, for example, Figure 1. Related languages like Catalan and Spanish or Danish and Norwegian have common roots and, therefore, use similar concepts and express them in similar grammatical structures. Spelling conventions can still be quite different but those differences a"
E12-1015,D10-1015,0,0.0213086,"us confusion of the word alignment model that very much relies on context-independent lexical translation probabilities. Hence, for character alignment, the lexical evidence is much less reliable without their context. It is certainly possible to find a compromise between word-level and character-level models in order to generalize below word boundaries but avoiding alignment problems as discussed above. Morpheme-based translation models have been explored in several studies with similar motivations as in our approach, a better generalization from sparse training data (Fishel and Kirik, 2010; Luong et al., 2010). However, these approaches have the drawback that they require proper morphological analyses. Data-driven techniques exist even for morphology, but their use in SMT still needs to be shown (Fishel, 2009). The situation is comparable to the problems of integrating linguistically motivated phrases into phrasebased SMT (Koehn et al., 2003). Instead we opt for a more general approach to extend context to facilitate, especially, the alignment step. Figure 3 shows how we can transform texts into sequences of bigrams that can be aligned with standard approaches without making any assumptions about l"
E12-1015,D09-1141,0,0.128266,"Missing"
E12-1015,J03-1002,0,0.0112965,"Missing"
E12-1015,P03-1021,0,0.0198882,"Utiyama and Isahara, 2007) and (Wu and Wang, 2009), we use the best n hypotheses from the translation of source sentences s to pivot sentences p and combine them with the top m hypotheses for translating these pivot sentences to target sentences t: tˆ ≈ argmax t L X k=1 sp pt pt αλsp k hk (s, p) + (1 − α)λk hk (p, t) where hxy k are feature functions for model xy 1 Basically, this with appropriate weights λxy k . means that we simply add the scores and, similar to related work, we assume that the feature weights can be set independently for each model using minimum error rate training (MERT) (Och, 2003). In our setup we added the parameter α that can be used to weight the importance of one model over the other. This can be useful as we do not consider the entire hypothesis space but only a small subset of N-best lists. In the simplest case, this weight is set to 0.5 making both models equally important. An alternative to fitting the interpolation weight would be to perform a global optimization procedure. However, a straightforward implementation of pivot-based MERT would be prohibitively slow due to the expensive two-step translation procedure over nbest lists. A general condition for the p"
E12-1015,steinberger-etal-2006-jrc,0,0.0307577,"Missing"
E12-1015,2009.eamt-1.3,1,0.833329,"language of the second step. This applies only to models that are used as the first step in pivot-based translations. For other models, we include a string similarity measure based on the longest common subsequence ratio (LCSR) (Stephen, 1992) in order to give an impression about the “closeness” of the system 144 output to the reference translations. 4 Experiments We conducted a series of experiments to test the ideas of (character-level) pivot translation for resource-poor languages. We chose to use data from a collection of translated subtitles compiled in the freely available OPUS corpus (Tiedemann, 2009b). This collection includes a large variety of languages and contains mainly short sentences and sentence fragments, which suits character-level alignment very well. The selected settings represent translation tasks between languages (and domains) for which only very limited training data is available or none at all. Below we present results from two general tasks:4 (i) Translating between English and a resource-poor language (in both directions) via a pivot language that is close related to the resource-poor language. (ii) Translating between two languages in a domain for which no indomain t"
E12-1015,N07-1061,0,0.0604915,"equire any data resources except for purposes of comparison. We will explore various models for the translation between the resource-poor language and the pivot language and most of them are not compatible with standard phrase-based translation models. Hence, triangulation methods (Cohn and Lapata, 2007) for combining phrase tables are not applicable in our case. Instead, we explore a cascaded approach (also called “transfer method” (Wu and Wang, 2009)) in which we translate the input text in two steps using a linear interpolation for rescoring N-best lists. Following the method described in (Utiyama and Isahara, 2007) and (Wu and Wang, 2009), we use the best n hypotheses from the translation of source sentences s to pivot sentences p and combine them with the top m hypotheses for translating these pivot sentences to target sentences t: tˆ ≈ argmax t L X k=1 sp pt pt αλsp k hk (s, p) + (1 − α)λk hk (p, t) where hxy k are feature functions for model xy 1 Basically, this with appropriate weights λxy k . means that we simply add the scores and, similar to related work, we assume that the feature weights can be set independently for each model using minimum error rate training (MERT) (Och, 2003). In our setup w"
E12-1015,W07-0705,0,0.640021,"e similar concepts and express them in similar grammatical structures. Spelling conventions can still be quite different but those differences are often very consistent. The BosnianMacedonian example also shows that we do not have to require any alphabetic overlap in order to obtain character-level similarities. Regularities between such closely related languages can be captured below the word level. We can also assume a more or less monotonic relation between the two languages which motivates the idea of translation models over character Ngrams treating translation as a transliteration task (Vilar et al., 2007). Conceptually it is straightforward to think of phrase-based models on the character level. Sequences of characters can be used instead of word N-grams for both, translation and language models. Training can proceed with the same tools and approaches. The basic task is to 1 Note, that we do not require the same feature functions in both models even though the formula above implies this for simplicity of representation. 2 Note that different samples may still include common sentences. 142 Figure 1: Some examples of movie subtitle translations between closely related languages (either sharing p"
E12-1015,P07-1108,0,0.28139,"Missing"
E12-1015,P09-1018,0,0.0549781,"ast some training data for the translation between pivot and the resource-poor language. However, for the original task (source-to-target translation) we do not require any data resources except for purposes of comparison. We will explore various models for the translation between the resource-poor language and the pivot language and most of them are not compatible with standard phrase-based translation models. Hence, triangulation methods (Cohn and Lapata, 2007) for combining phrase tables are not applicable in our case. Instead, we explore a cascaded approach (also called “transfer method” (Wu and Wang, 2009)) in which we translate the input text in two steps using a linear interpolation for rescoring N-best lists. Following the method described in (Utiyama and Isahara, 2007) and (Wu and Wang, 2009), we use the best n hypotheses from the translation of source sentences s to pivot sentences p and combine them with the top m hypotheses for translating these pivot sentences to target sentences t: tˆ ≈ argmax t L X k=1 sp pt pt αλsp k hk (s, p) + (1 − α)λk hk (p, t) where hxy k are feature functions for model xy 1 Basically, this with appropriate weights λxy k . means that we simply add the scores and"
E17-2102,Q16-1031,0,0.0595622,"lies primarily in the use of language vectors and the empirical evaluation using nearly a thousand languages. Concurrent with this work, Johnson et al. (2016) conducted a study using neural machine translation (NMT), where a sub-word decoder is told which language to generate by means of a special language identifier token in the source sentence. This is close to our model, although beyond a simple interpolation experiment (as in our Section 5.3) they did not further explore the language vectors, which would have been challenging to do given the small number of languages used in their study. Ammar et al. (2016) used one-hot language identifiers as input to a multilingual word-based dependency parser, based on multilingual word embeddings. Given that they report this resulting in higher accuracy than using features from a typological database, it is a reasonable guess that their system learned language vectors which were able to encode syntactic properties relevant to the task. Unfortunately, they also did not look closer at the language vector space, which would have been interesting given the relatively large and diverse sample of languages represented in the Universal Dependencies treebanks. Our e"
E17-2102,P82-1020,0,0.716917,"Missing"
E17-2102,Q17-1024,0,0.0428488,"Missing"
guillou-etal-2014-parcor,broscheit-etal-2010-extending,0,\N,Missing
guillou-etal-2014-parcor,de-marneffe-etal-2006-generating,0,\N,Missing
guillou-etal-2014-parcor,W11-1211,0,\N,Missing
guillou-etal-2014-parcor,J93-2004,0,\N,Missing
guillou-etal-2014-parcor,popescu-belis-etal-2012-discourse,0,\N,Missing
guillou-etal-2014-parcor,W11-1901,0,\N,Missing
guillou-etal-2014-parcor,D13-1037,1,\N,Missing
guillou-etal-2014-parcor,W11-1902,0,\N,Missing
guillou-etal-2014-parcor,W08-0325,0,\N,Missing
guillou-etal-2014-parcor,P06-1055,0,\N,Missing
guillou-etal-2014-parcor,E12-3001,1,\N,Missing
guillou-etal-2014-parcor,W10-1737,0,\N,Missing
guillou-etal-2014-parcor,2010.iwslt-papers.10,1,\N,Missing
guillou-etal-2014-parcor,prasad-etal-2008-penn,1,\N,Missing
guillou-etal-2014-parcor,hajic-etal-2012-announcing,0,\N,Missing
guillou-etal-2014-parcor,postolache-etal-2006-transferring,0,\N,Missing
guillou-etal-2014-parcor,M98-1029,0,\N,Missing
guillou-etal-2014-parcor,W13-3306,0,\N,Missing
guillou-etal-2014-parcor,2012.eamt-1.60,0,\N,Missing
H05-1118,C04-1188,0,0.200238,"Missing"
H05-1118,radev-etal-2002-evaluating,0,0.0221264,"at stop words have been removed. Altogether we have 109 different keyword types weighted: keywords can be weighted using a boost factor 942 using the layers and the restrictions defined above. Now the question is to select appropriate keyword types among them with the optimal parameters (weights) to maximize retrieval performance. The following section describes the optimization procedure used to adjust query parameters. (on which the retrieval ranking is based) is one of the clues used by the answer identification modules. Therefore, we use the mean of the total reciprocal ranks (MTRR), cf. (Radev et al., 2002), to combine features of all three measures: M T RR = 4 Optimization of query parameters i In the previous sections we have seen the internal structure of the multi-layer index and the queries we use in our passage retrieval component. Now we have to address the question of how to select layers and restrict keywords to optimize the performance of the system according to the QA task. For this we employ an automatic optimization procedure that learns appropriate parameter settings from example data. We use annotated training material that is described in the next section. Thereafter, the optimiz"
H05-1118,A97-1046,0,0.0735892,"Missing"
hein-etal-2002-scaling,1997.tmi-1.12,1,\N,Missing
hein-etal-2002-scaling,2000.tc-1.4,1,\N,Missing
I17-1018,W14-4012,0,0.0792603,"Missing"
I17-1018,P08-1102,0,0.049283,"Missing"
I17-1018,P09-1058,0,0.586714,"tal number of combinatory labels. The efficiency can be improved if we reduce k. For some POS tags, combining them with the full boundary tags is redundant. For instance, only the functional word 的 can be tagged as DEG in Chinese Treebank (Xue et al., 2005). Since it is a single-character word, combinatory tags of B-DEG, I-DEG, and E-DEG never occur in the experimental data and should therefore be pruned to reduce the search space. Similarly, if the maximum length of words under a given POS tag is two in the training data, we prune the corresponding label. Tagging Scheme Following the work of Kruengkrai et al. (2009a), the employed tags indicating the word boundaries are B, I, E, S representing a character at the beginning, inside, end of a word or as a singlecharacter word. The CRF layer models conditional scores over all possible combinatory labels given the input characters. Incorporating the transition scores between the successive labels, the op1 夏 Output Our baseline model is an adaptation of BiRNNCRF. As illustrated in Figure 1, the Chinese characters are represented as vectors and fed into the bidirectional recurrent layers. The character representations will be described in detail in the followi"
I17-1018,D15-1176,0,0.0393132,"n Chinese dictionaries. In our approach, they are retrieved via the unicode representation of Chinese characters as the characters that share the same radical are grouped together. They are organised in consistent with the categorisation in Kangxi Dictionary (康熙字典), in which all the Chinese characters are grouped under 214 different radicals. We only employ the radicals of the common characters in the unicode range of (U+4E00, U+9FFF). For the characters out of the range and the non-Chinese characters, we use a single special vector as their radical representations. where f is usually an RNN (Ling et al., 2015) or a CNN (dos Santos and Zadrozny, 2014). In this paper, instead of completely relying on the BiRNN to extract contextual features from context-free character representations, we encode rich local information in the character vectors via employing the incrementally concatenated n-gram representation as demonstrated in Figure 2. In the example, the vector representation of the pivot character 太 in the given context is the concatenation of the context-free vector representation Vi,i of 太 itself along with Vi−1,i of the bigram 天太 as well as Vi−1,i+1 of the trigram 天太热. Instead of constructing th"
I17-1018,P16-1101,0,0.0838702,"nn@helsinki.fi Abstract joint model which predicts the combinatory labels of segmentation boundaries and POS tags at the character level. Joint segmentation and POS tagging becomes a standard character-based sequence tagging problem and therefore the general machine learning algorithms for structured prediction can be applied. The bidirectional recurrent neural network (RNN) using conditional random fields (CRF) (Lafferty et al., 2001) as the output interface for sentence-level optimisation (BiRNN-CRF) achieves state-of-the-art accuracies on various sequence tagging tasks (Huang et al., 2015; Ma and Hovy, 2016) and outperforms the traditional linear statistical models. RNNs with gated recurrent cells, such as long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU) (Cho et al., 2014) are capable of capturing long dependencies and retrieving rich global information. The sequential CRF on top of the recurrent layers ensures that the optimal sequence of tags over the entire sentence is obtained. In this paper, we model joint segmentation and POS tagging as a fully character-based sequence tagging problem via predicting the combinatory labels. The BiRNN-CRF archit"
I17-1018,W04-3236,0,0.174321,"egmentation and POS tagging. 1 Introduction Word segmentation and part-of-speech (POS) tagging are core steps for higher-level natural language processing (NLP) tasks. Given the raw text, segmentation is applied at the very first step and POS tagging is performed on top afterwards. As by convention the words in Chinese are not delimited by spaces, segmentation is non-trivial, but its accuracy has a significant impact on POS tagging. Moreover, POS tags provide useful information for word segmentation. Thus, modelling word segmentation and POS tagging jointly can outperform the pipeline models (Ng and Low, 2004; Zhang and Clark, 2008). POS tagging is a typical sequence tagging problem over segmented words, while segmentation also can be modelled as a character-level tagging problem via predicting the labels that identify the word boundaries. Ng and Low (2004) propose a 173 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 173–183, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (summer) (Zhang and Clark, 2010), which is a state-of-theart joint tagger using structured perceptron and beam decoding. According to the experimental results, our pro"
I17-1018,D14-1162,0,0.0852471,"hinese characters are logograms. As opposed to alphabetical languages, there is rich information 175 Char. embedding size n-gram embedding size Radical embedding size Character font Character size GRU state size Conv. filter size Conv. filter number Max pooling size Fully-connected size Optimiser Initial learning rate Decay rate Gradient Clipping Dropout rate Batch size 2.3.3 Pre-trained Character Embeddings The context-free vector representations of single characters introduced in section 2.3.1 can be replaced by pre-trained character embeddings retrieved from large corpora. We employ GloVe (Pennington et al., 2014) to train our character embeddings on Wikipedia2 and the freely available Sogou News Corpora (SogouCS).3 We use randomly initialised vectors as the representations of the characters that are not in the embedding vocabulary. Pre-trained embeddings for higher-order n-grams are not employed in this paper. 2.4 Ensemble Decoding At the final decoding phase, we use ensemble decoding, a simple averaging technique, to mitigate the deviations led by random weight initialisation of the neural network. For the chain CRF decoder, the final sequence of the combinatory tags y is obtained via the conditional"
I17-1018,P14-2042,0,0.164784,"Missing"
I17-1018,P11-1139,0,0.136417,"Missing"
I17-1018,D16-1157,0,0.0159777,"training algorithm is employed for sentence level optimisation, which is the same as the training algorithm of the BiRNNCRF model. Their proposed model is not evaluated on CTB5 and therefore difficult to be compared with our system. Kong et al. (2015) apply segmental recurrent neural networks to joint segmentation and POS tagging but the evaluation results are substantially below the state-of-the-art on CTB5. Bojanowski et al. (2016) retrieve word embeddings via representing words as a bag of character n-grams for morphologically rich languages. A similar character n-gram model is proposed by Wieting et al. (2016). Sun et al. (2014) attempt to encode radical information into the conventional character embeddings. The radicalenhanced embeddings are employed and evaluated for Chinese segmentation. The results show that radical-enhanced embeddings outperform both skip-ngram and continues bag-of-word (Mikolov et al., 2013) in word2vec. Table 8: Result comparisions on CTB5 in F1scores. used for processing very large files. The memory demand of decoding is drastically milder compared to training, a large batch size therefore can be employed. The tagger takes constant time to build the sub-computational graph"
I17-1018,P16-1040,0,0.0693485,"e are in comparison to Single (** p &lt; 0.01, * p &lt; 0.05) performs very well despite being fully character based. Moreover, it has clear advantages when applied to smaller datasets like UD Chinese, while the prevalence is much smaller on CTB5. Both our model and ZPar segment OOV words in UD Chinese with higher accuracies than the ones in CTBs despite that UD Chinese is notably smaller and the overall OOV rate is higher. Compared to CTB, the words in UD Chinese are more fine-grained and the average word length is shorter, which makes it easier for the tagger to correctly segment the OOV words as Zhang et al. (2016) show that the longer words are more difficult to be segmented correctly. For joint POS tagging for OOV words, the two systems both perform significantly better on CTB5 as it is only composed of news text. BN CS FM MG NS SM SP WB Ensemble Seg Seg&Tag 97.89* 94.48** 96.67** 91.78** 96.54** 91.92** 94.54** 89.23** 97.56 93.92** 96.43** 91.78** 97.29** 93.93** 94.27** 88.44** Seg 97.68 95.61 96.30 94.22 97.49 96.13 96.69 93.38 ZPar Seg&Tag 94.22 90.15 91.51 88.60 93.70 90.32 93.35 86.88 Table 7: Evaluation on Broadcast News (BN), Conversations (CS), Forum (FM), Magazine (MG), News (NS), Short Mes"
I17-1018,P08-1101,0,0.127339,"S tagging. 1 Introduction Word segmentation and part-of-speech (POS) tagging are core steps for higher-level natural language processing (NLP) tasks. Given the raw text, segmentation is applied at the very first step and POS tagging is performed on top afterwards. As by convention the words in Chinese are not delimited by spaces, segmentation is non-trivial, but its accuracy has a significant impact on POS tagging. Moreover, POS tags provide useful information for word segmentation. Thus, modelling word segmentation and POS tagging jointly can outperform the pipeline models (Ng and Low, 2004; Zhang and Clark, 2008). POS tagging is a typical sequence tagging problem over segmented words, while segmentation also can be modelled as a character-level tagging problem via predicting the labels that identify the word boundaries. Ng and Low (2004) propose a 173 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 173–183, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (summer) (Zhang and Clark, 2010), which is a state-of-theart joint tagger using structured perceptron and beam decoding. According to the experimental results, our proposed model outperforms"
I17-1018,D10-1082,0,0.638575,"vide useful information for word segmentation. Thus, modelling word segmentation and POS tagging jointly can outperform the pipeline models (Ng and Low, 2004; Zhang and Clark, 2008). POS tagging is a typical sequence tagging problem over segmented words, while segmentation also can be modelled as a character-level tagging problem via predicting the labels that identify the word boundaries. Ng and Low (2004) propose a 173 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 173–183, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (summer) (Zhang and Clark, 2010), which is a state-of-theart joint tagger using structured perceptron and beam decoding. According to the experimental results, our proposed model outperforms ZPar on all the datasets in terms of accuracy. The main contributions of this work include: 1. We apply the BiRNN-CRF model for general sequence tagging to joint segmentation and POS tagging for Chinese and achieve state-of-the-art accuracy. The experimental results show that our tagger is robust and accurate across datasets of different sizes, genres and annotation schemes. 2. We propose a novel approach for vector representations of ch"
I17-1018,D13-1061,0,0.225031,"Missing"
I17-1018,I11-1035,0,\N,Missing
I17-1018,L16-1262,1,\N,Missing
J19-2006,N18-1083,1,0.697995,"rds can be embedded, so can languages, by associating each language with a real-valued vector known as a language representation, which can be used to measure similarities between languages. This type of representation can be obtained by, for example, training a multilingual model for ¨ some NLP task (Johnson et al. 2017; Malaviya, Neubig, and Littell 2017; Ostling and Tiedemann 2017). The focus of this work is on the evaluation of similarities between such representations. This is an important area of work, as computational approaches to typology (Dunn et al. 2011; Cotterell and Eisner 2017; Bjerva and Augenstein 2018) have the potential to answer research questions on a much larger scale than traditional typological research (Haspelmath 2001). Furthermore, having knowledge about the relationships between languages can help in NLP applications (Ammar et al. 2016), and having incorrect interpretations can be detrimental to multilingual NLP efforts. For instance, if the similarities between languages in an embedded language space were to be found to encode geographical distances (Figure 1), any conclusions drawn from use of these representations would not likely be of much use for most NLP tasks. The importan"
J19-2006,W17-6508,0,0.029981,"gly correlated with genealogical distance, significant differences can be observed. Romanian, as a member of the Balkan sprachbund, is distinct from the other Romance languages. The North Germanic (Danish, Swedish) and West Germanic (Dutch, German) branches are separated through considerable structural differences, with English grouped with the North Germanic languages despite its West Germanic origin. The Baltic languages (Latvian, Lithuanian) are grouped with the nearby Finnic languages (Estonian, Finnish) rather than their distant Slavic relatives. This idea has been explored previously by Chen and Gerdes (2017), who use a combination of relative frequency, length, and direction of deprels. We, by comparison, achieve an even richer representation by also taking head and dependent POS into account. 386 Bjerva et al. What Do Language Representations Really Represent? Figure 4 Correlations between similarities (Genetic, Geo., and Struct.) and language representations (Raw, Func, POS, Phrase, Deprel). Significance at p &lt; 0.001 is indicated by *. 5. Analysis of Similarities Although we are able to reconstruct phylogenetic language trees in a similar manner to previous work, we wish to investigate whether"
J19-2006,P17-1109,0,0.0302127,"ddings. Similarly to how words can be embedded, so can languages, by associating each language with a real-valued vector known as a language representation, which can be used to measure similarities between languages. This type of representation can be obtained by, for example, training a multilingual model for ¨ some NLP task (Johnson et al. 2017; Malaviya, Neubig, and Littell 2017; Ostling and Tiedemann 2017). The focus of this work is on the evaluation of similarities between such representations. This is an important area of work, as computational approaches to typology (Dunn et al. 2011; Cotterell and Eisner 2017; Bjerva and Augenstein 2018) have the potential to answer research questions on a much larger scale than traditional typological research (Haspelmath 2001). Furthermore, having knowledge about the relationships between languages can help in NLP applications (Ammar et al. 2016), and having incorrect interpretations can be detrimental to multilingual NLP efforts. For instance, if the similarities between languages in an embedded language space were to be found to encode geographical distances (Figure 1), any conclusions drawn from use of these representations would not likely be of much use for"
J19-2006,D18-1029,0,0.102725,"Missing"
J19-2006,2005.mtsummit-papers.11,0,0.200565,"lations as, e.g., speakers from certain 195 195 nt tothe abstract away from the surface forms of distance measures provide more explanation (RQ2). distance measures provide more explanation (RQ2). 194 tigate representation learning on monolingual English sentences, which are translations are captured by language representations, or if other talksame about thecertain same issues. We therefore dtions to will talk about the issues. We therefore 196 196 as,tend e.g.,tospeakers from regions 195 distance provide more explanation from various source languages English from the (RQ2). Europarl corpus (Koehn 2005). 4.1toGenetic Distance introduce several levels ofWe abstraction: i) training onmeasures 4.1 Genetic Distance eo several levels of abstraction: i) training on 197 talk about the same issues. therefore 197 196 They use a feature-engineering approach to predict source languages and learn an 198 Following Rabinovich al. (2017), phylo4.1 etFollowing Genetic Distance several1This levels of abstraction: i) training is the exactassame used byon Rabinovich al. 198 Rabinovich et al.et(2017), we we use use phylo197 is the exact same data useddata by asRabinovich et al. tree Indo-European family using t"
J19-2006,D17-1268,0,0.260431,"Missing"
J19-2006,E17-2102,1,0.817678,"national (CC BY-NC-ND 4.0) license Computational Linguistics Volume 45, Number 2 1. Introduction Words can be represented with distributed word representations, currently often in the form of word embeddings. Similarly to how words can be embedded, so can languages, by associating each language with a real-valued vector known as a language representation, which can be used to measure similarities between languages. This type of representation can be obtained by, for example, training a multilingual model for ¨ some NLP task (Johnson et al. 2017; Malaviya, Neubig, and Littell 2017; Ostling and Tiedemann 2017). The focus of this work is on the evaluation of similarities between such representations. This is an important area of work, as computational approaches to typology (Dunn et al. 2011; Cotterell and Eisner 2017; Bjerva and Augenstein 2018) have the potential to answer research questions on a much larger scale than traditional typological research (Haspelmath 2001). Furthermore, having knowledge about the relationships between languages can help in NLP applications (Ammar et al. 2016), and having incorrect interpretations can be detrimental to multilingual NLP efforts. For instance, if the sim"
J19-2006,P17-1049,0,0.0968921,"Missing"
J19-2006,L16-1680,0,0.0311049,"Missing"
J19-2006,P15-2034,1,\N,Missing
J19-2006,Q17-1024,0,\N,Missing
kotze-etal-2012-large,N07-1051,0,\N,Missing
kotze-etal-2012-large,J93-1004,0,\N,Missing
kotze-etal-2012-large,de-marneffe-etal-2006-generating,0,\N,Missing
kotze-etal-2012-large,tiedemann-2010-lingua,1,\N,Missing
kotze-etal-2012-large,C08-1139,0,\N,Missing
kotze-etal-2012-large,C08-1038,0,\N,Missing
kotze-etal-2012-large,W06-1661,0,\N,Missing
kotze-etal-2012-large,J93-2003,0,\N,Missing
kotze-etal-2012-large,W01-1406,0,\N,Missing
kotze-etal-2012-large,2001.mtsummit-ebmt.4,0,\N,Missing
kotze-etal-2012-large,C04-1154,0,\N,Missing
kotze-etal-2012-large,C90-3045,0,\N,Missing
kotze-etal-2012-large,P03-1054,0,\N,Missing
kotze-etal-2012-large,P02-1040,0,\N,Missing
kotze-etal-2012-large,P10-1032,0,\N,Missing
kotze-etal-2012-large,2010.eamt-1.38,1,\N,Missing
kotze-etal-2012-large,2006.jeptalnrecital-invite.2,0,\N,Missing
kotze-etal-2012-large,P07-2045,0,\N,Missing
kotze-etal-2012-large,2006.amta-papers.15,0,\N,Missing
kotze-etal-2012-large,W08-0411,0,\N,Missing
kotze-etal-2012-large,P09-1063,0,\N,Missing
kotze-etal-2012-large,P03-2041,0,\N,Missing
kotze-etal-2012-large,J03-1002,0,\N,Missing
kotze-etal-2012-large,2009.eamt-1.21,1,\N,Missing
kotze-etal-2012-large,J10-2004,0,\N,Missing
kotze-etal-2012-large,2005.mtsummit-papers.11,0,\N,Missing
kotze-etal-2012-large,D12-1079,0,\N,Missing
kotze-etal-2012-large,W09-4206,1,\N,Missing
kotze-etal-2012-large,W90-0102,0,\N,Missing
L16-1147,2012.eamt-1.33,0,0.057398,"Missing"
L16-1147,P07-2045,0,0.0334977,"rent language pairs. 2016 release 39.15 34.05 24.78 16.14 21.26 24.01 26.62 21.21 25.49 17.05 25.34 9.28 33.34 27.49 18.20 11.85 29.67 22.12 Table 4: BLEU scores for the SMT systems based on the bitexts of the 2013 and 2016 release of OpenSubtitles. Alternative subtitles can also be fruitfully exploited to: • Detect errors (spelling mistakes, erroneous encodings, etc.) in the corpus; BLEU scores In order to empirically evaluate the quality of the alignments, we performed an extrinsic evaluation using the bitexts as training material for a statistical machine translation system based on Moses (Koehn et al., 2007). For each language pair, we compiled a language model based on the monolingual data for the target language, extracted a phrase table and a lexicalised reordering table from the bitext, and tuned the model weights using MERT (Och, 2003) based on a small tuning set extracted from the same bitext. The test sets used in the evaluation comprised 10 subtitles for each language pair, and relied on the intra-lingual alignments (described in the next section) to provide alternative reference translations. Table 4 details the BLEU scores for 18 language pairs. As we can observe from the table, the 201"
L16-1147,P12-3005,0,0.0124397,"ociated with each subtitle. This meta-data includes the following information: • Generic attributes of the source material, such as the release year, original language, duration, and genre of the movie or TV episode. These attributes are extracted from the IMDb database. • Attributes of the subtitle, such as the subtitle language, upload date, subtitle rating on OpenSubtitles (online user votes), and subtitle duration. • Probability that the specified language of the subtitle matches the actual language used in the subtitle text, based on the output of the langid language identification tool (Lui and Baldwin, 2012). • Features of the conversion process, such as the number of extracted sentences, total number of tokens, number of detected OCR errors and file encoding. 4 Cross-lingual alignments Once the subtitles files are processed, they can be aligned with one another to form a parallel corpus. To align subtitles across distinct languages, we first need to determine which subtitles to align, as many alternative subtitles may exist for a given movie / TV episode. Once the subtitle pairs are selected, the sentences are aligned one by one using a timing-based approach (Tiedemann, 2008). Document alignment"
L16-1147,P11-2093,0,0.0205682,"mines the likelihood of the new block being a continuation of the previous sentence (line 4). This likelihood is determined from various heuristics such as the time gap between the two subtitles and the presence of punctuation markers between the two – for instance, three dots at the end of the previous subtitle is sometimes used as marker for an unfinished sentence. The process is repeated for each block in the subtitles, resulting in a sequence of tokenised sentences coupled with timing information. For Japanese and Chinese, the KyTea4 word segmentation library is used for the tokenisation (Neubig et al., 2011) along with pre-trained models. For other languages, the default tokeniser script from Moses is employed, along with language-specific non-breaking prefixes. We used the OCR-free subtitles as gold standard for the evaluation. Based on the intra-lingual alignments described in Section 5, we examined all sentence pairs that were either identical or only differed in a few characters likely to be misspellings, and then calculated the precision and recall on their tokens. The results in Table 2 are provided for two correction methods: a “conservative” corrector which only corrects tokens that are h"
L16-1147,P03-1021,0,0.030516,"es. Alternative subtitles can also be fruitfully exploited to: • Detect errors (spelling mistakes, erroneous encodings, etc.) in the corpus; BLEU scores In order to empirically evaluate the quality of the alignments, we performed an extrinsic evaluation using the bitexts as training material for a statistical machine translation system based on Moses (Koehn et al., 2007). For each language pair, we compiled a language model based on the monolingual data for the target language, extracted a phrase table and a lexicalised reordering table from the bitext, and tuned the model weights using MERT (Och, 2003) based on a small tuning set extracted from the same bitext. The test sets used in the evaluation comprised 10 subtitles for each language pair, and relied on the intra-lingual alignments (described in the next section) to provide alternative reference translations. Table 4 details the BLEU scores for 18 language pairs. As we can observe from the table, the 2016 release is able to yield substantial improvements in terms of BLEU scores compared to the previous release. The BLEU scores remain nevertheless quite low for some language pairs. This is in no small part due to the fact that subtitle t"
L16-1147,tiedemann-2008-synchronizing,1,0.477922,"ication tool (Lui and Baldwin, 2012). • Features of the conversion process, such as the number of extracted sentences, total number of tokens, number of detected OCR errors and file encoding. 4 Cross-lingual alignments Once the subtitles files are processed, they can be aligned with one another to form a parallel corpus. To align subtitles across distinct languages, we first need to determine which subtitles to align, as many alternative subtitles may exist for a given movie / TV episode. Once the subtitle pairs are selected, the sentences are aligned one by one using a timing-based approach (Tiedemann, 2008). Document alignment Let A and B be two arbitrary languages and I be the IMDb identifier for a given source material (movie or TV episode). We define SI,A and SI,B to respectively represent the two sets of subtitles for I in the languages A and B. The first step in the alignment process is to score each pair of subtitles (s1 , s2 ) ∈ SI,A ×SI,B according to a handcrafted scoring function on the following features: • Upload date of the subtitle (since more recent subtitles are often corrections of previous ones); we compute a recency feature based on the date relative to the first and the lates"
L16-1147,tiedemann-2012-parallel,1,0.199854,"Missing"
L16-1147,L16-1559,1,0.779112,"0 28.65 20.51 • Discover insertions and deletions that may, among others, refer to extra-linguistic information; • Extract paraphrases and translation alternatives. The current procedure is based on the same time-based algorithm as for inter-lingual alignment, but includes a BLEU-filter and search heuristics over neighbouring links to improve the alignment quality. Additionally, we use string similarity metrics based on edit distance to distinguish between different alignment categories that refer to possible spelling errors, insertions or paraphrases. Details of the approach are presented in Tiedemann (2016) and will be omitted here. 6 Conclusion This paper described the release of an extended and improved version of the OpenSubtitles collection of parallel corpora. The subtitles included in this release are first preprocessed to convert the subtitle blocks into tokenised sentences. The converted subtitles are then aligned with one another via a time-based approach. This alignment is performed both across languages, but also within alternative subtitles for the same language. The corpora is made freely available to the research community on the OPUS website: http://opus.lingfil.uu.se/OpenSubtitle"
L16-1147,zhang-etal-2014-dual,0,0.0317054,"Missing"
L16-1559,tiedemann-2008-synchronizing,1,0.778716,"a file is an alternative upload of subtitles for the same video or whether it refers to another episode of the same TV series. This fact complicates the extraction of candidates of translation alternatives. We handle this problem by a brute-force approach, aligning all possible combinations but selecting only those that pass a certain overlap threshold, which is explained in the next section. Finally, it is common that user-contributed subtitles use slightly different timings when synchronising to the video. This issue is already adressed when aligning different translations with each other (Tiedemann, 2008) and we treat it in the same way when aligning alternative subtitles in the same language. 3518 lang punct ar 119,612 bg 214,588 bn 4 br bs 54,346 ca 31 cs 646,407 da 61,261 de 68,730 el 757,978 en 18,195,828 eo 39 es 1,771,127 et 35,160 eu 6 fa 34,753 fi 49,707 fr 1,089,621 gl 414 he 200,041 hi 2 hr 291,406 hu 195,125 id 23,180 it 80,085 ja 3,027 ko 288 lt 453 lv 35 mk 2,484 ml 156 ms 363 nl 511,291 no 16,370 pl 1,336,093 pt 398,114 pt br 987,801 ro 498,345 ru 35,807 si 432 sk 7,477 sl 303,897 sq 1,886 sr 562,258 sv 64,601 th 2,870 tl 9 tr 423,006 uk 229 vi 1,137 zh 140,126 zh tw 17,798 inser"
L16-1559,tiedemann-2012-parallel,1,0.837104,"able) to use these automatic evaluation methods with one reference translation only and scores obtained in this way may be quite misleading as many acceptable translation alternatives are not considered. Furthermore, reference-based parameter tuning is also effected by this limitation. Nevertheless, most research is reported with single-reference test/development sets as multiple translations are difficult to obtain. In this paper, we investigate the use of user-contributed movie subtitles as a source of alternative translations. We base our study on the OpenSubtitles corpus included in OPUS (Tiedemann, 2012), which provides a large number of subtitles in various languages including alternative uploads for many movies and TV shows. In the following, we first discuss the issues of user-uploaded subtitles that can lead to translation differences and after that we propose our method for filtering the data set that makes it possible to identify various categories of subtitle alternatives. As a side product we also obtain cross-lingual links between all subtitle files via the monolingually aligned subtitle alternatives that make it possible to create truly multilingual parallel corpora across all langu"
L16-1559,W06-2405,1,0.758804,"Missing"
L16-1559,L16-1147,1,0.89156,"Missing"
L16-1559,P02-1040,0,0.108716,"on Multilingual parallel corpora are useful resources for many purposes such as machine translation, cross-lingual studies, bilingual lexicography but also monolingual tasks such as word sense disambiguation and discovery (Dyvik, 2002) and the detection of idiomatic expressions (Villada Moir´on and Tiedemann, 2006). In most cases, they contain exactly one version per language, referring to either the source text or its translation. However, there are cases in which one would like to consider alternative translations, for example, when evaluating machine translation using metrics such as BLEU (Papineni et al., 2002). In general it is not satisfactory (and reliable) to use these automatic evaluation methods with one reference translation only and scores obtained in this way may be quite misleading as many acceptable translation alternatives are not considered. Furthermore, reference-based parameter tuning is also effected by this limitation. Nevertheless, most research is reported with single-reference test/development sets as multiple translations are difficult to obtain. In this paper, we investigate the use of user-contributed movie subtitles as a source of alternative translations. We base our study o"
L18-1275,D16-1102,0,0.0647221,"Missing"
L18-1275,2012.eamt-1.33,0,0.0739353,"Missing"
L18-1275,J93-2003,0,0.145696,"uation markers increases the likelihood of a good alignment. 3.1 Measures of alignment quality The first step towards building the rescoring model is to determine a measure of alignment quality that can be used 1744 as target variable. The approach used in this paper relies on extracting a sample of sentence pairs from the bilingual corpora, computing their lexical translation probabilities (in both directions) based on existing lexical translation tables and using these probabilities as a proxy for the alignment quality3 . More specifically, we rely on the expectation formula of IBM Model 1 (Brown et al., 1993) to compute the log-probabilities of the target sentence t given the source s and the source s given the target t: log P (s|t) = α log P (t|s) = α ls X log lt X j=1 i=0 lt X ls X j=1 log ! t(sj |ti ) (1) ! t(tj |si ) (2) i=0 scoreraw (s, t) = min log P (t|s) log P (s|t) , ls lt scorefinal (s, t) = scaleLs ,Lt (scoreraw (s, t)) Features Three families of features are extracted from the sentence alignments: • Features extracted at the level of sentence pairs, such as the ratio of sentence length (measured in number of tokens or characters) in the source and target, or the use of similar punctuat"
L18-1275,P07-2045,0,0.0145459,"p measure. Those alternative links are stored in separate alignment files and may be used to complement the selection of top-ranked subtitle pairs. Furthermore, intralingual links will also be offered again based on the procedures of (Tiedemann, 2016). 3 142 00:07:19,102 --&gt; 00:07:21,935 - l’energia rossa negativa. - Ah, quella mi piace. The sentences are segmented using language-specific heuristics based on punctuation markers, time gaps between blocks, and capitalisation. 3. Tokenisation: Once the sentences are segmented, they are tokenised, using either the tokenisation scripts from Moses (Koehn et al., 2007) or the Kytea toolkit for Chinese word segmentation (Neubig et al., 2011). 4. Correction of OCR errors: Some subtitles are extracted from video streams using OCR (Optical Character Recognition), generating a number of recognition errors. A noisy channel approach is presented in (Lison and Tiedemann, 2016) to correct these errors based on language models derived from the Google N-grams. This spellchecking model is also used here with some minor improvements to better handle e.g. accented characters and proper names. In total, more than 9 million tokens were corrected using this approach (with 4"
L18-1275,P09-5002,0,0.0574329,"Missing"
L18-1275,W17-5546,1,0.850192,"Missing"
L18-1275,L16-1147,1,0.865036,"a rossa negativa. - Ah, quella mi piace. The sentences are segmented using language-specific heuristics based on punctuation markers, time gaps between blocks, and capitalisation. 3. Tokenisation: Once the sentences are segmented, they are tokenised, using either the tokenisation scripts from Moses (Koehn et al., 2007) or the Kytea toolkit for Chinese word segmentation (Neubig et al., 2011). 4. Correction of OCR errors: Some subtitles are extracted from video streams using OCR (Optical Character Recognition), generating a number of recognition errors. A noisy channel approach is presented in (Lison and Tiedemann, 2016) to correct these errors based on language models derived from the Google N-grams. This spellchecking model is also used here with some minor improvements to better handle e.g. accented characters and proper names. In total, more than 9 million tokens were corrected using this approach (with 4 million tokens just for English). 5. Inclusion of meta-data: Finally, the subtitles are enriched with meta-data extracted from IMDb, providing details such as the film genre and the original (spoken) language of the movie or TV episode. The new release contains additional information such as the version"
L18-1275,P11-2093,0,0.0119326,"s and may be used to complement the selection of top-ranked subtitle pairs. Furthermore, intralingual links will also be offered again based on the procedures of (Tiedemann, 2016). 3 142 00:07:19,102 --&gt; 00:07:21,935 - l’energia rossa negativa. - Ah, quella mi piace. The sentences are segmented using language-specific heuristics based on punctuation markers, time gaps between blocks, and capitalisation. 3. Tokenisation: Once the sentences are segmented, they are tokenised, using either the tokenisation scripts from Moses (Koehn et al., 2007) or the Kytea toolkit for Chinese word segmentation (Neubig et al., 2011). 4. Correction of OCR errors: Some subtitles are extracted from video streams using OCR (Optical Character Recognition), generating a number of recognition errors. A noisy channel approach is presented in (Lison and Tiedemann, 2016) to correct these errors based on language models derived from the Google N-grams. This spellchecking model is also used here with some minor improvements to better handle e.g. accented characters and proper names. In total, more than 9 million tokens were corrected using this approach (with 4 million tokens just for English). 5. Inclusion of meta-data: Finally, th"
L18-1275,W17-4733,1,0.893096,"Missing"
L18-1275,C16-1157,0,0.0658201,"Missing"
L18-1275,C16-1069,0,0.0256754,"Missing"
L18-1275,P16-1162,0,0.314243,"Missing"
L18-1275,tiedemann-2008-synchronizing,1,0.904906,"ed to train dialogue agents (Vinyals and Le, 2015; Lison and Bibauw, 2017). Parallel corpora derived from subtitles have a number of benefits. The first one is their size: the OpenSubtitles dataset is (to the best of our knowledge) the world’s largest open collection of parallel corpora. The latest release, which is presented in this paper, contains no less than 3.4 billion sentences (amounting to 22.2 billion tokens) spread over 60 languages and a total of 1782 language pairs. As subtitles are annotated with timestamps, they can also be efficiently synchronised using a linear-time algorithm (Tiedemann, 2008). Finally, their conversational nature make them ideal for exploring dialogue phenomena and properties of everyday language (Paetzold and Specia, 2016; van der Wees et al., 2016). However, the extraction of parallel corpora from subtitles must also face some challenges. One difficulty stems from the fact that subtitles are typicall not direct translations of one another. Rather, they should better be viewed as boileddown transcriptions of the same conversations across several languages. Subtitles will inevitably differ in how they “compress” the conversations, notably due to structural diverge"
L18-1275,L16-1559,1,0.884512,"ence boundary. To further improve the synchronisation, we use lexical cues to estimate offset and speed parameters using bilingual dictionaries extracted from word-aligned subtitles (Tiedemann, 2008). In contrast to our earlier releases, we now also keep alignments between alternative subtitle files besides the ones that show the best match according to an overlap measure. Those alternative links are stored in separate alignment files and may be used to complement the selection of top-ranked subtitle pairs. Furthermore, intralingual links will also be offered again based on the procedures of (Tiedemann, 2016). 3 142 00:07:19,102 --&gt; 00:07:21,935 - l’energia rossa negativa. - Ah, quella mi piace. The sentences are segmented using language-specific heuristics based on punctuation markers, time gaps between blocks, and capitalisation. 3. Tokenisation: Once the sentences are segmented, they are tokenised, using either the tokenisation scripts from Moses (Koehn et al., 2007) or the Kytea toolkit for Chinese word segmentation (Neubig et al., 2011). 4. Correction of OCR errors: Some subtitles are extracted from video streams using OCR (Optical Character Recognition), generating a number of recognition er"
L18-1275,C16-1242,0,0.064309,"Missing"
P06-2111,P98-2127,0,0.913685,"eke van der Plas & J¨org Tiedemann Alfa-Informatica University of Groningen P.O. Box 716 9700 AS Groningen The Netherlands {vdplas,tiedeman}@let.rug.nl Abstract that similar words share similar contexts. Systems based on distributional similarity provide ranked lists of semantically related words according to the similarity of their contexts. Synonyms are expected to be among the highest ranks followed by (co)hyponyms and hypernyms, since the highest degree of semantic relatedness next to identity is synonymy. However, this is not always the case. Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words. They use grammatical relations1 to determine the context of a target word. We will refer to such systems as monolingual syntax-based systems. These systems have proven to be quite successful at finding semantically related words. However, they do not make a clear distinction between synonyms on the one hand and related words such as antonyms, (co)hyponyms, hypernyms etc. on the other hand. In this paper we have defined context in a multilingual setting. In particular, translations of"
P06-2111,P01-1008,0,0.0804512,"condly, the authors use a parallel corpus that is bilingual whereas we use a multilingual corpus containing 11 languages in total. The authors show that the bilingual method outperforms the monolingual methods. However a combination of different methods leads to the best performance. 3 Methodology 2 Related Work 3.1 Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). Several authors have used a monolingual parallel corpus to find paraphrases (Ibrahim et al. (2003), Barzilay and McKeown (2001)). However, bilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as target word selection (Dagan et al., 1991) and separation of senses (Dyvik, 1998). The latter work derives relations such as synonymy and hyponymy from the separated senses by applying the method of semantic mirrors. Turney (2001) reports on an PMI and IR driven approach that acquires data by querying a Web search engine. He evaluates on the TOEFL test in which the system has to select the synonym among 4 candidates. Lin et al. (2003) try to tackle the problem of identifying syno"
P06-2111,tiedemann-nygaard-2004-opus,1,0.859148,"Missing"
P06-2111,J93-2003,0,0.0171928,"Missing"
P06-2111,P89-1010,0,0.168852,"bout their semantic similarity than the fact that two nouns both occur as object of squeeze. To account for this intuition, the frequency of occurrence in a vector can be replaced by a weighted score. The weighted score is an indication of the amount of information carried by that particular combination of a noun and its feature. We believe that this type of weighting is beneficial for calculating similarity between word alignment vectors as well. Word alignments that are shared by many different words are most probably mismatches. For this experiment we used Pointwise Mutual Information (I) (Church and Hanks, 1989). I(W, f ) = log 4 Evaluation Framework P (W, f ) P (W )P (f ) In the following, we describe the data used and measures applied. The evaluation method that is most suitable for testing with multiple settings is one that uses an available resource for synonyms as a gold standard. In our experiments we apply automatic evaluation using an existing hand-crafted synonym database, Dutch EuroWordnet (EWN, Vossen (1998)). In EWN, one synset consists of several synonyms which represent a single sense. Polysemous words occur in several synsets. We have combined for each target word the EWN synsets in wh"
P06-2111,W02-0908,0,0.416741,"ibutional Similarity Lonneke van der Plas & J¨org Tiedemann Alfa-Informatica University of Groningen P.O. Box 716 9700 AS Groningen The Netherlands {vdplas,tiedeman}@let.rug.nl Abstract that similar words share similar contexts. Systems based on distributional similarity provide ranked lists of semantically related words according to the similarity of their contexts. Synonyms are expected to be among the highest ranks followed by (co)hyponyms and hypernyms, since the highest degree of semantic relatedness next to identity is synonymy. However, this is not always the case. Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words. They use grammatical relations1 to determine the context of a target word. We will refer to such systems as monolingual syntax-based systems. These systems have proven to be quite successful at finding semantically related words. However, they do not make a clear distinction between synonyms on the one hand and related words such as antonyms, (co)hyponyms, hypernyms etc. on the other hand. In this paper we have defined context in a multilingual setting. In particular, tra"
P06-2111,P91-1017,0,0.0492041,"method outperforms the monolingual methods. However a combination of different methods leads to the best performance. 3 Methodology 2 Related Work 3.1 Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). Several authors have used a monolingual parallel corpus to find paraphrases (Ibrahim et al. (2003), Barzilay and McKeown (2001)). However, bilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as target word selection (Dagan et al., 1991) and separation of senses (Dyvik, 1998). The latter work derives relations such as synonymy and hyponymy from the separated senses by applying the method of semantic mirrors. Turney (2001) reports on an PMI and IR driven approach that acquires data by querying a Web search engine. He evaluates on the TOEFL test in which the system has to select the synonym among 4 candidates. Lin et al. (2003) try to tackle the problem of identifying synonyms among distributionally related words in two ways: Firstly, by looking at the overlap in translations of semantically similar words in multiple bilingual"
P06-2111,W03-1610,0,0.641533,"Missing"
P06-2111,W03-1608,0,0.0224368,"d in the dictionary. Secondly, the authors use a parallel corpus that is bilingual whereas we use a multilingual corpus containing 11 languages in total. The authors show that the bilingual method outperforms the monolingual methods. However a combination of different methods leads to the best performance. 3 Methodology 2 Related Work 3.1 Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). Several authors have used a monolingual parallel corpus to find paraphrases (Ibrahim et al. (2003), Barzilay and McKeown (2001)). However, bilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as target word selection (Dagan et al., 1991) and separation of senses (Dyvik, 1998). The latter work derives relations such as synonymy and hyponymy from the separated senses by applying the method of semantic mirrors. Turney (2001) reports on an PMI and IR driven approach that acquires data by querying a Web search engine. He evaluates on the TOEFL test in which the system has to select the synonym among 4 candidates. Lin et al. (2003) try to tackle th"
P06-2111,kilgarriff-yallop-2000-whats,0,0.0157336,"rds. In this paper we report on our findings trying to automatically acquire synonyms for Dutch using two different resources, a large monolingual corpus and a multilingual parallel corpus including 11 languages. A common approach to the automatic extraction of semantically related words is to use distributional similarity. The basic idea behind this is 1 One can define the context of a word in a non-syntactic monolingual way, that is as the document in which it occurs or the n words surrounding it. From experiments we have done and also building on the observations made by other researchers (Kilgarriff and Yallop, 2000) we can state that this approach generates a type of semantic similarity that is of a looser kind, an associative kind,for example doctor and disease. These words are typically not good candidates for synonymy. 866 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 866–873, c Sydney, July 2006. 2006 Association for Computational Linguistics imated by automatic word alignment. We will refer to this approach as multilingual alignmentbased approaches. We expect that these translations will give us synonyms and less semantically related words, because translations typically"
P06-2111,J90-1003,0,\N,Missing
P06-2111,C98-2122,0,\N,Missing
P12-2059,J93-2003,0,0.0293793,"rs character bigrams character trigrams words Macedonian 99 1,851 13,794 41,816 Bulgarian 101 1,893 14,305 30,927 Table 2: Vocabulary size of character-level alignment models and the corresponding word-level model. It turns out that bigrams constitute a good compromise between generality and contextual specificity, which yields useful character alignments with good performance in terms of phrase-based translation. In our experiments, we used GIZA++ (Och and Ney, 2003) with standard settings and the growdiagonal-final-and heuristics to symmetrize the final IBM-model-4-based Viterbi alignments (Brown et al., 1993). The phrases were extracted and scored using the Moses training tools (Koehn et al., 2007).1 We tuned the parameters of the log-linear SMT model using minimum error rate training (Och, 2003), optimizing BLEU (Papineni et al., 2002). 1 Note that the extracted phrase table does not include sequences of character n-grams. We map character n-gram alignments to links between single characters before extraction. 302 3 Transliteration We also built a character-level SMT system for word-level transliteration, which we trained on a list of automatically extracted pairs of likely cognates. 3.1 Cognate"
P12-2059,N06-1003,0,0.0452181,"cted pairs of likely cognates. 3.1 Cognate Extraction Classic NLP approaches to cognate extraction look for words with similar spelling that co-occur in parallel sentences (Kondrak et al., 2003). Since our Macedonian-Bulgarian bitext (MK–BG) was small, we further used a MK–EN and an EN–BG bitext. First, we induced IBM-model-4 word alignments for MK–EN and EN–BG, from which we extracted four conditional lexical translation probabilities: Pr(m|e) and Pr(e|m) for MK–EN, and Pr(b|e) and Pr(e|b) for EN–BG, where m, e, and b stand for a Macedonian, an English, and a Bulgarian word. Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007; Utiyama and Isahara, 2007), we induced conditional P lexical translation probabilities as Pr(m|b) = e Pr(m|e) Pr(e|b), where Pr(m|e) and Pr(e|b) are estimated using maximum likelihood from MK–EN and EN–BG word alignments. Then, we induced translation probability estimations for the reverse direction Pr(b|m) and we calculated the quantity Piv(m, b) = Pr(m|b) Pr(b|m). We calculated a similar quantity Dir(m, b), where the probabilities Pr(m|b) and Pr(b|m) are estimated using maximum likelihood from the MK–BG bitext directly. Finally, we calculated the similarity score S(m, b)"
P12-2059,P05-1066,0,0.106808,"Missing"
P12-2059,I08-8003,0,0.0091741,"like vreden, which means ‘valuable’ in Macedonian but ‘harmful’ in Bulgarian. Finally, competitive linking helps prevent issues related to word inflection that cannot be handled using the semantic component alone. 3.2 Transliteration Training For each pair in the list of cognate pairs, we added spaces between any two adjacent letters for both words, and we further appended special start and end characters. We split the resulting list into training, development and testing parts and we trained and tuned a character-level MacedonianBulgarian phrase-based monotone SMT system similar to that in (Finch and Sumita, 2008; Tiedemann and Nabende, 2009; Nakov and Ng, 2009; Nakov and Ng, 2012). The system used a character-level Bulgarian language model trained on words. We set the maximum phrase length and the language model order to 10, and we tuned the system using MERT. 3.3 Transliteration Lattice Generation Given a Macedonian sentence, we generated a lattice where each input Macedonian word of length three or longer was augmented with Bulgarian alternatives: n-best transliterations generated by the above character-level Macedonian-Bulgarian SMT system (after the characters were concatenated to form a word and"
P12-2059,N07-1047,0,0.0292559,"s in each tuning step to calculate the usual word-based BLEU score. character bigrams: MK: na av vi is st ti in na a ? ? BG: na ai is st ti in na a ? ? Figure 1: Preparing the training corpus for alignment. Statistical word alignment models heavily rely on context-independent lexical translation parameters and, therefore, are unable to properly distinguish character mapping differences in various contexts. The alignment models used in the transliteration literature have the same problem as they are usually based on edit distance operations and finite-state automata without contextual history (Jiampojamarn et al., 2007; Damper et al., 2005; Ristad and Yianilos, 1998). We, thus, transformed the input to sequences of character n-grams as suggested by Tiedemann (2012); examples are shown in Figure 1. This artificially increases the vocabulary as shown in Table 2, making standard alignment models and their lexical translation parameters more expressive. single characters character bigrams character trigrams words Macedonian 99 1,851 13,794 41,816 Bulgarian 101 1,893 14,305 30,927 Table 2: Vocabulary size of character-level alignment models and the corresponding word-level model. It turns out that bigrams consti"
P12-2059,N03-1017,0,0.0578816,"be different, and they may diverge more substantially at the level of morphology. However, the differences often constitute consistent regularities that can be generalized when translating. The language similarities and the regularities in morphological variation and spelling motivate the use of character-level translation models, which were applied to translation (Vilar et al., 2007; Tiedemann, 2009a) and transliteration (Matthews, 2007). Certainly, translation cannot be adequately modeled as simple transliteration, even for closelyrelated languages. However, the strength of phrasebased SMT (Koehn et al., 2003) is that it can support rather large sequences (phrases) that capture translations of entire chunks. This makes it possible to include mappings that go far beyond the edit-distancebased string operations usually modeled in transliteration. Table 1 shows how character-level phrase tables can cover mappings spanning over multi-word units. Thus, character-level phrase-based SMT models combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, as well as to various combinations thereof. 2 Training"
P12-2059,P07-2045,0,0.021785,"01 1,893 14,305 30,927 Table 2: Vocabulary size of character-level alignment models and the corresponding word-level model. It turns out that bigrams constitute a good compromise between generality and contextual specificity, which yields useful character alignments with good performance in terms of phrase-based translation. In our experiments, we used GIZA++ (Och and Ney, 2003) with standard settings and the growdiagonal-final-and heuristics to symmetrize the final IBM-model-4-based Viterbi alignments (Brown et al., 1993). The phrases were extracted and scored using the Moses training tools (Koehn et al., 2007).1 We tuned the parameters of the log-linear SMT model using minimum error rate training (Och, 2003), optimizing BLEU (Papineni et al., 2002). 1 Note that the extracted phrase table does not include sequences of character n-grams. We map character n-gram alignments to links between single characters before extraction. 302 3 Transliteration We also built a character-level SMT system for word-level transliteration, which we trained on a list of automatically extracted pairs of likely cognates. 3.1 Cognate Extraction Classic NLP approaches to cognate extraction look for words with similar spellin"
P12-2059,N03-2016,0,0.052787,"inear SMT model using minimum error rate training (Och, 2003), optimizing BLEU (Papineni et al., 2002). 1 Note that the extracted phrase table does not include sequences of character n-grams. We map character n-gram alignments to links between single characters before extraction. 302 3 Transliteration We also built a character-level SMT system for word-level transliteration, which we trained on a list of automatically extracted pairs of likely cognates. 3.1 Cognate Extraction Classic NLP approaches to cognate extraction look for words with similar spelling that co-occur in parallel sentences (Kondrak et al., 2003). Since our Macedonian-Bulgarian bitext (MK–BG) was small, we further used a MK–EN and an EN–BG bitext. First, we induced IBM-model-4 word alignments for MK–EN and EN–BG, from which we extracted four conditional lexical translation probabilities: Pr(m|e) and Pr(e|m) for MK–EN, and Pr(b|e) and Pr(e|b) for EN–BG, where m, e, and b stand for a Macedonian, an English, and a Bulgarian word. Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007; Utiyama and Isahara, 2007), we induced conditional P lexical translation probabilities as Pr(m|b) = e Pr(m|e) Pr(e|b), where Pr(m|e) and Pr(e|b) a"
P12-2059,J00-2004,0,0.0232873,"give equal weights to the translational similarity (the sum of the first two terms) and to the spelling similarity (twice LCSR). We excluded all words of length less than three, as well as all Macedonian-Bulgarian word pairs (m, b) for which Piv(m, b) + Dir(m, b) < 0.01, and those for which LCSR(m, b) was below 0.58, a value found by Kondrak et al. (2003) to work well for a number of European language pairs. Finally, using S(m, b), we induced a weighted bipartite graph, and we performed a greedy approximation to the maximum weighted bipartite matching in that graph using competitive linking (Melamed, 2000), to produce the final list of cognate pairs. Note that the above-described cognate extraction algorithm has three important components: (1) orthographic, based on LCSR, (2) semantic, based on word alignments and pivoting over English, and (3) competitive linking. The orthographic component is essential when looking for cognates since they must have similar spelling by definition, while the semantic component prevents the extraction of false friends like vreden, which means ‘valuable’ in Macedonian but ‘harmful’ in Bulgarian. Finally, competitive linking helps prevent issues related to word in"
P12-2059,D09-1141,1,0.654294,"t ‘harmful’ in Bulgarian. Finally, competitive linking helps prevent issues related to word inflection that cannot be handled using the semantic component alone. 3.2 Transliteration Training For each pair in the list of cognate pairs, we added spaces between any two adjacent letters for both words, and we further appended special start and end characters. We split the resulting list into training, development and testing parts and we trained and tuned a character-level MacedonianBulgarian phrase-based monotone SMT system similar to that in (Finch and Sumita, 2008; Tiedemann and Nabende, 2009; Nakov and Ng, 2009; Nakov and Ng, 2012). The system used a character-level Bulgarian language model trained on words. We set the maximum phrase length and the language model order to 10, and we tuned the system using MERT. 3.3 Transliteration Lattice Generation Given a Macedonian sentence, we generated a lattice where each input Macedonian word of length three or longer was augmented with Bulgarian alternatives: n-best transliterations generated by the above character-level Macedonian-Bulgarian SMT system (after the characters were concatenated to form a word and the special symbols were removed). 303 In the la"
P12-2059,J03-1002,0,0.0216691,"ses the vocabulary as shown in Table 2, making standard alignment models and their lexical translation parameters more expressive. single characters character bigrams character trigrams words Macedonian 99 1,851 13,794 41,816 Bulgarian 101 1,893 14,305 30,927 Table 2: Vocabulary size of character-level alignment models and the corresponding word-level model. It turns out that bigrams constitute a good compromise between generality and contextual specificity, which yields useful character alignments with good performance in terms of phrase-based translation. In our experiments, we used GIZA++ (Och and Ney, 2003) with standard settings and the growdiagonal-final-and heuristics to symmetrize the final IBM-model-4-based Viterbi alignments (Brown et al., 1993). The phrases were extracted and scored using the Moses training tools (Koehn et al., 2007).1 We tuned the parameters of the log-linear SMT model using minimum error rate training (Och, 2003), optimizing BLEU (Papineni et al., 2002). 1 Note that the extracted phrase table does not include sequences of character n-grams. We map character n-gram alignments to links between single characters before extraction. 302 3 Transliteration We also built a char"
P12-2059,P03-1021,0,0.0418781,"d-level model. It turns out that bigrams constitute a good compromise between generality and contextual specificity, which yields useful character alignments with good performance in terms of phrase-based translation. In our experiments, we used GIZA++ (Och and Ney, 2003) with standard settings and the growdiagonal-final-and heuristics to symmetrize the final IBM-model-4-based Viterbi alignments (Brown et al., 1993). The phrases were extracted and scored using the Moses training tools (Koehn et al., 2007).1 We tuned the parameters of the log-linear SMT model using minimum error rate training (Och, 2003), optimizing BLEU (Papineni et al., 2002). 1 Note that the extracted phrase table does not include sequences of character n-grams. We map character n-gram alignments to links between single characters before extraction. 302 3 Transliteration We also built a character-level SMT system for word-level transliteration, which we trained on a list of automatically extracted pairs of likely cognates. 3.1 Cognate Extraction Classic NLP approaches to cognate extraction look for words with similar spelling that co-occur in parallel sentences (Kondrak et al., 2003). Since our Macedonian-Bulgarian bitext"
P12-2059,P02-1040,0,0.0953179,"hat bigrams constitute a good compromise between generality and contextual specificity, which yields useful character alignments with good performance in terms of phrase-based translation. In our experiments, we used GIZA++ (Och and Ney, 2003) with standard settings and the growdiagonal-final-and heuristics to symmetrize the final IBM-model-4-based Viterbi alignments (Brown et al., 1993). The phrases were extracted and scored using the Moses training tools (Koehn et al., 2007).1 We tuned the parameters of the log-linear SMT model using minimum error rate training (Och, 2003), optimizing BLEU (Papineni et al., 2002). 1 Note that the extracted phrase table does not include sequences of character n-grams. We map character n-gram alignments to links between single characters before extraction. 302 3 Transliteration We also built a character-level SMT system for word-level transliteration, which we trained on a list of automatically extracted pairs of likely cognates. 3.1 Cognate Extraction Classic NLP approaches to cognate extraction look for words with similar spelling that co-occur in parallel sentences (Kondrak et al., 2003). Since our Macedonian-Bulgarian bitext (MK–BG) was small, we further used a MK–E"
P12-2059,2006.amta-papers.25,0,0.137489,"Missing"
P12-2059,2009.eamt-1.3,1,0.799268,"vel. Closely-related languages such as Macedonian and Bulgarian exhibit a large overlap in their vocabulary and strong syntactic and lexical similarities. Spelling conventions in such related languages can still be different, and they may diverge more substantially at the level of morphology. However, the differences often constitute consistent regularities that can be generalized when translating. The language similarities and the regularities in morphological variation and spelling motivate the use of character-level translation models, which were applied to translation (Vilar et al., 2007; Tiedemann, 2009a) and transliteration (Matthews, 2007). Certainly, translation cannot be adequately modeled as simple transliteration, even for closelyrelated languages. However, the strength of phrasebased SMT (Koehn et al., 2003) is that it can support rather large sequences (phrases) that capture translations of entire chunks. This makes it possible to include mappings that go far beyond the edit-distancebased string operations usually modeled in transliteration. Table 1 shows how character-level phrase tables can cover mappings spanning over multi-word units. Thus, character-level phrase-based SMT models"
P12-2059,E12-1015,1,0.649421,"1: Preparing the training corpus for alignment. Statistical word alignment models heavily rely on context-independent lexical translation parameters and, therefore, are unable to properly distinguish character mapping differences in various contexts. The alignment models used in the transliteration literature have the same problem as they are usually based on edit distance operations and finite-state automata without contextual history (Jiampojamarn et al., 2007; Damper et al., 2005; Ristad and Yianilos, 1998). We, thus, transformed the input to sequences of character n-grams as suggested by Tiedemann (2012); examples are shown in Figure 1. This artificially increases the vocabulary as shown in Table 2, making standard alignment models and their lexical translation parameters more expressive. single characters character bigrams character trigrams words Macedonian 99 1,851 13,794 41,816 Bulgarian 101 1,893 14,305 30,927 Table 2: Vocabulary size of character-level alignment models and the corresponding word-level model. It turns out that bigrams constitute a good compromise between generality and contextual specificity, which yields useful character alignments with good performance in terms of phra"
P12-2059,N07-1061,0,0.059458,"ction Classic NLP approaches to cognate extraction look for words with similar spelling that co-occur in parallel sentences (Kondrak et al., 2003). Since our Macedonian-Bulgarian bitext (MK–BG) was small, we further used a MK–EN and an EN–BG bitext. First, we induced IBM-model-4 word alignments for MK–EN and EN–BG, from which we extracted four conditional lexical translation probabilities: Pr(m|e) and Pr(e|m) for MK–EN, and Pr(b|e) and Pr(e|b) for EN–BG, where m, e, and b stand for a Macedonian, an English, and a Bulgarian word. Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007; Utiyama and Isahara, 2007), we induced conditional P lexical translation probabilities as Pr(m|b) = e Pr(m|e) Pr(e|b), where Pr(m|e) and Pr(e|b) are estimated using maximum likelihood from MK–EN and EN–BG word alignments. Then, we induced translation probability estimations for the reverse direction Pr(b|m) and we calculated the quantity Piv(m, b) = Pr(m|b) Pr(b|m). We calculated a similar quantity Dir(m, b), where the probabilities Pr(m|b) and Pr(b|m) are estimated using maximum likelihood from the MK–BG bitext directly. Finally, we calculated the similarity score S(m, b) = Piv(m, b)+Dir(m, b)+2×LCSR(m, b), where LCSR"
P12-2059,W07-0705,0,0.507999,"ns below the word level. Closely-related languages such as Macedonian and Bulgarian exhibit a large overlap in their vocabulary and strong syntactic and lexical similarities. Spelling conventions in such related languages can still be different, and they may diverge more substantially at the level of morphology. However, the differences often constitute consistent regularities that can be generalized when translating. The language similarities and the regularities in morphological variation and spelling motivate the use of character-level translation models, which were applied to translation (Vilar et al., 2007; Tiedemann, 2009a) and transliteration (Matthews, 2007). Certainly, translation cannot be adequately modeled as simple transliteration, even for closelyrelated languages. However, the strength of phrasebased SMT (Koehn et al., 2003) is that it can support rather large sequences (phrases) that capture translations of entire chunks. This makes it possible to include mappings that go far beyond the edit-distancebased string operations usually modeled in transliteration. Table 1 shows how character-level phrase tables can cover mappings spanning over multi-word units. Thus, character-level phrase"
P12-2059,P07-1108,0,0.0492959,". 3.1 Cognate Extraction Classic NLP approaches to cognate extraction look for words with similar spelling that co-occur in parallel sentences (Kondrak et al., 2003). Since our Macedonian-Bulgarian bitext (MK–BG) was small, we further used a MK–EN and an EN–BG bitext. First, we induced IBM-model-4 word alignments for MK–EN and EN–BG, from which we extracted four conditional lexical translation probabilities: Pr(m|e) and Pr(e|m) for MK–EN, and Pr(b|e) and Pr(e|b) for EN–BG, where m, e, and b stand for a Macedonian, an English, and a Bulgarian word. Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007; Utiyama and Isahara, 2007), we induced conditional P lexical translation probabilities as Pr(m|b) = e Pr(m|e) Pr(e|b), where Pr(m|e) and Pr(e|b) are estimated using maximum likelihood from MK–EN and EN–BG word alignments. Then, we induced translation probability estimations for the reverse direction Pr(b|m) and we calculated the quantity Piv(m, b) = Pr(m|b) Pr(b|m). We calculated a similar quantity Dir(m, b), where the probabilities Pr(m|b) and Pr(b|m) are estimated using maximum likelihood from the MK–BG bitext directly. Finally, we calculated the similarity score S(m, b) = Piv(m, b)+Dir(m,"
P12-3008,2009.mtsummit-posters.6,0,0.0380551,"ect is the adaptation of the Moses toolkit to fit into the rapid training, updating, and interactive access environment of the LetsMT! platform. The Moses SMT toolkit (Koehn et al., 2007) provides a complete statistical translation system distributed under the LGPL license. Moses includes all of the components needed to preprocess data and to train language and translation models. Moses is widely used in the research community and has also reached the commercial sector. While the use of the software is not closely monitored, Moses is known to be in commercial use by companies such as Systran (Dugast et al., 2009), Asia Online, Autodesk (Plitt and Masselot, 2010), Matrixware 2 , Adobe, Pangeanic, Logrus 3 , and Applied Language Solutions (Way et al., 2011). The SMT training pipeline implemented in Moses involves a number of steps that each require a separate program to run. In the framework of 2 Machine Translation at Matrixware: http://ir-facility.net/ downloads/mxw_factsheet_smt_200910.pdf 3 TDA Members doing business with Moses: http://www.tausdata.org/blog/2010/10/doing-business-withmoses-open-source-translation/ LetsMT!, this process is streamlined and made automatically configurable given a set o"
P12-3008,D07-1090,0,0.0299911,"process is streamlined and made automatically configurable given a set of userspecified variables (training corpora, language model data, tuning sets). SMT training is automated using the Moses experiment management system (Koehn, 2010). Other improvements of Moses, implemented by the University of Edinburgh as part of LetsMT! project, are:  the incremental training of SMT models (Levenberg et al., 2010);  randomised language models (Levenberg et al., 2009);  a server mode version of the Moses decoder and multithreaded decoding;  multiple translation models;  distributed language models (Brants et al., 2007). Many improvements in the Moses experiment management system were implemented to speed up SMT system training and to use the full potential of the HPC cluster. We revised and improved Moses training routines (i) by finding tasks that are executed sequentially but can be executed in parallel and (ii) by splitting big training tasks into smaller ones and executing them in parallel. 4 The application logic layer contains a set of modules responsible for the main functionality and logic of the system. It receives queries and commands from the interface layer and prepares answers or performs tasks"
P12-3008,J93-1004,0,0.30674,"identification tools with pre-trained moFigure 3. Resource repository overview dels for over 60 languages. requirements in terms of flexibility and efficiency. Furthermore, our system also includes tools for In particular, we use the table mode of TokyoCabinet that supports storage of arbitrary automatic sentence alignment. Import processes data records connected to a single key in the automatically align translated documents with each database. We use resource URL’s in our repository other using standard length-based sentence to define unique keys in the database, and data alignment methods (Gale and Church, 1993; Varga records attached to these keys may include any et al., 2005). Finally, we also integrated a general batchnumber of key-value pairs. In this way, we can add any kind of information to each addressable queuing system (SGE) to run off-line processes resource in the RR. The software also supports such as import jobs. In this way, we further keys with unordered lists of values, which is useful increase the scalability of the system by taking the for metadata such as languages (in a data load off repository servers. Data uploads collection) and for many other purposes. automatically trigger"
P12-3008,P07-2045,0,0.00811374,"Missing"
P12-3008,D09-1079,0,0.0369973,"Missing"
P12-3008,2011.eamt-1.7,1,0.864098,"latform for data sharing and MT building. The goal of the LetsMT! project is to facilitate the use of open source SMT tools and to involve users in the collection of training data. The LetsMT! project extends the use of existing stateof-the-art SMT methods by providing them as cloud-based services. An easy-to-use web interface empowers users to participate in data collection and MT customisation to increase the quality, domain coverage, and usage of MT. The LetsMT! project partners are companies TILDE (coordinator), Moravia, and SemLab, and Zagreb, LetsMT! Key Features The LetsMT! platform 1 (Vasiļjevs et al., 2011) gathers public and user-provided MT training data and enables generation of multiple MT systems by combining and prioritising this data. Users can upload their parallel corpora to an online repository and generate user-tailored SMT systems based on data selected by the user. Authenticated users with appropriate permissions can also store private corpora that can be seen and used only by this user (or a designated user group). All data uploaded into the LetsMT! repository is kept in internal format, and only its metadata is provided to the user. Data cannot be downloaded or accessed for readin"
P12-3008,N10-1062,0,\N,Missing
P12-3008,D09-1000,0,\N,Missing
P13-4033,W09-2404,0,0.256094,"på (particular attentive on) + Simplified expression Table 2: Example translation snippets with comments Feature Baseline TTR OVIX QW QP All BLEU 0.243 0.243 0.243 0.242 0.243 0.235 OVIX 56.88 55.25 54.65 57.16 57.07 47.80 LIX 51.17 51.04 51.00 51.16 51.06 49.29 Table 1: Results for adding single lexical consistency features to Docent To evaluate our system we used the BLEU score (Papineni et al., 2002) together with a set of readability metrics, since readability is what we hoped to improve by adding consistency features. Here we used OVIX to confirm a direct impact on consistency, and LIX (Björnsson, 1968), which is a common readability measure for Swedish. Unfortunately we do not have access to simplified translated text, so we calculate the MT metrics against a standard reference, which means that simple texts will likely have worse scores than complicated texts closer to the reference translation. We tuned the standard features using Moses and MERT, and then added each lexical consistency feature with a small weight, using a grid search approach to find values with a small impact. The results are shown in Table 1. As can be seen, for individual features the translation quality was maintained"
P13-4033,D12-1026,0,0.0267005,"se and published on Github1 to make it easy for other researchers to use it in their own experiments. Motivation 2 Most of the research on statistical machine translation (SMT) that was conducted during the last 20 years treated every text as a “bag of sentences” and disregarded all relations between elements in different sentences. Systematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2"
P13-4033,E12-3001,0,0.0603375,"the GNU General Public License and published on Github1 to make it easy for other researchers to use it in their own experiments. Motivation 2 Most of the research on statistical machine translation (SMT) that was conducted during the last 20 years treated every text as a “bag of sentences” and disregarded all relations between elements in different sentences. Systematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based S"
P13-4033,2010.iwslt-papers.10,1,0.855274,"s. The code is released under the GNU General Public License and published on Github1 to make it easy for other researchers to use it in their own experiments. Motivation 2 Most of the research on statistical machine translation (SMT) that was conducted during the last 20 years treated every text as a “bag of sentences” and disregarded all relations between elements in different sentences. Systematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-leve"
P13-4033,D12-1108,1,0.812834,"(Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2012). Our decoding algorithm is based on local search instead of dynamic programming and permits the integration of Document-Level Decoding with Local Search Our decoder is based on the phrase-based SMT model described by Koehn et al. (2003) and implemented, for example, in the popular Moses decoder (Koehn et al., 2007). Translation is performed by splitting the input sentence into a number of contiguous word sequences, called phrases, which are translated into the target language through a phrase dictionary lookup and optionally reordered. The choice between different translations of an ambiguous"
P13-4033,W11-2123,0,0.0578591,"e models that makes it possible to build a baseline system with a configuration comparable to that of a typical Moses baseline system. The published source code also includes prototype implementations of a few document-level models. These models should be considered work in progress and serve as a demonstration of the cross-sentence modelling capabilities of the decoder. They have not yet reached a state of maturity that would make them suitable for production use. The sentence-level models provided by Docent include the phrase table, n-gram language models implemented with the KenLM toolkit (Heafield, 2011), an unlexicalised distortion cost model with geometric decay (Koehn et al., 2003) and a word penalty cost. All of these features are designed to be compatible with the corresponding features in Moses. From among the typical set of baseline features in Moses, we have not implemented the lexicalised distortion model, but this model could easily be added if required. Docent uses the same binary file format for phrase tables as Moses, so the same training apparatus can be used. DP-based SMT decoders have a parameter called distortion limit that limits the difference in word order between the inpu"
P13-4033,D11-1125,0,0.0200806,"iscourselevel corpus annotations such as coreference links. 195 These annotations can then be accessed by the feature models. To allow for additional targetlanguage information such as morphological features of target words, Docent can handle simple word-level annotations that are encoded in the phrase table in the same way as target language factors in Moses. In order to optimise feature weights we have adapted the Moses tuning infrastructure to Docent. In this way we can take advantage of all its features, for instance using different optimisation algorithms such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), and selective tuning of a subset of features. Since document features only give meaningful scores on the document level and not on the sentence level, we naturally perform optimisation on document level, which typically means that we need more data than for the optimisation of sentence-based decoding. The results we obtain are relatively stable and competitive with sentence-level optimisation of the same models (Stymne et al., 2013a). 4 Implementing Feature Models Efficiently While translating a document, the local search decoder attempts to make a great number of moves. For each move, a sco"
P13-4033,N03-1017,0,0.011021,"ystematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2012). Our decoding algorithm is based on local search instead of dynamic programming and permits the integration of Document-Level Decoding with Local Search Our decoder is based on the phrase-based SMT model described by Koehn et al. (2003) and implemented, for example, in the popular Moses decoder (Koehn et al., 2007). Translation is"
P13-4033,P07-2045,0,0.0251314,"ed SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2012). Our decoding algorithm is based on local search instead of dynamic programming and permits the integration of Document-Level Decoding with Local Search Our decoder is based on the phrase-based SMT model described by Koehn et al. (2003) and implemented, for example, in the popular Moses decoder (Koehn et al., 2007). Translation is performed by splitting the input sentence into a number of contiguous word sequences, called phrases, which are translated into the target language through a phrase dictionary lookup and optionally reordered. The choice between different translations of an ambiguous source phrase and the ordering of the target phrases are guided by a scoring function that combines a set of scores taken from the phrase table with scores from other models such as an n-gram language model. The actual translation process is realised as a search for the highest-scoring translation in the space of a"
P13-4033,2007.tmi-papers.13,0,0.0462402,"ocument at every 1 https://github.com/chardmeier/docent/wiki 193 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 193–198, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics stage of the search progress. Search proceeds by making small changes to the current search state in order to transform it gradually into a better translation. This differs from the DP algorithm used in other decoders, which starts with an empty translation and expands it bit by bit. It is similar to previous work on phrase-based SMT decoding by Langlais et al. (2007), but enables the creation of document-level models, which was not addressed by earlier approaches. Docent currently implements two search algorithms that are different generalisations of the hill climbing local search algorithm by Hardmeier et al. (2012). The original hill climbing algorithm starts with an initial state and generates possible successor states by randomly applying simple elementary operations to the state. After each operation, the new state is scored and accepted if its score is better than that of the previous state, else rejected. Search terminates when the decoder cannot f"
P13-4033,W10-1737,0,0.254585,"Missing"
P13-4033,2012.amta-papers.20,0,0.0288297,"r other researchers to use it in their own experiments. Motivation 2 Most of the research on statistical machine translation (SMT) that was conducted during the last 20 years treated every text as a “bag of sentences” and disregarded all relations between elements in different sentences. Systematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2012). Our decoding algorithm is based on local"
P13-4033,W03-2117,0,0.0178285,"as originally proposed by Hardmeier et al. (2012) to improve lexical cohesion in a document. It is a cross-sentence model over sequences of content words that are scored based on their similarity in a word vector space. The readability models serve to improve the readability of the translation by encouraging the selection of easier and more consistent target words. They are described and demonstrated in more detail in section 5. Docent can read input files both in the NISTXML format commonly used to encode documents in MT shared tasks such as NIST or WMT and in the more elaborate MMAX format (Müller and Strube, 2003). The MMAX format makes it possible to include a wide range of discourselevel corpus annotations such as coreference links. 195 These annotations can then be accessed by the feature models. To allow for additional targetlanguage information such as morphological features of target words, Docent can handle simple word-level annotations that are encoded in the phrase table in the same way as target language factors in Moses. In order to optimise feature weights we have adapted the Moses tuning infrastructure to Docent. In this way we can take advantage of all its features, for instance using dif"
P13-4033,P03-1021,0,0.00654474,"e a wide range of discourselevel corpus annotations such as coreference links. 195 These annotations can then be accessed by the feature models. To allow for additional targetlanguage information such as morphological features of target words, Docent can handle simple word-level annotations that are encoded in the phrase table in the same way as target language factors in Moses. In order to optimise feature weights we have adapted the Moses tuning infrastructure to Docent. In this way we can take advantage of all its features, for instance using different optimisation algorithms such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), and selective tuning of a subset of features. Since document features only give meaningful scores on the document level and not on the sentence level, we naturally perform optimisation on document level, which typically means that we need more data than for the optimisation of sentence-based decoding. The results we obtain are relatively stable and competitive with sentence-level optimisation of the same models (Stymne et al., 2013a). 4 Implementing Feature Models Efficiently While translating a document, the local search decoder attempts to make a great number"
P13-4033,P02-1040,0,0.105205,"ound to genitive construction − Changing long compound to English-based abbreviation − Removal of important word − Bad grammar because of changed part of speech and missing verb planen (the plan) särskilt uppmärksam på (particular attentive on) + Simplified expression Table 2: Example translation snippets with comments Feature Baseline TTR OVIX QW QP All BLEU 0.243 0.243 0.243 0.242 0.243 0.235 OVIX 56.88 55.25 54.65 57.16 57.07 47.80 LIX 51.17 51.04 51.00 51.16 51.06 49.29 Table 1: Results for adding single lexical consistency features to Docent To evaluate our system we used the BLEU score (Papineni et al., 2002) together with a set of readability metrics, since readability is what we hoped to improve by adding consistency features. Here we used OVIX to confirm a direct impact on consistency, and LIX (Björnsson, 1968), which is a common readability measure for Swedish. Unfortunately we do not have access to simplified translated text, so we calculate the MT metrics against a standard reference, which means that simple texts will likely have worse scores than complicated texts closer to the reference translation. We tuned the standard features using Moses and MERT, and then added each lexical consisten"
P13-4033,W13-3308,1,0.807594,"oes not require that another phrase be moved in the opposite direction at the same time. A pair of operations called permute-phrases and linearisephrases can reorder a sequence of phrases into random order and back into the order corresponding to the source language. Since the search algorithm in Docent is stochastic, repeated runs of the decoder will generally produce different output. However, the variance of the output is usually small, especially when initialising with a DP search pass, and it tends to be lower than the variance introduced by feature weight tuning (Hardmeier et al., 2012; Stymne et al., 2013a). 3 Available Feature Models In its current version, Docent implements a selection of sentence-local feature models that makes it possible to build a baseline system with a configuration comparable to that of a typical Moses baseline system. The published source code also includes prototype implementations of a few document-level models. These models should be considered work in progress and serve as a demonstration of the cross-sentence modelling capabilities of the decoder. They have not yet reached a state of maturity that would make them suitable for production use. The sentence-level mo"
P13-4033,W13-5634,1,0.909717,"oes not require that another phrase be moved in the opposite direction at the same time. A pair of operations called permute-phrases and linearisephrases can reorder a sequence of phrases into random order and back into the order corresponding to the source language. Since the search algorithm in Docent is stochastic, repeated runs of the decoder will generally produce different output. However, the variance of the output is usually small, especially when initialising with a DP search pass, and it tends to be lower than the variance introduced by feature weight tuning (Hardmeier et al., 2012; Stymne et al., 2013a). 3 Available Feature Models In its current version, Docent implements a selection of sentence-local feature models that makes it possible to build a baseline system with a configuration comparable to that of a typical Moses baseline system. The published source code also includes prototype implementations of a few document-level models. These models should be considered work in progress and serve as a demonstration of the cross-sentence modelling capabilities of the decoder. They have not yet reached a state of maturity that would make them suitable for production use. The sentence-level mo"
P13-4033,W10-2602,1,0.794735,"the readability of texts by encouraging simple and consistent terminology (Stymne et al., 2013b). This work is a first step towards achieving joint SMT and text simplification, with the final goal of adapting MT to user groups such as people with reading disabilities. Lexical consistency modelling for SMT has been attempted before. The suggested approaches have been limited by the use of sentence-level decoders, however, and had to resort to procedures like post processing (Carpuat, 2009), multiple decoding runs with frozen counts from previous runs (Ture et al., 2012), or cache-based models (Tiedemann, 2010). In Docent, however, we always have access to a full document translation, which makes it straightforward to include features directly into the decoder. We implemented four features on the document level. The first two features are type token ratio (TTR) and a reformulation of it, OVIX, which is less sensitive to text length. These ratios have been related to the “idea density” of a text (Mühlenbock and Kokkinakis, 2009). We also wanted to encourage consistent translations of words, for which we used the Q-value (Deléger et al., 2006), which has been proposed to measure term quality. We appli"
P13-4033,N12-1046,0,0.0590301,"can be used in Docent in order to improve the readability of texts by encouraging simple and consistent terminology (Stymne et al., 2013b). This work is a first step towards achieving joint SMT and text simplification, with the final goal of adapting MT to user groups such as people with reading disabilities. Lexical consistency modelling for SMT has been attempted before. The suggested approaches have been limited by the use of sentence-level decoders, however, and had to resort to procedures like post processing (Carpuat, 2009), multiple decoding runs with frozen counts from previous runs (Ture et al., 2012), or cache-based models (Tiedemann, 2010). In Docent, however, we always have access to a full document translation, which makes it straightforward to include features directly into the decoder. We implemented four features on the document level. The first two features are type token ratio (TTR) and a reformulation of it, OVIX, which is less sensitive to text length. These ratios have been related to the “idea density” of a text (Mühlenbock and Kokkinakis, 2009). We also wanted to encourage consistent translations of words, for which we used the Q-value (Deléger et al., 2006), which has been p"
R13-1088,A00-1002,0,0.055846,"Missing"
R13-1088,D11-1125,0,0.0197306,"est BLEU scores are achieved for n = 2. The numbers in the table imply that the alignments become noisier for n-grams longer than two characters; look at the increasing number of phrases that can be extracted from the aligned corpus, many of which do not survive the filtering. 4.2 One possibility is to just apply the models tuned for the individual translation tasks, which is suboptimal. Therefore, we also introduce a global tuning approach, in which we generate k-best lists for the combined cascaded translation model and we tune corresponding end-to-end weights using MERT (Och, 2003) or PRO (Hopkins and May, 2011). We chose to set the size of the k-best lists to 20 in both steps to keep the size manageable, with 400 hypotheses for each tuning sentence. Another option is to combine (i) the direct translation model, (ii) the word-level pivot model, and (iii) the character-level pivot model. Throwing them all in one k-best reranking system does not work well when using the unnormalized model scores. However, global tuning helps reassign weights such that the interactions between the various components can be covered. We use the same global tuning model introduced above using a combined system as the black"
R13-1088,P05-1074,0,0.0454208,"684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜no, 2006; Wu and Wang, 2009). Pivot languages can also be used for paraphrasing and lexical adaptation (Bannard and Callison-Burch, 2005; Crego et al., 2010). None of this work exploits the similarity between the pivot and the source/target language. The first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006), Cantonese–Mandarin (Zhang, 1998). In contrast, we explore statistical approaches that are potentially applicable to many language pairs. Since we combine wo"
R13-1088,N07-1047,0,0.035904,"g characters as words, and using a special character for the original space character. Due to the reduced vocabulary, we can easily train models of higher order, thus capturing larger context and avoiding generating non-word sequences: we opted for models of order 10, both for the language model and for the maximal phrase length (normally, 5 and 7, respectively). One difficulty is that training these models requires the alignment of characters in bitexts. Specialized character-level alignment algorithms do exist, e.g., those developed for character-tophoneme translations (Damper et al., 2005; Jiampojamarn et al., 2007). However, Tiedemann (2012a) has demonstrated that standard tools for word alignment are in fact also very effective for character-level alignment, especially when extended with local context. Using character ngrams instead of single characters improves the expressive power of lexical translation parameters, which are one of the most important factors in standard word alignment models. For example, using character n-grams increases the vocabulary size of a 1.3M tokens-long Bulgarian text as follows: 101 single characters, 1,893 character bigrams, and 14,305 character trigrams; compared to 30,9"
R13-1088,2008.iwslt-papers.1,0,0.0151556,"translations, called bitexts, which are not available for most language pairs and textual domains. As a result, building an SMT system to translate directly between two languages is often not possible. A common solution to this problem is to use an intermediate, or pivot language to bridge the gap in training such a system. 2 Related Work SMT using pivot languages has been studied for several years. Cohn and Lapata (2007) used triangulation techniques for the combination of phrase tables. The lexical weights in such an approach can be estimated by bridging word alignments (Wu and Wang, 2007; Bertoldi et al., 2008). 676 Proceedings of Recent Advances in Natural Language Processing, pages 676–684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜no, 2006; Wu and Wang, 20"
R13-1088,D07-1103,0,0.0301062,"using character n-grams proposed by Tiedemann (2012a). Another direction we explore is the possibility of reducing the noise in the phrase table. Treating even closely related languages by transliteration techniques is only a rough approximation to the translation task at hand. Furthermore, during training we observe many example translations that are not literally translated from one language to another. Hence, the character-level phrase table will be filled with many noisy and unintuitive translation options. We, therefore, applied phrase table pruning techniques based on relative entropy (Johnson et al., 2007) to remove unreliable pairs. Table 1: Size of the datasets. The original data from OPUS is contributed by on-line users with little quality control and is thus quite noisy. Subtitles in OPUS are checked using automatic language identifiers and aligned using time information (Tiedemann, 2009b; Tiedemann, 2012b). However, we identified many misaligned files and, therefore, we realigned the corpus using hunalign (Varga et al., 2005). We also found several Bulgarian files misclassified as Macedonian and vice versa, which we addressed by filtering out any document pair for which the BLEU score exce"
R13-1088,J93-2003,0,0.0252105,"sely related languages largely overlap in vocabulary and exhibit strong syntactic and lexical similarities. Most words have common roots and express concepts with similar linguistic constructions. Spelling conventions and morphology can still differ, but these differences are typically regular and thus can easily be generalized. These similarities and regularities motivate the use of character-level SMT models, which can operate at the sub-word level, but also cover mappings spanning over words and multi-word units. Hence, we used GIZA++ (Och and Ney, 2003) to generate IBM model 4 alignments (Brown et al., 1993) for character n-grams, which we symmetrized using the grow-diag-final-and heuristics. We then converted the result to character alignments by dropping all characters behind the initial one. Finally, we used the Moses toolkit (Koehn et al., 2007) to build a character-level phrase table. 677 We tuned the parameters of the log-linear SMT model by optimizing BLEU (Papineni et al., 2002). Computing BLEU scores over character sequences does not make much sense, especially for small n-gram sizes (usually, n ≤ 4). Therefore, we post-processed the character-level n-best lists in each tuning step to ca"
R13-1088,W12-3102,0,0.0581772,"Missing"
R13-1088,P07-2045,0,0.0113947,"fer, but these differences are typically regular and thus can easily be generalized. These similarities and regularities motivate the use of character-level SMT models, which can operate at the sub-word level, but also cover mappings spanning over words and multi-word units. Hence, we used GIZA++ (Och and Ney, 2003) to generate IBM model 4 alignments (Brown et al., 1993) for character n-grams, which we symmetrized using the grow-diag-final-and heuristics. We then converted the result to character alignments by dropping all characters behind the initial one. Finally, we used the Moses toolkit (Koehn et al., 2007) to build a character-level phrase table. 677 We tuned the parameters of the log-linear SMT model by optimizing BLEU (Papineni et al., 2002). Computing BLEU scores over character sequences does not make much sense, especially for small n-gram sizes (usually, n ≤ 4). Therefore, we post-processed the character-level n-best lists in each tuning step to calculate word-level BLEU. Thus, we optimized word-level BLEU, while performing character-level translation. 4 We further used the Macedonian–English and the Bulgarian–English movie subtitles datasets from OPUS, which we split into dev/test (10K se"
R13-1088,P07-1092,0,0.0511864,"translation today, are easy to build and offer competitive performance in terms of translation quality. Unfortunately, training such systems requires large parallel corpora of sentences and their translations, called bitexts, which are not available for most language pairs and textual domains. As a result, building an SMT system to translate directly between two languages is often not possible. A common solution to this problem is to use an intermediate, or pivot language to bridge the gap in training such a system. 2 Related Work SMT using pivot languages has been studied for several years. Cohn and Lapata (2007) used triangulation techniques for the combination of phrase tables. The lexical weights in such an approach can be estimated by bridging word alignments (Wu and Wang, 2007; Bertoldi et al., 2008). 676 Proceedings of Recent Advances in Natural Language Processing, pages 676–684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by var"
R13-1088,2009.mtsummit-papers.7,0,0.0171824,"on of phrase tables. The lexical weights in such an approach can be estimated by bridging word alignments (Wu and Wang, 2007; Bertoldi et al., 2008). 676 Proceedings of Recent Advances in Natural Language Processing, pages 676–684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜no, 2006; Wu and Wang, 2009). Pivot languages can also be used for paraphrasing and lexical adaptation (Bannard and Callison-Burch, 2005; Crego et al., 2010). None of this work exploits the similarity between the pivot and the source/target language. The first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish"
R13-1088,C10-1027,0,0.0123116,"ber 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜no, 2006; Wu and Wang, 2009). Pivot languages can also be used for paraphrasing and lexical adaptation (Bannard and Callison-Burch, 2005; Crego et al., 2010). None of this work exploits the similarity between the pivot and the source/target language. The first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006), Cantonese–Mandarin (Zhang, 1998). In contrast, we explore statistical approaches that are potentially applicable to many language pairs. Since we combine word- and character-lev"
R13-1088,D10-1015,1,0.858695,"first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006), Cantonese–Mandarin (Zhang, 1998). In contrast, we explore statistical approaches that are potentially applicable to many language pairs. Since we combine word- and character-level models, a relevant line of research is on combining SMT models of different granularity, e.g., Luong et al. (2010) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tie"
R13-1088,2011.eamt-1.19,0,0.0253963,"Missing"
R13-1088,D09-1141,1,0.895904,"rish–Scottish Gaelic (Scannell, 2006), Cantonese–Mandarin (Zhang, 1998). In contrast, we explore statistical approaches that are potentially applicable to many language pairs. Since we combine word- and character-level models, a relevant line of research is on combining SMT models of different granularity, e.g., Luong et al. (2010) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tiedemann, 2012). Tiedemann (2012a) used pivoting with character-level SMT. 3 One drawback of character-level models is their inability to model long-distance word reorderings. However, we do not assume very large syntactic differences between closely related languages. Another issue is"
R13-1088,E12-1015,1,0.884559,"10) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tiedemann, 2012). Tiedemann (2012a) used pivoting with character-level SMT. 3 One drawback of character-level models is their inability to model long-distance word reorderings. However, we do not assume very large syntactic differences between closely related languages. Another issue is that sentences become longer, which causes an overhead in decoding time. In our experiments below, we use phrase-based SMT, treating characters as words, and using a special character for the original space character. Due to the reduced vocabulary, we can easily train models of higher order, thus capturing larger context and a"
R13-1088,tiedemann-2012-parallel,1,0.770407,"10) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tiedemann, 2012). Tiedemann (2012a) used pivoting with character-level SMT. 3 One drawback of character-level models is their inability to model long-distance word reorderings. However, we do not assume very large syntactic differences between closely related languages. Another issue is that sentences become longer, which causes an overhead in decoding time. In our experiments below, we use phrase-based SMT, treating characters as words, and using a special character for the original space character. Due to the reduced vocabulary, we can easily train models of higher order, thus capturing larger context and a"
R13-1088,P12-2059,1,0.830649,"et al. (2010) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tiedemann, 2012). Tiedemann (2012a) used pivoting with character-level SMT. 3 One drawback of character-level models is their inability to model long-distance word reorderings. However, we do not assume very large syntactic differences between closely related languages. Another issue is that sentences become longer, which causes an overhead in decoding time. In our experiments below, we use phrase-based SMT, treating characters as words, and using a special character for the original space character. Due to the reduced vocabulary, we can easily train models of higher order, thus capturing larger context and a"
R13-1088,N07-1061,0,0.0401329,"d by bridging word alignments (Wu and Wang, 2007; Bertoldi et al., 2008). 676 Proceedings of Recent Advances in Natural Language Processing, pages 676–684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜no, 2006; Wu and Wang, 2009). Pivot languages can also be used for paraphrasing and lexical adaptation (Bannard and Callison-Burch, 2005; Crego et al., 2010). None of this work exploits the similarity between the pivot and the source/target language. The first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006)"
R13-1088,C12-1121,1,0.893166,"Missing"
R13-1088,J03-1002,0,0.0125279,"contextual specificity. Character-level SMT Models Closely related languages largely overlap in vocabulary and exhibit strong syntactic and lexical similarities. Most words have common roots and express concepts with similar linguistic constructions. Spelling conventions and morphology can still differ, but these differences are typically regular and thus can easily be generalized. These similarities and regularities motivate the use of character-level SMT models, which can operate at the sub-word level, but also cover mappings spanning over words and multi-word units. Hence, we used GIZA++ (Och and Ney, 2003) to generate IBM model 4 alignments (Brown et al., 1993) for character n-grams, which we symmetrized using the grow-diag-final-and heuristics. We then converted the result to character alignments by dropping all characters behind the initial one. Finally, we used the Moses toolkit (Koehn et al., 2007) to build a character-level phrase table. 677 We tuned the parameters of the log-linear SMT model by optimizing BLEU (Papineni et al., 2002). Computing BLEU scores over character sequences does not make much sense, especially for small n-gram sizes (usually, n ≤ 4). Therefore, we post-processed th"
R13-1088,W07-0705,0,0.511628,"of different granularity, e.g., Luong et al. (2010) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tiedemann, 2012). Tiedemann (2012a) used pivoting with character-level SMT. 3 One drawback of character-level models is their inability to model long-distance word reorderings. However, we do not assume very large syntactic differences between closely related languages. Another issue is that sentences become longer, which causes an overhead in decoding time. In our experiments below, we use phrase-based SMT, treating characters as words, and using a special character for the original space character. Due to the reduced vocabulary, we can easily train models of hig"
R13-1088,P03-1021,0,0.019644,"ut filtering, the best BLEU scores are achieved for n = 2. The numbers in the table imply that the alignments become noisier for n-grams longer than two characters; look at the increasing number of phrases that can be extracted from the aligned corpus, many of which do not survive the filtering. 4.2 One possibility is to just apply the models tuned for the individual translation tasks, which is suboptimal. Therefore, we also introduce a global tuning approach, in which we generate k-best lists for the combined cascaded translation model and we tune corresponding end-to-end weights using MERT (Och, 2003) or PRO (Hopkins and May, 2011). We chose to set the size of the k-best lists to 20 in both steps to keep the size manageable, with 400 hypotheses for each tuning sentence. Another option is to combine (i) the direct translation model, (ii) the word-level pivot model, and (iii) the character-level pivot model. Throwing them all in one k-best reranking system does not work well when using the unnormalized model scores. However, global tuning helps reassign weights such that the interactions between the various components can be covered. We use the same global tuning model introduced above using"
R13-1088,D12-1027,1,0.907259,"Missing"
R13-1088,P02-1040,0,0.0876252,"of character-level SMT models, which can operate at the sub-word level, but also cover mappings spanning over words and multi-word units. Hence, we used GIZA++ (Och and Ney, 2003) to generate IBM model 4 alignments (Brown et al., 1993) for character n-grams, which we symmetrized using the grow-diag-final-and heuristics. We then converted the result to character alignments by dropping all characters behind the initial one. Finally, we used the Moses toolkit (Koehn et al., 2007) to build a character-level phrase table. 677 We tuned the parameters of the log-linear SMT model by optimizing BLEU (Papineni et al., 2002). Computing BLEU scores over character sequences does not make much sense, especially for small n-gram sizes (usually, n ≤ 4). Therefore, we post-processed the character-level n-best lists in each tuning step to calculate word-level BLEU. Thus, we optimized word-level BLEU, while performing character-level translation. 4 We further used the Macedonian–English and the Bulgarian–English movie subtitles datasets from OPUS, which we split into dev/test (10K sentence pairs for each) and train datasets. We made sure that the dev/test datasets for MK-BG, MK-EN and BG-EN do not overlap, and that all d"
R13-1088,P07-1108,0,0.0556458,"sentences and their translations, called bitexts, which are not available for most language pairs and textual domains. As a result, building an SMT system to translate directly between two languages is often not possible. A common solution to this problem is to use an intermediate, or pivot language to bridge the gap in training such a system. 2 Related Work SMT using pivot languages has been studied for several years. Cohn and Lapata (2007) used triangulation techniques for the combination of phrase tables. The lexical weights in such an approach can be estimated by bridging word alignments (Wu and Wang, 2007; Bertoldi et al., 2008). 676 Proceedings of Recent Advances in Natural Language Processing, pages 676–684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜n"
R13-1088,P09-1018,0,0.0136668,"The lexical weights in such an approach can be estimated by bridging word alignments (Wu and Wang, 2007; Bertoldi et al., 2008). 676 Proceedings of Recent Advances in Natural Language Processing, pages 676–684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜no, 2006; Wu and Wang, 2009). Pivot languages can also be used for paraphrasing and lexical adaptation (Bannard and Callison-Burch, 2005; Crego et al., 2010). None of this work exploits the similarity between the pivot and the source/target language. The first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish–Crimean Tatar (Alti"
R13-1088,2009.eamt-1.3,1,0.939893,"rity, e.g., Luong et al. (2010) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tiedemann, 2012). Tiedemann (2012a) used pivoting with character-level SMT. 3 One drawback of character-level models is their inability to model long-distance word reorderings. However, we do not assume very large syntactic differences between closely related languages. Another issue is that sentences become longer, which causes an overhead in decoding time. In our experiments below, we use phrase-based SMT, treating characters as words, and using a special character for the original space character. Due to the reduced vocabulary, we can easily train models of higher order, thus c"
R13-1088,P98-2238,0,0.0514886,"no, 2006; Wu and Wang, 2009). Pivot languages can also be used for paraphrasing and lexical adaptation (Bannard and Callison-Burch, 2005; Crego et al., 2010). None of this work exploits the similarity between the pivot and the source/target language. The first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006), Cantonese–Mandarin (Zhang, 1998). In contrast, we explore statistical approaches that are potentially applicable to many language pairs. Since we combine word- and character-level models, a relevant line of research is on combining SMT models of different granularity, e.g., Luong et al. (2010) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and N"
R13-1088,C98-2233,0,\N,Missing
skadins-etal-2014-billions,J93-1004,0,\N,Missing
skadins-etal-2014-billions,moore-2002-fast,0,\N,Missing
skadins-etal-2014-billions,steinberger-etal-2012-dgt,0,\N,Missing
skadins-etal-2014-billions,P02-1040,0,\N,Missing
skadins-etal-2014-billions,P07-2045,0,\N,Missing
skadins-etal-2014-billions,2005.mtsummit-papers.11,0,\N,Missing
skadins-etal-2014-billions,W13-2201,0,\N,Missing
skadins-etal-2014-billions,C12-1160,1,\N,Missing
skadins-etal-2014-billions,L14-1000,0,\N,Missing
skadins-etal-2014-billions,W11-2123,0,\N,Missing
skadins-etal-2014-billions,P13-2121,0,\N,Missing
skadins-etal-2014-billions,P03-1021,0,\N,Missing
skadins-etal-2014-billions,N13-1073,0,\N,Missing
tiedemann-2002-matslex,1997.tmi-1.12,0,\N,Missing
tiedemann-2002-matslex,2000.tc-1.4,0,\N,Missing
tiedemann-2006-isa,J93-1004,0,\N,Missing
tiedemann-2006-isa,W99-0604,0,\N,Missing
tiedemann-2006-isa,smith-jahr-2000-cairo,0,\N,Missing
tiedemann-2006-isa,E03-1086,0,\N,Missing
tiedemann-2006-isa,C04-1031,1,\N,Missing
tiedemann-2008-synchronizing,tiedemann-2006-isa,1,\N,Missing
tiedemann-2008-synchronizing,W96-0201,0,\N,Missing
tiedemann-2008-synchronizing,J03-1002,0,\N,Missing
tiedemann-2008-synchronizing,2007.mtsummit-papers.66,0,\N,Missing
tiedemann-2008-synchronizing,tiedemann-nygaard-2004-opus,1,\N,Missing
tiedemann-2010-lingua,C08-1139,0,\N,Missing
tiedemann-2010-lingua,C00-2092,0,\N,Missing
tiedemann-2010-lingua,W09-3804,0,\N,Missing
tiedemann-2010-lingua,D08-1024,0,\N,Missing
tiedemann-2010-lingua,P07-2045,0,\N,Missing
tiedemann-2010-lingua,W08-0411,0,\N,Missing
tiedemann-2010-lingua,N09-1025,0,\N,Missing
tiedemann-2010-lingua,W07-2441,0,\N,Missing
tiedemann-2010-lingua,J03-1002,0,\N,Missing
tiedemann-2010-lingua,W06-3119,0,\N,Missing
tiedemann-2010-lingua,2005.mtsummit-papers.11,0,\N,Missing
tiedemann-2010-lingua,W95-0106,0,\N,Missing
tiedemann-2010-lingua,J07-2003,0,\N,Missing
tiedemann-2012-parallel,vanallemeersch-2010-belgisch,0,\N,Missing
tiedemann-2012-parallel,tiedemann-2008-synchronizing,1,\N,Missing
tiedemann-2012-parallel,P11-2033,0,\N,Missing
tiedemann-2012-parallel,P07-2045,0,\N,Missing
tiedemann-2012-parallel,Y09-1013,0,\N,Missing
tiedemann-2012-parallel,P07-2053,0,\N,Missing
tiedemann-2012-parallel,J03-1002,0,\N,Missing
tiedemann-2012-parallel,gimenez-marquez-2004-svmtool,0,\N,Missing
tiedemann-etal-2012-distributed,J93-1004,0,\N,Missing
tiedemann-etal-2012-distributed,W05-0816,0,\N,Missing
tiedemann-nygaard-2004-opus,J93-1004,0,\N,Missing
tiedemann-nygaard-2004-opus,W01-0519,0,\N,Missing
tiedemann-nygaard-2004-opus,A00-1031,0,\N,Missing
W06-2405,W97-0207,0,0.867433,"xpressions). (1) Wat moeten lidstaten ondernemen om what must member states do to aan haar eisen te voldoen? at her demands to meet? ‘What must EU member states do to meet her demands?’ 1 Introduction Knowing whether an expression receives a literal meaning or an idiomatic meaning is important for natural language processing applications that require some sort of semantic interpretation. Some applications that would benefit from knowing this distinction are machine translation (Imamura et al., 2003), finding paraphrases (Bannard and Callison-Burch, 2005), (multilingual) information retrieval (Melamed, 1997a), etc. The purpose of this paper is to explore to what extent word-alignment in parallel corpora can be used to distinguish idiomatic multiword expressions from more transparent multiword expressions and fully productive expressions. (2) Deze situatie brengt de bestaande politieke this situation brings the existing political barri`eres zeer duidelijk aan het licht. barriers very clearly in the light ‘This situation brings the existing political limitations to light very clearly.’ 1 Here, we ignore morpho-syntactic and pragmatic factors that could help model the distinction. 33 pora can help"
W06-2405,W01-0715,0,0.0464972,"Missing"
W06-2405,W99-0604,0,0.0517389,"t contains about 29 million tokens in about 1.2 million sentences. The English, Spanish and German counterparts are of similar size between 28 and 30 million words in roughly the same number of sentences. Automatic word alignment has been done using GIZA++ (Och, 2003). We used standard settings of the system to produce Viterbi alignments of IBM model 4. Alignments have been produced for both translation directions (source to target and target to source) on tokenized plain text.3 We also used a well-known heuristics for combining the two directional alignments, the so-called refined alignment (Och et al., 1999). Word-to-word alignments have been merged such that words are connected with each other if they are linked to the same target. In this way we obtained three different word alignment files: source to target (src2trg) with possible multi-word units in the source language, target to source (trg2src) with possible multi-word units in the target language, and refined with possible multi-word units in both languages. We also created bilingual word type links from the different word-aligned corpora. These lists include alignment frequencies that we will use later on for extracting default alignments"
W06-2405,P05-1074,0,0.0400518,"hin a scale that ranges from fully transparent to opaque (in figurative expressions). (1) Wat moeten lidstaten ondernemen om what must member states do to aan haar eisen te voldoen? at her demands to meet? ‘What must EU member states do to meet her demands?’ 1 Introduction Knowing whether an expression receives a literal meaning or an idiomatic meaning is important for natural language processing applications that require some sort of semantic interpretation. Some applications that would benefit from knowing this distinction are machine translation (Imamura et al., 2003), finding paraphrases (Bannard and Callison-Burch, 2005), (multilingual) information retrieval (Melamed, 1997a), etc. The purpose of this paper is to explore to what extent word-alignment in parallel corpora can be used to distinguish idiomatic multiword expressions from more transparent multiword expressions and fully productive expressions. (2) Deze situatie brengt de bestaande politieke this situation brings the existing political barri`eres zeer duidelijk aan het licht. barriers very clearly in the light ‘This situation brings the existing political limitations to light very clearly.’ 1 Here, we ignore morpho-syntactic and pragmatic factors tha"
W06-2405,J93-1003,0,0.058019,"Missing"
W06-2405,tiedemann-nygaard-2004-opus,1,0.656511,"c, three different rankings of the candidate triples were produced. The three different ranks assigned to each triple were uniformly combined to form the final ranking. From this list, we selected the top 200 triples 2 Data and resources We base our investigations on the Europarl corpus consisting of several years of proceedings from the European Parliament (Koehn, 2003). We focus on Dutch expressions and their translations into English, Spanish and German.2 Thus, we used the entire sections of Europarl in these three languages. The corpus has been tokenized and aligned at the sentence level (Tiedemann and Nygaard, 2004). The Dutch part contains about 29 million tokens in about 1.2 million sentences. The English, Spanish and German counterparts are of similar size between 28 and 30 million words in roughly the same number of sentences. Automatic word alignment has been done using GIZA++ (Och, 2003). We used standard settings of the system to produce Viterbi alignments of IBM model 4. Alignments have been produced for both translation directions (source to target and target to source) on tokenized plain text.3 We also used a well-known heuristics for combining the two directional alignments, the so-called refi"
W06-2405,E03-1029,0,\N,Missing
W08-1803,J97-1003,0,0.754269,"Missing"
W08-1803,W02-1906,0,0.075828,"Missing"
W08-1803,P00-1071,0,0.0403962,"cuments into passages before indexing them. This is also called index-time passaging and refers to a one-step process of retrieving appropriate textual units for subsequent answer extraction modules (Roberts and Gaizauskas, 2004; Greenwood, 2004). This is in contrast to other strategies using a two-step procedure consisting of document retrieval and search-time passaging thereafter. Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al., 2000)). In general, allowing multiple passages per document is preferable for QA as possible answers can be contained at various positions in a document (Roberts and Gaizauskas, 2004). For this, an index-time approach has the advantage that the retrieval of multiple passages per documents is straightforward because all of them compete which each other in the same index using the same metric for ranking. A comparison between index-time and search1 Introduction Passage retrieval in question answering is different from information retrieval in general. Extracting relevant passages from large document"
W08-1803,J00-4005,0,0.0232888,"Missing"
W08-1803,2006.jeptalnrecital-invite.2,0,0.042434,"Missing"
W08-1803,M95-1005,0,\N,Missing
W08-1807,N04-1041,0,0.0911589,"Missing"
W08-1807,W98-0705,0,0.112213,"Missing"
W08-1807,D07-1002,0,0.0831903,"Missing"
W08-1807,W08-1803,1,0.838431,"expansions. Since all synonyms are equally similar, we do not have similarity scores for them to be used in a threshold. The categorised named entities were not only used to expand named entities with the corre6 Results In Table 2 the MRR (Mean Reciprocal Rank) is given for the various expansion techniques. Scores are given for expanding the several syntactic categories, where possible. The baseline does not 5 We used MRR instead of other common evaluation measures because of its stronger correlation with the overall performance of our QA system than, for example, coverage and redundancy (see Tiedemann and Mur (2008)). 4 Note that the number of nouns from EWN is the result of subtracting the proper names. 53 SynCat Nouns Adj Verbs Proper All EWN 51.52 52.33 52.40 52.59 51.65 MRR Syntax Align 51.15 51.21 52.27 52.38 52.33 52.21 50.16 51.21 51.02 Proxi 51.38 51.71 52.62 53.94 53.36 offs to keep the co-occurrence matrix manageable. The larger impact of the proximity-based nearest neighbours is probably partly due to this decision. The cutoffs for the alignment-based and syntaxbased method have been determined after evaluations on EuroWordNet (Vossen, 1998) (see also van der Plas (2008)). The largest impact r"
W08-1807,P06-2111,1,0.872885,"Missing"
W08-1807,W07-1206,0,0.068301,"Missing"
W08-1807,2006.jeptalnrecital-invite.2,0,0.0460105,"Missing"
W08-1807,P02-1005,0,0.0751087,"Missing"
W08-1807,E03-1070,0,0.0275792,"Missing"
W09-4205,J07-3002,0,0.0158681,"is that it is fast and easy to apply, it allows n:m alignments, and it makes our results comparable to the statistical alignments that include symmetrization. 3 Experiments For our experiments we will use well-known data sets that have been used before for word alignment experiments. Most related work on supervised alignment models reports results on the French-English data set from the shared task at WPT03 [12] derived from the parallel Canadian Hansards corpus. This data set caused a lot of discussion especially because of the flaws in evaluation measures used for word alignment experiments [5]. Therefore, we will apply this set for training purposes only (447 aligned sentences with 4,038 sure (S) links and 13,400 (P ) possible links) and stick to another set for evaluation [4]. This set includes English-French word alignment data for 100 sentences from the Europarl corpus [6] with a much smaller number of possible links (437 compared to 1,009 sure links) which hopefully leads to more reliable results. Some of the alignment clues require large parallel corpora for estimating reliable feature values (for example co-occurrence measures). For training we use the Canadian Hansards as pr"
W09-4205,2005.mtsummit-papers.11,0,0.0121309,"t related work on supervised alignment models reports results on the French-English data set from the shared task at WPT03 [12] derived from the parallel Canadian Hansards corpus. This data set caused a lot of discussion especially because of the flaws in evaluation measures used for word alignment experiments [5]. Therefore, we will apply this set for training purposes only (447 aligned sentences with 4,038 sure (S) links and 13,400 (P ) possible links) and stick to another set for evaluation [4]. This set includes English-French word alignment data for 100 sentences from the Europarl corpus [6] with a much smaller number of possible links (437 compared to 1,009 sure links) which hopefully leads to more reliable results. Some of the alignment clues require large parallel corpora for estimating reliable feature values (for example co-occurrence measures). For training we use the Canadian Hansards as provided for the WPT03 workshop and for evaluation these values are taken from the Europarl corpus. For evaluation we use the standard measures used in related research: |P ∩ A| |A| |S ∩ A| Rec(A, S) = |S| |P ∩ A |+ |S ∩ A| AER(A, P, S) = 1 − |S |+ |A|   (1 − α) α + F (A, P, S, α) = 1/ P"
W09-4205,P07-2045,0,0.00405811,": |P ∩ A| |A| |S ∩ A| Rec(A, S) = |S| |P ∩ A |+ |S ∩ A| AER(A, P, S) = 1 − |S |+ |A|   (1 − α) α + F (A, P, S, α) = 1/ P rec(A, P ) Rec(A, S) P rec(A, P ) = For the F-measure we give balanced values and also unbalanced F-values with α = 0.4. The latter is supposed to show a better correlation with BLEU scores. However, we did not perform any tests with statistical MT using our alignment techniques to verify this for the data we have used. For comparison we use the IBM model 4 alignments and the intersection and grow-diag-final-and symmetrizaton heuristics as implemented in the Moses toolkit [7]. We also compare our results with a discriminative alignment approach using the same alignment search algorithm, the same features and a global maximum entropy classifier [3] trained on the same training data (using default settings of the megam toolkit). 3.1 Alignment features A wide variety of features can be used to collect alignment evidence. We use, among others, similar features 30 as described in [18]. In particular, we use the Dice coefficient for measuring co-occurrence, the longest common subsequence ratio (LCSR) for string similarity, and other orthographic features such as identic"
W09-4205,P93-1003,0,0.142813,"of hand-aligned training data. We can show that this “evidence-based” approach can be used to improve the baseline of statistical alignment and also outperforms a discriminative approach based on a maximum entropy classifier. 1 Introduction Automatic word alignment has received a lot of attention mainly due to the intensive research on statistical machine translation. However, parallel corpora and word alignment are not only useful in that field but may be applied to various tasks such as computer aided language learning (see for example [15]) and bilingual terminology extraction (for example [8, 10]). The automatic alignment of corresponding words in translated sentences is a challenging task even for small translation units as the following Dutch-English example tries to illustrate. koffie vind ik lekker I like coffee Word alignment approaches have to consider crossing links and multiple links per word in both directions. Discontinuous units may also be aligned to corresponding parts in the other language as shown in the example above (vind...lekker - like). Various other issues due to translation divergency make word alignment a much more challenging task than, for instance, sentence a"
W09-4205,N06-1015,0,0.0770169,"ing parts in the other language as shown in the example above (vind...lekker - like). Various other issues due to translation divergency make word alignment a much more challenging task than, for instance, sentence alignment. Generative statistical models for word alignment usually have problems with nonmonotonic alignments and many-to-many links. In the literature several attempts are described in which additional features are integrated besides the distribution of surface words to overcome these difficulties. In recent years various discriminative approaches have been proposed for this task [18, 9, 13, 14, 11, 1]. They require word-aligned training data for estimating model parameters in contrast to the traditional IBM 2 Evidence-based alignment The evidence-based alignment approach is based on the techniques proposed by [19]. This approach applies the notion of link evidence derived from word alignment clues. An alignment clue C(rk |si , tj ) is used as a probabilistic score indicating a (positive) relation rk between two items si , tj in their contexts. Link evidence E(a, rk |si , tj ) is then defined as the product of this score and the likelihood of establishing a link given the relation indicated"
W09-4205,E09-1057,0,0.0195346,"of hand-aligned training data. We can show that this “evidence-based” approach can be used to improve the baseline of statistical alignment and also outperforms a discriminative approach based on a maximum entropy classifier. 1 Introduction Automatic word alignment has received a lot of attention mainly due to the intensive research on statistical machine translation. However, parallel corpora and word alignment are not only useful in that field but may be applied to various tasks such as computer aided language learning (see for example [15]) and bilingual terminology extraction (for example [8, 10]). The automatic alignment of corresponding words in translated sentences is a challenging task even for small translation units as the following Dutch-English example tries to illustrate. koffie vind ik lekker I like coffee Word alignment approaches have to consider crossing links and multiple links per word in both directions. Discontinuous units may also be aligned to corresponding parts in the other language as shown in the example above (vind...lekker - like). Various other issues due to translation divergency make word alignment a much more challenging task than, for instance, sentence a"
W09-4205,P05-1057,0,0.120611,"ing parts in the other language as shown in the example above (vind...lekker - like). Various other issues due to translation divergency make word alignment a much more challenging task than, for instance, sentence alignment. Generative statistical models for word alignment usually have problems with nonmonotonic alignments and many-to-many links. In the literature several attempts are described in which additional features are integrated besides the distribution of surface words to overcome these difficulties. In recent years various discriminative approaches have been proposed for this task [18, 9, 13, 14, 11, 1]. They require word-aligned training data for estimating model parameters in contrast to the traditional IBM 2 Evidence-based alignment The evidence-based alignment approach is based on the techniques proposed by [19]. This approach applies the notion of link evidence derived from word alignment clues. An alignment clue C(rk |si , tj ) is used as a probabilistic score indicating a (positive) relation rk between two items si , tj in their contexts. Link evidence E(a, rk |si , tj ) is then defined as the product of this score and the likelihood of establishing a link given the relation indicated"
W09-4205,W03-0301,0,0.0318998,"he “refined” heuristics defined in [16], that is known to produce good results for example for the symmetrization of directional statistical word alignment. The advantages of this approach is that it is fast and easy to apply, it allows n:m alignments, and it makes our results comparable to the statistical alignments that include symmetrization. 3 Experiments For our experiments we will use well-known data sets that have been used before for word alignment experiments. Most related work on supervised alignment models reports results on the French-English data set from the shared task at WPT03 [12] derived from the parallel Canadian Hansards corpus. This data set caused a lot of discussion especially because of the flaws in evaluation measures used for word alignment experiments [5]. Therefore, we will apply this set for training purposes only (447 aligned sentences with 4,038 sure (S) links and 13,400 (P ) possible links) and stick to another set for evaluation [4]. This set includes English-French word alignment data for 100 sentences from the Europarl corpus [6] with a much smaller number of possible links (437 compared to 1,009 sure links) which hopefully leads to more reliable resu"
W09-4205,H05-1011,0,0.100126,"ing parts in the other language as shown in the example above (vind...lekker - like). Various other issues due to translation divergency make word alignment a much more challenging task than, for instance, sentence alignment. Generative statistical models for word alignment usually have problems with nonmonotonic alignments and many-to-many links. In the literature several attempts are described in which additional features are integrated besides the distribution of surface words to overcome these difficulties. In recent years various discriminative approaches have been proposed for this task [18, 9, 13, 14, 11, 1]. They require word-aligned training data for estimating model parameters in contrast to the traditional IBM 2 Evidence-based alignment The evidence-based alignment approach is based on the techniques proposed by [19]. This approach applies the notion of link evidence derived from word alignment clues. An alignment clue C(rk |si , tj ) is used as a probabilistic score indicating a (positive) relation rk between two items si , tj in their contexts. Link evidence E(a, rk |si , tj ) is then defined as the product of this score and the likelihood of establishing a link given the relation indicated"
W09-4205,P06-1065,0,0.0649048,"ing parts in the other language as shown in the example above (vind...lekker - like). Various other issues due to translation divergency make word alignment a much more challenging task than, for instance, sentence alignment. Generative statistical models for word alignment usually have problems with nonmonotonic alignments and many-to-many links. In the literature several attempts are described in which additional features are integrated besides the distribution of surface words to overcome these difficulties. In recent years various discriminative approaches have been proposed for this task [18, 9, 13, 14, 11, 1]. They require word-aligned training data for estimating model parameters in contrast to the traditional IBM 2 Evidence-based alignment The evidence-based alignment approach is based on the techniques proposed by [19]. This approach applies the notion of link evidence derived from word alignment clues. An alignment clue C(rk |si , tj ) is used as a probabilistic score indicating a (positive) relation rk between two items si , tj in their contexts. Link evidence E(a, rk |si , tj ) is then defined as the product of this score and the likelihood of establishing a link given the relation indicated"
W09-4205,J03-1002,0,0.00687741,"epending on the model optimal alignments can be found [18, 9, 1] or greedy search heuristics are applied [11, 14]. We will use the second approach and model link dependencies in terms of contextual features. We believe that this gives us more flexibility when defining contextual dependencies and also keeps the model very simple with regards to training. For the alignment search problem we could still apply a model that allows optimal decoding, for example, the approach proposed in [18]. However, we will stick to a simple greedy search heuristics, similar to the “refined” heuristics defined in [16], that is known to produce good results for example for the symmetrization of directional statistical word alignment. The advantages of this approach is that it is fast and easy to apply, it allows n:m alignments, and it makes our results comparable to the statistical alignments that include symmetrization. 3 Experiments For our experiments we will use well-known data sets that have been used before for word alignment experiments. Most related work on supervised alignment models reports results on the French-English data set from the shared task at WPT03 [12] derived from the parallel Canadian"
W09-4205,H05-1010,0,0.353915,"ing parts in the other language as shown in the example above (vind...lekker - like). Various other issues due to translation divergency make word alignment a much more challenging task than, for instance, sentence alignment. Generative statistical models for word alignment usually have problems with nonmonotonic alignments and many-to-many links. In the literature several attempts are described in which additional features are integrated besides the distribution of surface words to overcome these difficulties. In recent years various discriminative approaches have been proposed for this task [18, 9, 13, 14, 11, 1]. They require word-aligned training data for estimating model parameters in contrast to the traditional IBM 2 Evidence-based alignment The evidence-based alignment approach is based on the techniques proposed by [19]. This approach applies the notion of link evidence derived from word alignment clues. An alignment clue C(rk |si , tj ) is used as a probabilistic score indicating a (positive) relation rk between two items si , tj in their contexts. Link evidence E(a, rk |si , tj ) is then defined as the product of this score and the likelihood of establishing a link given the relation indicated"
W09-4205,E03-1026,1,0.598336,"e statistical models for word alignment usually have problems with nonmonotonic alignments and many-to-many links. In the literature several attempts are described in which additional features are integrated besides the distribution of surface words to overcome these difficulties. In recent years various discriminative approaches have been proposed for this task [18, 9, 13, 14, 11, 1]. They require word-aligned training data for estimating model parameters in contrast to the traditional IBM 2 Evidence-based alignment The evidence-based alignment approach is based on the techniques proposed by [19]. This approach applies the notion of link evidence derived from word alignment clues. An alignment clue C(rk |si , tj ) is used as a probabilistic score indicating a (positive) relation rk between two items si , tj in their contexts. Link evidence E(a, rk |si , tj ) is then defined as the product of this score and the likelihood of establishing a link given the relation indicated by that clue: E(a, rk |si , tj ) = C(rk |si , tj )P (a|rk ) Various types of alignment clues can be found in parallel data. Association scores and similarity measures can be used to assign their values. For example,"
W09-4205,J93-2003,0,\N,Missing
W09-4205,graca-etal-2008-building,0,\N,Missing
W09-4205,P06-1009,0,\N,Missing
W09-4206,J07-3002,0,0.0244731,"S ∩ A|/|S|   α (1 − α) 1/ + P rec(A, P ) Rec(A, S) S refers here to the good alignments in the gold standard and P refers to the possible alignments which includes both, good and fuzzy. A are the links proposed by the system and α is used to define the balance between precision and recall in the F-score. We will only use a balanced F-score with α = 0.5. We also omit alignment error rates due to the discussion about this measure in the word alignment literature. Note that the proportion of fuzzy links seems reasonable and we do not expect severe consequences on our evaluation as discussed in [2] for word alignment experiments with unbalanced gold standards. 4.3 4 Evaluation Results The selection of appropriate features is very important in our approach. We tested a number of feature sets and combinations in order to see the impact of features on alignment results. Table 1 summarizes our experiments with various sets. The upper part represents the performance of separate feature types on their own. The lower part shows results of combined feature types. Link dependency features are added in the right-hand side columns – either child link dependencies or dependencies on all subtree nod"
W09-4206,P03-1011,0,0.0609538,"Missing"
W09-4206,C04-1154,0,0.413498,"Missing"
W09-4206,2007.tmi-papers.11,0,0.0179144,"n their alignment method: • independence with respect to language pair and constituent labelling schema • preservation of the given tree structures • minimal external resources required Most related work on tree alignment is done in the context of machine translation research. Several variants of syntax-based MT approaches have been proposed in • word-level alignments are guided by links higher up the trees, which provide more context information 33 NLP Methods and Corpora in Translation, Lexicography, and Language Learning 2009 – Borovets, Bulgaria, pages 33–39 In addition, the authors quote [6] in defining a set of well-formedness criteria and explaining that this should result in producing “enough information to allow the inference of complex translational patterns from a parallel treebank, including some idiosyncratic translational divergences” (2008:1106): (i) A node in a tree may only be linked once. (ii) Descendants/ancestors of a source linked node may only be linked to descendants/ancestors of its target linked counterpart. In short the alignment algorithm consists of the following steps: • Each source node s can link to any target node t and vice versa. Initially all these l"
W09-4206,2005.mtsummit-papers.11,0,0.0430654,"Missing"
W09-4206,P07-2045,0,0.00569667,"as follows: 1 if sx ≤ si ∨ ty ≤ tj 0 otherwise Note that the definition above is not restricted to word alignment. Other types of existing links between nodes dominated by the current subtree pair could be used in the same way. However, using the results of automatic word alignment we can compute these features from the links between terminal nodes. We can use various types of automatic word alignments. In our experiments we apply the Viterbi alignments produced by Giza++ [11] using the IBM 4 model in both directions, the union of these links and the intersection. For the latter we use Moses [8] which is also used for the estimation of lexical probabilities applied for lexical features described in the previous section. Yet another feature derived from word alignment can be used to improve the alignment of terminal nodes. This feature is set to one if and only if both nodes are terminal nodes and are linked in the underlying word alignment. 3.2.3 tree span similarity (tss). For the former we use the distances d(si , sroot ), d(ti , troot ) from the current candidate nodes to the root nodes of source and target language tree, respectively. Furthermore, we use the size of a tree (defin"
W09-4206,O01-2004,0,0.0568419,"Missing"
W09-4206,J03-1002,0,0.00879552,"hat we will use refers to the number of terminal nodes dominated by the candidate nodes. We define the ratio of leaf nodes as follows: 1 if sx ≤ si ∨ ty ≤ tj 0 otherwise Note that the definition above is not restricted to word alignment. Other types of existing links between nodes dominated by the current subtree pair could be used in the same way. However, using the results of automatic word alignment we can compute these features from the links between terminal nodes. We can use various types of automatic word alignments. In our experiments we apply the Viterbi alignments produced by Giza++ [11] using the IBM 4 model in both directions, the union of these links and the intersection. For the latter we use Moses [8] which is also used for the estimation of lexical probabilities applied for lexical features described in the previous section. Yet another feature derived from word alignment can be used to improve the alignment of terminal nodes. This feature is set to one if and only if both nodes are terminal nodes and are linked in the underlying word alignment. 3.2.3 tree span similarity (tss). For the former we use the distances d(si , sroot ), d(ti , troot ) from the current candidat"
W09-4206,H05-1010,0,0.0276351,"nk dependencies. However, this did not give us any significant improvements. Therefore, we will not report these results here. 3.3 Alignment Search Our tree alignment approach is based on a global binary classifier. This means that we actually classify individual node pairs even though we include contextual and first-order features as described above. Despite the fact that individual classification is possible in this way, the important notion of alignment competition is not explored in this way. That this is a strong drawback has already been pointed out in related research on word alignment [14]. However, similar to discriminative word alignment, competition can easily be integrated in the system by applying appropriate search strategies. Naturally, the best strategy would be to include competition explicitly in the alignment model and train parameters for a structural alignment approach. We will leave this for future research and concentrate our current work on feature selection in combination with simple greedy search heuristics. In particular, we will use a greedy best-first search similar to competitive linking used in early work on word alignment. One of the drawbacks in this te"
W09-4206,2007.mtsummit-papers.62,0,0.293831,"translation (MT) approach. Since well-aligned treebanks will play a substantial role in our MT model, finding an optimal solution to the problem of tree alignment is very important. In the next section, we provide a brief background of recent findings on the topic before presenting our own approach thereafter. 2 Related Work Gideon Kotz´e Alpha-Informatica Rijksuniversiteit Groningen Groningen, The Netherlands g.j.kotze@rug.nl recent years involving the alignment of syntactic structures. In general we can distinguish between tree-tostring (or vice versa) and tree-to-tree alignment approaches. [15] describe some recent attempts at subsentential alignment on the phrase level: [9] use a stochastic inversion transduction grammar to parse a source sentence and use the output to build up a target language parse, while also inducing alignments. The latter are extracted and converted into translation templates. [16] use a method they call “bilingual chunking”, where the words of a tree pair are aligned and during the process, chunks are extracted by using the tree structure, after which the chunks are POS tagged. However, the original tree structures are lost in the process. [3] proposes a met"
W09-4206,C02-1010,0,0.280058,"Missing"
W09-4206,C08-1139,0,0.565307,". Alignment Features The selection of appropriate features for classification is crucial in our approach. The input to the tree aligner is sentence aligned parse tree pairs from which various features can be extracted. Another important source is word alignment and information derived from statistical word alignment models. In the following we describe the different feature types that we apply. 3.2.1 Lexical Equivalence Features Lexical probabilities are used in unsupervised tree alignment approaches as explained earlier in section 2. We will also use the same inside/outside scores defined in [17] as our basic features as they have proven to be useful for tree alignment. However, we define additional features and feature combinations derived from automatic word alignment in order to enrich the alignment model. First of all, we use inside and outside scores as individual features besides their product. We also use individual α(x|y) scores as separate features. Furthermore, we define a variant of inside and outside scores using a slightly modified definition of the equivalence score α: αmax (x|y) = We believe that this definition better reflects the relations between words in sentences t"
W10-1728,W04-3225,0,0.0268809,"parameters. Therefore, we just added a simplistic approach filtering tokens by their length in characters instead. Assuming that longer items are more likely to be content words we simply set a threshold to decide whether to add a term to the cache or not. This threshold can be adjusted using command-line arguments. Finally, we also need to be careful about noise in the cache. This is essential as the caching approach is prone to error propagation. However, detecting noise is difficult. If there would be a notion of noise in translation hypotheses, the decoder would avoid it. In related work (Nepveu et al., 2004) have studied cache-based translation models in connection with interactive machine translation. In that case, one can assume correct input after post-editing the translation suggestions. One way to approach noise reduction in non-interactive MT is to make use of transition costs in the translation lattice. Assuming that this cost (which is estimated internally within the decoder during the expansion of translation hypotheses) refers to some kind of confidence we can discard translation options above a certain threshold, which is what we did in the implementation of our translation model cache"
W10-1728,J03-1002,0,0.00979727,"Missing"
W10-1728,P03-1021,0,0.00666117,"sts in the translation lattice. Assuming that this cost (which is estimated internally within the decoder during the expansion of translation hypotheses) refers to some kind of confidence we can discard translation options above a certain threshold, which is what we did in the implementation of our translation model cache. 3 and Ney, 2003) and Moses (Koehn et al., 2007) applying default settings and lowercased training data. Lexicalized reordering was trained on the combined data set. All baseline models were then tuned on the News test data from 2008 using minimum error rate training (MERT) (Och, 2003). The results in terms of lower-case BLEU scores are listed in table 1. de-en baseline de-en cache en-de baseline en-de cache es-en baseline es-en cache en-es baseline en-es cache BLEU 21.3 21.5 15.6 14.4 26.7 26.1 26.9 23.0 1 57.4 58.1 52.5 52.6 61.7 62.6 61.5 60.6 n-gram scores 2 3 27.8 15.1 28.1 15.2 21.7 10.6 21.0 9.9 32.7 19.9 32.7 19.8 33.3 20.5 30.4 17.6 4 8.6 8.7 5.5 4.9 12.6 12.5 12.9 10.4 Table 1: Results on the WMT10 test set. In the adaptation experiments we applied exactly the same models using the feature weights from the baseline with the addition of the caching components in bo"
W10-1728,P07-1065,0,0.0251947,"nglish (de-en) and EnglishGerman (en-de) using the constrained track, i.e. using the provided training and development data from Europarl and the News domain. Later we also added experiments for Spanish (es) and English using a similar setup. Our baseline system incorporates the following components: We trained two separate 5-gram language models for each language with the standard smoothing strategies (interpolation and KneserNey discounting), one for Europarl and one for the News data. All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. We also included two separate translation models, one for the combined Europarl and News data and one for the News data only. They were estimated using the standard tools GIZA++ (Och 191 the adaptive approach cannot cope with the noise added to the cache. baseline 3.1 reference cache Discussion baseline There are two important observations that should be mentioned here. First of all, the adaptive approach assumes coherent text input. However, the WMT test-set is composed of many short news headlines with various topics involved. We, therefore, a"
W10-1728,W10-2602,1,0.815876,"nse). Observe that this is true even in the latter case where synonymous translations such as “¨alskling” would be possible as well. In other words, deciding to stick to consistent lexical translations should be preferred in MT because the chance of alternative translations in repeated cases is low. Here again, common static translation models do not capture this property at all. In the following we explain our attempt to integrate contextual dependencies using cache-based adaptive models in a standard SMT setup. We have already successfully applied this technique to a domain-adaptation task (Tiedemann, 2010). 189 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 189–194, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Now we would like to investigate the robustness of this model in a more general case where some in-domain training data is available and input data is less repetitive. 2 The importance of a cached translation option exponentially decays and we normalize the sum of cached occurrences by the number of translation options with the same foreign language item that we condition on. Cache-based Adaptive Models"
W10-1728,P07-2045,0,\N,Missing
W10-2602,W09-0432,0,0.0852006,"ata. Statistical machine translation is no exception. Despite its popularity, standard SMT approaches fail to provide a framework for general application across domains unless appropriate training data is available and used in parameter estimation and tuning. The main problem is the general assumption of independent and identically distributed (i.i.d.) variables in machine learning approaches applied in the estimation of static global models. Recently, there has been quite some attention to the problem of domain switching in SMT (Zhao et al., 2004; Ueffing et al., 2007; Civera and Juan, 2007; Bertoldi and Federico, 2009) but ground breaking success is still missing. In this paper we report our findings in dynamic model adaptation using cache-based techniques when applying a standard model to the task of translating documents from a very different domain. The remaining part of the paper is organized as follows: First, we will motivate the chosen approach by reviewing the general phenomenon of 8 Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 8–15, c Uppsala, Sweden, 15 July 2010. 2010 Association for Computational Linguistics “They may also have episodes o"
W10-2602,W07-0722,0,0.011915,"erent to the training data. Statistical machine translation is no exception. Despite its popularity, standard SMT approaches fail to provide a framework for general application across domains unless appropriate training data is available and used in parameter estimation and tuning. The main problem is the general assumption of independent and identically distributed (i.i.d.) variables in machine learning approaches applied in the estimation of static global models. Recently, there has been quite some attention to the problem of domain switching in SMT (Zhao et al., 2004; Ueffing et al., 2007; Civera and Juan, 2007; Bertoldi and Federico, 2009) but ground breaking success is still missing. In this paper we report our findings in dynamic model adaptation using cache-based techniques when applying a standard model to the task of translating documents from a very different domain. The remaining part of the paper is organized as follows: First, we will motivate the chosen approach by reviewing the general phenomenon of 8 Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 8–15, c Uppsala, Sweden, 15 July 2010. 2010 Association for Computational Linguistics"
W10-2602,W08-0334,0,0.0156103,"es in case domain-specific training data is available. It has been shown that small(er) amounts of in-domain data are sufficient for such an approach (Koehn and Schroeder, 2007). However, this is not really a useful alternative for truly open-domain systems, which will be confronted with changing domains all the time including many new, previously unknown ones among them. There are also some interesting approaches to dynamic domain adaptation mainly using flexible mixture models or techniques for the automatic selection of appropriate resources (Hildebrand et al., 2005; Foster and Kuhn, 2007; Finch and Sumita, 2008). Ideally, a system would adjust itself to the current context (and thus to the current domain) without the need of explicit topic mixtures. Therefore, we like to investigate techniques for general context adaptation and their use in out-of-domain translation. There are two types of properties in natural language and translation that we like to explore. First of all, repetition is very common – much more than standard stochastic language models would predict. This is especially true for content words. See, for instance, the sample of a medical document shown in figure 1. Many content words are"
W10-2602,W07-0717,0,0.0566427,"ply supervised techniques in case domain-specific training data is available. It has been shown that small(er) amounts of in-domain data are sufficient for such an approach (Koehn and Schroeder, 2007). However, this is not really a useful alternative for truly open-domain systems, which will be confronted with changing domains all the time including many new, previously unknown ones among them. There are also some interesting approaches to dynamic domain adaptation mainly using flexible mixture models or techniques for the automatic selection of appropriate resources (Hildebrand et al., 2005; Foster and Kuhn, 2007; Finch and Sumita, 2008). Ideally, a system would adjust itself to the current context (and thus to the current domain) without the need of explicit topic mixtures. Therefore, we like to investigate techniques for general context adaptation and their use in out-of-domain translation. There are two types of properties in natural language and translation that we like to explore. First of all, repetition is very common – much more than standard stochastic language models would predict. This is especially true for content words. See, for instance, the sample of a medical document shown in figure"
W10-2602,H92-1045,0,0.373088,"Missing"
W10-2602,2005.eamt-1.19,0,0.0143295,"mpirical systems is to apply supervised techniques in case domain-specific training data is available. It has been shown that small(er) amounts of in-domain data are sufficient for such an approach (Koehn and Schroeder, 2007). However, this is not really a useful alternative for truly open-domain systems, which will be confronted with changing domains all the time including many new, previously unknown ones among them. There are also some interesting approaches to dynamic domain adaptation mainly using flexible mixture models or techniques for the automatic selection of appropriate resources (Hildebrand et al., 2005; Foster and Kuhn, 2007; Finch and Sumita, 2008). Ideally, a system would adjust itself to the current context (and thus to the current domain) without the need of explicit topic mixtures. Therefore, we like to investigate techniques for general context adaptation and their use in out-of-domain translation. There are two types of properties in natural language and translation that we like to explore. First of all, repetition is very common – much more than standard stochastic language models would predict. This is especially true for content words. See, for instance, the sample of a medical do"
W10-2602,C04-1059,0,0.0143441,"e when testing on data from a domain different to the training data. Statistical machine translation is no exception. Despite its popularity, standard SMT approaches fail to provide a framework for general application across domains unless appropriate training data is available and used in parameter estimation and tuning. The main problem is the general assumption of independent and identically distributed (i.i.d.) variables in machine learning approaches applied in the estimation of static global models. Recently, there has been quite some attention to the problem of domain switching in SMT (Zhao et al., 2004; Ueffing et al., 2007; Civera and Juan, 2007; Bertoldi and Federico, 2009) but ground breaking success is still missing. In this paper we report our findings in dynamic model adaptation using cache-based techniques when applying a standard model to the task of translating documents from a very different domain. The remaining part of the paper is organized as follows: First, we will motivate the chosen approach by reviewing the general phenomenon of 8 Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 8–15, c Uppsala, Sweden, 15 July 2010. 20"
W10-2602,W07-0733,0,0.0744998,"Missing"
W10-2602,P07-2045,0,0.00772876,"te a close integration of the caching procedure into the decoding process of fully automatic translation. For this, we fill our cache with translation options used in the best (final) translation Experiments Our experiments are focused on the unsupervised dynamic adaptation of language and translation models to a new domain using the cache-based mixture models as described above. We apply these techniques to a standard task of translating French to English using a model trained on the publicly available Europarl corpus (Koehn, 2005) using standard settings and tools such as the Moses toolkit (Koehn et al., 2007), GIZA++ (Och and Ney, 2003) and SRILM (Stolcke, 2002). The log-linear model is then tuned as usual with minimum error rate training (Och, 2003) on a separate development set coming from the same domain (Europarl). We modified SRILM to include a decaying cache model and implemented the phrase translation cache within the Moses decoder. Furthermore, we added the caching procedures and other features for testing the adaptive approach. Now we can simply switch the cache models on or off using additional command-line arguments when running Moses as usual. 4.1 Experimental Setup For testing we chos"
W10-2602,2005.mtsummit-papers.11,0,0.0365712,"translation and the source language input. In our experiments we investigate a close integration of the caching procedure into the decoding process of fully automatic translation. For this, we fill our cache with translation options used in the best (final) translation Experiments Our experiments are focused on the unsupervised dynamic adaptation of language and translation models to a new domain using the cache-based mixture models as described above. We apply these techniques to a standard task of translating French to English using a model trained on the publicly available Europarl corpus (Koehn, 2005) using standard settings and tools such as the Moses toolkit (Koehn et al., 2007), GIZA++ (Och and Ney, 2003) and SRILM (Stolcke, 2002). The log-linear model is then tuned as usual with minimum error rate training (Och, 2003) on a separate development set coming from the same domain (Europarl). We modified SRILM to include a decaying cache model and implemented the phrase translation cache within the Moses decoder. Furthermore, we added the caching procedures and other features for testing the adaptive approach. Now we can simply switch the cache models on or off using additional command-line"
W10-2602,W04-3225,0,0.019343,"nguage models to discriminate between various translation candidates. Therefore, successfully applying these adaptive language models in SMT is surprisingly difficult (Raab, 2007) especially due to the risk of adding noise (leading to error propagation) and corrupting local dependencies. In SMT another type of adaptation can be applied: cache-based adaptation of the translation model. Here, not only the repetition of content words is supported but also the consistency of translations as discussed earlier. This technique has already been tried in the context of interactive machine translation (Nepveu et al., 2004) in which cache features are introduced to adapt both the language model and the translation model. However, in their model they require an automatic alignment of words in the user edited translation and the source language input. In our experiments we investigate a close integration of the caching procedure into the decoding process of fully automatic translation. For this, we fill our cache with translation options used in the best (final) translation Experiments Our experiments are focused on the unsupervised dynamic adaptation of language and translation models to a new domain using the ca"
W10-2602,J03-1002,0,0.00981989,"caching procedure into the decoding process of fully automatic translation. For this, we fill our cache with translation options used in the best (final) translation Experiments Our experiments are focused on the unsupervised dynamic adaptation of language and translation models to a new domain using the cache-based mixture models as described above. We apply these techniques to a standard task of translating French to English using a model trained on the publicly available Europarl corpus (Koehn, 2005) using standard settings and tools such as the Moses toolkit (Koehn et al., 2007), GIZA++ (Och and Ney, 2003) and SRILM (Stolcke, 2002). The log-linear model is then tuned as usual with minimum error rate training (Och, 2003) on a separate development set coming from the same domain (Europarl). We modified SRILM to include a decaying cache model and implemented the phrase translation cache within the Moses decoder. Furthermore, we added the caching procedures and other features for testing the adaptive approach. Now we can simply switch the cache models on or off using additional command-line arguments when running Moses as usual. 4.1 Experimental Setup For testing we chose to use documents from the"
W10-2602,P03-1021,0,0.00735734,"tions used in the best (final) translation Experiments Our experiments are focused on the unsupervised dynamic adaptation of language and translation models to a new domain using the cache-based mixture models as described above. We apply these techniques to a standard task of translating French to English using a model trained on the publicly available Europarl corpus (Koehn, 2005) using standard settings and tools such as the Moses toolkit (Koehn et al., 2007), GIZA++ (Och and Ney, 2003) and SRILM (Stolcke, 2002). The log-linear model is then tuned as usual with minimum error rate training (Och, 2003) on a separate development set coming from the same domain (Europarl). We modified SRILM to include a decaying cache model and implemented the phrase translation cache within the Moses decoder. Furthermore, we added the caching procedures and other features for testing the adaptive approach. Now we can simply switch the cache models on or off using additional command-line arguments when running Moses as usual. 4.1 Experimental Setup For testing we chose to use documents from the medical domain coming from the EMEA corpus that is part of the freely available collection of parallel corpora OPUS2"
W10-3304,P05-1074,0,0.0424916,"irs that are output by the system and evaluated these on the test set described in Subsection 4.1. The method is composed of two main steps. In the ﬁrst step candidate terms are extracted from the corpus using a PoS ﬁlter, that is similar to the PoS ﬁlter we applied. In the second step pairs of candidate term variations are re-ranked on the basis of information from the Web. Phrasal patterns such as XorY are used to get synonym compatibility hits as opposed to XandY that points to non-synonymous terms. The second method we compare with is the phrase-based translation method ﬁrst introduced by Bannard and Callison-Burch (2005). Statistical word alignment can be used to measure the relation between source language items. Here, one makes use of the estimated translation likelihoods of phrases (p(f |e) and p(e|f )) that are used to build translation models in standard phrase-based statistical machine translation systems (Koehn et al., 2007). Bannard and Callison-Burch (2005) deﬁne the problem of paraphrasing as the following search problem: eˆ2 ≈ argmaxe2 :e2 =e1  f C fC p(fC |e1 )p(e2 |fC ) This is the approach that we also adapted for our comparison. The only difference in our implementation is that we applied a P"
W10-3304,J93-2003,0,0.0172852,"Missing"
W10-3304,D08-1021,0,0.148398,"relations such synonymy and hyponymy are detected based on intersection and inclusion among feature sets . Improving the syntax-based approach for synonym identiﬁcation using bilingual dictionaries has been discussed in Lin et al. (2003) and Wu and Zhou (2003). In the latter parallel corpora are also applied as a reference to assign translation likelihoods to candidates derived from the dictionary. Both of them are limited to single-word terms. Some researchers employ multilingual corpora for the automatic acquisition of paraphrases (Shimota and Sumita, 2002; Bannard and CallisonBurch, 2005; Callison-Burch, 2008). The last two are based on automatic word alignment as is our approach. Bannard and Callison-Burch (2005) use a method that is also rooted in phrase-based statistical machine translation. Translation probabilities provide a ranking of candidate paraphrases. These are reﬁned by taking contextual information into account in the form of a language model. The Europarl corpus (Koehn, 2005) is used. It has about 30 million words per language. 46 English phrases are selected as a test set for manual evaluation by two judges. When using automatic alignment, the precision reached without using context"
W10-3304,W02-0808,0,0.0358915,"ned to the English text and vice versa (dotted lines versus continuous lines). The alignment models produced are asymmetric. Several heuristics exist to combine directional word alignments which is usually called “symmetrization”. In order to cover multi-word terms standard phrase extraction techniques can be used to move from word alignment to linked phrases (see section 3.2 for more details). 2.2 DE Katze 17 2.3 Related work Multilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as separation of senses (Resnik and Yarowsky, 1997; Dyvik, 1998; Ide et al., 2002). However, taking sense separation as a basis, Dyvik (2002) derives relations such as synonymy and hyponymy by applying the method of semantic mirrors. The paper illustrates how the method works. First, different senses are identiﬁed on the basis of manual word translations in sentence-aligned Norwegian-English data (2,6 million words in total). Second, senses are grouped in semantic ﬁelds. Third, features are Measures for computing similarity Translational co-occurrence vectors are used to ﬁnd distributionally similar words. For ease of 30 assigned on the basis of inheritance. Lastly, semanti"
W10-3304,2005.mtsummit-papers.11,0,0.0495405,"Missing"
W10-3304,2006.jeptalnrecital-invite.2,0,0.035756,"Missing"
W10-3304,P98-2127,0,0.124418,"ann@lingfil.uu.se Abstract tions. These term variations could be used to enhance existing medical ontologies for the Dutch language. Our technique builds on the distributional hypothesis, the idea that semantically related words are distributed similarly over contexts (Harris, 1968). This is in line with the Firthian saying that, ’You shall know a word by the company it keeps.’ (Firth, 1957). In other words, you can grasp the meaning of a word by looking at its contexts. Context can be deﬁned in many ways. Previous work has been mainly concerned with the syntactic contexts a word is found in (Lin, 1998; Curran, 2003). For example, the verbs that are in a subject relation with a particular noun form a part of its context. In accordance with the Firthian tradition these contexts can be used to determine the semantic relatedness of words. For instance, words that occur in a object relation with the verb to drink have something in common: they are liquid. Other work has been concerned with the bagof-word context, where the context of a word are the words that are found in its proximity (Wilks et al., 1993; Sch¨utze, 1992). Yet another context, that is much less studied, is the translational con"
W10-3304,shimohata-sumita-2002-automatic,0,0.0470174,"Missing"
W10-3304,P06-2111,1,0.88343,"Missing"
W10-3304,W03-1610,0,\N,Missing
W10-3304,P07-2045,0,\N,Missing
W10-3304,C98-2122,0,\N,Missing
W10-3304,L08-1000,0,\N,Missing
W11-2144,S10-1021,0,0.0238005,"ds of texts. Anaphora can be local to a sentence, or it can cross sentence boundaries. Standard SMT methods do not handle this phenomenon in a satisfactory way at present: For sentence-internal anaphora, they depend on the n-gram language model with its limited history, while cross-sentence anaphora is left to chance. We therefore added a word-dependency model (Hardmeier and Federico, 2010) to our system to handle anaphora explicitly. Our processing of anaphoric pronouns follows the procedure outlined by Hardmeier and Federico (2010). We use the open-source coreference resolution system BART (Broscheit et al., 2010) to link pronouns to their antecedents in the text. Coreference links are handled differently depending on whether or not they cross sentence boundaries. If a coreference link points to a previous sentence, we process the sentence containing the antecedent with the SMT system and look up the translation of the antecedent in the translated output. If the coreference link is sentence-internal, the translation lookup is done dynamically by the decoder during search. In either case, the word-dependency model adds a feature function to the decoder score representing the probability of a particular"
W11-2144,de-marneffe-etal-2006-generating,0,0.0128364,"Missing"
W11-2144,D08-1089,0,0.301274,"ench Our submission to the English-French task was a phrase-based Statistical Machine Translation based on the Moses decoder (Koehn et al., 2007). Phrase tables were separately trained on Europarl, news commentary and UN data and then linearly interpolated with uniform weights. For language modelling, we used 5-gram models trained with the IRSTLM toolkit (Federico et al., 2008) on the monolingual News corpus and parts of the English-French 109 corpus. More unusual features of our system included a special component to handle pronominal anaphora and the hierarchical lexical reordering model by Galley and Manning (2008). Selected features of our system will be discussed in depth in the following sections. 1.1 Handling pronominal anaphora Pronominal anaphora is the use of pronominal expressions to refer to “something previously mentioned in the discourse” (Strube, 2006). It is a very common phenomenon found in almost all kinds of texts. Anaphora can be local to a sentence, or it can cross sentence boundaries. Standard SMT methods do not handle this phenomenon in a satisfactory way at present: For sentence-internal anaphora, they depend on the n-gram language model with its limited history, while cross-sentenc"
W11-2144,P07-2053,0,0.0307365,"Missing"
W11-2144,2010.iwslt-papers.10,1,0.918745,"ing sections. 1.1 Handling pronominal anaphora Pronominal anaphora is the use of pronominal expressions to refer to “something previously mentioned in the discourse” (Strube, 2006). It is a very common phenomenon found in almost all kinds of texts. Anaphora can be local to a sentence, or it can cross sentence boundaries. Standard SMT methods do not handle this phenomenon in a satisfactory way at present: For sentence-internal anaphora, they depend on the n-gram language model with its limited history, while cross-sentence anaphora is left to chance. We therefore added a word-dependency model (Hardmeier and Federico, 2010) to our system to handle anaphora explicitly. Our processing of anaphoric pronouns follows the procedure outlined by Hardmeier and Federico (2010). We use the open-source coreference resolution system BART (Broscheit et al., 2010) to link pronouns to their antecedents in the text. Coreference links are handled differently depending on whether or not they cross sentence boundaries. If a coreference link points to a previous sentence, we process the sentence containing the antecedent with the SMT system and look up the translation of the antecedent in the translated output. If the coreference li"
W11-2144,P06-1063,0,0.0313791,"Missing"
W11-2144,2005.iwslt-1.8,0,0.214486,"Missing"
W11-2144,P07-2045,1,0.0116502,"at WMT 2011. We created two largely independent systems for English-to-French and Haitian Creole-toEnglish translation to evaluate different features and components from our ongoing research on these language pairs. Key features of our systems include anaphora resolution, hierarchical lexical reordering, data selection for language modelling, linear transduction grammars for word alignment and syntaxbased decoding with monolingual dependency information. 1 English to French Our submission to the English-French task was a phrase-based Statistical Machine Translation based on the Moses decoder (Koehn et al., 2007). Phrase tables were separately trained on Europarl, news commentary and UN data and then linearly interpolated with uniform weights. For language modelling, we used 5-gram models trained with the IRSTLM toolkit (Federico et al., 2008) on the monolingual News corpus and parts of the English-French 109 corpus. More unusual features of our system included a special component to handle pronominal anaphora and the hierarchical lexical reordering model by Galley and Manning (2008). Selected features of our system will be discussed in depth in the following sections. 1.1 Handling pronominal anaphora"
W11-2144,J93-2004,0,0.0368938,"he overall best scores in both translation directions. The fact that both alignments lead to complementary information can be seen in the size of the phrase tables extracted (see table 3). 2.2 Syntax-based SMT We used Moses and its syntax-mode for our experiments with hierarchical phrase-based and syntaxaugmented models. Our main interest was to investigate the influence of monolingual parsing on the translation performance. In particular, we tried to integrate English dependency parses created by MaltParser (Nivre et al., 2007) trained on the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) extended with about 4000 questions 3 We actually swapped the development set and the test set by mistake. But, of course, we never mixed development and test data in any result reported. null from the Question Bank (Judge et al., 2006). The conversion to dependency trees was done using the Stanford Parser (de Marneffe et al., 2006). Again, we ran both translation directions to test our settings in more than just one task. Interesting here is also the question whether there are significant differences when integrating monolingual parses on the source or on the target side. The motivation for a"
W11-2144,J03-1002,0,0.00544101,"MT 2011 to build a large scale-background language model. The English data from the Haitian Creole task were used as a separate domain-specific language model. For the other translation direction we only used the in-domain data provided. We used standard 5-gram models with Witten-Bell discounting and backoff interpolation for all language models. For the translation model we applied standard techniques and settings for phrase extraction and score estimations. However, we applied two different systems for word alignment: One is the standard GIZA++ toolbox implementing the IBM alignment models (Och and Ney, 2003) and extensions and the other is based on transduction grammars which will briefly be introduced in the next section. 2.1.1 Alignment with PLITGs By making the assumption that the parallel corpus constitutes a linear transduction (Saers, 2011)2 we can induce a grammar that is the most likely to have generated the observed corpus. The grammar induced will generate a parse forest for each sentence pair in the corpus, and each parse tree in that forest will correspond to an alignment between the two sentences. Following Saers et al. (2010), the alignment corresponding to the best parse can be ext"
W11-2144,2011.eamt-1.42,1,0.822224,"e a grammar that is the most likely to have generated the observed corpus. The grammar induced will generate a parse forest for each sentence pair in the corpus, and each parse tree in that forest will correspond to an alignment between the two sentences. Following Saers et al. (2010), the alignment corresponding to the best parse can be extracted and used instead of other word alignment approaches such as GIZA++. There are several grammar types that generate linear transductions, and in this work, stochastic bracketing preterminalized linear inversion transduction grammars (PLITG) were used (Saers and Wu, 2011). Since we were mainly interested in the word alignments, we did not induce phrasal grammars. Although alignments from PLITGs may not reach the same level of translation quality as GIZA++, they make different mistakes, so both complement 2A transduction is a set of pairs of strings, and thus represents a relation between two languages. 375 each other. By duplicating the training corpus and aligning each copy of the corpus with a different alignment tool, the phrase extractor seems to be able to pick the best of both worlds, producing a phrase table that is superior to one produced with either"
W11-2144,N10-1050,1,0.845828,"dard GIZA++ toolbox implementing the IBM alignment models (Och and Ney, 2003) and extensions and the other is based on transduction grammars which will briefly be introduced in the next section. 2.1.1 Alignment with PLITGs By making the assumption that the parallel corpus constitutes a linear transduction (Saers, 2011)2 we can induce a grammar that is the most likely to have generated the observed corpus. The grammar induced will generate a parse forest for each sentence pair in the corpus, and each parse tree in that forest will correspond to an alignment between the two sentences. Following Saers et al. (2010), the alignment corresponding to the best parse can be extracted and used instead of other word alignment approaches such as GIZA++. There are several grammar types that generate linear transductions, and in this work, stochastic bracketing preterminalized linear inversion transduction grammars (PLITG) were used (Saers and Wu, 2011). Since we were mainly interested in the word alignments, we did not induce phrasal grammars. Although alignments from PLITGs may not reach the same level of translation quality as GIZA++, they make different mistakes, so both complement 2A transduction is a set of"
W11-2144,C08-1144,0,0.0330283,"lish test set with (=malt) or without (=hiero) English parse trees and various parse relaxation strategies. The final system submitted to WMT11 is malt(target)-samt2. rule extraction is based on tree manipulation and relaxed extraction algorithms. Moses implements several algorithms that have been proposed in the literature. Tree binarisation is one of them. This can be done in a left-branching and in a right-branching mode. We used a combination of both in the settings denoted as binarised. The other relaxation algorithms are based on methods proposed for syntaxaugmented machine translation (Zollmann et al., 2008). We used two of them: samt1 combines pairs of neighbouring children nodes into combined complex nodes and creates additional complex nodes of all children nodes except the first child and similar complex nodes for all but the last child. samt2 combines any pair of neighbouring nodes even if they are not children of the same parent. All of these relaxation algorithms lead to increased rule sets (table 4). In terms of translation performance there seems to 377 be a strong correlation between rule table size and translation quality as measured by BLEU. None of the dependency-based models beats t"
W12-3112,W12-3102,0,0.16462,"Missing"
W12-3112,gimenez-marquez-2004-svmtool,0,0.0199658,"Missing"
W12-3112,P07-2053,0,0.040547,"Missing"
W12-3112,2011.eamt-1.32,1,0.932815,"ystem, along with the models used and diagnostic output produced by the SMT system as well as manual translation quality annotations on a 1–5 scale for each sentence. Additionally, a set of 17 baseline features was made available to the participants. Systems were evaluated on a test set of 422 sentences annotated in the same way. Uppsala University submitted two systems to this shared task. Our systems were fairly successful and achieved results that were outperformed by only one competing group. They improve over the baseline performance in two ways, building on and extending earlier work by Hardmeier (2011), on which the system description in the following sections is partly based: On the one hand, we enhance the set of 17 baseline features provided by the organisers with another 82 explicitly defined features. On the other hand, we use syntactic tree kernels to extract implicit features from constituency and dependency parse trees over the input sentences and the Machine Translation (MT) output. The experimental results confirm the findings of our earlier work, showing tree kernels to be a valuable tool for rapid prototyping of QE systems. 2 Features Our QE systems used two types of features: O"
W12-3112,W10-2910,0,0.0230381,"al., 2006). POS tagging was done with HunPOS (Hal´acsy et al., 2007) for English and SVMTool (Gim´enez and M´arquez, 2004) for Spanish, with the models provided by the OPUS project (Tiedemann, 2009). As in previous work (Hardmeier, 2011), we treated the parser as a black box and made no attempt to handle the fact that parsing accuracy may be decreased over malformed SMT output. To be used with tree kernels, the output of the dependency parser had to be transformed into a single tree structure with a unique label per node and unlabelled edges, similar to a constituency parse tree. We followed Johansson and Moschitti (2010) in using a tree representation which encodes partof-speech tags, dependency relations and words as sequences of child nodes (see fig. 1). Figure 1: Representation of the dependency tree fragment VP S for the words Nicole ’s dad NP Mary brought NP V brought NP V VP N D N D N D N a cat a cat 2006b). Predicted scores less than 1 were set to 1 and predicted scores greater than 5 were set to 5 as this was known to be the range of valid scores. Our learning algorithm had some free hyperparameters. Three of them were optimised by joint grid search with 5-fold cross-validation over the training set:"
W12-3112,P03-1054,0,0.00290698,"with 1 : 1, 1 : n, n : 1 and m : n alignments (10 features) • average number of translations per word, unweighted and weighted by word frequency and reciprocal word frequency (3 features) 110 Parse trees Both the English input text and the Spanish Machine Translations were annotated with syntactic parse trees from which to derive implicit features. In English, we were able to produce both constituency and dependency parses. In Spanish, we were limited to dependency parses because of the better availability of parsing models. English constituency parses were produced with the Stanford parser (Klein and Manning, 2003) using the model bundled with the parser. For dependency parsing, we used MaltParser (Nivre et al., 2006). POS tagging was done with HunPOS (Hal´acsy et al., 2007) for English and SVMTool (Gim´enez and M´arquez, 2004) for Spanish, with the models provided by the OPUS project (Tiedemann, 2009). As in previous work (Hardmeier, 2011), we treated the parser as a black box and made no attempt to handle the fact that parsing accuracy may be decreased over malformed SMT output. To be used with tree kernels, the output of the dependency parser had to be transformed into a single tree structure with a"
W12-3112,E06-1015,0,0.689341,"D N N D D N N brought D nomial instead. The improvement an plan over the Gaussian NP NP NP a a a cat cat a a cat kernel was, however, marginal. N D N D direct stock purchase a cat N D V N A tree and some of its Partial Tree Fragments 3.3 Tree kernels To exploit parse tree information in our Machine Fig. 3. A tree with some of its partial trees Fig. 4. A dependency tree of a question. Figure 2: Tree fragments extracted by the Subset Tree Learning (ML) component, we used tree kernel (PTs). Kernel and by the Partial Tree Kernel. Illustrations by functions. Tree kernels (Collins and Duffy, 2001) Moschitti (2006a). are kernel functions defined over pairs of tree structures. They measure the similarity between two trees counting form the number of common substructures. 3 Machine Learning component constraint over the SSTs, we obtain a morebygeneral of substructures that we Implicitly, they define an infinite-dimensional feacall by the application of partial 3.1 partial Overviewtrees (PTs). These can be generated ture space whose dimensions correspond to all posproduction of the consequently [VP [V]] and are [VP The QE sharedrules task asked bothgrammar, for an estimate of sible tree fragments. Feature"
W12-3112,2009.mtsummit-papers.16,0,0.012036,"he complete set of 17 baseline features provided by the workshop organisers. Additionally, the UU best system also contained all the features presented by Hardmeier (2011) with the exception 109 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 109–113, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics of a few features specific to the film subtitle genre and inapplicable to the text type of the shared task, as well as a small number of features not included in that work. Many of these features were modelled on QE features described by Specia et al. (2009). In particular, the following features were included in addition to the baseline feature set: • number of words, length ratio (4 features) • source and target type-token ratios (2 features) • number of tokens matching particular patterns (3 features each): – – – – – – – – – – numbers opening and closing parentheses strong punctuation signs weak punctuation signs ellipsis signs hyphens single and double quotes apostrophe-s tokens short alphabetic tokens (≤ 3 letters) long alphabetic tokens (≥ 4 letters) • translation model entropy for the input words, cumulatively per sentence and averaged per"
W12-3112,nivre-etal-2006-maltparser,1,\N,Missing
W13-2229,J03-1002,0,0.0232763,"Missing"
W13-2229,D12-1108,1,0.797773,"n of the search space. While this decoding approach delivers excellent search performance at a very reasonable speed, it limits the information available to the feature models to an n-gram window similar to a language model history. In stack decoding, it is difficult to implement models with sentence-internal long-range dependencies and cross-sentence dependencies, where the model score of a given sentence depends on the translations generated for another sentence. In contrast to this very popular stack decoding approach, our decoder Docent implements a search procedure based on local search (Hardmeier et al., 2012). At any stage of the search process, its search state consists of a complete document translation, making it easy for feature models to access the complete document with its current translation at any point in time. The search algorithm is a stochastic variant of standard hill climbing. At each step, it generates a successor of the current search state by randomly applying We describe the Uppsala University system for WMT13, for English-to-German translation. We use the Docent decoder, a local search decoder that translates at the document level. We add tunable distortion limits, that is, sof"
W13-2229,P03-1021,0,0.00789271,"he translation models were trained using the Moses toolkit (Koehn et al., 2007), with standard settings with 5 features, phrase probabilities and lexical weighting in both directions and a phrase penalty. We applied significance-based filtering (Johnson et al., 2007) to the resulting phrase tables. For decoding we used the Docent decoder with random initialization and standard parameter settings (Hardmeier et al., 2012; Hardmeier et al., 2013), which beside translation and language model features include a word penalty and a distortion penalty. Parameter optimization was performed using MERT (Och, 2003) at the document-level (Stymne et al., 2013a). In this setup we calculate both model and metric scores on the document-level instead of on the sentence-level. We produce kbest lists by sampling from the decoder. In each optimization run we run 40,000 hill-climbing iterations of the decoder, and sample translations with interval 100, from iteration 10,000. This procedure has been shown to give competitive results to standard tuning with Moses (Koehn et al., 2007) with relatively stable results (Stymne et al., 2013a). For tuning data we concatenated the tuning sets news-test 2008–2010 and newssy"
W13-2229,P02-1040,0,0.0863828,"y the WMT13 workshop. We always concatenated the two bilingual corpora Europarl and News Commentary, which we will call EP-NC. We pre-processed all corpora by using the tools provided for tokenization and we also lower-cased all corpora. For the bilingual corpora we also filtered sentence pairs with a length ratio larger than three, or where either sentence was longer than 60 tokens. Recasing was performed as a post-processing step, trained using the resources To evaluate our system we use newstest2012, which has 99 documents and 3003 sentences. In this article we give lower-case Bleu scores (Papineni et al., 2002), except in Section 6 where we investigate the effect of different recasing models. 226 Cleaning None Basic Langid Alignment-based Sentences 2,399,123 2,271,912 2,072,294 1,512,401 Reduction sentences into four categories, cases where both languages were correctly identified, but under the confidence threshold of 0.999, cases where both languages were incorrectly identified, and cases where one language was incorrectly identified. Overall the language identification was accurate on 54 of the 93 removed sentences. In 18 of the cases where it was wrong, the sentences were not translation corresp"
W13-2229,P13-4033,1,0.906674,"at the document level. We add tunable distortion limits, that is, soft constraints on the maximum distortion allowed, to Docent. We also investigate cleaning of the noisy Common Crawl corpus. We show that we can use alignment-based filtering for cleaning with good results. Finally we investigate effects of corpus selection for recasing. 1 The Docent Decoder Introduction In this paper we present the Uppsala University submission to WMT 2013. We have submitted one system, for translation from English to German. In our submission we use the document-level decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013). In the current setup, we take advantage of Docent in that we introduce tunable distortion limits, that is, modeling distortion limits as soft constraints instead of as hard constraints. In addition we perform experiments on corpus cleaning. We investigate how the noisy Common Crawl corpus can be cleaned, and suggest an alignmentbased cleaning method, which works well. We also investigate corpus selection for recasing. In Section 2 we introduce our decoder, Docent, followed by a general system description in Section 3. In Section 4 we describe our experiments with corpus cleaning, and in Sect"
W13-2229,E12-1055,0,0.0123678,"ined two separate models, one on the German side of EP-NC, and one on the monolingual News corpus. In both cases we trained 5-gram models. For the large News corpus we used entropy-based pruning, with 10−8 as a threshold (Stolcke, 1998). The language models were trained using the SRILM toolkit (Stolcke, 2002) and during decoding we used the KenLM toolkit (Heafield, 2011). For the translation model we also trained two models, one with EP-NC, and one with Common Crawl. These two models were interpolated and used as a single model at decoding time, based on perplexity minimization interpolation (Sennrich, 2012), see details in Section 4. The translation models were trained using the Moses toolkit (Koehn et al., 2007), with standard settings with 5 features, phrase probabilities and lexical weighting in both directions and a phrase penalty. We applied significance-based filtering (Johnson et al., 2007) to the resulting phrase tables. For decoding we used the Docent decoder with random initialization and standard parameter settings (Hardmeier et al., 2012; Hardmeier et al., 2013), which beside translation and language model features include a word penalty and a distortion penalty. Parameter optimizati"
W13-2229,W11-2123,0,0.0206337,"-internal reordering by exploiting the fact that Docent implements distortion limits as soft constraints rather than strictly enforced limitations. We do not include any of our document-level feature functions. 3 For the language model we trained two separate models, one on the German side of EP-NC, and one on the monolingual News corpus. In both cases we trained 5-gram models. For the large News corpus we used entropy-based pruning, with 10−8 as a threshold (Stolcke, 1998). The language models were trained using the SRILM toolkit (Stolcke, 2002) and during decoding we used the KenLM toolkit (Heafield, 2011). For the translation model we also trained two models, one with EP-NC, and one with Common Crawl. These two models were interpolated and used as a single model at decoding time, based on perplexity minimization interpolation (Sennrich, 2012), see details in Section 4. The translation models were trained using the Moses toolkit (Koehn et al., 2007), with standard settings with 5 features, phrase probabilities and lexical weighting in both directions and a phrase penalty. We applied significance-based filtering (Johnson et al., 2007) to the resulting phrase tables. For decoding we used the Doce"
W13-2229,D07-1103,0,0.0298921,"lkit (Stolcke, 2002) and during decoding we used the KenLM toolkit (Heafield, 2011). For the translation model we also trained two models, one with EP-NC, and one with Common Crawl. These two models were interpolated and used as a single model at decoding time, based on perplexity minimization interpolation (Sennrich, 2012), see details in Section 4. The translation models were trained using the Moses toolkit (Koehn et al., 2007), with standard settings with 5 features, phrase probabilities and lexical weighting in both directions and a phrase penalty. We applied significance-based filtering (Johnson et al., 2007) to the resulting phrase tables. For decoding we used the Docent decoder with random initialization and standard parameter settings (Hardmeier et al., 2012; Hardmeier et al., 2013), which beside translation and language model features include a word penalty and a distortion penalty. Parameter optimization was performed using MERT (Och, 2003) at the document-level (Stymne et al., 2013a). In this setup we calculate both model and metric scores on the document-level instead of on the sentence-level. We produce kbest lists by sampling from the decoder. In each optimization run we run 40,000 hill-c"
W13-2229,N03-1017,0,0.0408954,"Missing"
W13-2229,W13-3308,1,0.864943,"of the complete document. On the downside, there is an increased risk of search errors because the document-level hill-climbing decoder cannot make as strong assumptions about the problem structure as the stack decoder does. In practice, this drawback can be mitigated by initializing the hill-climber with the output of a stack decoding pass using the baseline set of models without document-level features (Hardmeier et al., 2012). Since its inception, Docent has been used to experiment with document-level semantic language models (Hardmeier et al., 2012) and models to enhance text readability (Stymne et al., 2013b). Work on other discourse phenomena is ongoing. In the present paper, we focus on sentence-internal reordering by exploiting the fact that Docent implements distortion limits as soft constraints rather than strictly enforced limitations. We do not include any of our document-level feature functions. 3 For the language model we trained two separate models, one on the German side of EP-NC, and one on the monolingual News corpus. In both cases we trained 5-gram models. For the large News corpus we used entropy-based pruning, with 10−8 as a threshold (Stolcke, 1998). The language models were tra"
W13-2229,2005.iwslt-1.8,0,0.114614,"Missing"
W13-2229,W13-5634,1,0.871264,"of the complete document. On the downside, there is an increased risk of search errors because the document-level hill-climbing decoder cannot make as strong assumptions about the problem structure as the stack decoder does. In practice, this drawback can be mitigated by initializing the hill-climber with the output of a stack decoding pass using the baseline set of models without document-level features (Hardmeier et al., 2012). Since its inception, Docent has been used to experiment with document-level semantic language models (Hardmeier et al., 2012) and models to enhance text readability (Stymne et al., 2013b). Work on other discourse phenomena is ongoing. In the present paper, we focus on sentence-internal reordering by exploiting the fact that Docent implements distortion limits as soft constraints rather than strictly enforced limitations. We do not include any of our document-level feature functions. 3 For the language model we trained two separate models, one on the German side of EP-NC, and one on the monolingual News corpus. In both cases we trained 5-gram models. For the large News corpus we used entropy-based pruning, with 10−8 as a threshold (Stolcke, 1998). The language models were tra"
W13-2229,W08-0318,0,0.0188269,"s EP-NC-News EP-NC-CC-News EP-NC 13.8 13.9 13.9 13.9 13.9 EP-NC-CC 14.4 14.5 14.5 14.5 14.5 Language model News EP-NC-News 14.8 14.8 14.9 14.8 14.9 14.9 14.9 14.9 14.9 14.9 EP-NC-CC-News 14.8 14.8 14.9 14.9 15.0 Table 7: Case-sensitive Bleu scores with different corpus combinations for the language model and translation model (TM) for recasing Test system Docent (random) Docent (stack) Moses Docent (random) Docent (stack) Moses ing. It is common to train the system on truecased data instead of lower-cased data, which has been shown to lead to small gains for the English– German language pair (Koehn et al., 2008). In this framework there is still a need to find the correct case for the first word of each sentence, for which a similar corpus study might be useful. 7 Tuning system Docent Docent Docent Moses Moses Moses Bleu 15.7 15.9 15.9 15.9 16.8 16.8 Table 8: Bleu scores for Docent initialized randomly or with stack decoding compared to Moses. Tuning is performed with either Moses or Docent. For the top line we used tunable distortion limits 6,10 with Docent, in the other cases a standard hard distortion limit of 6, since Moses does not allow soft distortion limits. Comparison to Moses So far we have"
W13-2229,P12-3005,0,0.118617,"Missing"
W13-2229,P07-2045,0,\N,Missing
W13-3308,D11-1084,0,0.0941275,"in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing"
W13-3308,W12-3102,0,0.0373536,"Missing"
W13-3308,D07-1007,0,0.0195191,"onnectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing, 2010; Su et al., 2012). For a more thorough overview of discourse in SMT, see Hardmeier (2012). 3 Sentence-Level Tuning Traditionally, feature weight optimization, or tuning, for SMT is performed by an iterative process where a development set is translated to produce a k-best list. The parameters are then optimized using some procedure, generally to favor translations in the k-be"
W13-3308,D12-1026,0,0.234011,"ion (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing, 2010; Su et al., 2"
W13-3308,W09-2404,0,0.411772,"ng has been used to annotate surface forms either in the corpus or in the Discourse has largely been ignored in traditional machine translation (MT). Typically each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good translations of documents (Hatim and Mason, 1990). Most attempts to address discourse-level issues for statistical machine translation (SMT) have had to resort to solutions such as postprocessing to address lexical cohesion (Carpuat, 2009) or two-step translation to address pronoun anaphora (Le Nagard and Koehn, 2010). Recently, however, we presented Docent (Hardmeier et al., 2012; Hardmeier et al., 2013), a decoder based on local search that translates full documents. So far this decoder has not included a feature weight optimization framework. However, feature weight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Tr"
W13-3308,E12-3001,0,0.09251,"ic programming for exploring a large search space (Och et al., 2001). Because of the dynamic programming assumptions it is hard to directly include discourse-level features into a traditional SMT decoder. Nevertheless, there have been several attempts to integrate intersentential and long distance models for discourselevel phenomena into standard decoders, usually as ad-hoc additions to standard models, addressing a single phenomenon. Several studies have tried to improve pronoun anaphora by adding information about the antecedent, either by using two-step decoding (Le Nagard and Koehn, 2010; Guillou, 2012) or by extracting information from previously translated sentences (Hardmeier and Federico, 2010), unfortunately without any convincing results. To address the translation of discourse connectives, source-side pre-processing has been used to annotate surface forms either in the corpus or in the Discourse has largely been ignored in traditional machine translation (MT). Typically each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good tra"
W13-3308,P07-1005,0,0.016844,"ed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing, 2010; Su et al., 2012). For a more thorough overview of discourse in SMT, see Hardmeier (2012). 3 Sentence-Level Tuning Traditionally, feature weight optimization, or tuning, for SMT is performed by an iterative process where a development set is translated to produce a k-best list. The parameters are then optimized using some procedure, generally to favor translations in the k-best list that have a high score on some MT metric. The translation step"
W13-3308,2010.iwslt-papers.10,1,0.900759,"he dynamic programming assumptions it is hard to directly include discourse-level features into a traditional SMT decoder. Nevertheless, there have been several attempts to integrate intersentential and long distance models for discourselevel phenomena into standard decoders, usually as ad-hoc additions to standard models, addressing a single phenomenon. Several studies have tried to improve pronoun anaphora by adding information about the antecedent, either by using two-step decoding (Le Nagard and Koehn, 2010; Guillou, 2012) or by extracting information from previously translated sentences (Hardmeier and Federico, 2010), unfortunately without any convincing results. To address the translation of discourse connectives, source-side pre-processing has been used to annotate surface forms either in the corpus or in the Discourse has largely been ignored in traditional machine translation (MT). Typically each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good translations of documents (Hatim and Mason, 1990). Most attempts to address discourse-level issues f"
W13-3308,N12-1047,0,0.127102,"best lists. This is repeated until some end condition is satisfied, for instance for a set number of iterations, until there is only very small changes in parameter weights, or until there are no new translations in the k-best lists. SMT tuning is a hard problem in general, partly because the correct output is unreachable and also because the translation process includes latent variables, which means that many efficient standard optimization procedures cannot be used (Gimpel and Smith, 2012). Nevertheless, there are a number of techniques including MERT (Och, 2003), MIRA (Chiang et al., 2008; Cherry and Foster, 2012), PRO (Hopkins and May, 2011), and Rampion (Gimpel and Smith, 2012). All of these optimization methods can be plugged into the standard optimization loop. All of the methods work relatively well in practice, even though there are limitations, for instance that many methods are non-deterministic meaning that their results are somewhat unstable. However, there are some important differences. MERT is based on scores for the full test set, whereas the other methods are based on sentence-level scores. MERT also has the drawback that it only works well for small sets of features. In this paper we ar"
W13-3308,D12-1108,1,0.932337,"slation (MT). Typically each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good translations of documents (Hatim and Mason, 1990). Most attempts to address discourse-level issues for statistical machine translation (SMT) have had to resort to solutions such as postprocessing to address lexical cohesion (Carpuat, 2009) or two-step translation to address pronoun anaphora (Le Nagard and Koehn, 2010). Recently, however, we presented Docent (Hardmeier et al., 2012; Hardmeier et al., 2013), a decoder based on local search that translates full documents. So far this decoder has not included a feature weight optimization framework. However, feature weight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimiz"
W13-3308,D08-1024,0,0.0690584,"Missing"
W13-3308,P13-4033,1,0.695247,"each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good translations of documents (Hatim and Mason, 1990). Most attempts to address discourse-level issues for statistical machine translation (SMT) have had to resort to solutions such as postprocessing to address lexical cohesion (Carpuat, 2009) or two-step translation to address pronoun anaphora (Le Nagard and Koehn, 2010). Recently, however, we presented Docent (Hardmeier et al., 2012; Hardmeier et al., 2013), a decoder based on local search that translates full documents. So far this decoder has not included a feature weight optimization framework. However, feature weight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to i"
W13-3308,P11-2031,0,0.0276307,"ment-level feature weight optimization for SMT. We first describe the experimental setup, followed by baseline results using sentencelevel optimization. We then present validation experiments with standard sentence-level features, 1 http://www.statmt.org/wmt13/ translation-task.html 63 System Moses Docent-M Docent-R Moses Docent-M Docent-R million iterations. We show results on the Bleu (Papineni et al., 2002) and NIST (Doddington, 2002) metrics. For German–English we show the average result and standard deviation of three optimization runs, to control for optimizer instability as proposed by Clark et al. (2011). For English–Swedish we report results on single optimization runs, due to time constraints. 5.2 Bleu 17.7 17.7 15.2 (0.05) 18.3 (0.04) 18.3 (0.04) 18.1 (0.13) NIST 6.25 6.25 5.88 (0.00) 6.22 (0.01) 6.22 (0.01) 6.23 (0.01) Table 2: Baseline results, where Docent-M is initialized with Moses and Docent-R randomly Docs 111 345 100 200 100 200 300 Baselines Most importantly, we would like to show the effectiveness of the document-level tuning procedure described above. In order to do this, we created a baseline using sentence-level optimization with a tuning set of 2525 sentences and the News2009"
W13-3308,D11-1125,0,0.344821,"til some end condition is satisfied, for instance for a set number of iterations, until there is only very small changes in parameter weights, or until there are no new translations in the k-best lists. SMT tuning is a hard problem in general, partly because the correct output is unreachable and also because the translation process includes latent variables, which means that many efficient standard optimization procedures cannot be used (Gimpel and Smith, 2012). Nevertheless, there are a number of techniques including MERT (Och, 2003), MIRA (Chiang et al., 2008; Cherry and Foster, 2012), PRO (Hopkins and May, 2011), and Rampion (Gimpel and Smith, 2012). All of these optimization methods can be plugged into the standard optimization loop. All of the methods work relatively well in practice, even though there are limitations, for instance that many methods are non-deterministic meaning that their results are somewhat unstable. However, there are some important differences. MERT is based on scores for the full test set, whereas the other methods are based on sentence-level scores. MERT also has the drawback that it only works well for small sets of features. In this paper we are not concerned with the actu"
W13-3308,W12-3139,0,0.205173,"5.2 (0.05) 18.3 (0.04) 18.3 (0.04) 18.1 (0.13) NIST 6.25 6.25 5.88 (0.00) 6.22 (0.01) 6.22 (0.01) 6.23 (0.01) Table 2: Baseline results, where Docent-M is initialized with Moses and Docent-R randomly Docs 111 345 100 200 100 200 300 Baselines Most importantly, we would like to show the effectiveness of the document-level tuning procedure described above. In order to do this, we created a baseline using sentence-level optimization with a tuning set of 2525 sentences and the News2009 corpus for evaluation. Increasing the tuning set is known to give only modest improvements (Turchi et al., 2012; Koehn and Haddow, 2012). The feature weights optimized with the standard Moses decoder can then directly be used in our document-level decoder as we only include sentence-level features in our baseline model. As expected, these optimized weights also lead to a better performance in document-level decoding compared to an untuned model as shown in Table 2. Note, that Docent can be initialized in two ways, by Moses and randomly. Not surprisingly, the result for the runs initialized with Moses are identical with the pure sentence-level decoder. Initializing randomly gives a slightly lower Bleu score but with a larger va"
W13-3308,N12-1023,0,0.0116367,"repeated using the new weights for decoding, and optimization is continued on a new k-best list, or on a combination of all k-best lists. This is repeated until some end condition is satisfied, for instance for a set number of iterations, until there is only very small changes in parameter weights, or until there are no new translations in the k-best lists. SMT tuning is a hard problem in general, partly because the correct output is unreachable and also because the translation process includes latent variables, which means that many efficient standard optimization procedures cannot be used (Gimpel and Smith, 2012). Nevertheless, there are a number of techniques including MERT (Och, 2003), MIRA (Chiang et al., 2008; Cherry and Foster, 2012), PRO (Hopkins and May, 2011), and Rampion (Gimpel and Smith, 2012). All of these optimization methods can be plugged into the standard optimization loop. All of the methods work relatively well in practice, even though there are limitations, for instance that many methods are non-deterministic meaning that their results are somewhat unstable. However, there are some important differences. MERT is based on scores for the full test set, whereas the other methods are ba"
W13-3308,N03-1017,0,0.0371705,"rity of scoring and the process of extracting k-best lists. For document-level features we do not have meaningful scores on the sentence level which are required in standard optimization frameworks. Furthermore, the extraction of k-best lists is not as Here we instead choose to work with the recent document-level SMT decoder Docent (Hardmeier et al., 2012). Unlike in traditional decoding were documents are generated sentence by sentence, feature models in Docent always have access to the complete discourse context, even before decoding is finished. It implements the phrase-based SMT approach (Koehn et al., 2003) and is based on local search, where a state consists of a full translation of a document, which is improved by applying a series of operations to improve the translation. A hill-climbing strategy is used to find a (local) maximum. The operations allow changing the translation of a phrase, changing the word order by swapping the positions of two phrases, and resegmenting phrases. The initial state can either be initialized randomly in monotonic order, or be based on an initial run from a standard sentence-based decoder. The number of iterations in the decoder is controlled by two parameters, t"
W13-3308,P12-1048,0,0.0480289,"t al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing, 2010; Su et al., 2012). For a more thorough overview of discourse in SMT, see Hardmeier (2012). 3 Sentence-Level Tuning Traditionally, feature weight optimization, or tuning, for SMT is performed by an iterative process where a development set is translated to produce a k-best list. The parameters are then optimized using some procedure, generally to favor translations in the k-best list that have a high score on some MT metric. The translation step is then repeated using the new weights for decoding, and optimization is continued on a new k-best list, or on a combination of all k-best lists. This is repeated until"
W13-3308,W10-2602,1,0.888675,"shop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase t"
W13-3308,2005.mtsummit-papers.11,0,0.014426,"is already somewhat unstable this is a potential issue that needs to be explored further, which we do in Section 5. Implementation-wise we adapted Docent to output k-lists and adapted the infrastructure available for tuning in the Moses decoder (Koehn et al., 2007) to work with document-level scores. This setup allows us to use the variety of optimization procedures implemented there. 5 5.1 Experimental Setup Most of our experiments are for German-toEnglish news translation using data from the WMT13 workshop.1 We also show results with document-level features for English-to-Swedish Europarl (Koehn, 2005). The size of the training, tuning, and test sets are shown in Table 1. First of all, we need to extract documents for tuning and testing with Docent. Fortunately, the news data already contain document markup, corresponding to individual news articles. For Europarl we define a document as a consecutive sequence of utterances from a single speaker. To investigate the effect of the size of the tuning set, we used different subsets of the available tuning data. All our document-level experiments are carried out with Docent but we also contrast with the Moses decoder (Koehn et al., 2007). For the"
W13-3308,W10-1737,0,0.202315,"Missing"
W13-3308,N12-1046,0,0.199813,"el features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to int"
W13-3308,W12-0117,0,0.0131317,"ed a feature weight optimization framework. However, feature weight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has"
W13-3308,2011.mtsummit-papers.13,0,0.118068,"ts for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type o"
W13-3308,2012.amta-papers.20,0,0.335871,"eight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities f"
W13-3308,W01-1408,0,0.0344757,"res to model discourse phenomena such as anaphora, discourse connectives, and lexical consistency. In this paper, we therefore propose an approach that supports discourse-wide features in documentlevel decoding by adapting existing frameworks for sentence-level optimization. Furthermore, we include a thorough empirical investigation of this approach. Introduction 2 Discourse-Level SMT Traditional SMT systems translate texts sentence by sentence, assuming independence between sentences. This assumption allows efficient algorithms based on dynamic programming for exploring a large search space (Och et al., 2001). Because of the dynamic programming assumptions it is hard to directly include discourse-level features into a traditional SMT decoder. Nevertheless, there have been several attempts to integrate intersentential and long distance models for discourselevel phenomena into standard decoders, usually as ad-hoc additions to standard models, addressing a single phenomenon. Several studies have tried to improve pronoun anaphora by adding information about the antecedent, either by using two-step decoding (Le Nagard and Koehn, 2010; Guillou, 2012) or by extracting information from previously translat"
W13-3308,P03-1021,0,0.0673075,"est list, or on a combination of all k-best lists. This is repeated until some end condition is satisfied, for instance for a set number of iterations, until there is only very small changes in parameter weights, or until there are no new translations in the k-best lists. SMT tuning is a hard problem in general, partly because the correct output is unreachable and also because the translation process includes latent variables, which means that many efficient standard optimization procedures cannot be used (Gimpel and Smith, 2012). Nevertheless, there are a number of techniques including MERT (Och, 2003), MIRA (Chiang et al., 2008; Cherry and Foster, 2012), PRO (Hopkins and May, 2011), and Rampion (Gimpel and Smith, 2012). All of these optimization methods can be plugged into the standard optimization loop. All of the methods work relatively well in practice, even though there are limitations, for instance that many methods are non-deterministic meaning that their results are somewhat unstable. However, there are some important differences. MERT is based on scores for the full test set, whereas the other methods are based on sentence-level scores. MERT also has the drawback that it only works"
W13-3308,P02-1040,0,0.0886974,"riments, the decoder always stopped when reaching the rejection limit, usually between 1–5 Experiments In this section we report experimental results where we investigate several issues in connection with document-level feature weight optimization for SMT. We first describe the experimental setup, followed by baseline results using sentencelevel optimization. We then present validation experiments with standard sentence-level features, 1 http://www.statmt.org/wmt13/ translation-task.html 63 System Moses Docent-M Docent-R Moses Docent-M Docent-R million iterations. We show results on the Bleu (Papineni et al., 2002) and NIST (Doddington, 2002) metrics. For German–English we show the average result and standard deviation of three optimization runs, to control for optimizer instability as proposed by Clark et al. (2011). For English–Swedish we report results on single optimization runs, due to time constraints. 5.2 Bleu 17.7 17.7 15.2 (0.05) 18.3 (0.04) 18.3 (0.04) 18.1 (0.13) NIST 6.25 6.25 5.88 (0.00) 6.22 (0.01) 6.22 (0.01) 6.23 (0.01) Table 2: Baseline results, where Docent-M is initialized with Moses and Docent-R randomly Docs 111 345 100 200 100 200 300 Baselines Most importantly, we would like to sh"
W13-3308,W13-5634,1,0.917086,"an restarting the decoder from the previous best state. 62 Training Tuning Test German–English Type Sentences Europarl 1.9M News Commentary 178K News2009 2525 News2008-2010 7567 News2012 3003 Documents – – 111 345 99 English–Swedish Type Sentences Europarl 1.5M – – Europarl (Moses) 2000 Europarl (Docent) 1338 Europarl 690 Documents – – – 100 20 Table 1: Domain and number of sentences and documents for the corpora which can be compared to standard optimization. Finally, we report results with a set of documentlevel features that have been proposed for joint translation and text simplification (Stymne et al., 2013). As seen in Figure 1, there are some additional parameters in our procedure: the sample start iteration and the sample interval. We also need to set the number of decoder iterations to run. In Section 5 we empirically investigate the effect of these parameters. Compared to sentence-level optimization, we also have a smaller number of units to get scores from, since we use documents as units, and not sentences. The importance of this depends on the optimization algorithm. MERT calculates metric scores over the full tuning set, not for individual sentences, and should not be affected too much b"
W13-3308,P07-2045,0,\N,Missing
W13-5634,2012.eamt-1.33,0,0.0610232,"Missing"
W13-5634,W09-2404,0,0.0675313,"summarization, as measured by user studies (Margarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NO"
W13-5634,W12-3156,0,0.0177139,"easured by user studies (Margarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electr"
W13-5634,D07-1007,0,0.0351635,"relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 382 of 474] poetic form. They use features in the decoder such as rhyme and meter. They also introduce constraints over the target language output in order to adapt to the task-specific properties. However, they do not work on the document leve"
W13-5634,P07-1005,0,0.0350039,"bility and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 382 of 474] poetic form. They use features in the decoder such as rhyme and meter. They also introduce constraints over the target language output in order to adapt to the task-specific properties. However, they do not work on the document level, which would be an"
W13-5634,daelemans-etal-2004-automatic,0,0.0211848,"a har sagt det - EU:s möte i Lissabon lagt särskild vikt vid vår för att genomföra risk i så att den plan att bli klar under 2003. SL (high) Som ledamöterna vet vissa talare har nämnt - Europeiska rådet i Lissabon särskilt uppmärksammat främja våra ansträngningar att genomföra riskkapital så att handlingsplanen avslutas 2003. Table 5: Examples of translation output from a sample of systems sentence compression (e.g., Knight and Marcu, 2000; Specia, 2010). Furthermore, there is a wide range of publications using other methods for monolingual sentence compression and text simplification, (e.g., Daelemans et al., 2004; Cohn and Lapata, 2009). Readability has also been investigated as an effect of text summarization, as measured by user studies (Margarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexi"
W13-5634,D11-1108,0,0.0132082,"et al. (2012) investigate the task of translating subtitles where time and space constraints are important, which leads to the task of sentence compression, which is related to our work on simplifying translated texts. They introduce dynamic length penalties which they integrate in a standard SMT decoder. Their model successfully compresses subtitles on three data sets. However, they also show that a similar compression can be achieved with appropriate tuning data that meets the length constraints. There are also a number of studies that use SMT techniques for monolingual paraphrasing (e.g., Ganitkevitch et al., 2011) and Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 381 of 474] Source As the honourable Members know - some speakers have mentioned it - the European Council at Lisbon paid particular attention to promoting our efforts to implement risk capital in such a way that the action plan will be finished in 2003. Baseline Som de ärade ledamöterna vet - vissa talare har nämnt det - som Europeiska rådet i Lissabon ägnat särskild uppmärksamhet åt att främja våra ansträngningar att genomföra riskkapital på ett s"
W13-5634,D10-1016,0,0.0191157,"on metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 382 of 474] poetic form. They use features in the decoder such as rhyme and meter. They also introduce constraints over the target language output in order to adapt to the task-specific properties. However, they do not work on the document level, which would be an interesting direction for future work. 6 Conclusion a"
W13-5634,D12-1108,1,0.934115,"rt words as indicators. Our goal is to incorporate these features in machine translation in order to combine text simplification and adequate translation in one system. To the best of our knowledge, this has not been attempted before and represents a novel and challenging idea in the field of MT research. Global features such as the ones mentioned above require new approaches to the general problem of decoding in SMT. Fortunately, we have recently presented a new document-level decoder, which, contrary to standard SMT decoders, translates documents as a unit instead of sentences in isolation (Hardmeier et al., 2012). This allows us to define document-wide features in the target language to test our ideas. Our application is also a good test case for the capabilities of the decoder and we would like to use our findings in future developments of general user-targeted machine translation. The contributions of this paper are thus two-fold: (1) We show that document-wide decoding can effectively use global features and (2) we demonstrate that readability features can be used in SMT to produce simplified text translations. The remainder of the paper is organized as follows: First, we introduce important backgr"
W13-5634,2005.mtsummit-papers.11,0,0.0391844,"alue feature both on word level and on phrase level. On the phrase level we consider the phrases used by the SMT decoder, and on the word level we consider individual source words, and their alignment to 0 − N target words. TTR = Q-value = 4 C(tokens) C(types) f (st) n(s) + n(t) (7) (8) Experiments In the following, we show results for our experiments with the Docent decoder that include readability features and compare them to runs without them. The systems are evaluated using both MT and readability metrics. 4.1 Experimental Setup We evaluate our models on parliamentary texts from Europarl (Koehn, 2005), which contain both complex sentences and a lot of domain-specific terminology. All tests are performed for English–Swedish translation. Our system is trained on 1,488,322 sentences. For evaluation, we extracted 20 documents with a total of 690 sentences from a separate part of Europarl. A document is defined as a complete contiguous sequence of utterances of one speaker. We excluded documents that are shorter than 20 sentences and longer than 79 sentences. Moses (Koehn et al., 2007) was used for training the translation model and SRILM (Stolcke, 2002) for training the language model. We init"
W13-5634,P07-2045,0,0.0103962,"both MT and readability metrics. 4.1 Experimental Setup We evaluate our models on parliamentary texts from Europarl (Koehn, 2005), which contain both complex sentences and a lot of domain-specific terminology. All tests are performed for English–Swedish translation. Our system is trained on 1,488,322 sentences. For evaluation, we extracted 20 documents with a total of 690 sentences from a separate part of Europarl. A document is defined as a complete contiguous sequence of utterances of one speaker. We excluded documents that are shorter than 20 sentences and longer than 79 sentences. Moses (Koehn et al., 2007) was used for training the translation model and SRILM (Stolcke, 2002) for training the language model. We initialized our experiments with a Moses model that uses standard features of a phrase-based system: a 5-gram language model, five translation model features, a distance-based reordering penalty, and a word counter. These features were optimized using minimum error-rate training (Och, 2003) and the same weights were then used in Docent. Currently, we are developing the optimization procedure in Docent and could not use it in this work. We thus used a grid search approach for choosing weig"
W13-5634,N03-1017,0,0.0152721,"endence between the sentences in a text. This independence assumption is exploited in the most popular SMT decoding algorithms, which efficiently explore a very large search space by using dynamic programming (Och et al., 2001). Integrating discourse-wide information into traditional SMT decoders is difficult because of these dynamic programming assumptions. We therefore implement our document-level readability models in the recently published document-level SMT decoder Docent (Hardmeier et al., 2012), which does not have these limitations. The model implemented by Docent is phrase-based SMT (Koehn et al., 2003). The decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by applying a series of operations using a hill climbing strategy to find a (local) maximum of the score function. The three operations used are to change the translation of phrases, to swap the position of two phrases , and to resegment phrases. This setup is not limited by dynamic programming constraints, so we can define Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference"
W13-5634,P03-1021,0,0.0433779,"of Europarl. A document is defined as a complete contiguous sequence of utterances of one speaker. We excluded documents that are shorter than 20 sentences and longer than 79 sentences. Moses (Koehn et al., 2007) was used for training the translation model and SRILM (Stolcke, 2002) for training the language model. We initialized our experiments with a Moses model that uses standard features of a phrase-based system: a 5-gram language model, five translation model features, a distance-based reordering penalty, and a word counter. These features were optimized using minimum error-rate training (Och, 2003) and the same weights were then used in Docent. Currently, we are developing the optimization procedure in Docent and could not use it in this work. We thus used a grid search approach for choosing weights for the readability-based features with low, medium, and high impact relative to the standard features. We performed automatic evaluations using a set of common metrics for MT quality and readability. For MT quality we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Con"
W13-5634,W01-1408,0,0.0740805,"paper is organized as follows: First, we introduce important background on document-level decoding and readability. Thereafter, we present our experiments using a set of global features. Finally, we add some information about related work, summarize our findings and give ideas about future work. 2 Document-wide SMT Most current SMT systems translate sentences individually, assuming independence between the sentences in a text. This independence assumption is exploited in the most popular SMT decoding algorithms, which efficiently explore a very large search space by using dynamic programming (Och et al., 2001). Integrating discourse-wide information into traditional SMT decoders is difficult because of these dynamic programming assumptions. We therefore implement our document-level readability models in the recently published document-level SMT decoder Docent (Hardmeier et al., 2012), which does not have these limitations. The model implemented by Docent is phrase-based SMT (Koehn et al., 2003). The decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by applying a series of operations using a hill climb"
W13-5634,P02-1040,0,0.0861851,"ive translation model features, a distance-based reordering penalty, and a word counter. These features were optimized using minimum error-rate training (Och, 2003) and the same weights were then used in Docent. Currently, we are developing the optimization procedure in Docent and could not use it in this work. We thus used a grid search approach for choosing weights for the readability-based features with low, medium, and high impact relative to the standard features. We performed automatic evaluations using a set of common metrics for MT quality and readability. For MT quality we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 378 of 474] Feature Reference Baseline OVIX TTR Qw Qp nLW nXLW SL Weight – – low medium high low medium high low medium high low medium high low medium high low medium high low medium high BLEU↑ – 0.243 0.243 0.228 0.144 0.243 0.225 0.150 0.242 0.231 0.165 0.243 0.229 0.097 0.244 0.225 0.106 0.241 0.225 0.224 0.242 0.211 0.150 NIST↑ – 6.12 6.11 5.83 4.41 6.12 5.75 4.48 6.10 5.90 4.93 6.12 5.99 3.90 6.14 5.96 4.11 6.10 5.85 5"
W13-5634,W11-4627,0,0.0286822,"det i Lissabon särskilt uppmärksammat främja våra ansträngningar att genomföra riskkapital så att handlingsplanen avslutas 2003. Table 5: Examples of translation output from a sample of systems sentence compression (e.g., Knight and Marcu, 2000; Specia, 2010). Furthermore, there is a wide range of publications using other methods for monolingual sentence compression and text simplification, (e.g., Daelemans et al., 2004; Cohn and Lapata, 2009). Readability has also been investigated as an effect of text summarization, as measured by user studies (Margarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translatio"
W13-5634,P11-4010,1,0.864145,"Missing"
W13-5634,W10-2602,1,0.879769,"rgarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Pr"
W14-1614,de-marneffe-etal-2006-generating,0,0.0246402,"Missing"
W14-1614,D12-1001,0,0.257649,"can assume that SMT will produce output that is much closer to the input than manual translations in parallel texts usually are. Even if this may seem like a short-coming in general, in the case of annotation projection it should rather be an advantage, because it makes it more straightforward and less error-prone to transfer annotation from source to target. Furthermore, the alignment between words and phrases is inherently provided as an output of all common SMT models. Hence, no additional procedures have to be performed on top of the translated corpus. Recent research (Zhao et al., 2009; Durrett et al., 2012) has attempted to address synthetic data creation for syntactic parsing via bilingual lexica. We seek to build on this work by utilizing more advanced translation techniques. Further in the paper, we first describe the tools and resources used in our experiments (§2). We elaborate on our approach to translating treebanks (§3) and projecting syntactic annotations (§4) for a new language. Finally, we provide empirical evaluation of the suggested approach (§5) and observe a substantial increase in parsing accuracy over the delexicalized parsing baselines. 2 Resources and Tools In our experiments,"
W14-1614,D13-1205,0,0.0415106,"Missing"
W14-1614,D07-1097,1,0.448738,"Missing"
W14-1614,P13-2121,0,0.0129603,"Missing"
W14-1614,2005.mtsummit-papers.11,0,0.0215141,"anslation, we select the popular Moses toolbox (Koehn et al., 2007) and the phrasebased translation paradigm as our basic framework. Phrase-based SMT has the advantage of being straightforward and efficient in training and decoding, while maintaining robustness and reliability for many language pairs. More details about the setup and the translation procedures are given in Section 3 below. The most essential ingredient for translation performance is the parallel corpus used for training the translation models. For our experiments we use the freely available and widely used Europarl corpus v7 (Koehn, 2005).4 It is commonly used for training SMT models and includes parallel data for all languages represented in the Universal Treebank except Korean, which we will, therefore, leave out in our experiments. For tuning we apply the newstest 2012 data provided by the annual workshop on statistical machine translation.5 For language modeling, we use a combination of 2 http://www.maltparser.org/ http://nil.fdi.ucm.es/maltoptimizer/ 4 http://www.statmt.org/europarl/ 5 http://www.statmt.org/wmt14 3 DE DE EN ES FR SV 94 M 94 M 103 M 96 M 105 M 104 M 81 M 89 M 89 M 91 M SV 2.0 M 1.9 M 1.9 M 1.8 M 2.0 M 2.0"
W14-1614,D11-1006,0,0.540929,"Missing"
W14-1614,P80-1024,0,0.824454,"Missing"
W14-1614,P12-1066,0,0.342974,"ired. In addition, training can be performed on gold standard annotation. However, model transfer assumes a common fea130 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 130–140, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics ture representation across languages (McDonald et al., 2013), which can be a strong bottleneck. Several extensions have been proposed to make the approach more robust. First of all, multiple source languages can be involved to increase the statistical basis for learning (McDonald et al., 2011; Naseem et al., 2012), a strategy that can also be used in the case of annotation projection. Cross-lingual word clusters can be created to obtain additional universal features (T¨ackstr¨om et al., 2012). Techniques for target language adaptation can be used to improve model transfer with multiple sources (T¨ackstr¨om et al., 2013b). 1.2 The Translation Approach In this paper, we propose a third strategy, based on automatically translating training data to a new language in order to create annotated resources directly from the original source. Recent advances in statistical machine translation (SMT) combined with"
W14-1614,nivre-etal-2006-maltparser,1,0.855992,"Missing"
W14-1614,W06-2933,1,0.560504,"Missing"
W14-1614,J03-1002,0,0.00756484,"tom row = number of sentences in monolingual corpora. Europarl and News data provided from the same source. The statistics of the corpora are given in Table 1. 3 Translating Treebanks The main contribution of this paper is the empirical study of automatic treebank translation for parser transfer. We compare three different translation approaches in order to investigate the influence of several parameters. All of them are based on automatic word alignment and subsequent extraction of translation equivalents as common in phrase-based SMT. In particular, word alignment is performed using GIZA++ (Och and Ney, 2003) and IBM model 4 as the final model for creating the Viterbi word alignments for all parallel corpora used in our experiments. For the extraction of translation tables, we use the Moses toolkit with its standard settings to extract phrase tables with a maximum of seven tokens per phrase from a symmetrized word alignment. Symmetrization is done using the grow-diagfinal-and heuristics (Koehn et al., 2003). We tune phrase-based SMT models using minimum error rate training (Och, 2003) and the development data for each language pair. The language model is a standard 5-gram model estimated from the"
W14-1614,N03-1017,0,0.016743,"rs. All of them are based on automatic word alignment and subsequent extraction of translation equivalents as common in phrase-based SMT. In particular, word alignment is performed using GIZA++ (Och and Ney, 2003) and IBM model 4 as the final model for creating the Viterbi word alignments for all parallel corpora used in our experiments. For the extraction of translation tables, we use the Moses toolkit with its standard settings to extract phrase tables with a maximum of seven tokens per phrase from a symmetrized word alignment. Symmetrization is done using the grow-diagfinal-and heuristics (Koehn et al., 2003). We tune phrase-based SMT models using minimum error rate training (Och, 2003) and the development data for each language pair. The language model is a standard 5-gram model estimated from the monolingual data using modified Kneser-Ney smoothing without pruning (applying KenLM tools (Heafield et al., 2013)). Our first translation approach is based on a very simple word-by-word translation model. For this, we select the most reliable translations of single words from the phrase translation tables extracted from the parallel corpora as described above. We restrict the model to tokens with alpha"
W14-1614,P03-1021,0,0.0115556,"lation equivalents as common in phrase-based SMT. In particular, word alignment is performed using GIZA++ (Och and Ney, 2003) and IBM model 4 as the final model for creating the Viterbi word alignments for all parallel corpora used in our experiments. For the extraction of translation tables, we use the Moses toolkit with its standard settings to extract phrase tables with a maximum of seven tokens per phrase from a symmetrized word alignment. Symmetrization is done using the grow-diagfinal-and heuristics (Koehn et al., 2003). We tune phrase-based SMT models using minimum error rate training (Och, 2003) and the development data for each language pair. The language model is a standard 5-gram model estimated from the monolingual data using modified Kneser-Ney smoothing without pruning (applying KenLM tools (Heafield et al., 2013)). Our first translation approach is based on a very simple word-by-word translation model. For this, we select the most reliable translations of single words from the phrase translation tables extracted from the parallel corpora as described above. We restrict the model to tokens with alphabetic characters only using pre-defined Unicode character 132 sets. The selecti"
W14-1614,P07-2045,0,0.00757185,"Missing"
W14-1614,petrov-etal-2012-universal,0,0.209401,"Missing"
W14-1614,J96-1001,0,0.0271066,"this, we select the most reliable translations of single words from the phrase translation tables extracted from the parallel corpora as described above. We restrict the model to tokens with alphabetic characters only using pre-defined Unicode character 132 sets. The selection of translation alternatives is based on the Dice coefficient, which combines the two essential conditional translation probabilities given in the phrase table. The Dice coefficient is in fact the harmonic mean of these two probabilities and has successfully been used for the extraction of translation equivalents before (Smadja et al., 1996): 2 p(s, t) Dice(s, t) = =2 p(s) + p(t)  1 1 + p(s|t) p(t|s) O RIGINAL DE EN ES FR SV 14.0 0.00 7.90 13.3 4.20 WORD - BASED DE EN ES FR −1 SV MT DE EN ES FR SV – 43.3 54.9 68.2 34.1 49.1 – 25.1 39.6 5.20 62.6 27.6 – 32.8 21.6 52.8 34.8 12.3 – 33.7 60.4 0.00 18.3 57.8 – P HRASE - BASED MT DE Other association measures would be possible as well but Smadja et al. (1996) argue that the Dice coefficient is more robust with respect to low frequency events than other common metrics such as pointwise mutual information, which can be a serious issue with the unsmoothed probability estimations in stan"
W14-1614,N12-1052,0,0.404703,"Missing"
W14-1614,N13-1126,1,0.679337,"Missing"
W14-1614,H01-1035,0,0.857418,"nguages (Bender, 2013). Many applications require robust tools and the development of language-specific resources is expensive and time consuming. Furthermore, many tasks such as data-driven syntactic parsing require strong supervision to achieve reasonable results for real-world applications, since the performance of fully unsupervised methods lags behind by a large margin in comparison with the state of the Previous Cross-Lingual Approaches Annotation projection relies on the mapping of linguistic annotation across languages using parallel corpora and automatic alignment as basic resources (Yarowsky et al., 2001; Hwa et al., 2005; T¨ackstr¨om et al., 2013a). Tools that exist for the source language are used to annotate the source side of the corpus and projection heuristics are then applied to map the annotation through word alignment onto the corresponding target language text. Target language tools can then be trained on the projected annotation assuming that the mapping is sufficiently correct. Less frequent, but also possible, is the scenario where the source side of the corpus contains manual annotation (Agi´c et al., 2012). This addresses the problem created by projecting noisy annotations, but"
W14-1614,I08-3008,0,0.60358,"rsers on. More elaborated phrase-based models together with advanced annotation projection strategies do not necessarily lead to any improvements. As future work, we want to improve our model by (i) studying the impact of other SMT properties and improve the quality of treebank translation, (ii) implementing more sophisticated methods for annotation projection and (iii) using n-best lists provided by SMT models to introduce additional synthetic data using a single resource. We also aim at (iv) applying our approach to transfer parsing for closely related languages (see Agi´c et al. (2012) and Zeman and Resnik (2008) for related work), (v) testing it in a multi-source transfer scenario (McDonald et al., 2011) and, finally, (vi) comparing different dependency parsing paradigms within our experimental framework. Multi-source approaches are especially appealing using the translation approach. However, initial experiments (which we omit in this presentation) revealed that simple concatenation is not sufficient to obtain results that improve upon the single-best translated treebanks. A careful selection of appropriate training examples and their weights given to the training procedure seems to be essential to"
W14-1614,P11-2033,1,0.630094,"to slightly higher scores as we have noted in our experiments but we do not report those numbers here. Note also that the columns represent the target languages (used for testing), while the rows denote the source languages (used in training), as in McDonald et al. (2013). From the table, we can see that the baseline scores are compatible with the ones in the original experiments presented by (McDonald et al., 2013), included in Table 3 for reference. The differences are due to parser selection, as they use a transition-based parser with beam search and perceptron learning along the lines of Zhang and Nivre (2011) whereas we rely on greedy transition-based parsing with linear support vector machines. In the following, we will compare results to our baseline as we have a comparable setup in those experiments. However, most improvements shown below also apply in comparison with (McDonald et al., 2013). 135 5.2 Translated Treebanks Now we turn to the experiments on translated treebanks. We consider two setups. First, we look at the effect of translation when training delexicalized parsers. In this way, we can perform a direct comparison to the baseline performance presented WORD - BASED M ONOLINGUAL DE EN"
W14-1614,P09-1007,0,0.0505267,"omain. Secondly, we can assume that SMT will produce output that is much closer to the input than manual translations in parallel texts usually are. Even if this may seem like a short-coming in general, in the case of annotation projection it should rather be an advantage, because it makes it more straightforward and less error-prone to transfer annotation from source to target. Furthermore, the alignment between words and phrases is inherently provided as an output of all common SMT models. Hence, no additional procedures have to be performed on top of the translated corpus. Recent research (Zhao et al., 2009; Durrett et al., 2012) has attempted to address synthetic data creation for syntactic parsing via bilingual lexica. We seek to build on this work by utilizing more advanced translation techniques. Further in the paper, we first describe the tools and resources used in our experiments (§2). We elaborate on our approach to translating treebanks (§3) and projecting syntactic annotations (§4) for a new language. Finally, we provide empirical evaluation of the suggested approach (§5) and observe a substantial increase in parsing accuracy over the delexicalized parsing baselines. 2 Resources and To"
W14-1614,E12-2012,1,\N,Missing
W14-1614,W06-2920,0,\N,Missing
W14-1614,Q13-1001,1,\N,Missing
W14-1614,D07-1096,1,\N,Missing
W14-1614,P13-2017,1,\N,Missing
W14-3312,J92-4003,0,0.0276371,"nd 109 corpora. The first three of these corpora were included integrally into the training set after filtering out sentences of more than 80 words. The Common crawl and 109 data sets were run through an additional filtering step with an SVM classifier, closely following Mediani et al. (2011). The system includes three language models, a regular 6-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) trained with KenLM (Heafield, 2011), a 4-gram bilingual language model (Niehues et al., 2011) with Kneser-Ney smoothing trained with KenLM and a 9-gram model over Brown clusters (Brown et al., 1992) with Witten-Bell smoothing (Witten and Bell, 1991) trained with SRILM (Stolcke, 2002). 122 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122–129, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics place all pronouns that should be handled by the classifier, i.e. instances of il and elle aligned to it and instances of ils and elles aligned to they, with special placeholders. At decoding time, if a placeholder is encountered in a target language phrase, the applicable pronouns are generated with equal translation model proba"
W14-3312,P10-2033,0,0.0198937,"e case-sensitive phrase-based systems with lexicalized reordering trained on data provided by WMT. Word alignment is performed using fast align (Dyer et al., 2013). For tuning we use newstest2011. Additionally, we also test parallel data from OPUS (Tiedemann, 2012) filtered by a method adopted from Mediani et al. (2011). To contrast our baseline system, we trained a phrase-based model on parallel data that has been aligned on data pre-ordered using the reordering rules for German, which has been restored to the original word order after word alignment and before phrase extraction (similar to (Carpuat et al., 2010; Stymne et al., 2010)). We expect that the word alignment is improved by reducing crossings and long-distance links. However, the translation model as such has the same limitations as the baseline system in terms of long-range distortions. The final system is a two-step model in which we apply translation and language models trained on preordered target language data to perform the first step, which also includes a reordered POS language model. The second step is also treated as a translation problem as in Sudoh et al. (2011), and in our newstest2013 19.3 19.4 18.6 19.5 19.5 19.7 newstest2014"
W14-3312,W13-2210,0,0.0299303,"Missing"
W14-3312,P05-1066,0,0.162059,"Missing"
W14-3312,N13-1073,0,0.0402683,"US The fall of Saddam ushers in the right circumstances. Der Sturz von Saddam leitet solche richtigen Umst¨ande ein. Der Sturz von Saddam ein leitet solche richtigen Umst¨ande. Table 1: Two examples of pre-ordering outputs. The first two lines are the original English and German sentences and the third line shows the reordered sentence. We use three systems based on Moses to compare the effect of reordering on alignment and translation. All systems are case-sensitive phrase-based systems with lexicalized reordering trained on data provided by WMT. Word alignment is performed using fast align (Dyer et al., 2013). For tuning we use newstest2011. Additionally, we also test parallel data from OPUS (Tiedemann, 2012) filtered by a method adopted from Mediani et al. (2011). To contrast our baseline system, we trained a phrase-based model on parallel data that has been aligned on data pre-ordered using the reordering rules for German, which has been restored to the original word order after word alignment and before phrase extraction (similar to (Carpuat et al., 2010; Stymne et al., 2010)). We expect that the word alignment is improved by reducing crossings and long-distance links. However, the translation"
W14-3312,C10-1043,0,0.0141995,"l training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and often improves the quality of standard SMT systems significantly. Typically, the source language is reordered to better match the syntax of the target language when translating between languages that exhibit consistent word order differences, which are difficult to handle In our experiments, we focus on the translation from English to German. Post-ordering be"
W14-3312,E12-1074,0,0.0128122,"tables. Another reason is the possible distance of finite and infinitival verbs in German verb phrases that can lead to the same problems described above with verb-particle constructions. The auxiliary or modal verb is placed at the second position but the main verb appears at the end of the associated verb phrase. The distances can be arbitrarily long and long-range dependencies are quite frequent. Similarly, negation particles and adverbials move away from the inflected verb forms in certain constructions. For more details on specific phenomena in German, we refer to (Collins et al., 2005; Gojun and Fraser, 2012). Pre-ordering, i.e. moving English words into German word order does not seem to be a good option as we loose the connection between related items when moving particles and main verbs away from their associated elements. Hence, we are interested in reordering the target language German into English word order which can be beneficial in two ways: (i) Reordering the German part of the parallel training data makes it possible to improve word alignment (which tends to prefer monotonic mappings) and subsequent phrase extraction which leads to better translation models. (ii) We can explore a two-st"
W14-3312,D13-1037,1,0.676415,"n Smith Uppsala University Department of Linguistics and Philology firstname.lastname@lingfil.uu.se Abstract We describe the Uppsala University systems for WMT14. We look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for English–French. Furthermore, we investigate post-ordering and tunable POS distortion models for English– German. 1 Introduction In this paper we describe the Uppsala University systems for WMT14. We present three different systems. Two of them are based on the documentlevel decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). In our English–French system we extend Docent to handle pronoun anaphora, and in our English–German system we add partof-speech phrase-distortion models to Docent. For German–English we also have a system based on Moses (Koehn et al., 2007). Again the focus is on word order, this time by using pre- and postreordering. 2 Document-Level Decoding Traditional SMT decoders translate texts as bags of sentences, assuming independence between sentences. This assumption allows efficient algorithms for exploring a large search space based on dynamic programming (Och et al., 2001). Because of the dyn"
W14-3312,W11-2123,0,0.0336108,"em submitted by Cho et al. (2013) to the WMT 2013 shared task. Our phrase table is trained on data taken from the News commentary, Europarl, UN, Common crawl and 109 corpora. The first three of these corpora were included integrally into the training set after filtering out sentences of more than 80 words. The Common crawl and 109 data sets were run through an additional filtering step with an SVM classifier, closely following Mediani et al. (2011). The system includes three language models, a regular 6-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) trained with KenLM (Heafield, 2011), a 4-gram bilingual language model (Niehues et al., 2011) with Kneser-Ney smoothing trained with KenLM and a 9-gram model over Brown clusters (Brown et al., 1992) with Witten-Bell smoothing (Witten and Bell, 1991) trained with SRILM (Stolcke, 2002). 122 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122–129, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics place all pronouns that should be handled by the classifier, i.e. instances of il and elle aligned to it and instances of ils and elles aligned to they, with special pl"
W14-3312,D07-1103,0,0.0113749,"Common crawl data was filtered using the method of Stymne et al. (2013). We use factored models with POS tags as a second output factor for German. The possibility to use language models for different factors has been added to our Docent decoder. Language models include an in-domain news language model, an out-of-domain model trained on the target side of the parallel training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and o"
W14-3312,2005.iwslt-1.8,0,0.039688,"tymne et al., 2013) that it was useful to relax the hard distortion limit by either using a soft constraint, which could be tuned, or removing the limit completely. In that work we still used the standard parametrization of distortion, based on the positions of the first and last words in phrases. Our Docent decoder, however, always provides us with a full target translation that is step-wise improved, which means that we can apply distortion measures on the phrase-level without resorting to heuristics, which, for instance, are needed in the case of the lexicalized reordering models in Moses (Koehn et al., 2005). Because of this it is possible to use phrase-based distortion, where we calculate distortion based on the order of phrases, not on the order of some words. It is possible to parametrize phrase-distortion in different ways. In this work we use the phrase-distortion distance and a soft limit on the distortion distance, to mimic the word-based distortion. In our experiments we always set the soft limit to a distance of four phrases. In addition we use a measure based on how many crossings a phrase order gives rise to. We thus have three phrase-distortion features. As captured by lexicalized reo"
W14-3312,2009.mtsummit-posters.13,0,0.0172318,"e shortest distance between any pair of words in the aligned sets. The network is a binary classifier trained to discriminate positive examples extracted from human-made reference 123 amod nn auxpass by SMT systems with limited reordering capabilities such as phrase-based models. Preordering is often done on the entire training data as well to optimize translation models for the pre-ordered input. Less common is the idea of post-ordering, which refers to a separate step after translating source language input to an intermediate target language with corrupted (source-language like) word order (Na et al., 2009; Sudoh et al., 2011). punct nsubjpass prep pobj Domestic meat production is dominated by chicken . La production int´erieure de viande est domin´ee par le poulet . Figure 2: Dependency projection model translations from negative examples extracted from n-best lists generated by a baseline SMT system. 4 English–German For English–German we have two systems, one based on Moses, and one based on Docent. In both cases we have focused on word order, particularly for verbs and particles. Both our systems are trained on the same data made available by WMT. The Common crawl data was filtered using th"
W14-3312,D12-1108,1,0.882504,"mne J¨org Tiedemann Aaron Smith Uppsala University Department of Linguistics and Philology firstname.lastname@lingfil.uu.se Abstract We describe the Uppsala University systems for WMT14. We look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for English–French. Furthermore, we investigate post-ordering and tunable POS distortion models for English– German. 1 Introduction In this paper we describe the Uppsala University systems for WMT14. We present three different systems. Two of them are based on the documentlevel decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). In our English–French system we extend Docent to handle pronoun anaphora, and in our English–German system we add partof-speech phrase-distortion models to Docent. For German–English we also have a system based on Moses (Koehn et al., 2007). Again the focus is on word order, this time by using pre- and postreordering. 2 Document-Level Decoding Traditional SMT decoders translate texts as bags of sentences, assuming independence between sentences. This assumption allows efficient algorithms for exploring a large search space based on dynamic programming (Och et al., 2"
W14-3312,W09-0435,0,0.11068,"trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and often improves the quality of standard SMT systems significantly. Typically, the source language is reordered to better match the syntax of the target language when translating between languages that exhibit consistent word order differences, which are difficult to handle In our experiments, we focus on the translation from English to German. Post-ordering becomes attractive for several reasons: One reason is"
W14-3312,P13-4033,1,0.682907,"n Smith Uppsala University Department of Linguistics and Philology firstname.lastname@lingfil.uu.se Abstract We describe the Uppsala University systems for WMT14. We look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for English–French. Furthermore, we investigate post-ordering and tunable POS distortion models for English– German. 1 Introduction In this paper we describe the Uppsala University systems for WMT14. We present three different systems. Two of them are based on the documentlevel decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). In our English–French system we extend Docent to handle pronoun anaphora, and in our English–German system we add partof-speech phrase-distortion models to Docent. For German–English we also have a system based on Moses (Koehn et al., 2007). Again the focus is on word order, this time by using pre- and postreordering. 2 Document-Level Decoding Traditional SMT decoders translate texts as bags of sentences, assuming independence between sentences. This assumption allows efficient algorithms for exploring a large search space based on dynamic programming (Och et al., 2001). Because of the dyn"
W14-3312,W11-2124,0,0.0221714,"ared task. Our phrase table is trained on data taken from the News commentary, Europarl, UN, Common crawl and 109 corpora. The first three of these corpora were included integrally into the training set after filtering out sentences of more than 80 words. The Common crawl and 109 data sets were run through an additional filtering step with an SVM classifier, closely following Mediani et al. (2011). The system includes three language models, a regular 6-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) trained with KenLM (Heafield, 2011), a 4-gram bilingual language model (Niehues et al., 2011) with Kneser-Ney smoothing trained with KenLM and a 9-gram model over Brown clusters (Brown et al., 1992) with Witten-Bell smoothing (Witten and Bell, 1991) trained with SRILM (Stolcke, 2002). 122 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122–129, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics place all pronouns that should be handled by the classifier, i.e. instances of il and elle aligned to it and instances of ils and elles aligned to they, with special placeholders. At decoding time, if a placeholder is encounte"
W14-3312,W01-1408,0,0.0359546,"et al., 2012; Hardmeier et al., 2013a). In our English–French system we extend Docent to handle pronoun anaphora, and in our English–German system we add partof-speech phrase-distortion models to Docent. For German–English we also have a system based on Moses (Koehn et al., 2007). Again the focus is on word order, this time by using pre- and postreordering. 2 Document-Level Decoding Traditional SMT decoders translate texts as bags of sentences, assuming independence between sentences. This assumption allows efficient algorithms for exploring a large search space based on dynamic programming (Och et al., 2001). Because of the dynamic programming assumptions it is hard to directly include discourse-level and long-distance features into a traditional SMT decoder. In contrast to this very popular stack decoding approach, our decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a) implements a search procedure based on local search. At any stage of the search process, its search state consists of a complete document translation, making it easy for feature models to access the complete document Joakim Nivre with its current translation at any point in time. The search algorithm is a stochastic"
W14-3312,P03-1021,0,0.00912326,"our systems are trained on the same data made available by WMT. The Common crawl data was filtered using the method of Stymne et al. (2013). We use factored models with POS tags as a second output factor for German. The possibility to use language models for different factors has been added to our Docent decoder. Language models include an in-domain news language model, an out-of-domain model trained on the target side of the parallel training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain lang"
W14-3312,popovic-ney-2006-pos,0,0.0838413,"Missing"
W14-3312,2007.tmi-papers.21,0,0.0459927,"a and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and often improves the quality of standard SMT systems significantly. Typically, the source language is reordered to better match the syntax of the target language when translating between languages that exhibit consistent word order differences, which are difficult to handle In our experiments, we focus on the translation from English to German. Post-ordering becomes attractive for sever"
W14-3312,W10-1727,1,0.857022,"e-based systems with lexicalized reordering trained on data provided by WMT. Word alignment is performed using fast align (Dyer et al., 2013). For tuning we use newstest2011. Additionally, we also test parallel data from OPUS (Tiedemann, 2012) filtered by a method adopted from Mediani et al. (2011). To contrast our baseline system, we trained a phrase-based model on parallel data that has been aligned on data pre-ordered using the reordering rules for German, which has been restored to the original word order after word alignment and before phrase extraction (similar to (Carpuat et al., 2010; Stymne et al., 2010)). We expect that the word alignment is improved by reducing crossings and long-distance links. However, the translation model as such has the same limitations as the baseline system in terms of long-range distortions. The final system is a two-step model in which we apply translation and language models trained on preordered target language data to perform the first step, which also includes a reordered POS language model. The second step is also treated as a translation problem as in Sudoh et al. (2011), and in our newstest2013 19.3 19.4 18.6 19.5 19.5 19.7 newstest2014 19.1 19.3 18.7 19.3 1"
W14-3312,W13-2229,1,0.877852,"l., 2011). punct nsubjpass prep pobj Domestic meat production is dominated by chicken . La production int´erieure de viande est domin´ee par le poulet . Figure 2: Dependency projection model translations from negative examples extracted from n-best lists generated by a baseline SMT system. 4 English–German For English–German we have two systems, one based on Moses, and one based on Docent. In both cases we have focused on word order, particularly for verbs and particles. Both our systems are trained on the same data made available by WMT. The Common crawl data was filtered using the method of Stymne et al. (2013). We use factored models with POS tags as a second output factor for German. The possibility to use language models for different factors has been added to our Docent decoder. Language models include an in-domain news language model, an out-of-domain model trained on the target side of the parallel training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are give"
W14-3312,2011.mtsummit-papers.36,0,0.175414,"ce between any pair of words in the aligned sets. The network is a binary classifier trained to discriminate positive examples extracted from human-made reference 123 amod nn auxpass by SMT systems with limited reordering capabilities such as phrase-based models. Preordering is often done on the entire training data as well to optimize translation models for the pre-ordered input. Less common is the idea of post-ordering, which refers to a separate step after translating source language input to an intermediate target language with corrupted (source-language like) word order (Na et al., 2009; Sudoh et al., 2011). punct nsubjpass prep pobj Domestic meat production is dominated by chicken . La production int´erieure de viande est domin´ee par le poulet . Figure 2: Dependency projection model translations from negative examples extracted from n-best lists generated by a baseline SMT system. 4 English–German For English–German we have two systems, one based on Moses, and one based on Docent. In both cases we have focused on word order, particularly for verbs and particles. Both our systems are trained on the same data made available by WMT. The Common crawl data was filtered using the method of Stymne et"
W14-3312,tiedemann-2012-parallel,1,0.687825,"st¨ande ein. Der Sturz von Saddam ein leitet solche richtigen Umst¨ande. Table 1: Two examples of pre-ordering outputs. The first two lines are the original English and German sentences and the third line shows the reordered sentence. We use three systems based on Moses to compare the effect of reordering on alignment and translation. All systems are case-sensitive phrase-based systems with lexicalized reordering trained on data provided by WMT. Word alignment is performed using fast align (Dyer et al., 2013). For tuning we use newstest2011. Additionally, we also test parallel data from OPUS (Tiedemann, 2012) filtered by a method adopted from Mediani et al. (2011). To contrast our baseline system, we trained a phrase-based model on parallel data that has been aligned on data pre-ordered using the reordering rules for German, which has been restored to the original word order after word alignment and before phrase extraction (similar to (Carpuat et al., 2010; Stymne et al., 2010)). We expect that the word alignment is improved by reducing crossings and long-distance links. However, the translation model as such has the same limitations as the baseline system in terms of long-range distortions. The"
W14-3312,C04-1073,0,0.0399491,"et side of the parallel training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and often improves the quality of standard SMT systems significantly. Typically, the source language is reordered to better match the syntax of the target language when translating between languages that exhibit consistent word order differences, which are difficult to handle In our experiments, we focus on the translation from English to German. Po"
W14-3312,P07-2045,0,\N,Missing
W14-3312,W12-3144,0,\N,Missing
W14-3312,2011.iwslt-evaluation.9,0,\N,Missing
W14-3334,C10-1043,0,0.40539,"tion units in the same way as for alignment links. We will use these three measures to get a broader picture of TUs in alignment evaluation. Also in this case, 1−TUER is equivalent to F-measure. TUER(A, G) = 1 − 2|AU ∩ GU | |AU |+ |GU | Ahrenberg (2010) also proposed to measure reorderings. He does this by calculating the percentage of links with crossings of different lengths. To define this he only considers adjacent links in the source using the distance between corresponding target words, which means that his metric becomes a directional measure. Reorderings of alignments was also used by Genzel (2010), who used crossing score, the number of crossing links, to rank reordering rules. This is non-directional and simpler to calculate than Ahrenberg (2010)’s metrics, and implicitly covers length since a long distance reordering leads to a higher number of pairwise crossing links. Birch and Osborne (2011) suggest using squared Kendall τ distance (SKTD), see Eq. 8, where n is the number of links, as a basis of LR-score, an MT metric that takes reordering into account. They found that squaring τ better explained reordering, than using only τ . In this study we will use both, crossing score and SKT"
W14-3334,ahrenberg-etal-2000-evaluation,1,0.732969,"Missing"
W14-3334,P04-1064,0,0.0336358,"ents we use a gold standard that does not make use of distinctions between sure and possible links, as suggested by Fraser and Marcu (2007). With this, we can calculate the standard metrics P(recision) R(ecall) and F(-measure). We will mainly use balanced F-measure, but occasionally also report weighted F-measure. As noted before, 1−AER is equivalent to balanced F when only sure links are used, and will thus not be reported separately. Søgaard and Kuhn (2009) and Søgaard and Wu (2009) suggested working on the translation unit (TU) level, instead of the link level. A translation unit, or cept (Goutte et al., 2004), is defined as a maximally connected subgraph of an alignment. In Figure 1, the twelve links form nine translation units. Søgaard and Wu (2009) suggest the metric TUER, translation unit error rate, shown in Eq. 5, where AU are hypothesized translation units, and GU are gold standard translation units.1 They use TUER to establish lower bounds for the coverage of alignments from different formalisms, not to evaluate SMT. While they only use TUER, it (4) Ayan and Dorr (2006) on the other hand found some evidence for the importance of precision over recall. However, they used much smaller trainin"
W14-3334,ahrenberg-2010-alignment,0,0.150189,"nd Szał, 2012; Dyer et al., 2013). The relation between the two types of evaluation is often quite weak. Sev1 TUER is similar to CPER (Ayan and Dorr, 2006), which measures the error rate of extracted phrases. Due to how phrase extraction handle null links, there are differences, however. 277 is also possible to define Precision, Recall and Fmeasure over translation units in the same way as for alignment links. We will use these three measures to get a broader picture of TUs in alignment evaluation. Also in this case, 1−TUER is equivalent to F-measure. TUER(A, G) = 1 − 2|AU ∩ GU | |AU |+ |GU | Ahrenberg (2010) also proposed to measure reorderings. He does this by calculating the percentage of links with crossings of different lengths. To define this he only considers adjacent links in the source using the distance between corresponding target words, which means that his metric becomes a directional measure. Reorderings of alignments was also used by Genzel (2010), who used crossing score, the number of crossing links, to rank reordering rules. This is non-directional and simpler to calculate than Ahrenberg (2010)’s metrics, and implicitly covers length since a long distance reordering leads to a hi"
W14-3334,W09-0421,1,0.925521,"al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is Another type of approach to reordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word alignments on non-reordered texts (Holmqvist et al., 2009). For the latter approach a small study of the effect of gd and gdfa symmetrizations was presented, which only showed small variations in quality scores (Holmqvist et al., 2012). Below we present the two tasks that we study in this paper: part-of-speech-based reordering for creating input lattices for SMT and alignmentbased reordering for improving phrase-tables. We evaluate the performance of these tasks in relation to the use of different alignment models and symmetrization heuristics. For these tasks we are mainly interested in the full translation task, for which we report Bleu scores. In"
W14-3334,P06-1002,0,0.0214734,"nd Wu (2009) suggested working on the translation unit (TU) level, instead of the link level. A translation unit, or cept (Goutte et al., 2004), is defined as a maximally connected subgraph of an alignment. In Figure 1, the twelve links form nine translation units. Søgaard and Wu (2009) suggest the metric TUER, translation unit error rate, shown in Eq. 5, where AU are hypothesized translation units, and GU are gold standard translation units.1 They use TUER to establish lower bounds for the coverage of alignments from different formalisms, not to evaluate SMT. While they only use TUER, it (4) Ayan and Dorr (2006) on the other hand found some evidence for the importance of precision over recall. However, they used much smaller training data than Fraser and Marcu (2007). They also suggested using a measure called consistent phrase error-rate (CPER), but found that it was hard to assess the impact of alignment on MT, both with AER and CPER. Lambert et al. (2012) performed a study where they investigated the effect of word alignment on MT using a large number of word alignment indicators. They found that there was a difference between large and small datasets in that alignment precision was more important"
W14-3334,holmqvist-etal-2012-alignment,1,0.91956,"ordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word alignments on non-reordered texts (Holmqvist et al., 2009). For the latter approach a small study of the effect of gd and gdfa symmetrizations was presented, which only showed small variations in quality scores (Holmqvist et al., 2012). Below we present the two tasks that we study in this paper: part-of-speech-based reordering for creating input lattices for SMT and alignmentbased reordering for improving phrase-tables. We evaluate the performance of these tasks in relation to the use of different alignment models and symmetrization heuristics. For these tasks we are mainly interested in the full translation task, for which we report Bleu scores. In addition we also show fuzzy reordering score (FRS), which focuses 281 SMT, Bleu POSReo, FRS POSReo, Bleu AlignReo, FRS AlignReo, Bleu Total .33 −.80 −.64 −.77 −.81 SMT, Bleu POS"
W14-3334,P11-1103,0,0.0961479,"e does this by calculating the percentage of links with crossings of different lengths. To define this he only considers adjacent links in the source using the distance between corresponding target words, which means that his metric becomes a directional measure. Reorderings of alignments was also used by Genzel (2010), who used crossing score, the number of crossing links, to rank reordering rules. This is non-directional and simpler to calculate than Ahrenberg (2010)’s metrics, and implicitly covers length since a long distance reordering leads to a higher number of pairwise crossing links. Birch and Osborne (2011) suggest using squared Kendall τ distance (SKTD), see Eq. 8, where n is the number of links, as a basis of LR-score, an MT metric that takes reordering into account. They found that squaring τ better explained reordering, than using only τ . In this study we will use both, crossing score and SKTD. Figure 1 shows these scores for an example sentence. These two measures only tell us how much reordering there is. To quantify this relative to the gold standard we also report the absolute difference between the number of gold standard crossings and system crossings, which we call Crossdiff. To acco"
W14-3334,J93-2003,0,0.0624236,"A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P , links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard does not make a distinction between S and P. Word Alignment and SMT Word alignment is the task of relating words in one language to words in the translation in another language, see an example in Figure 1. Word alignment models can be learnt automatically from large corpora of sentence aligned data. Brown et al. (1993) proposed the so-called IBM models, which are still widely used. These five models estimate alignments from corpora using the expectation-maximization algorithm, and each model adds some complexity. Model 4 is commonly used in SMT systems. There have been many later suggestions of alternatives to these models. These are often alternatives to model 2, such as the HMM model (Vogel et al., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is general"
W14-3334,N03-1017,0,0.014538,"igned Table 1: Symmetrization strategies for word alignments AT S and AST in two directions these models require external tools (for creating linguistic features) and manually aligned training data, which we do not have for our data sets (besides the data we need for evaluation). Investigating these types of models are outside the scope of our current work. Word alignments are used as an important knowledge source for training SMT systems. In word-based SMT, the parameters of the generative word alignment models are essentially the translation model of the system. In phrase-based SMT (PBSMT) (Koehn et al., 2003), which is among the state-of-the-art systems today, word alignments are used as a basis for extracting phrases and estimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A"
W14-3334,P10-2033,0,0.0155304,"on, such as part-of-speech tags (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is Another type of approach to reordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word alignments on non-reordered texts (Holmqvist et al., 2009). For the latter approach a small study of the effect of gd and gdfa symmetrizations was presented, which only showed small variations in quality scores (Holmqvist et al., 2012). Below we present the two tasks that we study in this paper: part-of-speech-based reordering for creating input lattices for SMT and alignmentbased reordering for improving phrase-tables. We evaluate the performance of these tasks in relation to the use of different alignment models and symmetrization h"
W14-3334,2005.iwslt-1.8,0,0.177029,"e been many later suggestions of alternatives to these models. These are often alternatives to model 2, such as the HMM model (Vogel et al., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of |G ∩ A| |A| |G ∩ A| Recall(A, G) = |G| |P ∩ A |+ |S ∩ A| AER = 1 − |S |+ |A| Precision(A, G) = 276 (1) (2) (3) Crossing p =8"
W14-3334,J07-2003,0,0.467625,"s of models are outside the scope of our current work. Word alignments are used as an important knowledge source for training SMT systems. In word-based SMT, the parameters of the generative word alignment models are essentially the translation model of the system. In phrase-based SMT (PBSMT) (Koehn et al., 2003), which is among the state-of-the-art systems today, word alignments are used as a basis for extracting phrases and estimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P , links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard do"
W14-3334,P05-1066,0,0.119492,"Missing"
W14-3334,P07-2045,0,0.0152871,"68 .370 .455 .514 .486 .392 .444 .500 .545 .524 .452 .445 .507 .548 .495 .450 .503 .537 .584 .525 .515 Link crossings P R F – – – 41060 31660 27245 12101 8122 29429 20671 17053 6718 4146 6718 4547 3044 12764 9779 8340 8277 8197 15829 11585 20099 17481 18934 22086 20439 Crossdiff 0 Table 2: Values for alignment quality indicators for the different alignments, where 2–4, HMM, and fa are alignment models, and symmetrization strategies refer to Table 1 Total 22629 Section 3.2) of 2M sentences during alignment. For symmetrization we used all methods in Table 1, as implemented in the Moses toolkit (Koehn et al., 2007) and in fast align (Dyer et al., 2013). Link level ↓ P R F Based on the automatically aligned gold standard, we calculated all alignment indicators for all settings. The complete results can be found in Table 2, where we have ordered the symmetrization methods with the most sparse, intersection, on top. Overall we can see that while several of the alignment methods create a much higher number of alignment links than the gold standard, they do not produce many more translation units. This is very interesting and indicates why link level statistics may not be accurate enough to predict the perfo"
W14-3334,N13-1073,0,0.516213,"words in the translation in another language, see an example in Figure 1. Word alignment models can be learnt automatically from large corpora of sentence aligned data. Brown et al. (1993) proposed the so-called IBM models, which are still widely used. These five models estimate alignments from corpora using the expectation-maximization algorithm, and each model adds some complexity. Model 4 is commonly used in SMT systems. There have been many later suggestions of alternatives to these models. These are often alternatives to model 2, such as the HMM model (Vogel et al., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide r"
W14-3334,W08-0306,0,0.0189954,"l., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of |G ∩ A| |A| |G ∩ A| Recall(A, G) = |G| |P ∩ A |+ |S ∩ A| AER = 1 − |S |+ |A| Precision(A, G) = 276 (1) (2) (3) Crossing p =8 SKDT = 8/66 ≈ 0.65 6 1–1 links 3 multi links 0 null links Figure 1: An example alignment illustrating n–1, 1–m and crossing links. eral o"
W14-3334,D13-1049,0,0.0133298,"seline. These results confirm results from previous studies that link level measures, especially recall and weighted F-measure show some correlation with SMT quality whereas precision does not. 4 In the preordering studies cited above it is often not even stated which alignment model was used. A few authors mention the alignment tool that has been applied but no comparison between different alignment models is performed in any of the papers we are aware of. Li et al. (2007), for example, simply state that they used GIZA++ and gdf symmetrization and that they removed less probable multi links. Lerner and Petrov (2013) use the intersection of HMM alignments and claims that model 4 did not add much value. Genzel (2010) did mention that using a standard model 4 was not successful for his rule learning approach. Instead he used filtered model-1-alignments, which he claims was more successful. However, there are no further analyses or comparisons between the alignments reported in any of these papers. Reordering Tasks for SMT Reordering is an important part of any SMT system. One way to address it is to add reordering models to standard PBSMT systems, for instance lexicalized reordering models (Koehn et al., 20"
W14-3334,J07-3002,0,0.550421,"set of crossing links, like intersected HMM models, are preferred. Unlike SMT performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks. Moreover, we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on SMT reordering tasks. 1 Introduction Word alignment is a key component in all state-ofthe-art statistical machine translation (SMT) systems, and there has been some work exploring the connection between word alignment quality and translation quality (Och and Ney, 2003; Fraser and Marcu, 2007; Lambert et al., 2012). The standard way to evaluate word alignments in this context is by using metrics like alignment error rate (AER) and F-measure on the link level, and the general conclusion appears to be that translation quality benefits from alignments with high recall (rather than precision), at least for large training data. Although many other ways of measuring alignment 275 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 275–286, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Symmetrization int: intersection"
W14-3334,P07-1091,0,0.154673,"is a standard PBSMT system trained on WMT13 280 inter gd gdfa gdf union m2 18.1 20.4 20.4 19.4 19.2 m3 19.1 20.9 20.7 19.7 19.6 m4 19.3 20.9 20.8 20.1 19.8 HMM 18.8 20.5 20.5 19.9 19.7 fa 18.9 20.6 20.5 20.0 20.0 always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice of possible reorderings (Rottmann and Vogel, 2007; Zhang et al., 2007). Table 4: Baseline Bleu scores for different symmetrization heuristics suggesting that they measure similar things. Intuitively it seems important for SMT to match full translation units, but it might be the case that the phrase extraction strategy is robust as long as there are partial matches. There are no significant correlations with link degree or link crossings, except a negative correlation with Crossdiff, which means that it is good to have a similar number of crossings as the baseline. These resul"
W14-3334,N04-1035,0,0.0518221,"r current work. Word alignments are used as an important knowledge source for training SMT systems. In word-based SMT, the parameters of the generative word alignment models are essentially the translation model of the system. In phrase-based SMT (PBSMT) (Koehn et al., 2003), which is among the state-of-the-art systems today, word alignments are used as a basis for extracting phrases and estimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P , links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard does not make a distinction between S and P. Word"
W14-3334,P08-1112,0,0.0371481,"Missing"
W14-3334,N06-1014,0,0.0456501,"study where they investigated the effect of word alignment on MT using a large number of word alignment indicators. They found that there was a difference between large and small datasets in that alignment precision was more important with small data sets, and recall more important with large data sets. Overall they did not find any indicator that was significant over two language pairs and different corpus sizes. There were more significant indicators for large datasets, however. Most researchers who propose new alignment models perform both a gold standard evaluation and an SMT evaluation (Liang et al., 2006; Ganchev et al., 2008; Junczys-Dowmunt and Szał, 2012; Dyer et al., 2013). The relation between the two types of evaluation is often quite weak. Sev1 TUER is similar to CPER (Ayan and Dorr, 2006), which measures the error rate of extracted phrases. Due to how phrase extraction handle null links, there are differences, however. 277 is also possible to define Precision, Recall and Fmeasure over translation units in the same way as for alignment links. We will use these three measures to get a broader picture of TUs in alignment evaluation. Also in this case, 1−TUER is equivalent to F-measure. T"
W14-3334,W09-3805,0,0.0194029,"s the classic group where metrics are calculated on the alignment link level, which has been used in several studies. In our experiments we use a gold standard that does not make use of distinctions between sure and possible links, as suggested by Fraser and Marcu (2007). With this, we can calculate the standard metrics P(recision) R(ecall) and F(-measure). We will mainly use balanced F-measure, but occasionally also report weighted F-measure. As noted before, 1−AER is equivalent to balanced F when only sure links are used, and will thus not be reported separately. Søgaard and Kuhn (2009) and Søgaard and Wu (2009) suggested working on the translation unit (TU) level, instead of the link level. A translation unit, or cept (Goutte et al., 2004), is defined as a maximally connected subgraph of an alignment. In Figure 1, the twelve links form nine translation units. Søgaard and Wu (2009) suggest the metric TUER, translation unit error rate, shown in Eq. 5, where AU are hypothesized translation units, and GU are gold standard translation units.1 They use TUER to establish lower bounds for the coverage of alignments from different formalisms, not to evaluate SMT. While they only use TUER, it (4) Ayan and Dor"
W14-3334,P05-1057,0,0.0280805,"inks) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of |G ∩ A| |A| |G ∩ A| Recall(A, G) = |G| |P ∩ A |+ |S ∩ A| AER = 1 − |S |+ |A| Precision(A, G) = 276 (1) (2) (3) Crossing p =8 SKDT = 8/66 ≈ 0.65 6 1–1 links 3 multi links 0 null links Figure 1: An example alignment illustrating n–1, 1–m and crossing links. eral of these studies only show AER on their gold standard, despite its well-known shortcomings. Even though many studies have shown some relation between translation quality an"
W14-3334,W07-2456,0,0.0307144,"T Reordering Tasks Sara Stymne J¨org Tiedemann Joakim Nivre Uppsala University Department of Linguistics and Philology firstname.lastname@lingfil.uu.se Abstract quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study the relation between alignment and translation quality, with the exception of Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignments. For instance, source language reordering commonly relies on rules learnt automatically from word-aligned data (e.g., Xia and McCord (2004)). As far as we know, no one has studied the impact of alignment quality on these additional tasks, and it seems to be tacit"
W14-3334,W10-1727,1,0.858503,"eech tags (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is Another type of approach to reordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word alignments on non-reordered texts (Holmqvist et al., 2009). For the latter approach a small study of the effect of gd and gdfa symmetrizations was presented, which only showed small variations in quality scores (Holmqvist et al., 2012). Below we present the two tasks that we study in this paper: part-of-speech-based reordering for creating input lattices for SMT and alignmentbased reordering for improving phrase-tables. We evaluate the performance of these tasks in relation to the use of different alignment models and symmetrization heuristics. For these t"
W14-3334,H05-1011,0,0.0350232,"versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of |G ∩ A| |A| |G ∩ A| Recall(A, G) = |G| |P ∩ A |+ |S ∩ A| AER = 1 − |S |+ |A| Precision(A, G) = 276 (1) (2) (3) Crossing p =8 SKDT = 8/66 ≈ 0.65 6 1–1 links 3 multi links 0 null links Figure 1: An example alignment illustrating n–1, 1–m and crossing links. eral of these studies only show AER on their gold standard, despite its well-known shortcomings. Even though many studies have shown some relation between translation quality and AER or weig"
W14-3334,W09-0435,0,0.31843,"part of the evaluation of MT systems (Birch and Osborne, 2011). We can distinguish two main types of approaches to preordering in SMT, either by using hand-written rules, which often operate on syntactic trees (Collins et al., 2005), or by reordering rules that are learnt automatically based on a word aligned corpus (Xia and McCord, 2004). The latter approach is of interest to us, since it is based on word alignments. There has been much work on automatic learning of reordering rules, which can be based on different levels of annotation, such as part-of-speech tags (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is Another type of approach to reordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word al"
W14-3334,W12-0704,1,0.823036,"somewhat, the correlations with alignment indicators were stable. SMT Experiments For reference, we first study the impact of alignment on SMT performance. Our SMT system is a standard PBSMT system trained on WMT13 280 inter gd gdfa gdf union m2 18.1 20.4 20.4 19.4 19.2 m3 19.1 20.9 20.7 19.7 19.6 m4 19.3 20.9 20.8 20.1 19.8 HMM 18.8 20.5 20.5 19.9 19.7 fa 18.9 20.6 20.5 20.0 20.0 always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice of possible reorderings (Rottmann and Vogel, 2007; Zhang et al., 2007). Table 4: Baseline Bleu scores for different symmetrization heuristics suggesting that they measure similar things. Intuitively it seems important for SMT to match full translation units, but it might be the case that the phrase extraction strategy is robust as long as there are partial matches. There are no significant correlations w"
W14-3334,C00-2163,0,0.19044,"stimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P , links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard does not make a distinction between S and P. Word Alignment and SMT Word alignment is the task of relating words in one language to words in the translation in another language, see an example in Figure 1. Word alignment models can be learnt automatically from large corpora of sentence aligned data. Brown et al. (1993) proposed the so-called IBM models, which are still widely used. These five models estimate alignments from corpora usi"
W14-3334,N12-1052,0,0.053597,"Missing"
W14-3334,J03-1002,0,0.198899,"its, and on the subset of crossing links, like intersected HMM models, are preferred. Unlike SMT performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks. Moreover, we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on SMT reordering tasks. 1 Introduction Word alignment is a key component in all state-ofthe-art statistical machine translation (SMT) systems, and there has been some work exploring the connection between word alignment quality and translation quality (Och and Ney, 2003; Fraser and Marcu, 2007; Lambert et al., 2012). The standard way to evaluate word alignments in this context is by using metrics like alignment error rate (AER) and F-measure on the link level, and the general conclusion appears to be that translation quality benefits from alignments with high recall (rather than precision), at least for large training data. Although many other ways of measuring alignment 275 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 275–286, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Symmetriz"
W14-3334,W11-2102,0,0.112784,"−.57 .83 −.79 Total .65 −.23 .05 −.11 −.24 Total −.05 −.81 −.71 −.80 −.93 Translation units P R F −.20 .16 −.02 .90 .81 .89 .80 .80 .86 .90 .88 .92 .75 .64 .72 SKTD −.14 −.70 −.60 −.73 −.91 Link crossings P R −.09 .25 .90 .21 .79 .42 .94 .27 .86 −.07 P −.29 .82 .67 .81 .71 F .07 .86 .89 .92 .69 MWU R .59 −.45 −.23 −.37 −.53 F .44 .22 .35 .31 .04 Crossdiff −.63 −.41 −.49 −.38 −.52 Table 5: Pearson correlations between different alignment characteristics and scores for the translation and reordering tasks. Significant correlations are marked with bold (< 0.01). only on the reordering component (Talbot et al., 2011). It compares a system reordering to a reference reordering, by measuring how many chunks that have to be moved to get an identical word order, see Eq. 9, where C is the number of contiguously aligned chunks, and M the number of words. To find the reference ordering we apply the method of Holmqvist et al. (2009), described in Section 4.2, to the gold standard alignment. FRS = 1 − 4.1 C −1 M −1 inter gd gdfa gdf union m2 .577 .555 .540 .439 .442 m3 .575 .559 .540 .499 .492 m4 .581 .570 .559 .542 .544 HMM .596 .589 .579 .560 .563 fa .567 .546 .539 .495 .486 Table 6: Fuzzy reordering scores for p"
W14-3334,W99-0604,0,0.161786,"shown some relation between translation quality and AER or weighted F-measure, it has rarely been investigated thoroughly in its own right, and, as far as we are aware, not for other tasks than SMT. Furthermore, most of these studies considers nothing else but link level agreement. In this paper we take a broader view on alignment quality and explore the effect of other types of quality indicators as well. The relation between word alignment quality and PBSMT has been studied by some researchers. Och and Ney (2000) looked at the impact of IBM and HMM models on the alignment template approach (Och et al., 1999) in terms of AER. They found that AER correlates with human evaluation of sentence level quality, but not with word error rate. Fraser and Marcu (2007) found that there is no correlation between AER and Bleu (Papineni et al., 2002), especially not when the P set is large. They found that a balanced F-measure is a better indicator of Bleu, but that a weighted F-measure is even better (see Eq. 4) mostly with a higher weight for recall than for precision. This weight, however, needs to be optimized for each data set, language pair, and gold standard alignment separately. F(A, G, α) =  α 1−α + Pr"
W14-3334,H05-1010,0,0.0390747,"generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of |G ∩ A| |A| |G ∩ A| Recall(A, G) = |G| |P ∩ A |+ |S ∩ A| AER = 1 − |S |+ |A| Precision(A, G) = 276 (1) (2) (3) Crossing p =8 SKDT = 8/66 ≈ 0.65 6 1–1 links 3 multi links 0 null links Figure 1: An example alignment illustrating n–1, 1–m and crossing links. eral of these studies only show AER on their gold standard, despite its well-known shortcomings. Even though many studies have shown some relation between translation quality and AER or weighted F-measure, it has"
W14-3334,P03-1021,0,0.0838948,"on translation unit level. Significant correlations are marked with bold (< 0.01). data.2 We trained a German–English system on 2M sentences from Europarl and News Commentary. We used the target side of the parallel corpus and the SRILM toolkit (Stolcke, 2002) to train a 5gram language model. For training the translation model and for decoding we used the Moses toolkit (Koehn et al., 2007). We applied a standard feature set consisting of a language model feature, four translation model features, word penalty, phrase penalty, and distortion cost. For tuning we used minimum error-rate training (Och, 2003). In order to minimize the risk of tuning influencing the results, we used a fixed set of weights for each experiment, tuned on a model 4+gdfa alignment.3 For tuning we used newstest2009 with 2525 sentences, and for testing we used newstest2013 with 3000 sentences. Evaluation was performed using the Bleu metric (Papineni et al., 2002). The same system setup was used for the SMT systems with reordering. Table 4 shows the results on the SMT task. Model 3 and 4 with gd/gdfa symmetrization yield the highest scores. There is a larger difference between systems with different symmetrization than bet"
W14-3334,H05-1108,0,0.469573,"e.lastname@lingfil.uu.se Abstract quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study the relation between alignment and translation quality, with the exception of Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignments. For instance, source language reordering commonly relies on rules learnt automatically from word-aligned data (e.g., Xia and McCord (2004)). As far as we know, no one has studied the impact of alignment quality on these additional tasks, and it seems to be tacitly assumed that alignments that are good for translation are also good for other tasks. In this paper we set out to explore the"
W14-3334,C96-2141,0,0.612207,"f relating words in one language to words in the translation in another language, see an example in Figure 1. Word alignment models can be learnt automatically from large corpora of sentence aligned data. Brown et al. (1993) proposed the so-called IBM models, which are still widely used. These five models estimate alignments from corpora using the expectation-maximization algorithm, and each model adds some complexity. Model 4 is commonly used in SMT systems. There have been many later suggestions of alternatives to these models. These are often alternatives to model 2, such as the HMM model (Vogel et al., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et"
W14-3334,P02-1040,0,0.0905907,"studies considers nothing else but link level agreement. In this paper we take a broader view on alignment quality and explore the effect of other types of quality indicators as well. The relation between word alignment quality and PBSMT has been studied by some researchers. Och and Ney (2000) looked at the impact of IBM and HMM models on the alignment template approach (Och et al., 1999) in terms of AER. They found that AER correlates with human evaluation of sentence level quality, but not with word error rate. Fraser and Marcu (2007) found that there is no correlation between AER and Bleu (Papineni et al., 2002), especially not when the P set is large. They found that a balanced F-measure is a better indicator of Bleu, but that a weighted F-measure is even better (see Eq. 4) mostly with a higher weight for recall than for precision. This weight, however, needs to be optimized for each data set, language pair, and gold standard alignment separately. F(A, G, α) =  α 1−α + Precision(A,G) Recall(A,G) −1 3 Word Alignment Quality Indicators We investigate four groups of quality indicators. The first group is the classic group where metrics are calculated on the alignment link level, which has been used i"
W14-3334,C04-1073,0,0.545815,"Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignments. For instance, source language reordering commonly relies on rules learnt automatically from word-aligned data (e.g., Xia and McCord (2004)). As far as we know, no one has studied the impact of alignment quality on these additional tasks, and it seems to be tacitly assumed that alignments that are good for translation are also good for other tasks. In this paper we set out to explore the impact of alignment quality on two pre-reordering tasks for SMT. In doing so, we employ a wider range of quality indicators than is customary, and for reference these indicators are used also to assess overall translation quality. To allow an in-depth exploration of the connections between several aspects of word alignment and reordering, we limi"
W14-3334,postolache-etal-2006-transferring,0,0.0614037,"Missing"
W14-3334,P02-1039,0,0.321492,"e outside the scope of our current work. Word alignments are used as an important knowledge source for training SMT systems. In word-based SMT, the parameters of the generative word alignment models are essentially the translation model of the system. In phrase-based SMT (PBSMT) (Koehn et al., 2003), which is among the state-of-the-art systems today, word alignments are used as a basis for extracting phrases and estimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P , links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard does not make a distinction"
W14-3334,2007.tmi-papers.21,0,0.619227,"eparate tunings for each alignment. While the absolute results varied somewhat, the correlations with alignment indicators were stable. SMT Experiments For reference, we first study the impact of alignment on SMT performance. Our SMT system is a standard PBSMT system trained on WMT13 280 inter gd gdfa gdf union m2 18.1 20.4 20.4 19.4 19.2 m3 19.1 20.9 20.7 19.7 19.6 m4 19.3 20.9 20.8 20.1 19.8 HMM 18.8 20.5 20.5 19.9 19.7 fa 18.9 20.6 20.5 20.0 20.0 always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice of possible reorderings (Rottmann and Vogel, 2007; Zhang et al., 2007). Table 4: Baseline Bleu scores for different symmetrization heuristics suggesting that they measure similar things. Intuitively it seems important for SMT to match full translation units, but it might be the case that the phrase extraction strategy is robust as long as there are"
W14-3334,H01-1035,0,0.0602033,"nt of Linguistics and Philology firstname.lastname@lingfil.uu.se Abstract quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study the relation between alignment and translation quality, with the exception of Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignments. For instance, source language reordering commonly relies on rules learnt automatically from word-aligned data (e.g., Xia and McCord (2004)). As far as we know, no one has studied the impact of alignment quality on these additional tasks, and it seems to be tacitly assumed that alignments that are good for translation are also good for other tasks."
W14-3334,W09-2303,0,0.023373,"ndicators. The first group is the classic group where metrics are calculated on the alignment link level, which has been used in several studies. In our experiments we use a gold standard that does not make use of distinctions between sure and possible links, as suggested by Fraser and Marcu (2007). With this, we can calculate the standard metrics P(recision) R(ecall) and F(-measure). We will mainly use balanced F-measure, but occasionally also report weighted F-measure. As noted before, 1−AER is equivalent to balanced F when only sure links are used, and will thus not be reported separately. Søgaard and Kuhn (2009) and Søgaard and Wu (2009) suggested working on the translation unit (TU) level, instead of the link level. A translation unit, or cept (Goutte et al., 2004), is defined as a maximally connected subgraph of an alignment. In Figure 1, the twelve links form nine translation units. Søgaard and Wu (2009) suggest the metric TUER, translation unit error rate, shown in Eq. 5, where AU are hypothesized translation units, and GU are gold standard translation units.1 They use TUER to establish lower bounds for the coverage of alignments from different formalisms, not to evaluate SMT. While they only use"
W14-3334,2007.iwslt-1.3,0,0.141624,"20.4 19.4 19.2 m3 19.1 20.9 20.7 19.7 19.6 m4 19.3 20.9 20.8 20.1 19.8 HMM 18.8 20.5 20.5 19.9 19.7 fa 18.9 20.6 20.5 20.0 20.0 always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice of possible reorderings (Rottmann and Vogel, 2007; Zhang et al., 2007). Table 4: Baseline Bleu scores for different symmetrization heuristics suggesting that they measure similar things. Intuitively it seems important for SMT to match full translation units, but it might be the case that the phrase extraction strategy is robust as long as there are partial matches. There are no significant correlations with link degree or link crossings, except a negative correlation with Crossdiff, which means that it is good to have a similar number of crossings as the baseline. These results confirm results from previous studies that link level measures, especially recall and"
W14-4015,P13-4033,1,0.886979,"on of this architecture with the aim to improve consistency and coherence of Machine Translation. The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup. 1 Introduction Machine Translation (MT) systems are nowadays achieving a high-quality performance. However, they are typically developed at sentence level using only local information and ignoring the document-level one. Recent work claims that discourse-wide context can help to translate individual words in a way that leads to more coherent translations (Hardmeier et al., 2013; Hardmeier et al., 2012; Gong et al., 2011; Xiao et al., 2011). Standard SMT systems use n-gram models to represent words in the target language. However, there are other word representation techniques that use vectors of contextual information. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between wo"
W14-4015,P07-2045,0,0.00478793,"arding their accuracy when trying to predict related words (Section 3.1) and also regarding its possible effect within a translation system (Section 3.2). In both cases one observes that the quality of the translation and alignments previous to building the semantic models are bottlenecks for the final performance: part of the vocabulary, and therefore translation pairs, are lost in the training process. Future work includes studying different kinds of alignment heuristics. We plan to develop new features based on the semantic models to use them inside state-of-the-art SMT systems like Moses (Koehn et al., 2007) or discourse-oriented decoders like Docent (Hardmeier et al., 2013). 3.2 Cross-Lingual Lexical Substitution Another way to evaluate the semantic models is through the effect they have in translation. We implemented the Cross-Lingual Lexical Substitution task carried out in SemEval-2010 (Task2, 2010) 133 References Z. Gong, M. Zhang, and G. Zhou. 2011. Cache-based document-level statistical machine translation. In Proc. of the 2011 Conference on Empirical Methods in NLP, pages 909–919, UK. C. Hardmeier, J. Nivre, and J. Tiedemann. 2012. Document-wide decoding for phrase-based statistical machi"
W14-4015,W09-2412,0,0.0379861,"Missing"
W14-4015,tiedemann-2012-parallel,1,0.865268,"Missing"
W14-4015,2011.mtsummit-papers.13,0,0.179084,"erence of Machine Translation. The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup. 1 Introduction Machine Translation (MT) systems are nowadays achieving a high-quality performance. However, they are typically developed at sentence level using only local information and ignoring the document-level one. Recent work claims that discourse-wide context can help to translate individual words in a way that leads to more coherent translations (Hardmeier et al., 2013; Hardmeier et al., 2012; Gong et al., 2011; Xiao et al., 2011). Standard SMT systems use n-gram models to represent words in the target language. However, there are other word representation techniques that use vectors of contextual information. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between words and even across languages. However, they are not able to ha"
W14-4015,D11-1084,0,\N,Missing
W14-4015,D12-1108,1,\N,Missing
W14-4203,agic-ljubesic-2014-setimes,1,0.849797,"Missing"
W14-4203,D12-1001,0,0.038206,"tively small POS tagsets. In our contribution, the goal is to observe the properties of cross-lingual parsing in an environment of relatively free-word-order languages, which are related and characterized by rich morphology and very large morphosyntactic tagsets. We experiment with four different small- and medium-size dependency treebanks of Croatian and Slovene, and cross-lingually parse into Croatian, Serbian and Slovene. Along with monolingual and direct transfer parsing, we make use of the SMT framework of Tiedemann et al. (2014). We are motivated by: ∎ ∎ Other approaches: More recently, Durrett et al. (2012) suggested a hybrid approach that involves bilingual lexica in cross-lingual phrasebased parsing. In their approach, a source-side treebank is adapted to a target language by ”translating” the source words to target words through a bilingual lexicon. This approach is advanced by Tiedemann et al. (2014), who utilize fullscale statistical machine translation (SMT) systems for generating synthetic target language treebanks. This approach relates to annotation projection, while bypassing the issue of dependency parsing noise as gold standard annotations are projected. The SMT noise is in turn miti"
W14-4203,dzeroski-etal-2006-towards,0,0.0614452,"Missing"
W14-4203,erjavec-etal-2010-jos,1,0.904291,"Missing"
W14-4203,W13-4903,1,0.893948,"Missing"
W14-4203,agic-etal-2014-croatian,1,0.86035,"Missing"
W14-4203,W11-2123,0,0.0105658,"eebank translation experiments. Namely, our translations are based on automatic word alignment and subsequent extraction of translation equivalents as common in phrase-based SMT. We perform word alignment by using GIZA++ (Och and Ney, 2003), while utilizing IBM model 4 for creating the Viterbi word alignments for parallel corpora. For the extraction of translation tables, we use the de facto standard SMT toolbox Moses (Koehn et al., 2007) with default settings. Phrasebased SMT models are tuned using minimum error rate training (Och, 2003). Our monolingual language modeling using KenLM tools9 (Heafield, 2011) produces standard 5-gram language models using modified Kneser-Ney smoothing without pruning. For building the translation models, we use the OpenSubtitles parallel resources from OPUS10 (Tiedemann, 2009) for the Croatian-Slovene pair. Even if we expect this to be a rather noisy parallel resource, we justify the choice by (1) the fact that no other parallel corpora11 of Croatian and Slovene exist, other than Orwell’s 1984 from the Multext East project, which is too small for SMT training and falls into a very narrow domain, and (2) evidence from (Tiedemann et al., 2014) that the SMT-supported"
W14-4203,P07-2045,0,0.0036197,"cies are simply copied. Treebank Translation and Annotation Projection For machine translation, we closely adhere to the setup implemented by Tiedemann et al. (2014) in their treebank translation experiments. Namely, our translations are based on automatic word alignment and subsequent extraction of translation equivalents as common in phrase-based SMT. We perform word alignment by using GIZA++ (Och and Ney, 2003), while utilizing IBM model 4 for creating the Viterbi word alignments for parallel corpora. For the extraction of translation tables, we use the de facto standard SMT toolbox Moses (Koehn et al., 2007) with default settings. Phrasebased SMT models are tuned using minimum error rate training (Och, 2003). Our monolingual language modeling using KenLM tools9 (Heafield, 2011) produces standard 5-gram language models using modified Kneser-Ney smoothing without pruning. For building the translation models, we use the OpenSubtitles parallel resources from OPUS10 (Tiedemann, 2009) for the Croatian-Slovene pair. Even if we expect this to be a rather noisy parallel resource, we justify the choice by (1) the fact that no other parallel corpora11 of Croatian and Slovene exist, other than Orwell’s 1984"
W14-4203,berovic-etal-2012-croatian,1,0.897051,"Missing"
W14-4203,H05-1066,0,0.170863,"Missing"
W14-4203,E12-1009,0,0.0200767,"In contrast, the MTE 4 tagsets are not adjusted, i.e., each test set only has a single language-specific MTE 4 annotation. We rely on their underlying similarities in feature representations to suffice for improved cross-lingual parsing performance. 3 3.2 In all experiments, we use the graph-based dependency parser by Bohnet (2010) with default settings. We base our parser choice on its stateof-the-art performance across various morphologically rich languages in the SPMLR 2013 shared task (Seddah et al., 2013). While newer contributions targeted at joint morphological and syntactic analysis (Bohnet and Kuhn, 2012; Bohnet et al., 2013) report slightly higher scores, we chose the former one for speed and robustness, and because we use gold standard POS/MSD annotations. The choice of gold standard preprocessing is motivated by previous research in parsing Croatian and Serbian (Agi´c et al., 2013), and by insight of Seddah et al. (2013), who report a predictable linear decrease in accuracy for automatic preprocessing. This decrease amounts to approximately 3 points LAS for Croatian and Serbian across various test cases in (Agi´c et al., 2013). We observe effects of (de)lexicalization and of using full MSD"
W14-4203,D11-1006,0,0.0813785,"from the source treebank for training the parser (Zeman and Resnik, 2008; McDonald et al., 2013). This in turn relies on the same underlying feature model, typically drawing from a shared part-of-speech (POS) representation such as the Universal POS Tagset of Petrov et al. (2012). Negative effects of using such an impoverished shared representation are typically addressed by adapting the model to better fit the target language. This includes selecting source language data points appropriate for the target language (Søgaard, 2011; T¨ackstr¨om et al., 2013), transferring from multiple sources (McDonald et al., 2011) and using cross-lingual word clusters (T¨ackstr¨om et al., 2012). These approaches need no projection and enable the usage of source-side gold standard annotations, but they all rely on a shared feature representation across languages, which can be seen as a strong bottleneck. Also, while most of the earlier research made use of heterogenous treebanks and thus yielded linguistically implausible observations, research stemming from an uniform dependency scheme across languages (De Marneffe and Manning, 2008; McDonald et al., 2013) made it possible to perform more consistent experiments and to"
W14-4203,Q13-1034,0,0.0226137,"tagsets are not adjusted, i.e., each test set only has a single language-specific MTE 4 annotation. We rely on their underlying similarities in feature representations to suffice for improved cross-lingual parsing performance. 3 3.2 In all experiments, we use the graph-based dependency parser by Bohnet (2010) with default settings. We base our parser choice on its stateof-the-art performance across various morphologically rich languages in the SPMLR 2013 shared task (Seddah et al., 2013). While newer contributions targeted at joint morphological and syntactic analysis (Bohnet and Kuhn, 2012; Bohnet et al., 2013) report slightly higher scores, we chose the former one for speed and robustness, and because we use gold standard POS/MSD annotations. The choice of gold standard preprocessing is motivated by previous research in parsing Croatian and Serbian (Agi´c et al., 2013), and by insight of Seddah et al. (2013), who report a predictable linear decrease in accuracy for automatic preprocessing. This decrease amounts to approximately 3 points LAS for Croatian and Serbian across various test cases in (Agi´c et al., 2013). We observe effects of (de)lexicalization and of using full MSD tagset as opposed to"
W14-4203,C10-1011,0,0.0126143,"ne. The attributes are language-dependent, as well as their positions in the tag, which are also dependent on the part of speech, denoted by position zero in the tag. and we leave SMT and annotation projection into Serbian for future work. tation layer matching its training set. In contrast, the MTE 4 tagsets are not adjusted, i.e., each test set only has a single language-specific MTE 4 annotation. We rely on their underlying similarities in feature representations to suffice for improved cross-lingual parsing performance. 3 3.2 In all experiments, we use the graph-based dependency parser by Bohnet (2010) with default settings. We base our parser choice on its stateof-the-art performance across various morphologically rich languages in the SPMLR 2013 shared task (Seddah et al., 2013). While newer contributions targeted at joint morphological and syntactic analysis (Bohnet and Kuhn, 2012; Bohnet et al., 2013) report slightly higher scores, we chose the former one for speed and robustness, and because we use gold standard POS/MSD annotations. The choice of gold standard preprocessing is motivated by previous research in parsing Croatian and Serbian (Agi´c et al., 2013), and by insight of Seddah"
W14-4203,W06-2920,0,0.559283,"ing from the Multext East project (Erjavec, 2012). A detailed assessment of the current state of development for morphosyntactic and syntactic processing of these languages is given by Agi´c et al. (2013) and Uszkoreit and Rehm (2012). Here, we provide only a short description. 2.1 sl PDT: The PDT-based Slovene Dependency Treebank (Dˇzeroski et al., 2006) is built on top of a rather small portion of Orwell’s novel 1984 from the Multext East project (Erjavec, 2012). Even if the project was discontinued, it is still heavily used as part of the venerable CoNLL 2006 and 2007 shared task datasets (Buchholz and Marsi, 2006; Nivre et al., 2007).4 Treebanks sl SSJ: The Slovene take on simplifying syntactic annotations resulted in the 10-tag strong JOS Corpus of Slovene (Erjavec et al., 2010). Similar to hr SET, this new annotation scheme is loosely We use two Croatian and two Slovene dependency treebanks.1 One for each language is based on the Prague Dependency Treebank (PDT) (B¨ohmov´a et al., 2003) annotation scheme, while the other two introduced novel and more simplified syntactic tagsets. All four treebanks use adaptations of 2 HOBS is available through META-SHARE (Tadi´c and V´aradi, 2012). 3 http://nlp.ffz"
W14-4203,W08-1301,0,0.120382,"Missing"
W14-4203,J03-1002,0,0.00392914,"om full-blown SMT phrase tables on a much larger scale. The trees projection from source to target is trivial since the number and the ordering of words between them does not change. Thus, the dependencies are simply copied. Treebank Translation and Annotation Projection For machine translation, we closely adhere to the setup implemented by Tiedemann et al. (2014) in their treebank translation experiments. Namely, our translations are based on automatic word alignment and subsequent extraction of translation equivalents as common in phrase-based SMT. We perform word alignment by using GIZA++ (Och and Ney, 2003), while utilizing IBM model 4 for creating the Viterbi word alignments for parallel corpora. For the extraction of translation tables, we use the de facto standard SMT toolbox Moses (Koehn et al., 2007) with default settings. Phrasebased SMT models are tuned using minimum error rate training (Och, 2003). Our monolingual language modeling using KenLM tools9 (Heafield, 2011) produces standard 5-gram language models using modified Kneser-Ney smoothing without pruning. For building the translation models, we use the OpenSubtitles parallel resources from OPUS10 (Tiedemann, 2009) for the Croatian-Sl"
W14-4203,P03-1021,0,0.0162054,"re to the setup implemented by Tiedemann et al. (2014) in their treebank translation experiments. Namely, our translations are based on automatic word alignment and subsequent extraction of translation equivalents as common in phrase-based SMT. We perform word alignment by using GIZA++ (Och and Ney, 2003), while utilizing IBM model 4 for creating the Viterbi word alignments for parallel corpora. For the extraction of translation tables, we use the de facto standard SMT toolbox Moses (Koehn et al., 2007) with default settings. Phrasebased SMT models are tuned using minimum error rate training (Och, 2003). Our monolingual language modeling using KenLM tools9 (Heafield, 2011) produces standard 5-gram language models using modified Kneser-Ney smoothing without pruning. For building the translation models, we use the OpenSubtitles parallel resources from OPUS10 (Tiedemann, 2009) for the Croatian-Slovene pair. Even if we expect this to be a rather noisy parallel resource, we justify the choice by (1) the fact that no other parallel corpora11 of Croatian and Slovene exist, other than Orwell’s 1984 from the Multext East project, which is too small for SMT training and falls into a very narrow domain"
W14-4203,E12-1015,1,0.822716,"al., 2014) that the SMT-supported cross-lingual parsing approach is very robust to translation noise. For translating Croatian treebanks into Slovene and vice versa, we implement and test four different methods of translation. They are coupled with approaches to annotation projection from the source side gold dependency trees to the target translations via the word alignment information available from SMT. CHAR: By this acronym, we refer to an approach known as character-based statistical machine translation. It is shown to perform very well for closely related languages (Vilar et al., 2007; Tiedemann, 2012; Tiedemann and Nakov, 2013). The motivation for character-level translation is the ability of such models to better generalize the mapping between similar languages especially in cases of rich productive morphology and limited amounts of training data. With this, character-level models largely reduce the number of out-of-vocabulary words. In a nutshell, our character-based model performs word-to-word translation using character-level modeling. Similar to LOOKUP, this is also a word-to-word translation model, which also requires no adaptation of the source dependency trees – they are once agai"
W14-4203,petrov-etal-2012-universal,0,0.133887,"Missing"
W14-4203,C14-1175,1,0.913328,"h efforts directed towards their processing despite 13 Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 13–24, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics dency treebank, i.e., it is already manually annotated for syntactic dependencies (Agi´c et al., 2012). This removes the automatic parsing noise, while the issues with word alignment and annotation heuristics still remain. better word alignment quality for synthetic data. The influence of various projection algorithms in this approach is further investigated by Tiedemann (2014). This line of cross-lingual parsing research substantially improves over previous work. Model transfer: In its simplest form, transferring a model amounts to training a source language parser and running it directly on the target language. It is usually coupled with delexicalization, i.e., removing all lexical features from the source treebank for training the parser (Zeman and Resnik, 2008; McDonald et al., 2013). This in turn relies on the same underlying feature model, typically drawing from a shared part-of-speech (POS) representation such as the Universal POS Tagset of Petrov et al. (201"
W14-4203,W00-0726,0,0.19959,"Missing"
W14-4203,rosa-etal-2014-hamledt,0,0.0286999,"Missing"
W14-4203,W07-0705,0,0.0138065,"e from (Tiedemann et al., 2014) that the SMT-supported cross-lingual parsing approach is very robust to translation noise. For translating Croatian treebanks into Slovene and vice versa, we implement and test four different methods of translation. They are coupled with approaches to annotation projection from the source side gold dependency trees to the target translations via the word alignment information available from SMT. CHAR: By this acronym, we refer to an approach known as character-based statistical machine translation. It is shown to perform very well for closely related languages (Vilar et al., 2007; Tiedemann, 2012; Tiedemann and Nakov, 2013). The motivation for character-level translation is the ability of such models to better generalize the mapping between similar languages especially in cases of rich productive morphology and limited amounts of training data. With this, character-level models largely reduce the number of out-of-vocabulary words. In a nutshell, our character-based model performs word-to-word translation using character-level modeling. Similar to LOOKUP, this is also a word-to-word translation model, which also requires no adaptation of the source dependency trees – t"
W14-4203,P11-2120,0,0.0583405,"is usually coupled with delexicalization, i.e., removing all lexical features from the source treebank for training the parser (Zeman and Resnik, 2008; McDonald et al., 2013). This in turn relies on the same underlying feature model, typically drawing from a shared part-of-speech (POS) representation such as the Universal POS Tagset of Petrov et al. (2012). Negative effects of using such an impoverished shared representation are typically addressed by adapting the model to better fit the target language. This includes selecting source language data points appropriate for the target language (Søgaard, 2011; T¨ackstr¨om et al., 2013), transferring from multiple sources (McDonald et al., 2011) and using cross-lingual word clusters (T¨ackstr¨om et al., 2012). These approaches need no projection and enable the usage of source-side gold standard annotations, but they all rely on a shared feature representation across languages, which can be seen as a strong bottleneck. Also, while most of the earlier research made use of heterogenous treebanks and thus yielded linguistically implausible observations, research stemming from an uniform dependency scheme across languages (De Marneffe and Manning, 2008;"
W14-4203,zeman-etal-2012-hamledt,0,0.0349623,"Missing"
W14-4203,H01-1035,0,0.288855,"ing the under-resourced (target) language. Annotation projection: In this approach, dependency trees are projected from a source language to a target language using word alignments in parallel corpora. It is based on a presumption that source-target parallel corpora are more readily available than dependency treebanks. The approach comes in two varieties. In the first one, parallel corpora are exploited by applying the available state-of-the-art parsers on the source side and subsequent projection to the target side using word alignments and heuristics for resolving possible link ambiguities (Yarowsky et al., 2001; Hwa et al., 2005). Since dependency parsers typically make heavy use of various morphological and other features, the apparent benefit of this approach is the possibility of straightforward projection of these features, resulting in a featurerich representation for the target language. On the downside, the annotation projection noise adds up to dependency parsing noise and errors in word alignment, influencing the quality of the resulting target language parser. The other variety is rare, since it relies on parallel corpora in which the source side is a depenIntroduction A large majority of"
W14-4203,I08-3008,0,0.473743,"the issues with word alignment and annotation heuristics still remain. better word alignment quality for synthetic data. The influence of various projection algorithms in this approach is further investigated by Tiedemann (2014). This line of cross-lingual parsing research substantially improves over previous work. Model transfer: In its simplest form, transferring a model amounts to training a source language parser and running it directly on the target language. It is usually coupled with delexicalization, i.e., removing all lexical features from the source treebank for training the parser (Zeman and Resnik, 2008; McDonald et al., 2013). This in turn relies on the same underlying feature model, typically drawing from a shared part-of-speech (POS) representation such as the Universal POS Tagset of Petrov et al. (2012). Negative effects of using such an impoverished shared representation are typically addressed by adapting the model to better fit the target language. This includes selecting source language data points appropriate for the target language (Søgaard, 2011; T¨ackstr¨om et al., 2013), transferring from multiple sources (McDonald et al., 2011) and using cross-lingual word clusters (T¨ackstr¨om"
W14-4203,N12-1052,0,0.0746976,"Missing"
W14-4203,N13-1126,0,0.244645,"Missing"
W14-4203,W03-2906,0,0.0299337,"Missing"
W14-4203,C12-3054,0,0.062378,"Missing"
W14-4203,R13-1088,1,0.834557,"the SMT-supported cross-lingual parsing approach is very robust to translation noise. For translating Croatian treebanks into Slovene and vice versa, we implement and test four different methods of translation. They are coupled with approaches to annotation projection from the source side gold dependency trees to the target translations via the word alignment information available from SMT. CHAR: By this acronym, we refer to an approach known as character-based statistical machine translation. It is shown to perform very well for closely related languages (Vilar et al., 2007; Tiedemann, 2012; Tiedemann and Nakov, 2013). The motivation for character-level translation is the ability of such models to better generalize the mapping between similar languages especially in cases of rich productive morphology and limited amounts of training data. With this, character-level models largely reduce the number of out-of-vocabulary words. In a nutshell, our character-based model performs word-to-word translation using character-level modeling. Similar to LOOKUP, this is also a word-to-word translation model, which also requires no adaptation of the source dependency trees – they are once again simply copied to target se"
W14-4203,W14-1614,1,0.849521,"Missing"
W14-4203,W13-4917,0,\N,Missing
W14-4203,D07-1096,0,\N,Missing
W14-4203,P13-2017,0,\N,Missing
W14-5307,W14-5316,0,0.368134,"Missing"
W14-5307,Y08-1042,0,0.178551,"Groups - DSL 2014 Shared Task For this collection, randomly sampled sentences from journalistic corpora (and corpora collections) were selected for each of the 13 classes. Journalistic corpora were preferred because they represent standard language, which is an important factor to be considered when working with language varieties. Other data sources (e.g. Wikipedia) do not make any distinction between language varieties and they are therefore not suitable for the purpose of the shared task. A number of studies mentioned in the related work section use journalistic texts for similar reasons (Huang and Lee, 2008; Grouin et al., 2010; Zampieri and Gebre, 2012) Given what has been said in this section, we consider the collection to be a suitable comparable corpora from this task, which was compiled to avoid bias in classification towards source, register and topics. The 5 6 We considered a token as orthographic units delimited by white spaces. http://www.loc.gov/standards/iso639-2/php/English_list.php 60 DSL corpus collection was distributed in tab delimited format; the first column contains a sentence in the language/variety, the second column states its group and the last column refers to its languag"
W14-5307,W14-5317,0,0.274993,"within each group: binary for groups B-F and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian). An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called ‘white lists’ inspired by the ‘blacklist’ classifier (Tiedemann and Ljubeˇsi´c, 2012). These lists are word lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has been widely used for general-purpose language identification and its performance is regarded superior to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hierarchical"
W14-5307,P12-3005,0,0.0284087,"or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has been widely used for general-purpose language identification and its performance is regarded superior to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hierarchically firstly identifying the language group that a sentence belongs to and subsequently the specific language, achieving performance comparable to the state-of-the-art, but still slightly below the other three systems. The QMUL team (Purver, 2014) proposed a linear SVM classifier using words and characters as features. The author investigated the influence of the cost par"
W14-5307,U13-1003,0,0.384905,"between more languages1 , they still struggle to discriminate between similar languages such as Croatian and Serbian or Malay and Indonesian. From an NLP point of view, the difficulty systems face when discriminating between closely related languages is similar to the problem of discriminating between standard national language varieties (e.g. American English and British English or Brazilian Portuguese and European Portuguese), henceforth varieties. Recent studies show that language varieties can be discriminated automatically using words or characters as features (Zampieri and Gebre, 2012; Lui and Cook, 2013) . However, due to performance limitations, state-of-the-art general-purpose language identification systems do not distinguish texts from different national varieties, modelling pluricentric languages as unique classes. To evaluate how state-of-the-art systems perform in identifying similar languages and varieties, we decided to organize the Discriminating between Similar Languages (DSL)2 shared task. This shared task was organized within the scope of the workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial) in the 2014 edition of COLING. The motivation behind"
W14-5307,W14-5315,0,0.274321,"B-F and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian). An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called ‘white lists’ inspired by the ‘blacklist’ classifier (Tiedemann and Ljubeˇsi´c, 2012). These lists are word lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has been widely used for general-purpose language identification and its performance is regarded superior to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hierarchically firstly identifying the language"
W14-5307,W14-5314,0,0.685536,"ly: NRCCNRC, RAE, UMich, UniMelb-NLP and QMUL. The best scores were obtained by the NRC-CNRC (Goutte et al., 2014) team which proposed a twostep approach to predict first the language group than the language of each instance. The language group was predicted in a 6-way classification using a probabilistic model similar to a Naive Bayes classifier, and later the method applied SVM classifiers to discriminate within each group: binary for groups B-F and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian). An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called ‘white lists’ inspired by the ‘blacklist’ classifier (Tiedemann and Ljubeˇsi´c, 2012). These lists are word lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexical"
W14-5307,W13-1706,0,0.0421907,"own (2013) reports results on a system trained to recognize more than 1,100 languages 2 http://corporavm.uni-koeln.de/vardial/sharedtask.html 58 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58–67, Dublin, Ireland, August 23 2014. decade in which they were published. Other related shared tasks include the ALTW 2010 multilingual language identification shared task, a general-purpose language identification task containing data from 74 languages (Baldwin and Lui, 2010) and finally the Native Language Identification (NLI) shared task (Tetreault et al., 2013) where participants were provided English essays written by foreign students of 11 different mother tongues (Blanchard et al., 2013). Participants had to train their systems to identify the native language of the writer of each text. 2 Related Work Among the first studies to investigate the question of discriminating between similar languages is the study published by Ranaivo-Malanc¸on (2006). The author presents a semi-supervised model to distinguish between Indonesian and Malay, two closely related languages from the Austronesian family represented in the DSL shared task. The study uses the"
W14-5307,C12-1160,1,0.601337,"Missing"
W14-5307,U10-1003,0,\N,Missing
W14-5307,J14-1006,0,\N,Missing
W14-5307,W14-5318,0,\N,Missing
W15-1824,E12-2012,0,0.134186,"ll unaligned target words. These heuristics ensure that the projected structures are proper trees and that we can train dependency parsers that are capable of handling non-projective structures without modification. Note that POS tags are also projected along the remaining word alignments and that some words obtain dummy tags if there is no relation to a source language token that could be used for projection. In all our experiments, we apply MaltParser (Nivre et al., 2006) to train transition-based dependency parsers and we optimize feature models and learning parameters using MaltOptimizer (Ballesteros and Nivre, 2012). The parameters and feature models for the cross-lingual models are directly copied from the source language model in order to apply a realistic scenario for which no tuning data for the target language would be available. Table 1 lists the results in terms of labeled attachment scores of our baseline models for all language pairs in the test set. Rows correspond to each source language and columns represent the target language used for testing. Note that we restrict all our experiments to the languages for which the same kind of parallel data is available in Europarl. The baseline scores are"
W15-1824,P07-2053,0,0.0832032,"Missing"
W15-1824,P13-2121,0,0.0505768,"Missing"
W15-1824,N03-1017,0,0.029351,"ion (including POS labels) to the target side of the parallel corpus using word alignment links and the direct correspondence assumption. 3. Train a parser on the projected data and evaluate its performance on the test sets of the universal treebank for the target language. Word alignments are produced using IBM model 4 as implemented in GIZA++ (Och and Ney, 2003) trained in the typical pipeline as it is common in statistical machine translation using the Moses toolbox (Koehn et al., 2007). The asymmetric alignments are symmetrized with the intersection and the grow-diag-final-and heuristics (Koehn et al., 2003). We use the latter for the basic annotation projection presented in the next section. For evaluation, we use the test sets provided by the Universal Dependency Treebank (UDT) version 1 (McDonald et al., 2013). The harmonized annotation makes it possible to perform a fair evaluation across languages including labeled attachment scores, which we use as our essential evaluation metric. Note that all scores include attachments of punctuation which makes our results directly comparable to the results presented in the related literature (Tiedemann, 2014). 2.1 Baseline For our experiments, we use 40"
W15-1824,P07-2045,0,0.0180676,"Missing"
W15-1824,2005.mtsummit-papers.11,0,0.430228,"Missing"
W15-1824,J03-1002,0,0.0138302,"is based on the projection of syntactic information using existing parallel corpora. The basic setup is as follows: 1. Parse the source side of the parallel corpus with a parser trained on the source language treebank. 2. Project the syntactic information (including POS labels) to the target side of the parallel corpus using word alignment links and the direct correspondence assumption. 3. Train a parser on the projected data and evaluate its performance on the test sets of the universal treebank for the target language. Word alignments are produced using IBM model 4 as implemented in GIZA++ (Och and Ney, 2003) trained in the typical pipeline as it is common in statistical machine translation using the Moses toolbox (Koehn et al., 2007). The asymmetric alignments are symmetrized with the intersection and the grow-diag-final-and heuristics (Koehn et al., 2003). We use the latter for the basic annotation projection presented in the next section. For evaluation, we use the test sets provided by the Universal Dependency Treebank (UDT) version 1 (McDonald et al., 2013). The harmonized annotation makes it possible to perform a fair evaluation across languages including labeled attachment scores, which we"
W15-1824,P03-1021,0,0.0374721,"ed Treebanks Treebank translation has been proposed by Tiedemann et al. (2014). In this paper, we would like to explore the impact of our modifications of the projection algorithm on that approach as well. For this, we use the training sets of the Universal Dependency Treebank and translate them with standard SMT models to the target languages we would like to test. Our setup is very generic and uses the Moses toolbox for training, tuning and decoding. The translation models are trained on the entire Europarl corpus version 7 without language-pairspecific optimization. For tuning we use MERT (Och, 2003) and the newstest 2011 data provided by the annual workshop on statistical machine translation.1 The language model is a standard 5-gram model and is based on a combination of Europarl and News data provided from the same source. We apply modified Kneser-Ney smoothing without pruning, applying KenLM tools (Heafield et al., 2013) for estimating the LM parameters. 3.1 Phrase-based SMT Our baseline system is a standard phrase-based model and we use the standard DCA projection algorithm as proposed by Hwa et al. (2005). The results are shown in Table 8. With this, we can confirm the findings of Ti"
W15-1824,petrov-etal-2012-universal,0,0.179301,"Missing"
W15-1824,N12-1052,0,0.349649,"Missing"
W15-1824,N13-1126,0,0.316294,"Missing"
W15-1824,W14-1614,1,0.868602,"Missing"
W15-1824,tiedemann-2012-parallel,1,0.803618,"s based on a combination of Europarl and News data provided from the same source. We apply modified Kneser-Ney smoothing without pruning, applying KenLM tools (Heafield et al., 2013) for estimating the LM parameters. 3.1 Phrase-based SMT Our baseline system is a standard phrase-based model and we use the standard DCA projection algorithm as proposed by Hwa et al. (2005). The results are shown in Table 8. With this, we can confirm the findings of Tiedemann (2014) that the translation approach has some 1 http://www.statmt.org/wmt14. For Swedish we use a sample from the OpenSubtitles2012 corpus (Tiedemann, 2012). Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 195 DE EN ES FR SV DE EN ES FR SV – 53.09-2.69 50.54-2.40 49.89-3.19 53.83-1.29 53.36(4.55) – 50.39(2.65) 53.65(3.10) 50.93(2.17) 54.72-2.04 60.81(0.54) – 65.05(0.64) 60.61-0.15 58.07-0.45 64.23(2.37) 66.10(0.98) – 60.46-1.14 59.84-0.49 63.43(2.02) 60.56-0.41 58.38(0.78) – Table 8: Treebank translation with DCA-based projection (compared to the projection of parallel data from Table 1). advantages over the projection of automatically annotated parallel corpora. For some language pairs, the labeled attachme"
W15-1824,C14-1175,1,0.782044,"n and the grow-diag-final-and heuristics (Koehn et al., 2003). We use the latter for the basic annotation projection presented in the next section. For evaluation, we use the test sets provided by the Universal Dependency Treebank (UDT) version 1 (McDonald et al., 2013). The harmonized annotation makes it possible to perform a fair evaluation across languages including labeled attachment scores, which we use as our essential evaluation metric. Note that all scores include attachments of punctuation which makes our results directly comparable to the results presented in the related literature (Tiedemann, 2014). 2.1 Baseline For our experiments, we use 40,000 sentences from Europarl (Koehn, 2005) for each language pair following the basic setup of Tiedemann (2014). The baseline model applies the projection heuristics as presented by Hwa et al. (2005): one-to-one: For one-to-one alignments between the source words si and s j and the target words tx and ty : Copy the relation R(si , s j ) to R(tx ,ty ). unaligned source: Add dummy nodes in the target language that take all incoming and outgoing arcs of the unaligned source language word. one-to-many: Add a dummy node in the target sentence and attach"
W15-1824,H01-1035,0,0.29249,"n delexicalized parsers that heavily rely on universal POS tags. This simple technique has shown some success for closely related languages (McDonald et al., 2013). Several improvements can be achieved by using multiple source languages (McDonald et al., 2011; Naseem et al., 2012) and additional cross-lingual features that can be used to transfer models to a new language such as cross-lingual word clusters (T¨ackstr¨om et al., 2012) or word-typology information (T¨ackstr¨om et al., 2013). Annotation projection has already a long tradition in NLP. Initially proposed for tasks like POS tagging (Yarowsky et al., 2001), the seminal work for annotation projection in dependency parsing is presented by Hwa et al. (2005). The general idea is to make use of parallel corpora and automatic word alignment to transfer information from the source language to a target language translation that can then be used for training parsers. In most cases, treebanks are not taken from parallel corpora and, therefore, one has to rely on automatic annotation of the source language part of another (usually unrelated) bitext. Together with the noise in automatic word alignment, these steps are bottlenecks in the projection strategy"
W15-1824,D11-1006,0,0.234183,"Missing"
W15-1824,P80-1024,0,0.778134,"Missing"
W15-1824,P12-1066,0,0.155593,"gies that can be found in the literature: annotation projection and model transfer. Model transfer has attracted a lot of interest recently due to the availability of cross-lingually harmonized annotation (Petrov et al., 2012) that makes it possible to use universal features across languages. The most straightforward technique is to train delexicalized parsers that heavily rely on universal POS tags. This simple technique has shown some success for closely related languages (McDonald et al., 2013). Several improvements can be achieved by using multiple source languages (McDonald et al., 2011; Naseem et al., 2012) and additional cross-lingual features that can be used to transfer models to a new language such as cross-lingual word clusters (T¨ackstr¨om et al., 2012) or word-typology information (T¨ackstr¨om et al., 2013). Annotation projection has already a long tradition in NLP. Initially proposed for tasks like POS tagging (Yarowsky et al., 2001), the seminal work for annotation projection in dependency parsing is presented by Hwa et al. (2005). The general idea is to make use of parallel corpora and automatic word alignment to transfer information from the source language to a target language transl"
W15-1824,nivre-etal-2006-maltparser,0,0.137856,"Missing"
W15-2137,N15-1055,0,0.0506137,"Missing"
W15-2137,P12-1066,0,0.0434359,"new languages. There are basically two types of transfer that have been proposed in the literature: data transfer approaches and model transfer approaches. The former emphasizes the projection of data sets to new languages and it usually relies on parallel data sets and word alignment (Hwa et al., 2005; Tiedemann, 2014). Recently, machine translation was also introduced as yet another alternative to data transfer (Tiedemann et al., 2014). In model transfer, one tries to port existing parsers to new languages by (i) relying on universal features (McDonald et al., 2013; McDonald et al., 2011a; Naseem et al., 2012) and (ii) by adapting model parameters to the target language (T¨ackstr¨om et al., 2013). Universal features may refer to coarse part-of-speech sets that represent common word classes (Petrov et al., 2012) and may also include language-set-specific features such as cross-lingual word clusters (T¨ackstr¨om et al., 2012) or bilingual word embeddings (Xiao and Guo, 2014). Target language adaptation can be done using external linguistic resources such as prior knowledge about language families or lexical databases or any other existing tool for the target language. This paper is focused on data tr"
W15-2137,E12-1009,0,0.0496271,"eled structures. The contributions of this paper include the presentation of monolingual and cross-lingual baseline models for the recently published universal dependencies data sets (UD; release 1.0)1 and a detailed discussion of the impact of PoS labels. We systematically compare results on standard test sets with gold labels with corresponding experiments that rely on predicted labels, which reflects the typical real-world scenario. Let us first look at baseline models before starting our discussion of cross-lingual approaches. In all our experiments, we apply the Mate tools (Bohnet, 2010; Bohnet and Kuhn, 2012) for train1 http://universaldependencies.github.io/docs/ 340 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 340–349, Uppsala, Sweden, August 24–26 2015. ing dependency parsers and we use standard settings throughout the paper. 2 Baseline Models Universal Dependencies is a project that develops cross-linguistically consistent treebank annotation for many languages. The goal is to facilitate crosslingual learning, multilingual parser development and typological research from a syntactic perspective. The annotation scheme is derived from the univ"
W15-2137,C10-1011,0,0.0231765,"including labeled structures. The contributions of this paper include the presentation of monolingual and cross-lingual baseline models for the recently published universal dependencies data sets (UD; release 1.0)1 and a detailed discussion of the impact of PoS labels. We systematically compare results on standard test sets with gold labels with corresponding experiments that rely on predicted labels, which reflects the typical real-world scenario. Let us first look at baseline models before starting our discussion of cross-lingual approaches. In all our experiments, we apply the Mate tools (Bohnet, 2010; Bohnet and Kuhn, 2012) for train1 http://universaldependencies.github.io/docs/ 340 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 340–349, Uppsala, Sweden, August 24–26 2015. ing dependency parsers and we use standard settings throughout the paper. 2 Baseline Models Universal Dependencies is a project that develops cross-linguistically consistent treebank annotation for many languages. The goal is to facilitate crosslingual learning, multilingual parser development and typological research from a syntactic perspective. The annotation scheme"
W15-2137,W06-2920,0,0.0160723,"ble attention in recent years. The demand for robust NLP tools in many languages makes it necessary to port existing tools and resources to new languages in order to support low-resource languages without starting their development from scratch. Dependency parsing is one of the popular tasks in the NLP community (K¨ubler et al., 2009) that also found its way into commercial products and applications. Statistical parsing relies on annotated data sets, so-called treebanks. Several freely available data sets exist but still they only cover a small fraction of the linguistic variety in the world (Buchholz and Marsi, 2006; Nivre et al., 2007). Transferring linguistic information across languages is one approach to add support for new languages. There are basically two types of transfer that have been proposed in the literature: data transfer approaches and model transfer approaches. The former emphasizes the projection of data sets to new languages and it usually relies on parallel data sets and word alignment (Hwa et al., 2005; Tiedemann, 2014). Recently, machine translation was also introduced as yet another alternative to data transfer (Tiedemann et al., 2014). In model transfer, one tries to port existing"
W15-2137,J03-1002,0,0.0309072,"mann (2014), which are based on the projection heuristics proposed by Hwa et al. (2005). The data set that we use is a subset of the parallel Europarl corpus (version 7) which is a widely accepted data set primarily used in statistical machine translation (Koehn, 2005). We use a sample of 40,000 sentences for each language pair and annotate the data with our monolingual source language parsers presented in section 2. For the alignment, we use the symmetrized word alignments that are provided from OPUS (Tiedemann, 2012) that are created with standard statistical alignment tools such as Giza++ (Och and Ney, 2003) and Moses (Koehn et al., 2007). Our projection heuristics follow the direct correspondence assumption (DCA) algorithm of Hwa et al. (2005) but also apply the extensions proposed by Tiedemann (2014) that reduce the number of empty nodes and dummy labels. Figure 2 illustrates the effect of these extensions. Applying the annotation projection strategy, we obtain the parsing results shown in Table 5. For each language pair, we use the same procedure and the same amount of data taken from Europarl (40,000 sentences).3 From the results, we can see that we beat the delexicalized models by a large ma"
W15-2137,P11-1061,0,0.0217918,"Missing"
W15-2137,petrov-etal-2012-universal,0,0.0263374,"Missing"
W15-2137,de-marneffe-etal-2006-generating,0,0.030953,"Missing"
W15-2137,2005.mtsummit-papers.11,0,0.0272625,"ora. Secondly, we require accurate word alignments which are, however, often rather noisy when created automatically especially for non-literal human translations. Finally, we need to define heuristics to treat ambiguous alignments that cannot support one-to-one annotation projection. In our setup, we follow the suggested strategies of Tiedemann (2014), which are based on the projection heuristics proposed by Hwa et al. (2005). The data set that we use is a subset of the parallel Europarl corpus (version 7) which is a widely accepted data set primarily used in statistical machine translation (Koehn, 2005). We use a sample of 40,000 sentences for each language pair and annotate the data with our monolingual source language parsers presented in section 2. For the alignment, we use the symmetrized word alignments that are provided from OPUS (Tiedemann, 2012) that are created with standard statistical alignment tools such as Giza++ (Och and Ney, 2003) and Moses (Koehn et al., 2007). Our projection heuristics follow the direct correspondence assumption (DCA) algorithm of Hwa et al. (2005) but also apply the extensions proposed by Tiedemann (2014) that reduce the number of empty nodes and dummy labe"
W15-2137,D11-1006,0,0.30326,"roach to add support for new languages. There are basically two types of transfer that have been proposed in the literature: data transfer approaches and model transfer approaches. The former emphasizes the projection of data sets to new languages and it usually relies on parallel data sets and word alignment (Hwa et al., 2005; Tiedemann, 2014). Recently, machine translation was also introduced as yet another alternative to data transfer (Tiedemann et al., 2014). In model transfer, one tries to port existing parsers to new languages by (i) relying on universal features (McDonald et al., 2013; McDonald et al., 2011a; Naseem et al., 2012) and (ii) by adapting model parameters to the target language (T¨ackstr¨om et al., 2013). Universal features may refer to coarse part-of-speech sets that represent common word classes (Petrov et al., 2012) and may also include language-set-specific features such as cross-lingual word clusters (T¨ackstr¨om et al., 2012) or bilingual word embeddings (Xiao and Guo, 2014). Target language adaptation can be done using external linguistic resources such as prior knowledge about language families or lexical databases or any other existing tool for the target language. This pape"
W15-2137,N12-1052,0,0.0604767,"Missing"
W15-2137,N13-1126,0,0.0601303,"Missing"
W15-2137,W14-1614,1,0.739509,"Missing"
W15-2137,tiedemann-2012-parallel,1,0.93096,"one-to-one annotation projection. In our setup, we follow the suggested strategies of Tiedemann (2014), which are based on the projection heuristics proposed by Hwa et al. (2005). The data set that we use is a subset of the parallel Europarl corpus (version 7) which is a widely accepted data set primarily used in statistical machine translation (Koehn, 2005). We use a sample of 40,000 sentences for each language pair and annotate the data with our monolingual source language parsers presented in section 2. For the alignment, we use the symmetrized word alignments that are provided from OPUS (Tiedemann, 2012) that are created with standard statistical alignment tools such as Giza++ (Och and Ney, 2003) and Moses (Koehn et al., 2007). Our projection heuristics follow the direct correspondence assumption (DCA) algorithm of Hwa et al. (2005) but also apply the extensions proposed by Tiedemann (2014) that reduce the number of empty nodes and dummy labels. Figure 2 illustrates the effect of these extensions. Applying the annotation projection strategy, we obtain the parsing results shown in Table 5. For each language pair, we use the same procedure and the same amount of data taken from Europarl (40,000"
W15-2137,C14-1175,1,0.919914,"ted data sets, so-called treebanks. Several freely available data sets exist but still they only cover a small fraction of the linguistic variety in the world (Buchholz and Marsi, 2006; Nivre et al., 2007). Transferring linguistic information across languages is one approach to add support for new languages. There are basically two types of transfer that have been proposed in the literature: data transfer approaches and model transfer approaches. The former emphasizes the projection of data sets to new languages and it usually relies on parallel data sets and word alignment (Hwa et al., 2005; Tiedemann, 2014). Recently, machine translation was also introduced as yet another alternative to data transfer (Tiedemann et al., 2014). In model transfer, one tries to port existing parsers to new languages by (i) relying on universal features (McDonald et al., 2013; McDonald et al., 2011a; Naseem et al., 2012) and (ii) by adapting model parameters to the target language (T¨ackstr¨om et al., 2013). Universal features may refer to coarse part-of-speech sets that represent common word classes (Petrov et al., 2012) and may also include language-set-specific features such as cross-lingual word clusters (T¨ackst"
W15-2137,W14-1613,0,0.0991635,"ed as yet another alternative to data transfer (Tiedemann et al., 2014). In model transfer, one tries to port existing parsers to new languages by (i) relying on universal features (McDonald et al., 2013; McDonald et al., 2011a; Naseem et al., 2012) and (ii) by adapting model parameters to the target language (T¨ackstr¨om et al., 2013). Universal features may refer to coarse part-of-speech sets that represent common word classes (Petrov et al., 2012) and may also include language-set-specific features such as cross-lingual word clusters (T¨ackstr¨om et al., 2012) or bilingual word embeddings (Xiao and Guo, 2014). Target language adaptation can be done using external linguistic resources such as prior knowledge about language families or lexical databases or any other existing tool for the target language. This paper is focused on data transfer methods and especially annotation projection techniques that have been proposed in the related literature. There is an on-going effort on harmonized dependency annotations that makes it possible to transfer syntactic information across languages and to compare projected annotation and cross-lingual models even including labeled structures. The contributions of"
W15-2137,I08-3008,0,0.0371056,"en, August 24–26 2015. ing dependency parsers and we use standard settings throughout the paper. 2 Baseline Models Universal Dependencies is a project that develops cross-linguistically consistent treebank annotation for many languages. The goal is to facilitate crosslingual learning, multilingual parser development and typological research from a syntactic perspective. The annotation scheme is derived from the universal Stanford dependencies (De Marneffe et al., 2006), the Google universal part-of-speech (PoS) tags (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). The aim of the project is to provide a universal inventory of categories and consistent annotation guidelines for similar syntactic constructions across languages. In contrast to previous attempts to create universal dependency treebanks, the project explicitly allows language-specific extensions when necessary. Current efforts involve the conversion of existing treebanks to the UD annotation scheme. The first release includes ten languages: Czech, German, English, Spanish, Finnish, French, Irish, Italian, Swedish and Hungarian. We will use ISO 639-1 language codes throughout the paper (cs,"
W15-2137,D07-1096,0,\N,Missing
W15-2501,W15-2508,1,0.700494,"all submissions, both primary and secondary, in terms of macro-averaged F-score, several systems performed better in terms of accuracy. The other systems did not use explicit anaphora resolution, but attempted to gather relevant information about possible antecedents by considering a certain number of preceding, or preceding and following, noun phrases. They differed in the type of classifier and in the information sources used. UU - TIEDEMANN (Tiedemann, 2015) used a linear support vector machine with local features and simple surface features derived from preceding noun phrases. WHATELLES (Callin et al., 2015) used a neural network classifier based on work by Hardmeier et al. (2013b), but replacing all (explicit or latent) anaphora resolution with information extracted from preceding noun phrases. The IDIAP system (Luong et al., 2015) used a Naïve Bayes classifier and extracted features from both preceding and following noun phrases to account for the possibility of cataphoric references. The GENEVA system (Loáiciga, 2015) used maximum entropy classification; unlike the other submissions, it included features derived from syntactic parse trees. 12 2: secondary submission BASELINE - NP 0 UU - TIED U"
W15-2501,E06-1032,0,0.027114,"itself achieves the best scores, but considering the inadequacy of BLEU for pronoun evaluation, we do not see this as a major concern in itself. The other submissions fall behind in terms of automatic MT metrics. The UU - HARDMEIER system is similar to the other SMT systems, but uses different language and translation models, which evidently do not yield the same level of raw MT performance as the baseline system. ITS 2 is a rule-based system. Since it is well known that n-gram-based evaluation metrics do not always do full justice to rule-based MT approaches not using n-gram language models (Callison-Burch et al., 2006), it is difficult to draw definite conclusions from this system’s lower scores. Finally, the extremely low scores for the A 3-108 system indicate serious problems with translation quality, an impression that we easily confirmed by examining the system output. 8 5 The low scores for the ITS 2 system were partly due to a design decision. The anaphora prediction component of ITS 2 only generated the personal pronouns il, elle, ils and elles; this led to zero recall for ce and ça/cela and, as a consequence, to a large number of misses that would have been comparatively easy to predict with an n-gr"
W15-2501,P14-1065,1,0.749966,"Missing"
W15-2501,2012.eamt-1.60,1,0.298652,"est data using 200-best lists and MERT (Och, 2003). The resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version 7 (Koehn, 2005), News Commentary version 9 and the shuffled news data from WMT 2007–2013 (Bojar et al., 2014). test set IWSLT 2010 IWSLT 2012 BLEU 33.86 40.06 (BP=0.982) (BP=0.959) Table 4: Baseline models for English-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare"
W15-2501,2010.iwslt-papers.10,1,0.800853,"g such a setup makes it possible to explore a variety of approaches for solving the problem at hand since the participating groups independently come up with various ways to address it. All of this is highly beneficial for continued research as it creates a well-defined benchmark with a low entry barrier, a set of results to compare to, and a collection of properly evaluated ideas to start from. We decided to base this shared task on the problem of pronoun translation. Historically, this was one of the first discourse problems to be considered in the context of SMT (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010); yet, it is still far from being solved. For an overview of the existing work on pronoun translation, we refer the reader to Hardmeier (2014, Section 2.3.1). The typical case is an anaphoric pronoun – one that refers to an entity mentioned earlier in the discourse, its antecedent. Many languages have agreement constraints between pronouns and their antecedents. In translation, these constraints must be satisfied in the target language. Note that source language information is not enough for this task. To see why, consider the following example for English– French:1 We describe the design, the"
W15-2501,P13-4033,1,0.934301,"n software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), t"
W15-2501,chrupala-etal-2008-learning,0,0.0467164,"Missing"
W15-2501,W11-2107,0,0.0222412,"Missing"
W15-2501,D13-1037,1,0.793439,"n software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), t"
W15-2501,N13-1073,0,0.0272151,"inting characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment using the cleaning script provided by Moses, with 100 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million words in English and 70.0 million words in French. We word-aligned the data using fast_align (Dyer et al., 2013) and we symmetrized the word alignments using the grow-diag-final-and heuristics. The phrase tables were extracted from the word-aligned bitext using Moses with standard settings. We also filtered the resulting phrase table using significance testing (Johnson et al., 2007) with the recommended filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoot"
W15-2501,W15-2510,1,0.750725,"hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), they do not specifically focus on pronoun translation. 5 Machine Translation Evaluation http://stp.lingfil.uu.se/~ch/DiscoMT2015.maneval/index.php Machine Translation Eva"
W15-2501,J07-3002,0,0.00746111,"n order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and symmetrization performed best on the pronoun metric, followed by grow-diag and intersection, which also performed best for general alignments. Table 7 shows the results for different models with grow-diag-final-and symmetrization. We can see that, for all three models, the results on pronoun links are better than those on all links. More"
W15-2501,P13-2121,0,0.00705082,"Missing"
W15-2501,D07-1103,0,0.0077006,"00 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million words in English and 70.0 million words in French. We word-aligned the data using fast_align (Dyer et al., 2013) and we symmetrized the word alignments using the grow-diag-final-and heuristics. The phrase tables were extracted from the word-aligned bitext using Moses with standard settings. We also filtered the resulting phrase table using significance testing (Johnson et al., 2007) with the recommended filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoothing using KenLM (Heafield et al., 2013). We provided the language model in ARPA format and in binary format using a trie data structure with quantization and pointer compression. The SMT model was tuned on the IWSLT 2010 development data and IWSLT 2011 test data using 200"
W15-2501,guillou-etal-2014-parcor,1,0.687529,"g requirements: 1. The talks have been transcribed (in English) and translated into French. 2. They were not included in the training, development, and test datasets of any IWSLT evaluation campaign, so DiscoMT.tst2015 can be used as held-out data with respect to those. 3. They contain a sufficient number of tokens of the English pronouns it and they translated into the French pronouns listed in Table 1. 4. They amount to a total number of words suitable for evaluation purposes (e.g., tens of thousands). 2 http://www.ted.com 3 The following overview of text characteristics is based on work by Guillou et al. (2014). 3 To meet requirement 3, we selected talks for which the combined count of the rarer classes ça, cela, elle, elles and on was high. The resulting distribution of pronoun classes, according to the extraction procedure described in Section 5.1, can be found in Table 8 further below. We aimed to have at least one pair of talks given by the same speaker and at least one pair translated by the same translator. These two features are not required by the DiscoMT shared task, but could be useful for further linguistic analysis, such as the influence of speakers and translators on the use of pronouns"
W15-2501,W15-2509,0,0.0482925,"gy of having the annotators judge examples as good or bad, treating evaluation as a gap-filling task has the advantage of avoiding a bias in favour of solutions generated by the evaluated systems. Submitted Systems We received six submissions to the pronounfocused translation task, and there are system descriptions for five of them. Four submissions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T sy"
W15-2501,W14-3352,1,0.899484,"Missing"
W15-2501,D14-1027,1,0.902972,"Missing"
W15-2501,P07-2045,0,0.00942963,"2,565 5,989 4,520 2,836 3,413 2,828 4,109 4,636 3,383 3,078 6,229 3,438 2,802 6,416 4,738 2,702 3,568 3,023 J.J. Abrams A. Solomon S. Shah B. Barber A. Solomon S. Chandran P. Evans E. Snowden L. Page M. Laberge N. Negroponte H. Knabe total 2,093 45,351 48,122 – en tokens fr The parallel data were taken from OPUS (Tiedemann, 2012), which provides sentencealigned corpora with annotation. The latter is useful for finding document boundaries, which can be important when working with discourseaware translation models. All training data were pre-processed with standard tools from the Moses toolkit (Koehn et al., 2007), and the final datasets were lower-cased and normalized (punctuation was unified, and non-printing characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment using the cleaning script provided by Moses, with 100 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million w"
W15-2501,2005.mtsummit-papers.11,0,0.0282203,"e resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version 7 (Koehn, 2005), News Commentary version 9 and the shuffled news data from WMT 2007–2013 (Bojar et al., 2014). test set IWSLT 2010 IWSLT 2012 BLEU 33.86 40.06 (BP=0.982) (BP=0.959) Table 4: Baseline models for English-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare results with the provided baseline. For compl"
W15-2501,W15-2514,0,0.067491,"Missing"
W15-2501,W10-1737,0,0.740286,"Missing"
W15-2501,W11-1902,0,0.185739,"sions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with"
W15-2501,W15-2512,0,0.121441,"glish-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare results with the provided baseline. For completeness, we also provided a recasing model that was trained on the same dataset to render it straightforward to produce case-sensitive output, which we required as the final submission. 4.2 ITS 2 (Loáiciga and Wehrli, 2015) was a rulebased machine translation system using syntaxbased transfer. For the shared task, it was extended with an anaphora resolution component influenced by Binding Theory (Chomsky, 1981). For the sixth submission, A 3-108, no system description paper was submitted. Its output seemed to have been affected by problems at the basic MT level, yielding very bad translation quality. 4.3 Evaluation Methods Evaluating machine translations for pronoun correctness automatically is difficult because standard assumptions fail. In particular, it is incorrect to assume that a pronoun is translated corr"
W15-2501,2006.amta-papers.25,0,0.144804,"Missing"
W15-2501,W15-2511,0,0.0430542,"s used. UU - TIEDEMANN (Tiedemann, 2015) used a linear support vector machine with local features and simple surface features derived from preceding noun phrases. WHATELLES (Callin et al., 2015) used a neural network classifier based on work by Hardmeier et al. (2013b), but replacing all (explicit or latent) anaphora resolution with information extracted from preceding noun phrases. The IDIAP system (Luong et al., 2015) used a Naïve Bayes classifier and extracted features from both preceding and following noun phrases to account for the possibility of cataphoric references. The GENEVA system (Loáiciga, 2015) used maximum entropy classification; unlike the other submissions, it included features derived from syntactic parse trees. 12 2: secondary submission BASELINE - NP 0 UU - TIED UEDIN MALTA 2 MALTA WHATELLES UEDIN 2 UU - TIED 2 GENEVA GENEVA 2 IDIAP IDIAP 2 A 3-108 ( WITHDRAWN ) Macro-F Accuracy ce cela elle elles F-score il 0.584 0.579 0.571 0.565 0.561 0.553 0.550 0.539 0.437 0.421 0.206 0.164 0.129 0.122 0.663 0.742 0.723 0.740 0.732 0.721 0.714 0.734 0.592 0.579 0.307 0.407 0.240 0.325 0.817 0.862 0.823 0.875 0.853 0.862 0.823 0.849 0.647 0.611 0.282 0.152 0.225 0.220 0.346 0.235 0.213 0.1"
W15-2501,W14-3334,1,0.85254,"ed the same bitext as for the MT baseline in the first task (Section 4.1); we pre-processed it like before, except for lowercasing. Then, we generated the following two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignme"
W15-2501,W15-2513,0,0.169961,"ompared to the perhaps more obvious methodology of having the annotators judge examples as good or bad, treating evaluation as a gap-filling task has the advantage of avoiding a bias in favour of solutions generated by the evaluated systems. Submitted Systems We received six submissions to the pronounfocused translation task, and there are system descriptions for five of them. Four submissions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline"
W15-2501,tiedemann-2012-parallel,1,0.765503,"ic or human processing. Table 3 shows some statistics and metadata about the TED talks that are part of the DiscoMT.tst2015 set. talk id segs 205 1756 1819 1825 1894 1935 1938 1950 1953 1979 2043 2053 189 186 147 120 237 139 107 243 246 160 175 144 4,188 4,320 2,976 2,754 5,827 3,135 2,565 5,989 4,520 2,836 3,413 2,828 4,109 4,636 3,383 3,078 6,229 3,438 2,802 6,416 4,738 2,702 3,568 3,023 J.J. Abrams A. Solomon S. Shah B. Barber A. Solomon S. Chandran P. Evans E. Snowden L. Page M. Laberge N. Negroponte H. Knabe total 2,093 45,351 48,122 – en tokens fr The parallel data were taken from OPUS (Tiedemann, 2012), which provides sentencealigned corpora with annotation. The latter is useful for finding document boundaries, which can be important when working with discourseaware translation models. All training data were pre-processed with standard tools from the Moses toolkit (Koehn et al., 2007), and the final datasets were lower-cased and normalized (punctuation was unified, and non-printing characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment"
W15-2501,W15-2515,1,0.722711,"and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphor"
W15-2501,C96-2141,0,0.0650665,"for lowercasing. Then, we generated the following two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . Fo"
W15-2501,P00-1056,0,0.0244007,"reated automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and symmetrization performed best on the pronoun metric, followed by grow-diag and intersection, which also performed best for general alignments. Table 7 shows the results for different models with grow-diag-final-and symmetrization. We can see that, for all t"
W15-2501,W15-2516,0,0.0553089,"air, and (ii) the sums for each row/column; • accuracy; All six groups with system description papers used some form of machine learning. The main difference was whether or not they explicitly attempted to resolve pronominal coreference. Two systems relied on explicit anaphora resolution: UEDIN and MALTA. They both applied the Stanford coreference resolver (Lee et al., 2011) on the source language text, then projected the antecedents to the target language through the word alignments, and finally obtained morphological tags with the Morfette software (Chrupała et al., 2008). The UEDIN system (Wetzel et al., 2015) was built around a maximum entropy classifier. In addition to local context and antecedent information, it used the NADA tool (Bergsma and Yarowsky, 2011) to identify nonreferring pronouns and included predictions by a standard n-gram language model as a feature. The MALTA system (Pham and van der Plas, 2015) was based on a feed-forward neural network combined with word2vec continuous-space word embeddings (Mikolov et al., 2013). It used local context and antecedent information. • precision (P), recall (R), and F-score for each label; • micro-averaged P, R, F-score (note that in our setup, mi"
W15-2501,J03-1002,0,0.00752171,"ing two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and s"
W15-2501,P03-1021,0,0.00636797,"filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoothing using KenLM (Heafield et al., 2013). We provided the language model in ARPA format and in binary format using a trie data structure with quantization and pointer compression. The SMT model was tuned on the IWSLT 2010 development data and IWSLT 2011 test data using 200-best lists and MERT (Och, 2003). The resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version"
W15-2501,P02-1040,0,\N,Missing
W15-2501,W14-3302,0,\N,Missing
W15-2508,S10-1021,0,0.0757872,"Missing"
W15-2508,E12-3001,0,0.260488,"Missing"
W15-2508,2010.iwslt-papers.10,1,0.942001,"Missing"
W15-2508,P13-4033,1,0.916362,"Missing"
W15-2508,D13-1037,1,0.923019,"Missing"
W15-2508,W15-2501,1,0.764343,"Missing"
W15-2508,P14-6007,0,0.0889922,"Missing"
W15-2508,P07-2045,0,0.0057455,"Missing"
W15-2508,2005.mtsummit-papers.11,0,0.333417,"Missing"
W15-2508,W10-1737,0,0.664347,"Missing"
W15-2508,1995.tmi-1.6,0,0.128158,"Missing"
W15-2515,2010.iwslt-papers.10,0,0.143491,"it 0,3616 they 0,6641 TOTAL 0,5000 +PRON-LM it 0,3827 they 0,6800 TOTAL 0,5237 +DET+PRON-LM it 0,3793 they 0,6867 TOTAL 0,5213 BLEU 0.4000 0.3982 0.3969 Table 5: Translation with and without pronoun language model on development data. PRON uses words linked to English pronouns and DET+PRON includes words linked to determiners as well. In order to test our models on the specific task of translating pronouns in context, we also performed automatic evaluations of the translations we obtained for the development set. Table 6 lists the results for the three models using the evaluation approach of Hardmeier and Federico (2010). We can see that both augmented models improve the overall F1 scores mainly due to an increase in precision. The model that includes target language words linked to determiners performs best at least according to our automatic evaluation and, therefore, we selected this model as our primary submission. The differences are, however, very small and the manual evaluation of the test set translations revealed that our model could not even beat the phrase-based baseline without a pronoun-specific model. The official results of the translation task are shown in Table 4. We can see that the proposed"
W15-2515,D12-1108,1,0.905649,"Missing"
W15-2515,P13-4033,1,0.878317,"works reasonably well. Most problems can be found in the predictions of the female pronouns “elle” and “elles” but also the confusion between “cela” and “c¸a” is noticeable. For further details of the individual mistakes done by the classifier, 3 Pronoun-Focused Translation The pronoun-focused translation task at DiscoMT requires a full machine translation system. Our submission uses a phrase-based model with one additional document-level feature function that captures long-distance relations spanning over arbitrarily long distances within a given document and its translation. We use Docent (Hardmeier et al., 2013), a document-level decoder that supports such feature functions and test our model on the DiscoMT test set. 3.1 Document-Level Decoding The common strategy to decode phrase-based SMT models is to use a beam search algorithm based on dynamic programming and incremental hypotheses expansion (Koehn, 2010). This approach is very efficient and successful for local features such as context-independent translation options of word sequences and n-gram-based language models. Longdistance dependencies on the target language are impossible to incorporate which makes it difficult to account for coreferent"
W15-2515,P13-2121,0,0.0988393,"Missing"
W15-2515,J10-4005,0,0.0260942,"t DiscoMT requires a full machine translation system. Our submission uses a phrase-based model with one additional document-level feature function that captures long-distance relations spanning over arbitrarily long distances within a given document and its translation. We use Docent (Hardmeier et al., 2013), a document-level decoder that supports such feature functions and test our model on the DiscoMT test set. 3.1 Document-Level Decoding The common strategy to decode phrase-based SMT models is to use a beam search algorithm based on dynamic programming and incremental hypotheses expansion (Koehn, 2010). This approach is very efficient and successful for local features such as context-independent translation options of word sequences and n-gram-based language models. Longdistance dependencies on the target language are impossible to incorporate which makes it difficult to account for coreferential relations over arbitrary spans in order to resolve, for example, ambiguities in the translation of anaphoric pronouns. Docent implements a different decoding strategy that starts with a complete translation hypotheses of an entire document applying local changes to improve the translation according"
W15-2515,petrov-etal-2012-universal,0,0.0355366,"Missing"
W15-3021,de-marneffe-etal-2014-universal,1,0.838757,"Missing"
W15-3021,N13-1073,0,0.03498,"ing data which results in significant improvements without any language-specific optimization. In the following, we will first present our systems and the results achieved with our models before discussing the translation produced in more detail. The latter analyses pinpoint issues and problems that provide valuable insights for future development. 2 Basic Setup and Data Sets All our translation systems are based on Moses (Koehn et al., 2007) and standard components for training and tuning the models. We apply KenLM for language modeling (Heafield et al., 2013), fast align for word alignment (Dyer et al., 2013) and MERT for parameter tuning (Och, 2003). All our models use lowercased training data and the results that we report refer to lowercased output of our models. All language models are of order five and use the standard modified Kneser-Ney smoothing implemented in KenLM. All phrase tables are pruned based on significance testing (Johnson et al., 2007) and reducing translation options to at most 30 per phrase type. The maximum phrase length is seven. 177 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 177–183, c Lisboa, Portugal, 17-18 September 2015. 2015 Associatio"
W15-3021,E12-1068,0,0.0734406,"hological pre-processing of Finnish. 1 Figure 1: A sentence illustrating the inflective and compounding nature of Finnish in contrast to English. (ADE, INE: adessive, inessive cases, PASS: passive, PL: plural) Introduction The basic goal of our submissions is to establish some straightforward baselines for the translation between Finnish and English using standard technology such as phrase-based and factored statistical machine translation, in preparation for a more focused future effort in combination with the stateof-the-art techniques in SMT for morphologically complex languages (see e.g. (Fraser et al., 2012)). The translation between Finnish and English (in both directions) is a new task in this year’s workshop adding a new exciting challenge to the established setup. The main difficulty in this task is to manage the rich morphology of Finnish which has several implications on training and expected results with standard SMT models (see the illustration in Figure 1). Moreover, the monolingual and parallel training data is substantially smaller which makes the task even tougher compared with other languages pairs in the competition. In our contribution, we focus on Finnish-English emphasizing the n"
W15-3021,P13-2121,0,0.0509981,"Missing"
W15-3021,D07-1103,0,0.0424679,"2 Basic Setup and Data Sets All our translation systems are based on Moses (Koehn et al., 2007) and standard components for training and tuning the models. We apply KenLM for language modeling (Heafield et al., 2013), fast align for word alignment (Dyer et al., 2013) and MERT for parameter tuning (Och, 2003). All our models use lowercased training data and the results that we report refer to lowercased output of our models. All language models are of order five and use the standard modified Kneser-Ney smoothing implemented in KenLM. All phrase tables are pruned based on significance testing (Johnson et al., 2007) and reducing translation options to at most 30 per phrase type. The maximum phrase length is seven. 177 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 177–183, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. For processing Finnish, we use the Finnish dependency parser pipeline1 developed at the University of Turku (Haverinen et al., 2014). This pipeline integrates all pre-processing steps that are necessary for data-driven dependency parsing including tokenization, morphological analyses and part-ofspeech tagging, and prod"
W15-3021,niemi-linden-2012-representing,0,0.06502,"Missing"
W15-3021,P03-1021,0,0.0419687,"without any language-specific optimization. In the following, we will first present our systems and the results achieved with our models before discussing the translation produced in more detail. The latter analyses pinpoint issues and problems that provide valuable insights for future development. 2 Basic Setup and Data Sets All our translation systems are based on Moses (Koehn et al., 2007) and standard components for training and tuning the models. We apply KenLM for language modeling (Heafield et al., 2013), fast align for word alignment (Dyer et al., 2013) and MERT for parameter tuning (Och, 2003). All our models use lowercased training data and the results that we report refer to lowercased output of our models. All language models are of order five and use the standard modified Kneser-Ney smoothing implemented in KenLM. All phrase tables are pruned based on significance testing (Johnson et al., 2007) and reducing translation options to at most 30 per phrase type. The maximum phrase length is seven. 177 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 177–183, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. For proce"
W15-3021,W15-1821,1,0.885434,"Missing"
W15-3021,skadins-etal-2014-billions,1,0.864782,"Missing"
W15-3021,tiedemann-2012-parallel,1,0.881187,"Missing"
W15-3021,P07-2045,0,\N,Missing
W15-3908,W11-3216,0,0.0177874,"which significantly improves the overall transliteration performance. 1 2 Background Machine transliteration is often modelled as a sequence labelling problem in previous research. Thus, the existing algorithms for sequence labelling all can be used for solving the problem. The classical joint source-channel model (Li et al., 2004) is essentially a Hidden Markov Model (HMM), which allows direct mapping between the transliteration units in source and target languages. Given the source string as the input, when it passes through the joint source-channel, the output is generated simultaneously. Chen et al. (2011) extends the original sourcechannel model into multi-to-multi source-channel model and uses Moses as the decoder. As a popular experimental framework for machine translation, Moses is also applied to build phrase-based transliteration systems in some other related works (Finch and Sumita, 2010). Machine transliteration is treated as character level machine translation without distortion in their approaches. In addition, the use of Conditional Random Fields (CRF) (Lafferty et al., 2001) is another popular approach in previous studies. It is a powerful discriminative sequence labelling model tha"
W15-3908,W10-2406,0,0.0188122,"joint source-channel model (Li et al., 2004) is essentially a Hidden Markov Model (HMM), which allows direct mapping between the transliteration units in source and target languages. Given the source string as the input, when it passes through the joint source-channel, the output is generated simultaneously. Chen et al. (2011) extends the original sourcechannel model into multi-to-multi source-channel model and uses Moses as the decoder. As a popular experimental framework for machine translation, Moses is also applied to build phrase-based transliteration systems in some other related works (Finch and Sumita, 2010). Machine transliteration is treated as character level machine translation without distortion in their approaches. In addition, the use of Conditional Random Fields (CRF) (Lafferty et al., 2001) is another popular approach in previous studies. It is a powerful discriminative sequence labelling model that uses rich local features. However, it is very costly in terms of time complexity during the training process especially combined with the full transliteration task. Qin and Chen (2011) decomposes the Introduction Machine transliteration is an effective approach to process named entities that"
W15-3908,W11-3201,0,0.0190208,"el can be improved further via adding more multilingual resources, using more effective features for reranking and adopting better regression algorithms. Experimental Results and Analysis Table 4 shows the official experimental results. 4.1 Non-Standard Runs Standard Runs Since the test data sets are the same as the ones used in the NEWS transliteration shared tasks of 2011 and 2012, our systems are compared to the evaluated systems in the previous years. For English to Chinese, our system beats all the systems of 2012 (Zhang et al., 2012) but fails to beat the best performing system of 2011 (Zhang et al., 2011) according to ACC. Generally, the substring based system achieves better results than the character based system, which indicates that the CRF model is more effective in identifying phrase boundaries than Moses. For Chinese to English, our system is slightly worse than the best performing systems but still very competitive. We can see that the Chinese character based system yields better results. Compared to pinyin, Chinese characters contain more information that is useful to transliteration. As expected, the backoff systems perform best in both tasks. It is also notable that our systems perf"
W15-3908,N07-1047,0,0.084245,"aining process especially combined with the full transliteration task. Qin and Chen (2011) decomposes the Introduction Machine transliteration is an effective approach to process named entities that are out-of-vocabulary words in many NLP tasks, such as machine translation, corpus alignment and cross-language information retrieval. In this paper, using the experiment data from the NEWS 2015 machine transliteration shared task (Zhang et al., 2015), we develop machine transliteration systems respectively targeting English to Chinese and Chinese to English transliteration tasks. The M2M-aligner (Jiampojamarn et al., 2007) is used to preprocess the training data to obtain the boundaries and alignments of transliteration units between source and target language. We apply a hard-constrained estimation-maximization (EM) algorithm to post-process its outputs, which greatly reduces errors of segmentation and alignment. With the refined outputs, we build phrasebased transliteration systems using Moses (Koehn et al., 2007), a popular statistical machine translation framework. The results are submitted as standard runs. 56 Proceedings of the Fifth Named Entity Workshop, joint with 53rd ACL and the 7th IJCNLP, pages 56–"
W15-3908,W12-4402,0,0.113505,"ause b is never pronounced as v in English. Our multilingual reranking model can be improved further via adding more multilingual resources, using more effective features for reranking and adopting better regression algorithms. Experimental Results and Analysis Table 4 shows the official experimental results. 4.1 Non-Standard Runs Standard Runs Since the test data sets are the same as the ones used in the NEWS transliteration shared tasks of 2011 and 2012, our systems are compared to the evaluated systems in the previous years. For English to Chinese, our system beats all the systems of 2012 (Zhang et al., 2012) but fails to beat the best performing system of 2011 (Zhang et al., 2011) according to ACC. Generally, the substring based system achieves better results than the character based system, which indicates that the CRF model is more effective in identifying phrase boundaries than Moses. For Chinese to English, our system is slightly worse than the best performing systems but still very competitive. We can see that the Chinese character based system yields better results. Compared to pinyin, Chinese characters contain more information that is useful to transliteration. As expected, the backoff sy"
W15-3908,P07-2045,0,0.00804887,"iteration shared task (Zhang et al., 2015), we develop machine transliteration systems respectively targeting English to Chinese and Chinese to English transliteration tasks. The M2M-aligner (Jiampojamarn et al., 2007) is used to preprocess the training data to obtain the boundaries and alignments of transliteration units between source and target language. We apply a hard-constrained estimation-maximization (EM) algorithm to post-process its outputs, which greatly reduces errors of segmentation and alignment. With the refined outputs, we build phrasebased transliteration systems using Moses (Koehn et al., 2007), a popular statistical machine translation framework. The results are submitted as standard runs. 56 Proceedings of the Fifth Named Entity Workshop, joint with 53rd ACL and the 7th IJCNLP, pages 56–60, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics a|ber|nat|hy| a|ber|ne|thy| t|e|xi|do| wi|ll|c|o|x| full task into several subtasks and uses different CRF recognizers. Kuo et al. (2012) uses a twostage CRF system with accessor variety (AV) as an additional feature, which processes segmentation and mapping separately. 3 3.1 阿|伯|内|西| 阿|伯|内|西| 特|克|西|多| 威|尔|科|克|斯|"
W15-3908,P04-1021,0,0.601603,"dels with refined alignments provided by the M2M-aligner. For non-standard runs, we add multilingual resources to the systems designed for the standard runs and build different language specific transliteration systems. Linear regression is adopted to rerank the outputs afterwards, which significantly improves the overall transliteration performance. 1 2 Background Machine transliteration is often modelled as a sequence labelling problem in previous research. Thus, the existing algorithms for sequence labelling all can be used for solving the problem. The classical joint source-channel model (Li et al., 2004) is essentially a Hidden Markov Model (HMM), which allows direct mapping between the transliteration units in source and target languages. Given the source string as the input, when it passes through the joint source-channel, the output is generated simultaneously. Chen et al. (2011) extends the original sourcechannel model into multi-to-multi source-channel model and uses Moses as the decoder. As a popular experimental framework for machine translation, Moses is also applied to build phrase-based transliteration systems in some other related works (Finch and Sumita, 2010). Machine translitera"
W15-3908,W12-4412,0,\N,Missing
W15-5401,Y08-1042,0,0.153038,"iscusses related work, Section 3 describes the general setup of the task, Section 4 presents the results of the competition, Section 5 summarizes the approaches used by the participants, and Section 6 offers conclusions. 2 Related Work 2.2 Language identification has attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as Malay and Indonesian (RanaivoMalanc¸on, 2006), Persian and Dari (Malmasi and Dras, 2015a), Brazilian and European Portuguese (Zampieri and Gebre, 2012), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), and English varieties (Lui and Cook, 2013), among others. This interest has eventually given rise to special shared tasks, which allowed researchers to compare and benchmark various approaches on common standard datasets. Below we will describe some of these shared tasks, including the first edition of the DSL task. 2.1 The First Edition of the DSL Task For the first edition of the task, we compiled the DSL Corpus Collection (Tan et al., 2014), or DSLCC v.1.0, which included excerpts from journalistic texts from sources such as the SETimes Corpus1 (Tyers and Alperen, 2010), HC Corpora2 and t"
W15-5401,W15-5408,0,0.229488,"Missing"
W15-5401,W15-5410,0,0.0986295,"sk track, we further made available DSLCC v2.1, which extended DSLCC v2.0 with Mexican Spanish and Macanese Portuguese data. 6 The script we used to substitute named entities with placeholders is available here: https://github.com/ Simdiva/DSL-Task/blob/master/blindNE.py 4 Date May 20, 2015 June 22, 2015 June 24, 2015 June 26, 2015 July 20, 2015 Table 3: The DSL 2015 Shared Task schedule. 4 Team BOICEV BRUNIBP INRIA MAC MMS* NLEL NRC OSEVAL PRHLT SUKI Total Closed (Normal) X X X X X X X X X 9 Closed (No NEs) X X X X X X X 7 Open (Normal) X X X 3 Open (No NEs) X X X 3 System Description Paper (Bobicev, 2015) ´ et al., 2015) (Acs (Malmasi and Dras, 2015b) (Zampieri et al., 2015) (Fabra-Boluda et al., 2015) (Goutte and L´eger, 2015) (Franco-Salvador et al., 2015) (Jauhiainen et al., 2015a) 8 Table 4: The participating teams in the DSL 2015 Shared Task. 4 Rank 1 2-3 2-3 4 5 6 7 8 9 Results In this section, we present the results of the 2nd edition of the DSL shared task.7 Most of the participating teams used DSLCC v2.0 only, and thus took part in the closed submission track. Yet, three of the teams collected additional data or used DSLCC v1.0, and thereby participated in the open submission. 4.1 Sub"
W15-5401,D14-1069,0,0.183053,"Missing"
W15-5401,W14-5317,0,0.204271,"Missing"
W15-5401,W15-5409,0,0.310823,"Spanish and Macanese Portuguese data. 6 The script we used to substitute named entities with placeholders is available here: https://github.com/ Simdiva/DSL-Task/blob/master/blindNE.py 4 Date May 20, 2015 June 22, 2015 June 24, 2015 June 26, 2015 July 20, 2015 Table 3: The DSL 2015 Shared Task schedule. 4 Team BOICEV BRUNIBP INRIA MAC MMS* NLEL NRC OSEVAL PRHLT SUKI Total Closed (Normal) X X X X X X X X X 9 Closed (No NEs) X X X X X X X 7 Open (Normal) X X X 3 Open (No NEs) X X X 3 System Description Paper (Bobicev, 2015) ´ et al., 2015) (Acs (Malmasi and Dras, 2015b) (Zampieri et al., 2015) (Fabra-Boluda et al., 2015) (Goutte and L´eger, 2015) (Franco-Salvador et al., 2015) (Jauhiainen et al., 2015a) 8 Table 4: The participating teams in the DSL 2015 Shared Task. 4 Rank 1 2-3 2-3 4 5 6 7 8 9 Results In this section, we present the results of the 2nd edition of the DSL shared task.7 Most of the participating teams used DSLCC v2.0 only, and thus took part in the closed submission track. Yet, three of the teams collected additional data or used DSLCC v1.0, and thereby participated in the open submission. 4.1 Submitted Runs Accuracy 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 Table 5: Closed submissi"
W15-5401,W15-5403,0,0.148019,"used to substitute named entities with placeholders is available here: https://github.com/ Simdiva/DSL-Task/blob/master/blindNE.py 4 Date May 20, 2015 June 22, 2015 June 24, 2015 June 26, 2015 July 20, 2015 Table 3: The DSL 2015 Shared Task schedule. 4 Team BOICEV BRUNIBP INRIA MAC MMS* NLEL NRC OSEVAL PRHLT SUKI Total Closed (Normal) X X X X X X X X X 9 Closed (No NEs) X X X X X X X 7 Open (Normal) X X X 3 Open (No NEs) X X X 3 System Description Paper (Bobicev, 2015) ´ et al., 2015) (Acs (Malmasi and Dras, 2015b) (Zampieri et al., 2015) (Fabra-Boluda et al., 2015) (Goutte and L´eger, 2015) (Franco-Salvador et al., 2015) (Jauhiainen et al., 2015a) 8 Table 4: The participating teams in the DSL 2015 Shared Task. 4 Rank 1 2-3 2-3 4 5 6 7 8 9 Results In this section, we present the results of the 2nd edition of the DSL shared task.7 Most of the participating teams used DSLCC v2.0 only, and thus took part in the closed submission track. Yet, three of the teams collected additional data or used DSLCC v1.0, and thereby participated in the open submission. 4.1 Submitted Runs Accuracy 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 Table 5: Closed submission results for test set A. A total of 24 teams subscribed"
W15-5401,I11-1062,0,0.0379892,"Missing"
W15-5401,P12-3005,0,0.0283147,"Missing"
W15-5401,W13-1728,1,0.611887,"e group, and then chooses between languages or varieties within this group. The team achieved very strong results this year, ranking second in the closed submission on test set A, third on test set B, and first in the open submission on both test sets A and B. Two other participants used two-stage classification: NLEL (Fabra-Boluda et al., 2015) and ´ et al., 2015). BRUniBP (Acs The MMS team experimented with three approaches (Zampieri et al., 2015), and their best run combined TF.IDF weighting and an SVM classifier, which was previously successfully applied to native language identification (Gebre et al., 2013). The SUKI team (Jauhiainen et al., 2015a) used token-based backoff, which was previously applied to general-purpose language identification (Jauhiainen et al., 2015b). The BOBICEV team applied prediction by partial matching, which had not been used for this task before (Bobicev, 2015). Finally, the PRHLT team (Franco-Salvador et al., 2015) used word and sentence vectors, which is to our knowledge the first attempt to apply them to discriminating between similar languages. Table 7: Open submission results for test set A. This could be related to the availability of DSLCC v1.0 as an obvious add"
W15-5401,U13-1003,0,0.339119,"e general setup of the task, Section 4 presents the results of the competition, Section 5 summarizes the approaches used by the participants, and Section 6 offers conclusions. 2 Related Work 2.2 Language identification has attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as Malay and Indonesian (RanaivoMalanc¸on, 2006), Persian and Dari (Malmasi and Dras, 2015a), Brazilian and European Portuguese (Zampieri and Gebre, 2012), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), and English varieties (Lui and Cook, 2013), among others. This interest has eventually given rise to special shared tasks, which allowed researchers to compare and benchmark various approaches on common standard datasets. Below we will describe some of these shared tasks, including the first edition of the DSL task. 2.1 The First Edition of the DSL Task For the first edition of the task, we compiled the DSL Corpus Collection (Tan et al., 2014), or DSLCC v.1.0, which included excerpts from journalistic texts from sources such as the SETimes Corpus1 (Tyers and Alperen, 2010), HC Corpora2 and the Leipzig Corpora Collection (Biemann et al"
W15-5401,W15-5413,0,0.538185,"Missing"
W15-5401,W14-5315,0,0.370837,"Missing"
W15-5401,W14-5316,0,0.374689,"Missing"
W15-5401,W15-5407,0,0.1816,"ke the task more challenging and less dependent on the text topic and domain. The remainder of this paper is organized as follows: Section 2 discusses related work, Section 3 describes the general setup of the task, Section 4 presents the results of the competition, Section 5 summarizes the approaches used by the participants, and Section 6 offers conclusions. 2 Related Work 2.2 Language identification has attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as Malay and Indonesian (RanaivoMalanc¸on, 2006), Persian and Dari (Malmasi and Dras, 2015a), Brazilian and European Portuguese (Zampieri and Gebre, 2012), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), and English varieties (Lui and Cook, 2013), among others. This interest has eventually given rise to special shared tasks, which allowed researchers to compare and benchmark various approaches on common standard datasets. Below we will describe some of these shared tasks, including the first edition of the DSL task. 2.1 The First Edition of the DSL Task For the first edition of the task, we compiled the DSL Corpus Collection (Tan et al., 2014), or DSLCC"
W15-5401,C12-1160,1,0.783877,"Missing"
W15-5401,tiedemann-2012-parallel,1,0.851577,"Missing"
W15-5401,W14-5314,0,0.282427,"Missing"
W15-5401,xia-etal-2010-problems,0,0.0651996,"Missing"
W15-5401,W14-5904,0,0.0750336,"Missing"
W15-5401,W14-5307,1,0.737281,"Missing"
W15-5401,W15-5411,1,0.87912,"Missing"
W15-5401,W14-2505,0,0.0277162,"raining and development subsets, and we further prepared two test sets, as described in Section 3.3 below. As in 2014, teams could make two types of submissions (for each team, we allowed up to three runs per submission type; in the official ranking, we included the run with the highest score only): • Closed submission: Using only the DSLCC v2.0 for training. • Open submission: Using any dataset other than DSLCC v2.0 for training.3 3.2 The Unshared Task Track Along with the Shared Task, this year we proposed an Unshared Task track inspired by the unshared task in PoliInformatics held in 2014 (Smith et al., 2014). For this track, teams were allowed to use any version of DSLCC to investigate differences between similar languages and language varieties using NLP methods. We were interested in studying questions like these: • Are there fundamental grammatical differences in a language group? • What are the most distinctive lexical choices for each language? • Which text representation is most suitable to investigate language variation? • What is the impact of lexical and grammatical variation on NLP applications? Although eleven teams subscribed for the Unshared Task track, none of them ended up submitin"
W15-5401,W14-3907,0,0.0457835,"2015. 2015 Association for Computational Linguistics Another popular research direction has been on language identification on Twitter, which was driven by interest in geolocation prediction for end-user applications (Ljubeˇsi´c and Kranjˇci´c, 2015). This interest has given rise to the TweetLID shared task (Zubiaga et al., 2014), which asked participants to recognize the language of tweet messages, focusing on English and on languages spoken on the Iberian peninsula such as Basque, Catalan, Spanish, and Portuguese. The Shared Task on Language Identification in CodeSwitched Data held in 2014 (Solorio et al., 2014) is another related competition, where the focus was on tweets in which users were mixing two or more languages in the same tweet. First, in order to simulate a real-world language identification scenario, we included in the testing dataset some languages that were not present in the training dataset. Moreover, we included a second test set, where we substituted the named entities with placeholders to make the task more challenging and less dependent on the text topic and domain. The remainder of this paper is organized as follows: Section 2 discusses related work, Section 3 describes the gene"
W15-5401,U10-1003,0,\N,Missing
W15-5401,W14-5318,0,\N,Missing
W16-2326,E14-1061,1,0.903294,"s. The underlying idea of what we call re-inflection models in our submission is that we reduce all Finnish training data to an underspecified representation, where words are reduced to their lemmas and noun and adjective compounds are split into their component parts. Then, we train models and translate from English into this underspecified representation of Finnish and in a post-processing step we then merge compounds and predict morphological features for Finnish. This approach has been successfully applied to Russian and Arabic (Toutanova et al., 2008) and to German (Fraser et al. (2012), Cap et al. (2014)). Note however, that for example Fraser et al. (2012) relied on German prepositions to predict case-markers on underspecified German SMT output. In contrast to many other languages, Finnish only has a limited number of stand-alone pre- and postpositions. Instead, the prepositional meaning is encoded by case-marking. We thus adapt an approach by Tiedemann et al. (2015b) and introduce place-holder prepositions in the Finnish training data, which are likely to correspond to the prepositions used on the English side and thus improve word alignment quality. Place-holder Prepositions: In contrast t"
W16-2326,W15-2501,1,0.829554,"ents (system (c) in Table 2). In combination with the surfaceoriented translation model this also leads to a slight improvement over the non-factored model (without back-translated news), which is also evi394 3.4 ilar to Tiedemann et al. (2015b), we introduce place-holder prepositions at the beginning of noun phrases bearing the corresponding case-marker in order to support word alignment. Gappy Language Models Tiedemann (2015) introduces the use of language models over selected words in the framework of document-level SMT using Docent applied to the pronoun-aware translation task of DiscoMT (Hardmeier et al., 2015). We extended this idea by developing a general framework for what we call gappy language models that refer to monolingual or bilingual n-gram language models over selected words and their alignments. We can use different factors attached to the source and target language tokens to filter for word sequences that we would like to consider. Given word alignments are used to establish the link between source and target tokens. Gappy language models may cross sentence-boundaries but may also stop at those borders. Regular expressions can be used to make the selection more flexible. Multi-word alig"
W16-2326,N12-1047,0,0.0925095,"ta sets. We use the English models for sentence boundary detection and tokenisation provided by OpenNLP,2 which is compatible with the Penn Treebank style of tokenisation. This is important for the subsequent tagging and parsing steps, which we trained on the Universal Dependencies treebank for English using MarMoT and mate-tools. MT Tools: Most of our systems are based on Moses (Koehn et al., 2007) and common components for training and tuning models. We apply KenLM (Heafield et al., 2013) and SRILM (Stolcke, 2002) for estimating language model parameters and MERT (Och, 2003) and batch-MIRA (Cherry and Foster, 2012) for parameter tuning. Most of our models are based on lowercased training data. All language models use order five with modified Kneser-Ney smoothing if not stated otherwise. All MT systems apply the phrase-based paradigm, some of them with factored representations and generation models if necessary. For word alignment we experiment with different tools. We apply standard tools like GIZA++ 1 2 (Och and Ney, 2003) and fast align (Dyer et al., 2013) but also the recently proposed Bayesian ¨ word aligner efmaral (Ostling, 2015). Efmaral is an efficient implementation of a Markov-Chain aligner us"
W16-2326,N13-1138,0,0.0239033,"a to their heads and train the models on this representation. As we are using the words and lemmas as features for the CRFs, the reduction of compounds to their heads reduces data sparsity and allows the model to better generalise over all occurrences. For the translation output we remove all compound modifiers before case prediction. Morphological Generation The predicted casemarkers are then fed into the morphological generation automaton (Pirinen, 2015) in order to get fully inflected forms. In cases where this generation failed, we used a supervised machine learning approach as a backoff (Durrett and DeNero, 2013). Compound Processing In a final step, we merge compounds using a POS-matching strategy (Stymne et al., 2008). We merge the marked compound modifiers with the following word if it is a noun or adjective, and add hyphens for modifiers in coordinated compounds. Compounding forms of modifiers are restored based on corpus frequencies. Like Stymne et al. (2008) and Cap et al. (2014), we also merge compounds in every iteration of the tuning process before the translations are scored against the reference. All re-inflection systems are constrained systems. We used Europarl and Wikipedia as parallel r"
W16-2326,P13-2121,0,0.0411259,"Missing"
W16-2326,N13-1073,0,0.0338579,"e apply KenLM (Heafield et al., 2013) and SRILM (Stolcke, 2002) for estimating language model parameters and MERT (Och, 2003) and batch-MIRA (Cherry and Foster, 2012) for parameter tuning. Most of our models are based on lowercased training data. All language models use order five with modified Kneser-Ney smoothing if not stated otherwise. All MT systems apply the phrase-based paradigm, some of them with factored representations and generation models if necessary. For word alignment we experiment with different tools. We apply standard tools like GIZA++ 1 2 (Och and Ney, 2003) and fast align (Dyer et al., 2013) but also the recently proposed Bayesian ¨ word aligner efmaral (Ostling, 2015). Efmaral is an efficient implementation of a Markov-Chain aligner using Gibbs sampling with a Bayesian extension of the IBM alignment models. It is both fast and accurate and works as a straightforward plug-in replacement for standard tools in the SMT training pipeline. The aligner is faster than fast align but more accurate in terms of alignment error rate in various benchmark tests. The advantage of using Gibbs sampling rather than the Expectation-Maximisation algorithm (as do both fast align and GIZA++) is that"
W16-2326,W11-2123,0,0.0837537,"d compound modifiers with the following word if it is a noun or adjective, and add hyphens for modifiers in coordinated compounds. Compounding forms of modifiers are restored based on corpus frequencies. Like Stymne et al. (2008) and Cap et al. (2014), we also merge compounds in every iteration of the tuning process before the translations are scored against the reference. All re-inflection systems are constrained systems. We used Europarl and Wikipedia as parallel resources and all of the Finnish data available from WMT to train five-gram language models with SRILM (Stolcke, 2002) and KENLM (Heafield, 2011). No particular cleaning or preprocessing of the data has happened. This makes the re-inflection systems differ from all other systems in this paper. Otherwise, we trained a conventional phrase-based Moses system with default settings, tuned weights using batch-MIRA with ”safe-hope” (Cherry and Foster, 2012) and used an underspecified representation of the tuning reference set to derive BLEU scores. The final result of our system is listed in Table 4. • nouns and their alignments (sentence-internal only and even document-wide) • verbs and their alignments (sentence-internal only and even docum"
W16-2326,E12-1068,1,0.904335,"with the other systems. The underlying idea of what we call re-inflection models in our submission is that we reduce all Finnish training data to an underspecified representation, where words are reduced to their lemmas and noun and adjective compounds are split into their component parts. Then, we train models and translate from English into this underspecified representation of Finnish and in a post-processing step we then merge compounds and predict morphological features for Finnish. This approach has been successfully applied to Russian and Arabic (Toutanova et al., 2008) and to German (Fraser et al. (2012), Cap et al. (2014)). Note however, that for example Fraser et al. (2012) relied on German prepositions to predict case-markers on underspecified German SMT output. In contrast to many other languages, Finnish only has a limited number of stand-alone pre- and postpositions. Instead, the prepositional meaning is encoded by case-marking. We thus adapt an approach by Tiedemann et al. (2015b) and introduce place-holder prepositions in the Finnish training data, which are likely to correspond to the prepositions used on the English side and thus improve word alignment quality. Place-holder Preposit"
W16-2326,P13-4033,1,0.860325,"an fast align but more accurate in terms of alignment error rate in various benchmark tests. The advantage of using Gibbs sampling rather than the Expectation-Maximisation algorithm (as do both fast align and GIZA++) is that inference remains quadratic with respect to sentence length even when word order and fertility models are added, which enables the efficient use of higher-order models. This is the first time that the performance of this tool is reported in the setting of statistical machine translation. Besides Moses, we also apply another phrasebased machine translation decoder, Docent (Hardmeier et al., 2013), which implements a stochastic local search decoder that is able to incorporate features with long-distance dependencies even across sentence boundaries. Docent emphasises document-level decoding but includes standard local features that make the decoder comparable with standard phrase-based SMT. The decoding algorithm applies randomly selected statechange operations to complete translation hypotheses (covering the whole document) that may be accepted by a strict hill-climbing procedure or a simulated annealing schedule. The main motivation for using Docent in our setup is to introduce non-lo"
W16-2326,W15-3021,1,0.892607,"Helsinki Fabienne Cap Uppsala University Jenna Kanerva and Filip Ginter University of Turku Sara Stymne Uppsala University ¨ Robert Ostling University of Helsinki Marion Di Marco University of Stuttgart Abstract strained systems apply all the data provided by WMT and also the English Giga-Word corpus that is distributed by the LDC. Our best systems include additional parallel data sets coming from OPUS (Tiedemann, 2012) and syntactically analysed monolingual data from the Finnish Internet Parsebank (Luotolahti et al., 2015). Additional to the parallel data we used in our submission last year (Tiedemann et al., 2015a), we include the new version of the OpenSubtitle corpus (Lison and Tiedemann, 2016) with its 18.6 million aligned translation units in English and Finnish. Furthermore, we make use of alternative subtitle translations that have been aligned monolingually in the same collection (Tiedemann, 2016). Expanding the parallel corpus with alternative translations extends the subtitle corpus by roughly 350,000 translation units with about 6.8 million tokens (counting both languages together). The contribution is quite small compared to the original corpus with its 107 million Finnish tokens and 167 mi"
W16-2326,L16-1147,1,0.802036,"y of Turku Sara Stymne Uppsala University ¨ Robert Ostling University of Helsinki Marion Di Marco University of Stuttgart Abstract strained systems apply all the data provided by WMT and also the English Giga-Word corpus that is distributed by the LDC. Our best systems include additional parallel data sets coming from OPUS (Tiedemann, 2012) and syntactically analysed monolingual data from the Finnish Internet Parsebank (Luotolahti et al., 2015). Additional to the parallel data we used in our submission last year (Tiedemann et al., 2015a), we include the new version of the OpenSubtitle corpus (Lison and Tiedemann, 2016) with its 18.6 million aligned translation units in English and Finnish. Furthermore, we make use of alternative subtitle translations that have been aligned monolingually in the same collection (Tiedemann, 2016). Expanding the parallel corpus with alternative translations extends the subtitle corpus by roughly 350,000 translation units with about 6.8 million tokens (counting both languages together). The contribution is quite small compared to the original corpus with its 107 million Finnish tokens and 167 million English tokens, but, nevertheless, it contributes to the overall collection esp"
W16-2326,tiedemann-2012-parallel,1,0.855367,"Missing"
W16-2326,W15-2124,1,0.739948,"a, Better Models and Alternative Alignment and Translation Tools J¨org Tiedemann University of Helsinki Fabienne Cap Uppsala University Jenna Kanerva and Filip Ginter University of Turku Sara Stymne Uppsala University ¨ Robert Ostling University of Helsinki Marion Di Marco University of Stuttgart Abstract strained systems apply all the data provided by WMT and also the English Giga-Word corpus that is distributed by the LDC. Our best systems include additional parallel data sets coming from OPUS (Tiedemann, 2012) and syntactically analysed monolingual data from the Finnish Internet Parsebank (Luotolahti et al., 2015). Additional to the parallel data we used in our submission last year (Tiedemann et al., 2015a), we include the new version of the OpenSubtitle corpus (Lison and Tiedemann, 2016) with its 18.6 million aligned translation units in English and Finnish. Furthermore, we make use of alternative subtitle translations that have been aligned monolingually in the same collection (Tiedemann, 2016). Expanding the parallel corpus with alternative translations extends the subtitle corpus by roughly 350,000 translation units with about 6.8 million tokens (counting both languages together). The contribution"
W16-2326,W15-2515,1,0.854248,". Using this type of lexicalisation helps to find construction-like mappings between the two languages which seems to be beneficial for the system according to the scores in our experiments (system (c) in Table 2). In combination with the surfaceoriented translation model this also leads to a slight improvement over the non-factored model (without back-translated news), which is also evi394 3.4 ilar to Tiedemann et al. (2015b), we introduce place-holder prepositions at the beginning of noun phrases bearing the corresponding case-marker in order to support word alignment. Gappy Language Models Tiedemann (2015) introduces the use of language models over selected words in the framework of document-level SMT using Docent applied to the pronoun-aware translation task of DiscoMT (Hardmeier et al., 2015). We extended this idea by developing a general framework for what we call gappy language models that refer to monolingual or bilingual n-gram language models over selected words and their alignments. We can use different factors attached to the source and target language tokens to filter for word sequences that we would like to consider. Given word alignments are used to establish the link between source"
W16-2326,D13-1032,0,0.0777915,"Missing"
W16-2326,L16-1559,1,0.839226,"Sara Stymne Uppsala University ¨ Robert Ostling University of Helsinki Marion Di Marco University of Stuttgart Abstract strained systems apply all the data provided by WMT and also the English Giga-Word corpus that is distributed by the LDC. Our best systems include additional parallel data sets coming from OPUS (Tiedemann, 2012) and syntactically analysed monolingual data from the Finnish Internet Parsebank (Luotolahti et al., 2015). Additional to the parallel data we used in our submission last year (Tiedemann et al., 2015a), we include the new version of the OpenSubtitle corpus (Lison and Tiedemann, 2016) with its 18.6 million aligned translation units in English and Finnish. Furthermore, we make use of alternative subtitle translations that have been aligned monolingually in the same collection (Tiedemann, 2016). Expanding the parallel corpus with alternative translations extends the subtitle corpus by roughly 350,000 translation units with about 6.8 million tokens (counting both languages together). The contribution is quite small compared to the original corpus with its 107 million Finnish tokens and 167 million English tokens, but, nevertheless, it contributes to the overall collection esp"
W16-2326,J03-1002,0,0.0224522,"s for training and tuning models. We apply KenLM (Heafield et al., 2013) and SRILM (Stolcke, 2002) for estimating language model parameters and MERT (Och, 2003) and batch-MIRA (Cherry and Foster, 2012) for parameter tuning. Most of our models are based on lowercased training data. All language models use order five with modified Kneser-Ney smoothing if not stated otherwise. All MT systems apply the phrase-based paradigm, some of them with factored representations and generation models if necessary. For word alignment we experiment with different tools. We apply standard tools like GIZA++ 1 2 (Och and Ney, 2003) and fast align (Dyer et al., 2013) but also the recently proposed Bayesian ¨ word aligner efmaral (Ostling, 2015). Efmaral is an efficient implementation of a Markov-Chain aligner using Gibbs sampling with a Bayesian extension of the IBM alignment models. It is both fast and accurate and works as a straightforward plug-in replacement for standard tools in the SMT training pipeline. The aligner is faster than fast align but more accurate in terms of alignment error rate in various benchmark tests. The advantage of using Gibbs sampling rather than the Expectation-Maximisation algorithm (as do b"
W16-2326,P03-1021,0,0.0559308,"Missing"
W16-2326,P08-1059,0,0.0341143,"are, therefore, not directly comparable with the other systems. The underlying idea of what we call re-inflection models in our submission is that we reduce all Finnish training data to an underspecified representation, where words are reduced to their lemmas and noun and adjective compounds are split into their component parts. Then, we train models and translate from English into this underspecified representation of Finnish and in a post-processing step we then merge compounds and predict morphological features for Finnish. This approach has been successfully applied to Russian and Arabic (Toutanova et al., 2008) and to German (Fraser et al. (2012), Cap et al. (2014)). Note however, that for example Fraser et al. (2012) relied on German prepositions to predict case-markers on underspecified German SMT output. In contrast to many other languages, Finnish only has a limited number of stand-alone pre- and postpositions. Instead, the prepositional meaning is encoded by case-marking. We thus adapt an approach by Tiedemann et al. (2015b) and introduce place-holder prepositions in the Finnish training data, which are likely to correspond to the prepositions used on the English side and thus improve word alig"
W16-2326,W15-1844,0,0.0376448,"older Prepositions: In contrast to Tiedemann et al. (2015b), we do not apply factored models (with both, lemmatised and surface forms) here but strip the case-markers from those words and only keep the underspecified representation. Moreover, we apply the approach in the opposite translation direction, which requires a generation component. The place-holder prepositions will not only lead to improved word alignments, but we will also use them to predict case-markers after translation. Overall, we follow the processing pipeline of (Cap et al., 2014): we use a rule-based morphological analyser (Pirinen, 2015) to split compounds (using the Finnish parsing pipeline to disambiguate multiple analyses) and lemmatise all Finnish training data. Compound modifiers are reduced to their lemmas and marked with a symbol that distinguishes them from other words. SimBLEU 14.10 5.45 10.89 14.17 14.70 Table 2: Lower-cased BLEU scores for factored SMT models on development test data (newstest 2015). System (a) is the same as the constrained model in Table 1. System (b) uses a factored model that translates surface words to target lemmas and morphosyntactic features separately. System (c) keeps closed-class words i"
W16-2326,W15-1821,1,0.867757,"Missing"
W16-2326,W08-0317,1,0.79278,"the CRFs, the reduction of compounds to their heads reduces data sparsity and allows the model to better generalise over all occurrences. For the translation output we remove all compound modifiers before case prediction. Morphological Generation The predicted casemarkers are then fed into the morphological generation automaton (Pirinen, 2015) in order to get fully inflected forms. In cases where this generation failed, we used a supervised machine learning approach as a backoff (Durrett and DeNero, 2013). Compound Processing In a final step, we merge compounds using a POS-matching strategy (Stymne et al., 2008). We merge the marked compound modifiers with the following word if it is a noun or adjective, and add hyphens for modifiers in coordinated compounds. Compounding forms of modifiers are restored based on corpus frequencies. Like Stymne et al. (2008) and Cap et al. (2014), we also merge compounds in every iteration of the tuning process before the translations are scored against the reference. All re-inflection systems are constrained systems. We used Europarl and Wikipedia as parallel resources and all of the Finnish data available from WMT to train five-gram language models with SRILM (Stolck"
W16-2326,P07-2045,0,\N,Missing
W16-2345,D12-1133,0,0.253709,"set of the provided training data that has well-defined document boundaries in order to allow for meaningful extraction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddi"
W16-2345,W16-2348,0,0.0249438,"urce word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER class. The difference between the primary and contrastive systems is small. In the primary system, the feature val"
W16-2345,P06-1005,0,0.150956,"res based on the target-language model estimates provided by the baseline system, linguistic features concerning the source word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER clas"
W16-2345,2012.eamt-1.60,1,0.85967,"predicting all of the other pronouns, the system relied solely on the scores coming from the proposed PLM model. This target-side PLM model uses a large target-language training dataset to learn a probabilistic relation between each target pronoun and the distribution of the gender-number of its preceding nouns and pronouns. For prediction, given each source pronoun “it” or “they”, the system uses the PLM to score all possible candidates and to select the one with the highest score. In addition to the PoS-tagged lemmatised data that was provided for the shared task, the WIT3 parallel corpus (Cettolo et al., 2012), provided as part of the training data at the DiscoMT 2015 workshop, was used to train the PLM model. Furthermore, a French PoS-tagger, Morfette (Chrupala et al., 2008), was employed for gendernumber extraction. Before extracting the examples as feature vectors, the data is linguistically preprocessed usˇ ing the Treex framework (Popel and Zabokrtsk´ y, 2010). The source-language texts undergo a thorough analysis and are enriched with PoS tags, dependency syntax, as well as semantic roles and coreference for English. On the other hand, only grammatical genders are assigned to nouns in the tar"
W16-2345,chrupala-etal-2008-learning,0,0.0898214,"Missing"
W16-2345,W16-2350,1,0.838182,"on. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trai"
W16-2345,W11-2123,0,0.0192239,"e classifier is trained on a combination of semantic, based on lexical resources such as VerbNet (Schuler, 2005) and WordNet (Miller, 1995), and frequencies computed over the annotated Gigaword corpus (Napoles et al., 2012), syntactic, from the dependency parser in the Mate tools (Bohnet et al., 2013), and contextual features. The event classification results are modest, reaching only 54.2 F-score for the event class. The translation model, into which the classifier is integrated, is a 6-gram language model computed over target lemmata using modified KneserNey smoothing and the KenLM toolkit (Heafield, 2011). In addition to the pure target lemma context, it also has access to the identity of the sourcelanguage pronoun, used as a concatenated label to each REPLACE item. This provides information about the number marking of the pronouns in the source, and also allows for the incorporation of the output of the ‘it’-label classifier. To predict classes for an unseen test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels p"
W16-2345,W16-2349,0,0.0373816,"sing the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun translation decisions. The model performs reasonably well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features"
W16-2345,W10-1737,0,0.434398,"Missing"
W16-2345,guillou-etal-2014-parcor,1,0.910739,"the fact that all talks are originally given in English, which means that French–English translation is in reality a back-translation. • she: feminine singular subject pronoun; 3 1 We explain below in Section 3.3.3 how non-subject pronouns are filtered out from the data. 528 TED talks address topics of general interest and are delivered to a live public audience whose responses are also audible on the recordings. The talks generally aim to be persuasive and to change the viewers’ behaviour or beliefs. The genre of the TED talks is transcribed planned speech. As shown in analysis presented by Guillou et al. (2014), TED talks differ from other text types with respect to pronoun usage. TED speakers frequently use first- and second-person pronouns (singular and plural): first-person to refer to themselves and their colleagues or to themselves and the audience, second-person to refer to the audience, the larger set of viewers, or people in general. TED speakers often use the pronoun “they” without a specific textual antecedent, in sentences such as “This is what they think.” They also use deictic and third-person pronouns to refer to things in the spatio-temporal context shared by the speaker and the audie"
W16-2345,W16-2351,1,0.900928,"Missing"
W16-2345,E12-3001,1,0.880326,"it is required by syntax to fill the subject position. An event reference pronoun may refer to a verb phrase (VP), a clause, an entire sentence, or a longer passage of text. Examples of each of these pronoun functions are provided in Figure 1. It is clear that instances of the English pronoun “it” belonging to each of these functions would have different translation requirements in French and German. Introduction Pronoun translation poses a problem for current state-of-the-art Statistical Machine Translation (SMT) systems (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). 525 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 525–542, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2 The problem of pronouns in machine translation has long been studied. In particular, for SMT systems, the recent previous studies cited above have focused on the translation of anaphoric pronouns. In this case, a well-known constraint of languages with grammatical gender is that agreement must hold between an anaphoric pronoun and the NP with which it corefers, called its antecede"
W16-2345,W16-2352,1,0.881771,"Missing"
W16-2345,W16-2353,0,0.0435664,"s useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed sequences of these embeddings within a certain window to the left and to the right of the target pronoun. The window size used by the system is 50 tokens or until the end of the sentence boundary. All of these inputs are read"
W16-2345,2010.iwslt-papers.10,1,0.888921,"Missing"
W16-2345,D13-1037,1,0.883273,"3 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trained 6gram language model identical to the contrastive system of the UUPPSALA submission described above. The"
W16-2345,S16-1001,1,0.795211,"d 69.76 in macro-averaged recall. This is very much above the performance of baseline0 and baseline-1.5, which are in the low-mid 40s. It is also well above the majority/random baseline (not shown) at 11.11, which is outperformed by far by all systems. Note that the top-3 systems in terms of macro-averaged recall are also the top-3 in terms of accuracy, but in different order. Evaluation While in 2015 we used macro-averaged F1 as an official evaluation measure, this year we adopted macro-averaged recall, which was also recently adopted by some other competitions, e.g., by SemEval-2016 Task 4 (Nakov et al., 2016). Moreover, as in 2015, we also report accuracy as a secondary evaluation measure. Macro-averaged recall ranges in [0, 1], where a value of 1 is achieved by the perfect classifier,8 and a value of 0 is achieved by the classifier that misclassifies all examples. The value of 1/C, where C is the number of classes, is achieved by a trivial classifier that assigns the same class to all examples (regardless of which class is chosen), and is also the expected value of a random classifier. 8 If the test data did not have any instances of some of the classes, we excluded these classes from the macro-a"
W16-2345,W15-2501,1,0.657407,"ould replace a placeholder value (represented by the token REPLACE) in the target-language text. It requires no specific Machine Translation (MT) expertise and is interesting as a machine learning task in its own right. Within the context of SMT, one could think of the task of cross-lingual pronoun prediction as a component of an SMT system. This component may take the form of a decoder feature or it may be used to provide “corrected” pronoun translations in a post-editing scenario. The design of the WMT 2016 shared task has been influenced by the design and the results of a 2015 shared task (Hardmeier et al., 2015) organised at the EMNLP workshop on Discourse in MT (DiscoMT). The first intuition about evaluating pronoun translation is to require participants to submit MT systems — possibly with specific strategies for pronoun translation — and to estimate the correctness of the pronouns they output. This estimation, however, cannot be performed with full reliability only by comparing pronouns across candidate and reference translations because this would miss the legitimate variation of certain pronouns, as well as variations in gender or number of the antecedent itself. Human judges are thus required f"
W16-2345,W16-2354,0,0.0469939,"Missing"
W16-2345,H05-1108,0,0.0601982,"Missing"
W16-2345,W14-3334,1,0.800608,"he OTHER class. For the DiscoMT 2015 shared task, we explored this issue for English–French and found that GIZA++ model 4 and HMM with grow-diag-final-and symmetrisation gave the best results. For pronoun– pronoun links, we had an F-score of 0.96, with perfect recall and precision of 0.93 (Hardmeier et al., 2015). This was slightly higher than for other links, which had an F-score of 0.92. For German–English, we explored this issue this year since it is a new language pair. We used an aligned gold standard of 987 sentences from (Pad´o and Lapata, 2005), which has been extensively evaluated by Stymne et al. (2014). We used the same methodology as in 2015, and performed an evaluation on the subset of links between the pronouns we are interested in. We report precision and recall of links both for the pronoun subset and for all links, shown in Table 4. The alignment quality is considerably worse than for French–English both for all links and for pronouns, but again the results for pronouns is better than for all links in both precision and recall. 6 https://github.com/slavpetrov/ universal-pos-tags 530 Alignment Symmetrisation Model 4 fast-align gdfa HMM gd gdf ∪ ∩ All links P R Pronouns P R .75 .69 .80"
W16-2345,W16-2355,1,0.832701,"the test dataset is imbalanced. Thus, one cannot interpret the absolute value of accuracy (e.g., is 0.7 a good or a bad value?) without comparing it to a baseline that must be computed for each specific test dataset. In contrast, for macro-averaged recall, it is clear that a value of, e.g., 0.7, is well above the majority-class and the random baselines, which are both always 1/C (e.g., 0.5 with two classes, 0.33 with three classes, etc.). Standard F1 and macro-averaged F1 are also sensitive to class imbalance for the same reason; see Sebastiani (2015) for more detail. The UU-S TYMNE systems (Stymne, 2016) use linear SVM classifiers for all language pairs. A number of different features were explored, but anaphora is not explicitly modelled. The features used can be grouped in the following way: source pronouns, local context words/lemmata, preceding nouns, target PoS n-grams with two different PoS tag-sets, dependency heads of pronouns, target LM scores, alignments, and pronoun position. A joint tagger and dependency parser on the source text is used for some of the features. The primary system is a 2-step classifier where a binary classifier is first used to distinguish between the OTHER clas"
W16-2345,petrov-etal-2012-universal,0,0.0937891,"Missing"
W16-2345,W16-2356,1,0.48149,"networks, except for the embedding for the aligned pronoun. All outputs of the recurrent layers are concatenated to a single vector along with the embedding of the aligned pronoun. This vector is then used to make the pronoun prediction by a dense neural network layer. The primary systems are trained to optimise macro-averaged recall and the contrastive systems are optimised without preference towards rare classes. The system is trained only on the shared task data and all parts of the data, in-domain and out-of-domain, are used for training the system. 5.5 5.6 UHELSINKI The UHELSINKI system (Tiedemann, 2016) implements a simple linear classifier based on LibSVM with its L2-loss SVC dual solver. The system applies local source-language and target-language context using the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun tra"
W16-2345,W16-2357,0,0.0259257,"well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features: tokens and their PoS tags are extracted from a context window around source- and targetside pronouns. N -gram combinations of these features are included by concatenating adjacent tokens or PoS tags. Furthermore, the pleonastic use of a pronoun is detected with NADA (Bergsma and Yarowsky, 2011) on the source side. 534 This CRF approach has been applied only to German, but there are plans to extend it to other languages. This indicates that the NN mechanism is quite effective. Th"
W16-2345,sagot-2010-lefff,0,0.0184156,"traction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed seq"
W16-2345,schmid-etal-2004-smor,0,0.0349386,"test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the pr"
W16-2345,W12-3018,0,\N,Missing
W16-2345,2015.iwslt-evaluation.1,1,\N,Missing
W16-2345,W14-6111,0,\N,Missing
W16-2356,W15-2515,1,0.62695,"itted to the shared task on cross-lingual pronoun prediction at WMT 2016. The goal of the submission is to provide yet another baseline that is slightly more informed than the language model baseline provided by the organisers otherwise. In the following, we will briefly discuss the model and our feature engineering efforts. Thereafter, we discuss the results for each language pair and conclude. 2 The Model • Source language context before the pronoun in question Our model follows the setup of our submissions from last year to the same task at the workshop on discourse in machine translation (Tiedemann, 2015; Hardmeier et al., 2015). Again, we apply a • Source language context after the pronoun in question 616 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 616–619, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics he she it they you this these there OTHER -SUMhe 30 0 5 2 0 0 0 0 1 38 she 0 11 2 2 1 1 0 0 0 17 it 1 3 95 6 2 7 0 4 8 126 they 0 5 8 61 11 0 0 0 7 92 you 0 1 0 4 89 0 0 0 12 106 this 0 0 1 0 0 2 0 0 0 3 these 0 0 0 0 0 0 0 0 0 0 there 0 0 2 1 0 1 0 12 0 16 OTHER 0 1 1 2 3 2 0 0 76 85 31 21 114 78 106 1"
W16-4020,E12-1009,0,0.0197792,"Aligned Translations Finally, we can now use the aligned training data to transfer annotations from the parsed English glosses to the tokenized Ingush input. We use simple heuristics mapping universal part-of-speech labels and dependency relations based on a direct correspondence assumption. Figure 3 shows two examples of projected dependency structure using the annotation projection approach. We ignore all unaligned words and create a synthetic treebank that we can use to train a statistical parser. Several options are possible for training such a parser model and we opt for the mate-tools (Bohnet and Kuhn, 2012) that have been shown to produce highly accurate parsing models for a variety of languages. Unfortunately, at this point we cannot say much about the quality of this initial parser as we do not have any gold standard available. In future work, we will manually check transfered annotation and parsing results and iteratively bootstrap a usable parser based on some kind of active learning schema. 6 Conclusions This paper presents on-going work on creating NLP tools for a low-resource language, Ingush, using data from extensive linguistic fieldwork. We discuss the challenges of converting data set"
W16-4020,D13-1032,0,0.078304,"Missing"
W16-4020,L16-1680,0,0.0616665,"Missing"
W16-4020,C14-1175,1,0.870217,"Missing"
W16-4020,W14-1613,0,0.0613977,"Missing"
W16-4020,N13-1073,0,\N,Missing
W16-4315,W15-2903,0,0.0238805,"s languages. Studies directly evaluating the preservation of sentiments in translation have often focused on comparing them with other methods such as whether it is better to translate the original text to English and analyze the English sentiments or to translate the lexicon from English to the ”original” target language (work on Arabic (Salameh et al., 2015) and Chinese (Wan, 2008)). One study found that connotations change if texts are machine translated or manually translated and suggested that ”further cross-lingual studies should not use parallel corpora to project annotations blindly” (Carpuat, 2015). Related work does not provide a full picture of sentiment preservation in translation and we are interested in additional investigations with other data sets and setups. In particular, we would like to understand more clearly how sentiment preservation applies to the multidimensional task and whether there are differences between cases of similar versus less-related languages. For this purpose, we use lexicon-based methods and parallel data sets as a proxy for multilingual sentiment analyses on comparable texts. We also test the reliability of the purely lexical sentiment detection strategy"
W16-4315,L16-1147,1,0.839989,"nual evaluation of sentiment across languages and, finally, we discuss the results of 1 Source: https://en.wikipedia.org/wiki/Contrasting_and_categorization_of_emotions 139 automatic multi-dimensional sentiment classification based on an existing lexical resource. We conclude with our main findings and prospects for future research. 2 Multilingual Data Resources For our cross-lingual experiments we rely on publicly available parallel data sets. OPUS2 provides large quantities of sentence-aligned multilingual corpora including a comprehensive collection of movie subtitles in various languages (Lison and Tiedemann, 2016). Movies certainly contain a lot of emotional contents and their predominantly colloquial style makes them a good proxy for social media data we aim at with our multidimensional sentiment analyzer. As a comparison data set, we selected the Europarl parallel corpus (Tiedemann, 2012). Europarl represents a different genre and the translations come from professional sources, whereas the subtitle translations contain a much larger quantity of noise (due to the unreliability of user-generated / userprovided content, incomplete data sets as well as conversion and alignment errors). We are thus able"
W16-4315,N15-1078,0,0.0254747,"inions, Personality, and Emotions in Social Media, pages 138–142, Osaka, Japan, December 12 2016. Figure 1: Plutchik’s wheel of emotions 1 One of the main research question we ask in our study is whether fine-grained sentiment and emotions are preserved across languages. Studies directly evaluating the preservation of sentiments in translation have often focused on comparing them with other methods such as whether it is better to translate the original text to English and analyze the English sentiments or to translate the lexicon from English to the ”original” target language (work on Arabic (Salameh et al., 2015) and Chinese (Wan, 2008)). One study found that connotations change if texts are machine translated or manually translated and suggested that ”further cross-lingual studies should not use parallel corpora to project annotations blindly” (Carpuat, 2015). Related work does not provide a full picture of sentiment preservation in translation and we are interested in additional investigations with other data sets and setups. In particular, we would like to understand more clearly how sentiment preservation applies to the multidimensional task and whether there are differences between cases of simil"
W16-4315,L16-1680,0,0.0236219,"Missing"
W16-4315,tiedemann-2012-parallel,1,0.801599,"indings and prospects for future research. 2 Multilingual Data Resources For our cross-lingual experiments we rely on publicly available parallel data sets. OPUS2 provides large quantities of sentence-aligned multilingual corpora including a comprehensive collection of movie subtitles in various languages (Lison and Tiedemann, 2016). Movies certainly contain a lot of emotional contents and their predominantly colloquial style makes them a good proxy for social media data we aim at with our multidimensional sentiment analyzer. As a comparison data set, we selected the Europarl parallel corpus (Tiedemann, 2012). Europarl represents a different genre and the translations come from professional sources, whereas the subtitle translations contain a much larger quantity of noise (due to the unreliability of user-generated / userprovided content, incomplete data sets as well as conversion and alignment errors). We are thus able to compare two different quality-levels of translation as well besides the comparison of two dissimilar genres. In both cases, we used 1.5 million lines of aligned sentences from the parallel corpora for each language, which we lemmatized using the Turku Finnish Dependency Parser f"
W16-4315,D08-1058,0,0.0415214,"n Social Media, pages 138–142, Osaka, Japan, December 12 2016. Figure 1: Plutchik’s wheel of emotions 1 One of the main research question we ask in our study is whether fine-grained sentiment and emotions are preserved across languages. Studies directly evaluating the preservation of sentiments in translation have often focused on comparing them with other methods such as whether it is better to translate the original text to English and analyze the English sentiments or to translate the lexicon from English to the ”original” target language (work on Arabic (Salameh et al., 2015) and Chinese (Wan, 2008)). One study found that connotations change if texts are machine translated or manually translated and suggested that ”further cross-lingual studies should not use parallel corpora to project annotations blindly” (Carpuat, 2015). Related work does not provide a full picture of sentiment preservation in translation and we are interested in additional investigations with other data sets and setups. In particular, we would like to understand more clearly how sentiment preservation applies to the multidimensional task and whether there are differences between cases of similar versus less-related l"
W16-4315,W13-5626,0,\N,Missing
W16-4801,W16-4821,0,0.0339271,"Missing"
W16-4801,W16-4826,0,0.0502832,"Missing"
W16-4801,W16-4827,0,0.11257,"Missing"
W16-4801,W16-4819,0,0.0808398,"Missing"
W16-4801,W16-4816,0,0.0637155,"Missing"
W16-4801,W15-5410,0,0.238809,"Missing"
W16-4801,W16-4802,0,0.114498,"Missing"
W16-4801,W16-4831,0,0.0913939,"Missing"
W16-4801,W16-4830,0,0.0438028,"Missing"
W16-4801,D14-1154,0,0.0630846,"otivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with methods proposed for similar language identification. For this reason, in the 2016 edition of the DSL challenge we offered a subtask on Arabic dialect identification"
W16-4801,W16-4828,0,0.0387334,"Missing"
W16-4801,W15-5409,0,0.149587,"Missing"
W16-4801,W16-4829,0,0.0511602,"Missing"
W16-4801,W15-5403,0,0.27462,"Missing"
W16-4801,W16-4822,0,0.0759257,"Missing"
W16-4801,W15-5413,0,0.562658,"Missing"
W16-4801,W16-4823,0,0.0418563,"Missing"
W16-4801,W14-5316,0,0.385305,"Missing"
W16-4801,L16-1284,1,0.849869,"Missing"
W16-4801,W16-4824,0,0.0430542,"Missing"
W16-4801,W16-4817,0,0.0342647,"Missing"
W16-4801,W16-4815,0,0.0384942,"Missing"
W16-4801,Y08-1042,0,0.0574334,"ticipants. Below we present the task setup, the evaluation results, and a brief discussion about the features and learning methods that worked best. More detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic"
W16-4801,W16-4818,0,0.128184,"Missing"
W16-4801,J16-3005,0,0.112984,"Missing"
W16-4801,W15-5408,0,0.217783,"Missing"
W16-4801,W16-4820,0,0.38437,"Missing"
W16-4801,W14-5317,0,0.0630949,"Missing"
W16-4801,U13-1003,0,0.256503,"size and scope featuring two subtasks and attracting a record number of participants. Below we present the task setup, the evaluation results, and a brief discussion about the features and learning methods that worked best. More detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natur"
W16-4801,W14-5315,0,0.0614151,"Missing"
W16-4801,W15-5407,1,0.493189,"detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods incl"
W16-4801,W16-4814,1,0.85912,"Missing"
W16-4801,W16-4825,0,0.0362109,"Missing"
W16-4801,W14-5314,0,0.0924571,"Missing"
W16-4801,W15-3205,0,0.0361398,"ortuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with met"
W16-4801,W14-5313,0,0.115089,"of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with methods proposed for similar language identification. For this reason, in the 2016 edition of the DSL challenge we offered a subtask on Arabic dialect identification. Below, we discuss some related shared tasks including the first two editions of the DSL challenge. 2.1 Related Shared Tasks Several shared tasks related to the DSL task have been organized in recent years. Two examples are the ALTW language identification shared task (Baldwin and Lui, 2010) on general-purpose language identification,"
W16-4801,P11-1122,0,0.0270219,"Missing"
W16-4801,W14-5307,1,0.744553,"Missing"
W16-4801,W15-5411,1,0.882098,"Missing"
W17-1201,W16-4802,0,0.127115,"character ngrams and a Na”ive Bayes classifier. The system followed the work of the system submitted to the DSL 2016 by Barbaresi (2016). • CECL: The system uses a two-step approach as in (Goutte et al., 2014). The first step identifies the language group using an SVM classifier with a linear kernel trained on character n-grams (1-4) that occur at least 100 times in the dataset weighted by Okapi BM25 (Robertson et al., 1995). The second step discriminates between each language within the group using a set of SVM classifiers trained • tubasfs: Following the success of tubasfs at DSL 2016 (C¸o¨ ltekin and Rama, 2016), which was ranked first in the closed training track, this year’s tubasfs submission used a linear SVM classifier. The system used both characters and words as features, and carefully optimized hyperparameters: n-gram size and margin/regularization parameter for SVM. 5 In 2016 ADI and DSL were organized under the name DSL shared task, and ADI was run as a sub-task. 4 • gauge: This team submitted a total of three runs. Run 1 used an SVM classifier with character n-grams (2–6), run 2 (their best run) used logistic regression trained using character n-grams (1–6), and run 3 used hard voting of t"
W17-1201,W17-1221,0,0.532877,"of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent pu"
W17-1201,W17-1215,0,0.0474398,"Missing"
W17-1201,W17-1213,0,0.0702486,"pairs include a triple of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar langua"
W17-1201,W13-1728,1,0.0339111,"rovided lexical features. This year, we added a multi-model aspect to the task by further providing acoustic features. The system description paper of CECL (Bestgen, 2017) provides some interesting insights about the DSL task. First, they found out that BM25 weighting, which was previously applied to native language identification (NLI) (Wang et al., 2016), worked better than using TF.IDF. They further highlighted the similarity between similar language identification and NLI as evidenced by a number of entries in the DSL task that are adaptations of systems used for NLI (Goutte et al., 2013; Gebre et al., 2013; Jarvis et al., 2013). We observe that the variation in performance among the top ten teams is less than four percentage points. The team ranked last (eleventh) approached the task using LSTM and achieved an F1 score of 0.202. Unfortunately, they did not submit a system description paper, and thus we do not have much detail about their system. However, in the DSL 2016 task (Malmasi et al., 2016), neural network-based approaches already proved not to be very competitive for the task. See (Medvedeva et al., 2017) for a comparison between the performance of an SVM and an RNN approach for the DSL"
W17-1201,W17-1217,0,0.0300428,"Missing"
W17-1201,W15-5413,0,0.101469,"Missing"
W17-1201,W13-1712,0,0.199423,"Missing"
W17-1201,W14-5316,0,0.160565,"Missing"
W17-1201,U13-1003,0,0.160089,"est set A: in the closed training setting, where the systems were trained only using the training data provided by the DSL organizers, their accuracy dropped from 95.54 to 94.01, from 95.24 to 92.78, from 95.24 to 93.01, and from 94.67 to 93.02, respectively.3 Finally, inspired by recent work on language identification of user-generated content (Ljubeˇsi´c and Kranjˇci´c, 2015; Abainia et al., 2016), in the DSL 2016 task (Malmasi et al., 2016), we looked at how systems perform on discriminating between similar languages and language varieties across different domains, an aspect highlighted by Lui and Cook (2013) and Lui (2014). For this purpose, we provided an out-of-domain test set containing manually annotated microblog posts written in Bosnian, Croatian, Serbian, Brazilian and European Portuguese. 2.1 2.2 Dataset The DSLCC v4.04 contains 22,000 short excerpts of news texts for each language or language variety divided into 20,000 texts for training (18,000 texts) and development (2,000 texts), and 2,000 texts for testing. It contains a total of 8.6 million tokens for training and over half a million tokens for testing. The fourteen languages included in the v4.0 grouped by similarity are Bosnian,"
W17-1201,L16-1284,1,0.900242,"Missing"
W17-1201,W16-3928,0,0.0176574,"ranging from 3 for CLP to 11 for DSL. Below we describe the individual tasks. 2 Discriminating between Similar Languages (DSL) Discriminating between similar languages is one of the main challenges faced by language identification systems. Since 2014 the DSL shared task has been organized every year providing scholars and developers with an opportunity to evaluate language identification methods using a standard dataset and evaluation methodology. Albeit related to other shared tasks such as the 2014 TweetLID challenge (Zubiaga et al., 2014) and the 2016 shared task on Geolocation Prediction (Han et al., 2016), the DSL shared task continues to be the only shared task focusing on the discrimination between similar languages and language varieties. 1 The MAZA team submitted two separate papers: one for each task they participated in. 2 This number does not include the submissions to the Arabic Dialect Identification subtask of DSL in 2016. 2 At DSL 2015, the four best systems, MAC (Malmasi and Dras, 2015b), MMS (Zampieri et al., 2015a), NRC (Goutte and L´eger, 2015), and SUKI (Jauhiainen et al., 2015) performed similarly on test set B compared to test set A: in the closed training setting, where the"
W17-1201,W15-5407,1,0.88226,"ods using a standard dataset and evaluation methodology. Albeit related to other shared tasks such as the 2014 TweetLID challenge (Zubiaga et al., 2014) and the 2016 shared task on Geolocation Prediction (Han et al., 2016), the DSL shared task continues to be the only shared task focusing on the discrimination between similar languages and language varieties. 1 The MAZA team submitted two separate papers: one for each task they participated in. 2 This number does not include the submissions to the Arabic Dialect Identification subtask of DSL in 2016. 2 At DSL 2015, the four best systems, MAC (Malmasi and Dras, 2015b), MMS (Zampieri et al., 2015a), NRC (Goutte and L´eger, 2015), and SUKI (Jauhiainen et al., 2015) performed similarly on test set B compared to test set A: in the closed training setting, where the systems were trained only using the training data provided by the DSL organizers, their accuracy dropped from 95.54 to 94.01, from 95.24 to 92.78, from 95.24 to 93.01, and from 94.67 to 93.02, respectively.3 Finally, inspired by recent work on language identification of user-generated content (Ljubeˇsi´c and Kranjˇci´c, 2015; Abainia et al., 2016), in the DSL 2016 task (Malmasi et al., 2016), we l"
W17-1201,W17-1222,1,0.800029,"milar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al.,"
W17-1201,W17-1211,0,0.333885,"Danish, and Norwegian (TL) – Swedish (SL). Note that the latter two pairs include a triple of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the r"
W17-1201,W17-1220,1,0.878577,"milar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al.,"
W17-1201,W16-4801,1,0.679876,"Missing"
W17-1201,W17-1225,0,0.428199,"us Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al., 2016). We also saw the number of system submissions to the DSL challenge grow from 8 in 2014 to 10 in 2015 and then to 17 in 2016.2 The 2015 and the 2016 editions of the DSL"
W17-1201,W13-1714,0,0.148384,"ures. This year, we added a multi-model aspect to the task by further providing acoustic features. The system description paper of CECL (Bestgen, 2017) provides some interesting insights about the DSL task. First, they found out that BM25 weighting, which was previously applied to native language identification (NLI) (Wang et al., 2016), worked better than using TF.IDF. They further highlighted the similarity between similar language identification and NLI as evidenced by a number of entries in the DSL task that are adaptations of systems used for NLI (Goutte et al., 2013; Gebre et al., 2013; Jarvis et al., 2013). We observe that the variation in performance among the top ten teams is less than four percentage points. The team ranked last (eleventh) approached the task using LSTM and achieved an F1 score of 0.202. Unfortunately, they did not submit a system description paper, and thus we do not have much detail about their system. However, in the DSL 2016 task (Malmasi et al., 2016), neural network-based approaches already proved not to be very competitive for the task. See (Medvedeva et al., 2017) for a comparison between the performance of an SVM and an RNN approach for the DSL task. 2.5 Arabic Dial"
W17-1201,W17-1219,0,0.132166,"cia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al., 2016). We also saw the number of system submissions to th"
W17-1201,W15-5408,0,0.243101,"Missing"
W17-1201,W16-4820,0,0.425079,"Missing"
W17-1201,W17-1212,0,0.183999,"Missing"
W17-1201,W17-1226,0,0.0868204,"Missing"
W17-1201,L16-1641,1,0.367123,"Missing"
W17-1201,N15-1010,0,0.0146702,"Missing"
W17-1201,W15-3040,0,0.0163982,"ams (1–6), and run 3 used hard voting of three systems: SVM, Logistic Regression, and Na”ive Bayes and character ngrams (2–6) as features. • bayesline: This team participated with a Multinomial Na¨ıve Bayes (MNB) classifier similar to that of Tan et al. (2014), with no special parameter tuning, as this system was initially intended to serve as an intelligent baseline for the task (but now it has matured into a competitive system). In their bestperforming run 1, they relied primarily on character 4-grams as features. The feature sets they used were selected by a search strategy as proposed in (Scarton et al., 2015). • cic ualg: This team submitted three runs. Runs 1 and 2 first predict the language group, and then discriminate between the languages within that group. The first step uses an SVM classifier with a combination of character 3– 5-grams, typed character 3-grams, applying the character n-gram categories introduced by Sapkota et al. (2015), and word unigrams using TF-weighting. The second step uses the same features and different classifiers: SVMs + Multinominal Na¨ıve Bayes (MNB) in run 1, and MNB in run 2 (which works best). Run 3 uses a single MNB classifier to discriminate between all fourte"
W17-1201,D10-1112,1,0.910362,"CN i-vector (as in Run 2) with (ii) an SVM model trained on count bag of characters 2–4-grams, which yielded an F1 of 0.612. This year, we introduced a new dialectal area, which focused on German dialects of Switzerland. Indeed, the German-speaking part of Switzerland is characterized by the widespread use of dialects in everyday communication, and by a large number of different dialects and dialectal areas. There have been two major approaches to Swiss German dialect identification in the literature. The corpus-based approach predicts the dialect of any text fragment extracted from a corpus (Scherrer and Rambow, 2010; Hollenstein and Aepli, 2015). The dialectological approach tries to identify a small set of distinguishing dialectal features, which are then elicited interactively from the user in order to identify his or her dialect (Leemann et al., 2016). In this task, we adopt a corpus-based approach, and we develop a new dataset for this. • deepCybErNet: This team submitted two runs. Run 1 adopted a Bi-LSTM architecture using the lexical features, and achieved an F1 score of 0.208, while run 2 used the i-vector features and achieved an F1 of 0.574. 3.3 Results Table 5 shows the evaluation results for t"
W17-1201,W14-5307,1,0.307051,"Missing"
W17-1201,W15-5411,1,0.900323,"Missing"
W17-1201,L16-1680,0,0.0188911,"Missing"
W17-1201,N12-1052,0,0.0102303,"Missing"
W17-1201,W14-1614,1,0.904663,"Missing"
W17-1201,tiedemann-2012-parallel,1,0.0255189,"LP task: parallel training data. Participants were asked not to use the development data with their gold standard annotation of dependency relations for any training purposes. The purpose of the development datasets is entirely for testing model performance during system development. All the knowledge used for parsing should origin in the provided source language data. Other sources (except for target language sources) could also be used in unconstrained submissions, but none of the participants chose that option. For the constrained setup, we also provided parallel datasets coming from OPUS (Tiedemann, 2012) that could be used for training cross-lingual parsers in any way. The datasets included translated movie subtitles and contained quite a bit of noise in terms of alignment, encoding, and translation quality. They were also from a very different domain, which made the setup quite realistic considering that one would used whatever could be found for the task. The sizes of the parallel datasets are given in Table 8. In the setup of the shared task, we also provided simple baselines and an “upper bound” of a model trained on annotated target language data. The cross-lingual baselines included del"
W17-1201,C14-1175,1,0.927054,"nd without any optimization of the hyper parameters. The size of the source language data is given in Table 5. We can see that for Czech we have by far the largest corpus, which will also be reflected in the results we obtain. VarDial 2017 featured for the first time a crosslingual parsing task for closely related languages.7 Transfer learning and annotation projection are popular approaches in this field and various techniques and models have been proposed in the literature in particular in connection with dependency parsing (Hwa et al., 2005; McDonald et al., 2013; T¨ackstr¨om et al., 2012; Tiedemann, 2014). The motivation for cross-lingual models is the attempt to bootstrap tools for languages that do not have annotated resources, which are typically necessary for supervised data-driven techniques, using data and resources from other languages. This is especially successful for closely related languages with similar syntactic structures and strong lexical overlap (Agi´c et al., 2012). With this background, it is a natural extension for our shared task to consider cross-lingual parsing as well. We do so by simulating the resource-poor situation by selecting language pairs from the Universal Depe"
W17-1201,W15-2137,1,0.514813,"ad of around 0.7. 4.5 Summary This first edition of the GDI task was a success, given the short time between the 2016 and 2017 editions. In the future, we would like to better control transcriber effects, either by a more thorough selection of training and test data, or by adding transcriber-independent features such as acoustic features, as has been done in the ADI task this year. Further dialectal areas could also be added. 10 5 Cross-lingual Dependency Parsing (CLP) Avoiding gold labels is important here in order to avoid exaggerated results that blur the picture of a more realistic setup (Tiedemann, 2015). The tagger models are trained on the original target language treebanks using UDpipe (Straka et al., 2016) with standard settings and without any optimization of the hyper parameters. The size of the source language data is given in Table 5. We can see that for Czech we have by far the largest corpus, which will also be reflected in the results we obtain. VarDial 2017 featured for the first time a crosslingual parsing task for closely related languages.7 Transfer learning and annotation projection are popular approaches in this field and various techniques and models have been proposed in th"
W17-1201,W17-1216,1,0.921592,"shop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and G"
W17-1216,C14-1175,1,0.845294,"2013)). We will focus on data transfer in this paper using annotation projection and machine translation to transform source language treebanks to be used as training data for dependency parsers in the target language. Our previous work has shown that these techniques are quite robust and show better performance than simple transfer models based on delexicalized parsers (Tiedemann and Agi´c, 2016). This is especially true for real-world test 2 Methodology and Data Our submission is based on previous work and basically applies models and techniques that have been proposed by (Hwa et al., 2005; Tiedemann, 2014; Tiedemann et al., 2014). We made very lit131 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 131–136, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics label 2 label 2 label 1 label 1 label 3 src1 src2 src3 src4 pos1 pos2 pos4 trg1 trg2 pos3 trg3 label 2 label 3 label 1 src1 src2 src3 src4 pos1 pos2 pos3 pos4 dummy pos2 pos4 pos3 DUMMY trg1 trg2 trg3 DUMMY → → pos1 dummy label 2 dummy label 3 label 3 src1 src2 src3 src4 pos1 pos2 pos3 pos4 pos1 pos2 pos4 trg1 trg2 trg3 label 1 label 2 label 1 Figure 1: Annotation p"
W17-1216,C10-1011,0,0.0132888,"e pairs from the VarDial campaign and here we present the relevant results from our experiments. First of all, we need to mention that we created new baselines using the mate-tools to have fair comparisons of the cross-lingual models with respect to baseline approaches. The new figures (on development data) are given in Table 1. The same table also summarizes our basic results for all language pairs using the three approaches for data transfer as introduced in the previous section. All projections are made in collapseDummy mode as explained above. For training the parsers, we used mate-tools (Bohnet, 2010), which gave us significantly better results than UDPipe (Straka et al., 2016) without proper parameter optimization except for some delexicalized models. Table 1 compares the baseline models with the two different toolkits. We still apply UDPipe for PoS and morphological tagging using the provided tagger models for the target languages and similar ones trained on the UD treebanks for the source languages except for Czech, which did not work with standard settings due to the complexity of the tagset and limitations of the implementation of UDPipe. Instead, we apply Marmot (M¨uller and Sch¨utze"
W17-1216,W15-1824,1,0.906571,"Missing"
W17-1216,W17-1201,1,0.89402,"Missing"
W17-1216,N15-1055,0,0.0473012,"Missing"
W17-1216,L16-1262,0,0.103899,"Missing"
W17-1216,W17-1226,0,0.0956655,"Missing"
W17-1216,L16-1680,0,0.0880706,"Missing"
W17-1216,W15-2137,1,\N,Missing
W17-4731,L16-1629,0,0.0510028,"Missing"
W17-4731,C96-1096,1,0.636384,"Missing"
W17-4731,C04-1037,1,0.772679,"Missing"
W17-4731,C90-3030,0,0.822504,"Missing"
W17-4731,W15-1844,0,0.0453004,"Missing"
W17-4731,2009.freeopmt-1.5,0,0.0837337,"Missing"
W17-4731,N15-1177,0,0.0279886,"Missing"
W17-4733,W11-2123,0,0.0291972,", hyphen retokenization (HR), direction (forward or backward). The best result was submitted for manual evaluation, where it ranked #1 (tied with one unconstrained system). tical MT. Both techniques are popular in neural MT but their impact on statistical MT has not been evaluated properly before. Therefore, we started a systematic comparison of different setups including various types of segmentations and data collections. All systems are based on Moses (Koehn et al., 2007) and we use standard configurations for training non-factored phrase-based SMT models using KenLM for language modeling (Heafield, 2011) and BLEU-based MERT for tuning. The only difference to the standard pipeline is ¨ the use of efmaral (Ostling and Tiedemann, 2016), an efficient implementation of fertility-based IBM word alignment models with a Bayesian extension and Gibbs sampling.3 Table 5 summarizes the results of our SMT experiments during development. The first observation is that BPE (and also supervised morphological segmentation) is not very helpful. This is somewhat surprising as we expect a similar problem as with neural MT in the sense that the productive and rich morphology in Finnish causes problems due to data"
W17-4733,P07-2045,0,0.00799708,"17 WMT+back opus+osm Table 4: Submitted HNMT systems with official results. They vary with respect to decoder type, input normalization (IN), hyphen retokenization (HR), direction (forward or backward). The best result was submitted for manual evaluation, where it ranked #1 (tied with one unconstrained system). tical MT. Both techniques are popular in neural MT but their impact on statistical MT has not been evaluated properly before. Therefore, we started a systematic comparison of different setups including various types of segmentations and data collections. All systems are based on Moses (Koehn et al., 2007) and we use standard configurations for training non-factored phrase-based SMT models using KenLM for language modeling (Heafield, 2011) and BLEU-based MERT for tuning. The only difference to the standard pipeline is ¨ the use of efmaral (Ostling and Tiedemann, 2016), an efficient implementation of fertility-based IBM word alignment models with a Bayesian extension and Gibbs sampling.3 Table 5 summarizes the results of our SMT experiments during development. The first observation is that BPE (and also supervised morphological segmentation) is not very helpful. This is somewhat surprising as we"
W17-4733,P16-1100,0,0.062073,"Missing"
W17-4733,C16-1172,0,0.0398962,"Missing"
W17-4733,P10-2016,0,0.0239383,"Missing"
W17-4733,E06-1032,0,0.0313954,"and a standard phrase-based SMT model to translate parts of 2014-2016 news data. The statistics of the backtranslations are given in Table 6. sional translator. The impression of the reviewer was that the perceived quality of NMT far exceeds that of SMT, mainly due to the superior fluency of NMT. The BLEU scores of the systems also indicate a significant quality difference in favor of NMT. However, single-reference BLEU scores are known to be unreliable indicators of quality for morphologically complex languages (Bojar et al., 2010), and they are also known to favor SMT over other MT methods (Callison-Burch et al., 2006). Due to this, it is possible that the BLEU scores, impressive as they are, do not reflect the real qualitative impact of NMT for English–Finnish MT. To explore whether single-reference evaluation underestimates NMT quality, a sample of 68 sentences was extracted from the test set. Both SMT and NMT translations of the sample were postedited with minimal changes to the same quality level as the reference translation. The minimally edited MT was then used as a TER reference to obtain a more reliable estimate of the MT quality. The sample was chosen from sentences where SMT has a sentence-level T"
W17-4733,W16-2322,0,0.0179058,"on the development set, particularly early during training, the BLEU and chrF3 evaluations on development data made us decide against the slower context gates in the final run. 2.4 Ensembling HNMT supports two general modes of ensembling, as well as their combination: Gal and Ghahramani (2016) proposed a method for regularization of recurrent neural networks. This has also been implemented in HNMT, but preliminary experiments on Finnish did not indicate any improvement over the baseline system. While Sennrich et al. (2016a) reported large improvements for the Romanian news translation task at WMT 2016, the amount of training data is lower than what is available for Finnish, which should explain some of the difference. They also apply dropout on the word level, whereas the HNMT application currently only drops recurrent states. 2.3 Forward-Backward reranking 3 English–Finnish In our experiments, we used all English–Finnish parallel data sets provided by WMT except the Wiki headlines, which is a small and rather noisy data set that did not contribute anything in our experiments from last year. We also added substantial amounts of backtranslated data that has Coverage decoder Wu et al. (2016)"
W17-4733,W16-2323,0,0.100318,"ance to tune the balance between adequacy and fluency. While we obtained better cross-entropy on the development set, particularly early during training, the BLEU and chrF3 evaluations on development data made us decide against the slower context gates in the final run. 2.4 Ensembling HNMT supports two general modes of ensembling, as well as their combination: Gal and Ghahramani (2016) proposed a method for regularization of recurrent neural networks. This has also been implemented in HNMT, but preliminary experiments on Finnish did not indicate any improvement over the baseline system. While Sennrich et al. (2016a) reported large improvements for the Romanian news translation task at WMT 2016, the amount of training data is lower than what is available for Finnish, which should explain some of the difference. They also apply dropout on the word level, whereas the HNMT application currently only drops recurrent states. 2.3 Forward-Backward reranking 3 English–Finnish In our experiments, we used all English–Finnish parallel data sets provided by WMT except the Wiki headlines, which is a small and rather noisy data set that did not contribute anything in our experiments from last year. We also added subs"
W17-4733,P16-1009,0,0.106442,"ance to tune the balance between adequacy and fluency. While we obtained better cross-entropy on the development set, particularly early during training, the BLEU and chrF3 evaluations on development data made us decide against the slower context gates in the final run. 2.4 Ensembling HNMT supports two general modes of ensembling, as well as their combination: Gal and Ghahramani (2016) proposed a method for regularization of recurrent neural networks. This has also been implemented in HNMT, but preliminary experiments on Finnish did not indicate any improvement over the baseline system. While Sennrich et al. (2016a) reported large improvements for the Romanian news translation task at WMT 2016, the amount of training data is lower than what is available for Finnish, which should explain some of the difference. They also apply dropout on the word level, whereas the HNMT application currently only drops recurrent states. 2.3 Forward-Backward reranking 3 English–Finnish In our experiments, we used all English–Finnish parallel data sets provided by WMT except the Wiki headlines, which is a small and rather noisy data set that did not contribute anything in our experiments from last year. We also added subs"
W17-4733,P16-1162,0,0.42351,"ance to tune the balance between adequacy and fluency. While we obtained better cross-entropy on the development set, particularly early during training, the BLEU and chrF3 evaluations on development data made us decide against the slower context gates in the final run. 2.4 Ensembling HNMT supports two general modes of ensembling, as well as their combination: Gal and Ghahramani (2016) proposed a method for regularization of recurrent neural networks. This has also been implemented in HNMT, but preliminary experiments on Finnish did not indicate any improvement over the baseline system. While Sennrich et al. (2016a) reported large improvements for the Romanian news translation task at WMT 2016, the amount of training data is lower than what is available for Finnish, which should explain some of the difference. They also apply dropout on the word level, whereas the HNMT application currently only drops recurrent states. 2.3 Forward-Backward reranking 3 English–Finnish In our experiments, we used all English–Finnish parallel data sets provided by WMT except the Wiki headlines, which is a small and rather noisy data set that did not contribute anything in our experiments from last year. We also added subs"
W17-4733,W16-2326,1,0.868028,"Missing"
W17-4733,W15-3021,1,0.848314,"Missing"
W17-4733,Q17-1007,0,0.0227089,"s only works if the different θm are relatively similar, typically because they were saved at different points during the same training process. The overhead for proper ensembling is linear in the number of ensembled systems, both for training (assuming one is building an ensemble of separately trained models) and inference, while parameter averaging is essentially free. HNMT allows proper ensembling of groups of models where the parameters are averaged within each group. This flexible structure allows a number of setups, which are explored further in Section 3.2. Context gates Context gates (Tu et al., 2017) introduce an explicit model for selecting to which extent the target sentence generation should focus on the source sentence or the target context, giving the network a chance to tune the balance between adequacy and fluency. While we obtained better cross-entropy on the development set, particularly early during training, the BLEU and chrF3 evaluations on development data made us decide against the slower context gates in the final run. 2.4 Ensembling HNMT supports two general modes of ensembling, as well as their combination: Gal and Ghahramani (2016) proposed a method for regularization of"
W17-4733,J11-1005,0,0.0160451,"oints at 6, 7.5 and 11.5 days. Translating Chinese into English Chinese is a language without word boundaries, so word segmentation is necessary before using our hybrid encoder with Chinese source sentences. There are different segmentation methods at different granularities, and they will lead to different translations. In the work of Su et al. (2017), they proposed a lattice-based recurrent encoder which applied three segmentations at different granularities (from the CTB, PKU and MSRA corpora). In our model, we just tried two segmentations: One is a fine-grained method implemented in Zpar (Zhang and Clark, 2011), the other is a coarse-granularity method by THULAC (Sun et al., 2016). The model with THULAC segmen6 Conclusions This paper introduces the Helsinki Neural Machine Translation system (HNMT) and its succesful application to the news translation task in WMT 2017. The models we trained handle well the translation into morphologically complex lan345 guages such as Finnish and our submission scored best among the participants in the English–Finnish task. The evaluations show that the neural models are superior to the strong SMT baselines that exploit the same tricks such as backtranslated data and"
W17-4801,D12-1133,0,0.165034,"16 (Guillou et al., 2016), but the differences in the resulting evaluation scores are actually minor. As we have explained above, the shared task focused primarily on subject pronouns. However, in English and German, some pronouns are ambiguous between subject and object position, e.g., the English it and the German es and sie. In order to address this issue, in 2016 we introduced filtering of object pronouns based on dependency parsing. This filtering removed all pronoun instances that did not have a subject dependency label.6 For joint dependency parsing and POS-tagging, we used Mate Tools (Bohnet and Nivre, 2012), with default models. Since in 2016 we found that this filtering was very accurate, this year we performed only automatic filtering for the training and the development, and also for the test datasets. Note that since only subject pronouns can be realized as prodropped pronouns in Spanish, subject filtering was not necessary. 4 Baseline Systems The baseline system is based on an n-gram language model (LM). The architecture is the same as that used for the WMT 2016 cross-lingual pronoun prediction task.7 In 2016, most systems outperformed this baseline, and for the sake of comparison, we thoug"
W17-4801,W16-2350,1,0.857196,"Missing"
W17-4801,W17-4807,1,0.863641,"Missing"
W17-4801,2010.iwslt-papers.10,1,0.907647,"is is hard as selecting the correct pronoun may need discourse analysis as well as linguistic and world knowledge. Null subjects in pro-drop languages pose additional challenges as they express person and number within the verb’s morphology, rendering a subject pronoun or noun phrase redundant. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-au"
W17-4801,W15-2501,1,0.855394,"gradually raised interest in the research community for a shared task that would allow to compare various competing proposals and to quantify the extent to which they improve the translation of different pronouns for different language pairs and different translation directions. However, evaluating pronoun translation comes with its own challenges, as reference-based evaluation, which is standard for machine translation in general, cannot easily take into account legitimate variations of translated pronouns or their placement in the sentence. Thus, building upon experience from DiscoMT 2015 (Hardmeier et al., 2015) and WMT 2016 (Guillou et al., 2016), this year’s cross-lingual pronoun prediction shared task has been designed to test the capacity of the participating systems for translating pronouns correctly, in a framework that allows for objective evaluation, as we will explain below. 2 ce OTHER ce|PRON qui|PRON It ’s an idiotic debate . It has to stop . REPLACE 0 eˆ tre|VER un|DET d´ebat|NOM idiot|ADJ REPLACE 6 devoir|VER stopper|VER .|. 0-0 1-1 2-2 3-4 4-3 6-5 7-6 8-6 9-7 10-8 Figure 2: English→French example from the development dataset. First come the gold class labels, followed by the pronouns (t"
W17-4801,N13-1073,0,0.0460377,"raka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English direction. Another option would have been to create four separate groups of TED talks, one for each subtask. However, we chose the current setup as using a smaller set of documents r"
W17-4801,W17-4806,0,0.0618776,"ial anaphora) or in different sentences (inter-sentential anaphora). Most MT systems translate sentences in isolation, and thus inter-sentential anaphoric pronouns will be translated without knowledge of their antecedent, and thus pronoun-antecedent agreement cannot be guaranteed. NMT yields generally higher-quality translation, but is harder to analyze, and thus little is known about how well it handles pronoun translation. Yet, it is clear that it has access to larger context compared to phrase-based SMT models, potentially spanning multiple sentences, which can improve pronoun translation (Jean et al., 2017a). Motivated by these challenges, the DiscoMT 2017 workshop on Discourse in Machine Translation offered a shared task on cross-lingual pronoun prediction. This was a classification task, asking the participants to make predictions about which pronoun should replace a placeholder in the target-language text. The task required no MT expertise and was designed to be interesting as a machine learning task on its own right, e.g., for researchers working on co-reference resolution. Source Target POS tags Reference The above constraints start playing a role in pronoun translation in situations where"
W17-4801,E12-3001,0,0.0230275,"may need discourse analysis as well as linguistic and world knowledge. Null subjects in pro-drop languages pose additional challenges as they express person and number within the verb’s morphology, rendering a subject pronoun or noun phrase redundant. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the sou"
W17-4801,2005.mtsummit-papers.11,0,0.0869283,"teresting research challenges from the perspective of both speech recognition and machine translation. Therefore, both research communities are making increased use of them in building benchmarks. TED talks address topics of general interest and are delivered to a live public audience whose responses are also audible on the recordings. The talks generally aim to be persuasive and to change the viewers’ behaviour or beliefs. The genre of the TED talks is transcribed planned speech. 3.1.2 Europarl and News For training purposes, in addition to TED talks, we further made available the Europarl3 (Koehn, 2005) and News Commentary4 corpora for all language pairs but Spanish-English, for which only TED talks and Europarl were available. We used the alignments provided by OPUS, including the document boundaries from the original sources. For Europarl, we used ver. 7 of the data release, and for News Commentary we used ver. 9. 3.2 Test Set Selection We selected the test data from talks added recently to the TED repository such that: 1. The talks have been transcribed (in English) and translated into both German and French. 2. They were not used in the IWSLT evaluation campaigns, nor in the DiscoMT 2015"
W17-4801,2005.iwslt-1.8,0,0.121419,"OS tags using pre-defined mappings.5 For French, we clipped the morphosyntactic information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English dir"
W17-4801,W10-1737,0,0.13529,"Missing"
W17-4801,J03-1002,0,0.0103978,"rted the TreeTagger’s POS tags to the target coarse POS tags using pre-defined mappings.5 For French, we clipped the morphosyntactic information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction,"
W17-4801,P14-2050,0,0.026726,"that is aligned to the pronoun to be predicted. All input sequences are fed in an embedding layer followed by two layers of GRUs. The values in the last layer form a vector, which is further concatenated to the pronoun alignment embeddings, to form a larger vector, which is then used to make the final prediction using a dense neural network. The pretraining is a modification of the skip-gram model of WORD 2 VEC (Mikolov et al., 2013), in which along with the skip-gram token context, all target sentence pronouns are predicted as well. The process of pretraining is performed using WORD 2 VECF (Levy and Goldberg, 2014). 5.2 NYU The NYU system (Jean et al., 2017b) uses an attention-based neural machine translation model and three variants that incorporate information from the preceding source sentence. The sentence is added as an auxiliary input using additional encoder and attention models. The systems are not specifically designed for pronoun prediction and may be used to generate complete sentence translations. They are trained exclusively on the data provided for the task, using the text only and ignoring the provided POS tags and alignments. 5.4 UU-Hardmeier The UU- HARDMEIER system (Hardmeier, 2017) is"
W17-4801,petrov-etal-2012-universal,0,0.0455296,"Missing"
W17-4801,W16-2351,1,0.891541,"Missing"
W17-4801,S17-2088,1,0.872336,"Missing"
W17-4801,W16-2202,0,0.0276957,"Missing"
W17-4801,D15-1166,0,0.0502002,"nt. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the targetlanguage lemmata. The aim of the task was to predict, for each target-language pronoun placeholder, the word that should replace it from a small, closed set of cl"
W17-4801,W16-2353,0,0.0406223,"/development portions of the datasets. The additional monolingual news data comprises the shuffled news texts from WMT, including the 2014 editions for German and English, and the 2007– 2013 editions for French. 5 Submitted Systems A total of five teams participated in the shared task, submitting primary systems for all subtasks. Most teams also submitted contrastive systems, which have unofficial status for the purpose of ranking, but are included in the tables of results. 5.1 TurkuNLP The TurkuNLP system (Luotolahti et al., 2017) is an improvement of the last year’s system by the same team (Luotolahti et al., 2016). The improvement mainly consists of a pre-training scheme for vocabulary embeddings based on the task. The system is based on a recurrent neural network based on stacked Gated Recurrent Units (GRUs). The pretraining scheme involves a modification of WORD 2 VEC to use all target sequence pronouns along with typical skip-gram contexts in order to induce embeddings suitable for the task. 6 In 2016, we found that this filtering was too aggressive for German, since it also removed expletives, which had a different tag: EP. Still, we decided to use the same filtering this year, to keep the task sta"
W17-4801,L16-1680,0,0.054481,"Missing"
W17-4801,W17-4808,0,0.0205662,"mata constructed from news texts, parliament debates, and the TED talks of the training/development portions of the datasets. The additional monolingual news data comprises the shuffled news texts from WMT, including the 2014 editions for German and English, and the 2007– 2013 editions for French. 5 Submitted Systems A total of five teams participated in the shared task, submitting primary systems for all subtasks. Most teams also submitted contrastive systems, which have unofficial status for the purpose of ranking, but are included in the tables of results. 5.1 TurkuNLP The TurkuNLP system (Luotolahti et al., 2017) is an improvement of the last year’s system by the same team (Luotolahti et al., 2016). The improvement mainly consists of a pre-training scheme for vocabulary embeddings based on the task. The system is based on a recurrent neural network based on stacked Gated Recurrent Units (GRUs). The pretraining scheme involves a modification of WORD 2 VEC to use all target sequence pronouns along with typical skip-gram contexts in order to induce embeddings suitable for the task. 6 In 2016, we found that this filtering was too aggressive for German, since it also removed expletives, which had a differe"
W17-4801,W16-2355,1,0.796565,"the data is used in each epoch. For the primary system, all classes are sampled equally, as long as there are enough instances for each class. Although this sampling method biases the system towards macro-averaged recall, on the test data the system performed very well in terms of both macro-averaged recall and accuracy. The secondary system uses a sampling method in which the samples are proportional to the class distribution in the development dataset. 5.5 UU-Stymne16 The UU-S TYMNE 16 system uses linear SVM classifiers, and it is the same system that was submitted for the 2016 shared task (Stymne, 2016). It is based mainly on local features, and anaphora is not explicitly modeled. The features used include source pronouns, local context words/lemmata, target POS n-grams with two different POS tagsets, dependency heads of pronouns, alignments, and position of the pronoun. A joint tagger and dependency parser (Bohnet and Nivre, 2012) is used on the source text in order to produce some of the features. Overall, the source pronouns, the local context and the dependency features performed best across all language pairs. 8 7 Stymne (2016) describes several variations of the method, including both"
W17-4801,W17-4805,1,0.882705,"Missing"
W17-4801,S16-1001,1,0.80372,"erforming system here is T URKU NLP with a macro-averaged recall of 58.82. However, it is nearly tied with U PPSALA, and both are somewhat close to NYU. Noteworthy, though, is that the highest-scoring system on macro-average recall is the contrastive system of NYU; NYU also has the second-best accuracy, outperformed only by U PPSALA. Evaluation While in 2015 we used macro-averaged F1 as an official evaluation measure, this year we followed the setup of 2016, where we switched to macroaveraged recall, which was also recently adopted by some other competitions, e.g., by SemEval2016/2017 Task 4 (Nakov et al., 2016; Rosenthal et al., 2017). Moreover, as in 2015 and 2016, we also report accuracy as a secondary evaluation measure (but we abandon F1 altogether). Macro-averaged recall ranges in [0, 1], where a value of 1 is achieved by the perfect classifier,8 and a value of 0 is achieved by the classifier that misclassifies all examples. The value of 1/C, where C is the number of classes, is achieved by a trivial classifier that assigns the same class to all examples (regardless of which class is chosen), and is also the expected value of a random classifier. The advantage of macro-averaged recall over acc"
W17-4801,C96-2141,0,0.453266,"c information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English direction. Another option would have been to create four separate groups of TED t"
W17-4811,P13-2068,0,0.0608001,"Missing"
W17-4811,W09-2404,0,0.337422,"Missing"
W17-4811,W12-3156,0,0.0476943,"context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases. 1 Introduction Typical models of machine translation handle sentences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hard"
W17-4811,W10-1737,0,0.0691336,"tion consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Modest improvements could, for example, be shown for the translation of pronouns (Le Nagard and Koehn, 2010; Hardmeier and Fed2 Attention-Based NMT Encoder-decoder models with attention have been proposed by Bahdanau et al. (2014) and have become the de-facto standard in neural machine translation. The model is based on recurrent neural network layers that encode a given sentence in the source language into a distributed vector representation that will be decoded into the target language by another recurrent network. The attention 82 Proceedings of the Third Workshop on Discourse in Machine Translation, pages 82–92, c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Lingui"
W17-4811,L16-1147,1,0.8448,"ed to other genres such as newspaper texts or other edited written material. Utterances are even shortened substantially for space limitations. This property supports our experiments in which we want to include context beyond sentence boundaries. Similar to statistical MT, neural MT also struggles most with long sequences and, therefore, it is important to keep the segments short. On average there are about 8 tokens per language in each aligned translation unit (which may cover one or more sentences or sentence fragments). In particular, we use the publicly available OpenSubtitles2016 corpus (Lison and Tiedemann, 2016) for German and English2 and reserve 400 randomly selected movies for development and testing purposes. In total, there are 16,910 movies and TV series in the collection. We tokenized and truecased the data sets using standard tools from the Moses toolbox (Koehn et al., 2007). The final corpus comprises 13.9 million translation units with about 107 million tokens in German and 115 million tokens in English. The training data includes 13.5 million training instances and we selected the 5,000 first translation units of the test set for automatic evaluation. Note that we trust the alignment and d"
W17-4811,E12-3001,0,0.0786392,"s that improve textual coherence in translation at least in some selected cases. 1 Introduction Typical models of machine translation handle sentences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Modest improvements could, for example, be shown for the translation of pronouns (Le Nagard and Koehn, 2010; Hardmeier and Fed2 Attention-Based NMT Encoder-decoder models with attention have been proposed by Bahdanau et al. (2014) and h"
W17-4811,loaiciga-etal-2014-english,0,0.0176773,"tences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Modest improvements could, for example, be shown for the translation of pronouns (Le Nagard and Koehn, 2010; Hardmeier and Fed2 Attention-Based NMT Encoder-decoder models with attention have been proposed by Bahdanau et al. (2014) and have become the de-facto standard in neural machine translation. The model is based on recurrent neural network layers that encode a given sentence in the"
W17-4811,2010.iwslt-papers.10,0,0.54578,"Missing"
W17-4811,P16-1100,0,0.0245296,"out limiting the capacity of the internal representation. Previous work has shown that NMT models can successfully learn attention distributions that explain intuitively plausible connections between source and target language. This framework is very well suited for the study we conduct in this paper as we emphasise the capabilities of NMT to pick up contextual dependencies from wider context across sentence boundaries. In our work, we rely on the freely avail¨ able Helsinki NMT system (HNMT) (Ostling 1 et al., 2017) that implements a hybrid bidirectional encoder with character-level backoff (Luong and Manning, 2016) using recurrent LSTM units (Hochreiter and Schmidhuber, 1997). The system also features layer normalisation (Ba et al., 2016), variational dropout (Gal and Ghahramani, 2016), coverage penalties (Wu et al., 2016), beam search decoding and straightforward model ensembling. The backbone is Theano, which enables efficient GPU-based training and decoding with mini-batches. 3 least helps to incorporate more knowledge about the situation and in consequence leads to better translations, also stylistically. The final advantage of subtitles is the size of the translation units. Sentences (and sentence"
W17-4811,P13-4033,1,0.867193,"ual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases. 1 Introduction Typical models of machine translation handle sentences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Mo"
W17-4811,P11-1124,0,0.0626374,"Missing"
W17-4811,D13-1037,1,0.875891,"ual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases. 1 Introduction Typical models of machine translation handle sentences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Mo"
W17-4811,2012.amta-papers.20,0,0.0668273,"Missing"
W17-4811,W17-4733,1,0.86296,"Missing"
W17-4811,E12-3010,0,0.183068,"Missing"
W17-4811,P16-1162,0,0.318378,"in learning the influence of specific context word sequences on the translation of the focus sentence. An example is the ambiguous pronoun “sie” that could be a feminine singular or a plural third person pronoun. The use of grammatical gender in German also makes it possible to refer to an inanimate antecedent. Discourse-level information is needed to make correct decisions. The question is whether our model can actually pick this up and whether attention patterns can show the relevant connections. The NMT models that we train rely on subwordunits. We apply standard byte-pair encoding (BPE) (Sennrich et al., 2016) for splitting words into segments. For the extended source context models, we set a vocabulary size of 30,000 when training BPE codes and apply a vocabulary size of 60,000 when training the models (context words double the vocabulary because of their cc prefix). For the 2+2 model, we train BPE codes from both languages together (with a size of 60,000) and we set a vocabulary threshold of 50 when applying BPE to the data. 5 Experiments and Results We train attention-based models using the Helsinki NMT system with similar parameters but different training data to see the effect of contextual in"
W17-4811,W10-2602,1,0.922449,"Missing"
W17-4811,N12-1046,0,0.102001,"Missing"
W17-4811,W12-2503,0,0.0453393,"translation at least in some selected cases. 1 Introduction Typical models of machine translation handle sentences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Modest improvements could, for example, be shown for the translation of pronouns (Le Nagard and Koehn, 2010; Hardmeier and Fed2 Attention-Based NMT Encoder-decoder models with attention have been proposed by Bahdanau et al. (2014) and have become the de-facto standard in neural mac"
W17-4811,D13-1163,0,0.0812776,"ome selected cases. 1 Introduction Typical models of machine translation handle sentences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Modest improvements could, for example, be shown for the translation of pronouns (Le Nagard and Koehn, 2010; Hardmeier and Fed2 Attention-Based NMT Encoder-decoder models with attention have been proposed by Bahdanau et al. (2014) and have become the de-facto standard in neural machine translation. Th"
W17-4811,P07-2045,0,\N,Missing
W18-3901,W18-3913,0,0.217545,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3919,0,0.254393,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3932,0,0.129693,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W17-1223,0,0.131933,"data to reduce the transcriber effects seen last year. 7 • The LaMa system is a blend (weighted vote) of eight classifiers being stochastic gradient descent (hinge and modified Huber), multinomial Na¨ıve Bayes, both counts and tf-idf, FastText, and modified Kneser-Ney smoothing. The classifiers were trained using word n-grams (1-6) and character n-grams (1-8). The hyperparameters were determined with cross-validation and searching on the development set. • XAC system is a refined version of the n-gram-based Bayesline system described in last year’s XAC submission to the VarDial shared tasks (Barbaresi, 2017), and previously used as a baseline for the DSL shared task (Tan et al., 2014). The XAC team achieved their best results using a Na¨ıve Bayes classifier. • The GDI classification system is based on an ensemble of multiple SVM classifiers. The system was trained on various word- and character-level features. • The dkosmajac system is based on a normalized Euclidean distance measure. The distances are calculated between a sample and each class profile. The class profiles are generated by selecting the most frequent features for each class, which results in profiles that are of the same length fo"
W18-3901,W18-3918,0,0.0590619,"Missing"
W18-3901,W18-3925,0,0.142653,"Missing"
W18-3901,W17-1214,0,0.138486,"was part of the first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions gen"
W18-3901,W18-3909,0,0.15091,"Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive evaluation exercise with four shared tasks in 2017. This year, the VarDial workshop featured the second edition of the VarDial evaluation campaign w"
W18-3901,W18-3933,1,0.889149,"Missing"
W18-3901,W18-3920,1,0.880795,"Missing"
W18-3901,W18-3926,0,0.0553879,"Missing"
W18-3901,W18-3921,0,0.054375,"Missing"
W18-3901,W17-1225,0,0.134424,"r of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layers, which represents the prob"
W18-3901,W16-4818,0,0.048944,", duration (Dur.), in number of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layer"
W18-3901,W18-3915,0,0.0733204,"Missing"
W18-3901,W18-3929,0,0.411409,"Missing"
W18-3901,W18-3907,0,0.178349,"Missing"
W18-3901,W18-3922,0,0.0589219,"Missing"
W18-3901,W18-3928,0,0.0556119,"Missing"
W18-3901,kumar-2012-challenges,1,0.824307,"ed printed stories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Osl"
W18-3901,kumar-2014-developing,1,0.832345,"ories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Oslo team submit"
W18-3901,W14-0405,1,0.912975,"Missing"
W18-3901,W18-3917,1,0.897664,"Missing"
W18-3901,L16-1242,1,0.902855,"Missing"
W18-3901,L16-1676,1,0.914666,"Missing"
W18-3901,W16-4814,1,0.86288,"-Oslo system (C ¸ o¨ ltekin et al., 2018) is trained on word and character n-grams using a single SVM classifier, which is fine-tuned using cross-validation. It is similar to the submissions by the same authors to previous VarDial shared tasks (C¸o¨ ltekin and Rama, 2017; C¸o¨ ltekin and Rama, 2016). They also tried an approach based on RNN, which worked worse. • Arabic Identification system is based on an ensemble of SVM classifiers trained on character and word n-grams. The approach is similar to the systems ranked second and first in the previous two ADI tasks (Malmasi and Zampieri, 2017a; Malmasi and Zampieri, 2016). 5 5.3 Results Six teams submitted runs for the ADI shared task and the results are shown in Table 3. The best result, an F1 score of 0.589, was achieved by UnibucKernel,1 followed by safina, with an F1 score of 0.575. The following three teams are tied for the third place as they are not statistically different. Rank 1 2 3 3 3 4 Team F1 (Macro) UnibucKernel safina BZU SYSTRAN T¨ubingen-Oslo Arabic Identification 0.589 0.576 0.534 0.529 0.514 0.500 Table 3: ADI results: ranked taking statistical significance into account. 5.4 Summary We introduced multi-phoneme representation for the dialecta"
W18-3901,W17-1222,1,0.833986,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W17-1220,1,0.80558,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W16-4801,1,0.733081,"Missing"
W18-3901,W18-3927,0,0.0554913,"Missing"
W18-3901,W18-3914,0,0.175754,"ES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive eva"
W18-3901,W18-3924,0,0.0833746,"and the number of submissions varied widely across the tasks, ranging from 6 entries for ADI and MTT to 12 entries for DFS. Table 1 lists the participating teams, the shared tasks they took part in, and a reference to the system description paper. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participate"
W18-3901,L16-1641,1,0.509642,"Missing"
W18-3901,W17-1224,1,0.744067,"Missing"
W18-3901,W18-3923,1,0.879634,"Missing"
W18-3901,W14-5307,1,0.857122,"Missing"
W18-3901,W15-5401,1,0.855743,"Missing"
W18-3901,W17-1201,1,0.607978,"Missing"
W18-4510,W13-2711,0,0.222638,"work and MorphAdorner does not provide enough coverage for our data. This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 87 Proceedings of Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 87–96 Santa Fe, New Mexico, USA, August 25, 2018. Using a string similarity metric such as edit distance has been used in the past for normalization. An example of such is the automatically produced diachronic dictionary of spelling variants for German (Amoia and Martinez, 2013). In building the dictionary, the authors used Levenshtein edit distance to cluster similar words together with their modern counterparts. This was facilitated by the fact that they were dealing with recipes, which thanks to their limited vocabulary, are easy to cluster. In addition, words can be clustered based on their semantics by looking at the shared contexts of the words. Statistical machine translation models have also been used in solving the task by training a character based translation model using known historical spellings and their modern variants as training data. In (Samardzic e"
W18-4510,C16-1013,0,0.130993,"In addition, words can be clustered based on their semantics by looking at the shared contexts of the words. Statistical machine translation models have also been used in solving the task by training a character based translation model using known historical spellings and their modern variants as training data. In (Samardzic et al., 2015) such a model was trained for normalizing Swiss German dialects to a standard variant. A similar SMT based approach has also been used in the context of historical text in (Pettersson et al., 2013). Normalization has also been done by using deep learning. In (Bollmann and Søgaard, 2016), normalization is presented as a character-based sequence labeling task for bi-directional LSTMs. In their method a historical character does not need to be aligned with a single modern spelling character but can be aligned with a compound of characters in the training data. 3 The Corpus and Data Sources The primary corpus we use is the CEEC (Nevalainen et al., 1998 2006). Compiled for the purposes of historical sociolinguistics, it is a corpus of personal correspondence in English representing a time span from the 15th to the 19th century. The letters have been selected from published origin"
W18-4510,P13-2121,0,0.0182096,"er to make the SMT tool, Moses (Koehn et al., 2007), treat individual characters of a word as though they were words of a sentence. The parallel non-normalized to normalized word lists are aligned with GIZA++ (Och and Ney, 2003) as part of the machine translation process. An SMT based system also requires a language model. Without it, the system would be more likely to produce non-words as output. As a language model, we use the list of words extracted from the BNC. Again, these words are split into characters by whitespaces. We build a 10-gram language model based on the BNC data with KenLM (Heafield et al., 2013) and use this model with Moses3 . For tuning the model, we take a random sample of 2000 non-normalized and normalized word pairs from our parallel data set and run the tuning on that. We also tune century specific models with 2000 words from the era for the 15th and the 18th century to compare whether tuning for a given century yields better results for that century than a more generally tuned model. 4.4 Neural Machine Translation (NMT) Neural machine translation can be used for normalization in a similar fashion to the SMT approach. We use OpenNMT (Klein et al., 2017) to train against a chara"
W18-4510,P17-4012,0,0.0454911,"BNC data with KenLM (Heafield et al., 2013) and use this model with Moses3 . For tuning the model, we take a random sample of 2000 non-normalized and normalized word pairs from our parallel data set and run the tuning on that. We also tune century specific models with 2000 words from the era for the 15th and the 18th century to compare whether tuning for a given century yields better results for that century than a more generally tuned model. 4.4 Neural Machine Translation (NMT) Neural machine translation can be used for normalization in a similar fashion to the SMT approach. We use OpenNMT (Klein et al., 2017) to train against a character based machine translation model by using the parallel data extracted earlier. For validation of the model, we use the same 2000 word parallel list as we did for the general SMT model. For NMT, we train two different models for 13 epochs. The first model gets the same input as SMT, i.e. a parallel list of word forms. The second model has a specific input for the centuries in which certain non-normalized forms were used together with the actual word forms. This is done simply by appending the year of the century before each historical form in the data set. For some"
W18-4510,P07-2045,0,0.00747958,"orthography and pronunciation are not always clearly connected, we use Soundex2 with the size of 6 to produce an estimated pronunciation. 4.3 Statistical Machine Translation (SMT) Previous research has shown that SMT is a viable form of solving the problem of normalization. This is why we have decided to include an SMT based approach as a module in our system. We train a character based SMT model using the parallel data extracted earlier from the OED, MED and known normalizations in the CEEC. All the words are split into letters separated by a whitespace in order to make the SMT tool, Moses (Koehn et al., 2007), treat individual characters of a word as though they were words of a sentence. The parallel non-normalized to normalized word lists are aligned with GIZA++ (Och and Ney, 2003) as part of the machine translation process. An SMT based system also requires a language model. Without it, the system would be more likely to produce non-words as output. As a language model, we use the list of words extracted from the BNC. Again, these words are split into characters by whitespaces. We build a 10-gram language model based on the BNC data with KenLM (Heafield et al., 2013) and use this model with Mose"
W18-4510,J03-1002,0,0.024092,"T) Previous research has shown that SMT is a viable form of solving the problem of normalization. This is why we have decided to include an SMT based approach as a module in our system. We train a character based SMT model using the parallel data extracted earlier from the OED, MED and known normalizations in the CEEC. All the words are split into letters separated by a whitespace in order to make the SMT tool, Moses (Koehn et al., 2007), treat individual characters of a word as though they were words of a sentence. The parallel non-normalized to normalized word lists are aligned with GIZA++ (Och and Ney, 2003) as part of the machine translation process. An SMT based system also requires a language model. Without it, the system would be more likely to produce non-words as output. As a language model, we use the list of words extracted from the BNC. Again, these words are split into characters by whitespaces. We build a 10-gram language model based on the BNC data with KenLM (Heafield et al., 2013) and use this model with Moses3 . For tuning the model, we take a random sample of 2000 non-normalized and normalized word pairs from our parallel data set and run the tuning on that. We also tune century s"
W18-5431,E17-2039,0,0.0261532,"Missing"
W18-5431,W17-4711,0,0.0313364,"017a,b). Among the different architectures, the Transformer (Vaswani et al., 2017) has emerged as the dominant NMT paradigm.1 Relying only on attention mechanisms, the model is fast, highly accurate and has been proven to outperform the widely used recurrent networks with attention and ensembling (Wu et al., 2016) by more than 2 BLEU points. Improved translation quality is typically related to better representation of structural information. While other approaches make use of external information to improve the internal representation of NMT models (Arthur et al., 2016; Niehues and Cho, 2017; Alkhouli and Ney, 2017), the Transformer seems to be able to encode a lot of structural information without explicitly incorporating any structural constraints. However, being a rather new architecture, little is known about what the model exactly learns internally. A better understanding of the internal representations of neural models has become a major challenge in NMT (Koehn and Knowles, 2017). In this work we investigate the kind of linguistic information that is learned by the encoder. We start by training the Transformer system from English to seven languages, with different training set sizes, resulting in m"
W18-5431,E17-1022,0,0.0375763,"Missing"
W18-5431,D16-1162,0,0.0235999,"pply convolution networks (Gehring et al., 2017a,b). Among the different architectures, the Transformer (Vaswani et al., 2017) has emerged as the dominant NMT paradigm.1 Relying only on attention mechanisms, the model is fast, highly accurate and has been proven to outperform the widely used recurrent networks with attention and ensembling (Wu et al., 2016) by more than 2 BLEU points. Improved translation quality is typically related to better representation of structural information. While other approaches make use of external information to improve the internal representation of NMT models (Arthur et al., 2016; Niehues and Cho, 2017; Alkhouli and Ney, 2017), the Transformer seems to be able to encode a lot of structural information without explicitly incorporating any structural constraints. However, being a rather new architecture, little is known about what the model exactly learns internally. A better understanding of the internal representations of neural models has become a major challenge in NMT (Koehn and Knowles, 2017). In this work we investigate the kind of linguistic information that is learned by the encoder. We start by training the Transformer system from English to seven languages, w"
W18-5431,P17-1080,0,0.166264,"ing and Interpreting Neural Networks for NLP, pages 287–297 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics in order to find linguistic patterns. As the next step, we exploit the attention weights of the network to build a graph and induce tree structures for each sentence, showing whether syntactic dependencies between words have been learned or not in the spirit of Williams et al. (2018) and Liu and Lapata (2018). Additionally, following previous studies on how to analyze the internal representation of neural systems (Adi et al., 2016; Shi et al., 2016; Belinkov et al., 2017a), we probe the encoder weights of the trained models to address different sequence labeling tasks: Part-of-Speech tagging, Chunking, Named Entity Recognition and Semantic tagging. We evaluate the quality of the decoder on a given task to assess how discriminative the encoder representation is for that task. Lastly, in order to check whether the learned information can be transferred across models, we use the encoder weights of a high-resource language pair to initialize a low-resource language pair, inspired by the work of Zoph et al. (2016). We show that, also for the Transformer, the knowl"
W18-5431,I17-1001,0,0.0887117,"Missing"
W18-5431,D16-1025,0,0.0216254,"and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics. 1 Introduction Machine translation (MT) is one of the prominent tasks in Natural Language Processing, tackled in several ways (Bojar et al., 2017). Neural MT (NMT) has become the de-facto standard with a performance that clearly outperforms the alternative approach of Statistical Machine Translation (Luong et al., 2015b; Bojar et al., 2016; Bentivogli et al., 2016). NMT also improves training procedures due to the end-to-end fashion without tedious feature engineering and complex setups. During recent years, a lot of research has been done on NMT, designing new architectures, starting from the plain sequence-to-sequence model (Sutskever et al., 2014; Cho et al., 2014), to an improved version featuring an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a), to models that only use attention instead of recurrent layers (Vaswani et al., 2017) and models that apply convolution networks (Gehring et al., 2017a,b). Among the different architecture"
W18-5431,D14-1179,0,0.0727367,"Missing"
W18-5431,P18-1198,0,0.0658917,"Missing"
W18-5431,P17-1106,0,0.0538343,"tectures being published continuously each year. One of the first techniques to examine a neural network involves the analysis of activation patterns of the hidden layers (Elman, 1991; Giles et al., 1992). Nowadays, given its popularity, recurrent neural networks are the most evaluated networks, mainly investigated on the structures and linguistic properties they are encoding (Linzen et al., 2016; Enguehard et al., 2017; Kuncoro et al., 2017; Gulordava et al., 2018). Traditionally, a common way to inspect neural networks is by visualizing the hidden representation trained for a specific task (Ding et al., 2017; Strobelt et al., 2018a,b), and to evaluate them by assessing the properties through downstream tasks (Chung et al., 2014; Greff et al., 2017). Other recent studies look for hidden linguistic units that provide information on how the network works (Karpathy et al., 2015; Qian et al., 2016; K´ad´ar et al., 2017), while another line of analysis probes the representation learned by a neural network as input to a classifier of another task (Shi et al., 2016; Adi et al., 2016; Belinkov et al., 2017a; Tran et al., 2018). The most closely related work is by Belinkov et al. (2017b), in which they inv"
W18-5431,K17-3002,0,0.0155998,"the discovered patterns through visualization for the sentence: ”there is also an economic motive .”. Sample tree from the attention head 1, layer 3 Sample tree from the attention head 4, layer 3 Figure 3: Sample trees induced by the attention weights from the English-Czech model. tree algorithm. Specifically, we run the Chu-LiuEdmonds algorithm (Chu, 1965; Edmonds, 1967) for each attention head of each layer of the models to extract the maximum spanning trees. Table 3 shows the F1-score of the induced structures. For comparison purposes, in this dataset, a state of the art supervised parser (Dozat et al., 2017) reaches 88.22 UAS F1-score and our random baseline, i.e., induced trees with random weights and gold root, achieves 10.1 UAS F1-score on average.9 Given our findings in Section 5, we also computed a leftand right- branching baseline (with golden root), obtaining 10.39 and 35.08 UAS F1-score respectively. Although our models are not trained to produce trees, the best dependency trees induced on each layer are far better than the random baseline, suggesting that the models are learning some syntactic relationships. However, the best scores do not achieve results much beyond the right branching"
W18-5431,K17-1003,0,0.0254599,"n recurrent networks (Zoph et al., 2016). 9 Related Work The problem of interpreting and understanding neural networks is attracting more and more interest and work, with so many models and new architectures being published continuously each year. One of the first techniques to examine a neural network involves the analysis of activation patterns of the hidden layers (Elman, 1991; Giles et al., 1992). Nowadays, given its popularity, recurrent neural networks are the most evaluated networks, mainly investigated on the structures and linguistic properties they are encoding (Linzen et al., 2016; Enguehard et al., 2017; Kuncoro et al., 2017; Gulordava et al., 2018). Traditionally, a common way to inspect neural networks is by visualizing the hidden representation trained for a specific task (Ding et al., 2017; Strobelt et al., 2018a,b), and to evaluate them by assessing the properties through downstream tasks (Chung et al., 2014; Greff et al., 2017). Other recent studies look for hidden linguistic units that provide information on how the network works (Karpathy et al., 2015; Qian et al., 2016; K´ad´ar et al., 2017), while another line of analysis probes the representation learned by a neural network as inp"
W18-5431,P17-1012,0,0.0219656,"t al., 2015b; Bojar et al., 2016; Bentivogli et al., 2016). NMT also improves training procedures due to the end-to-end fashion without tedious feature engineering and complex setups. During recent years, a lot of research has been done on NMT, designing new architectures, starting from the plain sequence-to-sequence model (Sutskever et al., 2014; Cho et al., 2014), to an improved version featuring an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a), to models that only use attention instead of recurrent layers (Vaswani et al., 2017) and models that apply convolution networks (Gehring et al., 2017a,b). Among the different architectures, the Transformer (Vaswani et al., 2017) has emerged as the dominant NMT paradigm.1 Relying only on attention mechanisms, the model is fast, highly accurate and has been proven to outperform the widely used recurrent networks with attention and ensembling (Wu et al., 2016) by more than 2 BLEU points. Improved translation quality is typically related to better representation of structural information. While other approaches make use of external information to improve the internal representation of NMT models (Arthur et al., 2016; Niehues and Cho, 2017; Alk"
W18-5431,N18-1108,0,0.0205949,"lated Work The problem of interpreting and understanding neural networks is attracting more and more interest and work, with so many models and new architectures being published continuously each year. One of the first techniques to examine a neural network involves the analysis of activation patterns of the hidden layers (Elman, 1991; Giles et al., 1992). Nowadays, given its popularity, recurrent neural networks are the most evaluated networks, mainly investigated on the structures and linguistic properties they are encoding (Linzen et al., 2016; Enguehard et al., 2017; Kuncoro et al., 2017; Gulordava et al., 2018). Traditionally, a common way to inspect neural networks is by visualizing the hidden representation trained for a specific task (Ding et al., 2017; Strobelt et al., 2018a,b), and to evaluate them by assessing the properties through downstream tasks (Chung et al., 2014; Greff et al., 2017). Other recent studies look for hidden linguistic units that provide information on how the network works (Karpathy et al., 2015; Qian et al., 2016; K´ad´ar et al., 2017), while another line of analysis probes the representation learned by a neural network as input to a classifier of another task (Shi et al.,"
W18-5431,Q18-1017,0,0.015075,"test2018 test data for the transfer learning experiment. average sentence length is very short and so it is easier to predict. Overall, comparing the performance reached on these probing tasks with the BLEU score of each model, we can see again that the high resource language pairs achieve better results compared to our low resource language pair. Moreover, we notice that in general higher BLEU score correspond to higher probing results, confirming the trend that encoding linguistic proprieties within the encoder representation go on par with better translation quality (Niehues and Cho, 2017; Kiperwasser and Ballesteros, 2018). 8 Encoder Evaluation: Transfer learning To assess whether the knowledge encoded in the attention units can help other models in a low resource scenario, we additionally carried out an evaluation of the encoder representation in a transfer learning task. Similar to Zoph et al. (2016), we used the encoder weights from one high resource language, i.e., English-German, to train a Transformer system for our low resource language pair, English-Turkish. We provide two experiments: i) initializing and fine tuning the encoder weights (TL1), ii) initializing and keeping the encoder weights fixed (TL2)"
W18-5431,P17-4012,0,0.0266942,"nt of data, ranging from 200K for Turkish up to 51M for Czech. We trained each model for maximum 20 epochs, taking the best one according to the development set as model to evaluate. The BLEU score7 of each model is shown in Table 2. Even though the scores seem low for the Transformer architecture for the MT task, we have to note that each model is trained using full word forms in order to facilitate the analysis of the encoder representation (our results are in line with the comparison between subword units and full word forms done by Sennrich et al. (2016)). 3 We used the OpenNMT framework (Klein et al., 2017). The provided data are already preprocessed and freely available at http://data.statmt.org/wmt18/ translation-task/preprocessed/. 5 https://paracrawl.eu/ 6 We used the fasttext language identifier tool (Joulin et al., 2016b,a) from https://fasttext.cc/docs/ en/language-identification.html 7 We used the SACRE BLEU script (Post, 2018), with signature BLEU+case.mixed+lang.en{targetLanguage}+numrefs.1+smooth.exp+test.wmt{17,18}+ tok.13a+version.1.3.0 4 We do not aim at beating the best system on the test data, as our main point is to analyze different encoder representations across models with di"
W18-5431,2005.mtsummit-papers.11,0,0.00809701,"nsfer learning scenario in which we use the encoder representation of a high resource language pair to initialize the encoder of a low resource language pair. Here, we assume that a model is better at encoding abstract linguistic properties if it can share useful information to enhance another weaker model. 4 Model setup We trained Transformer models3 from English to seven languages, Czech, German, Estonian, Finnish, Russian, Turkish and Chinese, using the parallel data provided by the WMT18 shared task on news translation.4 The parallel data come from different sources, mainly from Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and ParaCrawl.5 The data sets are partially noisy, especially ParaCrawl being on its first release, and to filter out potentially incorrect parallel sentences we used a language identifier6 to tag each source and target sentence, discarding the sentences that do not match across languages (Stymne et al., 2013; Zarin¸a et al., 2015). As development set we used the provided newsdev data from the shared task, while using the newstest from WMT 2017 and 2018 as test data. A widely used technique to allow an open vocabulary is byte pair encoding (Sennrich et al.,"
W18-5431,W17-3204,0,0.0172639,"typically related to better representation of structural information. While other approaches make use of external information to improve the internal representation of NMT models (Arthur et al., 2016; Niehues and Cho, 2017; Alkhouli and Ney, 2017), the Transformer seems to be able to encode a lot of structural information without explicitly incorporating any structural constraints. However, being a rather new architecture, little is known about what the model exactly learns internally. A better understanding of the internal representations of neural models has become a major challenge in NMT (Koehn and Knowles, 2017). In this work we investigate the kind of linguistic information that is learned by the encoder. We start by training the Transformer system from English to seven languages, with different training set sizes, resulting in models that are not only trained for different target languages but also with expected differences in translation quality. First, we visually inspect the attention weights of the encoders, 1 Most submissions for the WMT18 shared task on news (http://matrix.statmt.org/) employ the Transformer architecture. 287 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and I"
W18-5431,E17-1117,0,0.0537135,"Missing"
W18-5431,Q16-1037,0,0.0495798,"g previous findings on recurrent networks (Zoph et al., 2016). 9 Related Work The problem of interpreting and understanding neural networks is attracting more and more interest and work, with so many models and new architectures being published continuously each year. One of the first techniques to examine a neural network involves the analysis of activation patterns of the hidden layers (Elman, 1991; Giles et al., 1992). Nowadays, given its popularity, recurrent neural networks are the most evaluated networks, mainly investigated on the structures and linguistic properties they are encoding (Linzen et al., 2016; Enguehard et al., 2017; Kuncoro et al., 2017; Gulordava et al., 2018). Traditionally, a common way to inspect neural networks is by visualizing the hidden representation trained for a specific task (Ding et al., 2017; Strobelt et al., 2018a,b), and to evaluate them by assessing the properties through downstream tasks (Chung et al., 2014; Greff et al., 2017). Other recent studies look for hidden linguistic units that provide information on how the network works (Karpathy et al., 2015; Qian et al., 2016; K´ad´ar et al., 2017), while another line of analysis probes the representation learned by"
W18-5431,D15-1166,0,0.0716807,"is sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics. 1 Introduction Machine translation (MT) is one of the prominent tasks in Natural Language Processing, tackled in several ways (Bojar et al., 2017). Neural MT (NMT) has become the de-facto standard with a performance that clearly outperforms the alternative approach of Statistical Machine Translation (Luong et al., 2015b; Bojar et al., 2016; Bentivogli et al., 2016). NMT also improves training procedures due to the end-to-end fashion without tedious feature engineering and complex setups. During recent years, a lot of research has been done on NMT, designing new architectures, starting from the plain sequence-to-sequence model (Sutskever et al., 2014; Cho et al., 2014), to an improved version featuring an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a), to models that only use attention instead of recurrent layers (Vaswani et al., 2017) and models that apply convolution networks (Gehring et"
W18-5431,P15-1002,0,0.0145517,"is sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics. 1 Introduction Machine translation (MT) is one of the prominent tasks in Natural Language Processing, tackled in several ways (Bojar et al., 2017). Neural MT (NMT) has become the de-facto standard with a performance that clearly outperforms the alternative approach of Statistical Machine Translation (Luong et al., 2015b; Bojar et al., 2016; Bentivogli et al., 2016). NMT also improves training procedures due to the end-to-end fashion without tedious feature engineering and complex setups. During recent years, a lot of research has been done on NMT, designing new architectures, starting from the plain sequence-to-sequence model (Sutskever et al., 2014; Cho et al., 2014), to an improved version featuring an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a), to models that only use attention instead of recurrent layers (Vaswani et al., 2017) and models that apply convolution networks (Gehring et"
W18-5431,W17-4708,0,0.134556,"orks (Gehring et al., 2017a,b). Among the different architectures, the Transformer (Vaswani et al., 2017) has emerged as the dominant NMT paradigm.1 Relying only on attention mechanisms, the model is fast, highly accurate and has been proven to outperform the widely used recurrent networks with attention and ensembling (Wu et al., 2016) by more than 2 BLEU points. Improved translation quality is typically related to better representation of structural information. While other approaches make use of external information to improve the internal representation of NMT models (Arthur et al., 2016; Niehues and Cho, 2017; Alkhouli and Ney, 2017), the Transformer seems to be able to encode a lot of structural information without explicitly incorporating any structural constraints. However, being a rather new architecture, little is known about what the model exactly learns internally. A better understanding of the internal representations of neural models has become a major challenge in NMT (Koehn and Knowles, 2017). In this work we investigate the kind of linguistic information that is learned by the encoder. We start by training the Transformer system from English to seven languages, with different training"
W18-5431,W18-6319,0,0.0297819,"Missing"
W18-5431,D16-1079,0,0.0790581,"Missing"
W18-5431,E17-2060,0,0.061544,"Missing"
W18-5431,P16-1162,0,0.136664,"arl (Koehn, 2005), News Commentary (Tiedemann, 2012) and ParaCrawl.5 The data sets are partially noisy, especially ParaCrawl being on its first release, and to filter out potentially incorrect parallel sentences we used a language identifier6 to tag each source and target sentence, discarding the sentences that do not match across languages (Stymne et al., 2013; Zarin¸a et al., 2015). As development set we used the provided newsdev data from the shared task, while using the newstest from WMT 2017 and 2018 as test data. A widely used technique to allow an open vocabulary is byte pair encoding (Sennrich et al., 2016), in which the source and target words are split into subword units. However, in this work we prefer to use the full word forms, allowing us to evaluate and compare the internal representation on standard sequence labeling benchmarks tagged with gold labels on the full word forms. Therefore, we use a large vocabulary of 100K words per language. General statistics on the training data are given in Table 1. As can be seen, we ended up having an heterogeneous amount of data, ranging from 200K for Turkish up to 51M for Czech. We trained each model for maximum 20 epochs, taking the best one accordi"
W18-5431,D16-1159,0,0.373624,"lackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 287–297 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics in order to find linguistic patterns. As the next step, we exploit the attention weights of the network to build a graph and induce tree structures for each sentence, showing whether syntactic dependencies between words have been learned or not in the spirit of Williams et al. (2018) and Liu and Lapata (2018). Additionally, following previous studies on how to analyze the internal representation of neural systems (Adi et al., 2016; Shi et al., 2016; Belinkov et al., 2017a), we probe the encoder weights of the trained models to address different sequence labeling tasks: Part-of-Speech tagging, Chunking, Named Entity Recognition and Semantic tagging. We evaluate the quality of the decoder on a given task to assess how discriminative the encoder representation is for that task. Lastly, in order to check whether the learned information can be transferred across models, we use the encoder weights of a high-resource language pair to initialize a low-resource language pair, inspired by the work of Zoph et al. (2016). We show that, also for the"
W18-5431,W18-5451,0,0.0744912,"ished continuously each year. One of the first techniques to examine a neural network involves the analysis of activation patterns of the hidden layers (Elman, 1991; Giles et al., 1992). Nowadays, given its popularity, recurrent neural networks are the most evaluated networks, mainly investigated on the structures and linguistic properties they are encoding (Linzen et al., 2016; Enguehard et al., 2017; Kuncoro et al., 2017; Gulordava et al., 2018). Traditionally, a common way to inspect neural networks is by visualizing the hidden representation trained for a specific task (Ding et al., 2017; Strobelt et al., 2018a,b), and to evaluate them by assessing the properties through downstream tasks (Chung et al., 2014; Greff et al., 2017). Other recent studies look for hidden linguistic units that provide information on how the network works (Karpathy et al., 2015; Qian et al., 2016; K´ad´ar et al., 2017), while another line of analysis probes the representation learned by a neural network as input to a classifier of another task (Shi et al., 2016; Adi et al., 2016; Belinkov et al., 2017a; Tran et al., 2018). The most closely related work is by Belinkov et al. (2017b), in which they investigate the representa"
W18-5431,W13-2229,1,0.817207,"former models3 from English to seven languages, Czech, German, Estonian, Finnish, Russian, Turkish and Chinese, using the parallel data provided by the WMT18 shared task on news translation.4 The parallel data come from different sources, mainly from Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and ParaCrawl.5 The data sets are partially noisy, especially ParaCrawl being on its first release, and to filter out potentially incorrect parallel sentences we used a language identifier6 to tag each source and target sentence, discarding the sentences that do not match across languages (Stymne et al., 2013; Zarin¸a et al., 2015). As development set we used the provided newsdev data from the shared task, while using the newstest from WMT 2017 and 2018 as test data. A widely used technique to allow an open vocabulary is byte pair encoding (Sennrich et al., 2016), in which the source and target words are split into subword units. However, in this work we prefer to use the full word forms, allowing us to evaluate and compare the internal representation on standard sequence labeling benchmarks tagged with gold labels on the full word forms. Therefore, we use a large vocabulary of 100K words per lang"
W18-5431,tiedemann-2012-parallel,1,0.581689,"h we use the encoder representation of a high resource language pair to initialize the encoder of a low resource language pair. Here, we assume that a model is better at encoding abstract linguistic properties if it can share useful information to enhance another weaker model. 4 Model setup We trained Transformer models3 from English to seven languages, Czech, German, Estonian, Finnish, Russian, Turkish and Chinese, using the parallel data provided by the WMT18 shared task on news translation.4 The parallel data come from different sources, mainly from Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and ParaCrawl.5 The data sets are partially noisy, especially ParaCrawl being on its first release, and to filter out potentially incorrect parallel sentences we used a language identifier6 to tag each source and target sentence, discarding the sentences that do not match across languages (Stymne et al., 2013; Zarin¸a et al., 2015). As development set we used the provided newsdev data from the shared task, while using the newstest from WMT 2017 and 2018 as test data. A widely used technique to allow an open vocabulary is byte pair encoding (Sennrich et al., 2016), in which the source and targ"
W18-5431,W00-0726,0,0.237508,"Missing"
W18-5431,W03-0419,0,0.140143,"Missing"
W18-5431,D18-1503,0,0.131568,"Missing"
W18-5431,Q18-1019,0,0.0172436,"e encoders, 1 Most submissions for the WMT18 shared task on news (http://matrix.statmt.org/) employ the Transformer architecture. 287 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 287–297 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics in order to find linguistic patterns. As the next step, we exploit the attention weights of the network to build a graph and induce tree structures for each sentence, showing whether syntactic dependencies between words have been learned or not in the spirit of Williams et al. (2018) and Liu and Lapata (2018). Additionally, following previous studies on how to analyze the internal representation of neural systems (Adi et al., 2016; Shi et al., 2016; Belinkov et al., 2017a), we probe the encoder weights of the trained models to address different sequence labeling tasks: Part-of-Speech tagging, Chunking, Named Entity Recognition and Semantic tagging. We evaluate the quality of the decoder on a given task to assess how discriminative the encoder representation is for that task. Lastly, in order to check whether the learned information can be transferred across models, we use"
W18-5431,W15-4924,0,0.0243135,"Missing"
W18-5431,K17-3001,0,0.0194195,"dencies in lower layers. 6 Encoder Evaluation: Inducing Tree Structure The architecture of the Transformer, linking each word with each other with an attention weight, can be seen as a weighted graph in which the words are the nodes and from which tree structure can be extracted. Even though the models are not trained to produce any trees or to a specific syntax task, we used the attention weights in each layer to extract a tree of the input sentences and inspect whether they reflect a dependency tree. We evaluated the induced trees on the English PUD treebank from the CoNLL 2017 Shared Task (Zeman et al., 2017). The PUD treebank consists of 1000 sentences randomly taken from on-line newswire and Wikipedia. We measure the performance as Unlabeled Attachment Score (UAS) with the official evaluation script8 from the shared task, using gold segmentation and tokenization. Plus, given that our weights have no knowledge about the root of the sentence, we decided to use the gold root as starting node for the maximum spanning 290 8 conll17 ud eval.py (version 1.1) Layer 0 attention head 0 attention head 1 attention head 2 attention head 3 attention head 4 attention head 5 attention head 6 attention head 7 en"
W18-5431,D16-1163,0,0.165001,"al systems (Adi et al., 2016; Shi et al., 2016; Belinkov et al., 2017a), we probe the encoder weights of the trained models to address different sequence labeling tasks: Part-of-Speech tagging, Chunking, Named Entity Recognition and Semantic tagging. We evaluate the quality of the decoder on a given task to assess how discriminative the encoder representation is for that task. Lastly, in order to check whether the learned information can be transferred across models, we use the encoder weights of a high-resource language pair to initialize a low-resource language pair, inspired by the work of Zoph et al. (2016). We show that, also for the Transformer, the knowledge of an encoder representation can be shared with other models, helping them to achieve better translation quality. Overall, our analysis leads to interesting insights about strengths and weaknesses of the attention weights of the Transformer, giving more empirical evidence about the kind of information the model is learning at each layer: • We find that each layer has at least one attention head that encodes a significant amount of syntactic dependencies. Figure 1: The Transformer architecture (illustration from Vaswani et al. (2017)). emb"
W18-6205,P17-1067,0,0.301991,"applied in sentiment analysis use binary or ternary annotation schemes (positive-negative, or positive, negative, and neutral) (Andreevskaia and Bergler, 2007), or some kind of combination of these (i.e. the addition of e.g. ”mixed” (Saif et al., 2013)). This is not enough if the aim is to detect emotions rather than overarching sentiment (de Albornoz et al., 2012; Li and Hovy, 2017; Cambria et al., 2013). Furthermore, many of the existing datasets or tools (Munezero et al., 2015; Eryigit et al., 2013; Musat et al., 2012; Kakkonen and Kakkonen, 2011; Calefato et al., 2017; Saif et al., 2013; Abdul-Mageed and Ungar, 2017) are domain-dependent (often Twitter data) and/or document-level. Very few of these are also open data or open source. An important questions is whether to show wider context or not. Boland et al. (2013) show that context can lead to the effect of double weighting for fine-grained annotations. For that reason, we also opted for the annotation of isolated sentences even though our tools would easily support other setups. 2.1 Figure 1: Inverted wheel of emotions Although Sentimentator takes into account the intensity of the emotions, the complexity of the annotation task does not increase linear"
W18-6205,de-albornoz-etal-2012-sentisense,0,0.06703,"Missing"
W18-6205,S07-1022,0,0.24247,"is the reason 24 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 24–30 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 interface, we have inverted the wheel (see figure 1). ends with a concluding discussion. 2 Related Work Sentiment analysis is a widely studied task in natural language processing. Most of the existing datasets applied in sentiment analysis use binary or ternary annotation schemes (positive-negative, or positive, negative, and neutral) (Andreevskaia and Bergler, 2007), or some kind of combination of these (i.e. the addition of e.g. ”mixed” (Saif et al., 2013)). This is not enough if the aim is to detect emotions rather than overarching sentiment (de Albornoz et al., 2012; Li and Hovy, 2017; Cambria et al., 2013). Furthermore, many of the existing datasets or tools (Munezero et al., 2015; Eryigit et al., 2013; Musat et al., 2012; Kakkonen and Kakkonen, 2011; Calefato et al., 2017; Saif et al., 2013; Abdul-Mageed and Ungar, 2017) are domain-dependent (often Twitter data) and/or document-level. Very few of these are also open data or open source. An important"
W18-6205,W11-4110,0,0.0514917,"died task in natural language processing. Most of the existing datasets applied in sentiment analysis use binary or ternary annotation schemes (positive-negative, or positive, negative, and neutral) (Andreevskaia and Bergler, 2007), or some kind of combination of these (i.e. the addition of e.g. ”mixed” (Saif et al., 2013)). This is not enough if the aim is to detect emotions rather than overarching sentiment (de Albornoz et al., 2012; Li and Hovy, 2017; Cambria et al., 2013). Furthermore, many of the existing datasets or tools (Munezero et al., 2015; Eryigit et al., 2013; Musat et al., 2012; Kakkonen and Kakkonen, 2011; Calefato et al., 2017; Saif et al., 2013; Abdul-Mageed and Ungar, 2017) are domain-dependent (often Twitter data) and/or document-level. Very few of these are also open data or open source. An important questions is whether to show wider context or not. Boland et al. (2013) show that context can lead to the effect of double weighting for fine-grained annotations. For that reason, we also opted for the annotation of isolated sentences even though our tools would easily support other setups. 2.1 Figure 1: Inverted wheel of emotions Although Sentimentator takes into account the intensity of the"
W18-6205,L16-1147,1,0.857068,"is and emotion detection in various languages. Annotation is on-going and the first real dataset will be available later in 2018. For now we have a set of sentences with validated annotations that we will use as our seed data to get the gamified annotation started. This dataset has already been used to investigate sentiment preservation in Finnish, French, and Italian (Kajava, 2018). We wanted to make the dataset as useful as possible to as many researchers as possible from the beginning. This is why we selected an open parallel corpus, namely the OPUS movie subtitles corpus (Tiedemann, 2012; Lison and Tiedemann, 2016). From this collection, approximately 9,000 English sentences were annotated into the following emotion classes: anger, anticipation, disgust, joy, fear, sadness, surprise, and trust. This yielded a preliminary dataset of between 649 to 908 sentences per class (see table 4). For the classification experiments, we did not take into account the measure of emotion intensity that we introduce later in our annotation framework, which is also part of the seed sentence annotation. 5 Validation of the Data Quality In order to test the quality of the data for the purpose of developing an automatic emot"
W18-6205,O04-3004,0,0.112819,"Missing"
W18-6205,W12-4001,0,0.148905,"ysis is a widely studied task in natural language processing. Most of the existing datasets applied in sentiment analysis use binary or ternary annotation schemes (positive-negative, or positive, negative, and neutral) (Andreevskaia and Bergler, 2007), or some kind of combination of these (i.e. the addition of e.g. ”mixed” (Saif et al., 2013)). This is not enough if the aim is to detect emotions rather than overarching sentiment (de Albornoz et al., 2012; Li and Hovy, 2017; Cambria et al., 2013). Furthermore, many of the existing datasets or tools (Munezero et al., 2015; Eryigit et al., 2013; Musat et al., 2012; Kakkonen and Kakkonen, 2011; Calefato et al., 2017; Saif et al., 2013; Abdul-Mageed and Ungar, 2017) are domain-dependent (often Twitter data) and/or document-level. Very few of these are also open data or open source. An important questions is whether to show wider context or not. Boland et al. (2013) show that context can lead to the effect of double weighting for fine-grained annotations. For that reason, we also opted for the annotation of isolated sentences even though our tools would easily support other setups. 2.1 Figure 1: Inverted wheel of emotions Although Sentimentator takes into"
W18-6205,devitt-ahmad-2008-sentiment,0,0.0352971,"for classification, which was done very successfully by Abdul-Mageed and Ungar (2017). Using Euclidean distance on the inverted wheel to calculate the similarity of annotations, we can see that annotations for neutral and low intensity emotions are in fact quite similar. This means we can avoid unnecessarily dismissing an annotation as noise, as might be the case if a more traditional interface was used where neutral was a separate category from low-intensity emotions. Crowd-sourcing Annotations The annotation of datasets can be very costly and time consuming (Andreevskaia and Bergler, 2007; Devitt and Ahmad, 2008) if done by expert annotators. Crowd-sourcing can often be a cheaper alternative to hiring expert annotators, and has been used successfully by several researchers to create different types of datasets (Turney, 2002; Greenhill et al., 2014; Mohammad and Turney, 2013). However, one issue with using non-experts to solicit annotations is that there is a risk of the quality suffering. Our solution to annotation-reliability related issues is gamification, which will be discussed in detail in section 3. 2.2 2.3 Classification Table 1 shows the accuracies achieved by a few other multidimensional appr"
W18-6205,W16-4315,1,0.857579,"ly once the correlation between crowd annotators allows to estimate further reliability scores. The main idea is that we can trust annotators that provide identical or at least similar judgments as other reliable annotators. With this scheme, we can move away from the use of limited seed sentences for confidence estimation to a more dynamic and selfperpetuating gold standard. Another fundamental decision in our setup is the use of multilingual material on which to base our annotations. We are interested in the crosslingual use of emotions and the development of ¨ multilingual classifiers (see Ohman et al., 2016). Therefore, we start with sentences extracted from movie subtitles (English originals in our case) for which we also have plenty of translations into a large number of languages. Movies contain a lot of emotional content and, as a side effect, it is interesting to see how that is reflected in subtitles and their translations. Before presenting Sentimentator itself, we will first discuss related work and the theoretical framework we work with. The presentation of the seed/pilot dataset and its application for emotion detection follows the description of the tool and This paper introduces a gam"
W18-6205,E12-1049,0,0.111024,"ations is that there is a risk of the quality suffering. Our solution to annotation-reliability related issues is gamification, which will be discussed in detail in section 3. 2.2 2.3 Classification Table 1 shows the accuracies achieved by a few other multidimensional approaches (generally those of Ekman (1971)) using various classification methods such as SVMs, neural networks, maximum entropy, Na¨ıve Bayes, and k-Nearest Neighbor to name a few. When more classes are included in a model, accuracies achieved are typically lower than what binary or ternary models of sentiment analysis achieve (Purver and Battersby, 2012; Tokuhisa et al., 2008), with the exception of Abdul-Mageed and Ungar (2017) who apply a gated recurrent neural network model. Theory of Emotion The underlying theory of emotion for Sentimentator is Plutchik’s theory of emotion (Plutchik, 1980). The eight core emotions he proposes are anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. He uses a wheel, or flower, to illustrate these emotions. For a more intuitive 1 25 Quoted in Purver and Battersby (2012) Study Go et al. (2009) Danisman and Alpkocak (2008) Chuang and Wu (2004) Chuang and Wu (2004) Ansari 20101 Purver and Ba"
W18-6205,D08-1027,0,0.197186,"Missing"
W18-6205,tiedemann-2012-parallel,1,0.752062,"sentiment analysis and emotion detection in various languages. Annotation is on-going and the first real dataset will be available later in 2018. For now we have a set of sentences with validated annotations that we will use as our seed data to get the gamified annotation started. This dataset has already been used to investigate sentiment preservation in Finnish, French, and Italian (Kajava, 2018). We wanted to make the dataset as useful as possible to as many researchers as possible from the beginning. This is why we selected an open parallel corpus, namely the OPUS movie subtitles corpus (Tiedemann, 2012; Lison and Tiedemann, 2016). From this collection, approximately 9,000 English sentences were annotated into the following emotion classes: anger, anticipation, disgust, joy, fear, sadness, surprise, and trust. This yielded a preliminary dataset of between 649 to 908 sentences per class (see table 4). For the classification experiments, we did not take into account the measure of emotion intensity that we introduce later in our annotation framework, which is also part of the seed sentence annotation. 5 Validation of the Data Quality In order to test the quality of the data for the purpose of"
W18-6205,C08-1111,0,0.517399,"sk of the quality suffering. Our solution to annotation-reliability related issues is gamification, which will be discussed in detail in section 3. 2.2 2.3 Classification Table 1 shows the accuracies achieved by a few other multidimensional approaches (generally those of Ekman (1971)) using various classification methods such as SVMs, neural networks, maximum entropy, Na¨ıve Bayes, and k-Nearest Neighbor to name a few. When more classes are included in a model, accuracies achieved are typically lower than what binary or ternary models of sentiment analysis achieve (Purver and Battersby, 2012; Tokuhisa et al., 2008), with the exception of Abdul-Mageed and Ungar (2017) who apply a gated recurrent neural network model. Theory of Emotion The underlying theory of emotion for Sentimentator is Plutchik’s theory of emotion (Plutchik, 1980). The eight core emotions he proposes are anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. He uses a wheel, or flower, to illustrate these emotions. For a more intuitive 1 25 Quoted in Purver and Battersby (2012) Study Go et al. (2009) Danisman and Alpkocak (2008) Chuang and Wu (2004) Chuang and Wu (2004) Ansari 20101 Purver and Battersby (2012) Seol et a"
W18-6205,P02-1053,0,0.00982795,"intensity emotions are in fact quite similar. This means we can avoid unnecessarily dismissing an annotation as noise, as might be the case if a more traditional interface was used where neutral was a separate category from low-intensity emotions. Crowd-sourcing Annotations The annotation of datasets can be very costly and time consuming (Andreevskaia and Bergler, 2007; Devitt and Ahmad, 2008) if done by expert annotators. Crowd-sourcing can often be a cheaper alternative to hiring expert annotators, and has been used successfully by several researchers to create different types of datasets (Turney, 2002; Greenhill et al., 2014; Mohammad and Turney, 2013). However, one issue with using non-experts to solicit annotations is that there is a risk of the quality suffering. Our solution to annotation-reliability related issues is gamification, which will be discussed in detail in section 3. 2.2 2.3 Classification Table 1 shows the accuracies achieved by a few other multidimensional approaches (generally those of Ekman (1971)) using various classification methods such as SVMs, neural networks, maximum entropy, Na¨ıve Bayes, and k-Nearest Neighbor to name a few. When more classes are included in a m"
W18-6425,P18-4020,0,0.0395573,"Missing"
W18-6425,P17-4012,0,0.0367393,"primary submissions. While the main focus of our work lay on the English-to-Finnish translation direction, we also participated in the Finnishto-English, English-to-Estonian and Estonian-toEnglish translation directions. In 2017, the University of Helsinki participated in WMT with an in-house implementation of an attentional encoder-decoder architecture based on the ¨ Theano framework, called HNMT (Ostling et al., 2017). Since then, the development of Theano has stopped, and various open-source Neural Machine Translation (NMT) toolkits based on alternative frameworks have been made available (Klein et al., 2017; Junczys-Dowmunt et al., 2018, inter alia). In parallel, a novel neural network architecture for machine translation, called Transformer, has been introduced (Vaswani et al., 2017). The Transformer follows the encoder-decoder paradigm, but does not use any recurrent layers. Instead, its architecture relies primarily on attention mechanisms, stacking on each layer multiple attention components. Preliminary experiments with the Transformer architecture and its implementation in OpenNMT-py (Klein et al., 2017) showed consistent performance improvements compared to our 2017 architecture. Conseque"
W18-6425,2005.mtsummit-papers.11,0,0.0177992,"son et al., 2016; Tars and Fishel, 2018) we added a domain label to each input sentence, according to the data source. For example, each sentence from the Europarl corpus was prepended with the hEUROPARLi label. The overall idea of domain labelling is that data coming from different sources are of different quality and represent different genres and writing styles. In this way, the translation model can be informed of the data source without increasing the number of parameters. 2 English→Finnish 2.1 NMT models We trained our systems on almost all parallel data made available by WMT: Europarl (Koehn, 2005), ParaCrawl1 , Rapid, as well as the WMT 2015 test and development sets. We did not use WikiHeadlines. For development and tuning of the system 488 1 https://paracrawl.eu/ Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 488–495 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64052 parameters, we used the WMT 2016 and 2017 test sets. A common strategy is to create synthetic training data by back-translation (Sennrich et al., 2016a). For our WMT 2017 submissio"
W18-6425,W17-4733,1,0.930303,"ch to our best neural model, analyzing the output and highlighting future research. 1 Introduction The University of Helsinki participated in the WMT 2018 shared task on news translation with seven primary submissions. While the main focus of our work lay on the English-to-Finnish translation direction, we also participated in the Finnishto-English, English-to-Estonian and Estonian-toEnglish translation directions. In 2017, the University of Helsinki participated in WMT with an in-house implementation of an attentional encoder-decoder architecture based on the ¨ Theano framework, called HNMT (Ostling et al., 2017). Since then, the development of Theano has stopped, and various open-source Neural Machine Translation (NMT) toolkits based on alternative frameworks have been made available (Klein et al., 2017; Junczys-Dowmunt et al., 2018, inter alia). In parallel, a novel neural network architecture for machine translation, called Transformer, has been introduced (Vaswani et al., 2017). The Transformer follows the encoder-decoder paradigm, but does not use any recurrent layers. Instead, its architecture relies primarily on attention mechanisms, stacking on each layer multiple attention components. Prelimi"
W18-6425,P16-1009,0,0.0362795,"data made available by WMT: Europarl (Koehn, 2005), ParaCrawl1 , Rapid, as well as the WMT 2015 test and development sets. We did not use WikiHeadlines. For development and tuning of the system 488 1 https://paracrawl.eu/ Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 488–495 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64052 parameters, we used the WMT 2016 and 2017 test sets. A common strategy is to create synthetic training data by back-translation (Sennrich et al., 2016a). For our WMT 2017 submission, we already used SMT to create 5.5M sentences of back-translated data from the Finnish news2014 and news2016 corpora. This year, we created another 5.5M sentences of back-translation from the Finnish news2014news2017 corpora using our previous NMT system ¨ (Ostling et al., 2017). The final submissions make use of both resources. We applied the standard preprocessing pipeline consisting of tokenization,2 normalization,3 truecasing and byte-pair encoding (Sennrich et al., 2016b). Following Vaswani et al. (2017), we have used a joint BPE vocabulary of 37 000 units."
W18-6425,P16-1162,0,0.0924233,"data made available by WMT: Europarl (Koehn, 2005), ParaCrawl1 , Rapid, as well as the WMT 2015 test and development sets. We did not use WikiHeadlines. For development and tuning of the system 488 1 https://paracrawl.eu/ Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 488–495 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64052 parameters, we used the WMT 2016 and 2017 test sets. A common strategy is to create synthetic training data by back-translation (Sennrich et al., 2016a). For our WMT 2017 submission, we already used SMT to create 5.5M sentences of back-translated data from the Finnish news2014 and news2016 corpora. This year, we created another 5.5M sentences of back-translation from the Finnish news2014news2017 corpora using our previous NMT system ¨ (Ostling et al., 2017). The final submissions make use of both resources. We applied the standard preprocessing pipeline consisting of tokenization,2 normalization,3 truecasing and byte-pair encoding (Sennrich et al., 2016b). Following Vaswani et al. (2017), we have used a joint BPE vocabulary of 37 000 units."
W18-6425,skadins-etal-2014-billions,1,0.88596,"Missing"
W18-6425,W17-4731,1,0.827394,"x than simple concatenation of subword units. First the BPE tags are joined and then the surface forms are generated using the FINTWOL generation functionality, which takes as input lemmas and morphological tags and output all compatible surface forms. The default tags are automatically added for lemmas which do not have explicit tags. Heuristics are used to select a surface form if several possibilities are generated. The submitted model was trained on the bilingual and back-translated data, as adding the backtranslated data greatly improved the quality of the translations. 2.3 Rule-based MT Hurskainen and Tiedemann (2017) propose a rulebased machine translation system for English– Finnish. During the past year, the rule-based MT system has been developed in several ways. In addition to the usual debugging and rule testing, also some major structural changes have been made. Below we will discuss the latter type of problems. Translating locative expressions: While English uses prepositions for marking location, Finnish uses locative cases. English has a bewildering number of prepositions for this purpose. At least the following preposition are used: in, on, at, with, by, to, into, for, of, from, over, through, a"
W18-6425,W17-4704,0,0.0426224,"Missing"
W18-6425,J82-2005,0,0.701555,"Missing"
W18-6425,E12-1015,1,0.798906,"t on each specific language pair. 5.1 for 20 epochs, evaluating each of them on the development set after every epoch, taking the best iteration as final model. As hyper-parameters, we used the base version of the Transformer architecture, following the suggestion of the OpenNMT-py tool,5 including a shared word embedding space between encoder and decoder among others. Unlike last year, we did not include any averaging or ensembling techniques. Synthetic data Another way to take advantage of the close etymological relationship between Estonian and Finnish is to create synthetic training data (Tiedemann, 2012). We explored this option in the following setup: 1. Train a character-level seq2seq system for Finnish-to-Estonian, using the Europarl and EUbookshop (Skadin¸sˇ et al., 2014) corpora. 2. Translate the Finnish side of the parallel English–Finnish corpus to Estonian. 3. Combine the Estonian and English parts of the corpus and use this dataset as backtranslations to train the final system. We were able to process 1.5M sentences using this approach. These sentences complemented the other training data, consisting of parallel data and direct English–Estonian back-translations. 6 Experiments In thi"
W18-6425,W16-2326,1,0.895598,"Missing"
W18-6425,D16-1163,0,0.025825,"l +Back +Synth HY-NMT Multilingual +Label Table 1: Number of training sentences, with and without back-translation (Back) and synthetic data (Synth). Et→En 21.6 20.3 26.5 25.4 26.5* 25.0 – 26.4 En→Et 16.7 17.6 – 21.8* – 21.0 – 20.8 Table 2: BLEU-cased scores on newstest2018 for the English–Estonian language pair in various configurations using domain labels (Label), backtranslated data (Back), or synthetic data (Synth). Our primary submissions are marked with *. two. In this way, even though Estonian has no parallel news data, the model will see the news label in the Finnish data. Inspired by Zoph et al. (2016), we first train the multilingual model with all languages in all directions, and then fine-tune it on each specific language pair. 5.1 for 20 epochs, evaluating each of them on the development set after every epoch, taking the best iteration as final model. As hyper-parameters, we used the base version of the Transformer architecture, following the suggestion of the OpenNMT-py tool,5 including a shared word embedding space between encoder and decoder among others. Unlike last year, we did not include any averaging or ensembling techniques. Synthetic data Another way to take advantage of the c"
W18-6439,W17-4746,0,0.0911741,"Missing"
W18-6439,W17-4747,0,0.0299311,"n sources to be merged. Different strategies to include image features both on the encoder and decoder side have been explored. We are inspired by the recent success of the Transformer architecture to adapt some of these strategies for use with the Transformer. Recurrent neural networks start their processing from some initial hidden state. Normally, a zero vector or a learned parameter vector is used, but the initial hidden state is also a natural location to introduce additional context e.g. from other modalities. Initializing can be applied in either the encoder (IMGE ) or decoder (IMGD ) (Calixto et al., 2017). These approaches are not directly applicable to the Transformer, as it is not a recurrent model, and lacks a comparable initial hidden state. Double attention is another popular choice, used by e.g. Caglayan et al. (2017). In this approach, two attention mechanisms are used, one for each modality. The attentions can be separate or hierarchical. While it would be possible to use double attention with the Transformer, we did not explore it in this work. The multiple multi-head attention mechanisms in the Transformer leave open many challenges in how this integration would be done. Multi-task l"
W18-6439,I17-1014,0,0.0585738,"former, as it is not a recurrent model, and lacks a comparable initial hidden state. Double attention is another popular choice, used by e.g. Caglayan et al. (2017). In this approach, two attention mechanisms are used, one for each modality. The attentions can be separate or hierarchical. While it would be possible to use double attention with the Transformer, we did not explore it in this work. The multiple multi-head attention mechanisms in the Transformer leave open many challenges in how this integration would be done. Multi-task learning has also been used, e.g. in the Imagination model (Elliott and Kádár, 2017), where the auxiliary task consists of reconstructing the visual features from the source encoding. Imagination could also have been used with the Transformer, but we did not explore it in this work. The source sequence itself is also a possible location for including the visual information. In the IMGW approach, the visual features are encoded as a pseudo-word embedding concatenated to the word embeddings of the source sentence. When the encoder is a bidirectional recurrent network, as in Calixto et al. (2017), it is beneficial to add the pseudo-word both at the beginning and the end to make"
W18-6439,P17-4012,0,0.080597,"Missing"
W18-6439,C16-1172,0,0.0926185,"Missing"
W18-6439,W17-4733,1,0.891591,"Missing"
W18-6439,W15-3049,0,0.0230282,"d ihr brauner hund rennt auf sie zu .” when not using the image features, but as masculine “ein besitzer …” when using them. The English text contains the word “her”. The person in the image has short hair and is wearing pants. size 4096 tokens, label smoothing 0.1, Adam with initial learning rate 2 and β2 0.998. For decoding, we use an ensemble procedure, in which the predictions of 3 independently trained models are combined by averaging after the softmax layer to compute combined prediction. We evaluate the systems using uncased BLEU using multibleu. During tuning, we also used characterF (Popovic, 2015) with β set to 1.0. There are no images paired with the sentences in OpenSubtitles. When using OpenSubtitles in training multi-modal models, we feed in the mean vector of all visual features in the training data as a dummy visual feature. 4.4 Results Based on the previous experiments, we chose the Transformer architecture, Multi30k+MSCOCO+subs3MLM data sets, Detectron mask surface visual features, and domain labeling. Table 5 shows the BLEU scores for this configuration with different ways of integrating the visual features. The results are inconclusive. The ranking according to chrF-1.0 was n"
W19-2005,P13-1158,0,0.0334914,"r to languageindependent meaning representations than bilingual models do. Hence, our hypothesis is that 35 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 35–42 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics Language English French Afrikaans Albanian Breton German Greek Frisian Hindi Italian Dutch Ossetian Polish Russian Slovene Spanish Serbian Swedish paraphrase generation using machine translation (Quirk et al., 2004; Finch et al., 2004; Prakash et al., 2016) based on parallel monolingual corpora (Lin et al., 2014; Fader et al., 2013), pivot-based translation (Bannard and CallisonBurch, 2005; Mallinson et al., 2017) and paraphrase databased extracted from parallel corpora (Ganitkevitch et al., 2013). Related work on multilingual sentence representation (Artetxe and Schwenk, 2018; Schwenk and Douze, 2017; Lample and Conneau, 2019) has focused on fixedsize vector representations that can be used in natural language inference (Conneau et al., 2018; Eriguchi et al., 2018) or other downstream tasks such as bitext mining (Artetxe and Schwenk, 2018) or (cross-lingual) document classification (Schwenk and Li, 2018). 2 Transl. 19 1"
W19-2005,N13-1092,0,0.194932,"Missing"
W19-2005,Q17-1024,0,0.043325,"Missing"
W19-2005,P18-4020,0,0.0253946,"Missing"
W19-2005,P05-1074,0,0.529478,"Missing"
W19-2005,E17-1083,0,0.0276671,"e, our hypothesis is that 35 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 35–42 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics Language English French Afrikaans Albanian Breton German Greek Frisian Hindi Italian Dutch Ossetian Polish Russian Slovene Spanish Serbian Swedish paraphrase generation using machine translation (Quirk et al., 2004; Finch et al., 2004; Prakash et al., 2016) based on parallel monolingual corpora (Lin et al., 2014; Fader et al., 2013), pivot-based translation (Bannard and CallisonBurch, 2005; Mallinson et al., 2017) and paraphrase databased extracted from parallel corpora (Ganitkevitch et al., 2013). Related work on multilingual sentence representation (Artetxe and Schwenk, 2018; Schwenk and Douze, 2017; Lample and Conneau, 2019) has focused on fixedsize vector representations that can be used in natural language inference (Conneau et al., 2018; Eriguchi et al., 2018) or other downstream tasks such as bitext mining (Artetxe and Schwenk, 2018) or (cross-lingual) document classification (Schwenk and Li, 2018). 2 Transl. 19 14 5 2 1 24 7 1 4 5 3 2 5 5 1 8 2 1 Verses 234,173 369,910 75,974 58,192 1,781 499,8"
W19-2005,P11-1020,0,0.119638,"Missing"
W19-2005,mayer-cysouw-2014-creating,0,0.0213217,"avoid unknown words and to improve generalisations. Note that in our setup we need to ensure that subword-level segmentations are consistent for each language involved in several translation tasks. We opted for languagedependent BPE models with 10,000 merge operations for each code table. The total vocabulary size then depends on the combination of languages that we use in training but the vocabulary stays exactly the same for each language involved in all experiments. 2.1 Training data and configurations The main data we use for our experiments comes from a collection of Bible translations (Mayer and Cysouw, 2014) that includes over a thousand languages. For high-density languages like English and French, various alternatives are available (see Table 1). Using the Bible makes it possible to easily extend our work with additional languages representing a wide range of linguistic variation, while at the same time keeping genre and content constant across languages. For the sake of discussion, we selected English 36 100 60 80 40 60 20 40 20 En g +B –Fra +S reton +O loven s e +S setian erb +S ian pa +G nish r +H eek +F indi +S risi +A wed an frik ish +P aans o +It lish +R alian +Gussia er n + ma +A Dut n l"
W19-2005,P02-1040,0,0.10355,"Missing"
W19-2005,C16-1275,0,0.0537101,"er multilingual machine translation models learn representations that are closer to languageindependent meaning representations than bilingual models do. Hence, our hypothesis is that 35 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 35–42 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics Language English French Afrikaans Albanian Breton German Greek Frisian Hindi Italian Dutch Ossetian Polish Russian Slovene Spanish Serbian Swedish paraphrase generation using machine translation (Quirk et al., 2004; Finch et al., 2004; Prakash et al., 2016) based on parallel monolingual corpora (Lin et al., 2014; Fader et al., 2013), pivot-based translation (Bannard and CallisonBurch, 2005; Mallinson et al., 2017) and paraphrase databased extracted from parallel corpora (Ganitkevitch et al., 2013). Related work on multilingual sentence representation (Artetxe and Schwenk, 2018; Schwenk and Douze, 2017; Lample and Conneau, 2019) has focused on fixedsize vector representations that can be used in natural language inference (Conneau et al., 2018; Eriguchi et al., 2018) or other downstream tasks such as bitext mining (Artetxe and Schwenk, 2018) or ("
W19-2005,W04-3219,0,0.635583,"n particular, we would like to see whether multilingual machine translation models learn representations that are closer to languageindependent meaning representations than bilingual models do. Hence, our hypothesis is that 35 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 35–42 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics Language English French Afrikaans Albanian Breton German Greek Frisian Hindi Italian Dutch Ossetian Polish Russian Slovene Spanish Serbian Swedish paraphrase generation using machine translation (Quirk et al., 2004; Finch et al., 2004; Prakash et al., 2016) based on parallel monolingual corpora (Lin et al., 2014; Fader et al., 2013), pivot-based translation (Bannard and CallisonBurch, 2005; Mallinson et al., 2017) and paraphrase databased extracted from parallel corpora (Ganitkevitch et al., 2013). Related work on multilingual sentence representation (Artetxe and Schwenk, 2018; Schwenk and Douze, 2017; Lample and Conneau, 2019) has focused on fixedsize vector representations that can be used in natural language inference (Conneau et al., 2018; Eriguchi et al., 2018) or other downstream tasks such as bit"
W19-2005,W17-2619,0,0.0422909,"al Linguistics Language English French Afrikaans Albanian Breton German Greek Frisian Hindi Italian Dutch Ossetian Polish Russian Slovene Spanish Serbian Swedish paraphrase generation using machine translation (Quirk et al., 2004; Finch et al., 2004; Prakash et al., 2016) based on parallel monolingual corpora (Lin et al., 2014; Fader et al., 2013), pivot-based translation (Bannard and CallisonBurch, 2005; Mallinson et al., 2017) and paraphrase databased extracted from parallel corpora (Ganitkevitch et al., 2013). Related work on multilingual sentence representation (Artetxe and Schwenk, 2018; Schwenk and Douze, 2017; Lample and Conneau, 2019) has focused on fixedsize vector representations that can be used in natural language inference (Conneau et al., 2018; Eriguchi et al., 2018) or other downstream tasks such as bitext mining (Artetxe and Schwenk, 2018) or (cross-lingual) document classification (Schwenk and Li, 2018). 2 Transl. 19 14 5 2 1 24 7 1 4 5 3 2 5 5 1 8 2 1 Verses 234,173 369,910 75,974 58,192 1,781 499,844 87,218 29,173 93,242 122,363 87,460 37,807 52,668 75,904 29,088 236,830 35,019 29,088 Tokens 6,750,869 10,529,929 2,329,773 1,648,242 44,316 13,712,459 2,357,095 852,582 2,829,274 3,429,18"
W19-2005,L18-1560,0,0.0181527,"Lin et al., 2014; Fader et al., 2013), pivot-based translation (Bannard and CallisonBurch, 2005; Mallinson et al., 2017) and paraphrase databased extracted from parallel corpora (Ganitkevitch et al., 2013). Related work on multilingual sentence representation (Artetxe and Schwenk, 2018; Schwenk and Douze, 2017; Lample and Conneau, 2019) has focused on fixedsize vector representations that can be used in natural language inference (Conneau et al., 2018; Eriguchi et al., 2018) or other downstream tasks such as bitext mining (Artetxe and Schwenk, 2018) or (cross-lingual) document classification (Schwenk and Li, 2018). 2 Transl. 19 14 5 2 1 24 7 1 4 5 3 2 5 5 1 8 2 1 Verses 234,173 369,910 75,974 58,192 1,781 499,844 87,218 29,173 93,242 122,363 87,460 37,807 52,668 75,904 29,088 236,830 35,019 29,088 Tokens 6,750,869 10,529,929 2,329,773 1,648,242 44,316 13,712,459 2,357,095 852,582 2,829,274 3,429,182 2,596,298 936,533 1,248,108 1,727,536 748,367 6,607,932 844,299 833,983 Table 1: Statistics about the Bible data in our collection: number of individual Bible translations, number of verses and number of tokens per language in the training data sets. Experimental Setup For our experiments, we apply a standa"
W19-2005,E17-3017,0,0.0777581,"Missing"
W19-2005,P16-1162,0,0.0385519,"g criterion, which is set to five subsequent failures of improving the validation score. The translations are done with a beam search decoder of size 12. The validation frequency is set to run each 2,500 mini-batches. For the multilingual setup, we follow Johnson et al. (2016) by adding target language flags to the source text placing them as pseudo tokens in the beginning of each input sentence. We always train models in both directions enabling the model to read and generate the same language without explicitly training that task (i.e. paraphrasing is modeled as zero-shot translation). BPE (Sennrich et al., 2016) is used to avoid unknown words and to improve generalisations. Note that in our setup we need to ensure that subword-level segmentations are consistent for each language involved in several translation tasks. We opted for languagedependent BPE models with 10,000 merge operations for each code table. The total vocabulary size then depends on the combination of languages that we use in training but the vocabulary stays exactly the same for each language involved in all experiments. 2.1 Training data and configurations The main data we use for our experiments comes from a collection of Bible tra"
W19-2509,W18-4510,1,0.565181,"Missing"
W19-2509,P17-4012,0,0.0455607,"rent gold standards. At first, we change one parameter at a time and compare the results to the default settings. We try two different encoder types, bi-directional recurrent neural networks (BRNNs) and mean, which is an encoder applying mean pooling. BRNN uses two independent encoders to encode the sequence reversed and without reversal. The default RNN, in contrast, only encodes the sequence normally without reversing it. In addition to the default attention model, we also try out the MLP (multi-layer perceptron) model proposed by Bahdanau et al. (2014). We The NMT Approach We use OpenNMT1 (Klein et al., 2017) to train the NMT models discussed in this paper. The models are trained on a character level. This means that the model is supplied with parallel lists of historical spellings and their modern counterparts, where the words have been split into individual characters separated by white spaces. The training is done for pairs of words, i.e. the normalization is to be conducted without a context. The NMT model would then treat individual characters as though they were words in a sentence and ”translate” them into the corresponding modernized spelling. 1 The Parallel Data Version 0.2.1 of opennmt-p"
W19-2509,W17-0504,0,0.103387,"alignment of the parallel data on both word and character level. SMT has also been used in normalization of contemporary dialectal language to the standardized normative form (Samardzic et al., 2015). They test normalization with word-by-word trans71 Proc. of the 3rd Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pp. 71–75 c Minneapolis, MN, USA, June 7, 2019. 2019 Association for Computational Linguistics 4.1 lation and character level SMT. The character level SMT improves the normalization of unseen and ambiguous words. Korchagina (2017) proposes an NMT based normalization for medieval German. It is supposedly one of the first attempts to use NMT for historical normalization. The study reports NMT outperforming the existing rule-based and SMT methods. A recent study by Tang et al. (2018) compared different NMT models for historical text normalization in five different languages. They report that NMT outperforms SMT in four of the five languages. In terms of performance, vanilla RNNs are comparable to LSTMs and GRUs, and also the difference between attention and no attention is small. 3 We use different sources of historical-m"
W19-2509,C18-1112,0,0.0229732,"oc. of the 3rd Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pp. 71–75 c Minneapolis, MN, USA, June 7, 2019. 2019 Association for Computational Linguistics 4.1 lation and character level SMT. The character level SMT improves the normalization of unseen and ambiguous words. Korchagina (2017) proposes an NMT based normalization for medieval German. It is supposedly one of the first attempts to use NMT for historical normalization. The study reports NMT outperforming the existing rule-based and SMT methods. A recent study by Tang et al. (2018) compared different NMT models for historical text normalization in five different languages. They report that NMT outperforms SMT in four of the five languages. In terms of performance, vanilla RNNs are comparable to LSTMs and GRUs, and also the difference between attention and no attention is small. 3 We use different sources of historical-modern English parallel data. These include the normalized words from the CEEC, the historical forms provided in the OED and the historical lemmas in the Middle English Dictionary (MED, n.d.) that have been linked to the OED lemmas with modern spelling. Th"
W19-4304,L18-1269,0,0.0270724,"We selected the best model according to the BLEU score on the validation set. We train all the models using the Europarl Corpus v7 (Koehn, 2005), focusing on 4 languages: English (EN), French (FR), German (DE) and Spanish (ES). First we train bilingual models for EN→DE; then we train multilingual models {DE,ES,FR}↔EN; lastly we train a final Many-to-Many model using the biggest size, i.e., 50 attention heads, involving all translation directions between the three languages, i.e., we also include DE–ES, DE–FR and ES–FR. To evaluate the sentence representations we utilize the SentEval toolkit (Conneau and Kiela, 2018) that combines various established downstream tasks for testing representations of English 1 As in Lu et al. (2018), we note that the attention bridge is independent of the underlying encoder and decoder. While we use LSTM, it could be easily replaced with a transformer type network (Vaswani et al., 2017) or with a CNN (Gehring et al., 2017). 2 Our fork implementation is available at https: //github.com/Helsinki-NLP/OpenNMT-py/ tree/att-brg. 28 SICK-E 77.09 78.77 79.34 79.36 AVG 71.46 72.02 72.68 72.60 STSB AVG k=1 k=10 k=25 k=50 SNLI 63.86 65.30 65.13 65.30 SICK-R en→de en→de en→de en→de en→d"
W19-4304,D16-1026,0,0.113383,"Missing"
W19-4304,N18-1032,0,0.0614647,"nificantly improving over the traditional statistical machine translation procedure (Bojar et al., 2018). Recently, several models and variants have been proposed with increased research efforts towards multilingual machine translation (Firat et al., 2016; Lakew et al., 2018; Wang et al., 2018; Blackwood et al., 2018; Lu et al., 2018). The main motivation of multilingual models is the effect of transfer learning that enables machine translation systems to benefit from relationships between languages and training signals that come from different datasets (Ha et al., 2016; Johnson et al., 2017; Gu et al., 2018). Another aspect that draws interest in translation models is the 27 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 27–32 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Figure 1: Architecture of our multilingual NMT system: (left) the attention bridge connects the language-specific encoders and decoders; (center) input x1 . . . xn is translated into the decoder states s1 . . . st via the encoder states H = h1 . . . hn and the attention bridge m1 . . . mk ; (right) Computation of the hidden representation matrix A, ne"
W19-4304,N16-1162,0,0.0817214,"Missing"
W19-4304,C18-1263,0,0.0472055,"y the encoded information that is useful for the specific task whereas nontrainable benchmarks can be confused by other types of information also encoded in the representation of a sentence. 1 Introduction Neural Machine Translation (NMT) has rapidly become the new Machine Translation (MT) paradigm, significantly improving over the traditional statistical machine translation procedure (Bojar et al., 2018). Recently, several models and variants have been proposed with increased research efforts towards multilingual machine translation (Firat et al., 2016; Lakew et al., 2018; Wang et al., 2018; Blackwood et al., 2018; Lu et al., 2018). The main motivation of multilingual models is the effect of transfer learning that enables machine translation systems to benefit from relationships between languages and training signals that come from different datasets (Ha et al., 2016; Johnson et al., 2017; Gu et al., 2018). Another aspect that draws interest in translation models is the 27 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 27–32 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Figure 1: Architecture of our multilingual NMT system:"
W19-4304,P17-4012,0,0.0221057,"with the only exception that the context vector is computed on the attention bridge, and the initialization is performed by a mean pooling over it. Hence, the decoder receives the information only through the shared attention bridge. The fixed-sized representation coming out of the shared layer can immediately be applied to downstream tasks.1 However, selecting a reasonable size of the attention bridge in terms of attention heads (mi in Figure 1) is crucial for the performance both in a bilingual and multilingual sce3 Experimental setup All models are implemented using the OpenNMT framework (Klein et al., 2017) trained using the same set of hyper-parameters.2 We use embedding layers of 512 dimensions, two stacked bidirectional LSTM layers with 512 hidden units (256 per direction) and an attentive decoder composed of two unidirectional LSTM layers with 512 units. Regarding the attention bridge, we experimented with four different configurations: 1, 10, 25 and 50 attention heads with 1024 hidden units each. For multilingual models, we used a language-rotating scheduler, in which each mini-batch contains sentences from a different language pair, cycling through all the language pairs uniformly. We sele"
W19-4304,D17-1040,0,0.0188738,"l., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings (C´ıfka and Bojar, 2018). However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear. Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018), on translation quality (Lu et al., 2018), on speed comparison (Britz et al., 2017), or only exploring a bilingual scenario (C´ıfka and Bojar, 2018). In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model. This shared layer serves as a fixedsize sentence representation that can be straightforwardly applied to downstream tasks. We examine this model with a systematic evaluation on different sizes of the attention bridge and extensive experiments to study the abstractions it learns from multiple translation tasks. In contrast to previous work (C´ıfka and Bojar, 2018), we demonst"
W19-4304,2005.mtsummit-papers.11,0,0.00839877,"ayers with 512 hidden units (256 per direction) and an attentive decoder composed of two unidirectional LSTM layers with 512 units. Regarding the attention bridge, we experimented with four different configurations: 1, 10, 25 and 50 attention heads with 1024 hidden units each. For multilingual models, we used a language-rotating scheduler, in which each mini-batch contains sentences from a different language pair, cycling through all the language pairs uniformly. We selected the best model according to the BLEU score on the validation set. We train all the models using the Europarl Corpus v7 (Koehn, 2005), focusing on 4 languages: English (EN), French (FR), German (DE) and Spanish (ES). First we train bilingual models for EN→DE; then we train multilingual models {DE,ES,FR}↔EN; lastly we train a final Many-to-Many model using the biggest size, i.e., 50 attention heads, involving all translation directions between the three languages, i.e., we also include DE–ES, DE–FR and ES–FR. To evaluate the sentence representations we utilize the SentEval toolkit (Conneau and Kiela, 2018) that combines various established downstream tasks for testing representations of English 1 As in Lu et al. (2018), we n"
W19-4304,C18-1054,0,0.0136229,"tream task enables the model to identify the encoded information that is useful for the specific task whereas nontrainable benchmarks can be confused by other types of information also encoded in the representation of a sentence. 1 Introduction Neural Machine Translation (NMT) has rapidly become the new Machine Translation (MT) paradigm, significantly improving over the traditional statistical machine translation procedure (Bojar et al., 2018). Recently, several models and variants have been proposed with increased research efforts towards multilingual machine translation (Firat et al., 2016; Lakew et al., 2018; Wang et al., 2018; Blackwood et al., 2018; Lu et al., 2018). The main motivation of multilingual models is the effect of transfer learning that enables machine translation systems to benefit from relationships between languages and training signals that come from different datasets (Ha et al., 2016; Johnson et al., 2017; Gu et al., 2018). Another aspect that draws interest in translation models is the 27 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 27–32 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Figure 1: Ar"
W19-4304,P18-1126,0,0.040596,"Missing"
W19-4304,D18-1326,0,0.0116776,"he model to identify the encoded information that is useful for the specific task whereas nontrainable benchmarks can be confused by other types of information also encoded in the representation of a sentence. 1 Introduction Neural Machine Translation (NMT) has rapidly become the new Machine Translation (MT) paradigm, significantly improving over the traditional statistical machine translation procedure (Bojar et al., 2018). Recently, several models and variants have been proposed with increased research efforts towards multilingual machine translation (Firat et al., 2016; Lakew et al., 2018; Wang et al., 2018; Blackwood et al., 2018; Lu et al., 2018). The main motivation of multilingual models is the effect of transfer learning that enables machine translation systems to benefit from relationships between languages and training signals that come from different datasets (Ha et al., 2016; Johnson et al., 2017; Gu et al., 2018). Another aspect that draws interest in translation models is the 27 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 27–32 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Figure 1: Architecture of our m"
W19-4304,W18-6309,0,0.24379,"signal (Hill et al., 2016; McCann et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings (C´ıfka and Bojar, 2018). However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear. Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018), on translation quality (Lu et al., 2018), on speed comparison (Britz et al., 2017), or only exploring a bilingual scenario (C´ıfka and Bojar, 2018). In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model. This shared layer serves as a fixedsize sentence representation that can be straightforwardly applied to downstream tasks. We examine this model with a systematic evaluation on different sizes of the attention bridge and extensive experiments to study the abstractions it learns from multiple translation tasks. In contrast to previous"
W19-4304,D15-1166,0,0.453525,": Architecture of our multilingual NMT system: (left) the attention bridge connects the language-specific encoders and decoders; (center) input x1 . . . xn is translated into the decoder states s1 . . . st via the encoder states H = h1 . . . hn and the attention bridge m1 . . . mk ; (right) Computation of the hidden representation matrix A, needed to obtain the fixed-size attentive matrix M = AH T . 2 Architecture nario as we will see in the experiments below. Our architecture follows the standard setup of an encoder-decoder model in machine translation with a traditional attention mechanism (Luong et al., 2015). However, we augment the network with language specific encoders and decoders to enable multilingual training as in Lu et al. (2018), plus we introduce an inner-attention layer (Liu et al., 2016; Lin et al., 2017) that summarizes the encoder information in a fixed-size vector representation that can easily be shared among different translation tasks with the language-specific encoders and decoders connecting to it. The overall architecture is illustrated in Figure 1 (see also V´azquez et al., 2019). Due to the attentive connection between encoders and decoders we call this layer attention bri"
W19-4304,P18-2037,0,0.0420262,"ranslation task as an auxiliary semantic signal (Hill et al., 2016; McCann et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings (C´ıfka and Bojar, 2018). However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear. Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018), on translation quality (Lu et al., 2018), on speed comparison (Britz et al., 2017), or only exploring a bilingual scenario (C´ıfka and Bojar, 2018). In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model. This shared layer serves as a fixedsize sentence representation that can be straightforwardly applied to downstream tasks. We examine this model with a systematic evaluation on different sizes of the attention bridge and extensive experiments to study the abstractions it learns from multiple"
W19-4304,W17-2619,0,0.185712,"Missing"
W19-4304,P16-1162,0,0.0258685,"s (r/ρ). The average across unsupervised similarity tasks on Pearson’s measures are displayed in the right-most column. Results with † taken from C´ıfka and Bojar (2018). sentences.3 In order to obtain a sentence vector out of multiple attention heads we apply mean pooling over the attention bridge. We are also interested in the translation quality to verify the appropriateness of our models with respect to the main objective they are trained for. For this, we adopt the in-domain development and evaluation dataset from the ACL-WMT07 shared task. Sentences are encoded using Byte-Pair Encoding (Sennrich et al., 2016), with 32,000 merge operations for each language. 4 of multilingual training. We can see that multilingual training objectives are generally helpful for the trainable downstream tasks. Particularly interesting is the fact that the Manyto-Many model performs best on average even though it does not add any further training examples for English (compared to the other multilingual models), which is the target language of the downstream tasks. This suggests that the model is able to improve generalizations even from other language pairs (DE–ES, FR–ES, FR–DE) that are not directly involved in traini"
W19-4304,W19-4305,1,0.814663,"Missing"
W19-4305,C18-1263,0,0.0323356,"ng this model and scheduled training. We have tested the approach in a systematic way with a multi-parallel data set. The model achieves substantial improvements over strong bilingual models and performs well for zero-shot translation, which demonstrates its ability of abstraction and transfer learning. 1 Introduction Neural machine translation (NMT) provides an ideal setting for multilingual MT because it can efficiently share model parameters and take advantage of the various similarities found by the model in the hidden layers and word embeddings (Firat et al., 2016a; Johnson et al., 2017; Blackwood et al., 2018). Furthermore, multilingual NMT has the potential of considerably improving the performance of neural translation systems for low-resource languages (Lakew et al., 2017) and enables zero-shot translation, i.e., translating between language pairs that were not seen during training (Firat et al., 2016b; Johnson et al., 2017). For this study we focus on models for multilingual translation that learn language-agnostic representations, where we outline the development of a language-independent representation based on 2 Model Architecture Our architecture follows the standard setup of an encoder-dec"
W19-4305,C18-1154,0,0.0544271,"Missing"
W19-4305,L18-1269,0,0.0213541,"73.51 77.25 61.92 31.15 67.75 70.96 61.75 74.85 0.652 0.616 68.32 60.40 72.98 78.64 62.02 32.10 69.84 68.83 64.52 75.46 0.659 0.618 GloVe-BoW 69.01 61.80 73.28 80.88 62.24 31.83 66.40 70.43 65.12 76.92 0.677 0.630 63.97 52.32 68.76 58.75 54.68 28.20 21.16 64.87 35.05 56.62 0.174 0.163 85.41 9.13 31.60 39.76 59.76 68.27 69.89 73.29 50.12 62.21 30.90 0.22 20.66 11.48 50.08 54.72 54.32 60.58 50.03 49.88 P ROBING TASKS Length WC Depth TopConst BShift Tense SubjNum ObjNum SOMO CoordInv We apply the sentence representations learned by our model to downstream tasks collected in the SentEval toolkit (Conneau and Kiela, 2018) to evaluate the quality of our language-agnostic sentence embeddings. We run each experiment with five different seeds, and present the average of these scores in Table 2, where we compare our bilingual models against a baseline consisting of the best score achieved by the bilingual models with attention bridge. Since our models were trained on limited data and are not directly comparable to models trained on large-scale data sets, for comparison purposes we present results obtained with GloVe-BoW vectors (Pennington et al., 2014) trained with the same BPE-encoded data as the models. The sent"
W19-4305,P18-1198,0,0.0408104,"Missing"
W19-4305,P15-1166,0,0.0565387,"raining (Firat et al., 2016b; Johnson et al., 2017). For this study we focus on models for multilingual translation that learn language-agnostic representations, where we outline the development of a language-independent representation based on 2 Model Architecture Our architecture follows the standard setup of an encoder-decoder model of machine translation with a traditional attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). However, to enable multilingual training we augment the network with language-specific encoders and decoders trainable with a language-rotating scheduler (Dong et al., 2015; Schwenk and Douze, 2017). We also incorporate a self-attention layer (attention bridge), shared among all language pairs, to serve as a language-agnostic layer (C´ıfka and Bojar, 2018; Lu et al., 2018) Attention bridge: Each encoder takes as input 33 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 33–39 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 3 a sequence of tokens (x1 , . . . , xn ) and produces n dh –dimensional hidden states, H = (h1 , . . . , hn ) with hi ∈ Rdh , in our case using a bidirectional long sho"
W19-4305,W16-3210,0,0.0742609,"Missing"
W19-4305,W18-6309,0,0.525152,"uage-independent representation based on 2 Model Architecture Our architecture follows the standard setup of an encoder-decoder model of machine translation with a traditional attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). However, to enable multilingual training we augment the network with language-specific encoders and decoders trainable with a language-rotating scheduler (Dong et al., 2015; Schwenk and Douze, 2017). We also incorporate a self-attention layer (attention bridge), shared among all language pairs, to serve as a language-agnostic layer (C´ıfka and Bojar, 2018; Lu et al., 2018) Attention bridge: Each encoder takes as input 33 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 33–39 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 3 a sequence of tokens (x1 , . . . , xn ) and produces n dh –dimensional hidden states, H = (h1 , . . . , hn ) with hi ∈ Rdh , in our case using a bidirectional long short-term memory (LSTM) (Graves and Schmidhuber, 2005)1 . Next, we encode this variable length sentence-embedding matrix H into a fixed size M ∈ Rdh ×k capable of focusing on k different components of the"
W19-4305,N16-1101,0,0.0382277,"ingual neural machine translation (NMT) using this model and scheduled training. We have tested the approach in a systematic way with a multi-parallel data set. The model achieves substantial improvements over strong bilingual models and performs well for zero-shot translation, which demonstrates its ability of abstraction and transfer learning. 1 Introduction Neural machine translation (NMT) provides an ideal setting for multilingual MT because it can efficiently share model parameters and take advantage of the various similarities found by the model in the hidden layers and word embeddings (Firat et al., 2016a; Johnson et al., 2017; Blackwood et al., 2018). Furthermore, multilingual NMT has the potential of considerably improving the performance of neural translation systems for low-resource languages (Lakew et al., 2017) and enables zero-shot translation, i.e., translating between language pairs that were not seen during training (Firat et al., 2016b; Johnson et al., 2017). For this study we focus on models for multilingual translation that learn language-agnostic representations, where we outline the development of a language-independent representation based on 2 Model Architecture Our architect"
W19-4305,D15-1166,0,0.401942,"ural translation systems for low-resource languages (Lakew et al., 2017) and enables zero-shot translation, i.e., translating between language pairs that were not seen during training (Firat et al., 2016b; Johnson et al., 2017). For this study we focus on models for multilingual translation that learn language-agnostic representations, where we outline the development of a language-independent representation based on 2 Model Architecture Our architecture follows the standard setup of an encoder-decoder model of machine translation with a traditional attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). However, to enable multilingual training we augment the network with language-specific encoders and decoders trainable with a language-rotating scheduler (Dong et al., 2015; Schwenk and Douze, 2017). We also incorporate a self-attention layer (attention bridge), shared among all language pairs, to serve as a language-agnostic layer (C´ıfka and Bojar, 2018; Lu et al., 2018) Attention bridge: Each encoder takes as input 33 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 33–39 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguis"
W19-4305,D16-1026,0,0.0792914,"Missing"
W19-4305,D14-1162,0,0.0835568,"r model to downstream tasks collected in the SentEval toolkit (Conneau and Kiela, 2018) to evaluate the quality of our language-agnostic sentence embeddings. We run each experiment with five different seeds, and present the average of these scores in Table 2, where we compare our bilingual models against a baseline consisting of the best score achieved by the bilingual models with attention bridge. Since our models were trained on limited data and are not directly comparable to models trained on large-scale data sets, for comparison purposes we present results obtained with GloVe-BoW vectors (Pennington et al., 2014) trained with the same BPE-encoded data as the models. The sentence embeddings produced by the multilingual models show consistent improvements, for the classification tasks of the SentEval collection, with only two exceptions. Moreover, our many-to-many model obtains better results in the 80.76 10.02 32.14 40.12 57.41 67.61 68.55 70.01 49.90 61.38 84.76 9.56 33.05 44.04 58.35 69.36 69.67 72.19 49.46 60.57 Table 2: Scores obtained in the SentEval tasks. The BASELINE column reports the best score among the bilingual models + att bridge. Green cells indicate the highest score. All tasks show the"
W19-4305,W18-6319,0,0.0202881,"ze M ∈ Rdh ×k capable of focusing on k different components of the sentence (Lin et al., 2017; Chen et al., 2018; C´ıfka and Bojar, 2018), using self-attention as follows: A = sof tmax W2 ReLU(W1 H T )  M = AH We conducted four translation experiments and tested the learned sentence representations via downstream tasks. We used the multi30k dataset (Elliott et al., 2016) for training and validation in all available languages: Czech, German, French and English, and tested the trained model with the flickr 2016 test data of the same dataset and obtained BLEU scores using the sacreBLEU script2 (Post, 2018). We lowercased, normalized and tokenized using the Moses toolkit (Koehn et al., 2007), and applied a 10K-operations Byte Pair Encoding (BPE) model per language (Sennrich et al., 2016). Each encoder consists of 2 stacked BiLSTMs of size dh = 512, i.e., the hidden states per direction are of size 256. Each decoder includes 2 stacked unidirectional LSTMs with hidden states of size 512. For the model input and output, the word embeddings have dimension dx = dy = 512. We used an attention bridge layer with 10 attention heads with dw = 1024, the dimensions of W1 and W2 from Eq. (1). We chose k = 10"
W19-4305,W19-4304,1,0.814733,"Missing"
W19-4305,W17-2619,0,0.0260417,"l., 2016b; Johnson et al., 2017). For this study we focus on models for multilingual translation that learn language-agnostic representations, where we outline the development of a language-independent representation based on 2 Model Architecture Our architecture follows the standard setup of an encoder-decoder model of machine translation with a traditional attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). However, to enable multilingual training we augment the network with language-specific encoders and decoders trainable with a language-rotating scheduler (Dong et al., 2015; Schwenk and Douze, 2017). We also incorporate a self-attention layer (attention bridge), shared among all language pairs, to serve as a language-agnostic layer (C´ıfka and Bojar, 2018; Lu et al., 2018) Attention bridge: Each encoder takes as input 33 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 33–39 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 3 a sequence of tokens (x1 , . . . , xn ) and produces n dh –dimensional hidden states, H = (h1 , . . . , hn ) with hi ∈ Rdh , in our case using a bidirectional long short-term memory (LSTM) (Gra"
W19-4305,P17-4012,0,0.0365558,"yer with 10 attention heads with dw = 1024, the dimensions of W1 and W2 from Eq. (1). We chose k = 10 because the mean length of a preprocessed sentence in the training data is 13.2 tokens in our case. Choosing a much smaller k would create a bottleneck in the flow of information, and a bigger one would make the model slower and prone to overfitting (Raganato et al., 2019). We used a Stochastic Gradient Descent (SGD) optimizer with a learning rate of 1.0 and batch size 64, and selected the best model on the development set for each experiment. We implemented our model on top of an OpenNMT-py (Klein et al., 2017) fork, which we make available for reproducibility purposes.3 (1) (2) where W1 ∈ Rdw ×dh and W2 ∈ Rk×dw are weight matrices, with dw a hyper-parameter set arbitrarily, and k is the number of attention heads in the attention bridge. Each decoder follows a common attention mechanism in NMT (Luong et al., 2015), with an initial state computed by mean pooling over M , and using M instead of the hidden states of the encoder for computing the context vector. Penalty term: The attention bridge matrix M from Eq. (2) could learn repetitive information for different attention heads. To address this issu"
W19-4305,P16-1162,0,0.0986588,"= sof tmax W2 ReLU(W1 H T )  M = AH We conducted four translation experiments and tested the learned sentence representations via downstream tasks. We used the multi30k dataset (Elliott et al., 2016) for training and validation in all available languages: Czech, German, French and English, and tested the trained model with the flickr 2016 test data of the same dataset and obtained BLEU scores using the sacreBLEU script2 (Post, 2018). We lowercased, normalized and tokenized using the Moses toolkit (Koehn et al., 2007), and applied a 10K-operations Byte Pair Encoding (BPE) model per language (Sennrich et al., 2016). Each encoder consists of 2 stacked BiLSTMs of size dh = 512, i.e., the hidden states per direction are of size 256. Each decoder includes 2 stacked unidirectional LSTMs with hidden states of size 512. For the model input and output, the word embeddings have dimension dx = dy = 512. We used an attention bridge layer with 10 attention heads with dw = 1024, the dimensions of W1 and W2 from Eq. (1). We chose k = 10 because the mean length of a preprocessed sentence in the training data is 13.2 tokens in our case. Choosing a much smaller k would create a bottleneck in the flow of information, and"
W19-5347,W18-6410,1,0.861487,"Missing"
W19-5347,P18-4020,0,0.0236348,"Missing"
W19-5347,P17-4012,0,0.0294692,"tence-level approaches In this section we describe our sentencelevel translation models and the experiments in the English-to-German, English-to-Finnish and Finnish-to-English translation directions. Table 3: Percentage of lines rejected by each filter for English–Finnish data sets. The strict version is the same as for English–German, and the relax version applies relaxed thresholds. 3.1 Model architectures We experimented with both NMT and rule-based systems. All of our neural sentence-level models are based on the transformer architecture (Vaswani et al., 2017). We used both the OpenNMTpy (Klein et al., 2017) and MarianNMT (Junczysplete sentences that end with proper final punctuation marks, and the filter might remove quite a bit of the useful data examples. However, our fi415 Dowmunt et al., 2018) frameworks. Our experiments focused on the following: BLEU news2018 Single model 5 save-points 5 save-points + 4 fine-tuned • Ensemble models: using ensembles with a combination of independent runs and savepoints from a single training run. 44.61 46.65 47.45 • Left-to-right and right-to-left models: Transformer models with decoding of the output in left-to-right and right-to-left order. Table 4: Englis"
W19-5347,D18-2012,0,0.0289023,"d BPE algorithm is run over the Omorfi-segmented text in order to split low-frequency morphemes. In this experiment, we compare two models for each translation direction: • One model segmented with the standard BPE algorithm (joint vocabulary size of 50 000, vocabulary frequency threshold of 50). English–Finnish and Finnish–English The problem of open-vocabulary translation is particularly acute for morphologically rich languages like Finnish. In recent NMT research, the standard approach consists of applying a word segmentation algorithm such as BPE (Sennrich et al., 2016b) or SentencePiece (Kudo and Richardson, 2018) during pre-processing. In recent WMT editions, various alternative segmentation approaches were examined for Finnish: hybrid models that back off to character-level rep¨ resentations (Ostling et al., 2017), and variants of the Morfessor unsupervised morphology algorithm (Gr¨onroos et al., 2018). This year, we exper• One model where the Finnish side is presegmented with Omorfi, and both the Omorfisegmented Finnish side and the English side are segmented with BPE (same parameters as above). All models are trained on filtered versions of Europarl, ParaCrawl, Rapid, Wikititles, newsdev2015 and ne"
W19-5347,P16-1009,0,0.234209,"sh-to-German (Section 4), and a comparison of different word segmentation approaches for Finnish (Section 3.3). The final submitted NMT systems are summarized in Section 5, while the rule-based machine translation system is described in Section 3.4. 2 Pre-processing • removing non-printing characters, • normalizing punctuation, • tokenization. In addition to these steps, we replaced a number of English contractions with the full form, e.g. “They’re” → “They are”. After the above steps, we applied a Moses truecaser model trained for individual languages, and finally a byte-pair encoding (BPE) (Sennrich et al., 2016b) segmentation using a set of codes for either language pair. For English–German, we initially pre-processed the data using only punctuation normalization and tokenization. We subsequently trained an English truecaser model using all monolingual English data as well as the English side of all parallel English–German datasets except the Rapid corpus (in which non-English characters were missing from a substantial portion of the German sentences). We also repeated the same for German. Afterwards, we used a heuristic cleanup script1 in Pre-processing, data filtering and back-translation It is we"
W19-5347,P16-1162,0,0.441043,"sh-to-German (Section 4), and a comparison of different word segmentation approaches for Finnish (Section 3.3). The final submitted NMT systems are summarized in Section 5, while the rule-based machine translation system is described in Section 3.4. 2 Pre-processing • removing non-printing characters, • normalizing punctuation, • tokenization. In addition to these steps, we replaced a number of English contractions with the full form, e.g. “They’re” → “They are”. After the above steps, we applied a Moses truecaser model trained for individual languages, and finally a byte-pair encoding (BPE) (Sennrich et al., 2016b) segmentation using a set of codes for either language pair. For English–German, we initially pre-processed the data using only punctuation normalization and tokenization. We subsequently trained an English truecaser model using all monolingual English data as well as the English side of all parallel English–German datasets except the Rapid corpus (in which non-English characters were missing from a substantial portion of the German sentences). We also repeated the same for German. Afterwards, we used a heuristic cleanup script1 in Pre-processing, data filtering and back-translation It is we"
W19-5347,P12-3005,0,0.0262377,"rovided parallel training data. This is especially true for the ParaCrawl and Rapid data sets. This is rather unexpected as a basic language identifier certainly must be part of the crawling and extraction pipeline. Nevertheless, after some random inspection of the data, we found it necessary to apply off-the-shelf language identifiers to the data for removing additional erroneous text from the training data. In particular, we applied the Compact Language Detector version 2 (CLD2) from the Google Chrome project (using the Python interface from pycld22 ), and the widely used langid.py package (Lui and Baldwin, 2012) to classify each sentence in the ParaCrawl, CommonCrawl, Rapid and Wikititles data sets. We removed all sentence pairs in which the language of one of the aligned sentences was not reliably detected. For this, we required the correct language ID from both classifiers, the reliable-flag set to “True” by CLD2 with a reliability score of 90 or more, and the detection probability of langid.py to be at least 0.9. Data filtering For data filtering we applied four types of filters: (i) rule-based heuristics, (ii) filters based on language identification, (iii) filters based on word alignment models,"
W19-5347,N19-1313,0,0.0317811,"for 1 epoch. The results for the NMT-HAN model are disappointing. The document-level model performs significantly worse than the sentence-level model. Hierarchical attention models A number of approaches have been developed to utilize the attention mechanism to capture extended context for document-level translation. We experimented with the two following models: • NMT-HAN: Sentence-level transformer model with a hierarchical attention network to capture the document-level context (Miculicich et al., 2018). • selectAttn: Selective attention model for context-aware neural machine translation (Maruf et al., 2019). For testing the selectAttn model, we used the same data with document-level information as we applied in the concatenation models. For NMTHAN we had to use a smaller training set due to lack of resources and due to the implementation not supporting data shards. For NMT-HAN we used only Europarl, NewsCommentary and Rapid for training. Table 11 summarizes the results on the development test data. Both of the tested models need to be trained on sentence-level first, before tuning the document-level components. 5 Model NMT-HAN selectAttn Sentence-level Document-level 35.03 35.26 31.73 34.75 Resu"
W19-5347,D18-1325,0,0.0246673,"uggested in the documentation with respect to optimizers, learning rates and dropout. Unfortunately, the results do not look very promising as we can see in Table 11. The document-level model does not even reach the performance of the sentence-level model even though we trained until convergence on development data with patience of 10 reporting steps, which is quite disappointing. Overall, the scores are below the standard transformer models of the other experiments, and hence, we did not try to further optimize the results using that model. For the NMT-HAN model we used the implementation of Miculicich et al. (2018) with the recommended hyperparameter values and settings. The system is based on the OpenNMT-py implementation of the transformer. The model includes 6 hidden layers on both the encoder and decoder side with a dimensionality of 512 and the multihead attention has 8 attention heads. We applied a sublayer and attention dropout of 0.1. The target and source vocabulary size is 30K. We trained the sentence-level model for 20 epochs after which we further fine-tuned the encoder side hierarchical attention for 1 epoch and the joint encoderdecoder hierarchical attention for 1 epoch. The results for th"
W19-5347,W17-4733,1,0.860954,"BPE algorithm (joint vocabulary size of 50 000, vocabulary frequency threshold of 50). English–Finnish and Finnish–English The problem of open-vocabulary translation is particularly acute for morphologically rich languages like Finnish. In recent NMT research, the standard approach consists of applying a word segmentation algorithm such as BPE (Sennrich et al., 2016b) or SentencePiece (Kudo and Richardson, 2018) during pre-processing. In recent WMT editions, various alternative segmentation approaches were examined for Finnish: hybrid models that back off to character-level rep¨ resentations (Ostling et al., 2017), and variants of the Morfessor unsupervised morphology algorithm (Gr¨onroos et al., 2018). This year, we exper• One model where the Finnish side is presegmented with Omorfi, and both the Omorfisegmented Finnish side and the English side are segmented with BPE (same parameters as above). All models are trained on filtered versions of Europarl, ParaCrawl, Rapid, Wikititles, newsdev2015 and newstest2015 as well as backtranslations. Following our experiments at WMT 5 https://flammie.github.io/ omorfi/pages/usage-examples.html# morphological-segmentation 417 2018 (Raganato et al., 2018), we also u"
W19-5347,W18-6427,0,0.0413463,"Missing"
W19-5347,W17-4811,1,0.87915,"2018. We then test our systems on both the original test set with coherent test data divided into short news documents and the shuffled test set with broken coherence. 4.1 System Baseline 2+1 3+1a 3+1b 1t+1s+1 2+2 BLEU news2018 Shuffled Coherent 38.96 36.62 33.90 34.14 36.82 38.53 38.96 37.17 34.30 34.39 37.24 39.08 Table 10: Comparison of concatenation approaches for English–German document-level translation. Concatenation models Some of the previously published approaches use concatenation of multiple source-side sentences in order to extend the context of the currently translated sentence (Tiedemann and Scherrer, 2017). In addition to the source-side concatenation model, we also tested an approach where we concatenate The results overall are rather disappointing. All but one of the concatenation models underperform and cannot beat the sentence-level baseline. Note that the concat-target model (1t+1s+1) even refers to an oracle experiment in which the reference 420 translation of the previous sentence is fed into the translation model for translating the current source sentence. As this is not very successful, we did not even try to run a proper evaluation with system output provided as target context during"
W19-5347,W15-1844,0,0.0159073,"scored n-best lists. The positive effect of beam search is further illustrated in Figure 1. All previous models were run with a beam size of 12. As we can see, the general trend is that larger beams lead to improved performance, at least until the limit of 64 in our experiments. Beam size 4 is an exception in the left-to-right models. 46.5 46.0 45.5 45.0 44.5 44.0 43.5 43.0 L2R R2L 1 2 4 8 16 Beam size 32 64 Figure 1: The effect of beam size on translation performance. All results use model ensembles and the scores are case-sensitive. imented with rule-based word segmentation based on Omorfi (Pirinen, 2015). Omorfi is a morphological analyzer for Finnish with a large-coverage lexicon. Its segmentation tool5 splits a word form into morphemes as defined by the morphological rules. In particular, it distinguishes prefixes, infixes and suffixes through different segmentation markers: Intia→ ←n ja Japani→ ←n p¨aa¨ → ←ministeri→ India GEN and Japan GEN prime minister ←t tapaa→ ←vat Tokio→ ←ssa PL meet 3 PL Tokyo INE While Omorfi provides word segmentation based on morphological principles, it does not rely on any frequency cues. Therefore, the standard BPE algorithm is run over the Omorfi-segmented te"
W19-5354,W18-6436,0,0.0245897,"th online systems (which do not provide an API for scoring) or with rule-based systems. Second, it is unclear to what extent the score of an MT system reflects its quality, as it might never have generated that particular sentence. Third, it requires the explicit construction of contrastive sentences, which is not trivial, especially for morphologically rich languages. For these reasons, the WMT test suite calls focus on translation test suites, where the participants are asked to produce translations of the source sentence instead of scoring given hypotheses. Following Rios et al. (2018) and Macketanz et al. (2018), who proposed small-scale translation test suites targeting WSD, we participated at WMT with modified versions of M U C OW. The modifications only concern step (3). As a result, we make available two variants of M U C OW, a multilingual contrastive word sense disambiguation test suite for machine translation. The scoring variant covers 11 language pairs with a total of almost 240 000 sentence pairs. The translation variant covers 9 language pairs with a total of 15 600 sentences. The data and scoring scripts are available at https://github. com/Helsinki-NLP/MuCoW. 2 that were aligned at least"
W19-5354,D17-1263,0,0.0781231,"Missing"
W19-5354,K17-1012,0,0.149717,"Missing"
W19-5354,W18-1812,0,0.0539437,"disambiguation test sets for machine translation Alessandro Raganato∗ † , Yves Scherrer∗ and J¨org Tiedemann∗ ∗ University of Helsinki † Basement AI {name.surname}@helsinki.fi Abstract tactic divergences between source and target language (Burchardt et al., 2017; Burlot and Yvon, 2017; Isabelle et al., 2017; Sennrich, 2017; Burlot et al., 2018; Macketanz et al., 2018) or on discourse phenomena (Guillou and Hardmeier, 2016; Bawden et al., 2018; M¨uller et al., 2018; Guillou et al., 2018). Another linguistic phenomenon that is challenging for translation is lexical ambiguity (Liu et al., 2018; Marvin and Koehn, 2018), i.e., words of the source language that have multiple translations in the target language representing different meanings. Recently, Rios Gonzales et al. (2017) introduced a lexical ambiguity benchmark called ContraWSD that is based on contrastive translation pairs: a sentence containing an ambiguous source word is paired with the correct reference translation and with a modified translation in which the ambiguous word has been replaced by a word of a different sense. Contrastive evaluation makes use of the ability of NMT systems to score given translations: a contrast is considered successf"
W19-5354,2005.mtsummit-papers.11,0,0.283026,"Missing"
W19-5354,D18-1512,0,0.0320985,"Missing"
W19-5354,W18-6307,0,0.0773078,"Missing"
W19-5354,L16-1147,1,0.893774,"Missing"
W19-5354,P06-1014,0,0.106146,"lds. An additional manual evaluation was carried out with 50 random German words3 and four settings that obtained high F1 or F0.5 scores. As shown in Table 3, the SW2V method with a threshold set at 0.3 obtained the highest precision value by a large margin and therefore also the best F0.5 score. We chose this setting for all languages. Source words that end up with a single synset as a result of this step are discarded. Step 2b: Refine sense clusters with sense embeddings It is known that lexical resources such as BabelNet tend to suffer from overly fine granularity of their sense inventory (Navigli, 2006; Palmer et al., 2007). We therefore introduce an additional merging step: i) we associate each Babel synset with an embedding, ii) compute pairwise cosine similarities between synsets, iii) and merge them if their embedding similarity is higher than a threshold γ. Choosing a good Babel synset embedding and an optimal threshold is a difficult task. We evaluated three Babel synset vector representations, using the existing German→English ContraWSD test suite as gold standard: Nasari (Camacho-Collados et al., 2016) is a vector representation built by combining the knowledge from Wikipedia and Wo"
W19-5354,N18-1121,0,0.113634,"rastive word sense disambiguation test sets for machine translation Alessandro Raganato∗ † , Yves Scherrer∗ and J¨org Tiedemann∗ ∗ University of Helsinki † Basement AI {name.surname}@helsinki.fi Abstract tactic divergences between source and target language (Burchardt et al., 2017; Burlot and Yvon, 2017; Isabelle et al., 2017; Sennrich, 2017; Burlot et al., 2018; Macketanz et al., 2018) or on discourse phenomena (Guillou and Hardmeier, 2016; Bawden et al., 2018; M¨uller et al., 2018; Guillou et al., 2018). Another linguistic phenomenon that is challenging for translation is lexical ambiguity (Liu et al., 2018; Marvin and Koehn, 2018), i.e., words of the source language that have multiple translations in the target language representing different meanings. Recently, Rios Gonzales et al. (2017) introduced a lexical ambiguity benchmark called ContraWSD that is based on contrastive translation pairs: a sentence containing an ambiguous source word is paired with the correct reference translation and with a modified translation in which the ambiguous word has been replaced by a word of a different sense. Contrastive evaluation makes use of the ability of NMT systems to score given translations: a contra"
W19-5354,P16-1162,0,0.351179,"rget sense of ambiguous source words. Here, we give some baseline results obtained with supervised NMT systems. Following Rios Gonzales et al. (2017), we score both reference and contrastive translations with the same NMT system. A correct decision is detected when the score of the reference is higher than the scores from all contrastive translations. The final test suite score corresponds to the accuracy over all decisions. Three models are examined for German→English: a 6-layer bi-LSTM model and a Transformer model4 trained on the provided training data from WMT17 plus backtranslations from Sennrich et al. (2016b), and the University of Edinburgh’s WMT17 submission, a deep LSTM model with additional synthetic data trained with Nematus (Sennrich et al., 2017b).5 The upper half of Table 5 reports ContraWSD Statistics We apply the three steps presented above to all toEnglish translation directions that were part of the Conference of Machine Translation (WMT) news translation task over the last years. Table 4 summarizes the statistics of these resources. The average number of senses per source word ranges between 2.0 and 2.11 (2.36–2.4 for ContraWSD). The lexicons for the Baltic languages are small due t"
W19-5354,W18-6456,0,0.0397964,"nd contrastive translations with pretrained NMT models, and as translation test suite for the WMT19 news shared task. We find that state-of-the-art and fine-tuned NMT systems still present some drawbacks on handling ambiguous words, especially when evaluated on out-of-domain data and when the encoder has to deal with a morphologically rich language. It will be particularly instructive to see how well the WSD test suite results correlate with human evaluation scores and with recently proposed evaluation metrics that are based on semantic representations of the translations (Gupta et al., 2015; Shimanaka et al., 2018). As future work we plan to further extend the test suite including more languages and parallel data, and make use of the contrastive sentences as adversarial examples during training. The Prague Bulletin of Mathematical Linguistics, 108:159–170. Franck Burlot, Yves Scherrer, Vinit Ravishankar, Ondˇrej Bojar, Stig-Arne Gr¨onroos, Maarit Koponen, Tommi Nieminen, and Franc¸ois Yvon. 2018. The WMT’18 morpheval test suites for English-Czech, English-German, English-Finnish and Turkish-English. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 546–560, Belgium"
W19-5354,W18-6437,0,0.115004,"Missing"
W19-5354,K17-3009,0,0.0299513,"Missing"
W19-5354,W17-4702,0,0.577585,"}@helsinki.fi Abstract tactic divergences between source and target language (Burchardt et al., 2017; Burlot and Yvon, 2017; Isabelle et al., 2017; Sennrich, 2017; Burlot et al., 2018; Macketanz et al., 2018) or on discourse phenomena (Guillou and Hardmeier, 2016; Bawden et al., 2018; M¨uller et al., 2018; Guillou et al., 2018). Another linguistic phenomenon that is challenging for translation is lexical ambiguity (Liu et al., 2018; Marvin and Koehn, 2018), i.e., words of the source language that have multiple translations in the target language representing different meanings. Recently, Rios Gonzales et al. (2017) introduced a lexical ambiguity benchmark called ContraWSD that is based on contrastive translation pairs: a sentence containing an ambiguous source word is paired with the correct reference translation and with a modified translation in which the ambiguous word has been replaced by a word of a different sense. Contrastive evaluation makes use of the ability of NMT systems to score given translations: a contrast is considered successfully detected if the reference translation obtains a higher score than an artificially modified translation. However, all these test suites require significant am"
W19-5354,W18-6304,0,0.0758876,"ransformer, we use the base version (Vaswani et al., 2017). 5 data.statmt.org/wmt17_systems/ Measuring machine translation WSD capability with M U C OW The aim of M U C OW is to examine the ability of current machine translation systems to choose the 473 and M U C OW accuracy scores as well as BLEU scores computed on the WMT17 test set. The ranking of the three models is consistent across the three tasks. Interestingly, the Transformer model (trained on far less data than the Nematus model) scores much better on the two test suites than the BLEU score would suggest, confirming the findings by Tang et al. (2018). The University of Edinburgh also makes available their NMT models for other WMT16 and WMT17 language pairs.6 M U C OW accuracy scores of these models are shown in the lower half of Table 5 together with the WMT test set BLEU scores reported by the authors (Sennrich et al., 2016a, 2017a). Even though we only assess the confidence of an NMT system in detecting the right sense of a single word within a sentence, the results show that WSD is still an issue in MT – even in stateof-the-art-systems – that requires further study. 4 Language Source Target In-dom Out-dom Senpair words synsets synsets"
W19-5354,E17-2060,0,0.0888622,"Missing"
W19-5354,tiedemann-2012-parallel,1,0.749947,"sions of M U C OW. The modifications only concern step (3). As a result, we make available two variants of M U C OW, a multilingual contrastive word sense disambiguation test suite for machine translation. The scoring variant covers 11 language pairs with a total of almost 240 000 sentence pairs. The translation variant covers 9 language pairs with a total of 15 600 sentences. The data and scoring scripts are available at https://github. com/Helsinki-NLP/MuCoW. 2 that were aligned at least 10 times each with at least two distinct target words. We use parallel corpora from the OPUS collection (Tiedemann, 2012),1 counting only one-to-one word alignment links. Table 1 provides an example. 2.2 Step 2a: Cluster target words via BabelNet For each source word of the previous step, those target words that potentially share the same meaning (for example synonyms) are clustered together. To this end, we exploit BabelNet (Navigli and Ponzetto, 2012), a wide-coverage multilingual encyclopedic dictionary obtained automatically from various resources (WordNet and Wikipedia, among others). BabelNet 4.0 covers 284 languages with almost 16 million entries, called Babel synsets. Each entry represents a given meanin"
W19-5354,W18-6312,0,0.0173874,"uage pairs presented in the WMT19 news shared translation task, plus on other 5 language pairs using pretrained NMT models. The M U C OW test suite is available at http://github. com/Helsinki-NLP/MuCoW. 1 Introduction Neural Machine Translation (NMT) has provided impressive advances in translation quality, leading to a discussion whether translations produced by professional human translators can still be distinguished from the output of NMT systems, and to what extent automatic evaluation measures can reliably account for these differences (Hassan Awadalla et al., 2018; L¨aubli et al., 2018; Toral et al., 2018). One answer to this question lies in the development of so-called test suites (Burchardt et al., 2017) or challenge sets (Isabelle et al., 2017) that focus on particular linguistic phenomena that are known to be difficult to evaluate with simple reference-based metrics such as BLEU. Existing test suites focus e.g. on morphosyntactic and syn470 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 470–480 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 177 50 29 28 27 26 input typing entering entry load"
W19-5354,E17-3017,0,0.063791,"Missing"
W19-5354,W16-2323,0,0.112409,"rget sense of ambiguous source words. Here, we give some baseline results obtained with supervised NMT systems. Following Rios Gonzales et al. (2017), we score both reference and contrastive translations with the same NMT system. A correct decision is detected when the score of the reference is higher than the scores from all contrastive translations. The final test suite score corresponds to the accuracy over all decisions. Three models are examined for German→English: a 6-layer bi-LSTM model and a Transformer model4 trained on the provided training data from WMT17 plus backtranslations from Sennrich et al. (2016b), and the University of Edinburgh’s WMT17 submission, a deep LSTM model with additional synthetic data trained with Nematus (Sennrich et al., 2017b).5 The upper half of Table 5 reports ContraWSD Statistics We apply the three steps presented above to all toEnglish translation directions that were part of the Conference of Machine Translation (WMT) news translation task over the last years. Table 4 summarizes the statistics of these resources. The average number of senses per source word ranges between 2.0 and 2.11 (2.36–2.4 for ContraWSD). The lexicons for the Baltic languages are small due t"
W19-5354,P16-1009,0,0.265498,"rget sense of ambiguous source words. Here, we give some baseline results obtained with supervised NMT systems. Following Rios Gonzales et al. (2017), we score both reference and contrastive translations with the same NMT system. A correct decision is detected when the score of the reference is higher than the scores from all contrastive translations. The final test suite score corresponds to the accuracy over all decisions. Three models are examined for German→English: a 6-layer bi-LSTM model and a Transformer model4 trained on the provided training data from WMT17 plus backtranslations from Sennrich et al. (2016b), and the University of Edinburgh’s WMT17 submission, a deep LSTM model with additional synthetic data trained with Nematus (Sennrich et al., 2017b).5 The upper half of Table 5 reports ContraWSD Statistics We apply the three steps presented above to all toEnglish translation directions that were part of the Conference of Machine Translation (WMT) news translation task over the last years. Table 4 summarizes the statistics of these resources. The average number of senses per source word ranges between 2.0 and 2.11 (2.36–2.4 for ContraWSD). The lexicons for the Baltic languages are small due t"
W19-5441,W18-2709,0,0.0575539,"parallel and monolingual corpora were provided for each language pair. We used the parallel datasets to train some of our scoring systems2 . Some descriptive Data-driven methodologies define the state of the art in a wide variety of language processing tasks. The availability of well-formed, clean data varies from language to language, and finding such data in sufficient amounts can prove challenging for some of the lower-resourced languages. In particular, the increasingly common neural machine translation systems are highly sensitive to the quality as well as the quantity of training data (Khayrallah and Koehn, 2018), which creates an impediment to achieving good-quality translations in a low-resource scenario. The web is a massive resource for text data in a wide array of languages. However, it is costly to manually extract high-quality parallel samples from the web, and automatically-crawled datasets such as the ParaCrawl Corpus1 are typically quite noisy. Designing automatic methods to select high-quality aligned samples from noisy parallel corpora can therefore make crawling the web a more viable option for compiling useful training data. To emphasize this untapped potential, Koehn et al. (2018) propo"
W19-5441,P07-2045,0,0.00544495,"xpectation maximization methods commonly used for training. This method is thus able to train and align large quantities of data in a small amount of time. Second, this software allows us to load model priors, a feature we use to initialize the aligner with previously stored model parameters. This is handy for our filtering needs, as we can now train a model on clean parallel data and apply that model to estimate alignment probabilities of noisy data sets. For obtaining model priors, we use the cleaned training data described above, tokenized with the generic tokenizer from the Moses toolkit (Koehn et al., 2007). We cut all words at 10 characters to improve statistics and training efficiency. With this, we train for both language pairs a Bayesian HMM alignment model with fertilities in both directions, and estimate the model priors from the symmetrized alignment. We then use those priors to run the alignment of the noisy datasets using only a single iteration of the final model to avoid a strong influence of the noisy data on alignment parameters. As it is intractable to estimate a fully normalized conditional probability of a sentence pair under the given higher-level word alignment model, eflomal e"
W19-5441,P12-3005,0,0.0321116,"e same sequence. Sinhala uses the same Western Arabic numerals used in the Latin alphabet. Nepali uses Devanagari numerals, following the same decimal system as Western Arabic numerals. This filter takes that into account, and first converts those to digits between [0, 9]. After numeric normalization, the filter extracts sequences of numerals from each Language identifiers. A third filter applies offthe-shelf language identifiers. In particular, we use the Python interface of the Compact Language Detector5 version 2 (CLD2) from the Google Chrome project, and the widely used langid.py package (Lui and Baldwin, 2012), to classify each sen4 VariKN is available from https://vsiivola. github.io/variKN/ 5 The Python implementation of CLD2 is available at https://github.com/aboSamoor/pycld2 296 pair of sentences, preserving their relative order. Considering that a leading zero can be omitted in some numeric sequences such as in dates and numbered lists, the digit ‘0’ is ignored. Finally, the score is calculated as a similarity measure between the extracted sequences in the range [0, 1] using SequenceMatcher.ratio() from Python’s difflib. 0.12 0.10 0.08 0.06 0.04 0.02 0.00 Clean-corpus filter Finally, we use th"
W19-5441,P16-1162,0,0.00850096,"e difference between the cross-entropy of aligned sentences is low as well. The intuition is that both models should be roughly similarly surprised when observing sentences that are translations of each other. In order to make the values comparable, we trained our language models on parallel data sets. As both training data sets are rather small, and as we aim for an efficient and cheap filter, we chose a traditional n-gram language model. To further avoid data sparseness and to improve comparability between source and target languages, we also base our language models on BPE-segmented texts (Sennrich et al., 2016) using a BPE model trained on the cleaned parallel data set with 37k merge operations per language. VariKN 4 (Siivola et al., 2007b,a) is the perfect toolkit for the purpose of estimating n-gram language models with subword units. It implements Kneser-Ney growing and revised KneserNey pruning methods with the support of n-grams of varying size and the estimation of word likelihoods from text segmented into subword units. In our case, we set the maximum n-gram size to 20, and a pruning threshold of 0.002. Finally, we compute cross-entropies for each sentence in the noisy parallel training data,"
W19-6129,N19-1423,0,0.0297797,"ll dataset is available for download here: https://github.com/ Helsinki-NLP/prosody. Although not discussed in this paper, the described acoustic annotation and text-based prediction methods can be applied to prosodic boundaries too, and the boundary labels will be included in the dataset at a later stage. 4 Experiments In this section we describe the experimental setup and the results from our experiments in predicting discrete prosodic prominence labels from text using the corpus described above. 4.1 Experimental Setup We performed experiments with the following models: • BERT-base uncased (Devlin et al., 2019) • 3-layer 600D Bidirectional Long Short-Term Memory (BiLSTM) (Hochreiter and Schmidhuber, 1997) • Minitagger (SVM) (Stratos and Collins, 2015) + GloVe (Pennington et al., 2014) • MarMoT (CRF) (Mueller et al., 2013) • Majority class per word The models were selected so that they cover a wide variety of different architectures from feature-based statistical approaches to neural networks and pre-trained language models. The models are described in more detail below. We use the Huggingface PyTorch implementation of BERT available in the pytorch transformers library,3 which 3 https://github.com/hu"
W19-6129,P04-1086,0,0.102109,"ext only. 2.2 Generating Prominence Annotations Throughout the literature a number of methods have been proposed for the labeling of prosodic prominence. These methods can be roughly categorized on the basis of the need for training data (manual prosodic annotations) into supervised and unsupervised, but crucially, on the basis of the information they utilize from speech and language to generate their predictions (prominence labels). As prominence perception has been found to correlate with acoustic-phonetic features (Lieberman, 1960), with the constituent syntactic structure of an utterance (Gregory and Altun, 2004; Wagner and Watson, 2010; Bresnan, 1973), with the frequency of occurrence of individual lexical items (Nenkova et al., 2007; Jurafsky et al., 2001), and with the probabilities of contiguous lexical sequences (Jurafsky, 1996), automatic methods have been developed utilizing these features either in combination or independently (Nenkova et al., 2007; Kakouros et al., 2016; Ostendorf et al., 1995; Levow, 2008). Overall, these features can be largely divided into two categories: (i) acoustic (derived from the sound pressure waveform of the speech signal) and (ii) language (extracted by studying"
W19-6129,I08-1029,0,0.0381727,"nence labels). As prominence perception has been found to correlate with acoustic-phonetic features (Lieberman, 1960), with the constituent syntactic structure of an utterance (Gregory and Altun, 2004; Wagner and Watson, 2010; Bresnan, 1973), with the frequency of occurrence of individual lexical items (Nenkova et al., 2007; Jurafsky et al., 2001), and with the probabilities of contiguous lexical sequences (Jurafsky, 1996), automatic methods have been developed utilizing these features either in combination or independently (Nenkova et al., 2007; Kakouros et al., 2016; Ostendorf et al., 1995; Levow, 2008). Overall, these features can be largely divided into two categories: (i) acoustic (derived from the sound pressure waveform of the speech signal) and (ii) language (extracted by studying the form of the language; for instance, semantic or syntactic factors in the language). Both acoustic and language-based features have been shown to provide good overall performance in detecting prominence (in both supervised and unsupervised cases), where, however, the methods utilizing acoustic features seem to provide better performance for the unsupervised detection of prominences in speech (Suni et al.,"
W19-6129,P03-1062,0,0.206322,"Missing"
W19-6129,D13-1032,0,0.0443229,"Missing"
W19-6129,N07-1002,0,0.495063,"g of prosodic prominence. These methods can be roughly categorized on the basis of the need for training data (manual prosodic annotations) into supervised and unsupervised, but crucially, on the basis of the information they utilize from speech and language to generate their predictions (prominence labels). As prominence perception has been found to correlate with acoustic-phonetic features (Lieberman, 1960), with the constituent syntactic structure of an utterance (Gregory and Altun, 2004; Wagner and Watson, 2010; Bresnan, 1973), with the frequency of occurrence of individual lexical items (Nenkova et al., 2007; Jurafsky et al., 2001), and with the probabilities of contiguous lexical sequences (Jurafsky, 1996), automatic methods have been developed utilizing these features either in combination or independently (Nenkova et al., 2007; Kakouros et al., 2016; Ostendorf et al., 1995; Levow, 2008). Overall, these features can be largely divided into two categories: (i) acoustic (derived from the sound pressure waveform of the speech signal) and (ii) language (extracted by studying the form of the language; for instance, semantic or syntactic factors in the language). Both acoustic and language-based feat"
W19-6129,D14-1162,0,0.0853366,"rediction methods can be applied to prosodic boundaries too, and the boundary labels will be included in the dataset at a later stage. 4 Experiments In this section we describe the experimental setup and the results from our experiments in predicting discrete prosodic prominence labels from text using the corpus described above. 4.1 Experimental Setup We performed experiments with the following models: • BERT-base uncased (Devlin et al., 2019) • 3-layer 600D Bidirectional Long Short-Term Memory (BiLSTM) (Hochreiter and Schmidhuber, 1997) • Minitagger (SVM) (Stratos and Collins, 2015) + GloVe (Pennington et al., 2014) • MarMoT (CRF) (Mueller et al., 2013) • Majority class per word The models were selected so that they cover a wide variety of different architectures from feature-based statistical approaches to neural networks and pre-trained language models. The models are described in more detail below. We use the Huggingface PyTorch implementation of BERT available in the pytorch transformers library,3 which 3 https://github.com/huggingface/ we further fine-tune during training. We take the last hidden layer of BERT and train a single fully-connected classifier layer on top of it, mapping the representati"
W19-6129,W15-1511,0,0.0910891,"acoustic annotation and text-based prediction methods can be applied to prosodic boundaries too, and the boundary labels will be included in the dataset at a later stage. 4 Experiments In this section we describe the experimental setup and the results from our experiments in predicting discrete prosodic prominence labels from text using the corpus described above. 4.1 Experimental Setup We performed experiments with the following models: • BERT-base uncased (Devlin et al., 2019) • 3-layer 600D Bidirectional Long Short-Term Memory (BiLSTM) (Hochreiter and Schmidhuber, 1997) • Minitagger (SVM) (Stratos and Collins, 2015) + GloVe (Pennington et al., 2014) • MarMoT (CRF) (Mueller et al., 2013) • Majority class per word The models were selected so that they cover a wide variety of different architectures from feature-based statistical approaches to neural networks and pre-trained language models. The models are described in more detail below. We use the Huggingface PyTorch implementation of BERT available in the pytorch transformers library,3 which 3 https://github.com/huggingface/ we further fine-tune during training. We take the last hidden layer of BERT and train a single fully-connected classifier layer on t"
W19-6146,J82-2005,0,0.707686,"Missing"
W98-1613,J96-1001,0,\N,Missing
W98-1613,J93-1004,0,\N,Missing
W98-1613,W96-0201,0,\N,Missing
W98-1613,J93-1007,0,\N,Missing
W98-1613,C94-2178,0,\N,Missing
W98-1613,P93-1001,0,\N,Missing
W98-1613,W98-1615,0,\N,Missing
W99-0626,W98-1615,0,0.0421543,"everal purposes in text-processing. One task is the extraction of cognates from bilingual text. In this paper three approaches to the automatic generation of language dependent string matching functions are presented. 1 Introduction String similarity metrics are extensively used in the processing of textual data for several purposes such as the detection and correction of spelling errors (Kukich, 1992), for sentence and word alignments (Church, 1993; Simard et al., 1992; Melamed, 1995), and the extraction of information from monolingnal as well as multi-lingual text (Resnik and Melamed, 1997; Borin, 1998; Tiedemann, 1998a). One important task is the identification of so-called cognates, token pairs with a significant similarity between them, in bilingual text. A commonly used technique for measuring string similarity is to look for the longest common subsequence (LCS) of characters in two strings; the characters in this sequence do not necessarily need to be contiguous in the original strings (Wagner and Fischer, 1974; Stephen, 1992). The length of the LCS is usually divided by the length of the longer string of the two original tokens in order to obtain a normalized value. This score is call"
W99-0626,P93-1001,0,0.0138686,"ied to word pairs from a Swedish/English text corpus and experimental results are presented for each of them. Abstract String similarity metrics are used for several purposes in text-processing. One task is the extraction of cognates from bilingual text. In this paper three approaches to the automatic generation of language dependent string matching functions are presented. 1 Introduction String similarity metrics are extensively used in the processing of textual data for several purposes such as the detection and correction of spelling errors (Kukich, 1992), for sentence and word alignments (Church, 1993; Simard et al., 1992; Melamed, 1995), and the extraction of information from monolingnal as well as multi-lingual text (Resnik and Melamed, 1997; Borin, 1998; Tiedemann, 1998a). One important task is the identification of so-called cognates, token pairs with a significant similarity between them, in bilingual text. A commonly used technique for measuring string similarity is to look for the longest common subsequence (LCS) of characters in two strings; the characters in this sequence do not necessarily need to be contiguous in the original strings (Wagner and Fischer, 1974; Stephen, 1992). Th"
W99-0626,W95-0115,0,0.0993144,"glish text corpus and experimental results are presented for each of them. Abstract String similarity metrics are used for several purposes in text-processing. One task is the extraction of cognates from bilingual text. In this paper three approaches to the automatic generation of language dependent string matching functions are presented. 1 Introduction String similarity metrics are extensively used in the processing of textual data for several purposes such as the detection and correction of spelling errors (Kukich, 1992), for sentence and word alignments (Church, 1993; Simard et al., 1992; Melamed, 1995), and the extraction of information from monolingnal as well as multi-lingual text (Resnik and Melamed, 1997; Borin, 1998; Tiedemann, 1998a). One important task is the identification of so-called cognates, token pairs with a significant similarity between them, in bilingual text. A commonly used technique for measuring string similarity is to look for the longest common subsequence (LCS) of characters in two strings; the characters in this sequence do not necessarily need to be contiguous in the original strings (Wagner and Fischer, 1974; Stephen, 1992). The length of the LCS is usually divide"
W99-0626,A97-1050,0,0.166592,"ity metrics are used for several purposes in text-processing. One task is the extraction of cognates from bilingual text. In this paper three approaches to the automatic generation of language dependent string matching functions are presented. 1 Introduction String similarity metrics are extensively used in the processing of textual data for several purposes such as the detection and correction of spelling errors (Kukich, 1992), for sentence and word alignments (Church, 1993; Simard et al., 1992; Melamed, 1995), and the extraction of information from monolingnal as well as multi-lingual text (Resnik and Melamed, 1997; Borin, 1998; Tiedemann, 1998a). One important task is the identification of so-called cognates, token pairs with a significant similarity between them, in bilingual text. A commonly used technique for measuring string similarity is to look for the longest common subsequence (LCS) of characters in two strings; the characters in this sequence do not necessarily need to be contiguous in the original strings (Wagner and Fischer, 1974; Stephen, 1992). The length of the LCS is usually divided by the length of the longer string of the two original tokens in order to obtain a normalized value. This"
W99-0626,1992.tmi-1.7,0,0.161221,"irs from a Swedish/English text corpus and experimental results are presented for each of them. Abstract String similarity metrics are used for several purposes in text-processing. One task is the extraction of cognates from bilingual text. In this paper three approaches to the automatic generation of language dependent string matching functions are presented. 1 Introduction String similarity metrics are extensively used in the processing of textual data for several purposes such as the detection and correction of spelling errors (Kukich, 1992), for sentence and word alignments (Church, 1993; Simard et al., 1992; Melamed, 1995), and the extraction of information from monolingnal as well as multi-lingual text (Resnik and Melamed, 1997; Borin, 1998; Tiedemann, 1998a). One important task is the identification of so-called cognates, token pairs with a significant similarity between them, in bilingual text. A commonly used technique for measuring string similarity is to look for the longest common subsequence (LCS) of characters in two strings; the characters in this sequence do not necessarily need to be contiguous in the original strings (Wagner and Fischer, 1974; Stephen, 1992). The length of the LCS i"
W99-0626,W98-1613,1,0.933489,"es in text-processing. One task is the extraction of cognates from bilingual text. In this paper three approaches to the automatic generation of language dependent string matching functions are presented. 1 Introduction String similarity metrics are extensively used in the processing of textual data for several purposes such as the detection and correction of spelling errors (Kukich, 1992), for sentence and word alignments (Church, 1993; Simard et al., 1992; Melamed, 1995), and the extraction of information from monolingnal as well as multi-lingual text (Resnik and Melamed, 1997; Borin, 1998; Tiedemann, 1998a). One important task is the identification of so-called cognates, token pairs with a significant similarity between them, in bilingual text. A commonly used technique for measuring string similarity is to look for the longest common subsequence (LCS) of characters in two strings; the characters in this sequence do not necessarily need to be contiguous in the original strings (Wagner and Fischer, 1974; Stephen, 1992). The length of the LCS is usually divided by the length of the longer string of the two original tokens in order to obtain a normalized value. This score is called the longest co"
W99-1022,P98-1004,0,0.203907,"renberg et al (1999) and Ahrenberg et al (forthcoming). In particular, each word correspondence in the bitext describes a link instance, or simply a link. A pair of link units that is instantiated in the bitext will be referred to as link type. Word alignment systems usually assume segmented bitext {sentence aligned bitext). Common bitext segments are sentence fragments, sentences, and sequences of sentences that have corresponding units in the translation. Dep>ending on its purpose, a word alignment system attempts to maximize the number of discovered links (-> word instance alignment) (e.g. Ahrenberg et al 1998, Melamed 1999) or the number of extracted link types (-> bilingual lexicon extraction) (e.g Melamed 1995, Resnik and Melamed 1997, Tiedemann 1998a). Lexicon extraction aims at providing correct translations whereas word alignment has to deal with insertions, deletions, and other modifications within the bitext as well. Furthermore, word alignment systems may focus on specific types of link units, e.g. terms (Dagan and Church 1994, van der Eijk 1993) and collocations (Smadja et al 1996). The task of word alignment is not trivial especially because it goes beyond simple oneto-one word correspon"
W99-1022,ahrenberg-etal-2000-evaluation,1,0.884589,"Missing"
W99-1022,W98-1615,0,0.0240596,"nt. 2 prob{S, T) Dice = prob{S) + prob{T) In our case S and T represent the link units in the source and the target language under consideration. The probabilities of S and T to occur in the text, and the probability of both units to co-occur in the same bitext segment (sentence alignment) can be estimated by appropriate frequency counts. Simple stemming functions are used in order to reduce the inflectional variety of words in different languages and to improve the statistical calculations. Proceedings of NODALIDA 1999 221 String similarity can be measured by different metrics (Melamed 1995, Borin 1998). UWA uses the Longest Common Subsequence Ratio (LCSR). UWA applies dynamic programming for computing the length of the longest common subsequence (LCS) of two strings (Stephen 1992). This value, divided by the length of the longer string, provides a measure for string similarity between them. In figure 2, the LCSR calculation is illustrated. In the figure, the application of the algorithm with MWUs is demonstrated as well. Figure 2: The longest common subsequence ratio of ‘see example’ and ‘se exempel’. s e e s 1 1 e 1 X 1 e 1 m 1 p 1 e 1 1 1 e L 2&apos; e X a m P 1 e 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2"
W99-1022,A94-1006,0,0.0349641,"ng units in the translation. Dep>ending on its purpose, a word alignment system attempts to maximize the number of discovered links (-> word instance alignment) (e.g. Ahrenberg et al 1998, Melamed 1999) or the number of extracted link types (-> bilingual lexicon extraction) (e.g Melamed 1995, Resnik and Melamed 1997, Tiedemann 1998a). Lexicon extraction aims at providing correct translations whereas word alignment has to deal with insertions, deletions, and other modifications within the bitext as well. Furthermore, word alignment systems may focus on specific types of link units, e.g. terms (Dagan and Church 1994, van der Eijk 1993) and collocations (Smadja et al 1996). The task of word alignment is not trivial especially because it goes beyond simple oneto-one word correspondences in many cases. Multi-word units (MWUs) have to be handled due to the use of non-compositional compounds, associated idiomatic expressions, multi-word names and so on. The difference in compounding between different languages increases the difficulties with the identification of appropriate Proceedings of NODALIDA 1999, pages 216-227 217 correspondences further. In addition, the text type is decisive for the word alignment p"
W99-1022,E93-1015,0,0.0352783,"Missing"
W99-1022,W97-0311,0,0.116037,"r simply a link. A pair of link units that is instantiated in the bitext will be referred to as link type. Word alignment systems usually assume segmented bitext {sentence aligned bitext). Common bitext segments are sentence fragments, sentences, and sequences of sentences that have corresponding units in the translation. Dep>ending on its purpose, a word alignment system attempts to maximize the number of discovered links (-> word instance alignment) (e.g. Ahrenberg et al 1998, Melamed 1999) or the number of extracted link types (-> bilingual lexicon extraction) (e.g Melamed 1995, Resnik and Melamed 1997, Tiedemann 1998a). Lexicon extraction aims at providing correct translations whereas word alignment has to deal with insertions, deletions, and other modifications within the bitext as well. Furthermore, word alignment systems may focus on specific types of link units, e.g. terms (Dagan and Church 1994, van der Eijk 1993) and collocations (Smadja et al 1996). The task of word alignment is not trivial especially because it goes beyond simple oneto-one word correspondences in many cases. Multi-word units (MWUs) have to be handled due to the use of non-compositional compounds, associated idiomat"
W99-1022,J99-1003,0,0.0286486,"nd Ahrenberg et al (forthcoming). In particular, each word correspondence in the bitext describes a link instance, or simply a link. A pair of link units that is instantiated in the bitext will be referred to as link type. Word alignment systems usually assume segmented bitext {sentence aligned bitext). Common bitext segments are sentence fragments, sentences, and sequences of sentences that have corresponding units in the translation. Dep>ending on its purpose, a word alignment system attempts to maximize the number of discovered links (-> word instance alignment) (e.g. Ahrenberg et al 1998, Melamed 1999) or the number of extracted link types (-> bilingual lexicon extraction) (e.g Melamed 1995, Resnik and Melamed 1997, Tiedemann 1998a). Lexicon extraction aims at providing correct translations whereas word alignment has to deal with insertions, deletions, and other modifications within the bitext as well. Furthermore, word alignment systems may focus on specific types of link units, e.g. terms (Dagan and Church 1994, van der Eijk 1993) and collocations (Smadja et al 1996). The task of word alignment is not trivial especially because it goes beyond simple oneto-one word correspondences in many"
W99-1022,A97-1050,0,0.0223866,"instance, or simply a link. A pair of link units that is instantiated in the bitext will be referred to as link type. Word alignment systems usually assume segmented bitext {sentence aligned bitext). Common bitext segments are sentence fragments, sentences, and sequences of sentences that have corresponding units in the translation. Dep>ending on its purpose, a word alignment system attempts to maximize the number of discovered links (-> word instance alignment) (e.g. Ahrenberg et al 1998, Melamed 1999) or the number of extracted link types (-> bilingual lexicon extraction) (e.g Melamed 1995, Resnik and Melamed 1997, Tiedemann 1998a). Lexicon extraction aims at providing correct translations whereas word alignment has to deal with insertions, deletions, and other modifications within the bitext as well. Furthermore, word alignment systems may focus on specific types of link units, e.g. terms (Dagan and Church 1994, van der Eijk 1993) and collocations (Smadja et al 1996). The task of word alignment is not trivial especially because it goes beyond simple oneto-one word correspondences in many cases. Multi-word units (MWUs) have to be handled due to the use of non-compositional compounds, associated idiomat"
W99-1022,W98-1613,1,0.595967,"k. A pair of link units that is instantiated in the bitext will be referred to as link type. Word alignment systems usually assume segmented bitext {sentence aligned bitext). Common bitext segments are sentence fragments, sentences, and sequences of sentences that have corresponding units in the translation. Dep>ending on its purpose, a word alignment system attempts to maximize the number of discovered links (-> word instance alignment) (e.g. Ahrenberg et al 1998, Melamed 1999) or the number of extracted link types (-> bilingual lexicon extraction) (e.g Melamed 1995, Resnik and Melamed 1997, Tiedemann 1998a). Lexicon extraction aims at providing correct translations whereas word alignment has to deal with insertions, deletions, and other modifications within the bitext as well. Furthermore, word alignment systems may focus on specific types of link units, e.g. terms (Dagan and Church 1994, van der Eijk 1993) and collocations (Smadja et al 1996). The task of word alignment is not trivial especially because it goes beyond simple oneto-one word correspondences in many cases. Multi-word units (MWUs) have to be handled due to the use of non-compositional compounds, associated idiomatic expressions,"
W99-1022,W99-0626,1,0.807383,"the LCSR calculation is illustrated. In the figure, the application of the algorithm with MWUs is demonstrated as well. Figure 2: The longest common subsequence ratio of ‘see example’ and ‘se exempel’. s e e s 1 1 e 1 X 1 e 1 m 1 p 1 e 1 1 1 e L 2&apos; e X a m P 1 e 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 H 3 3 3 3 3 3 3 2 3 3 Wi 2 3 3 4 2 3 3 4 2 3 3 4 2 3 3 4 2 3 3 4 2 3 3 4 4 4 5 5 5 5 5 5 5 5 5 5 5 4 4 5 5 5 5 1*6 6 ■ 6 7 6 7 4 5 5 6 7 7 4; 5 5 6 7 8_ length[LCS(S^,S^)] LCSR = ■ = 8/11 = 0.72 max[length{S^ ),length(S2 )] Further investigations on string similarity metrics have been carried out (Tiedemann 1999) but they have not yet been applied in the word alignment process. Another source of alignment candidates can be found in single word bitext segments. UWA considers each bitext segment with exactly one link unit in one language to be a valid alignment candidate. Low frequency link units cannot be recognised by statistical association scores. However, they represent a large portion of general text corpora. UWA applies a simple heuristic in order to extract alignment candidates of low frequent text units. Assuming two frequency thresholds // and U with /p &lt; the system removes all units that occu"
