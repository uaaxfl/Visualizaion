2021.emnlp-main.728,Towards Label-Agnostic Emotion Embeddings,2021,-1,-1,3,1,426,sven buechel,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Research in emotion analysis is scattered across different label formats (e.g., polarity types, basic emotion categories, and affective dimensions), linguistic levels (word vs. sentence vs. discourse), and, of course, (few well-resourced but much more under-resourced) natural languages and text genres (e.g., product reviews, tweets, news). The resulting heterogeneity makes data and software developed under these conflicting constraints hard to compare and challenging to integrate. To resolve this unsatisfactory state of affairs we here propose a training scheme that learns a shared latent representation of emotion independent from different label formats, natural languages, and even disparate model architectures. Experiments on a wide range of datasets indicate that this approach yields the desired interoperability without penalizing prediction quality. Code and data are archived under DOI 10.5281/zenodo.5466068."
2021.eacl-main.174,Acquiring a Formality-Informed Lexical Resource for Style Analysis,2021,-1,-1,3,1,3383,elisabeth eder,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"To track different levels of formality in written discourse, we introduce a novel type of lexicon for the German language, with entries ordered by their degree of (in)formality. We start with a set of words extracted from traditional lexicographic resources, extend it by sentence-based similarity computations, and let crowdworkers assess the enlarged set of lexical items on a continuous informal-formal scale as a gold standard for evaluation. We submit this lexicon to an intrinsic evaluation related to the best regression models and their effect on predicting formality scores and complement our investigation by an extrinsic evaluation of formality on a German-language email corpus."
2020.lrec-1.122,Allgemeine Musikalische Zeitung as a Searchable Online Corpus,2020,-1,-1,3,0,16864,bernd kampe,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The massive digitization efforts related to historical newspapers over the past decades have focused on mass media sources and ordinary people as their primary recipients. Much less attention has been paid to newspapers published for a more specialized audience, e.g., those aiming at scholarly or cultural exchange within intellectual communities much narrower in scope, such as newspapers devoted to music criticism, arts or philosophy. Only some few of these specialized newspapers have been digitized up until now, but they are usually not well curated in terms of digitization quality, data formatting, completeness, redundancy (de-duplication), supply of metadata, and, hence, searchability. This paper describes our approach to eliminate these drawbacks for a major German-language newspaper resource of the Romantic Age, the Allgemeine Musikalische Zeitung (General Music Gazette). We here focus on a workflow that copes with a posteriori digitization problems, inconsistent OCRing and index building for searchability. In addition, we provide a user-friendly graphic interface to empower content-centric access to this (and other) digital resource(s) adopting open-source software for the purpose of Web presentation."
2020.lrec-1.550,{C}od{E} Alltag 2.0 {---} A Pseudonymized {G}erman-Language Email Corpus,2020,-1,-1,3,1,3383,elisabeth eder,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The vast amount of social communication distributed over various electronic media channels (tweets, blogs, emails, etc.), so-called user-generated content (UGC), creates entirely new opportunities for today{'}s NLP research. Yet, data privacy concerns implied by the unauthorized use of these text streams as a data resource are often neglected. In an attempt to reconciliate the diverging needs of unconstrained raw data use and preservation of data privacy in digital communication, we here investigate the automatic recognition of privacy-sensitive stretches of text in UGC and provide an algorithmic solution for the protection of personal data via pseudonymization. Our focus is directed at the de-identification of emails where personally identifying information does not only refer to the sender but also to those people, locations, dates, and other identifiers mentioned in greetings, boilerplates and the content-carrying body of emails. We evaluate several de-identification procedures and systems on two hitherto non-anonymized German-language email corpora (CodE AlltagS+d and CodE AlltagXL), and generate fully pseudonymized versions for both (CodE Alltag 2.0) in which personally identifying information of all social actors addressed in these mails has been camouflaged (to the greatest extent possible)."
2020.lrec-1.564,"{P}ro{G}ene - A Large-scale, High-Quality Protein-Gene Annotated Benchmark Corpus",2020,-1,-1,4,1,17796,erik faessler,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Genes and proteins constitute the fundamental entities of molecular genetics. We here introduce ProGene (formerly called FSU-PRGE), a corpus that reflects our efforts to cope with this important class of named entities within the framework of a long-lasting large-scale annotation campaign at the Jena University Language {\&} Information Engineering (JULIE) Lab. We assembled the entire corpus from 11 subcorpora covering various biological domains to achieve an overall subdomain-independent corpus. It consists of 3,308 MEDLINE abstracts with over 36k sentences and more than 960k tokens annotated with nearly 60k named entity mentions. Two annotators strove for carefully assigning entity mentions to classes of genes/proteins as well as families/groups, complexes, variants and enumerations of those where genes and proteins are represented by a single class. The main purpose of the corpus is to provide a large body of consistent and reliable annotations for supervised training and evaluation of machine learning algorithms in this relevant domain. Furthermore, we provide an evaluation of two state-of-the-art baseline systems {---} BioBert and flair {---} on the ProGene corpus. We make the evaluation datasets and the trained models available to encourage comparable evaluations of new methods in the future."
2020.louhi-1.5,{GGPONC}: A Corpus of {G}erman Medical Text with Rich Metadata Based on Clinical Practice Guidelines,2020,-1,-1,7,0,18411,florian borchert,Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis,0,"The lack of publicly accessible text corpora is a major obstacle for progress in natural language processing. For medical applications, unfortunately, all language communities other than English are low-resourced. In this work, we present GGPONC (German Guideline Program in Oncology NLP Corpus), a freely dis tributable German language corpus based on clinical practice guidelines for oncology. This corpus is one of the largest ever built from German medical documents. Unlike clinical documents, clinical guidelines do not contain any patient-related information and can therefore be used without data protection restrictions. Moreover, GGPONC is the first corpus for the German language covering diverse conditions in a large medical subfield and provides a variety of metadata, such as literature references and evidence levels. By applying and evaluating existing medical information extraction pipelines for German text, we are able to draw comparisons for the use of medical language to other corpora, medical and non-medical ones."
2020.acl-main.112,Learning and Evaluating Emotion Lexicons for 91 Languages,2020,77,0,3,1,426,sven buechel,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications. Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature. In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language. Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model. Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each. We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables. Code and data are available at https://github.com/JULIELab/MEmoLon archived under DOI 10.5281/zenodo.3779901."
W19-4025,Continuous Quality Control and Advanced Text Segment Annotation with {WAT}-{SL} 2.0,2019,0,0,7,1,17797,christina lohr,Proceedings of the 13th Linguistic Annotation Workshop,0,"Today{'}s widely used annotation tools were designed for annotating typically short textual mentions of entities or relations, making their interface cumbersome to use for long(er) stretches of text, e.g, sentences running over several lines in a document. They also lack systematic support for hierarchically structured labels, i.e., one label being conceptually more general than another (e.g., anamnesis in relation to family anamnesis). Moreover, as a more fundamental shortcoming of today{'}s tools, they provide no continuous quality con trol mechanisms for the annotation process, an essential feature to intrinsically support iterative cycles in the development of annotation guidelines. We alleviated these problems by developing WAT-SL 2.0, an open-source web-based annotation tool for long-segment labeling, hierarchically structured label sets and built-ins for quality control."
W19-3513,At the Lower End of {L}anguage{---}{E}xploring the Vulgar and Obscene Side of {G}erman,2019,-1,-1,3,1,3383,elisabeth eder,Proceedings of the Third Workshop on Abusive Language Online,0,"In this paper, we describe a workflow for the data-driven acquisition and semantic scaling of a lexicon that covers lexical items from the lower end of the German language register{---}terms typically considered as rough, vulgar or obscene. Since the fine semantic representation of grades of obscenity can only inadequately be captured at the categorical level (e.g., obscene vs. non-obscene, or rough vs. vulgar), our main contribution lies in applying best-worst scaling, a rating methodology that has already been shown to be useful for emotional language, to capture the relative strength of obscenity of lexical items. We describe the empirical foundations for bootstrapping such a low-end lexicon for German by starting from manually supplied lexicographic categorizations of a small seed set of rough and vulgar lexical items and automatically enlarging this set by means of distributional semantics. We then determine the degrees of obscenity for the full set of all acquired lexical items by letting crowdworkers comparatively assess their pejorative grade using best-worst scaling. This semi-automatically enriched lexicon already comprises 3,300 lexical items and incorporates 33,000 vulgarity ratings. Using it as a seed lexicon for fully automatic lexical acquisition, we were able to raise its coverage up to slightly more than 11,000 entries."
W19-2501,Modeling Word Emotion in Historical Language: Quantity Beats Supposed Stability in Seed Word Selection,2019,0,2,3,1,24319,johannes hellrich,"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"To understand historical texts, we must be aware that language{---}including the emotional connotation attached to words{---}changes over time. In this paper, we aim at estimating the emotion which is associated with a given word in former language stages of English and German. Emotion is represented following the popular Valence-Arousal-Dominance (VAD) annotation scheme. While being more expressive than polarity alone, existing word emotion induction methods are typically not suited for addressing it. To overcome this limitation, we present adaptations of two popular algorithms to VAD. To measure their effectiveness in diachronic settings, we present the first gold standard for historical word emotions, which was created by scholars with proficiency in the respective language stages and covers both English and German. In contrast to claims in previous work, our findings indicate that hand-selecting small sets of seed words with supposedly stable emotional meaning is actually harm- rather than helpful."
W19-2003,The Influence of Down-Sampling Strategies on {SVD} Word Embedding Stability,2019,-1,-1,3,1,24319,johannes hellrich,Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for {NLP},0,"The stability of word embedding algorithms, i.e., the consistency of the word representations they reveal when trained repeatedly on the same data set, has recently raised concerns. We here compare word embedding algorithms on three corpora of different sizes, and evaluate both their stability and accuracy. We find strong evidence that down-sampling strategies (used as part of their training procedures) are particularly influential for the stability of SVD-PPMI-type embeddings. This finding seems to explain diverging reports on their stability and lead us to a simple modification which provides superior stability as well as accuracy on par with skip-gram embedding"
R19-1030,De-Identification of Emails: Pseudonymizing Privacy-Sensitive Data in a {G}erman Email Corpus,2019,0,0,3,1,3383,elisabeth eder,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"We deal with the pseudonymization of those stretches of text in emails that might allow to identify real individual persons. This task is decomposed into two steps. First, named entities carrying privacy-sensitive information (e.g., names of persons, locations, phone numbers or dates) are identified, and, second, these privacy-bearing entities are replaced by synthetically generated surrogates (e.g., a person originally named {`}John Doe{'} is renamed as {`}Bill Powers{'}). We describe a system architecture for surrogate generation and evaluate our approach on CodeAlltag, a German email corpus."
D19-5103,A Time Series Analysis of Emotional Loading in Central Bank Statements,2019,23,0,5,1,426,sven buechel,Proceedings of the Second Workshop on Economics and Natural Language Processing,0,"We examine the affective content of central bank press statements using emotion analysis. Our focus is on two major international players, the European Central Bank (ECB) and the US Federal Reserve Bank (Fed), covering a time span from 1998 through 2019. We reveal characteristic patterns in the emotional dimensions of valence, arousal, and dominance and find{---}despite the commonly established attitude that emotional wording in central bank communication should be avoided{---}a correlation between the state of the economy and particularly the dominance dimension in the press releases under scrutiny and, overall, an impact of the president in office."
W18-3103,A Corpus of Corporate Annual and Social Responsibility Reports: 280 Million Tokens of Balanced Organizational Writing,2018,-1,-1,7,0,28360,sebastian handschke,Proceedings of the First Workshop on Economics and Natural Language Processing,0,"We introduce JOCo, a novel text corpus for NLP analytics in the field of economics, business and management. This corpus is composed of corporate annual and social responsibility reports of the top 30 US, UK and German companies in the major (DJIA, FTSE 100, DAX), middle-sized (S{\&}P 500, FTSE 250, MDAX) and technology (NASDAQ, FTSE AIM 100, TECDAX) stock indices, respectively. Altogether, this adds up to 5,000 reports from 270 companies headquartered in three of the world{'}s most important economies. The corpus spans a time frame from 2000 up to 2015 and contains, in total, 282M tokens. We also feature JOCo in a small-scale experiment to demonstrate its potential for NLP-fueled studies in economics, business and management research."
N18-1173,Word Emotion Induction for Multiple Languages as a Deep Multi-Task Learning Problem,2018,0,10,2,1,426,sven buechel,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Predicting the emotional value of lexical items is a well-known problem in sentiment analysis. While research has focused on polarity for quite a long time, meanwhile this early focus has been shifted to more expressive emotion representation models (such as Basic Emotions or Valence-Arousal-Dominance). This change resulted in a proliferation of heterogeneous formats and, in parallel, often small-sized, non-interoperable resources (lexicons and corpus annotations). In particular, the limitations in size hampered the application of deep learning methods in this area because they typically require large amounts of input data. We here present a solution to get around this language data bottleneck by rephrasing word emotion induction as a multi-task learning problem. In this approach, the prediction of each independent emotion dimension is considered as an individual task and hidden layers are shared between these dimensions. We investigate whether multi-task learning is more advantageous than single-task learning for emotion prediction by comparing our model against a wide range of alternative emotion and polarity induction methods featuring 9 typologically diverse languages and a total of 15 conditions. Our model turns out to outperform each one of them. Against all odds, the proposed deep learning approach yields the largest gain on the smallest data sets, merely composed of one thousand samples."
L18-1028,Representation Mapping: A Novel Approach to Generate High-Quality Multi-Lingual Emotion Lexicons,2018,38,5,2,1,426,sven buechel,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"In the past years, sentiment analysis has increasingly shifted attention to representational frameworks more expressive than semantic polarity (being positive, negative or neutral). However, these richer formats (like Basic Emotions or Valence-Arousal-Dominance, and variants therefrom), rooted in psychological research, tend to proliferate the number of representation schemes for emotion encoding. Thus, a large amount of representationally incompatible emotion lexicons has been developed by various research groups adopting one or the other emotion representation format. As a consequence, the reusability of these resources decreases as does the comparability of systems using them. In this paper, we propose to solve this dilemma by methods and tools which map different representation formats onto each other for the sake of mutual compatibility and interoperability of language resources. We present the first large-scale investigation of such representation mappings for four typologically diverse languages and find evidence that our approach produces (near-)gold quality emotion lexicons, even in cross-lingual settings. Finally, we use our models to create new lexicons for eight typologically diverse languages."
L18-1201,Sharing Copies of Synthetic Clinical Corpora without Physical Distribution {---} A Case Study to Get Around {IPR}s and Privacy Constraints Featuring the {G}erman {JSYNCC} Corpus,2018,0,1,3,1,17797,christina lohr,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-2003,{J}e{S}em{E}: Interleaving Semantics and Emotions in a Web Service for the Exploration of Language Change Phenomena,2018,0,1,3,1,24319,johannes hellrich,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"We here introduce a substantially extended version of JeSemE, an interactive website for visually exploring computationally derived time-variant information on word meanings and lexical emotions assembled from five large diachronic text corpora. JeSemE is designed for scholars in the (digital) humanities as an alternative to consulting manually compiled, printed dictionaries for such information (if available at all). This tool uniquely combines state-of-the-art distributional semantics with a nuanced model of human emotions, two information streams we deem beneficial for a data-driven interpretation of texts in the humanities."
C18-1245,Emotion Representation Mapping for Automatic Lexicon Construction (Mostly) Performs on Human Level,2018,37,0,2,1,426,sven buechel,Proceedings of the 27th International Conference on Computational Linguistics,0,"Emotion Representation Mapping (ERM) has the goal to convert existing emotion ratings from one representation format into another one, e.g., mapping Valence-Arousal-Dominance annotations for words or sentences into Ekman{'}s Basic Emotions and vice versa. ERM can thus not only be considered as an alternative to Word Emotion Induction (WEI) techniques for automatic emotion lexicon construction but may also help mitigate problems that come from the proliferation of emotion representation formats in recent years. We propose a new neural network approach to ERM that not only outperforms the previous state-of-the-art. Equally important, we present a refined evaluation methodology and gather strong evidence that our model yields results which are (almost) as reliable as human annotations, even in cross-lingual settings. Based on these results we generate new emotion ratings for 13 typologically diverse languages and claim that they have near-gold quality, at least."
W17-0801,Readers vs. Writers vs. Texts: Coping with Different Perspectives of Text Understanding in Emotion Annotation,2017,-1,-1,2,1,426,sven buechel,Proceedings of the 11th Linguistic Annotation Workshop,0,"We here examine how different perspectives of understanding written discourse, like the reader{'}s, the writer{'}s or the text{'}s point of view, affect the quality of emotion annotations. We conducted a series of annotation experiments on two corpora, a popular movie review corpus and a genre- and domain-balanced corpus of standard English. We found statistical evidence that the writer{'}s perspective yields superior annotation quality overall. However, the quality one perspective yields compared to the other(s) seems to depend on the domain the utterance originates from. Our data further suggest that the popular movie review data set suffers from an atypical bimodal distribution which may decrease model performance when used as a training resource."
P17-4006,Exploring Diachronic Lexical Semantics with {J}e{S}em{E},2017,0,3,2,1,24319,johannes hellrich,"Proceedings of {ACL} 2017, System Demonstrations",0,None
P17-4016,{S}emedico: A Comprehensive Semantic Search Engine for the Life Sciences,2017,9,4,2,1,17796,erik faessler,"Proceedings of {ACL} 2017, System Demonstrations",0,None
E17-2092,{E}mo{B}ank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis,2017,26,31,2,1,426,sven buechel,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"We describe EmoBank, a corpus of 10k English sentences balancing multiple genres, which we annotated with dimensional emotion metadata in the Valence-Arousal-Dominance (VAD) representation format. EmoBank excels with a bi-perspectival and bi-representational design. On the one hand, we distinguish between writer{'}s and reader{'}s emotions, on the other hand, a subset of the corpus complements dimensional VAD annotations with categorical ones based on Basic Emotions. We find evidence for the supremacy of the reader{'}s perspective in terms of IAA and rating intensity, and achieve close-to-human performance when mapping between dimensional and categorical formats."
W16-4008,Feelings from the {P}ast{---}{A}dapting Affective Lexicons for Historical Emotion Analysis,2016,0,8,3,1,426,sven buechel,Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities ({LT}4{DH}),0,"We here describe a novel methodology for measuring affective language in historical text by expanding an affective lexicon and jointly adapting it to prior language stages. We automatically construct a lexicon for word-emotion association of 18th and 19th century German which is then validated against expert ratings. Subsequently, this resource is used to identify distinct emotional patterns and trace long-term emotional trends in different genres of writing spanning several centuries."
W16-2114,An Assessment of Experimental Protocols for Tracing Changes in Word Semantics Relative to Accuracy and Reliability,2016,13,2,2,1,24319,johannes hellrich,"Proceedings of the 10th {SIGHUM} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,None
W16-0423,Do Enterprises Have Emotions?,2016,32,2,2,1,426,sven buechel,"Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Emotional language of human individuals has been studied for quite a while dealing with opinions and value judgments people have and share with others. In our work, we take a different stance and investigate whether large organizations, such as major industrial players, have and communicate emotions, as well. Such an anthropomorphic perspective has recently been advocated in management and organization studies which consider organizations as social actors. We studied this assumption by analyzing 1,676 annual business and sustainability reports from 90 top-performing enterprises in the United States, Great Britain and Germany. We compared the measurements of emotions in this homogeneous corporate text corpus with those from RCV1, a heterogeneous Reuters newswire corpus. From this, we gathered empirical evidence that business reports compare well with typical emotion-neutral economic news, whereas sustainability reports are much more emotionally loaded, similar to emotion-heavy sports and fashion news from Reuters. Furthermore, our data suggest that these emotions are distinctive and relatively stable over time per organization, thus constituting an emotional profile for enterprises."
L16-1397,{UIMA}-Based {JC}o{R}e 2.0 Goes {G}it{H}ub and Maven Central â State-of-the-Art Software Resource Engineering and Distribution of {NLP} Pipelines,2016,13,7,1,1,10102,udo hahn,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We introduce JCoRe 2.0, the relaunch of a UIMA-based open software repository for full-scale natural language processing originating from the Jena University Language {\&} Information Engineering (JULIE) Lab. In an attempt to put the new release of JCoRe on firm software engineering ground, we uploaded it to GitHub, a social coding platform, with an underlying source code versioning system and various means to support collaboration for software development and code modification management. In order to automate the builds of complex NLP pipelines and properly represent and track dependencies of the underlying Java code, we incorporated Maven as part of our software configuration management efforts. In the meantime, we have deployed our artifacts on Maven Central, as well. JCoRe 2.0 offers a broad range of text analytics functionality (mostly) for English-language scientific abstracts and full-text articles, especially from the life sciences domain."
L16-1404,{C}od{E} Alltag: A {G}erman-Language {E}-Mail Corpus,2016,0,0,5,1,10782,ulrike kriegholz,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We introduce CODE ALLTAG, a text corpus composed of German-language e-mails. It is divided into two partitions: the first of these portions, CODE ALLTAG{\_}XL, consists of a bulk-size collection drawn from an openly accessible e-mail archive (roughly 1.5M e-mails), whereas the second portion, CODE ALLTAG{\_}S+d, is much smaller in size (less than thousand e-mails), yet excels with demographic data from each author of an e-mail. CODE ALLTAG, thus, currently constitutes the largest E-Mail corpus ever built. In this paper, we describe, for both parts, the solicitation process for gathering e-mails, present descriptive statistical properties of the corpus, and, for CODE ALLTAG{\_}S+d, reveal a compilation of demographic features of the donors of e-mails."
C16-1262,Bad {C}ompany{---}{N}eighborhoods in Neural Embedding Spaces Considered Harmful,2016,14,26,2,1,24319,johannes hellrich,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We assess the reliability and accuracy of (neural) word embeddings for both modern and historical English and German. Our research provides deeper insights into the empirically justified choice of optimal training methods and parameters. The overall low reliability we observe, nevertheless, casts doubt on the suitability of word neighborhoods in embedding spaces as a basis for qualitative conclusions on synchronic and diachronic lexico-semantic matters, an issue currently high up in the agenda of Digital Humanities."
hellrich-etal-2014-collaboratively,Collaboratively Annotating Multilingual Parallel Corpora in the Biomedical Domain{---}some {MANTRA}s,2014,13,1,3,1,24319,johannes hellrich,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The coverage of multilingual biomedical resources is high for the English language, yet sparse for non-English languagesâan observation which holds for seemingly well-resourced, yet still dramatically low-resourced ones such as Spanish, French or German but even more so for really under-resourced ones such as Dutch. We here present experimental results for automatically annotating parallel corpora and simultaneously acquiring new biomedical terminology for these under-resourced non-English languages on the basis of two types of language resources, namely parallel corpora (i.e. full translation equivalents at the document unit level) and (admittedly deficient) multilingual biomedical terminologies, with English as their anchor language. We automatically annotate these parallel corpora with biomedical named entities by an ensemble of named entity taggers and harmonize non-identical annotations the outcome of which is a so-called silver standard corpus. We conclude with an empirical assessment of this approach to automatically identify both known and new terms in multilingual corpora."
faessler-etal-2014-disclose,"Disclose Models, Hide the Data - How to Make Use of Confidential Corpora without Seeing Sensitive Raw Data",2014,8,2,3,1,17796,erik faessler,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Confidential corpora from the medical, enterprise, security or intelligence domains often contain sensitive raw data which lead to severe restrictions as far as the public accessibility and distribution of such language resources are concerned. The enforcement of strict mechanisms of data protection consitutes a serious barrier for progress in language technology (products) in such domains, since these data are extremely rare or even unavailable for scientists and developers not directly involved in the creation and maintenance of such resources. In order to by-pass this problem, we here propose to distribute trained language models which were derived from such resources as a substitute for the original confidential raw data which remain hidden to the outside world. As an example, we exploit the access-protected German-language medical FRAMED corpus from which we generate and distribute models for sentence splitting, tokenization and POS tagging based on software taken from OPENNLP, NLTK and JCORE, our own UIMA-based text analytics pipeline."
hahn-etal-2012-iterative,"Iterative Refinement and Quality Checking of Annotation Guidelines {---} How to Deal Effectively with Semantically Sloppy Named Entity Types, such as Pathological Phenomena",2012,13,2,1,1,10102,udo hahn,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We here discuss a methodology for dealing with the annotation of semantically hard to delineate, i.e., sloppy, named entity types. To illustrate sloppiness of entities, we treat an example from the medical domain, namely pathological phenomena. Based on our experience with iterative guideline refinement we propose to carefully characterize the thematic scope of the annotation by positive and negative coding lists and allow for alternative, short vs. long mention span annotations. Short spans account for canonical entity mentions (e.g., standardized disease names), while long spans cover descriptive text snippets which contain entity-specific elaborations (e.g., anatomical locations, observational details, etc.). Using this stratified approach, evidence for increasing annotation performance is provided by kappa-based inter-annotator agreement measurements over several, iterative annotation rounds using continuously refined guidelines. The latter reflects the increasing understanding of the sloppy entity class both from the perspective of guideline writers and users (annotators). Given our data, we have gathered evidence that we can deal with sloppiness in a controlled manner and expect inter-annotator agreement values around 80{\%} for PathoJen, the pathological phenomena corpus currently under development in our lab."
kafkas-etal-2012-calbc,{CALBC}: Releasing the Final Corpora,2012,7,6,6,0,43120,csenay kafkas,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"A number of gold standard corpora for named entity recognition are available to the public. However, the existing gold standard corpora are limited in size and semantic entity types. These usually lead to implementation of trained solutions (1) for a limited number of semantic entity types and (2) lacking in generalization capability. In order to overcome these problems, the CALBC project has aimed to automatically generate large scale corpora annotated with multiple semantic entity types in a community-wide manner based on the consensus of different named entity solutions. The generated corpus is called the silver standard corpus since the corpus generation process does not involve any manual curation. In this publication, we announce the release of the final CALBC corpora which include the silver standard corpus in different versions and several gold standard corpora for the further usage of the biomedical text mining community. The gold standard corpora are utilised to benchmark the methods used in the silver standard corpora generation process and released in a shared format. All the corpora are released in a shared format and accessible at www.calbc.eu."
W10-1838,A Proposal for a Configurable Silver Standard,2010,7,6,1,1,10102,udo hahn,Proceedings of the Fourth Linguistic Annotation Workshop,0,"Among the many proposals to promote alternatives to costly to create gold standards, just recently the idea of a fully automatically, and thus cheaply, to set up silver standard has been launched. However, the current construction policy for such a silver standard requires crucial parameters (such as similarity thresholds and agreement cut-offs) to be set a priori, based on extensive testing though, at corpus compile time. Accordingly, such a corpus is static, once it is released. We here propose an alternative policy where silver standards can be dynamically optimized and customized on demand (given a specific goal function) using a gold standard as an oracle."
P10-1118,A Cognitive Cost Model of Annotations Based on Eye-Tracking Data,2010,19,29,2,1,9758,katrin tomanek,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We report on an experiment to track complex decision points in linguistic metadata annotation where the decision behavior of annotators is observed with an eye-tracking device. As experimental conditions we investigate different forms of textual context and linguistic complexity classes relative to syntax and semantics. Our data renders evidence that annotation performance depends on the semantic and syntactic complexity of the decision points and, more interestingly, indicates that full-scale context is mostly negligible - with the exception of semantic high-complexity cases. We then induce from this observational data a cognitively grounded cost model of linguistic meta-data annotations and compare it with existing non-cognitive models. Our data reveals that the cognitively founded model explains annotation costs (expressed in annotation time) more adequately than non-cognitive ones."
buyko-etal-2010-genereg,The {G}ene{R}eg Corpus for Gene Expression Regulation Events {---} An Overview of the Corpus and its In-Domain and Out-of-Domain Interoperability,2010,16,14,3,1,43183,ekaterina buyko,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Despite the large variety of corpora in the biomedical domain their annotations differ in many respects, e.g., the coverage of different, highly specialized knowledge domains, varying degrees of granularity of targeted relations, the specificity of linguistic anchoring of relations and named entities in documents, etc. We here present GeneReg (Gene Regulation Corpus), the result of an annotation campaign led by the Jena University Language {\&} Information Engineering (JULIE) Lab. The GeneReg corpus consists of 314 abstracts dealing with the regulation of gene expression in the model organism E. coli. Our emphasis in this paper is on the compatibility of the GeneReg corpus with the alternative Genia event corpus and with several in-domain and out-of-domain lexical resources, e.g., the Specialist Lexicon, FrameNet, and WordNet. The links we established from the GeneReg corpus to these external resources will help improve the performance of the automatic relation extraction engine JREx trained and evaluated on GeneReg."
tomanek-hahn-2010-annotation,Annotation Time Stamps {---} Temporal Metadata from the Linguistic Annotation Process,2010,11,7,2,1,9758,katrin tomanek,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We describe the re-annotation of selected types of named entities (persons, organizations, locations) from the Muc7 corpus. The focus of this annotation initiative is on recording the time needed for the linguistic process of named entity annotation. Annotation times are measured on two basic annotation units -- sentences vs. complex noun phrases. We gathered evidence that decision times are non-uniformly distributed over the annotation units, while they do not substantially deviate among annotators. This data seems to support the hypothesis that annotation times very much depend on the inherent ''``hardness'''' of each single annotation decision. We further show how such time-stamped information can be used for empirically grounded studies of selective sampling techniques, such as Active Learning. We directly compare Active Learning costs on the basis of token-based vs. time-based measurements. The data reveals that Active Learning keeps its competitive advantage over random sampling in both scenarios though the difference is less marked for the time metric than for the token metric."
rebholz-schuhmann-etal-2010-calbc,The {CALBC} Silver Standard Corpus for Biomedical Named Entities {---} A Study in Harmonizing the Contributions from Four Independent Named Entity Taggers,2010,9,15,11,0,39311,dietrich rebholzschuhmann,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The production of gold standard corpora is time-consuming and costly. We propose an alternative: the {\^a}ÂÂsilver standard corpusÂ (SSC), a corpus that has been generated by the harmonisation of the annotations that have been delivered from a selection of annotation systems. The systems have to share the type system for the annotations and the harmonisation solution has use a suitable similarity measure for the pair-wise comparison of the annotations. The annotation systems have been evaluated against the harmonised set (630.324 sentences, 15,956,841 tokens). We can demonstrate that the annotation of proteins and genes shows higher diversity across all used annotation solutions leading to a lower agreement against the harmonised set in comparison to the annotations of diseases and species. An analysis of the most frequent annotations from all systems shows that a high agreement amongst systems leads to the selection of terms that are suitable to be kept in the harmonised set. This is the first large-scale approach to generate an annotated corpus from automated annotation systems. Further research is required to understand, how the annotations from different systems have to be combined to produce the best annotation result for a harmonised corpus."
J10-4008,Book Review: Introduction to Linguistic Annotation and Text Analytics by Graham Wilcock,2010,-1,-1,1,1,10102,udo hahn,Computational Linguistics,0,None
D10-1096,Evaluating the Impact of Alternative Dependency Graph Encodings on Solving Event Extraction Tasks,2010,26,22,2,1,43183,ekaterina buyko,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"In state-of-the-art approaches to information extraction (IE), dependency graphs constitute the fundamental data structure for syntactic structuring and subsequent knowledge elicitation from natural language documents. The top-performing systems in the BioNLP 2009 Shared Task on Event Extraction all shared the idea to use dependency structures generated by a variety of parsers --- either directly or in some converted manner --- and optionally modified their output to fit the special needs of IE. As there are systematic differences between various dependency representations being used in this competition, we scrutinize on different encoding styles for dependency information and their possible impact on solving several IE tasks. After assessing more or less established dependency representations such as the Stanford and CoNLL-X dependencies, we will then focus on trimming operations that pave the way to more effective IE. Our evaluation study covers data from a number of constituency- and dependency-based parsers and provides experimental evidence which dependency representations are particularly beneficial for the event extraction task. Based on empirical findings from our study we were able to achieve the performance of 57.2% F-score on the development data set of the BioNLP Shared Task 2009."
C10-2143,A Comparison of Models for Cost-Sensitive Active Learning,2010,15,13,2,1,9758,katrin tomanek,Coling 2010: Posters,0,"Active Learning (AL) is a selective sampling strategy which has been shown to be particularly cost-efficient by drastically reducing the amount of training data to be manually annotated. For the annotation of natural language data, cost efficiency is usually measured in terms of the number of tokens to be considered. This measure, assuming uniform costs for all tokens involved, is, from a linguistic perspective at least, intrinsically inadequate and should be replaced by a more adequate cost indicator, viz. the time it takes to manually label selected annotation examples. We here propose three different approaches to incorporate costs into the AL selection mechanism and evaluate them on the Muc7T corpus, an extension of the Muc7 newspaper corpus that contains such annotation time information. Our experiments reveal that using a cost-sensitive version of semi-supervised AL, up to 54% of true annotation time can be saved compared to random selection."
W09-3018,Timed Annotations {---} Enhancing {MUC}7 Metadata by the Time It Takes to Annotate Named Entities,2009,4,4,2,1,9758,katrin tomanek,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,We report on the re-annotation of selected types of named entities from the Muc7 corpus where our focus lies on recording the time it takes to annotate these entities given two basic annotation units -- sentences vs. complex noun phrases. Such information may be helpful to lay the empirical foundations for the development of cost measures for annotation processes based on the investment in time for decision-making per entity mention.
W09-1902,On Proper Unit Selection in Active Learning: Co-Selection Effects for Named Entity Recognition,2009,14,14,3,1,9758,katrin tomanek,Proceedings of the {NAACL} {HLT} 2009 Workshop on Active Learning for Natural Language Processing,0,"Active learning is an effective method for creating training sets cheaply, but it is a biased sampling process and fails to explore large regions of the instance space in many applications. This can result in a missed cluster effect, which signficantly lowers recall and slows down learning for infrequent classes. We show that missed clusters can be avoided in sequence classification tasks by using sentences as natural multi-instance units for labeling. Co-selection of other tokens within sentences provides an implicit exploratory component since we found for the task of named entity recognition on two corpora that entity classes co-occur with sufficient frequency within sentences."
W09-1403,Event Extraction from Trimmed Dependency Graphs,2009,14,66,4,1,43183,ekaterina buyko,Proceedings of the {B}io{NLP} 2009 Workshop Companion Volume for Shared Task,0,"We describe the approach to event extraction which the JulieLab Team from FSU Jena (Germany) pursued to solve Task 1 in the BioNLP'09 Shared Task on Event Extraction. We incorporate manually curated dictionaries and machine learning methodologies to sort out associated event triggers and arguments on trimmed dependency graph structures. Trimming combines pruning irrelevant lexical material from a dependency graph and decorating particularly relevant lexical material from that graph with more abstract conceptual class information. Given that methodological framework, the JulieLab Team scored on 2nd rank among 24 competing teams, with 45.8% precision, 47.5% recall and 46.7% F1-score on all 3,182 events."
W09-1305,How Feasible and Robust is the Automatic Extraction of Gene Regulation Events? A Cross-Method Evaluation under Lab and Real-Life Conditions,2009,12,13,1,1,10102,udo hahn,Proceedings of the {B}io{NLP} 2009 Workshop,0,"We explore a rule system and a machine learning (ML) approach to automatically harvest information on gene regulation events (GREs) from biological documents in two different evaluation scenarios -- one uses self-supplied corpora in a clean lab setting, while the other incorporates a standard reference database of curated GREs from RegulonDB, real-life data generated independently from our work. In the lab condition, we test how feasible the automatic extraction of GREs really is and achieve F-scores, under different, not directly comparable test conditions though, for the rule and the ML systems which amount to 34% and 44%, respectively. In the RegulonDB condition, we investigate how robust both methodologies are by comparing them with this routinely used database. Here, the best F-scores for the rule and the ML systems amount to 34% and 19%, respectively."
P09-1117,Semi-Supervised Active Learning for Sequence Labeling,2009,18,53,2,1,9758,katrin tomanek,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"While Active Learning (AL) has already been shown to markedly reduce the annotation efforts for many sequence labeling tasks compared to random selection, AL remains unconcerned about the internal structure of the selected sequences (typically, sentences). We propose a semi-supervised AL approach for sequence labeling where only highly uncertain subsequences are presented to human annotators, while all others in the selected sequences are automatically labeled. For the task of entity recognition, our experiments reveal that this approach reduces annotation efforts in terms of manually labeled tokens by up to 60% compared to the standard, fully supervised AL scheme."
W08-0507,Building a {B}io{W}ord{N}et Using {W}ord{N}et Data Structures and {W}ord{N}et{'}s Software Infrastructure{--}A Failure Story,2008,3,3,3,0,47811,michael poprat,"Software Engineering, Testing, and Quality Assurance for Natural Language Processing",0,None
P08-1098,Multi-Task Active Learning for Linguistic Annotations,2008,24,51,3,0,8675,roi reichart,Proceedings of ACL-08: HLT,1,"We extend the classical single-task active learning (AL) approach. In the multi-task active learning (MTAL) paradigm, we select examples for several annotation tasks rather than for a single one as usually done in the context of AL. We introduce two MTAL metaprotocols, alternating selection and rank combination, and propose a method to implement them in practice. We experiment with a twotask annotation scenario that includes named entity and syntactic parse tree annotations on three different corpora. MTAL outperforms random selection and a stronger baseline, onesided example selection, in which one task is pursued using AL and the selected examples are provided also to the other task."
tomanek-hahn-2008-approximating,Approximating Learning Curves for Active-Learning-Driven Annotation,2008,21,10,2,1,9758,katrin tomanek,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Active learning (AL) is getting more and more popular as a methodology to considerably reduce the annotation effort when building training material for statistical learning methods for various NLP tasks. A crucial issue rarely addressed, however, is when to actually stop the annotation process to profit from the savings in efforts. This question is tightly related to estimating the classifier performance after a certain amount of data has already been annotated. While learning curves are the default means to monitor the progress of the annotation process in terms of classifier performance, this requires a labeled gold standard which - in realistic annotation settings, at least - is often unavailable. We here propose a method for committee-based AL to approximate the progression of the learning curve based on the disagreement among the committee members. This method relies on a separate, unlabeled corpus and is thus well suited for situations where a labeled gold standard is not available or would be too expensive to obtain. Considering named entity recognition as a test case we provide empirical evidence that this approach works well under simulation as well as under real-world annotation conditions."
hahn-etal-2008-semantic,Semantic Annotations for Biology: a Corpus Development Initiative at the Jena University Language {\\&} Information Engineering ({JULIE}) Lab,2008,12,10,1,1,10102,udo hahn,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We provide an overview of corpus building efforts at the Jena University Language {\&} Information Engineering (JULIE) Lab which are focused on life science documents. Special emphasis is laid on semantic annotations in terms of a large amount of biomedical named entities (almost 100 entity types), semantic relations, as well as discourse phenomena, reference relations in particular."
C08-1012,Are Morpho-Syntactic Features More Predictive for the Resolution of Noun Phrase Coordination Ambiguity than Lexico-Semantic Similarity Scores?,2008,23,4,2,1,43183,ekaterina buyko,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Coordinations in noun phrases often pose the problem that elliptified parts have to be reconstructed for proper semantic interpretation. Unfortunately, the detection of coordinated heads and identification of elliptified elements notoriously lead to ambiguous reconstruction alternatives. While linguistic intuition suggests that semantic criteria might play an important, if not superior, role in disambiguating resolution alternatives, our experiments on the reannotated WSJ part of the Penn Treebank indicate that solely morpho-syntactic criteria are more predictive than solely lexico-semantic ones. We also found that the combination of both criteria does not yield any substantial improvement."
W07-1502,Efficient Annotation with the {J}ena {AN}notation {E}nvironment ({JANE}),2007,17,15,3,1,9758,katrin tomanek,Proceedings of the Linguistic Annotation Workshop,0,"With ever-increasing demands on the diversity of annotations of language data, the need arises to reduce the amount of efforts involved in generating such value-added language resources. We introduce here the Jena ANnotation Environment (Jane), a platform that supports the complete annotation life-cycle and allows for 'focused' annotation based on active learning. The focus we provide yields significant savings in annotation efforts by presenting only informative items to the annotator. We report on our experience with this approach through simulated and real-world annotations in the domain of immunogenetics for NE annotations."
W07-1505,An Annotation Type System for a Data-Driven {NLP} Pipeline,2007,19,17,1,1,10102,udo hahn,Proceedings of the Linguistic Annotation Workshop,0,"We introduce an annotation type system for a data-driven NLP core system. The specifications cover formal document structure and document meta information, as well as the linguistic levels of morphology, syntax and semantics. The type system is embedded in the framework of the Unstructured Information Management Architecture (UIMA)."
W07-1028,Quantitative Data on Referring Expressions in Biomedical Abstracts,2007,3,4,2,0,47811,michael poprat,"Biological, translational, and clinical language processing",0,We report on an empirical study that deals with the quantity of different kinds of referring expressions in biomedical abstracts.
D07-1051,An Approach to Text Corpus Construction which Cuts Annotation Costs and Maintains Reusability of Annotated Data,2007,19,82,3,1,9758,katrin tomanek,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We consider the impact Active Learning (AL) has on effective and efficient text corpus annotation, and report on reduction rates for annotation efforts ranging up until 72%. We also address the issue whether a corpus annotated by means of AL xe2x80x90 using a particular classifier and a particular feature set xe2x80x90 can be re-used to train classifiers different from the ones employed by AL, supplying alternative feature sets as well. We, finally, report on our experience with the AL paradigm under real-world conditions, i.e., the annotation of large-scale document corpora for the life sciences."
P06-1099,You Can{'}t Beat Frequency (Unless You Use Linguistic Knowledge) {--} A Qualitative Evaluation of Association Measures for Collocation and Term Extraction,2006,10,43,2,1,47009,joachim wermter,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"In the past years, a number of lexical association measures have been studied to help extract new scientific terminology or general-language collocations. The implicit assumption of this research was that newly designed term measures involving more sophisticated statistical criteria would outperform simple counts of co-occurrence frequencies. We here explicitly test this assumption. By way of four qualitative criteria, we show that purely statistics-based measures reveal virtually no difference compared with frequency of occurrence counts, while linguistically more informed metrics do reveal such a marked difference."
schulz-etal-2006-semantic,Semantic Atomicity and Multilinguality in the Medical Domain: Design Considerations for the {M}orpho{S}aurus Subword Lexicon,2006,6,1,4,1,16517,stefan schulz,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We present the lexico-semantic foundations underlying a multilingual lexicon the entries of which are constituted by so-called subwords. These subwords reflect semantic atomicity constraints in the medical domain which diverge from canonical lexicological understanding in NLP. We focus here on criteria to identify and delimit reasonable subword units, to group them into functionally adequate synonymy classes and relate them by two types of lexical relations. The lexicon we implemented on the basis of these considerations forms the lexical backbone for MorphoSaurus, a cross-language document retrieval engine for the medical domain."
H05-1106,Paradigmatic Modifiability Statistics for the Extraction of Complex Multi-Word Terms,2005,16,37,2,1,47009,joachim wermter,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We here propose a new method which sets apart domain-specific terminology from common non-specific noun phrases. It is based on the observation that terminological multi-word groups reveal a considerably lesser degree of distributional variation than non-specific noun phrases. We define a measure for the observable amount of paradigmatic modifiability of terms and, subsequently, test it on bigram, trigram and quadgram noun phrases extracted from a 104-million-word biomedical text corpus. Using a community-wide curated biomedical terminology system as an evaluation gold standard, we show that our algorithm significantly outperforms a variety of standard term identification measures. We also provide empirical evidence that our methodolgy is essentially domain- and corpus-size-independent."
2005.mtsummit-papers.3,Subword Clusters as Light-Weight Interlingua for Multilingual Document Retrieval,2005,-1,-1,1,1,10102,udo hahn,Proceedings of Machine Translation Summit X: Papers,0,"We introduce a light-weight interlingua for a cross-language document retrieval system in the medical domain. It is composed of equivalence classes of semantically primitive, language-specific subwords which are clustered by interlingual and intralingual synonymy. Each subword cluster represents a basic conceptual entity of the language-independent interlingua. Documents, as well as queries, are mapped to this interlingua level on which retrieval operations are performed. Evaluation experiments reveal that this interlingua-based retrieval model outperforms a direct translation approach."
wermter-hahn-2004-annotated,An Annotated {G}erman-Language Medical Text Corpus as Language Resource,2004,5,14,2,1,47009,joachim wermter,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
hahn-wermter-2004-pumping,Pumping Documents Through a Domain and Genre Classification Pipeline,2004,11,2,1,1,10102,udo hahn,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We propose a simple, yet effective, pipeline architecture for document classification. The task we intend to solve is to classify large and content-wise heterogeneous document streams on a layered nine-category system, which distinguishes medical from non-medical texts and sorts medical texts into various subgenres. While the document classification problem is often dealt with using computationally powerful and, hence, costly classifiers (e.g., Bayesian ones), we have gathered empirical evidence that a much simpler approach based on n-gram-statistics achieves a comparable level of classification performance."
C04-1117,Cognate Mapping - A Heuristic Strategy for the Semi-Supervised Acquisition of a {S}panish Lexicon from a {P}ortuguese Seed Lexicon,2004,15,35,5,1,16517,stefan schulz,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,We deal with the automated acquisition of a Spanish medical subword lexicon from an already existing Portuguese seed lexicon. Using two non-parallel monolingual corpora we determined Spanish lexeme candidates from Portuguese seed lexicon entries by heuristic cognate mapping. We validated the emergent lexical translation hypotheses by determining the similarity of fixed-window context vectors on the basis of Portuguese and Spanish text corpora.
C04-1140,High-Performance Tagging on Medical Texts,2004,13,15,1,1,10102,udo hahn,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We ran both Brill's rule-based tagger and TNT, a statistical tagger, with a default German newspaper-language model on a medical text corpus. Supplied with limited lexicon resources, TNT outperforms the Brill tagger with state-of-the-art performance figures (close to 97% accuracy). We then trained TNT on a large annotated medical text corpus, with a slightly extended tagset that captures certain medical language particularities, and achieved 98% tagging accuracy. Hence, statistical off-the-shelf POS taggers cannot only be immediately reused for medical NLP, but they also -- when trained on medical corpora -- achieve a higher performance level than for the newspaper genre."
C04-1141,Collocation Extraction Based on Modifiability Statistics,2004,10,44,2,1,47009,joachim wermter,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We introduce a new, linguistically grounded measure of collocativity based on the property of limited modifiability and test it on German PP-verb combinations. We show that our measure not only significantly outperforms the standard lexical association measures typically employed for collocation extraction, but also yields a valuable by-product for the creation of collocation databases, viz. possible structural and lexical attributes. Our approach is language-, structure-, and domain-independent because it only requires some shallow syntactic analysis (e.g., a POS-tagger and a phrase chunker)."
W02-0309,Biomedical text retrieval in languages with a complex morphology,2002,23,17,3,0,53256,stefan schultz,Proceedings of the {ACL}-02 Workshop on Natural Language Processing in the Biomedical Domain,0,"Document retrieval in languages with a rich and complex morphology - particularly in terms of derivation and (single-word) composition - suffers from serious performance degradation with the stemming-only query-term-to-text-word matching paradigm. We propose an alternative approach in which morphologically complex word forms are segmented into relevant subwords (such as stems, named entities, acronyms), and subwords constitute the basic unit for indexing and retrieval. We evaluate our approach on a large biomedical document collection."
hahn-schulz-2002-towards,Towards Very Large Ontologies for Medical Language Processing,2002,28,1,1,1,10102,udo hahn,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"We describe an ontology engineering methodology by which conceptual knowledge is extracted from an informal medical thesaurus (UMLS) and automatically converted into a formal description logics system. Our approach consists of four steps: concept definitions are automatically generated from the UMLS source, integrity checking of taxonomic and partonomic hierarchies is performed by the terminological classifier, cycles and inconsistencies are eliminated, and incremental refinement of the evolving knowledge base is performed by a domain expert. We report on experiments with a knowledge base composed of 164,000 concepts and 76,000 relations."
H01-1067,The {S}yn{D}i{KAT}e Text Knowledge Base Generator,2001,28,33,1,1,10102,udo hahn,Proceedings of the First International Conference on Human Language Technology Research,0,"SYNDIKATE comprises a family of text understanding systems for automatically acquiring knowledge from real-world texts, viz. information technology test reports and medical finding reports. Their content is transformed to formal representation structures which constitute corresponding text knowledge bases. SYNDIKATE's architecture integrates requirements from the analysis of single sentences, as well as those of referentially linked sentences forming cohesive texts. Besides centering-based discourse analysis mechanisms for pronominal, nominal and bridging anaphora, SYNDIKATE is supplied with a learning module for automatically bootstrapping its domain knowledge as text analysis proceeds."
C00-1040,An Integrated Model of Semantic and Conceptual Interpretation from Dependency Structures,2000,19,6,1,1,10102,udo hahn,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"We propose a two-layered model for computing semantic and conceptual interpretations from dependency structures. Abstract interpretation schemata generate semantic interpretations of 'minimal' dependency subgraphs, while production rules whose specification is rooted in ontological categories derive a canonical conceptual interpretation from semantic interpretation structures. Configurational descriptions of dependency graphs increase the linguistic generality of interpretation schemata, while interfacing schemata and productions to lexical and conceptual class hierarchies reduces the amount and complexity of semantic specifications."
A00-2043,An Empirical Assessment of Semantic Interpretation,2000,21,9,2,0,50543,martin romacker,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We introduce a framework for semantic interpretation in which dependency structures are mapped to conceptual representations based on a parsimonious set of interpretation schemata. Our focus is on the empirical evaluation of this approach to semantic interpretation, i.e., its quality in terms of recall and precision. Measurements are taken with respect to two real-world domains, viz. information technology test reports and medical finding reports."
J99-3001,Functional Centering {--} Grounding Referential Coherence on Information Structure,1999,55,147,2,0.588235,869,michael strube,Computational Linguistics,0,"Considering empirical evidence from a free-word-order language (German) we propose a revision of the principles guiding the ordering of discourse entities in the forward-looking center list within the centering model. We claim that grammatical role criteria should be replaced by criteria that reflect the functional information structure of the utterances. These new criteria are based on the distinction between hearer-old and hearer-new discourse entities. We demonstrate that such a functional model of centering can be successfully applied to the analysis of several forms of referential text phenomena, viz. pronominal, nominal, and functional anaphora. Our methodological and empirical claims are substantiated by two evaluation studies. In the first one, we compare success rates for the resolution of pronominal anaphora that result from a grammatical-role-driven centering algorithm and from a functional centering algorithm. The second study deals with a new cost-based evaluation methodology for the assessment of centering data, one which can be directly derived from and justified by the cognitive load premises of the centering model."
P98-1079,A Text Understander that Learns,1998,15,9,1,1,10102,udo hahn,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"We introduce an approach to the automatic acquisition of new concepts from natural language texts which is tightly integrated with the underlying text understanding process. The learning model is centered around the 'quality' of different forms of linguistic and conceptual evidence which underlies the incremental generation and refinement of alternative concept hypotheses, each one capturing a different conceptual reading for an unknown lexical item."
C98-1076,A Text Understander that Learns,1998,15,9,1,1,10102,udo hahn,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"We introduce an approach to the automatic acquisition of new concepts from natural language texts which is tightly integrated with the underlying text understanding process. The learning model is centered around the 'quality' of different forms of linguistic and conceptual evidence which underlies the incremental generation and refinement of alternative concept hypotheses, each one capturing a different conceptual reading for an unknown lexical item."
W97-0715,A Formal Model of Text Summarization Based on Condensation Operators of a Terminological Logic,1997,-1,-1,2,0,55558,ulrich reimer,Intelligent Scalable Text Summarization,0,None
P97-1014,Centering in-the-Large: Computing Referential Discourse Segments,1997,15,19,1,1,10102,udo hahn,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"We specify an algorithm that builds up a hierarchy of referential discourse segments from local centering data. The spatial extension and nesting of these discourse segments constrain the reachability of potential antecedents of an anaphoric expression beyond the local level of adjacent center pairs. Thus, the centering model is scaled up to the level of the global referential structure of discourse. An empirical evaluation of the algorithm is supplied."
1997.iwpt-1.14,Message-passing Protocols for Real-world Parsing - An Object-oriented Model and its Preliminary Evaluation,1997,29,7,1,1,10102,udo hahn,Proceedings of the Fifth International Workshop on Parsing Technologies,0,We argue for a performance-based design of natural language grammars and their associated parsers in order to meet the constraints imposed by real-world NLP. Our approach incorporates declarative and procedural knowledge about language and language use within an object-oriented specification framework. We discuss several message-passing protocols for parsing and provide reasons for sacrificing completeness of the parse in favor of efficiency based on a preliminary empirical evaluation.
P96-1036,Functional Centering,1996,22,73,2,0.952381,869,michael strube,34th Annual Meeting of the Association for Computational Linguistics,1,"Based on empirical evidence from a free word order language (German) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model. We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound discourse elements. This claim is backed up by an empirical evaluation of functional centering."
C96-1084,Bridging Textual Ellipses,1996,19,16,1,1,10102,udo hahn,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"We present a hybrid text understanding methodology for the resolution of textual ellipsis. It integrates language-independent conceptual criteria and language-dependent functional constraints. The methodological framework for text ellipsis resolution is the centering model that has been adapted to constraints reflecting the functional information structure within utterances, i.e., the distinction between context-bound and unbound discourse elements."
C96-1085,Restricted Parallelism in Object-Oriented Lexical Parsing,1996,23,11,2,0,55627,peter neuhaus,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"We present an approach to parallel natural language parsing which is based on a concurrent, object-oriented model of computation. A depth-first, yet incomplete parsing algorithm for a dependency grammar is specified and several restrictions on the degree of its parallelization are discussed."
E95-1033,{P}arse{T}alk about Sentence- and Text-Level Anaphora,1995,20,16,2,0.952381,869,michael strube,Seventh Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We provide a unified account of sentence-level and text-level anaphora within the framework of a dependency-based grammar model. Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB's binding theory, while those for text-level anaphora incorporate an adapted version of a Grosz-Sidner-style focus model."
C94-1061,Concurrent Lexicalized Dependency Parsing: The {P}arse{T}alk Model,1994,39,16,2,0,54674,norbert broker,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"A grammar model for concurrent, object-oriented natural language parsing is introduced. Complete lexical distribution of grammatical knowledge is achieved building upon the head-oriented notions of valency and dependency, while inheritance mechanisms are used to capture lexical generalizations. The underlying concurrent computation model relies upon the actor paradigm. We consider message passing protocols for establishing dependency relations and ambiguity handling."
C94-1080,Concurrent Lexicalized Dependency Parsing: A Behavioral View on {P}arse{T}alk Events,1994,14,4,2,0,56537,susanne schacht,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The behavioral specification of an object-oriented grammar model is considered. The model is based on full lexicalization, head-orientation via valency constraints and dependency relations, inheritance as a means for non-redundant lexicon specification, and concurrency of computation. The computation model relies upon the actors paradigm, with concurrency entering through asynchronous message passing between actors. In particular, we here elaborate on principles of how the global behavior of a lexically distributed grammar and its corresponding parser can be specified in terms of event type networks and event networks, resp."
C92-1008,On Text Coherence Parsing,1992,17,9,1,1,10102,udo hahn,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper global patterns of thematic text organization are considered within the framework of a distributed model of text understanding. Based on the parsing results of prior text cohesion analysis, specialized text grammar modules determine whether some well-defined text macro-organization pattern is computable from the available text representation structures. The model underlying text coherence parsing formalizes hither-to entirely intuitive textlinguistic notions whose origin can be traced back to Danes's work on thematic progression patterns."
C86-1118,{TOPIC} Essentials,1986,15,18,1,1,10102,udo hahn,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,"An overview of TOPIC is provided, a knowledge-based text information system for the analysis of German-language texts. TOPIC supplies text condensates (summaries) on variable degrees of generality and makes available facts acquired from the texts. The presentation focuses on the major methodological principles underlying the design of TOPIC: a frame representation model that incorporates various integrity constraints, text parsing with focus on text cohesion and text coherence properties of expository texts, a lexically distributed semantic text grammar in the format of word experts, a model of partial text parsing, and text graphs as appropriate representation structures for text condensates."
P84-1083,Textual Expertise in Word Experts: An Approach to Text Parsing Based on Topic/Comment Monitoring,1984,9,2,1,1,10102,udo hahn,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,"In this paper prototype versions of two word experts for text analysis are dealt with which demonstrate that word experts are a feasible tool for parsing texts on the level of text cohesion as well as text coherence. The analysis is based on two major knowledge sources: context information is modelled in terms of a frame knowledge base, while the co-text keeps record of the linear sequencing of text analysis. The result of text parsing consists of a text graph reflecting the thematic organization of topics in a text."
