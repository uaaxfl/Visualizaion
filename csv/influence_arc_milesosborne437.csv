2005.iwslt-1.8,P05-1066,1,0.16829,"inal (default) 48.8 40.4 33.9 28.9 15.4 final-and 48.5 39.9 35.7 32.4 9.6 grow-diag 49.9 39.0 27.7 31.7 8.1 grow 39.9 39.1 13.5 32.8 15.4 intersect 47.5 45.1 35.4 34.6 15.2 Table 2: BLEU scores for systems trained using different alignment methods We also carried out experiments to optimise GIZA++ parameters, but this did not yield any significant improvements. We would like to re-visit these experiments at some future time, since we did not have sufficient time for a thorough treatment at this time. We also tried to deal with language-specific problems, as previously done for German–English (Collins et al., 2005). We created hand-written rules that move the Japanese verb from the end of the sentence to the beginning. However, we could not consistently achieve improvements using these rules. Since we did not have a part-of-speech tagger for Japanese, we had to rely on the assumption that the last word of a Japanese sentence is the verb. We did not apply these rules in our official submission. 3.1 Optimising Word Alignment Our experience with GIZA++ alignments has been that IBM Model training performs poorly for source words that occur only once in the training corpus. These words are often incorrectly"
2005.iwslt-1.8,P04-1083,0,0.0145603,"s. 1 Introduction The statistical machine translation group at the University of Edinburgh has been focused on open domain text translation, so we welcomed the challenge to work on the IWSLT 2005 limited domain speech translation task. We participated in the transcription translation tasks for all five language pairs, using only the supplied corpora. Our MT system was originally developed for translation of European parliament texts from German to English (Koehn et al., 2003). We extended the system while working on the DARPA challenges to translate Chinese and Arabic news texts into English (Koehn, 2004a; Koehn et al., 2005). Now, we were faced with the challenge of speech data in mostly Asian languages. The translation of transcribed speech differs in many ways from our traditional translation scenario: Much less training data is available, the domain is more limited, and the text style is very different — short questions and statements. In some respect, the task is easier, since smaller training corpora result in faster training times for the system. But it also meant that we had to re-examine various components of our system. In this paper we present an overview of our current out-of-the-"
2005.iwslt-1.8,koen-2004-pharaoh,0,0.0508616,"s. 1 Introduction The statistical machine translation group at the University of Edinburgh has been focused on open domain text translation, so we welcomed the challenge to work on the IWSLT 2005 limited domain speech translation task. We participated in the transcription translation tasks for all five language pairs, using only the supplied corpora. Our MT system was originally developed for translation of European parliament texts from German to English (Koehn et al., 2003). We extended the system while working on the DARPA challenges to translate Chinese and Arabic news texts into English (Koehn, 2004a; Koehn et al., 2005). Now, we were faced with the challenge of speech data in mostly Asian languages. The translation of transcribed speech differs in many ways from our traditional translation scenario: Much less training data is available, the domain is more limited, and the text style is very different — short questions and statements. In some respect, the task is easier, since smaller training corpora result in faster training times for the system. But it also meant that we had to re-examine various components of our system. In this paper we present an overview of our current out-of-the-"
2005.iwslt-1.8,N03-1017,1,0.060637,"ranscription track. We achieved the highest BLEU score in 2 out of 5 language pairs and had competitive results for the other language pairs. 1 Introduction The statistical machine translation group at the University of Edinburgh has been focused on open domain text translation, so we welcomed the challenge to work on the IWSLT 2005 limited domain speech translation task. We participated in the transcription translation tasks for all five language pairs, using only the supplied corpora. Our MT system was originally developed for translation of European parliament texts from German to English (Koehn et al., 2003). We extended the system while working on the DARPA challenges to translate Chinese and Arabic news texts into English (Koehn, 2004a; Koehn et al., 2005). Now, we were faced with the challenge of speech data in mostly Asian languages. The translation of transcribed speech differs in many ways from our traditional translation scenario: Much less training data is available, the domain is more limited, and the text style is very different — short questions and statements. In some respect, the task is easier, since smaller training corpora result in faster training times for the system. But it als"
2005.iwslt-1.8,W02-1018,0,0.0130598,"code of the grow-diag-final method to symmetrise word alignments. See Section 2.3 for variations of this method. See Figure 5 for an illustration. Note that unaligned words may be included within and at the border of extracted phrase pairs (third example in Figure 5). Each phrase pair, however, must include at least one alignment point. Using word-level alignments to induce phrasebased translation models is common practise in the statistical machine translation community. It has been adopted by most groups participating in the NIST MT Evaluation (Lee and Przybocki, 2005). In contrast to this, Marcu and Wong (2002) have defined a method for directly estimating phrasal translation models from parallel corpora, rather than using heuristic methods to induce phrase alignments from word alignments. Their joint probability phrase-based model is computationally demanding, and as such has not been applied to large data sets. Our group has been implementing a scalable version of the joint probability model (Mayne, 2005), and we hope to submit it as a contrastive system in next year’s IWSLT. 2.5 GIZA++ alignments. Three neighbouring points are added. The alignment point between did and a is added in the grow(-dia"
2005.iwslt-1.8,P03-1021,0,0.0327751,"ght exists • discontinuous: no alignment points to the top left or top right Given these counts, we can learn probability distributions of the form: pr (orientation|¯ e, f¯) (5) For the estimation of the probability distribution, we smooth the collected counts. This lexicalised reordering model is motivated by similar work by Tillmann (2004). Discriminative Training Recall that the components of our machine translation system are combined in a log-linear way. The weight of the feature functions, or model components, is set by minimum error rate training. We reimplemented a method suggested by Och (2003). In short, we optimise the value of the parameter weights λm by iteratively: (a) running the decoder with a currently best weight setting, (b) extracting an n-best list of possible translations, and (c) finding a better weight setting that re-ranks the n-best-list, so that a better translation score is obtained. To score translation quality, we employ the BLEU score (Papineni et al., 2002). The search for the best weight setting is a line search for each λm , which is repeated until no improvement can be achieved. We thank David Chiang of the University of Maryland for providing us with a fas"
2005.iwslt-1.8,J03-1002,0,0.0170359,"in the corpus, extracting phrase pairs that are consistent with the word alignment, and then assigning probabilities (or scores) to the obtained phrase translations. 2.3 Figure 2: Obtaining a high precision, low recall word alignment by intersecting two GIZA++ alignments Word Alignment Word alignments are obtained by first using the GIZA++ toolkit in both translation directions and then symmetrising the two alignments. Since the IBM Models implemented in GIZA++ are not able to map one target (English) word to multiple source (foreign) words, the method of symmetrising — called refined method (Och and Ney, 2003) — effectively overcomes this deficiency. Figure 2 shows the first step in the symmetrisation process: The intersection of the two GIZA++ alignments is taken. Only word alignment points that occur in both alignments are preserved. This is the intersection alignment. In a second step, additional alignment points are added. Only alignment points that are in either of the two GIZA++ alignments (or, in the union of these alignments) are considered. In the growing step, potential alignment points that connect currently unaligned words and that neighbour established alignment points are added. Neigh"
2005.iwslt-1.8,P02-1040,0,0.110837,"the components of our machine translation system are combined in a log-linear way. The weight of the feature functions, or model components, is set by minimum error rate training. We reimplemented a method suggested by Och (2003). In short, we optimise the value of the parameter weights λm by iteratively: (a) running the decoder with a currently best weight setting, (b) extracting an n-best list of possible translations, and (c) finding a better weight setting that re-ranks the n-best-list, so that a better translation score is obtained. To score translation quality, we employ the BLEU score (Papineni et al., 2002). The search for the best weight setting is a line search for each λm , which is repeated until no improvement can be achieved. We thank David Chiang of the University of Maryland for providing us with a faster version of our implementation. 3 Adaptations to IWSLT’05 Task In a period of one month, we optimised our system to the IWSLT’05 task. We chose to only participate in the transcription task using the supplied data, since we did not have adequate additional resources or tools for these language pairs, and also had not enough time to investigate these. The advantage of limiting ourselves t"
2005.iwslt-1.8,N04-4026,0,0.382449,"nected to the previous phrase at all (discontinuous). See Figure 6 for an illustration. When collecting phrase pairs, can classify them into these three categories based on: • monotone: a word alignment point to the top left exists • swap: an alignment point to the top right exists • discontinuous: no alignment points to the top left or top right Given these counts, we can learn probability distributions of the form: pr (orientation|¯ e, f¯) (5) For the estimation of the probability distribution, we smooth the collected counts. This lexicalised reordering model is motivated by similar work by Tillmann (2004). Discriminative Training Recall that the components of our machine translation system are combined in a log-linear way. The weight of the feature functions, or model components, is set by minimum error rate training. We reimplemented a method suggested by Och (2003). In short, we optimise the value of the parameter weights λm by iteratively: (a) running the decoder with a currently best weight setting, (b) extracting an n-best list of possible translations, and (c) finding a better weight setting that re-ranks the n-best-list, so that a better translation score is obtained. To score translati"
2006.amta-papers.2,N03-1017,0,0.241887,"acent words, and, because word alignments inadequately represent the real dependencies between translations. Also, by heuristically creating phrasal alignments from the Viterbi word-level alignments, we throw away the probabilities that were estimated when learning word alignment parameters and we can introduce errors. In contrast, the Joint Model can search areas of the alignment space in order to learn a distribution of possible phrasal alignments that better handles the uncertainty inherent in the translation process. Models Standard Phrase-based Model Most phrase-based models (Och, 2003b; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based model as the Standard Model. The Standard Model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a relative di"
2006.amta-papers.2,koen-2004-pharaoh,0,0.0349132,"which is about a factor of eight. Experiments The experiments were run using the GermanEnglish Europarl corpus (Koehn, 2005). Europarl contains proceedings from the European Parliament covering the years 1996-2003. The test set consisted of the standard Europarl test set of 1755 sentences which ranged from 5 to 15 words in length. This makes results directly comparable to Koehn et al. (2003). For the language model we used the SRI Language Modelling Toolkit (Stolcke, 2002) to train a trigram model on the English section of the Europarl corpus. To perform the translations we used the Pharaoh (Koehn, 2004) beam search decoder version 1.2.8, with all the standard settings. Our evaluation metric was Bleu (Papineni et al., 2002) which compares the output sentences with human translated sentences using 4-gram precision. The translation models are included within a loglinear model (Och and Ney, 2002) which allows a weighted combination of features functions. Only Figure 3. Bleu scores using 5,000 sentences training data The effect of fast hill-climbing on the quality of translations can be seen in Figure 3. The default method slightly outperforms fast hill-climbing for 15 Corpus Size Standard Model"
2006.amta-papers.2,2005.mtsummit-papers.11,0,0.0426336,"tively slow even for the smallest data sets, so the first experiment explores the gains to be made by using fast hill-climbing on a training corpus of 5000 sentences. Figure 2. Time taken for EM training in minutes per iteration for 5,000 sentences on a machine with 2Gb RAM and a 2.4GHz CPU In Figure 2 we can see that fast hill-climbing is much faster than the normal hill-climbing. We have reduced the time taken to perform the first iteration from nearly 5 hours to about 40 minutes, which is about a factor of eight. Experiments The experiments were run using the GermanEnglish Europarl corpus (Koehn, 2005). Europarl contains proceedings from the European Parliament covering the years 1996-2003. The test set consisted of the standard Europarl test set of 1755 sentences which ranged from 5 to 15 words in length. This makes results directly comparable to Koehn et al. (2003). For the language model we used the SRI Language Modelling Toolkit (Stolcke, 2002) to train a trigram model on the English section of the Europarl corpus. To perform the translations we used the Pharaoh (Koehn, 2004) beam search decoder version 1.2.8, with all the standard settings. Our evaluation metric was Bleu (Papineni et a"
2006.amta-papers.2,W02-1018,0,0.489907,"irch Chris Callison-Burch Miles Osborne School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW, UK a.c.birch-mayne@sms.ed.ac.uk Abstract word-based models because phrases provide local context which leads to better lexical choice and more reliable local reordering. However, most phrasebased models extract their phrase pairs from previously word-aligned corpora using ad-hoc heuristics. These models perform no search for optimal phrasal alignments. Even though this is an efficient strategy, it is a departure from the rigourous statistical framework of the IBM Models. Marcu and Wong (2002) proposed a Joint Probability Model which directly estimates phrase translation probabilities from the corpus. This model neither relies on potentially sub-optimal word alignments nor on heuristics for phrase extraction. Instead, it searches the phrasal alignment space, simultaneously learning translation lexicons for both words and phrases. The Joint Model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation (Przybocki, 2004). However, considering all possible phrases and all their possible alignme"
2006.amta-papers.2,P02-1038,0,0.0294569,"s which ranged from 5 to 15 words in length. This makes results directly comparable to Koehn et al. (2003). For the language model we used the SRI Language Modelling Toolkit (Stolcke, 2002) to train a trigram model on the English section of the Europarl corpus. To perform the translations we used the Pharaoh (Koehn, 2004) beam search decoder version 1.2.8, with all the standard settings. Our evaluation metric was Bleu (Papineni et al., 2002) which compares the output sentences with human translated sentences using 4-gram precision. The translation models are included within a loglinear model (Och and Ney, 2002) which allows a weighted combination of features functions. Only Figure 3. Bleu scores using 5,000 sentences training data The effect of fast hill-climbing on the quality of translations can be seen in Figure 3. The default method slightly outperforms fast hill-climbing for 15 Corpus Size Standard Model Joint Model + IBM + IBM + Lex the first few iterations, but then fast hill-climbing overtakes it. The difference in performance between the two methods is small and we apply fast hillclimbing in the remaining experiments. 5.2 IBM Constraints 10,000 21.69 19.93 22.13 22.79 20,000 23.61 23.08 24."
2006.amta-papers.2,J04-4002,0,0.0123472,"hout compromising the quality of the resulting translations. 2 2.1 for optimal phrase pairs. Instead, it extracts phrase pairs (f i , ei ) in the following manner. First, it uses the IBM Models to learn the Viterbi alignments for English to Foreign and Foreign to English. It then uses a heuristic to reconcile the two alignments, starting from the points of high confidence in the intersection of the two Viterbi alignments and growing towards the points in the union. Points from the union are selected if they are adjacent to points from the intersection and their words are previously unaligned. Och and Ney (2004) discusses and compares variations on this strategy. Phrases are then extracted by selecting phrase pairs which are ‘consistent’ with the symmetrised alignment. Here ‘consistent’ means that all words within the source language phrase are only aligned to the words of the target language phrase and vice versa. Finally the phrase translation probability distribution is estimated using the relative frequencies of the extracted phrase pairs. This approach to phrase extraction means that phrasal alignments are locked into the symmetrised alignment. This is problematic because the symmetrisation proc"
2006.amta-papers.2,P03-1021,0,0.509867,"oduction Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages. It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data. The original IBM Models (Brown et al., 1993) learned only word-to-word alignment probabilities which made it computationally feasible to estimate model parameters from large amounts of training data. Phrase-based SMT models, such as the Alignment Template Model (Och, 2003b), improve on 10 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 10-18, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas space of the Joint Model to areas where most of the unpromising phrasal alignments are eliminated and yet as many potentially useful alignments as possible are still explored. The Joint Model is constrained to phrasal alignments which do not contradict a set high confidence word alignments for each sentence. These high confidence alignments can incorporate information from both statisti"
2006.amta-papers.2,P02-1040,0,0.108735,"(Koehn, 2005). Europarl contains proceedings from the European Parliament covering the years 1996-2003. The test set consisted of the standard Europarl test set of 1755 sentences which ranged from 5 to 15 words in length. This makes results directly comparable to Koehn et al. (2003). For the language model we used the SRI Language Modelling Toolkit (Stolcke, 2002) to train a trigram model on the English section of the Europarl corpus. To perform the translations we used the Pharaoh (Koehn, 2004) beam search decoder version 1.2.8, with all the standard settings. Our evaluation metric was Bleu (Papineni et al., 2002) which compares the output sentences with human translated sentences using 4-gram precision. The translation models are included within a loglinear model (Och and Ney, 2002) which allows a weighted combination of features functions. Only Figure 3. Bleu scores using 5,000 sentences training data The effect of fast hill-climbing on the quality of translations can be seen in Figure 3. The default method slightly outperforms fast hill-climbing for 15 Corpus Size Standard Model Joint Model + IBM + IBM + Lex the first few iterations, but then fast hill-climbing overtakes it. The difference in perfor"
2006.amta-papers.2,P92-1017,0,0.0866123,"18288740923 4.4145633531e+32 2.7340255177e+83 Table 1. The number of possible phrasal alignments for sentence pairs calculated using Stirling numbers of the second kind. Table 1 shows just how many phrasal alignments are possible between sentences of different length. Even for medium length sentences that are 20 words in lengths, the total number of alignments is huge. Apart from being intractable, when one has a very large parameter estimation space the EM algorithm struggles to discover good parameters. One approach to dealing with this problem is to constrain the search space. For example, Pereira and Schabes (1992) proposed a method for dealing with this problem for PCFG estimation from treebanks. They encouraged the probabilities into good regions of the parameter space by constraining the search to only consider parses that did not cross Penn-Treebank nodes. We adopt a similar approach for constraining the joint model, by only considering alignments that do not contradict high probability word alignments. During EM a very small proportion of the possible alignments are searched and many good alignments are likely to be missed. Normally alignments 2.2.2 Expectation Maximisation After initialising the t"
2006.amta-papers.2,J93-2003,0,0.0103816,"and linguistically motivated word alignments. This method reduces the complexity and size of the Joint Model and allows it to display performance superior to the standard phrase-based models for small amounts of training material. 1 Introduction Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages. It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data. The original IBM Models (Brown et al., 1993) learned only word-to-word alignment probabilities which made it computationally feasible to estimate model parameters from large amounts of training data. Phrase-based SMT models, such as the Alignment Template Model (Och, 2003b), improve on 10 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 10-18, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas space of the Joint Model to areas where most of the unpromising phrasal alignments are eliminated and yet as many potentially useful alignments as possible are s"
2006.amta-papers.2,P04-1023,1,0.129086,"al Viterbi alignments. Concepts which contain many f c(e, f ) = (1 − λ)p(e, f |E, F ) + λpc(e, f ) The fractional count for each concept in each sentence is calculated by interpolating the joint probability of the concept, based on the Stirling numbers, and the prior count, which reflects the probability of the phrasal alignment given the high confidence word alignments. The use of the weight to balance the two contributions allows us to adjust for differences in scale and our confidence in each of the two measures. After testing various settings for λ the value 0.5 gave the best Bleu scores. Callison-Burch et al. (2004) used a similar technique for combining word and sentence aligned data. However, they inserted data from labelled word alignments which meant that they did not need to sum over all possible alignments for a sentence pair. 4.2 Fast Hill-climbing The constraints on the Joint Model reduce its size by restricting the initialisation phase of the training. This is one of the two major drawbacks of the 14 three features were used for both the Joint and the Standard Model: p(e|f ), p(f |e) and the language model, and they were given equal weights. model discussed by Marcu and Wong (2002). The other ma"
2006.amta-papers.2,2003.mtsummit-papers.53,0,0.0230242,"cause word alignments inadequately represent the real dependencies between translations. Also, by heuristically creating phrasal alignments from the Viterbi word-level alignments, we throw away the probabilities that were estimated when learning word alignment parameters and we can introduce errors. In contrast, the Joint Model can search areas of the alignment space in order to learn a distribution of possible phrasal alignments that better handles the uncertainty inherent in the translation process. Models Standard Phrase-based Model Most phrase-based models (Och, 2003b; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based model as the Standard Model. The Standard Model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a relative distortion probability"
2006.amta-papers.2,W06-3105,0,0.415441,"ature functions were optimized using minimum error rate training (Och, 2003a). Joint + IBM Standard Model BLEU 26.17 28.35 Pruning eliminates many phrase pairs, but further investigation indicates that this has little impact on BLEU scores. The fact that only a small proportion of the alignment space is searched is very likely to be hampering the Joint Model’s performance. The small number of alignments visited leads to data sparseness and over-fitting. Another factor could be efficiency trade-offs like the fast but not optimal competitive linking search for phrasal alignments. 6 Related Work DeNero et al. (2006) argue that training a translation model at the phrase level results in inferior parameters to the standard, heuristic phrase-based models. They suggest that the reason for this is that EM optimizes by selecting different segmentations and loses important phrase translation ambiguity. They say that the model results in a very peaked distribution and entropy drops too low. However, their argument only holds for conditional models. In a conditional model, there is competition for the probability mass of the conditioned word, and instead of spreading that mass between different translations, diff"
2006.amta-papers.2,W06-3114,0,0.00807513,"no such competition and the resulting phrase table’s entropy is in fact higher than that of the Standard Model. Size 2.28 19.04 Table 6. Bleu scores and model size in millions of phrase pairs for Spanish-English The results in Table 6 show that the Joint Model is capable of training on larger data sets, with a reasonable performance. On smaller data sets, as shown in sections 5.2 and 5.3 the Joint Model shows performance superior or comparable to the Standard Model. However, here it seems that the Standard Model has an advantage which is statistically significant according to the sign method (Koehn and Monz, 2006). This is almost certainly related to the fact that the Joint Model results in a much smaller phrase table. The size of the resulting Joint Model is in fact comparable to the size of the model in previous experiments when training with just 20,000 sentences. This is because the model must be kept in memory for collecting fractional counts in EM and even though the corpus is bigger, the memory available remains the same (the Standard Model phrase table is created on disk). To keep the Joint Model within memory, pruning is necessary after initialization because this is where most phrase pairs ar"
2006.amta-papers.2,P97-1063,0,\N,Missing
2006.amta-papers.2,P05-1066,0,\N,Missing
2006.amta-papers.2,2005.iwslt-1.8,1,\N,Missing
2006.amta-papers.2,P00-1056,0,\N,Missing
C00-1085,J97-4005,0,0.413904,"Missing"
C00-1085,1997.iwpt-1.15,0,0.0267024,"rence in some corpus). 4. Map the normalised distances into probabilities. If d(p) is the normalised distance of TSG parse p, then associate with parse p the reference probability given by the maximum likelihood estimator: Px2d (p)d(x) (4) t Our approach therefore gives partial credit (a nonzero reference probability) to all parses in t . R is therefore not as discontinuous as the equivalent distribution used by Johnson et al. We therefore do not need to use simulated annealing or other numerically intensive techniques to estimate models. 4 Our distance metric is the same one used by Hektoen (Hektoen, 1997) 6 Experiments Here we present two sets of experiments. The rst set demonstrate the existence of an informative sample. It also shows some of the characteristics of three sampling strategies. The second set of experiments is larger in scale, and show RFMs (both lexicalised and unlexicalised) estimated using sentences up to 30 tokens long. Also, the e ects of a Gaussian prior are demonstrated as a way of (partially) dealing with over tting. 6.1 Testing the Various Sampling Max parses 1 2 3 5 10 100 1000 1 Size 6626 12331 17026 24878 39581 119694 246686 267400 Rand 25.2 (51.7) 37.9 (63.0) 43.2 ("
C00-1085,P99-1069,0,0.452857,"Missing"
C00-1085,A97-1052,0,\N,Missing
C02-1077,J97-4005,0,0.036326,"te-value grammars), and rstly show what happens when the initial weight settings are set uniformly (to zero). We then show what happens when these initial settings are randomly set, and nally, what happens when we average over randomly initialised maximum likelihood solutions. any  5 The exact behaviour of the algorithm will depend on the training data, so this is not the only imaginable outcome, but it is certainly a plausible one. 4 Log-linear Modelling of Attribute-Value Grammars Here we show how attribute-value grammars may be modelled using log-linear models. Abney gives fuller details (Abney, 1997). Let G be an attribute-value grammar, and D a set of sentences within the string-set de ned by L(G). A log-linear model, M , consist of two components: a set of , F and a set of , . The (unnormalised) total weight of a parse x, (x), is a function of the k features that are `active&apos; on a parse: features weights (x) = exp( X  f (x)) k i =1 i (6) i The probability of a parse, P (x M ), is simply the result of normalising the total weight associated with that parse: 1 (x) (7) P (x M ) = j j Z = XZ y 2 (y) (8) is the union of the set of parses assigned to each sentence in D by the grammar G, suc"
C02-1077,W02-2018,0,0.0318647,"ry large models, this may be prohibitively diÆcult and time-consuming. Model averaging can also cancel out variations caused by a particular choice of initial settings. However, this implies a greater computational burden as IIS will need to be run many times in order to gain a representative sample of models. The number of features in the model could be reduced using feature selection methods (for example (Mullen and Osborne, 2000)). Although IIS is a useful tool for estimating loglinear models, we have since moved-on to estimating     models using limited-memory variable-metric methods (Malouf, 2002). Our ndings show that convergence, for a range of problems, is faster. An interesting question is seeing the extent to which other numerical methods for estimating log-linear models are sensitive to initial parameter values. Finally, it should be noted that our theoretical results apply to a more general setting than that of log-linear models trained using the IIS algorithm. The problem of overlapping features could in principle occur in any situation in which a model has a linear combination of features, and a `hill-climbing&apos; algorithm is used to seek a maximum-likelihood solution. Acknowled"
C02-1077,W00-0709,1,0.869315,"starting point in the space of initial parameters. We could also examine all the features to determine which ones overlap, and force it to balance the nal weights of these features. For very large models, this may be prohibitively diÆcult and time-consuming. Model averaging can also cancel out variations caused by a particular choice of initial settings. However, this implies a greater computational burden as IIS will need to be run many times in order to gain a representative sample of models. The number of features in the model could be reduced using feature selection methods (for example (Mullen and Osborne, 2000)). Although IIS is a useful tool for estimating loglinear models, we have since moved-on to estimating     models using limited-memory variable-metric methods (Malouf, 2002). Our ndings show that convergence, for a range of problems, is faster. An interesting question is seeing the extent to which other numerical methods for estimating log-linear models are sensitive to initial parameter values. Finally, it should be noted that our theoretical results apply to a more general setting than that of log-linear models trained using the IIS algorithm. The problem of overlapping features could in"
C02-1077,C00-1085,1,0.761849,"ber of Models Throughout, we used the same training set. This consisted of a sample of 53795 parses (produced from sentences at most 15 tokens long, with at most 15 parses per sentence). The sentences were drawn from the parsed Wall Street Journal, and all could be parsed using our grammar. The motivation for this choice of training set came from the fact that when the sample of sentences is too small, the resulting model will tend to under t. Likewise, when the training set is too large, the model will tend to over t. A sample of appropriate size (which can be found using a simple search, as Osborne (2000) demonstrated) will therefore neither signi cantly under t nor over t. Quite apart from estimation issues related to sample size, because we repeatedly estimate models, using a sample that is just suÆciently large (and no larger) allows us to make signi cant computational savings. We used a disjoint development set and testing set. The development set consisted of 2620 parses, derived from parsing sentences at most 30 tokens long, with at most 100 parses per sentence. The testing set was randomly sampled from the Wall Street Journal, and consisted of 469 sentences, with each sentence at most 3"
C02-1077,A97-1052,0,\N,Missing
C02-1077,P99-1069,0,\N,Missing
D07-1049,P05-1033,0,0.00453226,"ient randomised data structure for representing sets, could be used to store corpus statistics efficiently. Here, we propose a framework for deriving smoothed n-gram models from such structures and show via machine translation experiments that these smoothed Bloom filter language models may be used as direct replacements for standard n-gram models in SMT. Introduction Language modelling (LM) is a crucial component in statistical machine translation (SMT). Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models (Chiang, 2005). Although it is well-known that higher-order language The space requirements of a Bloom filter are quite spectacular, falling significantly below informationtheoretic error-free lower bounds. This efficiency, however, comes at the price of false positives: the filter may erroneously report that an item not in the set is a member. False negatives, on the other hand, will 468 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 468–476, Prague, June 2007. 2007 Association for Computational Linguistics nev"
D07-1049,D07-1091,0,0.0120761,"plying such bounds on the performance of the model within an SMT decoder in the experiments below. Working upwards from the lower-order models also allows us to truncate the computation before the highest level if the denominator in the maximum likelihood term is found with a zero count at any stage (no higher-order terms can be non-zero given this). 472 All of our experiments use publically available resources. Our main experiments use the FrenchEnglish section of the Europarl (EP) corpus for parallel data and language modelling (Koehn, 2003). Decoding is carried-out using the Moses decoder (Koehn and Hoang, 2007). We hold out 1,000 test sentences and 500 development sentences from the parallel text for evaluation purposes. The parameters for the feature functions used in this log-linear decoder are optimised using minimum error rate (MER) training on our development set unless otherwise stated. All evaluation is in terms of the BLEU score on our test set (Papineni et al., 2002). Our baseline language models were created using the SRILM toolkit (Stolcke, 2002). We built 3, 4 and 5-gram models from the Europarl corpus using interpolated Witten-Bell smoothing (WB); no ngrams are dropped from these models"
D07-1049,P02-1040,0,0.117901,"riments use publically available resources. Our main experiments use the FrenchEnglish section of the Europarl (EP) corpus for parallel data and language modelling (Koehn, 2003). Decoding is carried-out using the Moses decoder (Koehn and Hoang, 2007). We hold out 1,000 test sentences and 500 development sentences from the parallel text for evaluation purposes. The parameters for the feature functions used in this log-linear decoder are optimised using minimum error rate (MER) training on our development set unless otherwise stated. All evaluation is in terms of the BLEU score on our test set (Papineni et al., 2002). Our baseline language models were created using the SRILM toolkit (Stolcke, 2002). We built 3, 4 and 5-gram models from the Europarl corpus using interpolated Witten-Bell smoothing (WB); no ngrams are dropped from these models or any of the BF-LMs. The number of distinct n-gram types in these baseline models as well as their sizes on disk and as compressed by gzip are given in Table 1; the gzip figures are given as an approximate (and optimistic) lower bound on lossless representations of these models.2 The BF-LM models used in these experiments were all created from the same corpora followi"
D07-1049,P07-1065,1,0.395757,"that hold between related n-gram statistics in order to further reduce the BF storage requirements and the error rate of the derived probabilities. We use these models as replacements for a conventional language model in machine translation experiments. 1 Against this background, we consider a radically different approach to language modelling. Instead of explicitly storing all distinct n-grams from our corpus, we create an implicit randomised representation of these statistics. This allows us to drastically reduce the space requirements of our models. In this paper, we build on recent work (Talbot and Osborne, 2007) that demonstrated how the Bloom filter (Bloom (1970); BF), a space-efficient randomised data structure for representing sets, could be used to store corpus statistics efficiently. Here, we propose a framework for deriving smoothed n-gram models from such structures and show via machine translation experiments that these smoothed Bloom filter language models may be used as direct replacements for standard n-gram models in SMT. Introduction Language modelling (LM) is a crucial component in statistical machine translation (SMT). Standard ngram language models assign probabilities to translation"
D08-1023,P08-1024,1,0.699798,"ases in translation performance. 1 Introduction The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention. Discriminative approaches are widely seen as a promising technique, potentially allowing us to further the state-of-the-art. Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (Liang et al., 2006; Watanabe et al., 2006), or rescaling a product of sub-models (Och, 2003; Ittycheriah and Roukos, 2007). Recent work by Blunsom et al. (2008) has shown how translation can be framed as a probabilistic log-linear model, where the distribution over translations is modelled in terms of a latent variable on derivations. Their approach was globally optimised and discriminative trained. However, a language model, an information source known to be Here, we show how language models can be incorporated into large-scale discriminative translation models, without losing the probabilistic interpretation of the model. The key insight is that we can use Monte-Carlo methods to approximate the partition function, thereby allowing us to tackle the"
D08-1023,J07-2003,0,0.837682,"nous context free grammar (SCFG, (Lewis II and Stearns, 1968)) describes the generation of pairs of strings. A string pair is generated by applying a series of paired context-free rewrite rules of the form, X → hα, γ, ∼i, where X is a nonterminal, α and γ are strings of terminals and nonterminals and ∼ specifies a one-to-one alignment between non-terminals in α and γ. In the context of SMT, by assigning the source and target languages to the respective sides of a SCFG it is possible to describe translation as the process of parsing the source sentence, while generating the target translation (Chiang, 2007). In this paper we only consider grammars extracted using the heuristics described for the Hiero SMT system (Chiang, 2007). Note however that our approach is general and could be used with other synchronous grammar transducers (e.g., (Galley et al., 2006)). SCFG productions can specify that the order of the child non-terminals is the same in both languages (a monotone production), or is reversed (a reordering production). Without loss of generality, here we add the restriction that non-terminals on the source and target sides of the grammar must have the same category. Figure 1 shows an exampl"
D08-1023,P06-1121,0,0.0451368,"α and γ are strings of terminals and nonterminals and ∼ specifies a one-to-one alignment between non-terminals in α and γ. In the context of SMT, by assigning the source and target languages to the respective sides of a SCFG it is possible to describe translation as the process of parsing the source sentence, while generating the target translation (Chiang, 2007). In this paper we only consider grammars extracted using the heuristics described for the Hiero SMT system (Chiang, 2007). Note however that our approach is general and could be used with other synchronous grammar transducers (e.g., (Galley et al., 2006)). SCFG productions can specify that the order of the child non-terminals is the same in both languages (a monotone production), or is reversed (a reordering production). Without loss of generality, here we add the restriction that non-terminals on the source and target sides of the grammar must have the same category. Figure 1 shows an example derivation for Chinese to English translation. 216 Using Equation (1), the conditional probability of a target translation given the source is the sum over all of its derivations: X pΛ (e|f ) = pΛ (d, e|f ) d∈∆(e,f ) where ∆(e, f ) is the set of all der"
D08-1023,2006.amta-papers.8,0,0.193237,"g us to tackle the extra computational burden associated with adding the language model. This approach is theoretically justified and means that the model continues to be both probabilistic and globally optimised. As expected, using a language model dramatically increases translation performance. Our second major contribution is an exploitation of syntactic features. By encoding source syntax as features allows the model to use, or ignore, this information as it sees fit, thereby avoiding the problems of coverage and sparsity associated with directly incorporating the syntax into the grammar (Huang et al., 2006; Mi et al., 2008). We report on translation gains using this approach. We begin by introducing the synchronous grammar approach to SMT in Section 2. In Section 3 we define the parametric form of our model and describe techniques for approximating the intractable space of all translations for a given source sentence. In Section 4 we evaluate the ability of our model to effectively estimate the highly dependent weights for the sparse features and realvalued language model. In addition we describe how 215 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pag"
D08-1023,N07-1008,0,0.0544121,"ctured syntactic features, and achieving increases in translation performance. 1 Introduction The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention. Discriminative approaches are widely seen as a promising technique, potentially allowing us to further the state-of-the-art. Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (Liang et al., 2006; Watanabe et al., 2006), or rescaling a product of sub-models (Och, 2003; Ittycheriah and Roukos, 2007). Recent work by Blunsom et al. (2008) has shown how translation can be framed as a probabilistic log-linear model, where the distribution over translations is modelled in terms of a latent variable on derivations. Their approach was globally optimised and discriminative trained. However, a language model, an information source known to be Here, we show how language models can be incorporated into large-scale discriminative translation models, without losing the probabilistic interpretation of the model. The key insight is that we can use Monte-Carlo methods to approximate the partition functi"
D08-1023,N04-1022,0,0.0228719,"rue distribu(e|f ) model we tion over derivations. For the p+LM Λ suffer the same problem as in training and cannot build the full chart. Instead a chart is built using the cube-pruning algorithm with a wide beam and we then draw samples from this chart. Although sampling from a reduced chart will result in biased samples, in Section 4 we show this approach to be effective in practice.2 In Section 4 we compare our sampling approach to the heuristic beam search proposed by Blunsom et al. (2008). It is of interest to compare our proposed decoding algorithms to minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004), a commonly used decoding method. From a theoretical standpoint, the summing of derivations for a given translation is exactly 2 We have experimented with using a Metropolis Hastings sampler, with p−LM (e|f ) as the proposal distribution, to samΛ ple from the true distribution with the language model. Unfortunately the sample rejection rate was very high such that this method proved infeasibly slow. equivalent to performing MBR with a 0/1 loss function over derivations. From a practical perspective, MBR is normally performed with B LEU as the loss and approximated using n-best lists. These n-"
D08-1023,P06-1096,0,0.29552,"Missing"
D08-1023,P08-1023,0,0.0289502,"xtra computational burden associated with adding the language model. This approach is theoretically justified and means that the model continues to be both probabilistic and globally optimised. As expected, using a language model dramatically increases translation performance. Our second major contribution is an exploitation of syntactic features. By encoding source syntax as features allows the model to use, or ignore, this information as it sees fit, thereby avoiding the problems of coverage and sparsity associated with directly incorporating the syntax into the grammar (Huang et al., 2006; Mi et al., 2008). We report on translation gains using this approach. We begin by introducing the synchronous grammar approach to SMT in Section 2. In Section 3 we define the parametric form of our model and describe techniques for approximating the intractable space of all translations for a given source sentence. In Section 4 we evaluate the ability of our model to effectively estimate the highly dependent weights for the sparse features and realvalued language model. In addition we describe how 215 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 215–223, c Hono"
D08-1023,P03-1021,0,0.173238,"acting structured syntactic features, and achieving increases in translation performance. 1 Introduction The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention. Discriminative approaches are widely seen as a promising technique, potentially allowing us to further the state-of-the-art. Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (Liang et al., 2006; Watanabe et al., 2006), or rescaling a product of sub-models (Och, 2003; Ittycheriah and Roukos, 2007). Recent work by Blunsom et al. (2008) has shown how translation can be framed as a probabilistic log-linear model, where the distribution over translations is modelled in terms of a latent variable on derivations. Their approach was globally optimised and discriminative trained. However, a language model, an information source known to be Here, we show how language models can be incorporated into large-scale discriminative translation models, without losing the probabilistic interpretation of the model. The key insight is that we can use Monte-Carlo methods to a"
D08-1023,P02-1040,0,0.103942,"on task (Eck and Hori, 2005), using the 2004 test set as development data for tuning the hyperparameters and MERT training the benchmark systems. The statistics for this data are presented in Table 1.3 The training data made available for this task consisted of 40k pairs of transcribed utterances, drawn from the travel domain. The development and test data for this task are somewhat unusual in that each sentence has a single human translated reference, and fifteen paraphrases of this reference, provided by monolingual annotators. Model performance is evaluated using the standard B LEU metric (Papineni et al., 2002) which measures average n-gram precision, n ≤ 4, and we use the NIST definition of the brevity penalty for multiple reference test sets. We provide evaluation against both the entire multi-reference sets, and the single human translation. Our translation grammar is induced using the standard alignment and rule extraction heuristics used in hierarchical translation models (Chiang, 2007).4 As these heuristics aren’t based on a generative model, and don’t guarantee that the target translation will be reachable from the source, we discard those sentence pairs for which we cannot produce a derivati"
D08-1023,P06-1098,0,0.0178298,"rate the power of the discriminative training paradigm by extracting structured syntactic features, and achieving increases in translation performance. 1 Introduction The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention. Discriminative approaches are widely seen as a promising technique, potentially allowing us to further the state-of-the-art. Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (Liang et al., 2006; Watanabe et al., 2006), or rescaling a product of sub-models (Och, 2003; Ittycheriah and Roukos, 2007). Recent work by Blunsom et al. (2008) has shown how translation can be framed as a probabilistic log-linear model, where the distribution over translations is modelled in terms of a latent variable on derivations. Their approach was globally optimised and discriminative trained. However, a language model, an information source known to be Here, we show how language models can be incorporated into large-scale discriminative translation models, without losing the probabilistic interpretation of the model. The key in"
D08-1023,2005.iwslt-1.1,0,\N,Missing
D08-1078,J93-2003,0,0.00596267,"ge pairs, translation quality is still low. Certain systematic differences between languages can be used to predict this. Many researchers have speculated on the reasons why machine translation is hard. However, there has never been, to our knowledge, an analysis of what the actual contribution of different aspects of language pairs is to translation performance. This understanding of where the difficulties lie will allow researchers to know where to most gainfully direct their efforts to improving the current models of machine translation. Many of the challenges of SMT were first outlined by Brown et al. (1993). The original IBM Models were broken down into separate translation and distortion models, recognizing the importance of word order differences in modeling translation. Brown et al. also highlighted the importance of modeling morphology, both for reducing sparse counts and improving parameter estimation and for the correct production of translated forms. We see these two factors, reordering and morphology, as fundamental to the quality of machine translation output, and we would like to quantify their impact on system performance. It is not sufficient, however, to analyze the morphological co"
D08-1078,W08-0309,1,0.276713,"ance levels of coefficients and R2 are also 751 = 0.16 pt da es = 0.24 fr nl fi = 0.32 de = 0.4 Target Languages Figure 7. System performance - the width of the squares indicates the system performance in terms of the B LEU score. Explanatory Variable Target Vocab. Size Language Similarity Reordering Amount Target Vocab. Size2 Language Similarity2 Interaction: Reord/Sim We used the phrase-based model Moses (Koehn et al., 2007) for the experiments with all the standard settings, including a lexicalized reordering model, and a 5-gram language model. Tests were run on the ACL WSMT 2008 test set (Callison-Burch et al., 2008). 6.1 sv en el Coefficient -3.885 3.274 -1.883 1.017 -1.858 -1.4536 *** *** *** *** ** *** Table 3. The impact of the various explanatory features on the B LEU score via their coefficients in the minimal adequate model. given where * means p &lt; 0.05, ** means p &lt; 0.01, and *** means p &lt; 0.001. 7 7.1 Results Combined Model The first question we are interested in answering is which factors contribute most and how they interact. We fit a multiple regression model to the data. The source vocabulary size has no significant effect on the outcome. All explanatory variable vectors were normalized to be"
D08-1078,W07-0729,0,0.0116329,"rms of the same lemma as completely independent of one another. This can result in sparse statistics and poorly estimated models. Furthermore, different variations of the lemma may result in crucial differences in meaning that affect the quality of the translation. Work on improving MT systems’ treatment of morphology has focussed on either reducing word forms to lemmas to reduce sparsity (Goldwater and McClosky, 2005; Talbot and Osborne, 2006) or including morphological information in decod746 en fr it es pt el nl sv da de fi Language Figure 1. Average vocabulary size for each language. ing (Dyer, 2007). Although there is a significant amount of research into improving the treatment of morphology, in this paper we aim to discover the effect that different levels of morphology have on translation. We measure the amount of morphological complexity that exists in both languages and then relate this to translation performance. Some languages seem to be intuitively more complex than others, for instance Finnish appears more complex than English. There is, however, no obvious way of measuring this complexity. One method of measuring complexity is by choosing a number of hand-picked, intuitive prop"
D08-1078,H05-1085,0,0.0139242,"lexity of the language pairs involved in translation is widely recognized as one of the factors influencing translation performance. However, most statistical translation systems treat different inflected forms of the same lemma as completely independent of one another. This can result in sparse statistics and poorly estimated models. Furthermore, different variations of the lemma may result in crucial differences in meaning that affect the quality of the translation. Work on improving MT systems’ treatment of morphology has focussed on either reducing word forms to lemmas to reduce sparsity (Goldwater and McClosky, 2005; Talbot and Osborne, 2006) or including morphological information in decod746 en fr it es pt el nl sv da de fi Language Figure 1. Average vocabulary size for each language. ing (Dyer, 2007). Although there is a significant amount of research into improving the treatment of morphology, in this paper we aim to discover the effect that different levels of morphology have on translation. We measure the amount of morphological complexity that exists in both languages and then relate this to translation performance. Some languages seem to be intuitively more complex than others, for instance Finnis"
D08-1078,N03-1017,1,0.0249735,"nce. 749 Experimental Design We select the German-English language pair because it has a reasonably high level of reordering. A manually aligned German-English corpus was provided by Chris Callison-Burch and consists of the first 220 sentences of test data from the 2006 ACL Workshop on Machine Translation (WMT06) test set. This test set is from a held out portion of the Europarl corpus. The automatic alignments were extracted by appending the manually aligned sentences on to the respective Europarl v3 corpora and aligning them using GIZA++ (Och and Ney, 2003) and the growfinal-diag algorithm (Koehn et al., 2003). 5.3.2 P Automatic Alignments Results In order to use automatic alignments to extract reordering statistics, we need to show that reorderings from automatic alignments are comparable to those from manual alignments. We first look at global reordering statistics and then we look in more detail at the reordering distribution of the corpora. Table 2 shows the amount of reordering in the WMT06 test corpora, with both manual and automatic alignments, and in the automatically aligned Europarl DE-EN parallel corpus. fi 0.8 nl Source Languages 0.4 0.6 ACL Test Manual ACL Test Automatic Euromatrix 0.2"
D08-1078,P07-2045,1,0.0191775,"etermine whether the coefficients for the independent variables are reliably different from zero. We also test how well the model explains the data using an R2 test. The two-tailed significance levels of coefficients and R2 are also 751 = 0.16 pt da es = 0.24 fr nl fi = 0.32 de = 0.4 Target Languages Figure 7. System performance - the width of the squares indicates the system performance in terms of the B LEU score. Explanatory Variable Target Vocab. Size Language Similarity Reordering Amount Target Vocab. Size2 Language Similarity2 Interaction: Reord/Sim We used the phrase-based model Moses (Koehn et al., 2007) for the experiments with all the standard settings, including a lexicalized reordering model, and a 5-gram language model. Tests were run on the ACL WSMT 2008 test set (Callison-Burch et al., 2008). 6.1 sv en el Coefficient -3.885 3.274 -1.883 1.017 -1.858 -1.4536 *** *** *** *** ** *** Table 3. The impact of the various explanatory features on the B LEU score via their coefficients in the minimal adequate model. given where * means p &lt; 0.05, ** means p &lt; 0.01, and *** means p &lt; 0.001. 7 7.1 Results Combined Model The first question we are interested in answering is which factors contribute m"
D08-1078,2005.mtsummit-papers.11,1,0.302052,"ance. It is not sufficient, however, to analyze the morphological complexity of the source and target languages. It is also very important to know how similar the morphology is between the two languages, as two languages which are morphologically complex in very similar ways, could be relatively easy to translate. Therefore, we also include a measure of the family relatedness of languages in our analysis. The impact of these factors on translation is measured by using linear regression models. We perform the analysis with data from 110 different language pairs drawn from the Europarl project (Koehn, 2005). This contains parallel data for the 11 official language pairs of the European Union, providing a rich variety of different language characteristics for our experiments. Many research papers report results on only one or two languages pairs. By analyzing so many language pairs, we are able to provide a much wider perspective on the challenges facing machine translation. This analysis is important as it provides very strong motivation for further research. The findings of this paper are as follows: (1) each of the main effects, reordering, target language complexity and language relatedness,"
D08-1078,J03-1002,0,0.00182469,"ce side, normalized by the length of the source sentence. 749 Experimental Design We select the German-English language pair because it has a reasonably high level of reordering. A manually aligned German-English corpus was provided by Chris Callison-Burch and consists of the first 220 sentences of test data from the 2006 ACL Workshop on Machine Translation (WMT06) test set. This test set is from a held out portion of the Europarl corpus. The automatic alignments were extracted by appending the manually aligned sentences on to the respective Europarl v3 corpora and aligning them using GIZA++ (Och and Ney, 2003) and the growfinal-diag algorithm (Koehn et al., 2003). 5.3.2 P Automatic Alignments Results In order to use automatic alignments to extract reordering statistics, we need to show that reorderings from automatic alignments are comparable to those from manual alignments. We first look at global reordering statistics and then we look in more detail at the reordering distribution of the corpora. Table 2 shows the amount of reordering in the WMT06 test corpora, with both manual and automatic alignments, and in the automatically aligned Europarl DE-EN parallel corpus. fi 0.8 nl Source Languages 0.4"
D08-1078,P02-1040,0,0.10138,"Missing"
D08-1078,2006.amta-papers.25,0,0.0889646,"Missing"
D08-1078,P06-1122,1,0.768304,"nvolved in translation is widely recognized as one of the factors influencing translation performance. However, most statistical translation systems treat different inflected forms of the same lemma as completely independent of one another. This can result in sparse statistics and poorly estimated models. Furthermore, different variations of the lemma may result in crucial differences in meaning that affect the quality of the translation. Work on improving MT systems’ treatment of morphology has focussed on either reducing word forms to lemmas to reduce sparsity (Goldwater and McClosky, 2005; Talbot and Osborne, 2006) or including morphological information in decod746 en fr it es pt el nl sv da de fi Language Figure 1. Average vocabulary size for each language. ing (Dyer, 2007). Although there is a significant amount of research into improving the treatment of morphology, in this paper we aim to discover the effect that different levels of morphology have on translation. We measure the amount of morphological complexity that exists in both languages and then relate this to translation performance. Some languages seem to be intuitively more complex than others, for instance Finnish appears more complex than"
D08-1078,J97-3002,0,0.271232,"largely driven by syntactic differences between languages and can involve complex rearrangements between nodes in synchronous trees. Modeling reordering exactly would require a synchronous tree-substitution grammar. This representation would be sparse and heterogeneous, limiting its usefulness as a basis for analysis. We make an important simplifying assumption in order for the detection and extraction of reordering data to be tractable and useful. We assume that reordering is a binary process occurring between two blocks that are adjacent in the source. This is similar to the ITG constraint (Wu, 1997), however our reorderings are not dependent on a synchronous grammar or a derivation which covers the sentences. There are also similarities with the Human-Targeted Transla748 A consistent block means that between Atmin and Atmax there are no target word positions aligned to source words outside of the block’s source span As . A reordering is consistent if the block projected from rAsmin to rBsmax is consistent. The following algorithm detects reorderings and determines the dimensions of the blocks involved. We step through all the source words, and if a word is reordered in the target with re"
D09-1079,D07-1090,0,0.260762,"The space savings are due to compact storage of counts and retention of only a small subset of the available n-grams in the data stream. Since the final LMs are still lossless (modulo counts), the resulting LM needs significant space. It is trivial to use probabilistic counting within our framework. 3.3 Figure 2: Stream-based translation. The online RLM uses data from the target stream and the last test point in the source stream for adaptation. Compact Exact Language Models Randomised algorithms are not the only compact representation schemes. Church et al. (2007) looked at Golomb Coding and Brants et al. (2007) used tries in a distributed setting. These methods are less succinct than randomised approaches. 3.4 A text stream can be thought of as a unbounded sequence of documents that are time-stamped and we have access to them in strict chronological order. The volume of the stream is so large we can afford only a limited number of passes over the data (typically one). Text streams naturally arise on the Web when millions of new documents are published each day in many languages. For instance, 18 thousand websites continuously publish news stories in 40 languages and there are millions of multilingua"
D09-1079,D07-1021,0,0.127535,"Missing"
D09-1079,N09-1058,0,0.463696,"Missing"
D09-1079,D07-1091,0,0.00563012,"t days. The arcs show that recency has a clear effect: populating LMs using material closer to the test data date produces improved perplexity performance. The LM chronologically closest to a given test set has perplexity closest to the results of the significantly larger baseline LM which uses all the stream. As expected, using all of the data yields the lowest perplexity. We note that this is a robust finding, since we also observe it in other domains. For example, we 5 SMT Experiments 5.1 Experimental Setup We used publicly available resources for all our tests: for decoding we used Moses (Koehn and Hoang, 2007) and our parallel data was taken from the Spanish-English section of Europarl. For test material, we translated 63 documents (800 sentences) from three randomly selected dates spaced throughout the RCV1 year (January 2nd, April 24, and August 19).1 This effectively divided the stream into three epochs between the test dates ( table 2). We held out 300 sentences for minimum error rate training (MERT) (Och, 2003) and optimised the parameters of the feature functions of the decoder for each experimental run. The RCV1 is not a large corpus when compared to the entire web but it is multilingual, ch"
D09-1079,W07-0733,0,0.0145366,"h data to store in its entirety an immediate question we would like to answer is: within our LM, which subset of the target text stream should Adaptive Language Models There is a large literature on adaptive LMs from the speech processing domain (Bellegarda, 2004). The primary difference between the O-RLM and other adaptive LMs is that we add and remove ngrams from the model instead of adapting only the parameters of the current support set. 3.5 Domain adaptation in Machine Translation Within MT there has been a variety of approaches dealing with domain adaption (for example (Wu et al., 2008; Koehn and Schroeder, 2007). Typically LMs are interpolated with one another, yielding good results. These models are usually statically trained, exact and unable to deal with an unbounded stream of monolingual data. Domain adaptation has similarities with streaming, in that our stream may be non-stationary. A crucial difference however is that the stream is of unbounded length, whereas domain adaptation usually assumes some finite and fixed training set. 4 Stream-based translation Streaming algorithms have numerous applications in mainstream computer science (Muthukrishnan, 2003) but to date there has been very little"
D09-1079,P03-1021,0,0.00740919,"lso observe it in other domains. For example, we 5 SMT Experiments 5.1 Experimental Setup We used publicly available resources for all our tests: for decoding we used Moses (Koehn and Hoang, 2007) and our parallel data was taken from the Spanish-English section of Europarl. For test material, we translated 63 documents (800 sentences) from three randomly selected dates spaced throughout the RCV1 year (January 2nd, April 24, and August 19).1 This effectively divided the stream into three epochs between the test dates ( table 2). We held out 300 sentences for minimum error rate training (MERT) (Och, 2003) and optimised the parameters of the feature functions of the decoder for each experimental run. The RCV1 is not a large corpus when compared to the entire web but it is multilingual, chronological, and large enough to enable us to test the effect of recency in a translation setting. 5.2 Adaption We looked at a number of ways of adapting the O-RLM: 1. (Random) Randomly sample the stream and for each new n-gram encountered, insert 1 As RCV1 is not a parallel corpus we translated the reference documents ourselves. This parallel corpus is available from the authors. 760 Order 1 2 3 4 5 Total Full"
D09-1079,rose-etal-2002-reuters,0,0.0621579,". Our experiments show that a possible way to tackle stream-based translation is to always focus the attention of the LM on the most recent part of the stream. This means we remove data from the model that came from the receding parts of the stream and replace it with the present. 200 180 20 25 30 35 40 45 50 weeks Figure 3: Perplexity results using streamed data. Perplexity decreases as we retrain LMs using data chronologically closer to the (two) test dates. we represent in our model? Using perplexity, we investigated this question using a text stream based on Reuter’s RCV1 text collection (Rose et al., 2002). This contains 800k time-stamped newswire stories from a full calender year (8.20.1996 - 8.19.1997). We used the SRILM (Stolcke, 2002) to construct an exact trigram model built using all the RCV1 data with the exception of the final week which we held out as test data. This served as an oracle since we store all of the stream. We then trained multiple exact LMs of much smaller sizes, coined subset LMs, to simulate memory constraints. For a given date in the RCV1 stream, these subset LMs were trained using a fixed window of previously seen documents up to that data. Then we obtained perplexity"
D09-1079,P08-1058,0,0.609558,"stems. They assign probabilities to generated hypotheses in the target language informing lexical selection. The most common form of LMs in SMT systems are smoothed n-gram models which predict a word based on a contextual history of n − 1 words. For some languages (such as English) trillions of words are available for training purposes. This fact, along with the observation that machine translation quality improves as the amount of monolingual training material increases, has lead to the introduction of randomised techniques for representing large LMs in small space (Talbot and Osborne, 2007; Talbot and Brants, 2008). Randomised LMs (RLMs) solve the problem of representing large, static LMs but they are batch oriented and cannot incorporate new data without fully retraining from scratch. This property 2 Online Bloomier Filter LM Our online randomised LM (O-RLM) is based on the dynamic Bloomier filter (Mortensen et al., 2005). It is a variant of the batch-based Bloomier filter LM of Talbot and Brants (2008) which we refer to as the TB-LM henceforth. As with the TB-LM, the O-RLM uses random hash functions to represent n-grams as fingerprints which is the main source of space savings for the model. 2.1 Onlin"
D09-1079,D07-1049,1,0.793922,"chine translation (SMT) systems. They assign probabilities to generated hypotheses in the target language informing lexical selection. The most common form of LMs in SMT systems are smoothed n-gram models which predict a word based on a contextual history of n − 1 words. For some languages (such as English) trillions of words are available for training purposes. This fact, along with the observation that machine translation quality improves as the amount of monolingual training material increases, has lead to the introduction of randomised techniques for representing large LMs in small space (Talbot and Osborne, 2007; Talbot and Brants, 2008). Randomised LMs (RLMs) solve the problem of representing large, static LMs but they are batch oriented and cannot incorporate new data without fully retraining from scratch. This property 2 Online Bloomier Filter LM Our online randomised LM (O-RLM) is based on the dynamic Bloomier filter (Mortensen et al., 2005). It is a variant of the batch-based Bloomier filter LM of Talbot and Brants (2008) which we refer to as the TB-LM henceforth. As with the TB-LM, the O-RLM uses random hash functions to represent n-grams as fingerprints which is the main source of space saving"
D09-1079,C08-1125,0,0.268968,"ves as the amount of monolingual training material increases, has lead to the introduction of randomised techniques for representing large LMs in small space (Talbot and Osborne, 2007; Talbot and Brants, 2008). Randomised LMs (RLMs) solve the problem of representing large, static LMs but they are batch oriented and cannot incorporate new data without fully retraining from scratch. This property 2 Online Bloomier Filter LM Our online randomised LM (O-RLM) is based on the dynamic Bloomier filter (Mortensen et al., 2005). It is a variant of the batch-based Bloomier filter LM of Talbot and Brants (2008) which we refer to as the TB-LM henceforth. As with the TB-LM, the O-RLM uses random hash functions to represent n-grams as fingerprints which is the main source of space savings for the model. 2.1 Online Perfect Hashing The key difference in our model as compared to the TB-LM is we use an online perfect hashing 756 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 756–764, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Figure 1: Inserting an n-gram into the dynamic Bloomier filter. Above: an n-gram is hashed to its target bucket. Below: the n-gram"
D15-1310,N10-1021,1,0.930219,"ce, news and government security. The most accurate approaches to FSD involve a runtime of O(n2 ) and cannot scale to unbounded high volume streams such as Twitter. We present a novel approach to FSD that operates in O(1) per tweet. Our method is able to process the load of the average Twitter Firehose3 stream on a single core of modest hardware while retaining effectiveness on par with one of the most accurate FSD systems. During the TDT program, FSD was applied to news wire documents and solely focused on effectiveness, neglecting efficiency and scalability. The traditional approach to FSD (Petrovic et al., 2010) computes the distance of each incoming document 1 e.g. a natural disaster or a scandal TDT by NIST - 1998-2004. http://www.itl.nist.gov/ iad/mig/tests/tdt/resources.html (Last Update: 2008) 3 5,700 tweets per second https://about.twitter .com/company (last updated: March 31, 2015) 2 Miles Osborne Bloomberg London mosborne29 @bloomberg.net Related Work Researchers have proposed a range of approaches to scale FSD to large data streams. Sankaranarayanan et al. (2009) were one of the first to apply FSD to Twitter. They reduced the volume by classifying documents into news/non-news and only compar"
D15-1310,P05-1077,0,0.111445,"Missing"
D15-1310,P15-1170,1,0.419462,"s and false alarm probabilities, which allows comparing different methods based on a single value metric. Efficiency is measured by the throughput of tweets per second and the memory footprint. To ensure a fair comparison, all reported numbers are averaged over 5 runs on an idle machine using a single core (Intel-Xeon CPU with 2.27GHz). 3.1 Data set We use the data set developed by Petrovic (2013), Petrovic et al. (2013b) as a test set, which consists of 27 topics and 116,000 tweets from the period of April till September 2011. Parameters were tuned using a sample of the data set annotated by Wurzer et al. (2015) as a training set. 3.2 Baselines We compare our system (k-term) against 3 baselines. UMass is a state-of-the-art FSD system, developed by Allan et al. (2000). It is known for its high effectiveness in the TDT2 and TDT3 competitions (Fiscus, 2001) and widely used as a benchmark for FSD systems (Petrovic et al., 2010; Kasiviswanathan et al., 2011; Petrovic 2013;). UMass makes use of an inverted index and k-nearest-neighbour clustering, which optimize the system for speed by ensuring a minimal number of comparisons. To maximise efficiency, we set-up UMass to operate in-memory by turning off its"
E03-1008,H91-1060,0,0.0320313,"Missing"
E03-1008,W99-0613,0,0.0609689,"Missing"
E03-1008,W01-0521,0,0.161063,"Missing"
E03-1008,J93-2004,0,0.0278308,"Missing"
E03-1008,W01-0501,0,0.185139,"Missing"
E03-1008,N01-1023,1,0.616825,"Missing"
E03-1008,P95-1026,0,0.323358,"Missing"
E03-1008,J03-4003,0,\N,Missing
E03-1008,P02-1046,0,\N,Missing
E06-1032,P04-1079,0,0.00865044,"rominent factors contribute to Bleu’s crudeness: • Synonyms and paraphrases are only handled if they are in the set of multiple reference translations. • The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty. was being led to the |calm as he was | would take |carry him |seemed quite | when |taken would receive the same Bleu score as the hypothesis translation from Table 1, even though human judges would assign it a much lower score. This problem is made worse by the fact that Bleu equally weights all items in the reference sentences (Babych and Hartley, 2004). Therefore omitting content-bearing lexical items does • The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall. Each of these failures contributes to an increased amount of inappropriately indistinguishable translations in the analysis presented above. Given that Bleu can theoretically assign equal scoring to translations of obvious different quality, it is logical that a higher Bleu score may not 252 Fluency How do you judge the fluency of this translation? 5 = Flawless English 4 = Good English 3 = Non-native English 2 ="
E06-1032,W05-0909,0,0.108748,"to inappropriate phrase movement by matching part-of-speech tag sequences against reference translations in addition to Bleu’s n-gram matches. Babych and Hartley (2004) extend Bleu by adding frequency weighting to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation. Conclusions In this paper we have shown theoretical a"
E06-1032,P05-1048,0,0.0875749,"Missing"
E06-1032,2003.mtsummit-papers.6,0,0.0277887,"Missing"
E06-1032,2003.mtsummit-papers.9,0,0.335611,"ric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003). Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations. Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation. All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality, which has been shown to hold in many cases (Doddington, 2002; Coughlin, 2003). However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee genuine translation improvements. If Bleu’s correlation with human judgments has been overestimated, then the field needs to ask itself whether it should continue to be driven by Bleu to the extent that it currently is. In this paper we give a number of counterexamples for Bleu’s correlation with human judgments. We show that under some circumstances an improvement in Bleu is not sufficient to reflect a genuine improvement in translation quality, and in other circumstances that it"
E06-1032,koen-2004-pharaoh,0,0.026394,"and seeing whether Bleu cor0.4 0.42 0.44 0.46 0.48 0.5 0.52 Bleu Score Figure 3: Bleu scores plotted against human judgments of fluency, with R2 = 0.002 when the outlier entry is included rectly ranked the systems. We used Systran for the rule-based system, and used the French-English portion of the Europarl corpus (Koehn, 2005) to train the SMT systems and to evaluate all three systems. We built the first phrase-based SMT system with the complete set of Europarl data (1415 million words per language), and optimized its feature functions using minimum error rate training in the standard way (Koehn, 2004). We evaluated it and the Systran system with Bleu using a set of 2,000 held out sentence pairs, using the same normalization and tokenization schemes on both systems’ output. We then built a number of SMT systems with various portions of the training 1 corpus, and selected one that was trained with 64 of the data, which had a Bleu score that was close to, but still higher than that for the rule-based system. We then performed a manual evaluation where we had three judges assign fluency and adequacy ratings for the English translations of 300 French sentences for each of the three systems. The"
E06-1032,2005.mtsummit-papers.11,1,0.0613044,"tput which has differing characteristics, and might end up in different regions of the human scores / Bleu score graph. We investigated this by performing a manual evaluation comparing the output of two statistical machine translation systems with a rule-based machine translation, and seeing whether Bleu cor0.4 0.42 0.44 0.46 0.48 0.5 0.52 Bleu Score Figure 3: Bleu scores plotted against human judgments of fluency, with R2 = 0.002 when the outlier entry is included rectly ranked the systems. We used Systran for the rule-based system, and used the French-English portion of the Europarl corpus (Koehn, 2005) to train the SMT systems and to evaluate all three systems. We built the first phrase-based SMT system with the complete set of Europarl data (1415 million words per language), and optimized its feature functions using minimum error rate training in the standard way (Koehn, 2004). We evaluated it and the Systran system with Bleu using a set of 2,000 held out sentence pairs, using the same normalization and tokenization schemes on both systems’ output. We then built a number of SMT systems with various portions of the training 1 corpus, and selected one that was trained with 64 of the data, wh"
E06-1032,N03-1020,0,0.24596,"to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation. Conclusions In this paper we have shown theoretical and practical evidence that Bleu may not correlate with human judgment to the degree that it is currently believed to do. We have shown that Bleu’s rather coarse model of allowable variation in translation"
E06-1032,N03-2021,0,0.459458,"s to an arithmetic average, and calculating the brevity penalty in a slightly different manner. Hovy and Ravichandra (2003) suggested increasing Bleu’s sensitivity to inappropriate phrase movement by matching part-of-speech tag sequences against reference translations in addition to Bleu’s n-gram matches. Babych and Hartley (2004) extend Bleu by adding frequency weighting to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization."
E06-1032,P02-1038,0,0.108484,"n that it was not fully automatic machine translation; instead the entry was aided by monolingual English speakers selecting among alternative automatic translations of phrases in the Arabic source sentences and post-editing the result (Callison-Burch, 2005). The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training (Och, 2003) to optimize the weights of their log linear models’ feature functions (Och and Ney, 2002). This opens the possibility that in order for Bleu to be valid only sufficiently similar systems should be compared with one another. For instance, when measuring correlation using Pearson’s we get a very low correlation of R2 = 0.14 when the outlier in Figure 2 is included, but a strong R2 = 0.87 when it is excluded. Similarly Figure 3 goes from R2 = 0.002 to a much stronger R2 = 0.742. Systems which explore different areas of translation space may produce output which has differing characteristics, and might end up in different regions of the human scores / Bleu score graph. We investigated"
E06-1032,P03-1021,0,0.33672,"s of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. 1 Introduction Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as Bleu (Papineni et al., 2002). The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003). Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations. Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation. All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality, which has been shown to hold in many cases (Doddington, 2002; Coughlin, 2003). However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee gen"
E06-1032,P02-1040,0,0.129827,"We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu’s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. 1 Introduction Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as Bleu (Papineni et al., 2002). The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003). Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations. Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation. All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation q"
E06-1032,P04-1078,0,0.0207213,"F as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation. Conclusions In this paper we have shown theoretical and practical evidence that Bleu may not correlate with human judgment to the degree that it is currently believed to do. We have shown that Bleu’s rather coarse model of allowable variation in translation can mean that an improved Bleu scor"
E06-1032,N04-1021,0,\N,Missing
I05-1078,W03-1018,0,0.204464,"er we take an alternative, Bayesian approach motivated more directly by our conﬁdence in the reliability of a feature’s empirical expected count. In equations (2), (3) and (4) the level of regularisation applied to a feature takes the form of a discount to the expected count of the feature on the training 900 A. Smith and M. Osborne data. It is natural, therefore, that the size of this discount, controlled through a hyperparameter, is related to our conﬁdence in the reliability of the empirical expected count. We formulate a measure of this conﬁdence. We follow the approach of Kazama & Tsujii [4], extending it to CRFs. [fk ], of a feature fk is given by: The empirical expected count, Ep(o,s) ˜      p˜(o, s) fk (st−1 , st , o, t) = p˜(o) p˜(s|o) fk (st−1 , st , o, t) o,s t o =  o t s p˜(o)  p˜(st−1 = s , st = s |o)fk (s , s , o, t) t,s ,s Now, our CRF features have the following form:  1 if st−1 = s1 , st = s2 and hk (o, t) = 1 fk (st−1 , st , o, t) = 0 otherwise where s1 and s2 are the labels associated with feature fk and hk (o, t) is a binaryvalued predicate deﬁned on observation sequence o at position t. With this feature deﬁnition, and contracting notation for the"
I05-1078,W02-2018,0,0.0846776,"nditional loglikelihood of a set of labelled training sequences. The log-likelihood is given by: T +1      p˜(o) p˜(s|o) λ · f (s, o, t) − p˜(o) log Z(o; λ) LL(λ) = o s t=1 o where p˜(s|O) and p˜(o) are empirical distributions deﬁned by the training set. At the maximum likelihood solution the model satisﬁes a set of feature constraints, whereby the expected count of each feature under the model is equal to its empirical count on the training data: [fk ] − Ep(s|o) [fk ] = 0, ∀k Ep(o,s) ˜ In general this cannot be solved for the λk in closed form so numerical routines must be used. Malouf [6] and Sha & Pereira [9] show that gradient-based algorithms, particularly limited memory variable metric (LMVM), require much less time to reach convergence, for some NLP tasks, than the iterative scaling methods previously used for log-linear optimisation problems. In all our experiments we use the LMVM method to train the CRFs. For CRFs with general graphical structure, calculation of Ep(s|o) [fk ] is intractable, but for the linear chain case Laﬀerty et al. [5] describe an eﬃcient 898 A. Smith and M. Osborne dynamic programming procedure for inference, similar in nature to the forwardbackwar"
I05-1078,N04-1042,0,0.73807,"ith the use of a prior, such as constant hyperparameter values across features. In addition, we contrast the eﬀectiveness of priors with an alternative, parameter-free approach. Speciﬁcally, we employ logarithmic opinion pools (LOPs). Our results show that a LOP of CRFs can outperform a standard unregularised CRF and attain a performance level close to that of a regularised CRF, without the need for intensive hyperparameter search. 1 Introduction Recent work on Conditional Random Fields (CRFs) has demonstrated the need for regularisation when applying these models to real-world NLP data sets ([8], [9]). Standard approaches to regularising CRFs, and log-linear models in general, has focused on the use of a Gaussian prior. Typically, for simplicity, this prior is assumed to have zero mean and constant variance across model parameters. To date, there has been little work exploring other possibilities. One exception is Peng & McCallum [8]. They investigated feature-dependent variance for a Gaussian prior, and explored diﬀerent families of feature sets. They also compared diﬀerent priors for CRFs on an information extraction task. In the ﬁrst part of this paper, we compare priors for CRFs"
I05-1078,N03-1028,0,0.504948,"he use of a prior, such as constant hyperparameter values across features. In addition, we contrast the eﬀectiveness of priors with an alternative, parameter-free approach. Speciﬁcally, we employ logarithmic opinion pools (LOPs). Our results show that a LOP of CRFs can outperform a standard unregularised CRF and attain a performance level close to that of a regularised CRF, without the need for intensive hyperparameter search. 1 Introduction Recent work on Conditional Random Fields (CRFs) has demonstrated the need for regularisation when applying these models to real-world NLP data sets ([8], [9]). Standard approaches to regularising CRFs, and log-linear models in general, has focused on the use of a Gaussian prior. Typically, for simplicity, this prior is assumed to have zero mean and constant variance across model parameters. To date, there has been little work exploring other possibilities. One exception is Peng & McCallum [8]. They investigated feature-dependent variance for a Gaussian prior, and explored diﬀerent families of feature sets. They also compared diﬀerent priors for CRFs on an information extraction task. In the ﬁrst part of this paper, we compare priors for CRFs on st"
I05-1078,P05-1003,1,0.864139,"Missing"
I05-1078,W00-0726,0,0.125007,"Missing"
I05-1078,W03-0419,0,0.0994181,"Missing"
N03-1031,A00-2018,0,0.0290607,"s are iteratively re-trained on each other’s output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human"
N03-1031,P96-1042,0,0.012772,"Missing"
N03-1031,N01-1023,1,0.558581,"e. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a subset of their output, chosen by some selection mechanism, is used in order to minimize errors. The choice of selection method significantly affects the quality of the resulting parsers. We investigate a novel approach of selecting training examples for co-training parsers by incorporating the idea of maximizing training uti"
N03-1031,E03-1008,1,0.765705,"Missing"
N03-1031,W00-1306,1,0.314843,"ility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples inc"
N03-1031,P02-1016,0,0.00757357,"rion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a sub"
N03-1031,J93-2004,0,0.0255646,"h, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the"
N03-1031,P00-1016,0,0.0133509,"Missing"
N03-1031,W01-0501,0,0.0173865,"have different goals, their selection methods focus on different criteria: co-training typically favors selecting accurately labeled examples, while sample selection typically favors selecting examples with high training utility, which often are not sentences that the parsers already label accurately. In this work, we investigate selection methods for co-training that explore the trade-off between maximizing training utility and minimizing errors. Empirical studies were conducted to compare selection methods under both co-training and a semi-supervised framework called corrected co-training (Pierce and Cardie, 2001), in which the selected examples are manually checked and corrected before being added to the 1 In the context of training parsers, a labeled example is a sentence with its parse tree. Throughout this paper, we use the term “label” and “parse” interchangeably. training data. For co-training, we show that the benefit of selecting examples with high training utility can offset the additional errors they contain. For corrected co-training, we show that selecting examples with high training utility reduces the number of sentences the human annotator has to check. For both frameworks, we show that"
N03-1031,J03-4003,0,\N,Missing
N03-1031,P02-1046,0,\N,Missing
N04-1012,W03-0403,1,\N,Missing
N04-1012,C02-2025,0,\N,Missing
N04-1012,W00-1306,0,\N,Missing
N04-1012,P02-1016,0,\N,Missing
N04-1012,P99-1069,0,\N,Missing
N04-1012,W02-2018,0,\N,Missing
N04-1012,P93-1024,0,\N,Missing
N04-1012,P01-1019,0,\N,Missing
N06-1003,H05-1085,0,0.0417204,"Missing"
N06-1003,E03-1076,1,0.247316,"more 23 Related Work Previous research on trying to overcome data sparsity issues in statistical machine translation has largely focused on introducing morphological analysis as a way of reducing the number of types observed in a training text. For example, Nissen and Ney (2004) apply morphological analyzers to English and German and are able to reduce the amount of training data needed to reach a certain level of translation quality. Goldwater and McClosky (2005) find that stemming Czech and using lemmas improves the word-to-word correspondences when training Czech-English alignment models. Koehn and Knight (2003) show how monolingual texts and parallel corpora can be used to figure out appropriate places to split German compounds. Still other approaches focus on ways of acquiring data. Resnik and Smith (2003) develop a method for gathering parallel corpora from the web. Oard et al. (2003) describe various methods employed for quickly gathering resources to create a machine translation system for a language with no initial resources. 7 Discussion cal machine translation to larger corpora and longer phrases. In Proceedings of ACL. In this paper we have shown that significant gains in coverage and transl"
N06-1003,N03-1017,1,0.047697,"Missing"
N06-1003,koen-2004-pharaoh,0,0.0150597,"ility, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phrase penalty, and a distortion cost. To set the weights, λm , we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al., 2002) as the objective function. The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training on each of the three training corpora. We used the Pharaoh beamsearch decoder (Koehn, 2004) to produce the translations after all of the model parameters had been set. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. This is the default behavior for many systems, as noted in Section 2.1. 20 4.2 Translation with paraphrases We extracted all source language (Spanish and French) phrases up to length 10 from the test and development sets which did not have translations in phrase tables that were generated for the three training corpora. For each of these phrases we generated a list of para"
N06-1003,2005.mtsummit-papers.11,1,0.103114,"definition of the paraphrase probability to include multiple corpora, as follows: P p(e2 |e1 ) ≈ c∈C f in c p(f |e1 )p(e2 |f ) P |C| (3) where c is a parallel corpus from a set of parallel corpora C. Thus multiple corpora may be used by summing over all paraphrase probabilities calculated from a single corpus (as in Equation 1) and normalized by the number of parallel corpora. 4 Experimental Design We examined the application of paraphrases to deal with unknown phrases when translating from Spanish and French into English. We used the publicly available Europarl multilingual parallel corpus (Koehn, 2005) to create six training corpora for the two language pairs, and used the standard Europarl development and test sets. 4.1 Baseline For a baseline system we produced a phrase-based statistical machine translation system based on the log-linear formulation described in (Och and Ney, 2002) ˆ = arg max p(e|f ) e e = arg max e M X λm hm (e, f ) (4) (5) m=1 The baseline model had a total of eight feature functions, hm (e, f ): a language model probability, a phrase translation probability, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probab"
N06-1003,W02-1018,0,0.0323214,"Missing"
N06-1003,J04-2003,0,0.064585,"Missing"
N06-1003,P02-1038,0,0.339431,"Missing"
N06-1003,J04-4002,0,0.120477,"Missing"
N06-1003,P03-1021,0,0.0206105,"For a baseline system we produced a phrase-based statistical machine translation system based on the log-linear formulation described in (Och and Ney, 2002) ˆ = arg max p(e|f ) e e = arg max e M X λm hm (e, f ) (4) (5) m=1 The baseline model had a total of eight feature functions, hm (e, f ): a language model probability, a phrase translation probability, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phrase penalty, and a distortion cost. To set the weights, λm , we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al., 2002) as the objective function. The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training on each of the three training corpora. We used the Pharaoh beamsearch decoder (Koehn, 2004) to produce the translations after all of the model parameters had been set. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. This is the default be"
N06-1003,P05-1074,1,0.629984,"slate it instead of encargarnos, and similarly for utilizado instead of usado. 3 Acquiring Paraphrases Paraphrases are alternative ways of expressing the same information within one language. The automatic generation of paraphrases has been the focus of a significant amount of research lately. Many methods for extracting paraphrases (Barzilay and McKeown, 2001; Pang et al., 2003) make use of monolingual parallel corpora, such as multiple translations of classic French novels into English, or the multiple reference translations used by many automatic evaluation metrics for machine translation. Bannard and Callison-Burch (2005) use bilingual parallel corpora to generate paraphrases. Paraphrases are identified by pivoting through phrases in another language. The foreign language translations of an English phrase are identified, all occurrences of those foreign phrases are found, and all English phrases that they translate back to are treated as potential paraphrases of the original English phrase. Figure 2 illustrates how a German phrase can be used as a point of identification for English paraphrases in this way. The method defined in Bannard and CallisonBurch (2005) has several features that make it an ideal candid"
N06-1003,P01-1008,0,0.667567,"that we employ for dealing with unknown source language words is to substitute paraphrases of those words, and then translate the paraphrases. Table 1 gives examples of paraphrases and their translations. If we had learned a translation of garantizar we could translate it instead of encargarnos, and similarly for utilizado instead of usado. 3 Acquiring Paraphrases Paraphrases are alternative ways of expressing the same information within one language. The automatic generation of paraphrases has been the focus of a significant amount of research lately. Many methods for extracting paraphrases (Barzilay and McKeown, 2001; Pang et al., 2003) make use of monolingual parallel corpora, such as multiple translations of classic French novels into English, or the multiple reference translations used by many automatic evaluation metrics for machine translation. Bannard and Callison-Burch (2005) use bilingual parallel corpora to generate paraphrases. Paraphrases are identified by pivoting through phrases in another language. The foreign language translations of an English phrase are identified, all occurrences of those foreign phrases are found, and all English phrases that they translate back to are treated as potent"
N06-1003,J93-2003,0,0.0092731,"nt approaches. 1 • Define a method for incorporating paraphrases of unseen source phrases into the statistical machine translation process. • Show that by translating paraphrases we achieve a marked improvement in coverage and translation quality, especially in the case of unknown words which to date have been left untranslated. • Argue that while we observe an improvement in Bleu score, this metric is particularly poorly suited to measuring the sort of improvements that we achieve. Introduction As with many other statistical natural language processing tasks, statistical machine translation (Brown et al., 1993) produces high quality results when ample training data is available. This is problematic for so called “low density” language pairs which do not have very large parallel corpora. For example, when words occur infrequently in a parallel corpus parameter estimates for word-level alignments can be inaccurate, which can in turn lead to inaccurate phrase translations. Limited amounts of training data can further lead to a problem of low coverage in that many phrases encountered at run-time are not ob• Present an alternative methodology for targeted manual evaluation that may be useful in other res"
N06-1003,P02-1040,0,0.110282,"ed statistical machine translation system based on the log-linear formulation described in (Och and Ney, 2002) ˆ = arg max p(e|f ) e e = arg max e M X λm hm (e, f ) (4) (5) m=1 The baseline model had a total of eight feature functions, hm (e, f ): a language model probability, a phrase translation probability, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phrase penalty, and a distortion cost. To set the weights, λm , we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al., 2002) as the objective function. The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training on each of the three training corpora. We used the Pharaoh beamsearch decoder (Koehn, 2004) to produce the translations after all of the model parameters had been set. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. This is the default behavior for many systems, as noted in Section 2.1. 20 4.2 T"
N06-1003,J03-3002,0,0.0379518,"number of types observed in a training text. For example, Nissen and Ney (2004) apply morphological analyzers to English and German and are able to reduce the amount of training data needed to reach a certain level of translation quality. Goldwater and McClosky (2005) find that stemming Czech and using lemmas improves the word-to-word correspondences when training Czech-English alignment models. Koehn and Knight (2003) show how monolingual texts and parallel corpora can be used to figure out appropriate places to split German compounds. Still other approaches focus on ways of acquiring data. Resnik and Smith (2003) develop a method for gathering parallel corpora from the web. Oard et al. (2003) describe various methods employed for quickly gathering resources to create a machine translation system for a language with no initial resources. 7 Discussion cal machine translation to larger corpora and longer phrases. In Proceedings of ACL. In this paper we have shown that significant gains in coverage and translation quality can be had by integrating paraphrases into statistical machine translation. In effect, paraphrases introduce some amount of generalization into statistical machine translation. Whereas b"
N06-1003,E06-1032,1,\N,Missing
N06-1003,N03-2026,1,\N,Missing
N06-1003,N03-1024,0,\N,Missing
N06-1003,P05-1032,1,\N,Missing
N10-1021,D09-1079,1,0.132959,"oise. A major benefit of doing new event detection from tweets is the added social component – we can understand the impact an event had and how people reacted to it. The speed and volume at which data is coming from Twitter warrants the use of streaming algorithms to make first story detection feasible. In the streaming model of computation (Muthukrishnan, 2005), items (tweets in our case) arrive continuously in a chronological order, and we have to process each new one in bounded space and time. Recent examples of problems set in the streaming model include stream-based machine translation (Levenberg and Osborne, 2009), approximating kernel matrices of data streams (Shi et al., 2009), and topic modelling on streaming document collections (Yao et al., 2009). The traditional approach to FSD, where each new story is compared to all, or a constantly growing subset, of previously seen stories, does not scale to the Twitter streaming setting. We present a FSD system that works in the streaming model and takes constant time to process each new document, while also using constant space. Constant processing time is achieved by employing locality sensitive hashing (LSH) (Indyk and Motwani, 1998), a randomized techniq"
N10-1021,P05-1077,0,0.164711,"rest one returned. Because we are dealing with textual documents, a particularly interesting measure of distance is the cosine between two documents. Allan et al. (2000) report that this distance outperforms the KL divergence, weighted sum, and language models as distance functions on the first story detection task. This is why in our work we use the hashing scheme proposed by Charikar (2002) in which the probability of two points colliding is proportional to the cosine of the angle between them. This scheme was used, e.g., for creating similarity lists of nouns collected from a web corpus in Ravichandran et al. (2005). It works by intersecting the space with random hyperplanes, and the buckets are defined by the subspaces formed this way. More precisely, the probability of two points x and y colliding under such a hashing scheme is θ(x, y) Pcoll = 1 − , (1) π where θ(x, y) is the angle between x and y. By using more than one hyperplane, we can decrease the probability of collision with a non-similar point. The number of hyperplanes k can be considered as a number of bits per key in this hashing scheme. In particular, if x · ui < 0, i ∈ [1 . . . k] for document x and hyperplane vector ui , we set the i-th b"
N10-1062,W09-0432,0,0.0129327,"are against the war and the occupation of Iraq by the United States and the United Kingdom, and we demand the immediate withdrawal of the occupying forces from this country . Figure 4: Example sentences and improvements to their translation fluency by the adaptation of the TM with recent sentences. In both examples we get longer matching phrases in the online translation compared to the static one. 5 Related Work 5.3 Streaming Language Models 5.1 Translation Model Domain Adaptation Our work is related to domain adaptation for translation models. See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). Most techniques center around using mixtures of translation models. Once trained, these models generally never change. They therefore fall under the batch training regime. The focus of this work instead is on incremental retraining and also on supporting bounded memory consumption. Our experiments examine updating model parameters in a single domain over different periods in time. Naturally, we could also use domain adaptation techniques to further improve how we incorporate new samples. 5.2 Online EM for SMT For stepwise online EM for SMT models, the only prior work we are aware of is Liang"
N10-1062,D07-1090,0,0.0372992,"Missing"
N10-1062,J93-2003,0,0.0175172,"g an HMM-based alignment model where the probability of alignment aj is dependent only on the previous alignment at aj−1 (Vogel et al., 1996). We can write Pr(f , a |e) = |f | X Y p(aj |aj−1 , |e|) · p(fj |eaj ) a′ ∈a j=1 where we assume a first-order dependence on previously aligned positions. To find the most likely parameter weights for the translation and alignment probabilities for the HMM-based alignments, we employ the EM algorithm via dynamic programming. Since HMMs have multiple local minima, we seed the HMM-based model probabilities with a better than random guess using IBM Model 1 (Brown et al., 1993) as is standard. IBM Model 1 is of the same form as the HMM-based model except it uses a uniform distribution instead of a first-order dependency. Although a series of more complex models are defined, IBM Models 2 to Model 6 (Brown et al., 1993; Och and Ney, 2003), researchers typically find that extracting phrase pairs or translation grammar rules using Model 1 and the HMM-based alignments results in equivalently high translation quality. Nevertheless, there is nothing in our approach which limits us to using just Model 1 and the HMM model. A high-level overview of the standard, batch EM algo"
N10-1062,P05-1032,1,0.260706,"990). Treating the entire corpus as a single string, a suffix array holds in lexicographical order (only) the starting index of each suffix of the string. After construction, since the corpus is now ordered, we can query the suffix array quickly using binary search to efficiently find all occurrences of a particular token or sequence of tokens. Then we can easily compute, on-the-fly, the statistics required such as translation probabilities for a given source phrase. Suffix arrays can also be compressed, which make them highly attractive structures for representing massive translation models (Callison-Burch et al., 2005; Lopez, 2008). We need to delete items if we wish to maintain Static Test Points 2.5 input stream model coverage Delta in BLEU scores 2 Unbounded Test Points input stream model coverage epoch 1 epoch 2 1.5 1 0.5 Bounded Test Points 0 input stream 5 model coverage epoch 2 15 20 25 30 35 epochs sliding windows Figure 1: Streaming coverage conditions. In traditional batch based modeling the coverage of a trained model never changes. Unbounded coverage operates without any memory constraints so the model is able to continually add data from the input stream. Bounded coverage uses just a fixed win"
N10-1062,J07-2003,0,0.0130749,"as source and English again as target. For testing we held out a total of 22k sentences from 10 evenly spaced intervals in the input stream which divided the input stream into 10 epochs. Stream statistics for three example epochs are shown in Table 1. We held out 4.5k sentence pairs as development data to optimize the feature function weights using minimum error rate training (Och, 2003) and these weights were used by all models. We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. Note that nothing hinges on whether we used a syntax or a phrase-based system. We used a 5-gram, Kneser-Ney smoothed language model (LM) trained on the initial segment of 398 Ep 00 03 06 10 From–To 04/1996–12/2000 02/2002–09/2002 10/2003–03/2004 03/2006–09/2006 Sent Pairs 600k 70k 60k 73k Source/Target 15.0M/16.0M 1.9M/2.0M 1.6M/1.7M 1.9M/2.0M Table 1: Date ranges, total sentence pairs, and source and target word counts encountered in the input stream for example epochs. Epoch 00 is baseline data that is also used as a seed corpus for the online models. the target side parallel d"
N10-1062,W07-0733,0,0.0127905,"n is clear and well known: we are against the war and the occupation of Iraq by the United States and the United Kingdom, and we demand the immediate withdrawal of the occupying forces from this country . Figure 4: Example sentences and improvements to their translation fluency by the adaptation of the TM with recent sentences. In both examples we get longer matching phrases in the online translation compared to the static one. 5 Related Work 5.3 Streaming Language Models 5.1 Translation Model Domain Adaptation Our work is related to domain adaptation for translation models. See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). Most techniques center around using mixtures of translation models. Once trained, these models generally never change. They therefore fall under the batch training regime. The focus of this work instead is on incremental retraining and also on supporting bounded memory consumption. Our experiments examine updating model parameters in a single domain over different periods in time. Naturally, we could also use domain adaptation techniques to further improve how we incorporate new samples. 5.2 Online EM for SMT For stepwise online EM for SMT models, the only pri"
N10-1062,D09-1079,1,0.816519,"eved an absolute improvement of +1.24 BLEU over the static baseline for the final test point. We get another absolute gain of +1.08 BLEU by allowing the LM coverage to adapt as well. Using an online, adaptive model gives a total gain of +2.32 BLEU over a static baseline that does not adapt. 4.6 Increasing LM Coverage A natural and interesting extension to the experiments above is to use the target side of the incoming stream to extend the LM coverage alongside the TM. 400 6 Although we batch retrain the LMs we could use an online LM that incorporates new vocabulary from the input stream as in Levenberg and Osborne (2009). Source: Die Kommission ist bereit, an der Schaffung eines solchen Rechtsrahmens unter Zugrundelegung von vier wesentlichen Prinzipien mitzuwirken. Reference: The commission is willing to cooperate in the creation of such a legal framework on the basis of four essential principles. Static: The commission is prepared, in the creation of a legal framework, taking account of four fundamental principles them. Online: The commission is prepared to participate in the creation of such a legal framework, based on four fundamental principles. Source: Unser Standpunkt ist klar und allseits bekannt: Wir"
N10-1062,W09-0424,1,0.738055,"ve were not limited to French-English, this time our parallel input stream was generated from the GermanEnglish language pair of Europarl with German as source and English again as target. For testing we held out a total of 22k sentences from 10 evenly spaced intervals in the input stream which divided the input stream into 10 epochs. Stream statistics for three example epochs are shown in Table 1. We held out 4.5k sentence pairs as development data to optimize the feature function weights using minimum error rate training (Och, 2003) and these weights were used by all models. We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. Note that nothing hinges on whether we used a syntax or a phrase-based system. We used a 5-gram, Kneser-Ney smoothed language model (LM) trained on the initial segment of 398 Ep 00 03 06 10 From–To 04/1996–12/2000 02/2002–09/2002 10/2003–03/2004 03/2006–09/2006 Sent Pairs 600k 70k 60k 73k Source/Target 15.0M/16.0M 1.9M/2.0M 1.6M/1.7M 1.9M/2.0M Table 1: Date ranges, total sentence pairs, and source and target word counts encountered in"
N10-1062,N09-1069,0,0.380716,"count collection begins anew using the new distribution θˆt+1 . When we move to processing an incoming data stream, however, the batch EM algorithm’s requirement that all data be available for each iteration becomes impractical since we do not have access to all n examples at once. Instead we receive examples from the input stream incrementally. For this reason online EM algorithms have been developed to update the probability model θˆ incrementally without needing to store and iterate through all the unlabeled training data repeatedly. Various online EM algorithms have been investigated (see Liang and Klein (2009) for an overview) but our focus is on the stepwise online EM (sOEM) algorithm (Cappe and Moulines, 2009). Instead of iterating over the full set of training examples, sOEM stochastically approximates the batch E-step and incorporates the information from the newly available streaming observations in steps. Each step is called a mini-batch and is comprised of one or more new examples encountered in the stream. Unlike in batch EM, in sOEM the expected counts are retained between EM iterations and not cleared. 4 As the M-step can be computed in closed form we desig¯ nate it in this work as θ(S)."
N10-1062,C08-1064,0,0.0559607,"pus as a single string, a suffix array holds in lexicographical order (only) the starting index of each suffix of the string. After construction, since the corpus is now ordered, we can query the suffix array quickly using binary search to efficiently find all occurrences of a particular token or sequence of tokens. Then we can easily compute, on-the-fly, the statistics required such as translation probabilities for a given source phrase. Suffix arrays can also be compressed, which make them highly attractive structures for representing massive translation models (Callison-Burch et al., 2005; Lopez, 2008). We need to delete items if we wish to maintain Static Test Points 2.5 input stream model coverage Delta in BLEU scores 2 Unbounded Test Points input stream model coverage epoch 1 epoch 2 1.5 1 0.5 Bounded Test Points 0 input stream 5 model coverage epoch 2 15 20 25 30 35 epochs sliding windows Figure 1: Streaming coverage conditions. In traditional batch based modeling the coverage of a trained model never changes. Unbounded coverage operates without any memory constraints so the model is able to continually add data from the input stream. Bounded coverage uses just a fixed window. constant"
N10-1062,J03-1002,0,0.0144271,"previously aligned positions. To find the most likely parameter weights for the translation and alignment probabilities for the HMM-based alignments, we employ the EM algorithm via dynamic programming. Since HMMs have multiple local minima, we seed the HMM-based model probabilities with a better than random guess using IBM Model 1 (Brown et al., 1993) as is standard. IBM Model 1 is of the same form as the HMM-based model except it uses a uniform distribution instead of a first-order dependency. Although a series of more complex models are defined, IBM Models 2 to Model 6 (Brown et al., 1993; Och and Ney, 2003), researchers typically find that extracting phrase pairs or translation grammar rules using Model 1 and the HMM-based alignments results in equivalently high translation quality. Nevertheless, there is nothing in our approach which limits us to using just Model 1 and the HMM model. A high-level overview of the standard, batch EM algorithm applied to HMM-based word alignment model is shown in Algorithm 1. 2.3 Stepwise EM for Word Alignments Application of sOEM to HMM and Model 1 based word aligning is straightforward. The process of collecting the counts over the expected conditional probabili"
N10-1062,P03-1021,0,0.0309733,"fixed sized sample of it). To ensure the recency results reported above were not limited to French-English, this time our parallel input stream was generated from the GermanEnglish language pair of Europarl with German as source and English again as target. For testing we held out a total of 22k sentences from 10 evenly spaced intervals in the input stream which divided the input stream into 10 epochs. Stream statistics for three example epochs are shown in Table 1. We held out 4.5k sentence pairs as development data to optimize the feature function weights using minimum error rate training (Och, 2003) and these weights were used by all models. We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. Note that nothing hinges on whether we used a syntax or a phrase-based system. We used a 5-gram, Kneser-Ney smoothed language model (LM) trained on the initial segment of 398 Ep 00 03 06 10 From–To 04/1996–12/2000 02/2002–09/2002 10/2003–03/2004 03/2006–09/2006 Sent Pairs 600k 70k 60k 73k Source/Target 15.0M/16.0M 1.9M/2.0M 1.6M/1.7M 1.9M/2.0M Table 1: Date ran"
N10-1062,2001.mtsummit-papers.68,0,0.0122748,"ted all 36 test points again using a new grammar for each document extracted from only the sentences contained in the epoch that was before it. To explicitly test the effect of recency 5 Available at http://www.statmt.org/europarl on the TM all other factors of the SMT pipeline remained constant including the language model and the feature weights. Hence, the only change from the static baseline to the epochs performance was the TM data which was based on recency. Note that at this stage we did not use any incremental retraining. Results are shown in Figure 2 as the differences in BLEU score (Papineni et al., 2001) between the baseline TM versus the translation models trained on material chronologically closer to the given test point. The consistently positive deltas in BLEU scores between the model that is never retrained and the models that are retrained show that we achieve a higher translation performance when using more upto-date TMs that incorporate recent sentence pairs. As the chronological distance between the initial, static model and the retrained models increases, we see ever-increasing differences in translation performance. This underlines the need to retrain translation models with timely"
N10-1062,C96-2141,0,0.151243,"slation to estimate word alignment probabilities between parallel sentences. From these alignments, bilingual rules or phrase pairs can be extracted. Given a set of parallel sentence examples, {F, E}, with F the set of source sentences and E the corresponding target sentences, we want to find the latent alignments a for a sentence pair (f , e) ∈ {F, E} that defines the most probable correspondence between words fj and ei such that aj = i. We can induce these alignments using an HMM-based alignment model where the probability of alignment aj is dependent only on the previous alignment at aj−1 (Vogel et al., 1996). We can write Pr(f , a |e) = |f | X Y p(aj |aj−1 , |e|) · p(fj |eaj ) a′ ∈a j=1 where we assume a first-order dependence on previously aligned positions. To find the most likely parameter weights for the translation and alignment probabilities for the HMM-based alignments, we employ the EM algorithm via dynamic programming. Since HMMs have multiple local minima, we seed the HMM-based model probabilities with a better than random guess using IBM Model 1 (Brown et al., 1993) as is standard. IBM Model 1 is of the same form as the HMM-based model except it uses a uniform distribution instead of a"
N10-1062,P02-1040,0,\N,Missing
N10-1062,2005.eamt-1.39,0,\N,Missing
N12-1034,N06-1003,1,0.676553,"s considered the most difficult one (Allan et al., 2000a). A good FSD system would be very useful for business or intelligence analysts where The problem of lexical variation plagues many IR and NLP tasks, and one way it has been addressed in the past is through the use of paraphrases. Paraphrases are alternative ways of expressing the same meaning in the same language. For example, the phrase he got married can be paraphrased as he tied the knot. Paraphrases were already shown to help in a number of tasks: for machine translation to translate unknown phrases by translating their paraphrases (Callison-Burch et al., 2006), for query expansion in information retrieval (Sp¨arck Jones and Tait, 1984; Jones et al., 2006), or for improving question answering (Riezler et al., 2007). A much more detailed discussion on the use of paraphrases and ways to extract them is given in (Madnani and Dorr, 2010). Here, we present the first work to use paraphrases for improving first story detection. Using paraphrases, we are able to detect that some documents previously thought to be about new events are actually paraphrases of the documents already 338 2012 Conference of the North American Chapter of the Association for Comput"
N12-1034,D08-1021,0,0.0504834,"d sentential paraphrases, where entire sentences are in a paraphrastic relationship. Here we use the simplest form, lexical paraphrases, but our approach, described in section 2.3, is general and it would be trivial to use phrasal paraphrases in the same way – we leave this for future work. We use three sources of paraphrases: Wordnet (Fellbaum, 1998), a carefully curated lexical database of English containing synonym sets, Microsoft Research paraphrase tables (Quirk et al., 2004), a set of paraphrase pairs automatically extracted from news texts, and syntacticallyconstrained paraphrases from Callison-Burch (2008) which are extracted from parallel text. We also considered using paraphrases from Cohn et al. (2008), but using them provided only minor improvement over the baseline model. This is likely due to the small size of that corpus (a total of 7 thousand pairs). We do not show results for this paraphrase corpus in our results section. Wordnet paraphrases contained 150 thousand word pairs extracted from Wordnet’s synsets, where all the pairs of words within one synset were considered to be paraphrases. MSR paraphrases were extracted from the phrase tables provided by MSR. Two words were considered p"
N12-1034,J08-4005,0,0.0134996,"Missing"
N12-1034,H05-1016,0,0.0903283,"00 UMass 1000 Best supervised system 0.721 0.706 0.661 Wordnet MSR Paraphrases Syntactic paraphrases 0.657 0.642 0.575 Precision 0.1 Precision 0.2 Precision 0.3 Precision 0.4 Precision 0.5 Recall 0.9 Recall 0.8 Recall 0.7 Recall 0.6 Recall 0.5 0.603 0.672 0.565 0.603 0.626 0.609 0.606 0.632 0.610 0.626 Table 1: TDT FSD results for different systems, lower is better. The number next to UMass system indicates the number of features kept for each document (selected according to their TFIDF). All paraphrasing systems work with full documents. Results for the best supervised system were taken from Kumaran and Allan (2005). The difference between our system and the UMass system is significant at p = 0.05 using a paired t-test over the individual topic costs. We were not able to test significance against the supervised state-of-theart because we did not have access to this system. In terms of efficiency, our approach is still O(1), like the approach in Petrovi´c et al. (2010), but in practice it is somewhat slower because hashing the expanded documents takes more time. We measured the running time of our system, and it is 3.5 times slower than the basic approach of Petrovi´c et al. (2010), but also 3.5 times fas"
N12-1034,J10-3003,0,0.0598622,"es. Paraphrases are alternative ways of expressing the same meaning in the same language. For example, the phrase he got married can be paraphrased as he tied the knot. Paraphrases were already shown to help in a number of tasks: for machine translation to translate unknown phrases by translating their paraphrases (Callison-Burch et al., 2006), for query expansion in information retrieval (Sp¨arck Jones and Tait, 1984; Jones et al., 2006), or for improving question answering (Riezler et al., 2007). A much more detailed discussion on the use of paraphrases and ways to extract them is given in (Madnani and Dorr, 2010). Here, we present the first work to use paraphrases for improving first story detection. Using paraphrases, we are able to detect that some documents previously thought to be about new events are actually paraphrases of the documents already 338 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 338–346, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics seen. Our approach is simple and we show a novel way of integrating paraphrases with locality sensitive hashing (LSH) (Indyk and"
N12-1034,N10-1021,1,0.300265,"Missing"
N12-1034,W04-3219,0,0.0649347,"Missing"
N12-1034,P07-1059,0,0.0135567,"variation plagues many IR and NLP tasks, and one way it has been addressed in the past is through the use of paraphrases. Paraphrases are alternative ways of expressing the same meaning in the same language. For example, the phrase he got married can be paraphrased as he tied the knot. Paraphrases were already shown to help in a number of tasks: for machine translation to translate unknown phrases by translating their paraphrases (Callison-Burch et al., 2006), for query expansion in information retrieval (Sp¨arck Jones and Tait, 1984; Jones et al., 2006), or for improving question answering (Riezler et al., 2007). A much more detailed discussion on the use of paraphrases and ways to extract them is given in (Madnani and Dorr, 2010). Here, we present the first work to use paraphrases for improving first story detection. Using paraphrases, we are able to detect that some documents previously thought to be about new events are actually paraphrases of the documents already 338 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 338–346, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics seen. Ou"
N12-1034,P10-2043,0,0.019871,"Missing"
N15-1158,P14-1073,0,0.0129427,"involves applying a scalable clustering algorithm to all the mentions. We consider streaming CDC, hence our system must conform to the streaming computational resource model (Muthukrishnan, 2005). Each mention is processed in bounded time and only a constant amount of memory is used. Honoring these constraints ensures our system can be applied to infinite streams such as newswire or social media. Storing all the mentions in memory is clearly infeasible, hence we need to either compress mentions Related Work There are many existing approaches to CDC (Bagga and Baldwin, 1998; Lee et al., 2012; Andrews et al., 2014). Few of them scale to large datasets. Singh et al. (2011) proposed a distributed hierarchical factor graph approach. While it can process large datasets, the scalability comes from distributing the problem. Wick et al. (2012) proposed a similar approach based on compressing mentions, while scalable it does not conform to the streaming resource model. The only prior work that addressed online/streaming CDC (Rao et al., 2010) was also not constrained to the streaming model. None of these approaches operate over an unbounded stream processing mentions in constant time/memory. 1391 Human Language"
N15-1158,P98-1012,0,0.687019,"ltiple documents. Typically large scale CDC involves applying a scalable clustering algorithm to all the mentions. We consider streaming CDC, hence our system must conform to the streaming computational resource model (Muthukrishnan, 2005). Each mention is processed in bounded time and only a constant amount of memory is used. Honoring these constraints ensures our system can be applied to infinite streams such as newswire or social media. Storing all the mentions in memory is clearly infeasible, hence we need to either compress mentions Related Work There are many existing approaches to CDC (Bagga and Baldwin, 1998; Lee et al., 2012; Andrews et al., 2014). Few of them scale to large datasets. Singh et al. (2011) proposed a distributed hierarchical factor graph approach. While it can process large datasets, the scalability comes from distributing the problem. Wick et al. (2012) proposed a similar approach based on compressing mentions, while scalable it does not conform to the streaming resource model. The only prior work that addressed online/streaming CDC (Rao et al., 2010) was also not constrained to the streaming model. None of these approaches operate over an unbounded stream processing mentions in"
N15-1158,D12-1045,0,0.0288697,"ly large scale CDC involves applying a scalable clustering algorithm to all the mentions. We consider streaming CDC, hence our system must conform to the streaming computational resource model (Muthukrishnan, 2005). Each mention is processed in bounded time and only a constant amount of memory is used. Honoring these constraints ensures our system can be applied to infinite streams such as newswire or social media. Storing all the mentions in memory is clearly infeasible, hence we need to either compress mentions Related Work There are many existing approaches to CDC (Bagga and Baldwin, 1998; Lee et al., 2012; Andrews et al., 2014). Few of them scale to large datasets. Singh et al. (2011) proposed a distributed hierarchical factor graph approach. While it can process large datasets, the scalability comes from distributing the problem. Wick et al. (2012) proposed a similar approach based on compressing mentions, while scalable it does not conform to the streaming resource model. The only prior work that addressed online/streaming CDC (Rao et al., 2010) was also not constrained to the streaming model. None of these approaches operate over an unbounded stream processing mentions in constant time/memo"
N15-1158,H05-1004,0,0.148701,"Missing"
N15-1158,P14-2112,1,0.879902,"Missing"
N15-1158,N10-1021,1,0.80969,"Missing"
N15-1158,C10-2121,0,0.0163986,"at is bursting) it is impossible to resolve references to other entities. • Window: We sample a moving window of the most recent mentions (first in, first out). For example this technique assumes that if we are processing mentions on Monday with a window of approximately 24 hours all relevant entities were mentioned since Sunday. These properties suggest we should take into account a notion of streaming discourse when sampling: mentions sampled should depend on the previous mentions (informed sampling). 4 text and contextual similarity (weighted 0.8 and 0.2 respectively) similar to Rao et al. (2010). Mention text similarity is measured using cosine similarity of character skip bigram indicator vectors and contextual similarity is measured using tf-idf weighted cosine similarity of tweet terms. The stream is processed sequentially: we resolve each mention by finding its nearest neighbor in the sample, linking the two mentions if the similarity is above the linking threshold. Approach We implemented a representative pairwise streaming CDC system using single link clustering. Mention similarity is a linear combination of mention 1392 Insertion: Add current mention to sample. Removal: Do not"
N15-1158,D11-1141,0,0.120459,"Missing"
N15-1158,P11-1080,0,0.210208,"e mentions. We consider streaming CDC, hence our system must conform to the streaming computational resource model (Muthukrishnan, 2005). Each mention is processed in bounded time and only a constant amount of memory is used. Honoring these constraints ensures our system can be applied to infinite streams such as newswire or social media. Storing all the mentions in memory is clearly infeasible, hence we need to either compress mentions Related Work There are many existing approaches to CDC (Bagga and Baldwin, 1998; Lee et al., 2012; Andrews et al., 2014). Few of them scale to large datasets. Singh et al. (2011) proposed a distributed hierarchical factor graph approach. While it can process large datasets, the scalability comes from distributing the problem. Wick et al. (2012) proposed a similar approach based on compressing mentions, while scalable it does not conform to the streaming resource model. The only prior work that addressed online/streaming CDC (Rao et al., 2010) was also not constrained to the streaming model. None of these approaches operate over an unbounded stream processing mentions in constant time/memory. 1391 Human Language Technologies: The 2015 Annual Conference of the North Ame"
N15-1158,P12-1040,0,0.0200393,"unded time and only a constant amount of memory is used. Honoring these constraints ensures our system can be applied to infinite streams such as newswire or social media. Storing all the mentions in memory is clearly infeasible, hence we need to either compress mentions Related Work There are many existing approaches to CDC (Bagga and Baldwin, 1998; Lee et al., 2012; Andrews et al., 2014). Few of them scale to large datasets. Singh et al. (2011) proposed a distributed hierarchical factor graph approach. While it can process large datasets, the scalability comes from distributing the problem. Wick et al. (2012) proposed a similar approach based on compressing mentions, while scalable it does not conform to the streaming resource model. The only prior work that addressed online/streaming CDC (Rao et al., 2010) was also not constrained to the streaming model. None of these approaches operate over an unbounded stream processing mentions in constant time/memory. 1391 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1391–1396, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 3 Entities in Streams Streams like"
N15-1158,C98-1012,0,\N,Missing
N16-1122,D10-1124,0,0.627365,"Missing"
N16-1122,W08-0804,1,0.684025,"t data. All other parameters used default settings. 3 Our reliance on text features created a very large feature space, but only a small fraction of these occur with any regularity. Previous work has shown feature selection helpful for geolocation (Han et al., 2014). We tried L1 regularization for feature selection without a significant change to our results. It may be that our larger volume of training data removes the need for feature selection. Alternatively, we use feature hashing (to a 31-bit feature space) which can be a form of regularization as feature collisions mitigate overfitting (Ganchev and Dredze, 2008; Weinberger et al., 2009). 4 Evaluation We report the four evaluation metrics of Han et al. (2014): city accuracy (AccCi), country accuracy (AccCo), accuracy within 161 km (100 miles) (Acc@161), and the median error in km (Median). Baselines We include two baselines: (1) the majority predictor: always predicts the most popular label. (2) alias matching: we create a list of aliases for each of the 2983 cities from the genomes dataset, which includes the smaller cities clustered together by Han et al. (2014). We search each tweet and the user’s profile location for these aliases, assigning a tw"
N16-1122,P14-5007,1,0.337563,"Missing"
N16-1122,D15-1256,0,0.124047,"Missing"
P04-1023,J93-2003,0,0.0355453,"importance of amplifying the contribution of word-aligned data during parameter estimation. This paper shows that word-level alignments improve the parameter estimates for translation models, which in turn results in improved statistical translation for languages that do not have large sentence-aligned parallel corpora. 2 Parameter Estimation Using Sentence-Aligned Corpora The task of statistical machine translation is to choose the source sentence, e, that is the most probable translation of a given sentence, f , in a foreign language. Rather than choosing e∗ that directly maximizes p(e|f ), Brown et al. (1993) apply Bayes’ rule and select the source sentence: e∗ = arg max p(e)p(f |e). e (1) In this equation p(e) is a language model probability and is p(f |e) a translation model probability. A series of increasingly sophisticated translation models, referred to as the IBM Models, was defined in Brown et al. (1993). The translation model, p(f |e) defined as a marginal probability obtained by summing over word-level alignments, a, between the source and target sentences: X p(f |e) = p(f , a|e). (2) a While word-level alignments are a crucial component of the IBM models, the model parameters are genera"
P04-1023,W01-1409,0,0.0135292,"of labeled and unlabeled data. 7 Discussion and Future Work In this paper we show with the appropriate modification of EM significant improvement gains can be had through labeling word alignments in a bilingual corpus. Because of this significantly less data is required to achieve a low alignment error rate or high Bleu score. This holds even when using noisy word alignments such as our automatically created set. One should take our research into account when trying to efficiently create a statistical machine translation system for a language pair for which a parallel corpus is not available. Germann (2001) describes the cost of building a Tamil-English parallel corpus from scratch, and finds that using professional translations is prohibitively high. In our experience it is quicker to manually word-align translated sentence pairs than to translate a sentence, and word-level alignment can be done by someone who might not be fluent enough to produce translations. It might therefore be possible to achieve a higher performance at a fraction of the cost by hiring a nonprofessional produce word-alignments after a limited set of sentences have been translated. We plan to investigate whether it is feas"
P04-1023,N03-1017,0,0.0757788,"act that using word-aligned data in estimating the parameters for machine translation leads to better alignments is predictable. A more significant result is whether it leads to improved translation quality. In order to test that our improved parameter estimates lead to better translation quality, we used a state-of-the-art phrase-based decoder to translate a held out set of German sentences into English. The phrase-based decoder extracts phrases from the word alignments produced by GIZA++, and computes translation probabilities based on the frequency of one phrase being aligned with another (Koehn et al., 2003). We trained a language model Ratio 0.1 0.2 0.3 0.5 0.7 0.9 AER when λ = Standard MLE 11.73 10.89 10.23 8.65 8.29 7.78 when λ = .9 9.40 8.66 8.13 8.19 8.03 7.78 Table 5: The effect of weighting word-aligned data more heavily that its proportion in the training data (corpus size 16000 sentence pairs) using the 34,000 English sentences from the training set. Table 4 shows that using word-aligned data leads to better translation quality than using sentencealigned data. Particularly, significantly less data is needed to achieve a high Bleu score when using word alignments. Training on a corpus of"
P04-1023,W03-0301,0,0.0551552,"Missing"
P04-1023,J03-1002,0,0.0361955,"the word aligned data. The example alignments were held out sentence pairs that were aligned after training on 500 sentence pairs. The alignments produced when the training on word-aligned data are dramatically better than when training on sentence-aligned data. We contrasted these improvements with the improvements that are to be had from incorporating a bilingual dictionary into the estimation process. For this experiment we allowed a bilingual dictionary to constrain which words can act as translations of each other during the initial estimates of translation probabilities (as described in Och and Ney (2003)). As can be seen in Table 3, using a dictionary reduces the AER when compared to using GIZA++ without a dictionary, but not as dramatically as integrating the word-alignments. We further tried combining a dictionary with our word-alignments but found that the dictionary results in only very minimal improvements over using word-alignments alone. 1. Verifying that the use of word-aligned data has an impact on the quality of alignments predicted by the IBM Models, and comparing the quality increase to that gained by using a bilingual dictionary in the estimation stage. 2. Evaluating whether impr"
P04-1023,2001.mtsummit-papers.68,0,0.0186586,"Missing"
P04-1023,J03-3002,0,0.00588674,"on probabilistic translation models (Brown et al., 1993) are generally trained using sentence-aligned parallel corpora. For many language pairs these exist in abundant quantities. However for new domains or uncommon language pairs extensive parallel corpora are often hard to come by. Two factors could increase the performance of statistical machine translation for new language pairs and domains: a reduction in the cost of creating new training data, and the development of more efficient methods for exploiting existing training data. Approaches such as harvesting parallel corpora from the web (Resnik and Smith, 2003) address the creation of data. We take the second, complementary approach. We address the problem of efficiently exploiting existing parallel corpora by adding explicit word-level alignments between a number of the sentence pairs in the training corpus. We modify the standard parameter estimation procedure for IBM Models and HMM variants so that they can exploit these additional wordlevel alignments. Our approach uses both word- and sentence-level alignments for training material. 1. Describe how the parameter estimation framework of Brown et al. (1993) can be adapted to incorporate word-level"
P04-1023,P02-1040,0,\N,Missing
P05-1002,W02-2018,0,0.0757911,"likelihood L is given by L = X log p(y(i) |x(i) ) i =  (i) +1 X T X X i −  t=1 log Z(x(i) ) (i) (i) λk fk (t, yt−1 , yt , x(i) ) k o where x(i) and y(i) are the ith observation and label sequence. Note that a prior is often included in the L formulation; it has been excluded here for clarity of exposition. CRF estimation methods include generalised iterative scaling (GIS), improved iterative scaling (IIS) and a variety of gradient based methods. In recent empirical studies on maximum entropy models and CRFs, limited memory variable metric (LMVM) has proven to be the most efficient method (Malouf, 2002; Wallach, 2002); accordingly, we have used LMVM for CRF estimation. Every iteration of LMVM training requires the computation of the log-likelihood and its derivative with respect to each parameter. The partition function Z(x) can be calculated efficiently using dynamic programming with the forward algorithm. P Z(x) is given by y αT (y) where α are the forward values, defined recursively as αt+1 (y) = X y0 αt (y 0 ) exp X k λk fk (t + 1, y 0 , y, x) 3 The derivative of the log-likelihood is given by ∂L ∂λk  (i) +1 X T X = i X −  (i) (i) fk (t, yt−1 , yt , x(i) ) t=1 p(y|x(i) ) y (i) +1 TX"
P05-1002,W03-0430,0,0.0584727,"onditional Random Fields (CRFs) have been applied with considerable success to a number of natural language processing tasks. However, these tasks have mostly involved very small label sets. When deployed on tasks with larger label sets, the requirements for computational resources mean that training becomes intractable. Efficient inference and training methods exist when the graphical structure of the model forms a chain, where each position in a sequence is connected to its adjacent positions. CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003), simplified part-of-speech (POS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of tabular data (Pinto et al., 2003), among other tasks. This paper describes a method for training CRFs on such tasks, using error correcting output codes (ECOC). A number of CRFs are independently trained on the separate binary labelling tasks of distinguishing between a subset of the labels and its complement. During decoding, these models are combined to produce a predicted label sequence which is resilient to errors by individual models. Error-correcting CRF train"
P05-1002,P04-1007,0,0.075963,"Missing"
P05-1002,W00-0726,0,0.0710543,"less than that of regular CRF training methods. We have evaluated the error-correcting CRF on the CoNLL 2003 named entity recognition (NER) task (Sang and Meulder, 2003), where we show that the method yields similar generalisation performance to standardly formulated CRFs, while requiring only a fraction of the resources, and no increase in training time. We have also shown how the errorcorrecting CRF scales when applied to the larger task of POS tagging the Penn Treebank and also the even larger task of simultaneously noun phrase chunking (NPC) and POS tagging using the CoNLL 2000 data-set (Sang and Buchholz, 2000). 2 Conditional random fields CRFs are undirected graphical models used to specify the conditional probability of an assignment of output labels given a set of input observations. We consider only the case where the output labels of the 11 model are connected by edges to form a linear chain. The joint distribution of the label sequence, y, given the input observation sequence, x, is given by TX +1 X 1 λk fk (t, yt−1 , yt , x) exp Z(x) t=1 k p(y|x) = where T is the length of both sequences and λk are the parameters of the model. The functions fk are feature functions which map properties of the"
P05-1002,N03-1028,0,0.192363,"cessing tasks. However, these tasks have mostly involved very small label sets. When deployed on tasks with larger label sets, the requirements for computational resources mean that training becomes intractable. Efficient inference and training methods exist when the graphical structure of the model forms a chain, where each position in a sequence is connected to its adjacent positions. CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003), simplified part-of-speech (POS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of tabular data (Pinto et al., 2003), among other tasks. This paper describes a method for training CRFs on such tasks, using error correcting output codes (ECOC). A number of CRFs are independently trained on the separate binary labelling tasks of distinguishing between a subset of the labels and its complement. During decoding, these models are combined to produce a predicted label sequence which is resilient to errors by individual models. Error-correcting CRF training is much less resource intensive and has a much faster training time than a standardly formulated CRF, while"
P05-1002,P05-1003,1,0.832079,"j (q|x) is the predicted probability of q given x by the j th weak learner, bj (y) is the bit string representing y for the j th weak learner and Z 0 (x) is the partition function. The log probability is X {Fj (bj (y), x) · λj − log Zj (x)} − log Z 0 (x) j +1 where Fj (y, x) = Tt=1 fj (t, yt−1 , yt , x). This log probability can then be maximised using the Viterbi algorithm as before, noting that the two log terms are constant with respect to y and thus need not be evaluated. Note that this decoding is an equivalent formulation to a uniformly weighted logarithmic opinion pool, as described in Smith et al. (2005). Of the three decoding methods, Standalone has the lowest complexity, requiring only a binary Viterbi decoding for each weak learner. Marginals is slightly more complex, requiring the forward and backward values. Product, however, requires Viterbi decoding with the full label set, and many features – the union of the features of each weak learner – which can be quite computationally demanding. P 3.3 Choice of code The accuracy of ECOC methods are highly dependent on the quality of the code. The ideal code has diverse rows, yielding a high error-correcting capability, and diverse columns such"
P05-1002,N03-1033,0,0.00898622,"Missing"
P05-1002,W03-0419,0,\N,Missing
P05-1003,P05-1002,1,0.872976,"Missing"
P05-1003,W03-0424,0,0.021869,"onsists only of features that involve label X at the current or previous positions. These experts therefore focus on trying to model the distribution of a particular label. 4.3 Expert sets For each task we compare the performance of the LOP-CRF to that of the standard CRF by defining a single, complex CRF, which we call a monolithic CRF, and a range of expert sets. The monolithic CRF for NER comprises a number of word and POS tag features in a window of five words around the current word, along with a set of orthographic features defined on the current word. These are based on those found in (Curran and Clark, 2003). Examples include whether the current word is capitalised, is an initial, contains a digit, contains punctuation, etc. The monolithic CRF for NER has 450, 345 features. The monolithic CRF for POS tagging comprises word and POS features similar to those in the NER monolithic model, but over a smaller number of orthographic features. The monolithic model for POS tagging has 188, 448 features. Each of our expert sets consists of a number of CRF experts. Usually these experts are designed to 4 See (Cohn et al., 2005) for a scaling method allowing the full POS tagging task with CRFs. 22 • Random c"
P05-1003,W02-2018,0,0.0593803,"is the partition function that ensures (1) represents a probability distribution. The functions f k are feature functions representing the occurrence of different events in the sequences s and o. The parameters λk can be estimated by maximising the conditional log-likelihood of a set of labelled training sequences. The log-likelihood is given by: L (λ ) = ˜ s) log p(s |o; λ ) ∑ p(o, o,s "" T +1 = ˜ s) ∑ λ · f(s, o,t) ∑ p(o, o,s − t=1 ˜ log Z(o; λ ) ∑ p(o) # E p(o,s) [ fk ] − E p(s|o) [ fk ] = 0, ∀k ˜ In general this cannot be solved for the λk in closed form so numerical routines must be used. Malouf (2002) and Sha and Pereira (2003) show that gradient-based algorithms, particularly limited memory variable metric (LMVM), require much less time to reach convergence, for some NLP tasks, than the iterative scaling methods (Della Pietra et al., 1997) previously used for log-linear optimisation problems. In all our experiments we use the LMVM method to train the CRFs. For CRFs with general graphical structure, calculation of E p(s|o) [ fk ] is intractable, but for the linear chain case Lafferty et al. (2001) describe an efficient dynamic programming procedure for inference, similar in nature to the f"
P05-1003,W03-0430,0,0.0221269,"s (LOP-CRFs). We apply the LOP-CRF to two sequencing tasks. Our results show that unregularised expert CRFs with an unregularised CRF under a LOP can outperform the unregularised CRF, and attain a performance level close to the regularised CRF. LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search. 1 Introduction In recent years, conditional random fields (CRFs) (Lafferty et al., 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004). In general, this work has demonstrated the susceptibility of CRFs to overfit the training data during parameter estimation. As miles@inf.ed.ac.uk a consequence, it is now standard to use some form of overfitting reduction in CRF training. Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction (McCallum, 2003) and a full Bayesian approach to training and inference (Qi et al., 2005). These advanced methods tend to be difficult to implement and"
P05-1003,N04-1012,1,0.69984,"actable, but for the linear chain case Lafferty et al. (2001) describe an efficient dynamic programming procedure for inference, similar in nature to the forward-backward algorithm in hidden Markov models. 3 Logarithmic Opinion Pools In this paper an expert model refers a probabilistic model that focuses on modelling a specific subset of some probability distribution. The concept of combining the distributions of a set of expert models via a weighted product has previously been used in a range of different application areas, including economics and management science (Bordley, 1982), and NLP (Osborne and Baldridge, 2004). In this paper we restrict ourselves to sequence models. Given a set of sequence model experts, indexed by α , with conditional distributions pα (s |o) and a set of non-negative normalised weights wα , a logarithmic opinion pool 2 is defined as the distribution: pLOP (s |o) = o where p(o, ˜ s) and p(o) ˜ are empirical distributions defined by the training set. At the maximum likelihood solution the model satisfies a set of feature constraints, whereby the expected count of each feature under the model is equal to its empirical count on the training data: 1 In this paper we assume there is a o"
P05-1003,N04-1042,0,0.0472658,"show that unregularised expert CRFs with an unregularised CRF under a LOP can outperform the unregularised CRF, and attain a performance level close to the regularised CRF. LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search. 1 Introduction In recent years, conditional random fields (CRFs) (Lafferty et al., 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004). In general, this work has demonstrated the susceptibility of CRFs to overfit the training data during parameter estimation. As miles@inf.ed.ac.uk a consequence, it is now standard to use some form of overfitting reduction in CRF training. Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction (McCallum, 2003) and a full Bayesian approach to training and inference (Qi et al., 2005). These advanced methods tend to be difficult to implement and are often computationally expensive. Consequently, due to its ease of imp"
P05-1003,N03-1028,0,0.195665,"this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs). We apply the LOP-CRF to two sequencing tasks. Our results show that unregularised expert CRFs with an unregularised CRF under a LOP can outperform the unregularised CRF, and attain a performance level close to the regularised CRF. LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search. 1 Introduction In recent years, conditional random fields (CRFs) (Lafferty et al., 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004). In general, this work has demonstrated the susceptibility of CRFs to overfit the training data during parameter estimation. As miles@inf.ed.ac.uk a consequence, it is now standard to use some form of overfitting reduction in CRF training. Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction (McCallum, 2003) and a full Bayesian approach to training and inference (Qi et al., 2005). These adva"
P05-1003,W00-0726,0,0.035673,"3 shared task dataset (Tjong Kim Sang and De Meulder, 2003). For this dataset the entity types are: persons (PER), locations (LOC), organisations (ORG) and miscellaneous (MISC). The training set consists of 14, 987 sentences and 204, 567 tokens, the development set consists of 3, 466 sentences and 51, 578 tokens and the test set consists of 3, 684 sentences and 46, 666 tokens. 4.2 Part-of-Speech Tagging POS tagging involves labelling each word in a sentence with its part-of-speech, for example noun, verb, adjective, etc. For our experiments we use the CoNLL-2000 shared task dataset (Tjong Kim Sang and Buchholz, 2000). This has 48 different POS tags. In order to make training time manageable4 , we collapse the number of POS tags from 48 to 5 following the procedure used in (McCallum et al., 2003). In summary: focus on modelling a particular aspect or subset of the distribution. As we saw earlier, the aim here is to define experts that model parts of the distribution well while retaining mutual diversity. The experts from a particular expert set are combined under a LOP-CRF and the weights are trained as described previously. We define our range of expert sets as follows: • Simple consists of the monolithic"
P05-1003,W03-0419,0,0.0675712,"Missing"
P05-1003,W98-1120,0,\N,Missing
P05-1003,I05-1014,0,\N,Missing
P05-1003,W96-0102,0,\N,Missing
P05-1003,M98-1015,0,\N,Missing
P05-1003,J93-2004,0,\N,Missing
P05-1003,W04-3230,0,\N,Missing
P05-1003,W96-0213,0,\N,Missing
P05-1003,W03-0403,1,\N,Missing
P05-1003,E99-1001,0,\N,Missing
P05-1003,W06-2918,1,\N,Missing
P05-1003,C04-1081,0,\N,Missing
P05-1003,W03-0428,0,\N,Missing
P05-1003,W03-0425,0,\N,Missing
P05-1003,C00-2137,0,\N,Missing
P05-1003,W03-0423,0,\N,Missing
P05-1003,N03-1002,0,\N,Missing
P05-1003,W02-1002,0,\N,Missing
P05-1003,J94-2001,0,\N,Missing
P05-1003,J95-4004,0,\N,Missing
P05-1003,W05-0622,1,\N,Missing
P05-1003,W03-1019,0,\N,Missing
P05-1003,P04-1014,0,\N,Missing
P05-1003,P04-1007,0,\N,Missing
P05-1003,P04-1086,0,\N,Missing
P05-1003,kruengkrai-etal-2006-conditional,0,\N,Missing
P05-1003,W02-2024,0,\N,Missing
P05-1003,I05-1078,1,\N,Missing
P05-1003,M98-1004,0,\N,Missing
P05-1003,C96-1079,0,\N,Missing
P05-1003,W03-1018,0,\N,Missing
P05-1003,N06-1012,0,\N,Missing
P05-1003,P02-1062,0,\N,Missing
P06-1122,J93-2003,0,0.0219851,"y conflating redundant types, e.g. vert = {vert, verte, verts, vertes}, and averaging bilingual statistics associated with these events. Lexical redundancy between languages In statistical MT, the source and target lexicons are usually defined as the sets of distinct types observed in the parallel training corpus for each language. Such models may not be optimal for certain language pairs and training regimes. A word-level statistical translation model approximates the probability P r(E|F ) that a source type indexed by F will be translated as a target type indexed by E. Standard models, e.g. Brown et al. (1993), consist of discrete probability distributions with separate parameters for each unique pairing of a source and target types; no attempt is made to leverage structure within the event spaces E and F during parameter estimation. This results in a large number of parameters that must be estimated from limited amounts of parallel corpora. We refer to distinctions made between lexical types in one language that do not result in different distributions over types in the other language as lexically redundant for the language pair. Since the role of the translation model is to determine a distributi"
P06-1122,cmejrek-etal-2004-prague,0,0.0244349,"e use them to smooth the phrase translation model (see Exp. 2). Here we consider a simple interpolation scheme; they could also be used within a backoff model (Yang and Kirchhoff, 2006). 4.2 Experimental set-up The system we use is described in (Koehn, 2004). The phrase-based translation model includes phrase-level and lexical weightings in both directions. We use the decoder’s default behaviour for unknown words copying them verbatim to the output. Smoothed trigram language models are estimated on training sections of the parallel corpus. We used the parallel sections of the Prague Treebank (Cmejrek et al., 2004), French and English sections of the Europarl corpus (Koehn, 2005) and parallel text from the Welsh Assembly4 (see Table1). The source languages, Czech, French and Welsh, were chosen on the basis that they may exhibit different degrees of redundancy with respect to English and that they differ morphologically. Only the Czech corpus has explicit morphological annotation. Experiments Phrase-based SMT systems have been shown to outperform word-based approaches (Koehn et al., 2003). We evaluate the effects of lexicon model selection on translation quality by considering two applications within a p"
P06-1122,H05-1085,0,0.660305,"stering has been motivated somewhat differently and not made use of cluster-based models to assign translation probabilities directly (Wang et al., 1996), (Och, 1998). There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). These approaches have used morphological annotation (e.g. lemmas and part of speech tags) to provide explicit supervision. They have also involved manually specifying which morphological distinc975 References tions are redundant (Goldwater and McClosky, 2005). In contrast, we attempt to learn both equivalence classes and redundant relations automatically. Our experiments with orthographic features suggest that some morphological redundancies can be acquired in an unsupervised fashion. The marginal likelihood hard-clustering algorithm that we propose here for translation model selection can be viewed as a Bayesian k-means algorithm and is an application of Bayesian model selection techniques, e.g., (Wolpert, 1995). The Markov random field prior over model structure extends the fixed uniform prior over clusters implicit in k-means clustering and is"
P06-1122,N03-1017,0,0.0242829,"Missing"
P06-1122,koen-2004-pharaoh,0,0.0248596,"e original corpus (see Exp. 1). To extrapolate a mapping over phrases from our type-level models we can map each type within a phrase to its corresponding cluster label. This, however, results in a large number of distinct phrases being collapsed down to a single ‘clustered phrase’. Using these directly may spread probability mass too widely. Instead we use them to smooth the phrase translation model (see Exp. 2). Here we consider a simple interpolation scheme; they could also be used within a backoff model (Yang and Kirchhoff, 2006). 4.2 Experimental set-up The system we use is described in (Koehn, 2004). The phrase-based translation model includes phrase-level and lexical weightings in both directions. We use the decoder’s default behaviour for unknown words copying them verbatim to the output. Smoothed trigram language models are estimated on training sections of the parallel corpus. We used the parallel sections of the Prague Treebank (Cmejrek et al., 2004), French and English sections of the Europarl corpus (Koehn, 2005) and parallel text from the Welsh Assembly4 (see Table1). The source languages, Czech, French and Welsh, were chosen on the basis that they may exhibit different degrees o"
P06-1122,2005.mtsummit-papers.11,0,0.0438925,"sider a simple interpolation scheme; they could also be used within a backoff model (Yang and Kirchhoff, 2006). 4.2 Experimental set-up The system we use is described in (Koehn, 2004). The phrase-based translation model includes phrase-level and lexical weightings in both directions. We use the decoder’s default behaviour for unknown words copying them verbatim to the output. Smoothed trigram language models are estimated on training sections of the parallel corpus. We used the parallel sections of the Prague Treebank (Cmejrek et al., 2004), French and English sections of the Europarl corpus (Koehn, 2005) and parallel text from the Welsh Assembly4 (see Table1). The source languages, Czech, French and Welsh, were chosen on the basis that they may exhibit different degrees of redundancy with respect to English and that they differ morphologically. Only the Czech corpus has explicit morphological annotation. Experiments Phrase-based SMT systems have been shown to outperform word-based approaches (Koehn et al., 2003). We evaluate the effects of lexicon model selection on translation quality by considering two applications within a phrase-based SMT system. 4.1 Types 54K 53K 46K Table 1: Parallel co"
P06-1122,J04-2003,0,0.0656603,"rrored by higher compression rates for these lexicons (see Table. 6) supporting the conjecture that word-alignment requires less information than full-blown translation. The results of the lemma6 Related Work Previous work on automatic bilingual word clustering has been motivated somewhat differently and not made use of cluster-based models to assign translation probabilities directly (Wang et al., 1996), (Och, 1998). There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). These approaches have used morphological annotation (e.g. lemmas and part of speech tags) to provide explicit supervision. They have also involved manually specifying which morphological distinc975 References tions are redundant (Goldwater and McClosky, 2005). In contrast, we attempt to learn both equivalence classes and redundant relations automatically. Our experiments with orthographic features suggest that some morphological redundancies can be acquired in an unsupervised fashion. The marginal likelihood hard-clustering algorithm that we propose here for translation model selection can b"
P06-1122,J03-1002,0,0.00508419,"ntiation to work with a large implicit feature set defined by a constrained string edit. The algorithm has two free parameters: α determining the strength of the Dirichlet prior used in the marginal likelihood, p(D|C), and β which determines the contribution of pMRF (C) to Eq. (1). 4 Tokens 468K 5682K 4578K Applications to phrase-based SMT A phrase-based translation model can be estimated in two stages: first a parallel corpus is aligned at the word-level and then phrase pairs are extracted (Koehn et al., 2003). Aligning tokens in parallel sentences using the IBM Models (Brown et al., 1993), (Och and Ney, 2003) may require less information than full-blown translation since the task is constrained by the source and target tokens present in each sentence pair. In the phrase-level translation table, however, the model must assign 4.3 Models All models used in the experiments are defined as mappings of the source and target vocabularies. The target vocabulary includes all distinct types 4 This Welsh-English parallel text is in the public domain. Contact the first author for details. 973 experiments on development data. Wordalignments were generated using the optimal max-pref mapping for each training se"
P06-1122,E06-1006,0,0.382481,"he number of tokens in a sentence, hence the word-alignments can be associated with the original corpus (see Exp. 1). To extrapolate a mapping over phrases from our type-level models we can map each type within a phrase to its corresponding cluster label. This, however, results in a large number of distinct phrases being collapsed down to a single ‘clustered phrase’. Using these directly may spread probability mass too widely. Instead we use them to smooth the phrase translation model (see Exp. 2). Here we consider a simple interpolation scheme; they could also be used within a backoff model (Yang and Kirchhoff, 2006). 4.2 Experimental set-up The system we use is described in (Koehn, 2004). The phrase-based translation model includes phrase-level and lexical weightings in both directions. We use the decoder’s default behaviour for unknown words copying them verbatim to the output. Smoothed trigram language models are estimated on training sections of the parallel corpus. We used the parallel sections of the Prague Treebank (Cmejrek et al., 2004), French and English sections of the Europarl corpus (Koehn, 2005) and parallel text from the Welsh Assembly4 (see Table1). The source languages, Czech, French and"
P06-1122,N04-1033,0,0.00848523,"ents in Exp. 1 were mirrored by higher compression rates for these lexicons (see Table. 6) supporting the conjecture that word-alignment requires less information than full-blown translation. The results of the lemma6 Related Work Previous work on automatic bilingual word clustering has been motivated somewhat differently and not made use of cluster-based models to assign translation probabilities directly (Wang et al., 1996), (Och, 1998). There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). These approaches have used morphological annotation (e.g. lemmas and part of speech tags) to provide explicit supervision. They have also involved manually specifying which morphological distinc975 References tions are redundant (Goldwater and McClosky, 2005). In contrast, we attempt to learn both equivalence classes and redundant relations automatically. Our experiments with orthographic features suggest that some morphological redundancies can be acquired in an unsupervised fashion. The marginal likelihood hard-clustering algorithm that we propose here for translat"
P06-1122,E99-1010,0,\N,Missing
P07-1065,P05-1033,0,0.00367382,"the filter may erroneously report that an item not in the set is a member. False negatives, on the other hand, will never occur: the error is said to be one-sided. In this paper, we show that a Bloom filter can be used effectively for language modelling within an SMT decoder and present the log-frequency Bloom filter, an extension of the standard Boolean BF that Language modelling (LM) is a crucial component in statistical machine translation (SMT). Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models, e.g. (Chiang, 2005). Although it is well-known that higher-order LMs and 1 For extensions of the framework presented here to standmodels trained on additional monolingual corpora alone smoothed Bloom filter language models, we refer the can yield better translation performance, the chal- reader to a companion paper (Talbot and Osborne, 2007). 512 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 512–519, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics takes advantage of the Zipf-like distribution of corpus statistics to allow frequen"
P07-1065,D07-1091,0,0.0761375,"Missing"
P07-1065,D07-1049,1,\N,Missing
P08-1024,P06-1009,1,0.580917,"Missing"
P08-1024,D07-1007,0,0.0256813,"training sentences Figure 6. Learning curve showing that the model continues to improve as we increase the number of training sentences (development set) Our model avoids the estimation biases associated with heuristic frequency count approaches and uses standard regularisation techniques to avoid degenerate maximum likelihood solutions. Having demonstrated the efficacy of our model with very simple features, the logical next step is to investigate more expressive features. Promising features might include those over source side reordering rules (Wang et al., 2007) or source context features (Carpuat and Wu, 2007). Rule frequency features extracted from large training corpora would help the model to overcome the issue of unreachable reference sentences. Such approaches have been shown to be effective in log-linear wordalignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006). Finally, while in this paper we have focussed on the science of discriminative machine translation, we believe that with suitable engineering this model will advance the state-of-the-art. To do so would require integrating a language model feature into the max-translation decoding algorithm. The u"
P08-1024,P05-1033,0,0.482695,"ns are analogous to the empirical observation of maximum entropy classifiers. Given these two charts we can calculate the loglikelihood of the reference translation as the insidescore from the sentence spanning cell of the reference chart, normalised by the inside-score of the spanning cell from the full chart. The gradient is calculated as the difference of the feature expectations of the two charts. Clark and Curran (2004) provides a more complete discussion of parsing with a loglinear model and latent variables. The full derivation chart is produced using a CYK parser in the same manner as Chiang (2005), and has complexity O(|e|3 ). We produce the reference chart by synchronously parsing the source and reference sentences using a variant of CYK algorithm over two dimensions, with a time complexity of O(|e|3 |f |3 ). This is an instance of the ITG alignment algorithm (Wu, 1997). This step requires the reference translation for each training instance to be contained in the model’s hypothesis space. Achieving full coverage implies inducing a grammar which generates all observed source-target pairs, which is difficult in practise. Instead we discard the unreachable portion of the training sample"
P08-1024,J07-2003,0,0.866734,"rivation. However, doing so exactly is NP-complete. For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely (Ittycheriah and Roukos, 2007; Watanabe et al., 2007), or else ignore the problem and treat derivations as translations (Liang et al., 2006; Tillmann and Zhang, 2007). In this paper we directly address the problem of spurious ambiguity in discriminative models. We use a synchronous context free grammar (SCFG) translation system (Chiang, 2007), a model which has yielded state-of-the-art results on many translation tasks. We present two main contributions. First, we develop a log-linear model of translation which is globally trained on a significant number of parallel sentences. This model maximises the conditional likelihood of the data, p(e|f ), where e and f are the English and foreign sentences, respectively. Our estimation method is theoretically sound, avoiding the biases of the heuristic relative frequency estimates 200 Proceedings of ACL-08: HLT, pages 200–208, c Columbus, Ohio, USA, June 2008. 2008 Association for Computati"
P08-1024,P04-1014,0,0.0239743,"ng sentences, D = {(e, f )}, penalised using a prior, i.e., ΛM AP = arg maxΛ pΛ (D)p(Λ). We use a zero-mean Gaussian prior with the probability  density function p0 (λk ) ∝ exp −λ2k /2σ 2 .2 This results in the following log-likelihood objective and corresponding gradient: X X L= log pΛ (e|f ) + log p0 (λk ) (4) (e,f )∈D k λk ∂L = EpΛ (d|e,f ) [hk ] − EpΛ (e|f ) [hk ] − 2 ∂λk σ (5) In order to train the model, we maximise equation (4) using L-BFGS (Malouf, 2002; Sha and Pereira, 2003). This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using inside-outside inference over the feature forest defined by the SCFG parse chart of f yielding the partition function, ZΛ (f ), required for the log-likelihood, and the marginals, required for its derivatives. Efficiently calculating the objective and its gradient requires two separate packed charts, each representing a derivation forest. The first one is the full chart over the space of possible derivations given the 2 In general, any con"
P08-1024,J07-4004,0,0.0170903,"slation given the source is the sum over all of its derivations: X pΛ (e|f ) = pΛ (d, e|f ) (3) d∈∆(e,f ) where ∆(e, f ) is the set of all derivations of the target sentence e from the source f. Most prior work in SMT, both generative and discriminative, has approximated the sum over derivations by choosing a single ‘best’ derivation using a Viterbi or beam search algorithm. In this work we show that it is both tractable and desirable to directly account for derivational ambiguity. Our findings echo those observed for latent variable log-linear models successfully used in monolingual parsing (Clark and Curran, 2007; Petrov et al., 2007). These models marginalise over derivations leading to a dependency structure and splits of non-terminal categories in a PCFG, respectively. 3.2 Training The parameters of our model are estimated from our training sample using a maximum a posteriori (MAP) estimator. This maximises the likelihood of the parallel training sentences, D = {(e, f )}, penalised using a prior, i.e., ΛM AP = arg maxΛ pΛ (D)p(Λ). We use a zero-mean Gaussian prior with the probability  density function p0 (λk ) ∝ exp −λ2k /2σ 2 .2 This results in the following log-likelihood objective and correspo"
P08-1024,W06-3105,0,0.716213,"odels must also allow for efficient training. Past work on discriminative SMT only address some of these problems. To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007), or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006). These systems all include regularisation, thereby addressing Problem 2. An interesting counterpoint is the work of DeNero et al. (2006), who show that their unregularised model finds degenerate solutions. Some of these discriminative systems have been trained on large training sets (Problem 3); these systems are the local models, for which training is much simpler. Both the global models (Liang et al., 2006; Watanabe et al., 2007) use fairly small training sets, and there is no evidence that their techniques will scale to larger data sets. Our model addresses all three of the above problems within a global model, without resorting to nbest lists or local independence assumptions. Furthermore, our model explicitly accounts for"
P08-1024,N04-1035,0,0.0260784,"Missing"
P08-1024,P06-1121,0,0.0566119,"source and target languages to the respective sides of a SCFG it is possible to describe translation as the process of parsing the source sentence using a CFG, while generating the target translation from 202 the other (Chiang, 2007). All the models we present use the grammar extraction technique described in Chiang (2007), and are bench-marked against our own implementation of this hierarchical model (Hiero). Figure 3 shows a simple instance of a hierarchical grammar with two non-terminals. Note that our approach is general and could be used with other synchronous grammar transducers (e.g., Galley et al. (2006)). 3.1 A global log-linear model Our log-linear translation model defines a conditional probability distribution over the target translations of a given source sentence. A particular sequence of SCFG rule applications which produces a translation from a source sentence is referred to as a derivation, and each translation may be produced by many different derivations. As the training data only provides source and target sentences, the derivations are modelled as a latent variable. The conditional probability of a derivation, d, for a target translation, e, conditioned on the source, f , is give"
P08-1024,N07-1008,0,0.122961,"les, depending on the type of system. Existing discriminative models require a reference derivation to optimise against, however no parallel corpora annotated for derivations exist. Ideally, a model would account for this ambiguity by marginalising out the derivations, thus predicting the best translation rather than the best derivation. However, doing so exactly is NP-complete. For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely (Ittycheriah and Roukos, 2007; Watanabe et al., 2007), or else ignore the problem and treat derivations as translations (Liang et al., 2006; Tillmann and Zhang, 2007). In this paper we directly address the problem of spurious ambiguity in discriminative models. We use a synchronous context free grammar (SCFG) translation system (Chiang, 2007), a model which has yielded state-of-the-art results on many translation tasks. We present two main contributions. First, we develop a log-linear model of translation which is globally trained on a significant number of parallel sentences. This model maximises the conditional likeliho"
P08-1024,N03-1017,0,0.0355966,"uch problems as non-literal translations, poor sentence- and word-alignments. A model which exactly translates the training data will inevitably perform poorly on held-out data. This problem of over-fitting is exacerbated in discriminative models with large, expressive, feature sets. Regularisation is essential for models with more than a handful of features. ● 1e+05 derivations ● ● ● 1e+03 ● ● ● ● 5 7 9 11 13 15 sentence length Figure 1. Exponential relationship between sentence length and the average number of derivations (on a log scale) for each reference sentence in our training corpus. (Koehn et al., 2003). Second, within this framework, we model the derivation, d, as a latent variable, p(e, d|f ), which is marginalised out in training and decoding. We show empirically that this treatment results in significant improvements over a maximum-derivation model. The paper is structured as follows. In Section 2 we list the challenges that discriminative SMT must face above and beyond the current systems. We situate our work, and previous work, on discriminative systems in this context. We present our model in Section 3, including our means of training and decoding. Section 4 reports our experimental s"
P08-1024,P06-1096,0,0.694261,"Missing"
P08-1024,W02-2018,0,0.0254207,"ur model are estimated from our training sample using a maximum a posteriori (MAP) estimator. This maximises the likelihood of the parallel training sentences, D = {(e, f )}, penalised using a prior, i.e., ΛM AP = arg maxΛ pΛ (D)p(Λ). We use a zero-mean Gaussian prior with the probability  density function p0 (λk ) ∝ exp −λ2k /2σ 2 .2 This results in the following log-likelihood objective and corresponding gradient: X X L= log pΛ (e|f ) + log p0 (λk ) (4) (e,f )∈D k λk ∂L = EpΛ (d|e,f ) [hk ] − EpΛ (e|f ) [hk ] − 2 ∂λk σ (5) In order to train the model, we maximise equation (4) using L-BFGS (Malouf, 2002; Sha and Pereira, 2003). This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using inside-outside inference over the feature forest defined by the SCFG parse chart of f yielding the partition function, ZΛ (f ), required for the log-likelihood, and the marginals, required for its derivatives. Efficiently calculating the objective and its gradient requires two separate"
P08-1024,P03-1021,0,0.184772,"x-inspired approaches. Progress within these approaches however has been less dramatic. We believe this is because these frequency count based1 models cannot easily incorporate non-independent and overlapping features, which are extremely useful in describing the translation process. Discriminative models of translation can include such features without making assumptions of independence or explicitly modelling their interdependence. However, while discriminative models promise much, they have not been shown to deliver significant gains 1 We class approaches using minimum error rate training (Och, 2003) frequency count based as these systems re-scale a handful of generative features estimated from frequency counts and do not support large sets of non-independent features. over their simpler cousins. We argue that this is due to a number of inherent problems that discriminative models for SMT must address, in particular the problems of spurious ambiguity and degenerate solutions. These occur when there are many ways to translate a source sentence to the same target sentence by applying a sequence of steps (a derivation) of either phrase translations or synchronous grammar rules, depending on"
P08-1024,N03-1028,0,0.00944121,"stimated from our training sample using a maximum a posteriori (MAP) estimator. This maximises the likelihood of the parallel training sentences, D = {(e, f )}, penalised using a prior, i.e., ΛM AP = arg maxΛ pΛ (D)p(Λ). We use a zero-mean Gaussian prior with the probability  density function p0 (λk ) ∝ exp −λ2k /2σ 2 .2 This results in the following log-likelihood objective and corresponding gradient: X X L= log pΛ (e|f ) + log p0 (λk ) (4) (e,f )∈D k λk ∂L = EpΛ (d|e,f ) [hk ] − EpΛ (e|f ) [hk ] − 2 ∂λk σ (5) In order to train the model, we maximise equation (4) using L-BFGS (Malouf, 2002; Sha and Pereira, 2003). This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using inside-outside inference over the feature forest defined by the SCFG parse chart of f yielding the partition function, ZΛ (f ), required for the log-likelihood, and the marginals, required for its derivatives. Efficiently calculating the objective and its gradient requires two separate packed charts, each repr"
P08-1024,D07-1077,0,0.0181604,"Missing"
P08-1024,D07-1080,0,0.724756,"system. Existing discriminative models require a reference derivation to optimise against, however no parallel corpora annotated for derivations exist. Ideally, a model would account for this ambiguity by marginalising out the derivations, thus predicting the best translation rather than the best derivation. However, doing so exactly is NP-complete. For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely (Ittycheriah and Roukos, 2007; Watanabe et al., 2007), or else ignore the problem and treat derivations as translations (Liang et al., 2006; Tillmann and Zhang, 2007). In this paper we directly address the problem of spurious ambiguity in discriminative models. We use a synchronous context free grammar (SCFG) translation system (Chiang, 2007), a model which has yielded state-of-the-art results on many translation tasks. We present two main contributions. First, we develop a log-linear model of translation which is globally trained on a significant number of parallel sentences. This model maximises the conditional likelihood of the data, p(e|f ),"
P08-1024,2006.amta-papers.28,0,0.0147855,"examples and typically many iterations of a solver during training. While current models focus solely on efficient decoding, discriminative models must also allow for efficient training. Past work on discriminative SMT only address some of these problems. To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007), or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006). These systems all include regularisation, thereby addressing Problem 2. An interesting counterpoint is the work of DeNero et al. (2006), who show that their unregularised model finds degenerate solutions. Some of these discriminative systems have been trained on large training sets (Problem 3); these systems are the local models, for which training is much simpler. Both the global models (Liang et al., 2006; Watanabe et al., 2007) use fairly small training sets, and there is no evidence that their techniques will scale to larger data sets. Our model addresses all three of the above problems"
P08-1024,J97-3002,0,0.142806,"Missing"
P09-1088,N06-2013,0,0.00554308,"), and thus produce far more valid phrases/rules. ● ● ● ● ● 476 ● 20 40 60 80 100 120 140 160 180 200 220 240 Number of Sampling Passes Figure 6: The posterior for the single CPU sampler and distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D 2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize B LEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent runs for each scenario. Both sets of runs performed hyperparameter inference every twenty"
P09-1088,N07-1018,0,0.0346801,"× 4.2 A Gibbs sampler for derivations Markov chain Monte Carlo sampling allows us to perform inference for the model described in 4.1 without restricting the infinite space of possible translation rules. To do this we need a method for sampling a derivation for a given sentence pair from p(d|d− ). One possible approach would be to first build a packed chart representation of the derivation forest, calculate the inside probabilities of all cells in this chart, and then sample derivations top-down according to their inside probabilities (analogous to monolingual parse tree sampling described in Johnson et al. (2007)). A problem with this approach is that building the derivation forest would take O(|f |3 |e|3 ) time, which would be impractical for long sentences. Instead we develop a collapsed Gibbs sampler (Teh et al., 2006) which draws new samples by making local changes to the derivations used in a previous sample. After a period of burn in, the derivations produced by the sampler will be drawn from the posterior distribution, p(d|x). The advantage of this algorithm is that we only store the current derivation for each training sentence pair (together these constitute the state of the sampler), but nev"
P09-1088,N03-1017,0,0.592202,"nous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchrono"
P09-1088,P07-2045,1,0.0237661,"iero grammars is built from every 50th sample after the burn-in, up until the 1500th sample. We evaluate the translation models using IBM B LEU (Papineni et al., 2001). Table 1 lists the statistics of the corpora used in these experiments. 4.5 Extracting a translation model Although we could use our model directly as a decoder to perform translation, its simple hierarchical reordering parameterisation is too weak to be effective in this mode. Instead we use our sampler to sample a distribution over translation models for state-of-the-art phrase based (Moses) and hierarchical (Hiero) decoders (Koehn et al., 2007; Chiang, 2007). Each sample from our model defines a hierarchical alignment on which we can apply the standard extraction heuristics of these models. By extracting from a sequence of samples we can directly infer a distribution over phrase tables or Hiero grammars. 5 Evaluation Our evaluation aims to determine whether the phrase/SCFG rule distributions created by sampling from the model described in Section 4 impact upon the performance of state-of-theart translation systems. We conduct experiments translating both Chinese (high reordering) and Arabic (low reordering) into English. We use the"
P09-1088,J93-2003,0,0.0378426,"lel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches. 1 Introduction The field of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003). Although very few researchers use word-based models for translation per se, such models are still widely used in the training of phrase-based models. These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced (either finite state (Koehn et al., 2003) or synchronous context free grammar (Chiang, 2007)). Although wide-spread, the disconnect between the translation model and the alignment model is artificial and c"
P09-1088,W08-0336,0,0.019233,", containing fewer spurious off-diagonal alignments, than the heuristic (see Figure 5), and thus produce far more valid phrases/rules. ● ● ● ● ● 476 ● 20 40 60 80 100 120 140 160 180 200 220 240 Number of Sampling Passes Figure 6: The posterior for the single CPU sampler and distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D 2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize B LEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent"
P09-1088,W02-1018,0,0.720719,".ed.ac.uk Trevor Cohn∗ tcohn@inf.ed.ac.uk Chris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted"
P09-1088,W07-0403,0,0.847523,"tcohn@inf.ed.ac.uk Chris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context f"
P09-1088,W06-1606,0,0.0297598,"siderably more efficient for long sentences. Following the broad shift in the field from finite state transducers to grammar transducers (Chiang, 2007), recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference (Wu, 1997; Related work Most machine translation systems adopt the approach of Koehn et al. (2003) for ‘training’ a phrase-based translation model.2 This method starts with a word-alignment, usually the latent state of an unsupervised word-based aligner such 2 We include grammar based transducers, such as Chiang (2007) and Marcu et al. (2006), in our definition of phrasebased models. 783 Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). However this asymptotic time complexity is of high enough order (O(|f |3 |e|3 )) that inference is impractical for real translation data. Proposed solutions to this problem include imposing sentence length limits, using small training corpora and constraining the search space using a word-alignment model or parse tree. None of these limitations are particularly desirable as they bias inference. As a result phrase-based alignment models are not yet practical for the wider machine tra"
P09-1088,J07-2003,0,0.855119,"eld of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003). Although very few researchers use word-based models for translation per se, such models are still widely used in the training of phrase-based models. These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced (either finite state (Koehn et al., 2003) or synchronous context free grammar (Chiang, 2007)). Although wide-spread, the disconnect between the translation model and the alignment model is artificial and clearly undesirable. Word-based models are incapable of learning translational equivalences between non-compositional phrasal units, while the algorithms used for inducing weighted transducers from word-alignments are based on heuristics with little theoretical justification. A 1 f and e are the input and output sentences respectively. 782 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 782–790, c Suntec, Singapore, 2-7 August 2009. 2009 ACL a"
P09-1088,P08-2007,0,0.0824084,"m aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihoo"
P09-1088,J03-1002,0,0.0161441,"that good practical parallel performance can be achieved by having multiple processors independently sample disjoint subsets of the corpus. Each process maintains a set of rule counts for the entire corpus and communicates the changes it has made to its section of the corpus only after sampling every sentence in that section. In this way each process is sampling according to a slightly ‘out-of-date’ distribution. However, as we confirm in Section 5 the performance of this approximation closely follows the exact collapsed Gibbs sampler. GIZA++ implementation of IBM Model 4 (Brown et al., 1993; Och and Ney, 2003) coupled with the phrase extraction heuristics of Koehn et al. (2003) and the SCFG rule extraction heuristics of Chiang (2007) as our benchmark. All the SCFG models employ a single X non-terminal, we leave experiments with multiple non-terminals to future work. Our hypothesis is that our grammar based induction of translation units should benefit language pairs with significant reordering more than those with less. While for mostly monotone translation pairs, such as Arabic-English, the benchmark GIZA++-based system is well suited due to its strong monotone bias (the sequential Markov model an"
P09-1088,W06-3105,0,0.305426,"er polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints fr"
P09-1088,P03-1021,0,0.0148685,"nd distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D 2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize B LEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent runs for each scenario. Both sets of runs performed hyperparameter inference every twenty passes through the data. It is clear from the training curves that the distributed approximation tracks the corpus probability of the correct sampler sufficiently closely. Th"
P09-1088,D08-1033,0,0.492948,"Missing"
P09-1088,2001.mtsummit-papers.68,0,0.0114687,"ard alignment points until the node factorises correctly. As the alignments contain many such non-factorisable nodes, these trees are of poor quality. However, all samplers used in these experiments are first ‘burnt-in’ for 1000 full passes through the data. This allows the sampler to diverge from its initialisation condition, and thus gives us confidence that subsequent samples will be drawn from the posterior. An expectation over phrase tables and Hiero grammars is built from every 50th sample after the burn-in, up until the 1500th sample. We evaluate the translation models using IBM B LEU (Papineni et al., 2001). Table 1 lists the statistics of the corpora used in these experiments. 4.5 Extracting a translation model Although we could use our model directly as a decoder to perform translation, its simple hierarchical reordering parameterisation is too weak to be effective in this mode. Instead we use our sampler to sample a distribution over translation models for state-of-the-art phrase based (Moses) and hierarchical (Hiero) decoders (Koehn et al., 2007; Chiang, 2007). Each sample from our model defines a hierarchical alignment on which we can apply the standard extraction heuristics of these models"
P09-1088,J97-3002,0,0.890151,", 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a nov"
P09-1088,N04-1035,0,0.0196258,"thout resorting to heuristic restrictions on the model. Initial experiments suggest that this model performs well on languages for which the monotone bias of existing alignment and heuristic phrase extraction approaches fail. These results open the way for the development of more sophisticated models employing grammars capable of capturing a wide range of translation phenomena. In future we envision it will be possible to use the techniques developed here to directly induce grammars which match state-of-the-art decoders, such as Hiero grammars or tree substitution grammars of the form used by Galley et al. (2004). (2007) who also observed very little empirical difference between the sampler and its distributed approximation. Tables 3 and 4 show the result on the two NIST corpora when running the distributed sampler on a single 8-core machine.5 These scores tally with our initial hypothesis: that the hierarchical structure of our model suits languages that exhibit less monotone reordering. Figure 5 shows the projected alignment of a headline from the thousandth sample on the NIST Chinese data set. The effect of the grammar based alignment can clearly be seen. Where the combination of GIZA++ and the heu"
P09-1088,C08-1136,0,0.571081,"ris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 19"
P09-1088,P07-1094,0,0.0150239,"pler to permute the internal structure of the trees more easily. ... ... ... ... Figure 4: Rule insert/delete sampler. A pair of adjacent nodes in a ternary rule can be re-parented as a binary rule, or vice-versa. 4.3 Hyperparameter Inference Our model is parameterised by a vector of hyperparameters, α = (αR , αN , αP , αPE , αPF , αnull ), which control the sparsity assumption over various model parameters. We could optimise each concentration parameter on the training corpus by hand, however this would be quite an onerous task. Instead we perform inference over the hyperparameters following Goldwater and Griffiths (2007) by defining a vague gamma prior on each concentration parameter, αx ∼ Gamma(10−4 , 104 ). This hyper-prior is relatively benign, allowing the model to consider a wide range of values for the hyperparameter. We sample a new value for each αx using a log-normal distribution with mean αx and variance 0.3, which is then accepted into the distribution p(αx |d, α− ) using the MetropolisHastings algorithm. Unlike the Gibbs updates, this calculation cannot be distributed over a cluster (see Section 4.4) and thus is very costly. Therefore for small corpora we re-sample the hyperparameter after every p"
P09-1088,P08-1012,0,0.818829,"ris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 19"
P09-1088,P06-1085,0,0.00708198,"with a geometric distribution in which a string of length k will be more probable than its segmentations. We define P1null as the string probability of the non-null part of the rule:  1 E null 2 P0 (e) if |f |= 0 P1 (z → he, f i) = 1 F 2 P0 (f ) if |e |= 0 The terminal translation phrase pair distribution is a hierarchical Dirichlet Process in which each phrase are independently distributed according to DPs:4 F P1P (z → he, f i) = φE z (e) × φz (f ) PE φE , P0E ) z ∼ DP(α 4 This prior is similar to one used by DeNero et al. (2008), who used the expected table count approximation presented in Goldwater et al. (2006). However, Goldwater et al. (2006) contains two major errors: omitting P0 , and using the truncated Taylor series expansion (Antoniak, 1974) which fails for small αP0 values common in these models. In this work we track table counts directly. 785 ... ... ... ... ... ... ... ... ... Figure 2: Split/Join sampler applied between a pair of adjacent terminals sharing the same parent. The dashed line indicates the source position being sampled, boxes indicate source and target tokens, while a solid line is a null alignment. ... ... ... ... the Split/Join operator in Figure 3. In order for this opera"
P09-1088,P02-1040,0,\N,Missing
P09-1088,2005.iwslt-1.1,0,\N,Missing
P11-1103,W05-0909,0,0.0286258,"quality of word order is poor (Birch et al., 2010). There are currently two main approaches to evaluating reordering. The first is exemplified by the B LEU score (Papineni et al., 2002), which counts the number of matching n-grams between the reference and the hypothesis. Word order is captured by the proportion of longer n-grams which match. This method does not consider the position of matching words, and only captures ordering differences if there is an exact match between the words in the translation and the reference. Another approach is taken by two other commonly used metrics, METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). They both search for an alignment between the translation and the reference, and from this they calculate a penalty based on the number of differences in order between the two sentences. When block moves are allowed the search space is very large, and matching stems and synonyms introduces errors. Importantly, none of these metrics capture the distance by which words are out of order. Also, they conflate reordering performance with the quality of the lexical items in the translation, making it difficult to tease apart the impact of changes. More sophisticated me"
P11-1103,D09-1030,0,0.0491438,"Missing"
P11-1103,N10-1080,0,0.035834,"to optimise, leading to the joint best scores at test time. This is an important result, as it shows that by training with the LRscore objective function, B LEU scores do not decrease, which is desirable as B LEU scores are usually reported in the field. The LRscore also results in better scores when evaluated with itself and the other two baseline metrics, TER and METEOR. Reordering and the lexical metrics are orthogonal information sources, and this shows that combining them results in better performing systems. B LEU has shown to be a strong baseline metric to use as an objective function (Cer et al., 2010), and so the LRscore performance in Table 5 is a good result. Examining the weights that result from the different MERT runs, the only notable difference is that the weight of the distortion cost is considerably lower with the LRscore. This shows more trust in the quality of reorderings. Although it is interesting to look at the model weights, any final conclusion on the impact of the metrics on training must depend on human evaluation of translation quality. Type Sentence Reference silicon valley is still a rich area in the united states. the average salary in the area was us $62,400 a year,"
P11-1103,N03-1017,0,0.0582288,"Missing"
P11-1103,2005.mtsummit-papers.11,0,0.00692566,"n the source and the reference sentences, and the source and the translated sentences. In an ideal scenario, the translation system outputs the alignments and the reference set can be selected to have gold standard human alignments. However, the data that we use to evaluate metrics does not have any gold standard alignments and we must train automatic alignment models to generate them. We used version two of the Berkeley alignment model (Liang et al., 2006), with the posterior threshold set at 0.5. Our Spanish-, French- and German-English alignment models are trained using Europarl version 5 (Koehn, 2005). The Czech-English alignment model is trained on sections 0-2 of the Czech-English Parallel Corpus, version 0.9 (Bojar and Zabokrtsky, 2009). The metric scores are calculated for the test set from the 2009 workshop on machine translation. It consists of 2525 sentences in English, French, German, Spanish and Czech. These sentences have been translated by different machine translation systems and the output submitted to the workshop. The system output along with human evaluations can be downloaded from the web1 . The B LEU score has five parameters, one for each n-gram, and one for the brevity"
P11-1103,W08-0312,0,0.0169787,"metric scores are calculated for the test set from the 2009 workshop on machine translation. It consists of 2525 sentences in English, French, German, Spanish and Czech. These sentences have been translated by different machine translation systems and the output submitted to the workshop. The system output along with human evaluations can be downloaded from the web1 . The B LEU score has five parameters, one for each n-gram, and one for the brevity penalty. These parameters are set to a default uniform value of one. METEOR has 3 parameters which have been trained for human judgements of rank (Lavie and Agarwal, 2008). METEOR version 0.7 was used. The other baseline metric used was TER version 0.7.25. We adapt TER by subtracting it from one, so that all 1 http://www.statmt.org/wmt09/results.html metric increases mean an improvement in the translation. The TER metric has five parameters which have not been trained. Using rank judgements, we do not have absolute scores and so we cannot compare translations across different sentences and extract correlation statistics. We therefore use the method adopted in the 2009 workshop on machine translation (Callison-Burch et al., 2009). We ascertained how consistent t"
P11-1103,N06-1014,0,0.0164325,"and out of English. In total there were 52,265 pairwise rank judgements collected. Our reordering metric relies upon word alignments that are generated between the source and the reference sentences, and the source and the translated sentences. In an ideal scenario, the translation system outputs the alignments and the reference set can be selected to have gold standard human alignments. However, the data that we use to evaluate metrics does not have any gold standard alignments and we must train automatic alignment models to generate them. We used version two of the Berkeley alignment model (Liang et al., 2006), with the posterior threshold set at 0.5. Our Spanish-, French- and German-English alignment models are trained using Europarl version 5 (Koehn, 2005). The Czech-English alignment model is trained on sections 0-2 of the Czech-English Parallel Corpus, version 0.9 (Bojar and Zabokrtsky, 2009). The metric scores are calculated for the test set from the 2009 workshop on machine translation. It consists of 2525 sentences in English, French, German, Spanish and Czech. These sentences have been translated by different machine translation systems and the output submitted to the workshop. The system o"
P11-1103,C04-1072,0,0.0283945,"measure of the precision of the word choice. The 4-gram B LEU score includes some measure of the local reordering success in the precision of the longer n-grams. B LEU is an important baseline, and improving on it by including more reordering information is an interesting result. The lexical component of the system can be any meaningful metric for a particular target language. If a researcher was interested in morphologically rich languages, for example, METEOR could be used. We use the LRscore to return sentence level scores as well system level scores, and when doing so the smoothed B LEU (Lin and Och, 2004) is used. 3 Consistency with Human Judgements Automatic metrics must be validated by comparing their scores with human judgements. We train the metric parameter to optimise consistency with human preference judgements across different language pairs and then we show that the LRscore is 1030 more consistent with humans than other commonly used metrics. 3.1 Experimental Design Human judgement of rank has been chosen as the official determinant of translation quality for the 2009 Workshop on Machine Translation (Callison-Burch et al., 2009). We used human ranking data from this workshop to evalua"
P11-1103,P03-1021,0,0.098362,"glish and English-Czech language pairs, which have the least amount of reordering. METEOR lags behind for the language pairs with the most reordering, the German-English and English-German pairs. Here LR-KB4 is the best metric, which shows that metrics which are sensitive to the distance words are out of order are more appropriate for situations with a reasonable amount of reordering. 4 Optimising Translation Models Automatic metrics are useful for evaluation, but they are essential for training model parameters. In this section we apply the LRscore as the objective function in MERT training (Och, 2003). MERT minimises translation errors according to some automatic evaluation metric while searching for the best parameter settings over the N-best output. A MERT trained model is likely to exhibit the properties that Metric METEOR TER B LEU1 B LEU LR-HB1 LR-HB4 LR-KB1 LR-KB4 de-en 58.6 53.2 56.1 58.7 59.7 60.4 60.4 61.0 es-en 58.3 50.1 57.0 55.5 60.0 57.3 59.7 57.2 fr-en 58.3 52.6 56.7 57.7 58.6 58.7 58.0 58.5 cz-en 59.4 47.5 52.5 57.2 53.2 57.2 54.0 58.6 en-de 52.6 48.6 52.1 54.1 54.6 54.8 54.1 54.8 en-es 55.7 49.6 54.2 56.7 55.6 57.3 54.7 56.8 en-fr 61.2 58.3 62.3 63.7 63.7 63.3 63.4 63.1 en-"
P11-1103,P02-1040,0,0.108428,"aining with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of B LEU scores. 1 Introduction Research in machine translation has focused broadly on two main goals, improving word choice and improving word order in translation output. Current machine translation metrics rely upon indirect methods for measuring the quality of the word order, and their ability to capture the quality of word order is poor (Birch et al., 2010). There are currently two main approaches to evaluating reordering. The first is exemplified by the B LEU score (Papineni et al., 2002), which counts the number of matching n-grams between the reference and the hypothesis. Word order is captured by the proportion of longer n-grams which match. This method does not consider the position of matching words, and only captures ordering differences if there is an exact match between the words in the translation and the reference. Another approach is taken by two other commonly used metrics, METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). They both search for an alignment between the translation and the reference, and from this they calculate a penalty based on the"
P11-1103,2006.amta-papers.25,0,0.108377,"ch et al., 2010). There are currently two main approaches to evaluating reordering. The first is exemplified by the B LEU score (Papineni et al., 2002), which counts the number of matching n-grams between the reference and the hypothesis. Word order is captured by the proportion of longer n-grams which match. This method does not consider the position of matching words, and only captures ordering differences if there is an exact match between the words in the translation and the reference. Another approach is taken by two other commonly used metrics, METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). They both search for an alignment between the translation and the reference, and from this they calculate a penalty based on the number of differences in order between the two sentences. When block moves are allowed the search space is very large, and matching stems and synonyms introduces errors. Importantly, none of these metrics capture the distance by which words are out of order. Also, they conflate reordering performance with the quality of the lexical items in the translation, making it difficult to tease apart the impact of changes. More sophisticated metrics, such as the RTE metric"
P11-1103,D08-1027,0,0.0272483,"Missing"
P11-1103,W09-0401,0,\N,Missing
P13-2132,W11-2125,0,0.0330772,"Missing"
P13-2132,P05-1077,0,0.014595,"Missing"
P13-2132,W11-2210,0,0.0449389,"Missing"
P14-2112,P11-2004,1,0.885442,"Missing"
P14-2112,P97-1048,0,0.172164,"stream length) for each item in the stream. We address two problems: language changes over time, and the observation that space is a problem, even for compact sketches. Statistical language models often assume either a local Markov property (when working with utterances, or sentences), or that content is generated fully i.i.d. (such as in document-level topic models). However, language shows observable priming effects, sometimes called triggers, where the occurrence of a given term decreases the surprisal of some other term later in the same discourse (Lau et al., 1993; Church and Gale, 1995; Beeferman et al., 1997; Church, 2000). Conventional cache and trigger models typically do not deal with new terms and can be seen as adjusting the parameters of a fixed model. Accounting for previously unseen entries in a language model can be naively simple: as they appear in new training data, add them to the model! However in practice we are constrained by available space: how many unique phrases can we store, given the target application environment? Our work is concerned with modeling language that might change over time, in accordance with current trending discourse topics, but under a strict space constraint"
P14-2112,D12-1005,1,0.87926,"Missing"
P14-2112,C00-1027,0,0.0293647,"item in the stream. We address two problems: language changes over time, and the observation that space is a problem, even for compact sketches. Statistical language models often assume either a local Markov property (when working with utterances, or sentences), or that content is generated fully i.i.d. (such as in document-level topic models). However, language shows observable priming effects, sometimes called triggers, where the occurrence of a given term decreases the surprisal of some other term later in the same discourse (Lau et al., 1993; Church and Gale, 1995; Beeferman et al., 1997; Church, 2000). Conventional cache and trigger models typically do not deal with new terms and can be seen as adjusting the parameters of a fixed model. Accounting for previously unseen entries in a language model can be naively simple: as they appear in new training data, add them to the model! However in practice we are constrained by available space: how many unique phrases can we store, given the target application environment? Our work is concerned with modeling language that might change over time, in accordance with current trending discourse topics, but under a strict space constraint. With a fixed"
P14-2112,N09-1058,0,0.054755,"Missing"
P14-2112,P11-1038,0,0.0229182,".5 304.4 Table 5: Perplexities for differently selected samples over Gigaword (sample size = 10 blocks, β = 1.1). Lower is better. Table 3: Perplexities for different sample sizes over Twitter. Lower is better. 4.4 Static 416.5 436.7 461.8 315.6 319.1 462.5 5 Conclusion We have introduced exponential reservoir sampling as an elegant way to model a stream of unbounded size, yet using fixed space. It naturally allows one to take account of recency effects present in many natural streams. We expect that our language model could improve other Social Media tasks, for example lexical normalisation (Han and Baldwin, 2011) or even event detection (Lin et al., 2011). The approach is fully general and not just limited to language modelling. Future work should look at other distributions for sampling and consider tasks such as machine translation over Social Media. GigaWord Twitter is a fast moving, rapidly changing multilingual stream and it is not surprising that our exponential reservoir sampling proves beneficial. Is it still useful for a more conventional stream that is drawn from a much smaller population of reporters? We repeated our experiments, using the same rolling training and testing evaluation as bef"
P14-2112,W11-2123,0,0.0724046,".9 614.3 615.0 620.0 647.2 650.1 633.0 636.5 630.4 634.5 608.4 610.8 603.3 604.4 2.0 619.4 611.1 612.1 621.6 628.1 658.0 644.6 641.6 618.4 610.0 Table 2: Perplexities for different β values over Twitter (sample size = five days). Lower is better. We test the model on unseen data from all of the next day (or block). Afterwards, we advance to the next day (block) and repeat, potentially incorporating the previously seen test data into the current training data. Evaluation is in terms of perplexity (which is standard for language modelling). We used KenLM for building models and evaluating them (Heafield, 2011). Each model was an unpruned trigram, with Kneser-Ney smoothing. Increasing the language model order would not change the results. Here the focus is upon which data is used in a model (that is, which data is added and which data is removed) and not upon making it compact or making retraining efficient. • Static. This model was trained using data from the start of the duration and never varied. It is a baseline. • Exact. This model was trained using all available data from the start of the stream and acts as an upper bound on performance. • Moving Window. This model used all data in a fixed-siz"
P14-2112,D09-1079,1,0.679706,"uable, but there is also signal in the previous stream. Introduction Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al. (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches.1 While efficient, these structures still scale linearly in the number of items stored, and do not handle deletions well: if processing an unbounded stream of text, with new words and phrases being regularly added to the model, then with a fixed amount of space, errors will only increase over time. This was pointed out by Levenberg and Osborne (2009), who investigated an alternate approach employing perfecthashing to allow for deletions over time. Their deletion criterion was task-specific and based on how a machine translation system queried a language model. Our sampling methods are based on reservoir sampling (Vitter, 1985), a popularly known method in some areas of computer science, but which has seen little use within computational linguistics.2 Standard reservoir sampling is a method for maintaining a uniform sample over a dynamic stream of elements, using constant space. Novel to this community, we consider a variant owing to Aggar"
P14-2112,P07-1065,1,0.786246,"red by this community until now. Using language models over Twitter and Newswire as a testbed, our experimental results based on perplexity support the intuition that recently observed data generally outweighs that seen in the past, but that at times, the past can have valuable signals enabling better modelling of the present. 1 We show experimentally that a moving window is better than uniform sampling, and further that exponential (biased) sampling is best of all. For streaming data, recently encountered data is valuable, but there is also signal in the previous stream. Introduction Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al. (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches.1 While efficient, these structures still scale linearly in the number of items stored, and do not handle deletions well: if processing an unbounded stream of text, with new words and phrases being regularly added to the model, then with a fixed amount of space, errors will only increase over time. This was pointed out by Levenberg and Osborne (2009), who investigated an alternate approach employing perfecthashing to allow"
P14-5007,I13-1013,1,0.713754,"Missing"
P14-5007,N10-1021,1,0.615233,"Missing"
P14-5007,P12-3005,0,\N,Missing
P14-5007,D13-1100,0,\N,Missing
P15-1170,P05-1077,0,0.121752,"proximated Tracking Our first attempt to reach sub-linear execution time uses random segmentation of the vector space using hashing techniques. We frame the tracking process as a nearest neighbour search problem, as defined by Gionis et al. (1999). Documents arriving from a stream are seen as queries and the closest topics are the nearest neighbours to be identified. We explore locality sensitive hashing (LSH), as described by Indyk et al. (1998), to approach high dimensional nearest neighbour search for topic tracking in sub-linear time. LSH, which has been used to speed up NLP applications (Ravichandran et al., 2005), provides hash functions that guarantee that similar documents are more likely to be hashed to the same binary hash key than distant ones. Hash functions capture similarities between vectors in high dimensions and represent them on a low dimensional binary level. We apply the scheme by Charikar (2002), which describes the probabilistic bounds for the cosine similarity between two vectors. Each bit in a hash key represents a documents position with respect to a randomly placed hyperplane. Those planes segment the vector space, forming high dimensional polygon shaped buckets. Documents and topi"
P15-1170,N10-1021,1,0.886686,"t stream. We introduce two tracking approaches which are fully applicable to true streaming environments. When tracking 4.4 million topics against 52 million documents in constant time and space, we demonstrate that counter to expectations, simple single-pass clustering can outperform locality sensitive hashing for nearest neighbour search on streams. 1 Miles Osborne Bloomberg London mosborne29 @bloomberg.net Introduction The emergence of massive social media streams has sparked a growing need for systems able to process them. While previous research (Hassan et al., 2009; Becker et al., 2009; Petrovic et al., 2010; Cataldi et al., (2010); Weng et al., (2011); Petrovic 2013) has focused on detecting new topics in unbounded textual streams, less attention was paid to following (tracking) the steadily growing set of topics. Standard topic tracking (Allan, 2002) deals with helping human analysts follow and monitor ongoing events on massive data streams. By pairing topics with relevant documents, topic tracking splits a noisy stream of documents into sub-streams grouped by their target topics. This is a crucial task for financial and security analysts who are interested in pulling together relevant informat"
W00-0709,J97-4005,0,0.139885,"ic elements within the features. The resulting models are more general than the original models, and contain fewer parameters. Empirical results from the task of parse selection suggest that the improvement in performance over repeated iterations of iterative scaling is more reliable with such generalized models than with ungeneralized models. 1 Introduction The maximum entropy technique of statistical modeling using random fields has proved to be an effective way of dealing with a variety of linguistic phenomena, in particular where modeling of attribute-valued grammars (AVG&apos;s) is concerned (Abney, 1997). This is largely because its capacity for considering overlapping information sources allows the most to be made of situations where data is sparse. Nevertheless, it is important that the statistical features employed be appropriate to the job. If the information contributed by the features is too specific to the training data, overfitting becomes a problem (Chen and Rosenfeld, 1999; Osborne, 2000). In this event, a peak in model performance will be reached early on, and continued training yields progressive deterioration in performance. From a theoretical standpoint, overfitting indicates th"
W00-0709,P98-1022,0,0.0706264,"Missing"
W00-0709,A97-1052,0,0.0558763,". Even if the events are not noisy or inaccurate in actual fact, they may still contribute to overfitting if their features occur too infrequently in the data to give accurate frequencies. The merging procedure seeks to address overfitting at the level of the features themselves and remain true to the spirit of the maximum entropy approach, which seeks to represent what is unknown about the Experiments The experiments described here were conducted using the Wall Street Journal Penn Treebank corpus (Marcus et al., 1993). The grammar used was a manually written broad coverage DCG style grammar (Briscoe and Carroll, 1997). Parses of WSJ sentences produced by the grammar were ranked empirically using the treebank parse as a gold standard according to a weighted linear combination of crossing brackets, precision, and recall. If more than fifty parses were produced for a sentence, the 51 best fifty were used and the rest discarded. For the training data, the empirical rankings of all parses for each sentence were normalized so the total parse scores for each sentence added to a constant. The events of the training data consisted of parses and their corresponding normalized score. These scores were furthermore tre"
W00-0709,P96-1025,0,0.034637,"e features is too specific to the training data, overfitting becomes a problem (Chen and Rosenfeld, 1999; Osborne, 2000). In this event, a peak in model performance will be reached early on, and continued training yields progressive deterioration in performance. From a theoretical standpoint, overfitting indicates that the model distribution is unrepresentative of the actual probabilities. In practice, it makes the performance of the model dependent upon early stopping of training. The 49 2 Maximum selection entropy-based parse cal parsers such as those of Charniak (1997), Magerman (1995) and Collins (1996). In the present approach, each feature in the feature set corresponds to a depth-one tree structure in the data, i.e. a mother node and all of its daughters. Within this general structure various schemata may be used to derive actual features, where the information about each node employed in the feature is determined by which schema is used. For example, one schema might call for POS information from all nodes and lexical information only from head nodes. Another might call for lexical information only from nodes which also contain the POS tag for prepositions. The term compositional is used"
W00-0709,P95-1037,0,0.0481061,"on contributed by the features is too specific to the training data, overfitting becomes a problem (Chen and Rosenfeld, 1999; Osborne, 2000). In this event, a peak in model performance will be reached early on, and continued training yields progressive deterioration in performance. From a theoretical standpoint, overfitting indicates that the model distribution is unrepresentative of the actual probabilities. In practice, it makes the performance of the model dependent upon early stopping of training. The 49 2 Maximum selection entropy-based parse cal parsers such as those of Charniak (1997), Magerman (1995) and Collins (1996). In the present approach, each feature in the feature set corresponds to a depth-one tree structure in the data, i.e. a mother node and all of its daughters. Within this general structure various schemata may be used to derive actual features, where the information about each node employed in the feature is determined by which schema is used. For example, one schema might call for POS information from all nodes and lexical information only from head nodes. Another might call for lexical information only from nodes which also contain the POS tag for prepositions. The term co"
W00-0709,J93-2004,0,0.0361044,"to produce more general features which occur more often, resulting in fewer total features used. Even if the events are not noisy or inaccurate in actual fact, they may still contribute to overfitting if their features occur too infrequently in the data to give accurate frequencies. The merging procedure seeks to address overfitting at the level of the features themselves and remain true to the spirit of the maximum entropy approach, which seeks to represent what is unknown about the Experiments The experiments described here were conducted using the Wall Street Journal Penn Treebank corpus (Marcus et al., 1993). The grammar used was a manually written broad coverage DCG style grammar (Briscoe and Carroll, 1997). Parses of WSJ sentences produced by the grammar were ranked empirically using the treebank parse as a gold standard according to a weighted linear combination of crossing brackets, precision, and recall. If more than fifty parses were produced for a sentence, the 51 best fifty were used and the rest discarded. For the training data, the empirical rankings of all parses for each sentence were normalized so the total parse scores for each sentence added to a constant. The events of the trainin"
W00-0709,C00-1085,1,0.91603,"tical modeling using random fields has proved to be an effective way of dealing with a variety of linguistic phenomena, in particular where modeling of attribute-valued grammars (AVG&apos;s) is concerned (Abney, 1997). This is largely because its capacity for considering overlapping information sources allows the most to be made of situations where data is sparse. Nevertheless, it is important that the statistical features employed be appropriate to the job. If the information contributed by the features is too specific to the training data, overfitting becomes a problem (Chen and Rosenfeld, 1999; Osborne, 2000). In this event, a peak in model performance will be reached early on, and continued training yields progressive deterioration in performance. From a theoretical standpoint, overfitting indicates that the model distribution is unrepresentative of the actual probabilities. In practice, it makes the performance of the model dependent upon early stopping of training. The 49 2 Maximum selection entropy-based parse cal parsers such as those of Charniak (1997), Magerman (1995) and Collins (1996). In the present approach, each feature in the feature set corresponds to a depth-one tree structure in th"
W00-0709,C98-1022,0,\N,Missing
W00-0731,P98-1034,0,0.03068,"Missing"
W00-0731,W95-0107,0,0.115469,"Missing"
W00-0731,W99-0629,0,0.0474404,"Missing"
W00-0731,C98-1034,0,\N,Missing
W01-0712,A00-2018,0,\N,Missing
W01-0712,J93-2004,0,\N,Missing
W01-0712,A00-2007,1,\N,Missing
W01-0712,W00-0729,1,\N,Missing
W01-0712,C00-1085,1,\N,Missing
W01-0712,C00-1034,1,\N,Missing
W01-0712,W00-0702,1,\N,Missing
W01-0712,A88-1019,0,\N,Missing
W01-0712,W00-0733,1,\N,Missing
W01-0712,J96-1002,0,\N,Missing
W01-0712,W00-0726,1,\N,Missing
W01-0712,W00-0731,1,\N,Missing
W01-0712,W01-0702,1,\N,Missing
W01-0712,P98-1081,0,\N,Missing
W01-0712,C98-1078,0,\N,Missing
W01-0712,W00-0727,1,\N,Missing
W01-0712,C00-2124,1,\N,Missing
W01-0712,W99-0708,1,\N,Missing
W01-0712,W00-0730,0,\N,Missing
W02-0401,J96-1002,0,0.0347924,"Missing"
W02-0401,C00-1012,0,0.0419157,"Missing"
W02-0401,W97-0710,0,0.0324493,"Missing"
W02-0401,A00-2018,0,\N,Missing
W02-2008,rose-etal-2002-reuters,0,\N,Missing
W02-2008,P97-1048,0,\N,Missing
W02-2008,P02-1030,1,\N,Missing
W02-2008,W02-1030,0,\N,Missing
W02-2008,P01-1005,0,\N,Missing
W03-0310,H94-1028,0,0.0728098,"Missing"
W03-0310,J93-2003,0,0.00831638,"Missing"
W03-0310,J93-2004,0,0.0296624,"Missing"
W03-0310,P00-1056,0,0.0237332,"ng source strings in six languages a greater reduction in word error rate was achieved. Our work is similar in spirit, although instead of using multi-source translation at the time of translation, we integrate it into the training stage. Whereas Och and Ney use multiple source strings to improve the quality of one translation only, our co-training method attempts to improve the accuracy of all translation models by bootstrapping more training data from multiple source documents. 3.1 Software The software that we used to train the statistical models and to produce the translations was GIZA++ (Och and Ney, 2000), the CMU-Cambridge Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), and the ISI ReWrite Decoder. The sizes of the language models used in each experiment were fixed throughout, in order to ensure that any gains that were made were not due to the trivial reason of the language model improving (which could be done by building a larger monolingual corpus of the target language). The experiments that we conducted used GIZA++ to produce IBM Model 4 translation models. It should be observed, however, that our co-training algorithm is entirely general and may be applied to any formulation o"
W03-0310,2001.mtsummit-papers.46,0,0.0882293,"through cotraining with related languages, the translation models for distant languages will better learn word order mappings to the target language. In all these cases the diversity afforded by multiple translation models increases the chances that the machine translated sentences added to the initial bilingual corpora will be accurate. Our co-training algorithm allows many source languages to be used. 3 Experimental Results In order to conduct co-training experiments we first needed to assemble appropriate corpora. The corpus used in our experiments was assembled from the data used in the (Och and Ney, 2001) multiple source translation paper. The data was gathered from the Bulletin of the European Union which is published on the Internet in the eleven official languages of the European Union. We used a subset of the data to create a multi-lingual corpus, aligning sentences between French, Spanish, German, Italian and Portuguese (Simard, 1999). Additionally we created bilingual corpora between English and each of the five languages using sentences that were not included in the multi-lingual corpus. Och and Ney (2001) used the data to find a translation that was most probable given multiple source"
W03-0310,W99-0604,0,0.0602328,"g data for a Spanish translation model contained the masculine form of a adjective, but not the feminine. Because languages vary in how they use morphology (some languages have grammatical gender whereas others don’t) one language’s translation model might have the translation of a particular word form whereas another’s would not. Thus co-training can increase the inventory of word forms and reduce the problem that morphology poses to simple statistical translation models. • improved word order – A significant source of errors in statistical machine translation is the word reordering problem (Och et al., 1999). The word order between related languages is often similar while word order between distant language may differ significantly. By including more examples through cotraining with related languages, the translation models for distant languages will better learn word order mappings to the target language. In all these cases the diversity afforded by multiple translation models increases the chances that the machine translated sentences added to the initial bilingual corpora will be accurate. Our co-training algorithm allows many source languages to be used. 3 Experimental Results In order to con"
W03-0310,2001.mtsummit-papers.68,0,0.0175181,"28.5 28 27.5 corpora for its training data. Coaching of German The performance of translation models was evaluated using a held-out set of 1,000 sentences in each language, with reference translations into English. Each translation model was used to produce translation of these sentences and the machine translations were compared to the reference human translations using word error rate (WER). The results are reported in terms of increasing accuracy, rather than decreasing error. We define accuracy as 100 minus WER. Other evaluation metrics such as position independent WER or the Bleu method (Papineni et al., 2001) could have been used. While WER may not be the best measure of translation quality, it is sufficient to track performance improvements in the following experiments. 3.3 27 10000 Evaluation 20000 25000 30000 Training Corpus Size (number of sentence pairs) 35000 40000 45.2 45 Co-training Table 1 gives the result of co-training using the most accurate translation from the candidate translations produced by five translation models. Each translation model was initially trained on bilingual corpora consisting of around 20,000 human translated sentences. These translation models were used to transla"
W03-0310,W01-0501,0,0.0422181,"Missing"
W03-0310,resnik-1998-parallel,0,0.0767729,"Missing"
W03-0310,W02-1013,0,0.0244848,"Missing"
W03-0310,P02-1040,0,\N,Missing
W03-0310,P02-1046,0,\N,Missing
W03-0403,C00-1085,1,\N,Missing
W03-0403,P97-1003,0,\N,Missing
W03-0403,C02-2025,0,\N,Missing
W03-0403,W00-1306,0,\N,Missing
W03-0403,W02-2030,0,\N,Missing
W03-0403,P02-1034,0,\N,Missing
W03-0403,P96-1042,0,\N,Missing
W03-0403,P02-1046,0,\N,Missing
W03-0403,P99-1069,0,\N,Missing
W03-0403,W02-2018,0,\N,Missing
W03-0407,A00-1031,0,0.703418,"data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model T N T tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the T N T tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labell"
W03-0407,W99-0613,0,0.226777,"ment-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy"
W03-0407,W02-2018,0,0.0219854,"d to define the conditional probabilities of a tag given some context. The advantage of ME models over the Markov model used by T N T is that arbitrary features can easily be included in the context; so as well as considering the target word and the previous two tags (which is the information T N T uses), the ME models also consider the words either side of the target word and, for unknown and infrequent words, various properties of the string of the target word. A disadvantage is that the training times for ME models are usually relatively slow, especially with iterative scaling methods (see Malouf (2002) for alternative methods). Here we use Generalised Iterative Scaling (Darroch and Ratcliff, 1972), but our implementation is much faster than Ratnaparkhi’s publicly available tagger. The C&C tagger trains in less than 7 minutes on the 1 million words of the Penn Treebank, and tags slightly faster than T N T. Since the taggers share many common features, one might think they are not different enough for effective co-training to be possible. In fact, both taggers are sufficiently different for co-training to be effective. Section 4 shows that both taggers can benefit significantly from the infor"
W03-0407,J94-2001,0,0.329951,"Missing"
W03-0407,W01-0501,0,0.305384,"gging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS tagge"
W03-0407,N01-1023,0,0.328731,"results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We i"
W03-0407,E03-1008,1,0.830279,"that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-tr"
W03-0407,W02-2006,0,0.0259342,"a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model T N T tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the T N T tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data should be accurate, but also useful, providing the tagger with new information. Our work differs from the Blum and Mitchell (1998)"
W03-0407,P95-1026,0,0.0370871,"ing literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly la"
W03-0407,E03-1071,1,0.88395,"ly used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model T N T tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the T N T tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data"
W03-0407,zavrel-daelemans-2000-bootstrapping,0,0.10034,"example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model T N T tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the T N T tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data should be accurate, but also useful, providing the tagger with new information. Our work differs f"
W03-0407,A94-1009,0,0.124495,"Missing"
W03-0407,P02-1046,0,\N,Missing
W04-3202,W00-1306,0,0.436543,"to be extended with a new labeled example {hxi , y i i}. The information gain for some model is maximized after selecting, labeling, and adding a new example xi to Dn such that the noise level of xi is low and both the bias and variance of some model using Dn ∪ {hxi , y i i} are minimized (Cohn et al., 1995). In practice, selecting data points for labeling such that a model’s variance and/or bias is maximally minimized is computationally intractable, so approximations are typically used instead. One such approximation is uncertainty sampling. Uncertainty sampling (also called tree entropy by Hwa (2000)), measures the uncertainty of a model over the set of parses of a given sentence, based on the conditional 1 This eyeball step is not always taken, but Redwoods does not contain information about when this occurred, so we apply the cost for the step uniformly for all examples. distribution it assigns to them. Following Hwa, we use the following measure to quantify uncertainty: AL results are usually presented in terms of the amount of labeling necessary to achieve given performance levels. We say that one method is betX ter than another method if, for a given performance fus (s, τ, Mk ) = − P"
W04-3202,W01-0710,0,0.0351381,"ling. For most situations, n-best automation is beneficial: the gap introduced by reuse can be reduced. nbest automation never results in an increase in cost. This is still true even if we do not allow ourselves to reuse those discriminants which were used to select the best parse from the n-best subset and the best parse was not actually present in that subset. 9 Related work There is a large body of AL work in the machine learning literature, but less so within natural language processing (NLP). Most work in NLP has primarily focused upon uncertainty sampling (Hwa, 2000; Tang et al., 2002). Hwa (2001) considered reuse of examples selected for one parser by another with uncertainty sampling. This performed better than sequential sampling but was only half as effective as self-selection. Here, we have considered reuse with respect to many models and their co-relatedness. Also, we compare reuse performance against against random sampling, which we showed previously to be a much stronger baseline than sequential sampling for the Redwoods corpus (Osborne and Baldridge, 2004). Hwa et al. (2003) showed that for parsers, AL outperforms the closely related co-training, and that some of the labeling"
W04-3202,P99-1069,0,0.00940286,"notator, annotators make use of discriminants which disambiguate the parse forest more rapidly, as described in section 3. In this paper, we report results using the third growth of Redwoods, which contains English sentences from appointment scheduling and travel planning domains of Verbmobil. In all, there are 5302 sentences for which there are at least two parses and a unique preferred parse is identified. These sentences have 9.3 words and 58.0 parses on average. 2.2 Modeling parse selection As is now standard for feature-based grammars, we mainly use log-linear models for parse selection (Johnson et al., 1999). For log-linear models, the conditional probability of an analysis ti given a sentence with a set of analyses τ = {t . . .} is given as: Pm P (ti |s, Mk ) = exp( j=1 fj (ti )wj ) Z(s) (1) where fj (ti ) returns the number of times feature j occurs in analysis t, wj is a weight from model Mk , and Z(s) is a normalization factor for the sentence. The parse with the highest probability is taken as the preferred parse for the model. We use the limited memory variable metric algorithm to determine the weights. We do not regularize our loglinear models since labeled data -necessary to set hyperpara"
W04-3202,C02-2025,0,0.0209056,"Missing"
W04-3202,N04-1012,1,0.73703,"ng the later model to improve in quality, or else reusing the labeled training material using a different machine learning algorithm) performance of later models can be significantly undermined when training upon material created using AL. The key to knowing how well one model will be able to use material selected by another is their relatedness – yet there may be no means to determine this prior to annotation, leading to a chicken-and-egg problem. Our reusability results thus demonstrate that, additionally, other strategies must be adopted to ensure we reduce the total cost of annotation. In Osborne and Baldridge (2004), we showed that ensemble models can increase model performance and also produce annotation savings when incorporated into the AL process. An obvious next step is automating some decisions. Here, we consider a simple automation strategy that reduces annotation costs independently of AL and examine its effect on reusability. We find that using both semi-automation and AL with high-quality models can eliminate the performance gap found in many reuse scenarios. However, for weak models, we show that semi-automation with random sampling is more effective for improving reusability than using it wit"
W04-3202,P02-1016,0,0.282827,"ing with random sampling. For most situations, n-best automation is beneficial: the gap introduced by reuse can be reduced. nbest automation never results in an increase in cost. This is still true even if we do not allow ourselves to reuse those discriminants which were used to select the best parse from the n-best subset and the best parse was not actually present in that subset. 9 Related work There is a large body of AL work in the machine learning literature, but less so within natural language processing (NLP). Most work in NLP has primarily focused upon uncertainty sampling (Hwa, 2000; Tang et al., 2002). Hwa (2001) considered reuse of examples selected for one parser by another with uncertainty sampling. This performed better than sequential sampling but was only half as effective as self-selection. Here, we have considered reuse with respect to many models and their co-relatedness. Also, we compare reuse performance against against random sampling, which we showed previously to be a much stronger baseline than sequential sampling for the Redwoods corpus (Osborne and Baldridge, 2004). Hwa et al. (2003) showed that for parsers, AL outperforms the closely related co-training, and that some of"
W06-2918,W03-0424,0,0.0505676,"rst names (1 219 entries), names of places (27 635 entries), names of companies (20 638 and 279 195 entries) and names of other organisations (425 entries). 4.3 Model Feature set Our experiments are centred around two CRF models, one with and one without gazetteer features. The model without gazetteer features, which we call standard, comprises features defined in a window of five words around the current word. These include features encoding n-grams of words and POS tags, and features encoding orthographic properties of the current word. The orthographic features are based on those found in (Curran and Clark, 2003). Examples include whether the current word is capitalised, is an initial, contains a digit, contains punctuation, etc. In total there are 450 345 features in the standard model. We call the second model, with gazetteer features, standard+g. This includes all the features contained in the standard model as well as 8 329 gazetteer features. Our gazetteer features are a typical way to represent gazetteer information in maxent-style models. They are divided into two categories: unlexicalised and lexicalised. The unlexicalised features model the dependency between a word’s presence in a gazetteer"
W06-2918,M98-1015,0,0.0492993,"in a variety of information extraction systems, including both rulebased systems and statistical models. In addition to lists of people names, locations, etc., recent work in the biomedical domain has utilised gazetteers of biological and genetic entities such as gene names (Finkel et al., 2005; McDonald and Pereira, 2005). In general gazetteers are thought to provide a useful source of external knowledge that is helpful when an entity cannot be identified from knowledge contained solely within the data set used for training. However, some research has questioned the usefulness of gazetteers (Krupka and Hausman, 1998). Other work has supported the use of gazetteers in general but has found that lists of only moderate size are sufficient to provide most of the benefit (Mikheev et al., 1999). Therefore, to date the effective use of gazetteers for information extraction has in general been regarded as a “black art”. In this paper we explain some of the likely reasons for these findings, and propose ways to more effectively handle gazetteers when they are used by maxent-style models. In work developed independently and in parallel to the work presented here, Sutton et al. (2006) identify general problems with"
W06-2918,W03-0430,0,0.0636837,"y informative, and their inclusion in the model should in principle result in higher model accuracy. However, these features can also introduce hidden negative effects taking the form of labelling errors that the model makes at places where a model without the gazetteer features would have labelled correctly. Consequently, ensuring optimal usage of gazetteers can be difficult. 1 Introduction In recent years discriminative probabilistic models have been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003) and information extraction from research papers (Peng and McCallum, 2004). Discriminative models offer a significant advantage In this paper we describe and analyse the labelling errors made by a model, and show that they generally result from the model’s over-dependence on the gazetteer features for making labelling decisions. By including gazetteer features in the model we may, in some cases, transfer too much explanatory dependency to the gazetteer features from the nongazetteer features. In order to avoid this problem, a more careful treatment"
W06-2918,E99-1001,0,0.11712,"Missing"
W06-2918,N04-1042,0,0.0438605,"atures can also introduce hidden negative effects taking the form of labelling errors that the model makes at places where a model without the gazetteer features would have labelled correctly. Consequently, ensuring optimal usage of gazetteers can be difficult. 1 Introduction In recent years discriminative probabilistic models have been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003) and information extraction from research papers (Peng and McCallum, 2004). Discriminative models offer a significant advantage In this paper we describe and analyse the labelling errors made by a model, and show that they generally result from the model’s over-dependence on the gazetteer features for making labelling decisions. By including gazetteer features in the model we may, in some cases, transfer too much explanatory dependency to the gazetteer features from the nongazetteer features. In order to avoid this problem, a more careful treatment of these features is required during training. We demonstrate that a traditional regularisation approach, where differe"
W06-2918,N03-1028,0,0.65182,"l should in principle result in higher model accuracy. However, these features can also introduce hidden negative effects taking the form of labelling errors that the model makes at places where a model without the gazetteer features would have labelled correctly. Consequently, ensuring optimal usage of gazetteers can be difficult. 1 Introduction In recent years discriminative probabilistic models have been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003) and information extraction from research papers (Peng and McCallum, 2004). Discriminative models offer a significant advantage In this paper we describe and analyse the labelling errors made by a model, and show that they generally result from the model’s over-dependence on the gazetteer features for making labelling decisions. By including gazetteer features in the model we may, in some cases, transfer too much explanatory dependency to the gazetteer features from the nongazetteer features. In order to avoid this problem, a more careful treatment of these features is required during training"
W06-2918,P05-1003,1,0.919166,"to recognise uncommon entities that cannot be reliably identified from local context alone. Approaches to such tasks often involve the use of maximum entropy-style models, where gazetteers usually appear as highly informative features in the model. Although such features can improve model accuracy, they can also introduce hidden negative effects. In this paper we describe and analyse these effects and suggest ways in which they may be overcome. In particular, we show that by quarantining gazetteer features and training them in a separate model, then decoding using a logarithmic opinion pool (Smith et al., 2005), we may achieve much higher accuracy. Finally, we suggest ways in which other features with gazetteer feature-like behaviour may be identified. In a task such as NER, one sometimes encounters an entity which is difficult to identify using local contextual cues alone because the entity has not be seen before. In these cases, a gazetteer or dictionary of possible entity identifiers is often useful. Such identifiers could be names of people, places, companies or other organisations. Using gazetteers one may define additional features in the model that represent the dependencies between a word’s"
W06-2918,N06-1012,0,0.0592062,"usefulness of gazetteers (Krupka and Hausman, 1998). Other work has supported the use of gazetteers in general but has found that lists of only moderate size are sufficient to provide most of the benefit (Mikheev et al., 1999). Therefore, to date the effective use of gazetteers for information extraction has in general been regarded as a “black art”. In this paper we explain some of the likely reasons for these findings, and propose ways to more effectively handle gazetteers when they are used by maxent-style models. In work developed independently and in parallel to the work presented here, Sutton et al. (2006) identify general problems with gazetteer features and propose a solution similar to ours. They present results on NP-chunking in addition to NER, and provide a slightly more general approach. By contrast, we motivate the problem more thoroughly through analysis of the actual errors observed and through consideration of the success of other candidate solutions, such as traditional regularisation over feature subsets. 4 Our Experiments In this section we describe our experimental setup, and provide results for the baseline models. 4.1 Task and Dataset Named entity recognition (NER) involves the"
W06-2918,W03-0419,0,\N,Missing
W06-2918,M98-1004,0,\N,Missing
W06-3123,J93-2003,0,0.0104668,"translation lexicons for both words and phrases. The joint model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation (Przybocki, 2004). Introduction Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages. It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data. The original IBM Models (Brown et al., 1993) learn word-to-word alignment probabilities which makes it computationally feasible to estimate model parameters from large amounts of training data. Phrase-based SMT models, such as the alignment template model (Och, 2003), improve on word-based models because phrases provide local context which leads to better lexical choice and more reliable local reordering. However, most phrase-based models extract their phrase pairs from previously word-aligned corpora using adhoc heuristics. These models perform no search 154 However, considering all possible phrases and all their possible alignments va"
W06-3123,P05-1066,1,0.176316,"hill-climbing are retained. Only a very small proportion of the alignment space can be searched and this reduces the chances of finding optimum parameters. The small number of alignments visited would lead to data sparseness and over-fitting. Another factor could be efficiency trade-offs like the fast but not optimal competitive linking search for phrasal alignments. 4.3 German-English submission We also submitted a German-English system using the standard approach to phrase extraction. The purpose of this submission was to validate the syntactic reordering method that we previously proposed (Collins et al., 2005). We parse the German training and test corpus and reorder it according to a set of manually devised rules. Then, we use our phrase-based system with standard phraseextraction, lexicalized reordering, lexical scoring, 5-gram LM, and the Pharaoh decoder. On the development test set, the syntactic reordering improved performance from 26.86 to 27.70. The best submission in last year’s shared task achieved a score of 24.77 on this set. 5 Conclusion We presented the first attempt at creating a systematic framework which uses word alignment constraints to guide phrase-based EM training. This shows c"
W06-3123,N03-1017,1,0.177164,"idence word alignments for each sentence. These high confidence alignments could incorporate information from both statistical and linguistic sources. In this paper we use the points of high confidence from the intersection of the bi-directional Viterbi word alignments to constrain the model, increasing performance and decreasing complexity. Proceedings of the Workshop on Statistical Machine Translation, pages 154–157, c New York City, June 2006. 2006 Association for Computational Linguistics 2 2.1 Translation Models Standard Phrase-based Model Most phrase-based translation models (Och, 2003; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based translation model as the standard model. The standard model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a"
W06-3123,W02-1018,0,0.2878,"the computational complexity of estimating parameters at the phrase level. We present the first model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training. Constraining the joint model improves performance, showing results that are very close to stateof-the-art phrase-based models. It also allows it to scale up to larger corpora and therefore be more widely applicable. 1 for optimal phrasal alignments. Even though this is an efficient strategy, it is a departure from the rigorous statistical framework of the IBM Models. Marcu and Wong (2002) proposed the joint probability model which directly estimates the phrase translation probabilities from the corpus in a theoretically governed way. This model neither relies on potentially sub-optimal word alignments nor on heuristics for phrase extraction. Instead, it searches the phrasal alignment space, simultaneously learning translation lexicons for both words and phrases. The joint model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation (Przybocki, 2004). Introduction Machine translation i"
W06-3123,P97-1063,0,0.0262337,"ssible alignments C, each of which is defined as the product of the probability of all individual concepts: p(F, E) = X Y p(&lt; ei , f i &gt;) (1) C∈C &lt;ei ,f i &gt;∈C The model is trained by initializing the translation table using Stirling numbers of the second kind to efficiently estimate p(&lt; ei , f i &gt;) by calculating the proportion of alignments which contain p(&lt; ei , f i &gt;) compared to the total number of alignments in the sentence (Marcu and Wong, 2002). EM is then performed by first discovering an initial phrasal alignments using a greedy algorithm similar to the competitive linking algorithm (Melamed, 1997). The highest probability phrase pairs are iteratively selected until all phrases are are linked. Then hill-climbing is performed by searching once for each iteration for all merges, splits, moves and swaps that improve the probability of the initial phrasal alignment. Fractional counts are collected for all alignments visited. Training the IBM models is computationally challenging, but the joint model is much more demanding. Considering all possible segmentations of phrases and all their possible alignments vastly increases the number of possible alignments that can be formed between two sent"
W06-3123,P02-1038,0,0.0182161,"er corpora. After the initialization phase of the training, all phrase pairs with counts less 156 No. Concepts BLEU Time(min) Unconstrained 6,178k 19.93 299 Constrained 1,457k 22.13 169 Table 1. The impact of constraining the joint model trained on 10,000 sentences of the German-English Europarl corpora and tested with the Europarl test set used in Koehn et al. (2003) than 10 million times that of the phrase pair with the highest count, are pruned from the phrase table. The model is also parallelized in order to speed up training. The translation models are included within a log-linear model (Och and Ney, 2002) which allows a weighted combination of features functions. For the comparison of the basic systems in Table 2 only three features were used for both the joint and the standard model: p(e|f ), p(f |e) and the language model, and they were given equal weights. The results in Table 2 show that the joint model is capable of training on large data sets, with a reasonable performance compared to the standard model. However, here it seems that the standard model has a slight advantage. This is almost certainly related to the fact that the joint model results in a much smaller phrase table. Pruning e"
W06-3123,2003.mtsummit-papers.53,0,0.0236842,"ts for each sentence. These high confidence alignments could incorporate information from both statistical and linguistic sources. In this paper we use the points of high confidence from the intersection of the bi-directional Viterbi word alignments to constrain the model, increasing performance and decreasing complexity. Proceedings of the Workshop on Statistical Machine Translation, pages 154–157, c New York City, June 2006. 2006 Association for Computational Linguistics 2 2.1 Translation Models Standard Phrase-based Model Most phrase-based translation models (Och, 2003; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based translation model as the standard model. The standard model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a relative distortion"
W06-3123,koen-2004-pharaoh,0,\N,Missing
W06-3123,P02-1040,0,\N,Missing
W06-3123,P04-1023,1,\N,Missing
W06-3123,J04-4002,0,\N,Missing
W06-3123,W06-3105,0,\N,Missing
W06-3123,W06-3114,1,\N,Missing
W06-3123,2005.mtsummit-papers.11,1,\N,Missing
W06-3123,2005.iwslt-1.8,1,\N,Missing
W06-3123,P00-1056,0,\N,Missing
W06-3123,P03-1021,0,\N,Missing
W07-0702,J99-2004,0,0.392573,"Missing"
W07-0702,N03-2002,0,0.0405063,"general form α/β or αβ where α and β are themselves categories. An example of a CCG parse is given: Peter However, in many cases multiple dependencies are desirable. For instance translating CCG supertags independently of words could introduce errors. Multiple dependencies require some form of backing off to simpler models in order to cover the cases where, for instance, the word has been seen in training, but not with that particular supertag. Different backoff paths are possible, and it would be interesting but prohibitively slow to apply a strategy similar to generalised parallel backoff (Bilmes and Kirchhoff, 2003) which is used in factored language models. Backoff in factored language models is made more difficult because there is no obvious backoff path. This is compounded for factored phrase-based translation models where one has N X eats apples NP (SNP)/NP NP > SNP &lt; S where the derivation proceeds as follows: “eats” is combined with “apples” under the operation of forward application. “eats” can be thought of as a function that takes a NP to the right and returns a SNP. Similarly the phrase “eats apples” can be thought of as a function which takes a noun phrase NP to the left and returns a sente"
W07-0702,2005.mtsummit-papers.11,1,0.0284006,"se, far from the position of the English verb. 4.1 Experimental Setup The experiments were run using Moses2 , an open source factored statistical machine translation system. The SRILM language modelling toolkit (Stolcke, 2002) was used with modified Kneser-Ney discounting and interpolation. The CCG supertagger (Clark, 2002; Clark and Curran, 2004) was provided with the C&C Language Processing Tools3 . The supertagger was trained on the CCGBank in English (Hockenmaier and Steedman, 2005) and in German (Hockenmaier, 2006). The Dutch-English parallel training data comes from the Europarl corpus (Koehn, 2005) and excludes the proceedings from the last quarter of 2000. 2 3 This consists of 855,677 sentences with a maximum of 50 words per sentence. 500 sentences of tuning data and the 2000 sentences of test data are taken from the ACL Workshop on Building and Using Parallel Texts4 . The German-English experiments use data from the NAACL 2006 Workshop on Statistical Machine Translation5 . The data consists of 751,088 sentences of training data, 500 sentences of tuning data and 3064 sentences of test data. The English and German training sets were POS tagged and supertagged before lowercasing. The lan"
W07-0702,N03-1019,0,0.0194211,"actored translation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings. 1 Introduction In large-scale machine translation evaluations, phrase-based models generally outperform syntaxbased models1 . Phrase-based models are effective because they capture the lexical dependencies between languages. However, these models, which are equivalent to finite-state machines (Kumar and Byrne, 2003), are unable to model long range word order differences. Phrase-based models also lack the ability to incorporate the generalisations implicit in syntactic knowledge and they do not respect linguistic phrase boundaries. This makes it difficult to improve reordering in phrase-based models. Syntax-based models can overcome some of the problems associated with phrase-based models because they are able to capture the long range structural mappings that occur in translation. Recently In this paper we investigate the idea of incorporating syntax into phrase-based models, thereby leveraging the stren"
W07-0702,E06-1032,1,0.156668,"Missing"
W07-0702,W06-1606,0,0.0525496,"Missing"
W07-0702,P05-1033,0,0.0609583,".c.birch-mayne@sms.ed.ac.uk Miles Osborne Philipp Koehn miles@inf.ed.ac.uk pkoehn@inf.ed.ac.uk School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW, UK Abstract there have been a few syntax-based models that show performance comparable to the phrase-based models (Chiang, 2005; Marcu et al., 2006). However, reliably learning powerful rules from parallel data is very difficult and prone to problems with sparsity and noise in the data. These models also suffer from a large search space when decoding with an integrated language model, which can lead to search errors (Chiang, 2005). Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level. The challenge is incorporating this information into the translation process. Factored translation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings. 1 Introduction In large-scale machine translation"
W07-0702,P03-1021,0,0.0379746,"s) with more general models (which only depends on words). This paper is the first to suggest this approach for combining multiple information sources in machine translation. Although the addition of supertags to phrasebased translation does show some improvement, their overall impact is limited. Sequence models over supertags clearly result in some improvements in local reordering but syntactic information contains long distance dependencies which are simply not utilised in phrase-based models. 2 Factored Models forms. The factored translation model combines features in a log-linear fashion (Och, 2003). The most likely target sentence tˆ is calculated using the decision rule in Equation 1: ( tˆ = arg max t M X tˆ ∝ M X ) λm hm (sF1 s , tF1 t ) (1) m=1 λm hm (sF1 s , tF1 t ) (2) m=1 where M is the number of features, hm (sF1 s , tF1 t ) are the feature functions over the factors, and λ are the weights which combine the features which are optimised using minimum error rate training (Venugopal and Vogel, 2005). Each function depends on a vector sF1 s of source factors and a vector tF1 t of target factors. An example of a factored model used in upcoming experiments is: tˆ ∝ M X λm hm (sw , twc"
W07-0702,P04-1014,0,0.00805384,"ct of CCG supertags on the source, translating from German into English. These language pairs present a considerable reordering challenge. For example, Dutch and German have SOV word order in subordinate clauses. This means that the verb often appears at the end of the clause, far from the position of the English verb. 4.1 Experimental Setup The experiments were run using Moses2 , an open source factored statistical machine translation system. The SRILM language modelling toolkit (Stolcke, 2002) was used with modified Kneser-Ney discounting and interpolation. The CCG supertagger (Clark, 2002; Clark and Curran, 2004) was provided with the C&C Language Processing Tools3 . The supertagger was trained on the CCGBank in English (Hockenmaier and Steedman, 2005) and in German (Hockenmaier, 2006). The Dutch-English parallel training data comes from the Europarl corpus (Koehn, 2005) and excludes the proceedings from the last quarter of 2000. 2 3 This consists of 855,677 sentences with a maximum of 50 words per sentence. 500 sentences of tuning data and the 2000 sentences of test data are taken from the ACL Workshop on Building and Using Parallel Texts4 . The German-English experiments use data from the NAACL 2006"
W07-0702,P05-1034,0,0.0456775,"ted in range. If it can replace 14 CCG supertags, it suggests that supertags’ influence is also within a local range. 4.7 CCG Supertags on Source Sequence models over supertags improve the performance of phrase-based machine translation. However, this is a limited way of leveraging the rich syntactic information available in the CCG categories. We explore the potential of letting supertags direct translation by including them as a factor on the source. This is similar to syntax-directed translation originally proposed for compiling (Aho and Ullman, 1969), and also used in machine translation (Quirk et al., 2005; Huang et al., 2006). Information about the source words’ syntactic function and subcategorisation can directly influence the hypotheses being searched in decoding. These experiments were performed on the German to English translation task, in contrast to the Dutch to English results given in previous experiments. We use a model which combines more specific dependencies on source words and source CCG supertags, with a more general model which only has dependancies on the source word, see Equation 4. We explore two different ways of balancing the statistical evidence from these multiple source"
W07-0702,W02-2203,0,0.352084,"e “eats apples” can be thought of as a function which takes a noun phrase NP to the left and returns a sentence S. This operation is called backward application. A sentence together with its CCG categories already contains most of the information present in a full parse. Because these categories are lexicalised, they can easily be included into factored phrasebased translation. CCG supertags are categories that have been provided by a supertagger. Supertags were introduced by Bangalore (1999) as a way of increasing parsing efficiency by reducing the number of structures assigned to each word. Clark (2002) developed a suppertagger for CCG which uses a conditional maximum entropy model to estimate the probability of words being assigned particular categories. Here is an example of a sentence that has been supertagged in the training corpus: We all agree on that . NP NPNP (S[dcl]NP)/PP PP/NP NP . The verb “agree” has been assigned a complex supertag (S[dcl]NP)/PP which determines the type and direction of its arguments. This information can be used to improve the quality of translation. 4 Experiments The first set of experiments explores the effect of CCG supertags on the target, translating f"
W07-0702,P07-1037,0,0.518309,"Missing"
W07-0702,P06-1064,0,0.010386,"d order in subordinate clauses. This means that the verb often appears at the end of the clause, far from the position of the English verb. 4.1 Experimental Setup The experiments were run using Moses2 , an open source factored statistical machine translation system. The SRILM language modelling toolkit (Stolcke, 2002) was used with modified Kneser-Ney discounting and interpolation. The CCG supertagger (Clark, 2002; Clark and Curran, 2004) was provided with the C&C Language Processing Tools3 . The supertagger was trained on the CCGBank in English (Hockenmaier and Steedman, 2005) and in German (Hockenmaier, 2006). The Dutch-English parallel training data comes from the Europarl corpus (Koehn, 2005) and excludes the proceedings from the last quarter of 2000. 2 3 This consists of 855,677 sentences with a maximum of 50 words per sentence. 500 sentences of tuning data and the 2000 sentences of test data are taken from the ACL Workshop on Building and Using Parallel Texts4 . The German-English experiments use data from the NAACL 2006 Workshop on Statistical Machine Translation5 . The data consists of 751,088 sentences of training data, 500 sentences of tuning data and 3064 sentences of test data. The Engli"
W07-0702,W06-3601,0,0.00472148,"can replace 14 CCG supertags, it suggests that supertags’ influence is also within a local range. 4.7 CCG Supertags on Source Sequence models over supertags improve the performance of phrase-based machine translation. However, this is a limited way of leveraging the rich syntactic information available in the CCG categories. We explore the potential of letting supertags direct translation by including them as a factor on the source. This is similar to syntax-directed translation originally proposed for compiling (Aho and Ullman, 1969), and also used in machine translation (Quirk et al., 2005; Huang et al., 2006). Information about the source words’ syntactic function and subcategorisation can directly influence the hypotheses being searched in decoding. These experiments were performed on the German to English translation task, in contrast to the Dutch to English results given in previous experiments. We use a model which combines more specific dependencies on source words and source CCG supertags, with a more general model which only has dependancies on the source word, see Equation 4. We explore two different ways of balancing the statistical evidence from these multiple sources. The first way to c"
W07-0702,N03-1017,1,0.00863018,"actors. Phrase-based models are limited to sequences of words as their units with no access to additional linguistic knowledge. Factors allow for richer translation models, for example, the gender or tense of a word can be expressed. Factors also allow the model to generalise, for example, the lemma of a word could be used to generalise to unseen inflected 10 Figure 1. Factored translation with source words determining target words and CCG supertags For our experiments we used the following features: the translation probabilities P r(sF1 s |tF1 t ) and P r(tF1 t |sF1 s ), the lexical weights (Koehn et al., 2003) lex(sF1 s |tF1 t ) and lex(tF1 t |sF1 s ), and a phrase penalty e, which allows the model to learn a preference for longer or shorter phrases. Added to these features is the word penalty e−1 which allows the model to learn a preference for longer or shorter sentences, the distortion model d that prefers monotone word order, and the language model probability P r(t). All these features are logged when combined in the log-linear model in order to retain the impact of very unlikely translations or sequences. One of the strengths of the factored model is it allows for n-gram distributions over fa"
W07-0702,W06-2918,1,0.902719,"overcome some of the problems associated with phrase-based models because they are able to capture the long range structural mappings that occur in translation. Recently In this paper we investigate the idea of incorporating syntax into phrase-based models, thereby leveraging the strengths of both the phrase-based models and syntactic structures. This is done using CCG supertags, which provide a rich source of syntactic information. CCG contains most of the structure of the grammar in the lexicon, which makes it possible to introduce CCG supertags as a factor in a factored translation model (Koehn et al., 2006). Factored models allow words to be vectors of features: one factor could be the surface form and other factors could contain linguistic information. Factored models allow for the easy inclusion of supertags in different ways. The first approach is to generate CCG supertags as a factor in the target and then apply an n-gram model over them, increasing the probability of more frequently seen sequences of supertags. This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by Hassan et al. (2007). For both Arabic-English (Hassan et al., 2007) an"
W07-0702,P05-1003,1,0.828589,"Missing"
W07-0702,N04-4026,0,0.0161138,"solation contributes more to translation performance than any other sequence model. Even with a high order language model, applying the CCG supertag sequence model still seems to improve performance. This means that even if we use a more powerful language model, the structural information contained in the supertags continues to be beneficial. 4.6 Lexicalised Reordering vs. Supertags In this experiment we investigate using a stronger reordering model to see how it compares to the contribution that CCG supertag sequence models make. Moses implements the lexicalised reordering model described by Tillman (2004), which learns whether phrases prefer monotone, inverse or disjoint orientations with regard to adjacent phrases. We apply this reordering models to the following experiments. Model sw , tw sw , twc None 23.97 24.42 Lex. Reord. 24.72 24.78 Table 5. Dutch-English models with and without a lexicalised reordering model. In Table 5 we can see that lexicalised reordering improves translation performance for both models. However, the improvement that was seen using CCG supertags without lexicalised reordering, almost disappears when using a stronger reordering model. This suggests that CCG supertags"
W07-0702,P07-2045,1,\N,Missing
W07-0702,2006.iwslt-evaluation.8,0,\N,Missing
W09-0434,D08-1078,1,0.823619,"e of these systems adequately deal with longer range reordering. Our analysis provides a deeper understanding of why hierarchical models demonstrate better performance for Chinese-English translation, and also why phrase-based approaches do well at Arabic-English. We begin by reviewing related work in Section 2. Section 3 describes our method for extracting and measuring reorderings in aligned and parsed parallel corpora. We apply our techniques to human aligned parallel treebank sentences in Section 4, and to machine translation outputs in Section 5. We summarise our findings in Section 6. 2 Birch et al. (2008) proposed a method for extracting reorderings from aligned parallel sentences. We extend this method in order to constrain the reorderings to a derivation over the source sentence where possible. 3 Measuring Reordering Reordering is largely driven by syntactic differences between languages and can involve complex rearrangements between nodes in synchronous trees. Modeling reordering exactly would be sparse and heterogeneous and thus we make an important simplifying assumption in order for the detection and extraction of reordering data to be tractable and useful. We assume that reordering is a"
W09-0434,E06-1032,1,0.705755,"it very difficult to model long distance movement of words between languages. Synchronous grammar models can encode structural mappings between languages which allow complex, long distance reordering. Some grammar-based models such as the hierarchical model (Chiang, 2005) and the syntactified target language phrases model (Marcu et al., 2006) have shown better performance than phrase-based models on certain language pairs. To date our understanding of the variation in reordering performance between phrase-based and synchronous grammar models has been limited to relative B LEU scores. However, Callison-Burch et al. (2006) showed that B LEU score alone is insufficient for comparing reordering as it only measures a partial ordering on n-grams. There has been little direct research on empirically evaluating reordering. We evaluate the reordering characteristics of these two paradigms on Chinese-English and Arabic-English translation. Our main findings are as follows: (1) Chinese-English parallel sentences exhibit many medium and long-range reorderings, but less short range ones than Arabic-English, (2) phrase-based models account for short-range reorderings better than hierarchical models do, (3) Reordering is a"
W09-0434,H05-1098,0,0.0367168,"ere are few empirical studies of reordering behaviour in the statistical machine translation literature. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. This study was performed manually and did not compare different language pairs or translation paradigms. There are some comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by using the frequency of POS sequences in the output. This method is a first step towards a better understanding of comparative reordering performance, but neglects the question of what kind of reordering is occurring in corpora and in translation output. Zollmann et al. (2008) performed an empirical comparison of the B LEU score performance of hierarchical models with phrase-based models. They tried to ascertain which is the stronger model under different reordering scenarios by varying distortion limits"
W09-0434,P05-1033,0,0.0716465,"last few years, showing state-of-the-art performance for many language pairs. They search all possible reorderings within a restricted window, and their output is guided by the language model and a lexicalised reordering model (Och et al., 2004), both of which are local in scope. However, the lack of structure in phrase-based models makes it very difficult to model long distance movement of words between languages. Synchronous grammar models can encode structural mappings between languages which allow complex, long distance reordering. Some grammar-based models such as the hierarchical model (Chiang, 2005) and the syntactified target language phrases model (Marcu et al., 2006) have shown better performance than phrase-based models on certain language pairs. To date our understanding of the variation in reordering performance between phrase-based and synchronous grammar models has been limited to relative B LEU scores. However, Callison-Burch et al. (2006) showed that B LEU score alone is insufficient for comparing reordering as it only measures a partial ordering on n-grams. There has been little direct research on empirically evaluating reordering. We evaluate the reordering characteristics of"
W09-0434,J07-2003,0,0.0242398,"e the reorderings in the parallel corpora with the reorderings that exist in the translated sentences. We com200 367 379 367 379 250 1.51 0.57 200 0.82 0.25 Table 1. The RQuantity and the number of sentences for each reordering test set. 0 2 3 4 5 6 7−8 9−10 16−20 Widths of Reorderings 22 Figure 6. Number of reorderings in the CH-EN test set plotted against the total width of the reorderings. 16 18 20 MOSES HIERO Reordering Test Corpus 14 5.1 50 pare two state-of-the-art models: the phrase-based system Moses (Koehn et al., 2007) (with lexicalised reordering), and the hierarchical model Hiero (Chiang, 2007). We use default settings for both models: a distortion limit of seven for Moses, and a maximum source span limit of 10 words for Hiero. We trained both models on subsets of the NIST 2008 data sets, consisting mainly of news data, totalling 547,420 CH-EN and 1,069,658 AREN sentence pairs. We used a trigram language model on the entire English side (211M words) of the NIST 2008 Chinese-English training corpus. Minimum error rate training was performed on the 2002 NIST test for CH-EN, and the 2004 NIST test set for AR-EN. Low Medium High 150 High 100 Medium Number of Reorderings None Low Average"
W09-0434,W02-1039,0,0.161844,"we can see a sentence pair with an alignment and a parse tree over the source. We perform a depth first recursion through the tree, extracting the reorderings that occur between whole sibling nodes. Initially a reordering is detected between the leaf nodes P and NN. The block growing algorithm described in Birch et al. (2008) is then used to grow block A to include NT and NN, and block B to include P and NR. The source and target spans of these nodes do not overlap the spans Related Work There are few empirical studies of reordering behaviour in the statistical machine translation literature. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. This study was performed manually and did not compare different language pairs or translation paradigms. There are some comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by usin"
W09-0434,N03-1017,0,0.040066,"Missing"
W09-0434,P07-2045,1,0.00797446,"on models perform specifically with regard to reordering. To evaluate this, we compare the reorderings in the parallel corpora with the reorderings that exist in the translated sentences. We com200 367 379 367 379 250 1.51 0.57 200 0.82 0.25 Table 1. The RQuantity and the number of sentences for each reordering test set. 0 2 3 4 5 6 7−8 9−10 16−20 Widths of Reorderings 22 Figure 6. Number of reorderings in the CH-EN test set plotted against the total width of the reorderings. 16 18 20 MOSES HIERO Reordering Test Corpus 14 5.1 50 pare two state-of-the-art models: the phrase-based system Moses (Koehn et al., 2007) (with lexicalised reordering), and the hierarchical model Hiero (Chiang, 2007). We use default settings for both models: a distortion limit of seven for Moses, and a maximum source span limit of 10 words for Hiero. We trained both models on subsets of the NIST 2008 data sets, consisting mainly of news data, totalling 547,420 CH-EN and 1,069,658 AREN sentence pairs. We used a trigram language model on the entire English side (211M words) of the NIST 2008 Chinese-English training corpus. Minimum error rate training was performed on the 2002 NIST test for CH-EN, and the 2004 NIST test set for AR"
W09-0434,W04-3250,0,0.218516,"Missing"
W09-0434,W06-1606,0,0.0281669,"uage pairs. They search all possible reorderings within a restricted window, and their output is guided by the language model and a lexicalised reordering model (Och et al., 2004), both of which are local in scope. However, the lack of structure in phrase-based models makes it very difficult to model long distance movement of words between languages. Synchronous grammar models can encode structural mappings between languages which allow complex, long distance reordering. Some grammar-based models such as the hierarchical model (Chiang, 2005) and the syntactified target language phrases model (Marcu et al., 2006) have shown better performance than phrase-based models on certain language pairs. To date our understanding of the variation in reordering performance between phrase-based and synchronous grammar models has been limited to relative B LEU scores. However, Callison-Burch et al. (2006) showed that B LEU score alone is insufficient for comparing reordering as it only measures a partial ordering on n-grams. There has been little direct research on empirically evaluating reordering. We evaluate the reordering characteristics of these two paradigms on Chinese-English and Arabic-English translation."
W09-0434,J04-4002,0,0.0234622,"Missing"
W09-0434,P06-1123,0,0.0178902,"lude P and NR. The source and target spans of these nodes do not overlap the spans Related Work There are few empirical studies of reordering behaviour in the statistical machine translation literature. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. This study was performed manually and did not compare different language pairs or translation paradigms. There are some comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by using the frequency of POS sequences in the output. This method is a first step towards a better understanding of comparative reordering performance, but neglects the question of what kind of reordering is occurring in corpora and in translation output. Zollmann et al. (2008) performed an empirical comparison of the B LEU score performance of hierarchical models with phrase-based models. They tried to ascertain"
W09-0434,J97-3002,0,0.241369,"lex rearrangements between nodes in synchronous trees. Modeling reordering exactly would be sparse and heterogeneous and thus we make an important simplifying assumption in order for the detection and extraction of reordering data to be tractable and useful. We assume that reordering is a binary process occurring between two blocks that are adjacent in the source. We extend the methods proposed by Birch et al. (2008) to identify and measure reordering. Modeling reordering as the inversion in order of two adjacent blocks is similar to the approach taken by the Inverse Transduction Model (ITG) (Wu, 1997), except that here we are not limited to a binary tree. We also detect and include non-syntactic reorderings as they constitute a significant proportion of the reorderings. Birch et al. (2008) defined the extraction process for a sentence pair that has been word aligned. This method is simple, efficient and applicable to all aligned sentence pairs. However, if we have access to the syntax tree, we can more accurately determine the groupings of embedded reorderings, and we can also access interesting information about the reordering such as the type of constituents that get reordered. Figure 1"
W09-0434,P03-1019,0,0.0432249,", and block B to include P and NR. The source and target spans of these nodes do not overlap the spans Related Work There are few empirical studies of reordering behaviour in the statistical machine translation literature. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. This study was performed manually and did not compare different language pairs or translation paradigms. There are some comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by using the frequency of POS sequences in the output. This method is a first step towards a better understanding of comparative reordering performance, but neglects the question of what kind of reordering is occurring in corpora and in translation output. Zollmann et al. (2008) performed an empirical comparison of the B LEU score performance of hierarchical models with phrase-based models"
W09-0434,C08-1144,0,0.0520931,"comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by using the frequency of POS sequences in the output. This method is a first step towards a better understanding of comparative reordering performance, but neglects the question of what kind of reordering is occurring in corpora and in translation output. Zollmann et al. (2008) performed an empirical comparison of the B LEU score performance of hierarchical models with phrase-based models. They tried to ascertain which is the stronger model under different reordering scenarios by varying distortion limits the strength of language models. They show that the hierarchical models do slightly better for Chinese-English systems, but worse for Arabic-English. However, there was no analysis of the reorderings existing in their parallel corpora, or on what kinds of reorderings were produced in their output. We perform a focused evaluation of these issues. 198 A A B B Figure"
W09-0434,N04-1021,0,\N,Missing
W10-1749,P03-1069,0,0.0217815,"ic R is calculated as follows: The Kendall’s tau metric is possibly the most interesting for measuring reordering as it is sensitive to all relative orderings. It consequently measures not only how many reordering there are but also the distance that words are reordered. In statistics, Spearman’s rho and Kendall’s tau are widely used non-parametric measures of association for two rankings. In natural language processing research, Kendall’s tau has been used as a means of estimating the distance between a system-generated and a human-generated goldstandard order for the sentence ordering task (Lapata, 2003). Kendall’s tau has also been used in machine translation as a cost function in a reordering model (Eisner and Tromble, 2006) and an MT metric called ROUGE-S (Lin and Och, R = d ∗ BP Where we either take the Hamming distance dH or the Kendall’s tau distance dτ as the reordering distance d and then we apply the brevity penalty BP . The brevity penalty is calculated as:  1 if t > r BP = e1−r/t if t ≤ r where t is the length of the translation, and r is the closest reference length. R is calculated at the sentence level, and the scores are averaged over a test set. This average is then combined"
W10-1749,P04-1077,0,0.0139617,"Missing"
W10-1749,P02-1040,0,0.108188,"Missing"
W10-1749,W05-0909,0,0.0515296,"Missing"
W10-1749,2006.amta-papers.25,0,0.0476832,"aluating Lexical and Reordering Quality in MT Alexandra Birch University of Edinburgh United Kingdom a.c.birch-mayne@s0454866.ed.ac.uk Abstract to measure the quality of word order is to count the number of matching n-grams between the reference and the hypothesis. This is the approach taken by the BLEU score (Papineni et al., 2002). This method discounts any n-gram which is not identical to a reference n-gram, and also does not consider the relative position of the strings. They can be anywhere in the sentence. Another common approach is typified by METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). They calculate an ordering penalty for a hypothesis based on the minimum number of chunks the translation needs to be broken into in order to align it to the reference. The disadvantage of the second approach is that aligning sentences with very different words can be inaccurate. Also there is no notion of how far these blocks are out of order. More sophisticated metrics, such as the RTE metric (Pad´o et al., 2009), use higher level syntactic or even semantic analysis to determine the quality of the translation. These approaches are useful, but can be very slow, require annotation, they are"
W10-1749,W09-0401,0,\N,Missing
W11-2122,D07-1090,0,0.272624,"ams. We have simply Ct = K [ Skt (1) k=1 179 stream K LM 3 stream K LM 2 Figure 2: Each stream 1 . . . K gets its own stream-based LM using the multiple LM approach. where each of the K streams is combined into a single model and the n-grams counts are merged linearly. Here we carry no n-grams over from the LM Ct−1 from the previous epoch. The space needed is the number of unique n-grams present in the combined streams for each epoch. Resulting LM To query the resulting LM Ct during decoding with a test n-gram win = (wi , . . . , wn ) we use a simple smoothing algorithm called Stupid Backoff (Brants et al., 2007). This returns the probability of an n-gram as i−1 P (wi |wi−n+1 ) :=    i Ct (wi−n+1 ) i−1 Ct (wi−n+1 ) i−1 αP (wi |wi−n+2 ) i if Ct (wi−n+1 )&gt;0 (2) otherwise where Ct (.) denotes the frequency count returned by the LM for an n-gram and α is a backoff parameter. The recursion ends once the unigram is reached in which case the probability is P (wi ) := wi /N where N is the size of the current training corpus. Each stream provides a distribution over the ngrams contained in it and, for SMT, if a separate LM was constructed for each domain it would most likely cause the decoder to derive diff"
W11-2122,N09-1058,0,0.0243267,"Missing"
W11-2122,D07-1091,0,0.017311,"the amount of data that we store in the final LM we can expect to take a performance hit based on the rate of acceptance given by the parameters fk . By using subsampling with the history combination approach we obtain good performance for all streams in small space. 181 Experiments Here we report on our SMT experiments with multiple streams for translation using the approaches outlined in the previous section. 5.1 Experimental Setup The SMT setup we employ is standard and all resources used are publicly available. We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder. Our parallel data came from Europarl. We use three streams (all are timestamped): RCV1 (Rose et al., 2002), Europarl (EP) (Koehn, 2003), and Gigaword (GW) (Graff et al., 2007). GW is taken from six distinct newswire sources but in our initial experiments we limit the incoming stream from Gigaword to one of the sources (xie). GW and RCV1 are both newswire domain streams with high rates of incoming data whereas EP is a more nuanced, smaller throughput domain of spoken transcripts taken from sessions of the European Parliament. The RCV1 corpus only spans one calender year from Oc"
W11-2122,W07-0733,0,0.0371653,") a text stream was used to initially train and subsequently adapt an online, randomised LM (ORLM) with good results. However, a weakness of Levenberg and Osborne (2009) is that the experiments were all conducted over a single input stream. It is an oversimplification to assume that all test material for a SMT system will be from a single domain. No work was done on the multi-stream case where we have more than one incoming stream from arbitrary domains. 2.3 Domain Adaptation for SMT Within MT there has been a variety of approaches dealing with domain adaptation (for example (Wu et al., 2008; Koehn and Schroeder, 2007)). Our work is related to domain adaptation but differs in that we are not skewing the distribution of an out-of-domain LM to accommodate some test data for which we have little or no training data for. Rather, we have varying amounts of training data from all the domains via the incoming streams and the LM must account for each domain appropriately. However, known domain adaptation techniques are potentially applicable to multi-stream translation as well. 178 3 Multiple Streams and their Properties Any source that provides a continuous sequence of natural language documents over time can be t"
W11-2122,D09-1079,1,0.401054,"ms arise naturally on the Web where millions of new documents are published each day in many different languages. Examples in the streaming domain include the thousands of multilingual websites that continuously publish newswire stories, the official proceedings of governments and other bureaucratic organisations, as well as the millions of “bloggers” and host of users on social network services such as Facebook and Twitter. Recent work has shown good results using an incoming text stream as training data for either a static or online language model (LM) in an SMT setting (Goyal et al., 2009; Levenberg and Osborne, 2009). A drawback of prior work is the oversimplified scenario that all training and test data is drawn from the same distribution using a single, in-domain stream. In a real world scenario multiple incoming streams are readily available and test sets from dissimilar domains will be translated continuously. As we show, using stream data from one domain to translate another results in poor average performance for both streams. However, combining streams naively together hurts performance further still. In this paper we consider this problem of multiple stream translation. Since monolingual data is v"
W11-2122,2001.mtsummit-papers.68,0,0.014506,"er two streams so our timeline consists of the same full calendar year for all streams. For this work we use the ORLM. The crux of the ORLM is an online perfect hash function that provides the ability to insert and delete from the data structure. Consequently the ORLM has the ability to adapt to an unbounded input stream whilst maintaining both constant memory usage and error rate. All the ORLMs were 5-gram models built with training data from the streams discussed above and used Stupid Backoff smoothing for n-gram scoring (Brants et al., 2007). All results are reported using the BLEU metric (Papineni et al., 2001). For testing we held-out three random test points LM Type RCV1 (Static) RCV1 (Online) EP (Online) RCV1+EP (Online) RCV1+EP+GW (Online) Test 1 39.30 39.30 30.22 39.00 41.29 Test 2 38.28 40.64 30.31 40.15 41.73 Test 3 33.06 39.19 26.66 39.46 40.41 Table 2: Results for the RCV1 test points. RCV1 and GW streams are in-domain and EP is out-of-domain. Translation results are improved using more stream data since most n-grams are in-domain to the test points. from both the RCV1 and EP stream’s timeline for a total of six test points. This divided the streams into three epochs, and we updated the onl"
W11-2122,rose-etal-2002-reuters,0,0.0333286,"by the parameters fk . By using subsampling with the history combination approach we obtain good performance for all streams in small space. 181 Experiments Here we report on our SMT experiments with multiple streams for translation using the approaches outlined in the previous section. 5.1 Experimental Setup The SMT setup we employ is standard and all resources used are publicly available. We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder. Our parallel data came from Europarl. We use three streams (all are timestamped): RCV1 (Rose et al., 2002), Europarl (EP) (Koehn, 2003), and Gigaword (GW) (Graff et al., 2007). GW is taken from six distinct newswire sources but in our initial experiments we limit the incoming stream from Gigaword to one of the sources (xie). GW and RCV1 are both newswire domain streams with high rates of incoming data whereas EP is a more nuanced, smaller throughput domain of spoken transcripts taken from sessions of the European Parliament. The RCV1 corpus only spans one calender year from October, 1996 through September, 1997 so we selected only data in this time frame from the other two streams so our timeline"
W11-2122,P08-1058,0,0.0164328,"reams within a single LM using equal memory (Tables 6 and 7). We also show additive im177 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 177–186, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics provements using this approach when using a large background LM consisting of over one billion ngrams. To our knowledge our approach is the first in the literature to deal with adapting an online LM to multiple streams in small space. 2 Previous Work 2.1 Randomised LMs Randomised techniques for LMs from Talbot and Osborne (2007) and Talbot and Brants (2008) are currently industry state-of-the-art for fitting very large datasets into much smaller amounts of memory than lossless representations for the data. Instead of representing the n-grams exactly, the randomised representation exchanges a small, one-sided error of false positives for massive space savings. 2.2 Stream-based LMs An unbounded text stream is an input source of natural language documents that is received sequentially and so has an implicit timeline attached. In Levenberg and Osborne (2009) a text stream was used to initially train and subsequently adapt an online, randomised LM (O"
W11-2122,D07-1049,1,0.834458,"28 and 42.73 using multiple streams within a single LM using equal memory (Tables 6 and 7). We also show additive im177 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 177–186, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics provements using this approach when using a large background LM consisting of over one billion ngrams. To our knowledge our approach is the first in the literature to deal with adapting an online LM to multiple streams in small space. 2 Previous Work 2.1 Randomised LMs Randomised techniques for LMs from Talbot and Osborne (2007) and Talbot and Brants (2008) are currently industry state-of-the-art for fitting very large datasets into much smaller amounts of memory than lossless representations for the data. Instead of representing the n-grams exactly, the randomised representation exchanges a small, one-sided error of false positives for massive space savings. 2.2 Stream-based LMs An unbounded text stream is an input source of natural language documents that is received sequentially and so has an implicit timeline attached. In Levenberg and Osborne (2009) a text stream was used to initially train and subsequently adap"
W11-2122,C08-1125,0,0.015706,"gle LM using equal memory (Tables 6 and 7). We also show additive im177 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 177–186, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics provements using this approach when using a large background LM consisting of over one billion ngrams. To our knowledge our approach is the first in the literature to deal with adapting an online LM to multiple streams in small space. 2 Previous Work 2.1 Randomised LMs Randomised techniques for LMs from Talbot and Osborne (2007) and Talbot and Brants (2008) are currently industry state-of-the-art for fitting very large datasets into much smaller amounts of memory than lossless representations for the data. Instead of representing the n-grams exactly, the randomised representation exchanges a small, one-sided error of false positives for massive space savings. 2.2 Stream-based LMs An unbounded text stream is an input source of natural language documents that is received sequentially and so has an implicit timeline attached. In Levenberg and Osborne (2009) a text stream was used to initially train and subsequently adapt an online, randomised LM (O"
W11-2122,P02-1040,0,\N,Missing
W12-3152,W10-0710,0,0.068614,"t guess, an X marks a false positive, and a • denotes a false negative. Hiero’s extraction heuristics yield 4 rules for the top alignment and 16 for the bottom. reasonable translation accuracy. Closely related to our work here is that of Novotney and Callison-Burch (2010), who showed that transcriptions for training speech recognition systems could be obtained from Mechanical Turk with near baseline recognition performance and at a significantly lower cost. They also showed that redundant annotation was not worthwhile, and suggested that money was better spent obtaining more data. Separately, Ambati and Vogel (2010) probed the MTurk worker pool for workers capable of translating a number of low-resource languages, including Hindi, Telugu, and Urdu, demonstrating that such workers could be found and quantifying acceptable 408 We have described the collection of six parallel corpora containing four-way redundant translations of the source-language text. The Indian languages of these corpora are low-resource and understudied, and exhibit markedly different linguistic properties compared to English. We performed baseline experiments quantifying the translation performance of a number of systems, investigated"
W12-3152,J07-2003,0,0.227477,"present in these languages is a high degree of morphological complexity relative to English (Figure 2). Indian languages can be highly agglutinative, which means that words are formed by concatenating morphological affixes that convey information such as tense, person, number, gender, mood, and voice. Morphological complexity is a considerable hindrance at all stages of the MT pipeline, but particularly alignment, where inflectional variations mask patterns from alignment tools that treat words as atoms. 3 We use hierarchical to denote translation grammars that use only a single nonterminal (Chiang, 2007), in contrast to syntactic systems, which make use of linguistic annotations (Zollmann and Venugopal, 2006; Galley et al., 2006). 402 The source of the documents for our translation task for each of the languages in Table 1 was the set of the top-100 most-viewed documents from each language’s Wikipedia. These lists were obtained using page view statistics compiled from dammit.lt/ wikistats over a one year period. We did not apply any filtering for topic or content. Table 2 contains a manually categorized list of documents for Hindi, with some minimal annotations indicating how the documents re"
W12-3152,P11-2031,0,0.0160195,"on results? 3. What is the effect of data quality on model quality? 4.1 Setup A principal point of comparison in this paper is between Hiero grammars (Chiang, 2007) and SAMT grammars (Zollmann and Venugopal, 2006), the latter of which make use of linguistic annotations to 405 improve nonterminal reordering. These grammars were trained with the Thrax grammar extractor using its default settings, and translated using Joshua (Weese et al., 2011). We tuned with minimum errorrate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al., 2011). MBR reranking (Kumar and Byrne, 2004) was applied to Joshua’s 300-best (unique) output, and evaluation was conducted with case-insensitive BLEU with four references. The training data was produced by pairing a source sentence with each of its four translations. We also added the dictionaries to the training data. We built five-gram language models from the target side of the training data using interpolated KneserNey smoothing. We also experimented with a largerscale language model built from English Gigaword, but, notably, found a drop of over a point in BLEU score. This points forward to s"
W12-3152,N04-1035,0,0.0124259,"able 1: Languages. L1 is the worldwide number of native speakers according to Lewis (2009). 3 Data collection In addition to a general desire to collect suitable training corpora for low-resource languages, Indian languages demonstrate a variety of linguistic phenomena that are divergent from English and understudied. One example is head-finalness, exhibited most obviously in a subject-object-verb (SOV) pattern of sentence structure, in contrast to the general SVO ordering of English sentences. One of the motivations underlying linguistically-motivated syntactic translation systems like GHKM (Galley et al., 2004; Galley et al., 2006) or SAMT (Zollmann and Venugopal, 2006) is to describe such transformations. This difference in word order has the potential to serve as a better test bed for syntax-based MT3 compared to translating between English and European languages, most of which largely share its word order. Figure 1 contains an example of SOV reordering in Tamil. A second important phenomenon present in these languages is a high degree of morphological complexity relative to English (Figure 2). Indian languages can be highly agglutinative, which means that words are formed by concatenating morpho"
W12-3152,P06-1121,0,0.00996435,"is the worldwide number of native speakers according to Lewis (2009). 3 Data collection In addition to a general desire to collect suitable training corpora for low-resource languages, Indian languages demonstrate a variety of linguistic phenomena that are divergent from English and understudied. One example is head-finalness, exhibited most obviously in a subject-object-verb (SOV) pattern of sentence structure, in contrast to the general SVO ordering of English sentences. One of the motivations underlying linguistically-motivated syntactic translation systems like GHKM (Galley et al., 2004; Galley et al., 2006) or SAMT (Zollmann and Venugopal, 2006) is to describe such transformations. This difference in word order has the potential to serve as a better test bed for syntax-based MT3 compared to translating between English and European languages, most of which largely share its word order. Figure 1 contains an example of SOV reordering in Tamil. A second important phenomenon present in these languages is a high degree of morphological complexity relative to English (Figure 2). Indian languages can be highly agglutinative, which means that words are formed by concatenating morphological affixes that c"
W12-3152,W01-1409,0,0.0421683,"ally translated data. Therefore, we could not measure the BLEU score of the Turkers. Hindi 0 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 Figure 3: The total volume of translations (measured in English words) as a function of elapsed days. For Malayalam, we collected half a million words of translations in just under a week. week. For comparison, the Europarl corpus (Koehn, 2005) has about 50 million words of English for each of the Spanish and French parallel corpora. As has been previously reported (Zbib et al., 2012), cost is another advantage of building training data on Mechanical Turk. Germann (2001) puts the cost of professionally translated English at about $0.30 per word for translation from Tamil. Our translations were obtained for less than $0.01 per word. The rate of collection could likely be increased by raising these payments, but it is unclear whether quality would be affected by raising the base pay (although it could be improved by paying for subsequent quality control HITs, like editing). The tradeoff for low-cost translations is increased variance in translation quality when compared to the more consistently-good professional translations. Figure 4 contains some hand-picked"
W12-3152,N03-1017,0,0.0236059,"g data (plus dictionary), selected in two ways: best (result of vote), and random. There is little difference, suggesting quality control may not be terribly important. We did not collect votes for Malayalam. misspelling japenese japans japenes japenies japeneses japeneese japense count 91 40 9 3 3 1 1 Table 8: Misspellings of japanese (947) in the training portion of the Urdu-English data, along with their counts. 5.2 pairs and data conditions. Figure 6 (top) contains an example of a particularly poor alignment produced by the default alignment heuristic, the grow-diagand method described in Koehn et al. (2003). As a means of testing this, we varied the alignment combination heuristics using five alternatives described in Koehn et al. (2003) and available in the symal program distributed with Moses (Koehn et al., 2007). Experiments on Tamil produce a range of BLEU scores between 7.45 and 10.19 (each result is the average of three MERT runs). If we plot grammar size versus BLEU score, we observe a general trend that larger grammars seem to positively correlate with BLEU score. We tested this more generally across languages using the Berkeley aligner5 (Liang et al., 2006) instead of GIZA alignments, a"
W12-3152,P07-2045,1,0.0155163,"spelling japenese japans japenes japenies japeneses japeneese japense count 91 40 9 3 3 1 1 Table 8: Misspellings of japanese (947) in the training portion of the Urdu-English data, along with their counts. 5.2 pairs and data conditions. Figure 6 (top) contains an example of a particularly poor alignment produced by the default alignment heuristic, the grow-diagand method described in Koehn et al. (2003). As a means of testing this, we varied the alignment combination heuristics using five alternatives described in Koehn et al. (2003) and available in the symal program distributed with Moses (Koehn et al., 2007). Experiments on Tamil produce a range of BLEU scores between 7.45 and 10.19 (each result is the average of three MERT runs). If we plot grammar size versus BLEU score, we observe a general trend that larger grammars seem to positively correlate with BLEU score. We tested this more generally across languages using the Berkeley aligner5 (Liang et al., 2006) instead of GIZA alignments, and found a consistent increase in BLEU score for the Hiero grammars, often putting them on par with the original SAMT results (Table 9). Manual analysis suggests that the Berkeley aligner produces fewer, more rea"
W12-3152,2005.mtsummit-papers.11,0,0.0437349,"ask, and likely desire to maximize their throughput (and thus their wage). Unlike Zaidan and CallisonBurch (2011), who embed controls containing source language sentences with known professional translations, we had no professionally translated data. Therefore, we could not measure the BLEU score of the Turkers. Hindi 0 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 Figure 3: The total volume of translations (measured in English words) as a function of elapsed days. For Malayalam, we collected half a million words of translations in just under a week. week. For comparison, the Europarl corpus (Koehn, 2005) has about 50 million words of English for each of the Spanish and French parallel corpora. As has been previously reported (Zbib et al., 2012), cost is another advantage of building training data on Mechanical Turk. Germann (2001) puts the cost of professionally translated English at about $0.30 per word for translation from Tamil. Our translations were obtained for less than $0.01 per word. The rate of collection could likely be increased by raising these payments, but it is unclear whether quality would be affected by raising the base pay (although it could be improved by paying for subsequ"
W12-3152,N04-1022,0,0.0140679,"data quality on model quality? 4.1 Setup A principal point of comparison in this paper is between Hiero grammars (Chiang, 2007) and SAMT grammars (Zollmann and Venugopal, 2006), the latter of which make use of linguistic annotations to 405 improve nonterminal reordering. These grammars were trained with the Thrax grammar extractor using its default settings, and translated using Joshua (Weese et al., 2011). We tuned with minimum errorrate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al., 2011). MBR reranking (Kumar and Byrne, 2004) was applied to Joshua’s 300-best (unique) output, and evaluation was conducted with case-insensitive BLEU with four references. The training data was produced by pairing a source sentence with each of its four translations. We also added the dictionaries to the training data. We built five-gram language models from the target side of the training data using interpolated KneserNey smoothing. We also experimented with a largerscale language model built from English Gigaword, but, notably, found a drop of over a point in BLEU score. This points forward to some of the difficulties encountered wit"
W12-3152,N06-1014,0,0.0128778,"w-diagand method described in Koehn et al. (2003). As a means of testing this, we varied the alignment combination heuristics using five alternatives described in Koehn et al. (2003) and available in the symal program distributed with Moses (Koehn et al., 2007). Experiments on Tamil produce a range of BLEU scores between 7.45 and 10.19 (each result is the average of three MERT runs). If we plot grammar size versus BLEU score, we observe a general trend that larger grammars seem to positively correlate with BLEU score. We tested this more generally across languages using the Berkeley aligner5 (Liang et al., 2006) instead of GIZA alignments, and found a consistent increase in BLEU score for the Hiero grammars, often putting them on par with the original SAMT results (Table 9). Manual analysis suggests that the Berkeley aligner produces fewer, more reasonable-looking alignments than the Moses heuristics (Figure 6). This suggest a fruitful approaches in revisiting assumptions underlying alignment heuristics. 6 Related Work Alignments Inconsistent orthography fragments the training data, exacerbating problems already present due to morpohological richness. One place this is manifested is during alignment,"
W12-3152,N10-1024,1,0.728684,"(to increase quality) or translate more foreign sentences (to increase coverage). To test this, we constructed two smaller datasets, each making use of only one of the four translations of each source sentence: • Selected randomly • Selected by choosing the translation that received a plurality of the votes (§3.3), breaking ties randomly (best) We again included the dictionaries in the training data (where available). Table 7 contains results on the same test sets as before. These results do not clearly indicate that quality control through redundant translations are worth the extra expense. Novotney and Callison-Burch (2010) had a similar finding for crowdsourced transcriptions. 5 Further Analysis The previous section has shown that reasonable BLEU scores can be obtained from baseline translation systems built from these corpora. While translation quality is an issue (for example, very lit406 eral translations, etc), the previous section’s voted dataset experiments suggest this is not one of the most important issues to address. In this section, we undertake a manual analysis of the collected datasets to inform future work. There are a number of issues that arise due to non-Roman scripts, high-variance translatio"
W12-3152,P03-1021,0,0.0249439,". How well can we translate the test sets? 2. Do linguistically motivated translation models improve translation results? 3. What is the effect of data quality on model quality? 4.1 Setup A principal point of comparison in this paper is between Hiero grammars (Chiang, 2007) and SAMT grammars (Zollmann and Venugopal, 2006), the latter of which make use of linguistic annotations to 405 improve nonterminal reordering. These grammars were trained with the Thrax grammar extractor using its default settings, and translated using Joshua (Weese et al., 2011). We tuned with minimum errorrate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al., 2011). MBR reranking (Kumar and Byrne, 2004) was applied to Joshua’s 300-best (unique) output, and evaluation was conducted with case-insensitive BLEU with four references. The training data was produced by pairing a source sentence with each of its four translations. We also added the dictionaries to the training data. We built five-gram language models from the target side of the training data using interpolated KneserNey smoothing. We also experimented with a largerscale languag"
W12-3152,W11-2160,1,0.493032,"ance. The experiments aim to address the following questions: 1. How well can we translate the test sets? 2. Do linguistically motivated translation models improve translation results? 3. What is the effect of data quality on model quality? 4.1 Setup A principal point of comparison in this paper is between Hiero grammars (Chiang, 2007) and SAMT grammars (Zollmann and Venugopal, 2006), the latter of which make use of linguistic annotations to 405 improve nonterminal reordering. These grammars were trained with the Thrax grammar extractor using its default settings, and translated using Joshua (Weese et al., 2011). We tuned with minimum errorrate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al., 2011). MBR reranking (Kumar and Byrne, 2004) was applied to Joshua’s 300-best (unique) output, and evaluation was conducted with case-insensitive BLEU with four references. The training data was produced by pairing a source sentence with each of its four translations. We also added the dictionaries to the training data. We built five-gram language models from the target side of the training data using interpolated KneserNey smoo"
W12-3152,P11-1122,1,0.421888,"e m s s fu ov ll i fo e r ✓ ✓ ✓ X ✓ ✓ ✓ . pair Bengali Hindi GIZA++ Berkeley 15m 34m 12m 19m 28m 38m 27m 60m 27m 30m 46m 58m Malayalam Tamil Telugu Urdu BLEU 13.54 16.47 12.70 10.10 13.36 20.41 gain +0.82 +0.94 -1.02 +0.29 +0.90 +0.88 Table 9: Hiero translation results using Berkeley alignments instead of GIZA++ heuristics. The gain columns denotes improvements relative to the Hiero systems in Table 5. In many cases (bold gains), the BLEU scores are at or above even the SAMT models from that table. wages and collection rates. The techniques described here are similar to those அ&quot;# described in Zaidan and Callison-Burch (2011), who $மா&apos;( showed that crowdsourcing with appropriate quality controls could be used to produce professional-level )த+ translations for Urdu-English translation. This paெவ./0 per extends that work by applying their techniques to a larger set of Indian languages and scaling it to பட3 training-data-set sizes. ஆைச ✓ X X ✓ 7 Summary . Figure 6: A bad Tamil alignment produced with the grow-diag-and alignment combination heuristic (top); the Berkeley aligner is better (bottom). A ✓ is a correct guess, an X marks a false positive, and a • denotes a false negative. Hiero’s extraction heuristics yield"
W12-3152,N12-1006,1,0.732385,"taining source language sentences with known professional translations, we had no professionally translated data. Therefore, we could not measure the BLEU score of the Turkers. Hindi 0 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 Figure 3: The total volume of translations (measured in English words) as a function of elapsed days. For Malayalam, we collected half a million words of translations in just under a week. week. For comparison, the Europarl corpus (Koehn, 2005) has about 50 million words of English for each of the Spanish and French parallel corpora. As has been previously reported (Zbib et al., 2012), cost is another advantage of building training data on Mechanical Turk. Germann (2001) puts the cost of professionally translated English at about $0.30 per word for translation from Tamil. Our translations were obtained for less than $0.01 per word. The rate of collection could likely be increased by raising these payments, but it is unclear whether quality would be affected by raising the base pay (although it could be improved by paying for subsequent quality control HITs, like editing). The tradeoff for low-cost translations is increased variance in translation quality when compared to t"
W12-3152,W06-3119,0,0.0610624,"tive speakers according to Lewis (2009). 3 Data collection In addition to a general desire to collect suitable training corpora for low-resource languages, Indian languages demonstrate a variety of linguistic phenomena that are divergent from English and understudied. One example is head-finalness, exhibited most obviously in a subject-object-verb (SOV) pattern of sentence structure, in contrast to the general SVO ordering of English sentences. One of the motivations underlying linguistically-motivated syntactic translation systems like GHKM (Galley et al., 2004; Galley et al., 2006) or SAMT (Zollmann and Venugopal, 2006) is to describe such transformations. This difference in word order has the potential to serve as a better test bed for syntax-based MT3 compared to translating between English and European languages, most of which largely share its word order. Figure 1 contains an example of SOV reordering in Tamil. A second important phenomenon present in these languages is a high degree of morphological complexity relative to English (Figure 2). Indian languages can be highly agglutinative, which means that words are formed by concatenating morphological affixes that convey information such as tense, person"
W97-1010,C92-2066,0,0.0517135,"he basis of many popular language learning systems, examples of which include the Baum-Welch algorithm for estimating hidden Markov models (Baum, 1972) and the InsideOutside algorithm for estimating CFGs (Baker, 1990). As is well known, Bayes&apos; theorem takes the following form: P(H I D) = P(H)P(D I H) P(D) Smooth the resulting parameters in the hope that they back-off from the training data and apportion more of the probability mass to account for unseen material. Examples of the first approach can be seen most clearly with the usage of CNF grammars by the Inside-Outside algorithm (Pereira and Schabes, 1992, Lari and Young, 1990). A grammar in CNF does not contain rules of an arbitrary arity, and so when learning CNF grammars, the Inside-Outside algorithm cannot find the maximal likelihood estimation of some training set. The problem with this language restriction is that there is no a priori reason why one should settle with any particular limit on rule arity; some grammars mainly contain binary rules, but others (for example those implicitly within tree-banks) sometimes contain rules with many right-hand side categories. Any language restriction, in lieu of some theory of rule arity, must rema"
W97-1010,P92-1017,0,\N,Missing
W97-1010,H92-1024,0,\N,Missing
W97-1010,P96-1025,0,\N,Missing
W99-0708,J97-4005,0,0.0139649,"ext-free manner. One way to achieve this is as follows. For each category in the grammar that is distinct in terms of features, invent an atomic non-terminal symbol. With these atomic symbols, create a SCFG by mapping each category in a DCG rule to an atomic symbol, yielding a context free (backbone) grammar, and with this grammar, specify a SCFG, /Vii. Naturally, this is not the most accurate probabilistic model for feature-based grammars, but for the interim, is sufficient (see Abney for a good discussion of how one might define a more accurate probabilistic model for feature-based grammars [1]). SCFGs are standardly defined as follows. Let P(A -~ (~ ] A) be the probability of expanding (backbone) nonterminal symbol A with the (backbone) rule A --+ o when deriving some sentence si. The probability of the jth derivation of si is defined as the product of the probabilities of all backbone rules used in that derivation. T h a t is, if derivation j followed from an application of the rules A~ -~ c~ . . . . , A~ ~ a~, feature, in a given category, will take a certain value. Let P(v I fO be the probability that feature f takes the value v, in category i of all rules in the grammar. Each v"
W99-0708,P98-1010,0,0.0475963,"ovide a useful test-bed for exploring some of the (syntactic) factors involved with NP identification. By contrast, other approaches at NP identification more usually only consider lexical/part-of-speech influences. In this paper, we consider, from an estimation perspective, how dependent NPs are upon their (surrounding) syntactic context. We varied the information content of the training set and measured the effect this had upon NP identification accuracy. Results suggest that: 1 Introduction Identification of Noun Phrases (NPs) in free text has been tackled in a number of ways (for example, [25, 9, 2]). Usually however, only relatively simple NPs, such as 'base' NPs (NPs that do not contain nested NPs or postmodifying clauses) are recovered. The motivation for this decision seems to be pragmatic, driven in part by a lack of technology capable of parsing large quantities of free text. With the advent of broad coverage grammars (for example [15] and attendant efficient parsers [11], however, we need not make this restriction: we now can identify 'full' NPs, NPs that might contain pre and/or post-modifying complements, in free text. Full NPs m'e more interesting than base NPs to estimate: • U"
W99-0708,P93-1005,0,0.0234219,"of parses for a sentence and for computational reasons, we are unable to evaluate all the models encoded within these parses. We therefore use a probabilistic unpacking strategy that efficiently selects the n most likely parses, where n is much less thml the total number of parses possible for some sentence [11]. There is insufficient space here to describe how we rank parses, but the underlying parse selection model is based upon the SCFG used to evaluate models. Currently. it is not lexicalised, so parse selection performance is subject to the well-known limitations of non-lexicalised SCFGs [5]. Whilst estimating models, we simultaneously estimate the parse selection model in terms of tile parse used to produce the model picked. Probabilistic unpacking is crucial to our approach, and it is this that makes our learner computationally feasible. After extracting n parses, we can then go on to construct k I models, evaluate their posterior probabilities, and then select the model that maximises this term. However, as was shown by P-t-S, when training material consists of just (limited quantities) of raw text. classical, single model parameter estimation often results in a model that pro"
W99-0708,P92-1017,0,0.0298863,"sentence s~ in the list of sentences So... 's# ... sn. If a sentence s# cannot be generated (the grammar contained within the model lacks the required rules), we need to find a new model with a high, nonzero posterior probability given the sentences s o . . . s# seen so far. Our (for computational reasons, necessarily suboptimal) approach selects such a model by&quot; carrying out a local search over the space of models with a nonzero posterior probability, given all sentences see so far. We use a MDL-based prior to help us compute a posterior probability. Analogously to Pereira and Schabes (P+S) [23], we also constrain the search using parsed corpora. Unlike P + S , we not only use parsed corpora to constrain parameter estimation, we also use it to constrain model selection. We replace M0 with the newly constructed (locally) maximal a posterior model and after processing all sentences in this incremental manner, terminate with a model that generates all sentences seen in the training set. Key aspects of our approach are: • Learning from positive-only data. We do not require negative examples, nor do we require human intervention. This enables us to transparently use grammar learning as pa"
W99-0708,1997.iwpt-1.13,0,0.020493,"Missing"
W99-0708,W95-0107,0,0.0405414,"ovide a useful test-bed for exploring some of the (syntactic) factors involved with NP identification. By contrast, other approaches at NP identification more usually only consider lexical/part-of-speech influences. In this paper, we consider, from an estimation perspective, how dependent NPs are upon their (surrounding) syntactic context. We varied the information content of the training set and measured the effect this had upon NP identification accuracy. Results suggest that: 1 Introduction Identification of Noun Phrases (NPs) in free text has been tackled in a number of ways (for example, [25, 9, 2]). Usually however, only relatively simple NPs, such as 'base' NPs (NPs that do not contain nested NPs or postmodifying clauses) are recovered. The motivation for this decision seems to be pragmatic, driven in part by a lack of technology capable of parsing large quantities of free text. With the advent of broad coverage grammars (for example [15] and attendant efficient parsers [11], however, we need not make this restriction: we now can identify 'full' NPs, NPs that might contain pre and/or post-modifying complements, in free text. Full NPs m'e more interesting than base NPs to estimate: • U"
W99-0708,1997.iwpt-1.15,0,0.0152761,"l, single model parameter estimation often results in a model that produces worse parse selection results than when the estimation process is constrained to only consider derivations compatible with the parsecl corpora. In our context, we can use parsed corpora to constrain both parameter estimation and also model selection. We simply re-rank the n parses produced during model construction using a tree similarity metric that compares how 'close' an automatically constructed parse is to a manually written parse, and take the q parses that all minimise the metric and are all scored equally well [17]. From these q parses we can then build models as usual. When q = 1, there is no need to rely upon MDL-based model selection. Otherwise. when q is greater than one, we have a set of parses, all equally consistent with the manually created tree, and so fallback upon the usual model selection strate~.v. Our use of parsed corpora differs from P + S ' s in that we use it as a soft constraint: we may still keep parses even if they violate constraints in the manually constructed parse tree. The reason for this decision is that we do not construct all possible parses for a sentence., and so at times"
W99-0708,P98-1034,0,\N,Missing
W99-0708,C98-1034,0,\N,Missing
W99-0708,C98-1010,0,\N,Missing
