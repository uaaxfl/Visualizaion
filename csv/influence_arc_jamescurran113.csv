C04-1041,J99-2004,0,0.827347,"ation space using constraints on category combination. The result is an accurate wide-coverage CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms. 1 Introduction Lexicalised grammar formalisms such as Lexicalized Tree Adjoining Grammar (LTAG) and Combinatory Categorial Grammar (CCG) assign one or more syntactic structures to each word in a sentence which are then manipulated by the parser. Supertagging was introduced for LTAG as a way of increasing parsing efficiency by reducing the number of structures assigned to each word (Bangalore and Joshi, 1999). Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003). Supertagging accuracy is relatively high for manually constructed LTAGs (Bangalore and Joshi, 1999). However, for LTAGs extracted automatically from the Penn Treebank, performance is much lower (Chen et al., 1999; Chen et al., 2002). In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al., 2000). In this paper we demonstrate that CCG supertagging accuracy is not only sufficient for accurate and robust parsing using an automatically extract"
C04-1041,2000.iwpt-1.9,0,0.114603,"Missing"
C04-1041,E99-1025,0,0.0287391,"d Combinatory Categorial Grammar (CCG) assign one or more syntactic structures to each word in a sentence which are then manipulated by the parser. Supertagging was introduced for LTAG as a way of increasing parsing efficiency by reducing the number of structures assigned to each word (Bangalore and Joshi, 1999). Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003). Supertagging accuracy is relatively high for manually constructed LTAGs (Bangalore and Joshi, 1999). However, for LTAGs extracted automatically from the Penn Treebank, performance is much lower (Chen et al., 1999; Chen et al., 2002). In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al., 2000). In this paper we demonstrate that CCG supertagging accuracy is not only sufficient for accurate and robust parsing using an automatically extracted grammar, but also offers several practical advantages. James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Our wide-coverage CCG parser uses a log-linear model to select an analysis. The model paramaters are estimated using a discriminative"
C04-1041,W02-2236,0,0.0512536,"orial Grammar (CCG) assign one or more syntactic structures to each word in a sentence which are then manipulated by the parser. Supertagging was introduced for LTAG as a way of increasing parsing efficiency by reducing the number of structures assigned to each word (Bangalore and Joshi, 1999). Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003). Supertagging accuracy is relatively high for manually constructed LTAGs (Bangalore and Joshi, 1999). However, for LTAGs extracted automatically from the Penn Treebank, performance is much lower (Chen et al., 1999; Chen et al., 2002). In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al., 2000). In this paper we demonstrate that CCG supertagging accuracy is not only sufficient for accurate and robust parsing using an automatically extracted grammar, but also offers several practical advantages. James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Our wide-coverage CCG parser uses a log-linear model to select an analysis. The model paramaters are estimated using a discriminative method, that is, on"
C04-1041,W03-1013,1,0.301602,"using L - BFGS (Nocedal and Wright, 1999), an iterative algorithm from the numerical optimisation literature. The algorithm requires the gradient of the objective function, and the value of the objective function, at each iteration. Calculation of these values requires all derivations for each sentence in the training data. In Clark and Curran (2004) we describe efficient methods for performing the calculations using packed charts. However, a very large amount of memory is still needed to store the packed charts for the complete training data even though the representation is very compact; in Clark and Curran (2003) we report a memory usage of 30 GB. To handle this we have developed a parallel implementation of the estimation algorithm which runs on a Beowulf cluster. The need for large high-performance computing resources is a disadvantage of our earlier approach. In the next section we show how use of the supertagger, combined with normal-form constraints on the derivations, can significantly reduce the memory requirements for the model estimation. 4 Generating Parser Training Data Since the training data contains the correct lexical categories, we ensure the correct category is assigned to each word w"
C04-1041,P04-1014,1,0.60491,"on the word’s POS tag is used. The table demonstrates the significant reduction in the average number of categories that can be achieved through the use of a supertagger. To give one example, the number of categories in the tag dictionary’s entry for the word is is 45 (only considering categories which have appeared at least 10 times in the training data). However, in the sentence Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group., the supertagger correctly assigns 1 category to is for β = 0.1, and 3 categories for β = 0.01. 3 The Parser The parser is described in detail in Clark and Curran (2004). It takes POS tagged sentences as input with each word assigned a set of lexical categories. A packed chart is used to efficiently represent all of the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart. Clark and Curran (2004) evaluate a number of log-linear parsing models for CCG. In this paper we use the normal-form model, which defines probabilities with the conditional log-linear form in (1), where y is a derivation and x is a sentence. Features are defined in terms of the local trees in the derivation, including"
C04-1041,P02-1042,1,0.547487,"erivation and x is a sentence. Features are defined in terms of the local trees in the derivation, including lexical head information and wordword dependencies. The normal-form derivations in CCGbank provide the gold standard training data. The feature set we use is from the best performing normal-form model in Clark and Curran (2004). For a given sentence the output of the parser is a dependency structure corresponding to the most probable derivation, which can be found using the Viterbi algorithm. The dependency relations are defined in terms of the argument slots of CCG lexical categories. Clark et al. (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. 3.1 Model Estimation In Clark and Curran (2004) we describe a discriminative method for estimating the parameters of a log-linear parsing model. The estimation method maximises the following objective function: L0 (Λ) = L(Λ) − G(Λ) m n Y X λ2i = log PΛ (d j |S j ) − 2σ2 j=1 i=1 (2) The data consists of sentences S 1 , . . . , S m , together with gold standard normal-form derivations, d1 , . . . , dm . L(Λ) is the log-likelihood of model Λ, and G(Λ) is a Gaussian prior term used to avoid overfitting (n is the"
C04-1041,E03-1071,1,0.684772,"e CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms. 1 Introduction Lexicalised grammar formalisms such as Lexicalized Tree Adjoining Grammar (LTAG) and Combinatory Categorial Grammar (CCG) assign one or more syntactic structures to each word in a sentence which are then manipulated by the parser. Supertagging was introduced for LTAG as a way of increasing parsing efficiency by reducing the number of structures assigned to each word (Bangalore and Joshi, 1999). Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003). Supertagging accuracy is relatively high for manually constructed LTAGs (Bangalore and Joshi, 1999). However, for LTAGs extracted automatically from the Penn Treebank, performance is much lower (Chen et al., 1999; Chen et al., 2002). In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al., 2000). In this paper we demonstrate that CCG supertagging accuracy is not only sufficient for accurate and robust parsing using an automatically extracted grammar, but also offers several practical advantages. James R. Curran School of Inform"
C04-1041,P96-1011,0,0.297857,"how normal-form constraints can further reduce the derivation space. 4.1 Normal-Form Constraints As well as the supertagger, we use two additional strategies for reducing the derivation space. The first, following Hockenmaier (2003), is to only allow categories to combine if the combination has been seen in sections 2-21 of CCGbank. For example, NP/NP could combine with NP/NP according to CCG’s combinatory rules (by forward composition), but since this particular combination does not appear in CCGbank the parser does not allow it. The second strategy is to use Eisner’s normalform constraints (Eisner, 1996). The constraints SUPERTAGGING / PARSING CONSTRAINTS β = 0.01 → 0.05 → 0.1 CCGbank constraints Eisner constraints β = 0.05 → 0.1 USAGE DISK 17 GB 13 GB 9 GB 2 GB MEMORY 31 GB 23 GB 16 GB 4 GB Table 3: Space requirements for model training data prevent any constituent which is the result of a forward (backward) composition serving as the primary functor in another forward (backward) composition or a forward (backward) application. Eisner only deals with a grammar without type-raising, and so the constraints do not guarantee a normalform parse when using a grammar extracted from CCGbank. However"
C04-1041,hockenmaier-steedman-2002-acquiring,0,0.0823348,"Missing"
C04-1041,N04-1013,0,0.149848,"rk and Curran (2004). 5.2 Comparison with Other Work The only other work we are aware of to investigate the impact of supertagging on parsing efficiency is the work of Sarkar et al. (2000) for LTAG. Sarkar et al. did find that LTAG supertagging increased parsing speed, but at a significant cost in coverage: only 1,324 sentences out of a test set of 2,250 received a parse. The parse times reported are also not as good as those reported here: the time taken to parse the 2,250 test sentences was over 5 hours. 5 Multiplying by an estimate of the outside score may improve the efficacy of the beam. Kaplan et al. (2004) report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank. They also report speeds for the publicly available Collins parser (Collins, 1999): 2.8 sentences per second for the same set. The best speeds we have reported for the CCG parser are an order of magnitude faster. 6 Conclusions This paper has shown that by tightly integrating a supertagger with a CCG parser, very fast parse times can be achieved for Penn Treebank WSJ text. As far as we are aware, the times reported here are an order of"
C04-1041,C00-1085,0,0.0958703,"Missing"
C04-1041,W00-1605,0,0.0451858,". Supertagging was introduced for LTAG as a way of increasing parsing efficiency by reducing the number of structures assigned to each word (Bangalore and Joshi, 1999). Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003). Supertagging accuracy is relatively high for manually constructed LTAGs (Bangalore and Joshi, 1999). However, for LTAGs extracted automatically from the Penn Treebank, performance is much lower (Chen et al., 1999; Chen et al., 2002). In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al., 2000). In this paper we demonstrate that CCG supertagging accuracy is not only sufficient for accurate and robust parsing using an automatically extracted grammar, but also offers several practical advantages. James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Our wide-coverage CCG parser uses a log-linear model to select an analysis. The model paramaters are estimated using a discriminative method, that is, one which requires all incorrect parses for a sentence as well as the correct parse. Since an automatically extracted CCG grammar c"
C04-1041,J03-4003,0,\N,Missing
C04-1180,briscoe-carroll-2002-robust,0,0.0181404,"Missing"
C04-1180,E99-1042,0,0.00750892,"Missing"
C04-1180,2003.mtsummit-papers.6,0,0.0315977,"Missing"
C04-1180,A00-2018,0,0.0299319,"Missing"
C04-1180,W03-1013,1,0.567339,"mma and returns a sentential modifier of the same type. Type-raising is applied to the categories NP, PP and S adj NP (adjectival phrase), and is implemented by adding the relevant set of type-raised categories to the chart whenever an NP, PP or S adj NP is present. The sets of type-raised categories are based on the most commonly used typeraising rule instantiations in sections 2-21 of CCGbank, and currently contain 8 type-raised categories for NP and 1 each for PP and S adj NP. For a given sentence, the automatically extracted grammar can produce a very large number of derivations. Clark and Curran (2003) and Clark and Curran (2004b) describe how a packed chart can be used to efficiently represent the derivation space, and also efficient algorithms for finding the most probable derivation. The parser uses a log-linear model over normal-form derivations.3 Features are defined in terms of the local trees in the derivation, including lexical head information and word-word dependencies. The normal-form derivations in CCGbank provide the gold standard training data. For a given sentence, the output of the parser is a set of syntactic dependencies corresponding to the 3 most probable derivation. How"
C04-1180,C04-1041,1,0.441407,"es are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the target word’s context to dec"
C04-1180,P04-1014,1,0.669575,"es are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the target word’s context to dec"
C04-1180,P02-1042,1,0.779112,"of the derivation and of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger us"
C04-1180,hockenmaier-steedman-2002-acquiring,1,0.878485,"nd of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the tar"
C04-1180,P02-1043,1,0.826238,"nd of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the tar"
C04-1180,N04-1013,0,0.0413167,"Missing"
C04-1180,C02-1105,0,0.0414764,"Missing"
C04-1180,J03-4003,0,\N,Missing
C10-1122,boxwell-white-2008-projecting,0,0.0350592,"Missing"
C10-1122,D09-1126,1,0.86776,"a given language – the multi-modal CCG (Baldridge, 2002) allows these distinctions to be lexically speciﬁed. Introducing non-CCG rules decrease categorial ambiguity at the expense of deviating from the formalism. Hockenmaier and Steedman (2002) show that these greatly improve lexical coverage. Their analysis of English employs non-CCG rules to coerce a verb phrase headed by a participle (category S[ng]NP) to a post-nominal modiﬁer: S[ng]NP −→ NPNP (1) This frees verbs from having to possess a distinct category in each position, thus trading off lexical ambiguity for derivational ambiguity. Honnibal and Curran (2009) extended CCG with hat categories, enabling the lexical speciﬁcation of these unary type-change rules. Hockenmaier and Steedman (2002, 2007) developed CCGbank, the ﬁrst wide-coverage English CCG corpus, by converting 1.2 million words from the Wall Street Journal section of the PTB. CCGbank has made possible the development of widecoverage statistical parsers for CCG in English, notably C&C (Clark and Curran, 2007). 1 Abbreviations in this paper: The directionless slash | stands for one of {/, }. We also use the verbal category abbreviations VP ≡ SNP and TV ≡ (SNP)/NP. 3 Penn Chinese Treeba"
C10-1122,Y04-1016,0,0.0194995,"for a given formalism and language. An alternative to the enormous cost of hand-annotating a corpus for a speciﬁc formalism is to convert from an existing corpus. The Penn Treebank (PTB; Marcus et al., 1994) has been converted to HPSG (Miyao et al., 2004), LFG (Cahill et al., 2002), LTAG (Xia, 1999), and CCG (Hockenmaier, 2003). Dependency corpora, e.g. the German Tiger corpus, have also been converted (Hockenmaier, 2006). The Penn Chinese Treebank (PCTB; Xue et al., 2005) provides analyses for 770,000 words of Chinese. Existing PCTB conversions have targeted TAG (Chen et al., 2005) and LFG (Burke and Lam, 2004; Guo et al., 2007). We present Chinese CCGbank, a Chinese corpus of CCG derivations automatically induced from the PCTB. Combinatory Categorial Grammar (CCG; Steedman, 2000) is a lexicalised grammar formalism offering a uniﬁed account of local and nonlocal dependencies. We harness the facilities of 2 Combinatory Categorial Grammar (Steedman, 2000) is a lexicalised grammar formalism, with a transparent syntax-semantics interface, a ﬂexible view of constituency enabling concise accounts of various phenomena, and a consistent account of local/non-local dependencies. It consists of categories, wh"
C10-1122,P03-1056,0,0.0639373,"node and its children. Xue et al. developed this principle to assist conversions from the PTB, e.g. Hockenmaier (2003), in resolving argument/adjunct distinctions. PCTB derivations are pre-segmented, pretokenised, and POS tagged. Owing to the dearth of morphology in Chinese, the concept of part of speech is more ﬂuid than that of English – the word 比较 bijiao ‘compare’ might be glossed as a verb, adjective, adverb, or noun depending on its context. Noun/verb mis-taggings are a frequent error case for PCFG parsing on PCTB data, compounded in Chinese by the lack of function words and morphology (Levy and Manning, 2003). This ambiguity is better handled by the adaptive multitagging approach used by Clark and Curran (2007) for CCG supertagging, in which each lexical item is tagged with a set of CCG categories. We present our CCG analysis of Chinese syntax below, followed by our conversion algorithm. 2 We 1084 use the Penn Chinese Treebank 6.0 (LDC2007T36). 4 The syntax of Chinese 4.1 (5) (As for) trade, it has developed rapidly. Chinese is typologically SVO, with some OV elements (relative clauses, adjunct PPs and noun modiﬁers precede their heads). Numbers and determiners may not modify nouns directly; a mea"
C10-1122,J07-4004,1,0.952582,"ominal modiﬁer: S[ng]NP −→ NPNP (1) This frees verbs from having to possess a distinct category in each position, thus trading off lexical ambiguity for derivational ambiguity. Honnibal and Curran (2009) extended CCG with hat categories, enabling the lexical speciﬁcation of these unary type-change rules. Hockenmaier and Steedman (2002, 2007) developed CCGbank, the ﬁrst wide-coverage English CCG corpus, by converting 1.2 million words from the Wall Street Journal section of the PTB. CCGbank has made possible the development of widecoverage statistical parsers for CCG in English, notably C&C (Clark and Curran, 2007). 1 Abbreviations in this paper: The directionless slash | stands for one of {/, }. We also use the verbal category abbreviations VP ≡ SNP and TV ≡ (SNP)/NP. 3 Penn Chinese Treebank Xue et al. (2005) developed the Penn Chinese Treebank (PCTB), the ﬁrst syntactically annotated corpus for Chinese. The corpus includes newswire text, magazine articles, and transcribed speech.2 Xue et al. establishes several principles for a more disciplined and consistent style of annotation compared to the original PTB. These principles include complement/adjunct marking: allowing the recovery of predicate-arg"
C10-1122,P06-1064,0,0.105281,"r Chinese CCGbank will enable the development of similarly efﬁcient wide-coverage CCG parsers for Chinese. CCG Introduction An annotated corpus is typically used to develop statistical parsers for a given formalism and language. An alternative to the enormous cost of hand-annotating a corpus for a speciﬁc formalism is to convert from an existing corpus. The Penn Treebank (PTB; Marcus et al., 1994) has been converted to HPSG (Miyao et al., 2004), LFG (Cahill et al., 2002), LTAG (Xia, 1999), and CCG (Hockenmaier, 2003). Dependency corpora, e.g. the German Tiger corpus, have also been converted (Hockenmaier, 2006). The Penn Chinese Treebank (PCTB; Xue et al., 2005) provides analyses for 770,000 words of Chinese. Existing PCTB conversions have targeted TAG (Chen et al., 2005) and LFG (Burke and Lam, 2004; Guo et al., 2007). We present Chinese CCGbank, a Chinese corpus of CCG derivations automatically induced from the PCTB. Combinatory Categorial Grammar (CCG; Steedman, 2000) is a lexicalised grammar formalism offering a uniﬁed account of local and nonlocal dependencies. We harness the facilities of 2 Combinatory Categorial Grammar (Steedman, 2000) is a lexicalised grammar formalism, with a transparent s"
C10-1122,hockenmaier-steedman-2002-acquiring,0,0.0374941,"ors govern the interaction of categories for lexical items, while slashes specify argument directionality. The combinators allow us to reduce lexical ambiguity, by preserving a word’s canonical category even when displaced from its canonical position. This facility is a strength of CCG, but elevates its generative power to mild context-sensitivity. Some combinators may be disabled in a given language – the multi-modal CCG (Baldridge, 2002) allows these distinctions to be lexically speciﬁed. Introducing non-CCG rules decrease categorial ambiguity at the expense of deviating from the formalism. Hockenmaier and Steedman (2002) show that these greatly improve lexical coverage. Their analysis of English employs non-CCG rules to coerce a verb phrase headed by a participle (category S[ng]NP) to a post-nominal modiﬁer: S[ng]NP −→ NPNP (1) This frees verbs from having to possess a distinct category in each position, thus trading off lexical ambiguity for derivational ambiguity. Honnibal and Curran (2009) extended CCG with hat categories, enabling the lexical speciﬁcation of these unary type-change rules. Hockenmaier and Steedman (2002, 2007) developed CCGbank, the ﬁrst wide-coverage English CCG corpus, by converting 1"
C10-1122,J07-3004,0,0.296951,"ss of modiﬁers, they should receive a single category; Over-generation: the atom set should not overgeneralise to accept ungrammatical examples; Efﬁciency: the representation may be motivated by the needs of applications such as parsers. Table 2 shows the eight atomic categories chosen for our corpus. Two of these categories: LCP (localisers) and M (measure words) have variously been argued to be special sub-classes of nouns (Huang et al., 2008). However, based on our overgeneration criterion, we decided to represent these as atomic categories. We adopt the bare/non-bare noun distinction from Hockenmaier and Steedman (2007) on parsing efﬁciency grounds. Although they roughly correspond to English PPs, the distributional differences between PPs, LCPs and QPs justify their inclusion as atoms in Chinese. Future work in training a wide-coverage parser on Chinese CCGbank will evaluate the impact of these choices. Labelling algorithm We developed a recursive algorithm which applies one of several labelling functions based on the markers on a node and its children. The algorithm proceeds top-down and assigns a CCG category to every node. The markers on a node’s children are matched against the schema of Table 3, applyi"
C10-1122,P07-1031,1,0.883931,"Missing"
C10-1122,J08-2004,0,0.0608013,"Missing"
C10-1122,J93-2004,0,\N,Missing
C10-1122,P02-1043,0,\N,Missing
C10-2051,J99-2004,0,0.0568758,"intransitive verb like sleeping receives the same argument structure as the base form sleep, but with the appropriate inflectional feature. This scheme works well for rule-based parsers, but it is less well suited for statistical parsers, as the rules propose categories but do not help the model estimate their likelihood or assign them feature weights. Statistical parsers for lexicalised formalisms such as CCG are very sensitive to the number of categories in the lexicon and the complexity of the mapping between words and categories. The sub-task of assigning lexical categories, supertagging (Bangalore and Joshi, 1999), is most of the parsing task. Supertaggers mitigate sparse data problems by using a label frequency threshold to prune rare categories from the search space. Clark and Curran (2007) employ a tag dictionary that restricts the model to assigning word/category pairs seen in the training data for frequent words. The tag dictionary causes some level of undergeneration, because not all valid word/category pairs will occur in the limited training data available. The morphological tokens we introduce help to mitigate this, by bringing together what were distinct verbs and argument structures, using l"
C10-2051,J02-2002,0,0.0146751,"categories and a few inflectional types. 3 The features are necessary for satisfactory analyses. Without inflectional features, there is no Inflectional Categories We implement the morphemic categories that have been discussed in the CCG literature 446 be −ing good and −ing do good (S [b]NP )/ADJ (S [ng ]NP)(S [b]NP) ADJ conj (S [b]NP )/NP (S [ng ]NP)(S [b]NP) NP (S [ng]NP )/ADJ S [ng]NP <B× (S [ng]NP )/NP &gt; S [ng]NP (S [ng]NP )(S [ng]NP ) <B× &gt; <Φ&gt; < S [ng]NP Figure 1: A single inflection category (in bold) can serve many different argument structures. Freq. Category Example (Bozsahin, 2002; Cha et al., 2002). The inflected 32.964 (S [dcl ]NP )(S [b]NP ) He ran form is broken into two morphemes, and each is 11,431 (S [pss]NP )(S [b]NP ) He was run down assigned a category. The category for the inflec11,324 (S [ng]NP )(S [b]NP ) He was running 4,343 (S [pt]NP )(S [b]NP ) He has run tional suffix is a function from a category with the 3,457 (N /N )(S [b]NP ) the running man bare-form feature [b] to a category that has an in2,011 S [dcl ]S “..”, he says flectional feature. This prevents verbal categories 1,604 (S [dcl ]S )(S [b]S ) “..”, said the boy 169 (S [dcl ]"
C10-2051,J08-1002,0,0.0211112,"Missing"
C10-2051,C04-1041,1,0.886299,"Missing"
C10-2051,J07-4004,1,0.926601,", but it is less well suited for statistical parsers, as the rules propose categories but do not help the model estimate their likelihood or assign them feature weights. Statistical parsers for lexicalised formalisms such as CCG are very sensitive to the number of categories in the lexicon and the complexity of the mapping between words and categories. The sub-task of assigning lexical categories, supertagging (Bangalore and Joshi, 1999), is most of the parsing task. Supertaggers mitigate sparse data problems by using a label frequency threshold to prune rare categories from the search space. Clark and Curran (2007) employ a tag dictionary that restricts the model to assigning word/category pairs seen in the training data for frequent words. The tag dictionary causes some level of undergeneration, because not all valid word/category pairs will occur in the limited training data available. The morphological tokens we introduce help to mitigate this, by bringing together what were distinct verbs and argument structures, using lemmatisation and factoring inflection away from argument structures. The tag dictionaries for the inflectional morphemes will have very high coverage, because there are only a few in"
C10-2051,P06-1088,1,0.885699,"Missing"
C10-2051,J07-3004,0,0.155251,"ntence is associated with a category that specifies its argument structure and the type and features of the constituent that it heads. For instance, in might head a PP -typed constituent with one NP -typed argument, written as PP /NP . The / operator denotes an argument to the right;  denotes an argument to the left. For example, a transitive verb is a function from a rightward NP to and a leftward NP to a sentence, (S NP )/NP . The grammar consists of a few schematic rules to combine the categories: X /Y Y X /Y Y X Y Y /Z Y  X Y Y /Z X Y ⇒&gt; ⇒< ⇒&gt;B X X X /Z ⇒<B ⇒<B × X  X /Z CCGbank (Hockenmaier and Steedman, 2007) extends this grammar with a set of type-changing rules, designed to strike a better balance between sparsity in the category set and ambiguity in the grammar. We mark such productions TC. In wide-coverage descriptions, categories are generally modelled as typed feature structures (Shieber, 1986), rather than atomic symbols. This allows the grammar to include head indices, and to unify under-specified features. In our notation features are annotated in square-brackets, e.g. S [dcl ]. Head-finding indices are annotated on categories as subscripts, e.g. (NPy NPy )/NPz . We occasionally abbrevia"
C10-2051,D09-1126,1,0.856433,"Missing"
C10-2168,J99-2004,0,0.347837,"s on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al., 1998), and ﬁrst-order logical forms (Bos et al., 2004). One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical categories, or CCG supertags, with the words in a sentence (Steedman, 2000). Clark and Curran (2004) adapt the technique of supertagging (Bangalore and Joshi, 1999) to CCG, using a standard maximum entropy tagger to assign small sets of supertags to each word. The reduction in ambiguity resulting from the supertagging stage results in a surprisingly efﬁcient parser, given the rich structural output, operating at tens of newspaper sentences per second. In this paper we demonstrate that the CCG parser can be made more than twice as fast, with little or no loss in accuracy. A noteworthy feature of the CCG parser is that, after the supertagging 1471 Coling 2010: Poster Volume, pages 1471–1479, Beijing, August 2010 stage, the parser builds a complete packed c"
C10-2168,C04-1180,1,0.805962,"is a system that performs open information extraction on the web (Lin et al., 2009). However, the text processing that is performed by TextRunner, in particular the parsing, is rudimentary: ﬁnite-state shallow parsing technology that In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al., 1998), and ﬁrst-order logical forms (Bos et al., 2004). One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical categories, or CCG supertags, with the words in a sentence (Steedman, 2000). Clark and Curran (2004) adapt the technique of supertagging (Bangalore and Joshi, 1999) to CCG, using a standard maximum entropy tagger to assign small sets of supertags to each word. The reduction in ambiguity resulting from the supertagging stage results in a surprisingly efﬁcient parser, given the rich structural output, operating at tens of newspaper sentences per second. In this paper we demonstrate that the CCG pa"
C10-2168,P06-2006,0,0.0364055,"Missing"
C10-2168,P05-1022,0,0.187649,"ate that the CCG parser can be made more than twice as fast, with little or no loss in accuracy. A noteworthy feature of the CCG parser is that, after the supertagging 1471 Coling 2010: Poster Volume, pages 1471–1479, Beijing, August 2010 stage, the parser builds a complete packed chart, storing all sentences consistent with the assigned supertags and the parser’s CCG combinatory rules, with no chart pruning whatsoever. The use of chart pruning techniques, typically some form of beam search, is essential for practical parsing using Penn Treebank parsers (Collins, 1999; Petrov and Klein, 2007; Charniak and Johnson, 2005), as well as practical parsers based on linguistic formalisms, such as HPSG (Ninomiya et al., 2005) and LFG (Kaplan et al., 2004). However, in the CCG case, the use of the supertagger means that enough ambiguity has already been resolved to allow the complete chart to be represented. Despite the effectiveness of the supertagging stage, the number of derivations stored in a packed chart can still be enormous for typical newspaper sentences. Hence it is an obvious question whether chart pruning techniques can be profitably applied to the CCG parser. Some previous work (Djordjevic et al., 2007) h"
C10-2168,A00-2018,0,0.336784,"he supertagger output can be stored in a packed chart. However, wide-coverage CCG parsers still produce a very large number of derivations for typical newspaper or Wikipedia sentences. In this paper we investigate two forms of chart pruning, and develop a novel method for pruning complete cells in a parse chart. The result is a widecoverage CCG parser that can process almost 100 sentences per second, with little or no loss in accuracy over the baseline with no pruning. Many of the popular wide-coverage parsers available today operate at around one newspaper sentence per second (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). Introduction Many NLP tasks and applications require the processing of massive amounts of textual data. For example, knowledge acquisition efforts can involve processing billions of words of text (Curran, 2004). Also, the increasing need to process large amounts of web data places an efﬁciency"
C10-2168,C04-1041,1,0.967384,"e-state shallow parsing technology that In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al., 1998), and ﬁrst-order logical forms (Bos et al., 2004). One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical categories, or CCG supertags, with the words in a sentence (Steedman, 2000). Clark and Curran (2004) adapt the technique of supertagging (Bangalore and Joshi, 1999) to CCG, using a standard maximum entropy tagger to assign small sets of supertags to each word. The reduction in ambiguity resulting from the supertagging stage results in a surprisingly efﬁcient parser, given the rich structural output, operating at tens of newspaper sentences per second. In this paper we demonstrate that the CCG parser can be made more than twice as fast, with little or no loss in accuracy. A noteworthy feature of the CCG parser is that, after the supertagging 1471 Coling 2010: Poster Volume, pages 1471–1479, B"
C10-2168,P10-1036,1,0.767973,"Missing"
C10-2168,N06-1020,0,0.0609905,"taggers. 5.2.1 Final experiments using gold training and self training In this section we report our ﬁnal tests using Wikipedia data. We used two methods to derive training data for the taggers. The ﬁrst is the standard method, which is to transform gold-standard parse trees into begin and end tag sequences. This method is the method that we used for all previous experiments, and we call it “gold training”. In addition to gold training, we also investigate an alternative method, which is to obtain training data for the taggers from the output of the parser itself, in a form of self-training (McClosky et al., 2006). The intuition is that the tagger will learn what constituents a trained parser will eventually choose, and as long as the constituents favoured by the parsing model are not pruned, no reduction in accuracy can occur. There is the potential for an increase in speed, however, due to the pruning effect. For gold training, we used sections 02-21 of 1477 Model baseline binary gold binary 40K binary 200K binary 1M level gold level 40K level 200K level 1M Speed 47.6 80.8 75.5 77.4 78.6 93.7 92.8 92.5 96.6 CCGbank) did not improve the self-training results. We did see the usual speed improvements fr"
C10-2168,W05-1511,0,0.0222118,"orthy feature of the CCG parser is that, after the supertagging 1471 Coling 2010: Poster Volume, pages 1471–1479, Beijing, August 2010 stage, the parser builds a complete packed chart, storing all sentences consistent with the assigned supertags and the parser’s CCG combinatory rules, with no chart pruning whatsoever. The use of chart pruning techniques, typically some form of beam search, is essential for practical parsing using Penn Treebank parsers (Collins, 1999; Petrov and Klein, 2007; Charniak and Johnson, 2005), as well as practical parsers based on linguistic formalisms, such as HPSG (Ninomiya et al., 2005) and LFG (Kaplan et al., 2004). However, in the CCG case, the use of the supertagger means that enough ambiguity has already been resolved to allow the complete chart to be represented. Despite the effectiveness of the supertagging stage, the number of derivations stored in a packed chart can still be enormous for typical newspaper sentences. Hence it is an obvious question whether chart pruning techniques can be profitably applied to the CCG parser. Some previous work (Djordjevic et al., 2007) has investigated this question but with little success. In this paper we investigate two types of ch"
C10-2168,J07-4004,1,0.948597,"essing of massive amounts of textual data. For example, knowledge acquisition efforts can involve processing billions of words of text (Curran, 2004). Also, the increasing need to process large amounts of web data places an efﬁciency demand on existing NLP tools. TextRunner, for example, is a system that performs open information extraction on the web (Lin et al., 2009). However, the text processing that is performed by TextRunner, in particular the parsing, is rudimentary: ﬁnite-state shallow parsing technology that In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al., 1998), and ﬁrst-order logical forms (Bos et al., 2004). One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical categories, or CCG supertags, with the words in a sentence (Steedman, 2000). Clark and Curran (2004) adapt the technique of supertagging (Bangalore and Joshi, 1999) to CCG, using a standard maximum entropy tagger to a"
C10-2168,C04-1010,0,0.0481003,"in a parse chart. The result is a widecoverage CCG parser that can process almost 100 sentences per second, with little or no loss in accuracy over the baseline with no pruning. Many of the popular wide-coverage parsers available today operate at around one newspaper sentence per second (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). Introduction Many NLP tasks and applications require the processing of massive amounts of textual data. For example, knowledge acquisition efforts can involve processing billions of words of text (Curran, 2004). Also, the increasing need to process large amounts of web data places an efﬁciency demand on existing NLP tools. TextRunner, for example, is a system that performs open information extraction on the web (Lin et al., 2009). However, the text processing that is performed by TextRunner, in particular the parsing, is rudimentary: ﬁnite-state shallow parsing technology that In this paper"
C10-2168,N07-1051,0,0.469875,"utput can be stored in a packed chart. However, wide-coverage CCG parsers still produce a very large number of derivations for typical newspaper or Wikipedia sentences. In this paper we investigate two forms of chart pruning, and develop a novel method for pruning complete cells in a parse chart. The result is a widecoverage CCG parser that can process almost 100 sentences per second, with little or no loss in accuracy over the baseline with no pruning. Many of the popular wide-coverage parsers available today operate at around one newspaper sentence per second (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). Introduction Many NLP tasks and applications require the processing of massive amounts of textual data. For example, knowledge acquisition efforts can involve processing billions of words of text (Curran, 2004). Also, the increasing need to process large amounts of web data places an efﬁciency demand on existing NLP t"
C10-2168,E03-1071,1,0.80584,"Missing"
C10-2168,W96-0213,0,0.368336,"Missing"
C10-2168,N09-1073,0,0.236863,"epresented. Despite the effectiveness of the supertagging stage, the number of derivations stored in a packed chart can still be enormous for typical newspaper sentences. Hence it is an obvious question whether chart pruning techniques can be profitably applied to the CCG parser. Some previous work (Djordjevic et al., 2007) has investigated this question but with little success. In this paper we investigate two types of chart pruning: a standard beam search, similar to that used in the Collins parser (Collins, 1999), and a more aggressive strategy in which complete cells are pruned, following Roark and Hollingshead (2009). Roark and Hollingshead use a ﬁnite-state tagger to decide which words in a sentence can end or begin constituents, from which whole cells in the chart can be removed. We develop a novel extension to this approach, in which a tagger is trained to infer the maximum length constituent that can begin or end at a particular word. These lengths can then be used in a more agressive pruning strategy which we show to be signiﬁcantly more effective than the basic approach. Both beam search and cell pruning are highly effective, with the resulting CCG parser able to process almost 100 sentences per sec"
C10-2168,W07-2206,1,0.892852,"harniak and Johnson, 2005), as well as practical parsers based on linguistic formalisms, such as HPSG (Ninomiya et al., 2005) and LFG (Kaplan et al., 2004). However, in the CCG case, the use of the supertagger means that enough ambiguity has already been resolved to allow the complete chart to be represented. Despite the effectiveness of the supertagging stage, the number of derivations stored in a packed chart can still be enormous for typical newspaper sentences. Hence it is an obvious question whether chart pruning techniques can be profitably applied to the CCG parser. Some previous work (Djordjevic et al., 2007) has investigated this question but with little success. In this paper we investigate two types of chart pruning: a standard beam search, similar to that used in the Collins parser (Collins, 1999), and a more aggressive strategy in which complete cells are pruned, following Roark and Hollingshead (2009). Roark and Hollingshead use a ﬁnite-state tagger to decide which words in a sentence can end or begin constituents, from which whole cells in the chart can be removed. We develop a novel extension to this approach, in which a tagger is trained to infer the maximum length constituent that can be"
C10-2168,J07-3004,0,0.0352215,"with pointers to the children used in the creation. Equivalence is deﬁned in terms of the category and head of the constituent, to enable the Viterbi algorithm to efﬁciently ﬁnd the highest scoring derivation.1 A textbook treatment of CKY applied to statistical parsing is given in Jurafsky and Martin (2000). 2 We performed efﬁciency and accuracy tests on newspaper and Wikipedia data. For the newspaper data, we used the standard test sections from The CCG Parser The parser is described in detail in Clark and Curran (2007). It is based on CCGbank, a CCG version of the Penn Treebank developed by Hockenmaier and Steedman (2007). 3 Data and Evaluation Metrics 1 Use of the Viterbi algorithm in this way requires the features in the parser model to be local to a single rule application; Clark and Curran (2007) has more discussion. 1472 (ncmod num hundred 1 Seven 0) (conj and 2 sixty-one 3) (conj and 2 hundred 1) (dobj in 6 total 7) (ncmod made 5 in 6) (aux made 5 were 4) (ncsubj made 5 and 2 obj) (passive made 5) β Baseline 0.001 0.002 0.005 0.01 Seven hundred and sixty-one were made in total. Figure 1: Example Wikipedia test sentence annotated with grammatical relations. CCGbank. Following Clark and Curran (2007) we us"
C10-2168,J00-4006,0,0.00657188,"starting with two-word constituents (assuming the supertagging phase has been completed), incrementally increasing the span until the whole sentence is covered. The chart is packed in the standard sense that any two equivalent constituents created during the parsing process are placed in the same equivalence class, with pointers to the children used in the creation. Equivalence is deﬁned in terms of the category and head of the constituent, to enable the Viterbi algorithm to efﬁciently ﬁnd the highest scoring derivation.1 A textbook treatment of CKY applied to statistical parsing is given in Jurafsky and Martin (2000). 2 We performed efﬁciency and accuracy tests on newspaper and Wikipedia data. For the newspaper data, we used the standard test sections from The CCG Parser The parser is described in detail in Clark and Curran (2007). It is based on CCGbank, a CCG version of the Penn Treebank developed by Hockenmaier and Steedman (2007). 3 Data and Evaluation Metrics 1 Use of the Viterbi algorithm in this way requires the features in the parser model to be local to a single rule application; Clark and Curran (2007) has more discussion. 1472 (ncmod num hundred 1 Seven 0) (conj and 2 sixty-one 3) (conj and 2 h"
C10-2168,N04-1013,0,\N,Missing
C10-2168,J03-4003,0,\N,Missing
C12-1018,A00-2018,0,0.293304,"that follows the top-down approach, so we can later constrain it and compare it with the RNN model. 284 In this hypothetical model, the structure and meaning of any sentence can be represented by a 100 dimensional root vector. This root vector generates two child vectors, one representing a span of text on the left, and one representing a span on the right, and each of these child vectors continue generating their own two children vectors until some stopping process makes leaf vectors that generate words. In this way, the process implements a top down generative conditional probability model (Charniak, 2000) in which the children are conditionally dependent on their parents. To parse of a piece of text, samples from every possible root node vector generate every possible tree and their resulting texts. These trees and latent variables are then selected only where the generated text equals the supplied text. From this sample, the most likely parse is determined to be the most likely tree that generated the text. The parameters of this hypothetical model would be tuned to create an optimum distribution at the root node, and an optimum conditional child generation distribution and stopping process t"
C12-1018,P05-1022,0,0.0989671,"Missing"
C12-1018,P96-1041,0,0.0704643,"learn 16 million transition probabilities. This makes computation slow. Other parsers like the C&C parser (Clark and Curran, 2004) speed up this process by effectively limiting the transition space through operator rules that reduce the transition space. 2.2 Word Vectors Instead of representing the sparse high dimensional distributions of word selection, classes and transitions directly, the word-vector approach attempts to encode such sparse distributions into a much shorter (say 100 dimensions) dense vectors of latent states. This method was used in the adjacent field of language modeling (Chen and Goodman, 1996), which aims to predict the smoothed frequency of n-gram distributions. Using Neural Networks in a method similar to Principle Component Analysis (Pearson, 1901), Mnih and Hinton (2007) show a log-bilinear model having a low perplexity in predicting the last word of an n-gram. This approach effectively encodes words into same-sized word vectors which can be combined to maximally represent n-gram distributional information through a neural network. 2.3 Neural Networks Neural Networks are the natural extension of logistic regression that can be used to transform word-vectors into sub-class distr"
C12-1018,P04-1014,1,0.715191,"de-outside algorithm to calculate the distribution of latent subcategories from transition probabilities, β: #{A x → B y Cz } β(A x → B y Cz ) := P y 0 z 0 {A x → B y 0 Cz 0 } (1) The algorithm is an application of the Expectation Maximization algorithm (Dempster et al., 1977) for a tree based graphical model. Each tree node maintains a distribution of being in each class with probability A x . The Berkeley parser uses about 4000 different classes, requiring the optimization algorithm to learn 16 million transition probabilities. This makes computation slow. Other parsers like the C&C parser (Clark and Curran, 2004) speed up this process by effectively limiting the transition space through operator rules that reduce the transition space. 2.2 Word Vectors Instead of representing the sparse high dimensional distributions of word selection, classes and transitions directly, the word-vector approach attempts to encode such sparse distributions into a much shorter (say 100 dimensions) dense vectors of latent states. This method was used in the adjacent field of language modeling (Chen and Goodman, 1996), which aims to predict the smoothed frequency of n-gram distributions. Using Neural Networks in a method si"
C12-1018,P07-2009,1,0.825929,"Missing"
C12-1018,P11-2003,0,0.0137922,"should provide a good solution of P(T1 ), the mean-field approximation diverges sufficiently enough that incorporating contextual features makes a significant improvement to the results (Socher et al., 2010). This suggests that a superior scoring classifier might also further improve the results. Recalling that RBM s are better at modeling general distributions, and that when given the complementary prior, the probability of P(T1 ) takes the form of Equation (17), this motivates that s(p) would be better modeled by a Restricted Boltzmann Machines (RBM). RBM s have been used before in parsing (Garg and Henderson, 2011). They aim to model a generative probability P(x, h) ∝ exp(xW h) and are often trained through Gibbs sampling, one layer at a time. Layer-wise training is harder for a recursive model, however, they can be used to model the distribution of the scoring function s(p) in the approximate RNN model: s(p) ∝ X exp(hU p + ap + bh) h 288 (24) 4.2.1 Configuration RBM s can be used in several configurations for modeling the probability of s(p). One method involves using two RBMs, one with energy functions E1 = h1 U1 p to model the probability that the parent node is part of the correct parse, and one wit"
C12-1018,P03-1054,0,0.00901528,"wide range of conditional tree-based models. 2 Background The Charniak (2000) parser represents a parse probability as the product of the top down production probabilities of a parse. One intuition is that the head node represents the entire sentence, and lower nodes are the probabilities of expressing a span of the text in each possible way. This makes each parse a downwards pointing conditional graphical model, and as explained by Charniak (1997), parser performance increases as more conditional information is used in calculating these production probabilities. 280 The more recent Stanford (Klein and Manning, 2003) and Berkeley (Petrov and Klein, 2007) parsers use increasingly fine grained sub-class schemes to convey rich information to each parse node to increase performance. 2.1 Berkeley Parser The Berkeley parser solves the graphical model using the inside-outside algorithm to calculate the distribution of latent subcategories from transition probabilities, β: #{A x → B y Cz } β(A x → B y Cz ) := P y 0 z 0 {A x → B y 0 Cz 0 } (1) The algorithm is an application of the Expectation Maximization algorithm (Dempster et al., 1977) for a tree based graphical model. Each tree node maintains a distribution o"
C12-1018,J93-2004,0,0.0633717,"Missing"
C12-1018,N06-1020,0,0.157706,"Missing"
C12-1018,N07-1051,0,0.0546625,"dels. 2 Background The Charniak (2000) parser represents a parse probability as the product of the top down production probabilities of a parse. One intuition is that the head node represents the entire sentence, and lower nodes are the probabilities of expressing a span of the text in each possible way. This makes each parse a downwards pointing conditional graphical model, and as explained by Charniak (1997), parser performance increases as more conditional information is used in calculating these production probabilities. 280 The more recent Stanford (Klein and Manning, 2003) and Berkeley (Petrov and Klein, 2007) parsers use increasingly fine grained sub-class schemes to convey rich information to each parse node to increase performance. 2.1 Berkeley Parser The Berkeley parser solves the graphical model using the inside-outside algorithm to calculate the distribution of latent subcategories from transition probabilities, β: #{A x → B y Cz } β(A x → B y Cz ) := P y 0 z 0 {A x → B y 0 Cz 0 } (1) The algorithm is an application of the Expectation Maximization algorithm (Dempster et al., 1977) for a tree based graphical model. Each tree node maintains a distribution of being in each class with probability"
C12-1018,D11-1014,0,0.0247645,"Missing"
C12-1018,P07-1080,0,0.28054,"Missing"
C12-1088,P06-2006,0,0.0253461,"reebank WSJ data to CCG derivations and dependencies (Hockenmaier and Steedman, 2007). We use the normal-form model described in Clark and Curran (2007), which models the probability of derivations. We also follow the convention of using section 00 of CCGbank as development data, sections 02-21 as training data, and section 23 for final testing. The standard evaluation metric is labeled dependency recovery, as described by Clark and Hockenmaier (2002). Clark and Curran (2007) develop a conversion from CCG dependencies to Briscoe and Carrollstyle grammatical relations (GRs) (King et al., 2003; Briscoe and Carroll, 2006). GRs provide a useful abstraction as they allow the conflation of many CCG dependencies that are semantically similar but structurally different. For example, since subcategorization information is fully specified in categories, the verb-subject relationship is expressed in many different forms in CCG depending on the transitivity of the verb. In the GR scheme, they map to a general ncsubj dependency, echoing the underlying similarity between the CCG dependencies. Rimell and Clark (2009) adapt the C&C parsing for the biomedical domain, and in the process they developed a mapping from CCG depe"
C12-1088,P05-1022,1,0.71672,"Missing"
C12-1088,J07-4004,1,0.92699,"e mediating category for long-range dependencies. 2.3 CCG parsing The C&C parser is a fast and accurate wide-coverage CCG parser. It is a two-stage system, where a supertagger assigns probable categories to words in a sentence and the parser combines them using the CKY algorithm. The parser has been found to be particularly accurate at recovering long-range dependencies (Clark et al., 2002; Rimell et al., 2009). C & C is trained on CCGbank, a conversion of the Penn Treebank WSJ data to CCG derivations and dependencies (Hockenmaier and Steedman, 2007). We use the normal-form model described in Clark and Curran (2007), which models the probability of derivations. We also follow the convention of using section 00 of CCGbank as development data, sections 02-21 as training data, and section 23 for final testing. The standard evaluation metric is labeled dependency recovery, as described by Clark and Hockenmaier (2002). Clark and Curran (2007) develop a conversion from CCG dependencies to Briscoe and Carrollstyle grammatical relations (GRs) (King et al., 2003; Briscoe and Carroll, 2006). GRs provide a useful abstraction as they allow the conflation of many CCG dependencies that are semantically similar but str"
C12-1088,P02-1042,0,0.034116,"equire co-indexation in phrase-structure parses. These dependencies have the following form: 〈to , PP/NP1 , 1, report , (NPNP)/(S[dcl]/NP)〉, which includes the head word, its category, the argument slot, argument word, and the mediating category for long-range dependencies. 2.3 CCG parsing The C&C parser is a fast and accurate wide-coverage CCG parser. It is a two-stage system, where a supertagger assigns probable categories to words in a sentence and the parser combines them using the CKY algorithm. The parser has been found to be particularly accurate at recovering long-range dependencies (Clark et al., 2002; Rimell et al., 2009). C & C is trained on CCGbank, a conversion of the Penn Treebank WSJ data to CCG derivations and dependencies (Hockenmaier and Steedman, 2007). We use the normal-form model described in Clark and Curran (2007), which models the probability of derivations. We also follow the convention of using section 00 of CCGbank as development data, sections 02-21 as training data, and section 23 for final testing. The standard evaluation metric is labeled dependency recovery, as described by Clark and Hockenmaier (2002). Clark and Curran (2007) develop a conversion from CCG dependenci"
C12-1088,W08-1301,0,0.102231,"Missing"
C12-1088,W11-2924,0,0.0347568,"Missing"
C12-1088,J07-3004,0,0.0338431,"ludes the head word, its category, the argument slot, argument word, and the mediating category for long-range dependencies. 2.3 CCG parsing The C&C parser is a fast and accurate wide-coverage CCG parser. It is a two-stage system, where a supertagger assigns probable categories to words in a sentence and the parser combines them using the CKY algorithm. The parser has been found to be particularly accurate at recovering long-range dependencies (Clark et al., 2002; Rimell et al., 2009). C & C is trained on CCGbank, a conversion of the Penn Treebank WSJ data to CCG derivations and dependencies (Hockenmaier and Steedman, 2007). We use the normal-form model described in Clark and Curran (2007), which models the probability of derivations. We also follow the convention of using section 00 of CCGbank as development data, sections 02-21 as training data, and section 23 for final testing. The standard evaluation metric is labeled dependency recovery, as described by Clark and Hockenmaier (2002). Clark and Curran (2007) develop a conversion from CCG dependencies to Briscoe and Carrollstyle grammatical relations (GRs) (King et al., 2003; Briscoe and Carroll, 2006). GRs provide a useful abstraction as they allow the confla"
C12-1088,W05-1506,0,0.0914008,"Missing"
C12-1088,W07-2416,0,0.0531777,"r our experiments. In contrast to other grammars used in this paper, this dependency scheme contains only unlabeled word-word arcs. Stanford: de Marneffe and Manning (2008) introduced the dependency scheme used in the Stanford parser2 . We used the Stanford parser’s built-in converter to transform Penn Treebank trees into dependencies. The Stanford scheme has different variants; for this work we use the basic projective tree schema. LTH: The LTH dependency scheme was developed with the aim of making better use of the linguistic information present in the Penn Treebank from version II onwards (Johansson and Nugues, 2007). We generated these dependencies using the LTH converter3 over the NPbracketed version of the Penn Treebank described by Vadas and Curran (2007). The converter was configured to produce a functional rather than lexical DG. Fanse: Another conversion of the Penn Treebank with more fine-grained labels was presented in Tratz and Hovy (2011). The Fanse scheme is linguistically rich, featuring both non-projective dependencies and shallow semantic interpretation in its analyses. We used the freely available converter4 , which also requires the Vadas and Curran (2007) NP-bracketed Penn Treebank. Figu"
C12-1088,W03-2401,0,0.0367326,"rsion of the Penn Treebank WSJ data to CCG derivations and dependencies (Hockenmaier and Steedman, 2007). We use the normal-form model described in Clark and Curran (2007), which models the probability of derivations. We also follow the convention of using section 00 of CCGbank as development data, sections 02-21 as training data, and section 23 for final testing. The standard evaluation metric is labeled dependency recovery, as described by Clark and Hockenmaier (2002). Clark and Curran (2007) develop a conversion from CCG dependencies to Briscoe and Carrollstyle grammatical relations (GRs) (King et al., 2003; Briscoe and Carroll, 2006). GRs provide a useful abstraction as they allow the conflation of many CCG dependencies that are semantically similar but structurally different. For example, since subcategorization information is fully specified in categories, the verb-subject relationship is expressed in many different forms in CCG depending on the transitivity of the verb. In the GR scheme, they map to a general ncsubj dependency, echoing the underlying similarity between the CCG dependencies. Rimell and Clark (2009) adapt the C&C parsing for the biomedical domain, and in the process they devel"
C12-1088,J93-2004,0,0.04036,"Missing"
C12-1088,P05-1012,0,0.125133,"h-malt:VB:IN Table 2: The generation process for pair-dependency features. Each feature template follows a similar pattern. parser-predicted DG dependencies. The gold experiment allowed us to investigate the upper performance bound of our reranking technique and of our DG-derived features. We evaluate using the standard CCG dependency recovery metric over section 00 of CCGbank. We use the reranker settings that Ng et al. (2010) found to provide best performance: regression learning, 10-best mode, and no feature pruning. We use the same experimental settings reported in Nivre et al. (2010) and McDonald et al. (2005) for the Maltparser and MSTparser respectively. This means that both parsers will produce a projective dependency tree for each scheme that we experimented with. 5.1 Overall Comparison section 00 (dev) C & C normal ’07 Reranker ’10 CoNLL features Stanford features Gold LTH features Fanse features CoNLL features Stanford features Malt Predicted LTH features Fanse features CoNLL features Stanford features MST Predicted LTH features Fanse features Baselines Table 3: Parsing performance for the four over section 00 of CCGbank DG LP 87.27 87.57 89.17 88.97 88.95 89.61 87.74 87.80 87.43 87.82 87.65"
C12-1088,D07-1013,0,0.0671167,"Missing"
C12-1088,P12-1052,1,0.872524,"Missing"
C12-1088,U10-1014,1,0.924719,"e in a base parser. More informative features can be considered in reranking as the entire parse tree is available, as opposed to the fragments considered in parsing. In this paper, we propose a simple method for improving the performance of the C&C Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2007). We parse sentences using the C&C n-best parser and a 1-best dependency grammar (DG) parser, and generate DG-derived features by comparing the extracted dependencies from the C&C parser with the DG dependencies. We then incorporate the DG-derived features into the CCG reranker of Ng et al. (2010) to reorder the n-best CCG parses using the external parse information. We experiment with both the Maltparser (Nivre et al., 2007b) and the MSTparser (McDonald et al., 2005) as the DG parser. This is the first cross-formalism parser combination experiment for CCG parsing that we are aware of, combining the features and strengths of two different formalisms together. Previous work has shown that dependency parsers such as the Maltparser perform better on short-range dependencies (McDonald and Nivre, 2007), whereas the C&C parser deals with long-range dependencies more reliably (Clark et al., 2"
C12-1088,C10-1094,0,0.0127589,"atch-malt:see:IN, nomatch-malt:VB:IN Table 2: The generation process for pair-dependency features. Each feature template follows a similar pattern. parser-predicted DG dependencies. The gold experiment allowed us to investigate the upper performance bound of our reranking technique and of our DG-derived features. We evaluate using the standard CCG dependency recovery metric over section 00 of CCGbank. We use the reranker settings that Ng et al. (2010) found to provide best performance: regression learning, 10-best mode, and no feature pruning. We use the same experimental settings reported in Nivre et al. (2010) and McDonald et al. (2005) for the Maltparser and MSTparser respectively. This means that both parsers will produce a projective dependency tree for each scheme that we experimented with. 5.1 Overall Comparison section 00 (dev) C & C normal ’07 Reranker ’10 CoNLL features Stanford features Gold LTH features Fanse features CoNLL features Stanford features Malt Predicted LTH features Fanse features CoNLL features Stanford features MST Predicted LTH features Fanse features Baselines Table 3: Parsing performance for the four over section 00 of CCGbank DG LP 87.27 87.57 89.17 88.97 88.95 89.61 87."
C12-1088,D09-1085,0,0.01536,"in phrase-structure parses. These dependencies have the following form: 〈to , PP/NP1 , 1, report , (NPNP)/(S[dcl]/NP)〉, which includes the head word, its category, the argument slot, argument word, and the mediating category for long-range dependencies. 2.3 CCG parsing The C&C parser is a fast and accurate wide-coverage CCG parser. It is a two-stage system, where a supertagger assigns probable categories to words in a sentence and the parser combines them using the CKY algorithm. The parser has been found to be particularly accurate at recovering long-range dependencies (Clark et al., 2002; Rimell et al., 2009). C & C is trained on CCGbank, a conversion of the Penn Treebank WSJ data to CCG derivations and dependencies (Hockenmaier and Steedman, 2007). We use the normal-form model described in Clark and Curran (2007), which models the probability of derivations. We also follow the convention of using section 00 of CCGbank as development data, sections 02-21 as training data, and section 23 for final testing. The standard evaluation metric is labeled dependency recovery, as described by Clark and Hockenmaier (2002). Clark and Curran (2007) develop a conversion from CCG dependencies to Briscoe and Carr"
C12-1088,P07-1079,0,0.223745,"endency, binary indicator features were generated based on our feature templates (depicted in Figure 5). These features represent fragments of one or more dependency arcs that the reranker learns to favour or disprefer. Each feature includes components specified by the template, a directionality marker, and all four combinations of the word and POS tag for the head and dependent. Some of our templates also generate additional features for each mismatching dependency, conjoined with a label indicating whether the dependency existed only in the CDP or the MDP. This approach differs from that of Sagae et al. (2007) since our reranker learns a separate penalty parameter for each combination of DG and CCG constructions as a feature of our regularised MaxEnt reranker model; these weights are learnt as part of the reranker training procedure. This enables our reranker to learn which DG constructions are most reliable and informative for CCG parsing, and which DG constructions should be ignored. Following are descriptions of our DG-derived feature templates, which correspond to various dependency relations shown in Figure 5: Pair-dependency features encode the head-dependent pair and a flag indicating a matc"
C12-1088,C04-1024,0,0.0699741,"Missing"
C12-1088,P07-1031,1,0.916549,"effe and Manning (2008) introduced the dependency scheme used in the Stanford parser2 . We used the Stanford parser’s built-in converter to transform Penn Treebank trees into dependencies. The Stanford scheme has different variants; for this work we use the basic projective tree schema. LTH: The LTH dependency scheme was developed with the aim of making better use of the linguistic information present in the Penn Treebank from version II onwards (Johansson and Nugues, 2007). We generated these dependencies using the LTH converter3 over the NPbracketed version of the Penn Treebank described by Vadas and Curran (2007). The converter was configured to produce a functional rather than lexical DG. Fanse: Another conversion of the Penn Treebank with more fine-grained labels was presented in Tratz and Hovy (2011). The Fanse scheme is linguistically rich, featuring both non-projective dependencies and shallow semantic interpretation in its analyses. We used the freely available converter4 , which also requires the Vadas and Curran (2007) NP-bracketed Penn Treebank. Figures 3 and 4 demonstrate some of the differences between the four dependency schemes. For instance, auxiliaries take the lexical verb as a depende"
C12-1088,C10-1011,0,\N,Missing
C12-1088,D11-1116,0,\N,Missing
C12-1088,D07-1096,0,\N,Missing
C14-1072,clarke-etal-2012-nlp,0,0.128303,"was developed. However, it appears to lag behind behind the Java API in development effort and usefulness, with many undocumented components, numerous external dependencies, and with substantial missing functionality provided by the Java API. Additionally, the C++ API is written in an non-idiomatic manner, making it harder for developers to use. Publicly available CL pipelining tools have emerged in recent years, providing a way to perform a wide range of CL processes over documents. The Stanford NLP pipeline1 is one such example, but is Java only and must be run on a single machine. CURATOR (Clarke et al., 2012) provides a cross-language NLP pipeline using Thrift to provide cross-language communication and RPC. CURATOR requires a server to coordinate the components within the pipeline. Using pipelining functionality within a framework often the inspection of per-component contributions more difficult. We are not aware of any DRFs which use a streaming model to utilise UNIX pipelines, a paradigm CL researchers are already familiar with. 3 The docrep document representation framework (/d6krEp/), a portmanteau of document representation, is a lightweight, efficient, and modern document representation fr"
C14-1072,P02-1022,0,0.217741,"ed over the original text; e.g. tokens, sentences, parts-of-speech, named entities, coreference relations, and trees. The scale and complexity of the data demands efficient representations. A document representation framework (DRF) should support the creation, storage, and retrieval of different annotation layers over collections of heterogeneous documents. DRFs typically store their annotations as stand-off annotations, treating the source document as immutable and annotations “stand-off” with offsets back into the document. Researchers may choose to use a heavy-weight DRF, for example GATE (Cunningham et al., 2002) or UIMA (G¨otz and Suhre, 2004), but this can require substantial investment to learn and apply the framework. Alternatively, researchers may “roll-their-own” framework for a particular project. While this is not inherently bad, our experience is that the scope of such smaller DRFs often creeps, without the benefits of the features and stability present in mature DRFs. Moreover, some DRFs are based on object serialisation, restricting the user to a specific language. In sum, while DRFs provide substantial benefits, they can come at an opportunity cost to valuable research time. DOCREP aims to"
C14-1072,P10-2013,0,0.017726,"notation formats have emerged over the years, including Annotation Graphs (AG) (Bird and Liberman, 1999), the Linguistic Annotation Format (LAF) (Ide and Romary, 2004, 2006), and more recently, the Graph Annotation Framework (GRAF) (Ide and Suderman, 2007). GRAF is a serialisation of the LAF model, using XML stand-off annotations to store layers of annotation. The GRAF representation is sufficiently abstract as to be used as a pivot format between other annotation schemes. Ide and Suderman (2009) use GRAF as an intermediate format to convert annotations between GATE and UIMA. The MASC corpus (Ide et al., 2010) has multiple layers of annotation which are distributed in GRAF. Neumann et al. (2013) provide insight into the effectiveness of GRAF as a format for corpus distribution when they import MASC into an annotation database. These linguistic annotation formalisations provide a useful set of requirements for DRFs. While these abstract formalisations are constructive from a theoretical perspective, they do not take into account the runtime performance of abstract representations, nor their ease of use for programmers. Several DRFs have been developed and used within the CL community. GATE (Cunningh"
C14-1072,ide-romary-2006-representing,0,0.069564,"Missing"
C14-1072,W07-1501,0,0.0306444,"Ireland, August 23-29 2014. will be useful for any researcher who wants rapid development with multi-layered annotation that performs well at scale, but at minimal technical cost. 2 Background Easily and efficiently storing and retrieving linguistic annotations over corpora is a core issue for datadriven linguistics. A number of attempts to formalise linguistic annotation formats have emerged over the years, including Annotation Graphs (AG) (Bird and Liberman, 1999), the Linguistic Annotation Format (LAF) (Ide and Romary, 2004, 2006), and more recently, the Graph Annotation Framework (GRAF) (Ide and Suderman, 2007). GRAF is a serialisation of the LAF model, using XML stand-off annotations to store layers of annotation. The GRAF representation is sufficiently abstract as to be used as a pivot format between other annotation schemes. Ide and Suderman (2009) use GRAF as an intermediate format to convert annotations between GATE and UIMA. The MASC corpus (Ide et al., 2010) has multiple layers of annotation which are distributed in GRAF. Neumann et al. (2013) provide insight into the effectiveness of GRAF as a format for corpus distribution when they import MASC into an annotation database. These linguistic"
C14-1072,W09-3004,0,0.0216432,"istic annotations over corpora is a core issue for datadriven linguistics. A number of attempts to formalise linguistic annotation formats have emerged over the years, including Annotation Graphs (AG) (Bird and Liberman, 1999), the Linguistic Annotation Format (LAF) (Ide and Romary, 2004, 2006), and more recently, the Graph Annotation Framework (GRAF) (Ide and Suderman, 2007). GRAF is a serialisation of the LAF model, using XML stand-off annotations to store layers of annotation. The GRAF representation is sufficiently abstract as to be used as a pivot format between other annotation schemes. Ide and Suderman (2009) use GRAF as an intermediate format to convert annotations between GATE and UIMA. The MASC corpus (Ide et al., 2010) has multiple layers of annotation which are distributed in GRAF. Neumann et al. (2013) provide insight into the effectiveness of GRAF as a format for corpus distribution when they import MASC into an annotation database. These linguistic annotation formalisations provide a useful set of requirements for DRFs. While these abstract formalisations are constructive from a theoretical perspective, they do not take into account the runtime performance of abstract representations, nor"
C14-1072,W13-2312,0,0.0163236,"d and Liberman, 1999), the Linguistic Annotation Format (LAF) (Ide and Romary, 2004, 2006), and more recently, the Graph Annotation Framework (GRAF) (Ide and Suderman, 2007). GRAF is a serialisation of the LAF model, using XML stand-off annotations to store layers of annotation. The GRAF representation is sufficiently abstract as to be used as a pivot format between other annotation schemes. Ide and Suderman (2009) use GRAF as an intermediate format to convert annotations between GATE and UIMA. The MASC corpus (Ide et al., 2010) has multiple layers of annotation which are distributed in GRAF. Neumann et al. (2013) provide insight into the effectiveness of GRAF as a format for corpus distribution when they import MASC into an annotation database. These linguistic annotation formalisations provide a useful set of requirements for DRFs. While these abstract formalisations are constructive from a theoretical perspective, they do not take into account the runtime performance of abstract representations, nor their ease of use for programmers. Several DRFs have been developed and used within the CL community. GATE (Cunningham et al., 2002; Cunningham, 2002) has a focus on the human annotation of textual docum"
C14-1072,P12-2045,1,0.773978,"and knowledge from web-based hyperlinks on the other. For some processes these data sources were to be treated identically, and for some differently. DOCREP’s extensibility easily supported this use-case, while providing a consistent polymorphic abstraction that made development straightforward, while incorporating many other layers of annotation such as extracted temporal relations. Separately, describing the relationship between a pair of documents in DOCREP was a challenging use-case that required more engineering and fore-thought than most DOCREP applications so far. Related publication: Nothman et al. (2012). 769 Named Entity Linking Our approach to NEL uses a pipeline of components and we initially wrote our own DRF using Python’s object serialisation. While this worked well initially, we accrued technical debt as we added features with minimal refactoring. Before too long, a substantial part of our experiment runtime was devoted to dataset loading and storage. DOCREP made this easier and using UNIX pipelines over structured document objects is a productive workflow. Related publications: Radford et al. (2012); Pink et al. (2013). Quote Extraction and Attribution For this task we performed exper"
C14-1072,P13-2092,1,0.849792,"Missing"
C14-1072,W13-3516,0,0.0438188,"Missing"
C14-1072,W09-1119,0,\N,Missing
C14-1072,W03-0804,0,\N,Missing
C14-1201,D08-1031,0,0.123919,"Missing"
C14-1201,N07-1030,0,0.0499756,"Missing"
C14-1201,D13-1203,0,0.109957,"Missing"
C14-1201,W12-4502,0,0.204589,"Missing"
C14-1201,J95-2003,0,0.71053,"Missing"
C14-1201,R11-1025,0,0.0355911,"Missing"
C14-1201,D13-1027,0,0.0927163,"Missing"
C14-1201,W11-1902,0,0.0884402,"Missing"
C14-1201,D12-1082,0,0.0292018,"Missing"
C14-1201,H05-1004,0,0.286595,"Missing"
C14-1201,P02-1014,0,0.151701,"Missing"
C14-1201,N06-1025,0,0.166303,"Missing"
C14-1201,W12-4501,0,0.124497,"Missing"
C14-1201,D10-1048,0,0.214412,"Missing"
C14-1201,D09-1101,0,0.0789233,"Missing"
C14-1201,N13-1071,0,0.0709447,"Missing"
C14-1201,P10-2029,0,0.137415,"Missing"
C14-1201,M95-1005,0,0.355224,"Missing"
C14-1201,C12-2136,0,0.048474,"Missing"
C14-1201,D08-1067,0,\N,Missing
C14-1201,J01-4004,0,\N,Missing
C14-1201,D13-1057,0,\N,Missing
D09-1126,P90-1024,0,0.0228518,"semantic categories. This isomorphism is disrupted by the addition of PSG rules, since the grammar is no longer lexicalised. Often, the rules can be semantically annotated, restoring the isomorphism; but sometimes, this cannot be done. For instance, the extraposition rule in Figure 2 transforms the NP category into SS. There is no syntactic argument on the NP category to map the dependency to, so the dependency cannot be created (and is in fact missing from CCGbank). 4.2 Lexical Rules and Zero Morphemes The CCGbank PSG extension is closely related to the zero morpheme categories proposed by Aone and Wittenburg (1990), which they suggest be compiled into unary type-changing rules for processing. At first glance, it seems that conceptualising the rules as zero morphemes offers a way to locate them in the lexicon, avoiding the linguistic difficulties of having a language-specific grammar. However, CCG aims to provide a transparent interface between the surface form and the semantic analysis, so epsilon categories, traces, movement rules and other unrealised structures are explicitly banned (Steedman, 2000). From a processing standpoint, if zero morpheme categories are not compiled into phrasestructure rules,"
D09-1126,E03-1036,0,0.650201,"inguistic minimalism. One aim of the theory is to explain universal constraints on natural language syntax, so the generative power of the formalism is intended to closely match what natural language seems to require. Steedman and Baldridge (2007) argue that the requirements can be fulfilled almost entirely by two basic rule types: application and composition. Direction specific instances of these types yields a grammar that consists of just six rules. Initially, it seemed that some of the rules had to be restricted to certain contexts, particularly in languages that did not allow scrambling. Baldridge and Kruijff (2003) have since shown that rules could be restricted lexically, using a hierarchy of slash subtypes. This relieved the need for any language specific meta-rules, allowing CCG to offer a completely universal grammar, and therefore a theory of the innate human language faculty. With a universal grammar, language specific variation is confined to the lexicon. A CCG lexical category is either an atomic type, like N, or a function that specifies an argument in a particular direction, and a result, like SNP (where S is the result, NP the argument, and  indicates the argument must be found to the left)"
D09-1126,J99-2004,0,0.152743,"lly, this mapping is lexically specified, by linking lexical entries to semantic analyses. This property, lexicalisation, is central to some of the linguistic theories behind deep grammars, particularly Combinatory Categorial Grammar (Steedman, 2000) and Lexicalised Tree Adjoining Grammar (Joshi, 1999). Lexicalisation can also help deep grammars achieve satisfactory parse times. Lexicalised grammars use few rules, which simply manipulate the lexical categories. The categories can be quickly assigned in a supertagging pre-process, dramatically reducing the search space the parser must explore (Bangalore and Joshi, 1999). Combinatory Categorial Grammar (CCG) is well suited to this strategy, and Clark and Curran (2007) have highlighted the division of labour between the parser and the supertagger as one of the The lion NP The lion NP was VP/VP waited, VP lying still VP VPVP lying still VPVP (VPVP)(VPVP) Analyses like these are problematic because the training data is unlikely to include examples of each word in every syntactic environment that requires a new category. Hockenmaier and Steedman’s (2007) solution was to add category specific phrase-structure rules to the grammar, which disrupts the linguisti"
D09-1126,J07-4004,1,0.931257,"ty, lexicalisation, is central to some of the linguistic theories behind deep grammars, particularly Combinatory Categorial Grammar (Steedman, 2000) and Lexicalised Tree Adjoining Grammar (Joshi, 1999). Lexicalisation can also help deep grammars achieve satisfactory parse times. Lexicalised grammars use few rules, which simply manipulate the lexical categories. The categories can be quickly assigned in a supertagging pre-process, dramatically reducing the search space the parser must explore (Bangalore and Joshi, 1999). Combinatory Categorial Grammar (CCG) is well suited to this strategy, and Clark and Curran (2007) have highlighted the division of labour between the parser and the supertagger as one of the The lion NP The lion NP was VP/VP waited, VP lying still VP VPVP lying still VPVP (VPVP)(VPVP) Analyses like these are problematic because the training data is unlikely to include examples of each word in every syntactic environment that requires a new category. Hockenmaier and Steedman’s (2007) solution was to add category specific phrase-structure rules to the grammar, which disrupts the linguistic principles of the formalism, and introduces over-generation and ambiguity as shown in Figure 1. T"
D09-1126,P96-1011,0,0.426888,"Missing"
D09-1126,hockenmaier-steedman-2002-acquiring,0,0.408922,"ctive resilient, attaching it as an argument using forward application. This prevents resilient from having to subcategorise for adjuncts, since they are optional. The problem is that unusually must subcategorise for the function of its head. If resilient changes function and becomes a noun modifier, its modifiers must change category too: (4) an unusually resilient strain NP/N (N/N)/(N/N) N/N N There is often a way to analyse around the need for type-changing operations in CCG. However, these solutions tend to cause new difficulties, and the resulting category ambiguity is quite problematic (Hockenmaier and Steedman, 2002). The fact is that form-to-function coercions are quite common in English, so the grammar needs a way to have a constituent be modified according to its form, before undergoing a type-change to its function category. One way to describe the problem is to say that CCG categories have an over-extended domain of locality (Joshi, 1999), the part of the derivation that it describes. A category should specify all and only the dependencies it governs, but CCG modifier categories are often forced to specify their heads’ dependencies as well. These undesirable notational dependencies can also prevent m"
D09-1126,J07-3004,0,0.389484,"ave since shown that rules could be restricted lexically, using a hierarchy of slash subtypes. This relieved the need for any language specific meta-rules, allowing CCG to offer a completely universal grammar, and therefore a theory of the innate human language faculty. With a universal grammar, language specific variation is confined to the lexicon. A CCG lexical category is either an atomic type, like N, or a function that specifies an argument in a particular direction, and a result, like SNP (where S is the result, NP the argument, and  indicates the argument must be found to the left). Hockenmaier and Steedman (2007) showed that a CCG corpus could be created by adapting the Penn Treebank (Marcus et al., 1993). CCGbank has since been used to train fast and accurate CCG parsers (Clark and Curran, 2007). The Need for Type-changing in CCG We argue that there is a clear need for some sort of type-changing mechanism in CCG. The practical need for this has been known since at least Hockenmaier (2003), who introduced a type-changing mechanism into CCGbank in order to control the problem referred to as modifier category proliferation. We briefly describe the problem, and then the prominent solutions that have been"
D09-1126,J93-2004,0,0.0321278,"ved the need for any language specific meta-rules, allowing CCG to offer a completely universal grammar, and therefore a theory of the innate human language faculty. With a universal grammar, language specific variation is confined to the lexicon. A CCG lexical category is either an atomic type, like N, or a function that specifies an argument in a particular direction, and a result, like SNP (where S is the result, NP the argument, and  indicates the argument must be found to the left). Hockenmaier and Steedman (2007) showed that a CCG corpus could be created by adapting the Penn Treebank (Marcus et al., 1993). CCGbank has since been used to train fast and accurate CCG parsers (Clark and Curran, 2007). The Need for Type-changing in CCG We argue that there is a clear need for some sort of type-changing mechanism in CCG. The practical need for this has been known since at least Hockenmaier (2003), who introduced a type-changing mechanism into CCGbank in order to control the problem referred to as modifier category proliferation. We briefly describe the problem, and then the prominent solutions that have been proposed. Unlike formalisms like LTAG and HPSG, CCG does not use different grammatical rules"
D12-1072,E03-1071,1,0.459016,"our machine learning experiments use the same text encoding, which is explained below, and all use the category predictions when they are available. 6.1 Text Encoding We encode our text similarly to Elson and McKeown (2010). The major steps are: 1. Replace all quotes and speakers with special symbols; 2. Replace all reported speech verbs with a symbol. Elson and McKeown (2010) provided us with their list of reported speech verbs; 3. Part-of-Speech (POS) tag the text and remove adjectives, adverbs, and other parts of speech that do not contribute useful information. We used the POS tagger from Curran and Clark (2003); 1. Search backwards in the text from the end of the sentence the quote appears in for a reported speech verb 4. Remove any paragraphs or sentences where no quotes, pronouns or names occur. 2. If the verb is found return the entity mention nearest the verb (ignoring mentions in quotes), in the current sentence or any sentence preceding it All features that will be discussed are calculated with respect to this encoding (e.g. word distance would be the number of words in the encoded text, rather than the number of words in the original text). 3. If not, return the mention of an entity nearest t"
D12-1072,P05-1045,0,0.10613,"Missing"
D12-1072,pareti-2012-database,1,0.280952,"Missing"
D12-1072,sagot-etal-2010-lexicon,0,0.021408,"Missing"
D12-1096,H91-1060,0,0.579673,"Missing"
D12-1096,P11-1048,0,0.0156051,"hat can be em1055 ployed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over sentences, it gives a close approximation. The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets. While there is improvement on all errors when using the reranker, there is very little additional gain beyond the first 5-10 parses. Even for"
D12-1096,D11-1037,0,0.0179858,"ement of the oracle is considerably higher than that of the reranker, particularly compared to the differences for other errors, suggesting that the reranker lacks the features necessary to make the decision better than the parser. The other interesting outlier is NP internal structure, which continues to make improvements for longer lists, unlike the other error types. 5.2 Out-of-Domain Parsing performance drops considerably when shifting outside of the domain a parser was trained on (Gildea, 2001). Clegg and Shepherd (2005) evaluated parsers qualitatively on node types and rule productions. Bender et al. (2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena. To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al., 1056 Corpus WSJ 23 Brown F Brown G Brown K Brown L Brown M Brown N Brown P Brown R G-Web Blogs G-Web Email Description Sentences Av. Length Newswire 2416 23.5 Popular 3164 23.4 Biographies 3279 25.5 General 3881 17.2 Mystery 3714 15.7 Science 881"
D12-1096,J04-4004,0,0.0152738,"from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes i"
D12-1096,P11-1045,0,0.014823,"into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner"
D12-1096,P06-2006,0,0.0118261,"o account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are stro"
D12-1096,cer-etal-2010-parsing,0,0.0400421,"Missing"
D12-1096,P05-1022,0,0.0927601,"For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed"
D12-1096,A00-2018,0,0.0745506,"ation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the"
D12-1096,W05-1102,0,0.0192051,"using on the rows for K = 2 we can also see two interesting outliers. The PP attachment improvement of the oracle is considerably higher than that of the reranker, particularly compared to the differences for other errors, suggesting that the reranker lacks the features necessary to make the decision better than the parser. The other interesting outlier is NP internal structure, which continues to make improvements for longer lists, unlike the other error types. 5.2 Out-of-Domain Parsing performance drops considerably when shifting outside of the domain a parser was trained on (Gildea, 2001). Clegg and Shepherd (2005) evaluated parsers qualitatively on node types and rule productions. Bender et al. (2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena. To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al., 1056 Corpus WSJ 23 Brown F Brown G Brown K Brown L Brown M Brown N Brown P Brown R G-Web Blogs G-Web Email Description Sentences Av. Length Newswire 2416 23."
D12-1096,P97-1003,0,0.0512335,"rectly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderso"
D12-1096,J03-4003,0,0.0837746,"2 Background Most attempts to understand the behaviour of constituency parsers have focused on overall evaluation metrics. The three main methods are intrinsic evaluation with PARSEVAL, evaluation on dependencies extracted from the constituency parse, and evaluation on downstream tasks that rely on parsing. Intrinsic evaluation with PARSEVAL, which calculates precision and recall over labeled tree nodes, is a useful indicator of overall performance, but does not pinpoint which structures the parser has most difficulty with. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves ex"
D12-1096,W11-2927,0,0.0149009,"r, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk 1049 and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011). While some of these approaches give a better sense of the impact of parse errors, they require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers w"
D12-1096,W11-2920,0,0.0121497,"ly classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004)."
D12-1096,foster-van-genabith-2008-parser,0,0.0378682,"Missing"
D12-1096,W01-0521,0,0.0158019,"ngle error. Focusing on the rows for K = 2 we can also see two interesting outliers. The PP attachment improvement of the oracle is considerably higher than that of the reranker, particularly compared to the differences for other errors, suggesting that the reranker lacks the features necessary to make the decision better than the parser. The other interesting outlier is NP internal structure, which continues to make improvements for longer lists, unlike the other error types. 5.2 Out-of-Domain Parsing performance drops considerably when shifting outside of the domain a parser was trained on (Gildea, 2001). Clegg and Shepherd (2005) evaluated parsers qualitatively on node types and rule productions. Bender et al. (2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena. To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al., 1056 Corpus WSJ 23 Brown F Brown G Brown K Brown L Brown M Brown N Brown P Brown R G-Web Blogs G-Web Email Description Sentences A"
D12-1096,W07-2202,0,0.0204252,"e nodes, is a useful indicator of overall performance, but does not pinpoint which structures the parser has most difficulty with. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and va"
D12-1096,D09-1121,0,0.019996,"arsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk 1049 and Corston-Oliver, 2006), information extraction (Miyao"
D12-1096,N03-1014,0,0.0513104,"s (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44 91.70 44.87 1.8 Charniak-R 91.41 91.78 91.04 44.04 1.8 Charniak-S 91.02 91.16 90.89 40.77 1.8 S P NP VP PRP VBD He was STANDARD PARSERS Berkeley Charniak SSN BUBS Bikel Collins-3 Collins-2 Collins-1 Stanford"
D12-1096,P04-1013,0,0.0234916,"unlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44 91.70 44.87 1.8 Charniak-R 91.41 91.78 91.04 44.04 1.8 Charniak-S 91.02 91.16 90.89 40.77 1.8 S P NP VP PRP VBD He was STANDARD PARSERS Berkeley Charniak SSN BUBS Bikel Collins-3 Collins-2 Collins-1 Stanford-L Stanford-U 90.0"
D12-1096,P08-1067,0,0.0174942,"limits the range of features that can be em1055 ployed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over sentences, it gives a close approximation. The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets. While there is improvement on all errors when using the reranker, there is very little additional gain"
D12-1096,W03-2401,0,0.0081588,"rs is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for c"
D12-1096,P03-1054,1,0.0285971,"er grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44 91.70 44.87 1.8 Charniak-R 91.41 91.78 91.04 44.04 1.8 Charniak-S 91.02 91.16 90.89 40.77 1.8 S P NP VP PRP VBD He was STANDARD PARSERS Berkeley Charniak SSN BUBS Bikel Collins-3 Collins-2 Collins-1 Stanford-L Stanford-U 90.06 89.71 89.42 88.50 88.16 87.66 87.62 87.09 86.42 85.78 90.30 89.88 89.96 88.57 88.23 87.82 87.77 87.29 86.35 86.48 89."
D12-1096,J93-2004,0,0.04189,"Missing"
D12-1096,N06-1020,0,0.214552,"with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44"
D12-1096,D07-1013,0,0.0423525,"method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk 1049 and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic depend"
D12-1096,P08-1006,0,0.0174097,"2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk 1049 and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011). While some of these approaches give a better sense of the impact of parse errors, they require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range o"
D12-1096,U10-1014,1,0.836424,"nge of features that can be em1055 ployed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over sentences, it gives a close approximation. The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets. While there is improvement on all errors when using the reranker, there is very little additional gain beyond the first"
D12-1096,C10-1094,0,0.0190793,"Missing"
D12-1096,N07-1051,1,0.581759,"into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models,"
D12-1096,P06-1055,1,0.700645,"y require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised pa"
D12-1096,W06-1608,0,0.0132702,"Missing"
D12-1096,W11-2907,0,0.01367,"l indicator of overall performance, but does not pinpoint which structures the parser has most difficulty with. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008;"
D12-1096,S10-1009,0,0.0191066,"Missing"
D13-1101,krestel-etal-2008-minding,0,0.283044,"ns annotated within them. SMHC has a higher density of quotations per document, 8.3 vs. 4.6 in PARC, since articles are fully annotated and 1 The agreement was calculated using the agr metric described in Wiebe and Riloff (2005) as the proportion of commonly annotated ARs with respect to the ARs identified overall by Annotator A and Annotator B respectively 992 Bsay Blist k-NN P 94.4 75.4 88.9 R 43.5 71.1 72.6 F 59.5 73.2 79.9 Table 3: Results for the k-NN verb-cue classifier. Bsay classifies as verb-cue all instances of say while Blist marks as verb-cues all verbs from a pre-compiled list in Krestel et al. (2008). were selected to contain at least one quotation. PARC is instead only partially annotated and comprises articles with no quotations. Excluding null-quotation articles from PARC, the average incidence of annotated quotations per article raises to 7.1. The corpora also differ in quotation type distribution, with direct quotations being largely predominant in SMHC while indirect are more common in PARC. 4 4.1 Experimental Setup Quotation Extraction Quotation extraction is the task of extracting the content span of all of the direct, indirect, and mixed quotations within a given document. More p"
D13-1101,D12-1072,1,0.6309,"Missing"
D13-1101,pareti-2012-database,1,0.483337,"Finally, we use the direct quotation attribution methods described in O’Keefe et al. (2012) and show that they can be successfully applied to indirect and mixed quotations, albeit with lower accuracy. This leads us to conclude that attributing indirect and mixed quotations to speakers is harder than attributing direct quotations. With this work, we set a new state of the art in quotation extraction. We expect that the main contribution of this work will be that future methods can be evaluated in a comparable way, so that the relative merit of various approaches can be determined. 2 Background Pareti (2012) defines an attribution as having a source span, a cue span, and a content span: Source is the span of text that indicates who the content is attributed to, e.g. ‘president Obama’, ‘analysts’, ‘China’, ‘she’. Cue is the lexical anchor of the attribution relation, 990 usually a verb, e.g. ‘say’, ‘add’, ‘quip’. Content is the span of text that is attributed. Based on the type of attitude the source expresses towards a proposition or eventuality, attributions are subcategorised (Prasad et al., 2006) into assertions (Ex.2a) and beliefs (Ex.2b), which imply different degrees of commitment, facts (E"
D13-1101,W06-0305,0,0.180179,"ted in a comparable way, so that the relative merit of various approaches can be determined. 2 Background Pareti (2012) defines an attribution as having a source span, a cue span, and a content span: Source is the span of text that indicates who the content is attributed to, e.g. ‘president Obama’, ‘analysts’, ‘China’, ‘she’. Cue is the lexical anchor of the attribution relation, 990 usually a verb, e.g. ‘say’, ‘add’, ‘quip’. Content is the span of text that is attributed. Based on the type of attitude the source expresses towards a proposition or eventuality, attributions are subcategorised (Prasad et al., 2006) into assertions (Ex.2a) and beliefs (Ex.2b), which imply different degrees of commitment, facts (Ex.2c), expressing evaluation or knowledge, and eventualities (Ex.2d), expressing intention or attitude. (2) a. b. c. d. Mr Abbott said that he will win the election. Mr Abbott thinks he will win the election. Mr Abbott knew that Gillard was in Sydney. Mr Abbott agreed to the public sector cuts. Only assertion attributions necessarily imply a speech act. Their content corresponds to a quotation span and their source is generally referred to in the literature as the speaker. Direct, indirect and mi"
D13-1101,I05-6007,0,0.401373,"Missing"
D14-1089,D11-1142,0,0.0665831,"ace form, dependency path, or phrase structure subtree between them; and surrounding context (Zhou et al., 2005; Mintz et al., 2009; Zhang et al., 2013). We evaluate the length of dependency path between entities as a variable affecting SF candidate recall, and apply na¨ıve entity pair bootstrapping (Brin, 1998; Agichtein and Gravano, 2000) to assess the generalisation over dependency paths from examples. 2013; Zhang et al., 2013) or semi-supervised techniques (Agichtein and Gravano, 2000; Wang et al., 2011; Carlson et al., 2010). Relation phrases or patterns may be identified without labels (Fader et al., 2011; Mausam et al., 2012) or clustered (Yao et al., 2012) into types. Generating candidate entity pairs and using the syntactic or surface path between them to decide whether a relation exists are common threads in RE that also form part of the SF pipeline. In some RE tasks, entities mentioned may already be identified in a document and provided to a RE system; in general, automatic NER is required. Some tasks are defined more generally to include common noun phrases (Fader et al., 2011; Carlson et al., 2010). SF specifically includes slots that can be filled by arbitrary strings such as per:caus"
D14-1089,P05-1045,0,0.0106791,"Missing"
D14-1089,P11-2050,0,0.0297709,"on constraints imposed by query matching, entity form, and sentence and syntactic context. We combine these filters in series in a number of configurations. The use or absence of coreference varies across our configurations, as the need to identify the query mention and terms that refer to the query mention is critical. Finally, we experiment with a bootTypically, a RE system will only consider entities mentioned together in a sentence. When seeking all instances of a given relation between known entities, coreference resolution is necessary to substantially expand the set of candidate pairs (Gabbard et al., 2011). Coreference resolution may not be necessary where each relation is redundantly mentioned in a large corpus, as in SF; in this vein, “Open” approaches prefer precision and avoid automatic coreference resolution (Banko et al., 2007). Moreover, previous analysis attributed substantial SF error to these tools (Ji and Grish822 the query and the fill must be mentioned in the same sentence; COREF NNP: as for COREF, but the query and the fill must have coreferent proper noun mentions in the same sentence; NA¨I VE NNP: as for COREF NNP, but instead of using a full coreference system and identifying p"
D14-1089,P11-1055,0,0.0166064,"hat also form part of the SF pipeline. In some RE tasks, entities mentioned may already be identified in a document and provided to a RE system; in general, automatic NER is required. Some tasks are defined more generally to include common noun phrases (Fader et al., 2011; Carlson et al., 2010). SF specifically includes slots that can be filled by arbitrary strings such as per:cause of death, which make up a large number of slot fills but may require the use of different techniques for extraction, separate from names. NER may be further enhanced by resolving names to a KB (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Wang et al., 2011), reducing noise in learning and extraction processes, but we do not take this step in this work. 4 Experimental setup We begin with a set of queries (a query being a NE entity grounded in a mention in a document) and, for each query q, the documents Dq known to contain any slot fill for q, as determined by oracle information retrieval (IR) from human annotation and judged system output. Filling every slot in q with every n-gram in Dq constitutes a system with nearly perfect recall. We apply a series of increasingly restrictive filters over this set."
D14-1089,P11-1115,0,0.0605268,"Missing"
D14-1089,P03-1054,0,0.0268555,"Missing"
D14-1089,W11-1902,0,0.101981,"Missing"
D14-1089,D12-1048,0,0.022943,"path, or phrase structure subtree between them; and surrounding context (Zhou et al., 2005; Mintz et al., 2009; Zhang et al., 2013). We evaluate the length of dependency path between entities as a variable affecting SF candidate recall, and apply na¨ıve entity pair bootstrapping (Brin, 1998; Agichtein and Gravano, 2000) to assess the generalisation over dependency paths from examples. 2013; Zhang et al., 2013) or semi-supervised techniques (Agichtein and Gravano, 2000; Wang et al., 2011; Carlson et al., 2010). Relation phrases or patterns may be identified without labels (Fader et al., 2011; Mausam et al., 2012) or clustered (Yao et al., 2012) into types. Generating candidate entity pairs and using the syntactic or surface path between them to decide whether a relation exists are common threads in RE that also form part of the SF pipeline. In some RE tasks, entities mentioned may already be identified in a document and provided to a RE system; in general, automatic NER is required. Some tasks are defined more generally to include common noun phrases (Fader et al., 2011; Carlson et al., 2010). SF specifically includes slots that can be filled by arbitrary strings such as per:cause of death, which make"
D14-1089,min-grishman-2012-challenges,0,0.0513086,"Missing"
D14-1089,W08-1301,0,0.0248048,"Missing"
D14-1089,P09-1113,0,0.320262,"racle docs alias match exact match candidate generation NP n-grams types NEs sentence filter coref NNP coref no coref NNP naive answer merging and ranking dependency filters nonunique answer extraction length Figure 1: Candidate filters within the standard SF pipeline. Arrows indicate a sequence of filters. man, 2011). Our work evaluates NER, locality heuristics and coreference within a SF context. Classification features for RE typically encode: attributes of the entities; the surface form, dependency path, or phrase structure subtree between them; and surrounding context (Zhou et al., 2005; Mintz et al., 2009; Zhang et al., 2013). We evaluate the length of dependency path between entities as a variable affecting SF candidate recall, and apply na¨ıve entity pair bootstrapping (Brin, 1998; Agichtein and Gravano, 2000) to assess the generalisation over dependency paths from examples. 2013; Zhang et al., 2013) or semi-supervised techniques (Agichtein and Gravano, 2000; Wang et al., 2011; Carlson et al., 2010). Relation phrases or patterns may be identified without labels (Fader et al., 2011; Mausam et al., 2012) or clustered (Yao et al., 2012) into types. Generating candidate entity pairs and using th"
D14-1089,N13-1008,0,0.0880377,"Missing"
D14-1089,E14-2023,0,0.0288997,"Missing"
D14-1089,D12-1042,0,0.0250938,"he SF pipeline. In some RE tasks, entities mentioned may already be identified in a document and provided to a RE system; in general, automatic NER is required. Some tasks are defined more generally to include common noun phrases (Fader et al., 2011; Carlson et al., 2010). SF specifically includes slots that can be filled by arbitrary strings such as per:cause of death, which make up a large number of slot fills but may require the use of different techniques for extraction, separate from names. NER may be further enhanced by resolving names to a KB (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Wang et al., 2011), reducing noise in learning and extraction processes, but we do not take this step in this work. 4 Experimental setup We begin with a set of queries (a query being a NE entity grounded in a mention in a document) and, for each query q, the documents Dq known to contain any slot fill for q, as determined by oracle information retrieval (IR) from human annotation and judged system output. Filling every slot in q with every n-gram in Dq constitutes a system with nearly perfect recall. We apply a series of increasingly restrictive filters over this set. As in Figure 1, SF syst"
D14-1089,R11-1004,0,0.0240874,"Missing"
D14-1089,N03-1033,0,0.00714243,"Missing"
D14-1089,P12-1075,0,0.0161489,"between them; and surrounding context (Zhou et al., 2005; Mintz et al., 2009; Zhang et al., 2013). We evaluate the length of dependency path between entities as a variable affecting SF candidate recall, and apply na¨ıve entity pair bootstrapping (Brin, 1998; Agichtein and Gravano, 2000) to assess the generalisation over dependency paths from examples. 2013; Zhang et al., 2013) or semi-supervised techniques (Agichtein and Gravano, 2000; Wang et al., 2011; Carlson et al., 2010). Relation phrases or patterns may be identified without labels (Fader et al., 2011; Mausam et al., 2012) or clustered (Yao et al., 2012) into types. Generating candidate entity pairs and using the syntactic or surface path between them to decide whether a relation exists are common threads in RE that also form part of the SF pipeline. In some RE tasks, entities mentioned may already be identified in a document and provided to a RE system; in general, automatic NER is required. Some tasks are defined more generally to include common noun phrases (Fader et al., 2011; Carlson et al., 2010). SF specifically includes slots that can be filled by arbitrary strings such as per:cause of death, which make up a large number of slot fills"
D14-1089,P13-2141,0,0.0162751,"ch exact match candidate generation NP n-grams types NEs sentence filter coref NNP coref no coref NNP naive answer merging and ranking dependency filters nonunique answer extraction length Figure 1: Candidate filters within the standard SF pipeline. Arrows indicate a sequence of filters. man, 2011). Our work evaluates NER, locality heuristics and coreference within a SF context. Classification features for RE typically encode: attributes of the entities; the surface form, dependency path, or phrase structure subtree between them; and surrounding context (Zhou et al., 2005; Mintz et al., 2009; Zhang et al., 2013). We evaluate the length of dependency path between entities as a variable affecting SF candidate recall, and apply na¨ıve entity pair bootstrapping (Brin, 1998; Agichtein and Gravano, 2000) to assess the generalisation over dependency paths from examples. 2013; Zhang et al., 2013) or semi-supervised techniques (Agichtein and Gravano, 2000; Wang et al., 2011; Carlson et al., 2010). Relation phrases or patterns may be identified without labels (Fader et al., 2011; Mausam et al., 2012) or clustered (Yao et al., 2012) into types. Generating candidate entity pairs and using the syntactic or surfac"
D14-1089,P05-1053,0,0.0512135,"ocument retrieval oracle docs alias match exact match candidate generation NP n-grams types NEs sentence filter coref NNP coref no coref NNP naive answer merging and ranking dependency filters nonunique answer extraction length Figure 1: Candidate filters within the standard SF pipeline. Arrows indicate a sequence of filters. man, 2011). Our work evaluates NER, locality heuristics and coreference within a SF context. Classification features for RE typically encode: attributes of the entities; the surface form, dependency path, or phrase structure subtree between them; and surrounding context (Zhou et al., 2005; Mintz et al., 2009; Zhang et al., 2013). We evaluate the length of dependency path between entities as a variable affecting SF candidate recall, and apply na¨ıve entity pair bootstrapping (Brin, 1998; Agichtein and Gravano, 2000) to assess the generalisation over dependency paths from examples. 2013; Zhang et al., 2013) or semi-supervised techniques (Agichtein and Gravano, 2000; Wang et al., 2011; Carlson et al., 2010). Relation phrases or patterns may be identified without labels (Fader et al., 2011; Mausam et al., 2012) or clustered (Yao et al., 2012) into types. Generating candidate entit"
E03-1071,J96-1002,0,0.00385442,"have been successfully modelled in the ME framework, including POS tagging, with state of the art performance (van Halteren et al., 2001), ""supertagging"" (Clark, 2002) and chunking (Koeling, 2000). Generalised Iterative Scaling (GIs) is a very simple algorithm for estimating the parameters of a ME model. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a ""correction"", or ""slack"", feature to each event Improved Iterative Scaling (us) (Berger et al., 1996; Della Pietra et al., 1997) eliminated the correction feature to improve the convergence rate of the algorithm. However, the extra book keeping required for us means that GIS is often faster in practice (Malouf, 2002). This paper shows, by a simple adaptation of Berger&apos;s proof for the convergence of HS (Berger, 1997), that GIS does not require a correction feature. We also investigate how the use of a correction feature affects the performance of ME taggers. GIS and HS obtain a maximum likelihood estimate (mLE) of the parameters, and, like other MLE methods, are susceptible to overfitting. A"
E03-1071,A00-1031,0,0.106095,"Missing"
E03-1071,W02-1001,0,0.051123,"Missing"
E03-1071,W00-1308,0,0.0407202,"Missing"
E03-1071,J01-2002,0,0.0399112,"Missing"
E03-1071,P02-1002,0,0.0122848,"Missing"
E03-1071,hockenmaier-steedman-2002-acquiring,0,0.0121682,"Missing"
E03-1071,P99-1069,0,0.166136,"es for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar. 1 Introduction The use of maximum entropy (ME) models has become popular in Statistical NLP; some example applications include part-of-speech (Pos) tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1999; Johnson et al., 1999) and language modelling (Rosenfeld, 1996). Many tagging problems have been successfully modelled in the ME framework, including POS tagging, with state of the art performance (van Halteren et al., 2001), ""supertagging"" (Clark, 2002) and chunking (Koeling, 2000). Generalised Iterative Scaling (GIs) is a very simple algorithm for estimating the parameters of a ME model. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a ""correction"", or"
E03-1071,W02-2018,0,0.312726,"ing (GIs) is a very simple algorithm for estimating the parameters of a ME model. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a ""correction"", or ""slack"", feature to each event Improved Iterative Scaling (us) (Berger et al., 1996; Della Pietra et al., 1997) eliminated the correction feature to improve the convergence rate of the algorithm. However, the extra book keeping required for us means that GIS is often faster in practice (Malouf, 2002). This paper shows, by a simple adaptation of Berger&apos;s proof for the convergence of HS (Berger, 1997), that GIS does not require a correction feature. We also investigate how the use of a correction feature affects the performance of ME taggers. GIS and HS obtain a maximum likelihood estimate (mLE) of the parameters, and, like other MLE methods, are susceptible to overfitting. A simple technique used to avoid overfitting is a frequency cutoff, in which only frequently occurring features are included in the model (Ratnaparkhi, 1998). However, more sophisticated smoothing techniques exist, such"
E03-1071,W00-0729,0,\N,Missing
E06-1030,W95-0104,0,0.194744,"Missing"
E06-1030,halacsy-etal-2004-creating,0,0.121735,"s and consistently outperformed the baseline, the best performing system is usually a supervised method trained on annotated data. Keller and Lapata concluded that having access linguistic information (accurate n-gram counts, POS tags, and parses) outperforms using a large amount of web data. 2.2 Spidered Web Corpora A few projects have utilised data downloaded from the web. Ravichandran et al. (2005) used a collection of 31 million web pages to produce noun similarity lists. They found that most NLP algorithms are unable to run on web scale data, especially those with quadratic running time. Halacsy et al. (2004) created a Hungarian corpus from the web by downloading text from the .hu domain. From a 18 million page crawl of the web a 1 billion word corpus is created (removing duplicates and non-Hungarian text). A terabyte-sized corpus of the web was collected at the University of Waterloo in 2001. A breadth first search from a seed set of university home pages yielded over 53 billion words, requiring 960GB of storage. Clarke et al. (2002) and Terra and Clarke (2003) used this corpus for their question answering system. They obtained increasing performance with increasing corpus size but began reaching"
E06-1030,J03-3005,0,0.552778,"Introduction Traditional written corpora for linguistics research are created primarily from printed text, such as newspaper articles and books. With the growth of the World Wide Web as an information resource, it is increasingly being used as training data in Natural Language Processing (NLP) tasks. There are many advantages to creating a corpus from web data rather than printed text. All web data is already in electronic form and therefore readable by computers, whereas not all printed data is available electronically. The vast amount of text available on the web is a major advantage, with Keller and Lapata (2003) estimating that over 98 billion words were indexed by Google in 2003. The performance of NLP systems tends to improve with increasing amount of training data. Banko and Brill (2001) showed that for contextsensitive spelling correction, increasing the training data size increases the accuracy, for up to 1 billion words in their experiments. To date, most NLP tasks that have utilised web data have accessed it through search engines, using only the hit counts or examining a limited number of results pages. The tasks are reduced to determining n-gram probabilities which are then estimated by hit"
E06-1030,W03-1023,0,0.0529376,"a similarity task, where synonyms of a target word are extracted from a corpus of unlabelled text. Our evaluation demonstrates that web text can be used for the same tasks as search engine hit counts and newspaper text. However, there is a much larger quantity of freely available web text to exploit. 233 2 Existing Web Corpora The web has become an indispensible resource with a vast amount of information available. Many NLP tasks have successfully utilised web data, including machine translation (Grefenstette, 1999), prepositional phrase attachment (Volk, 2001), and other-anaphora resolution (Modjeska et al., 2003). 2.1 Search Engine Hit Counts Most NLP systems that have used the web access it via search engines such as Altavista and Google. N-gram counts are approximated by literal queries “w1 ... wn ”. Relations between two words are approximated in Altavista by the NEAR operator (which locates word pairs within 10 tokens of each other). The overall coverage of the queries can be expanded by morphological expansion of the search terms. Keller and Lapata (2003) demonstrated a high degree of correlation between n-gram estimates from search engine hit counts and n-gram frequencies obtained from tradition"
E06-1030,N03-1032,0,0.0328988,"un similarity lists. They found that most NLP algorithms are unable to run on web scale data, especially those with quadratic running time. Halacsy et al. (2004) created a Hungarian corpus from the web by downloading text from the .hu domain. From a 18 million page crawl of the web a 1 billion word corpus is created (removing duplicates and non-Hungarian text). A terabyte-sized corpus of the web was collected at the University of Waterloo in 2001. A breadth first search from a seed set of university home pages yielded over 53 billion words, requiring 960GB of storage. Clarke et al. (2002) and Terra and Clarke (2003) used this corpus for their question answering system. They obtained increasing performance with increasing corpus size but began reaching asymptotic behaviour at the 300-500GB range. 3 Creating the Web Corpus There are many challenges in creating a web corpus, as the World Wide Web is unstructured and without a definitive directory. No simple method exists to collect a large representative sample of the web. Two main approaches exist for collecting representative web samples – IP address sampling and random walks. The IP address sampling technique randomly generates IP addresses 234 and explo"
E06-1030,P01-1005,0,0.369245,"as an information resource, it is increasingly being used as training data in Natural Language Processing (NLP) tasks. There are many advantages to creating a corpus from web data rather than printed text. All web data is already in electronic form and therefore readable by computers, whereas not all printed data is available electronically. The vast amount of text available on the web is a major advantage, with Keller and Lapata (2003) estimating that over 98 billion words were indexed by Google in 2003. The performance of NLP systems tends to improve with increasing amount of training data. Banko and Brill (2001) showed that for contextsensitive spelling correction, increasing the training data size increases the accuracy, for up to 1 billion words in their experiments. To date, most NLP tasks that have utilised web data have accessed it through search engines, using only the hit counts or examining a limited number of results pages. The tasks are reduced to determining n-gram probabilities which are then estimated by hit counts from search engine queries. This method only gathers information from the hit counts but does not require the computationally expensive downloading of actual text for analysis"
E06-1030,P05-1077,0,0.0110742,"variety of generation tasks (e.g. machine translation candidate selection) and analysis tasks (e.g. prepositional phrase attachment, countability detection). They showed that while web counts usually outperformed BNC counts and consistently outperformed the baseline, the best performing system is usually a supervised method trained on annotated data. Keller and Lapata concluded that having access linguistic information (accurate n-gram counts, POS tags, and parses) outperforms using a large amount of web data. 2.2 Spidered Web Corpora A few projects have utilised data downloaded from the web. Ravichandran et al. (2005) used a collection of 31 million web pages to produce noun similarity lists. They found that most NLP algorithms are unable to run on web scale data, especially those with quadratic running time. Halacsy et al. (2004) created a Hungarian corpus from the web by downloading text from the .hu domain. From a 18 million page crawl of the web a 1 billion word corpus is created (removing duplicates and non-Hungarian text). A terabyte-sized corpus of the web was collected at the University of Waterloo in 2001. A breadth first search from a seed set of university home pages yielded over 53 billion word"
E09-1070,P03-2031,0,0.153491,"Missing"
E09-1070,M98-1001,0,0.0458324,"of sports scores. Each corpus uses a different set of entity labels. MUC marks locations ( LOC), organisations (ORG ) and personal names (PER), in addition to numerical and time information. The CoNLL NER shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) mark PER, ORG and LOC entities, as well as a broad miscellaneous class Evaluating NER performance One challenge for NER research is establishing an appropriate evaluation metric (Nadeau and Sekine, 2007). In particular, entities may be correctly delimited but mis-classified, or entity boundaries may be mismatched. MUC (Chinchor, 1998) awarded equal score for matching type, where an entity’s class is identified with at least one boundary matching, and text, where an entity’s boundaries are precisely delimited, irrespective of the classification. This equal weighting is unrealistic, as some boundary errors are highly significant, while others are arbitrary. CoNLL awarded exact (type and text) phrasal matches, ignoring boundary issues entirely and providing a lower-bound measure of NER performance. Manning (2006) argues that CoNLLstyle evaluation is biased towards systems which leave entities with ambiguous boundaries untagge"
E09-1070,U08-1016,1,0.470071,"Missing"
E09-1070,P08-1001,0,0.192491,"Missing"
E09-1070,W03-0424,1,0.887382,"instances need to be grouped by more than their class labels. We used the following groups: sequences: Types of candidate entities may often be distinguished by their POS tags, e.g. nationalities are often JJ or NNPS. Wordtypes: Collins (2002) proposed wordtypes where all uppercase characters map to A, lowercase to a, and digits to 0. Adjacent characters in the same orthographic class were collapsed. However, we distinguish single from multiple characters by duplication. e.g. USS Nimitz (CVN-68) has wordtype AA Aaa (AA-00). POS 4 Comparing gold-standard corpora We trained the C&C NER tagger (Curran and Clark, 2003) to build separate models for each goldstandard corpus. The C&C tagger utilises a number 614 TRAIN MUC CoNLL BBN With MISC CoNLL BBN — — 81.2 62.3 54.7 86.7 Without MISC MUC CoNLL BBN 73.5 55.5 67.5 65.9 82.1 62.4 77.9 53.9 88.4 Table 2: Gold standard F -scores (exact-match) of orthographic, contextual and in-document features, as well as gazetteers for personal names. Table 2 shows that each training set performs much better on corresponding (same corpus) test sets (italics) than on test sets from other sources, also identified by (Ciaramita and Altun, 2005). NER research typically deals with"
E09-1070,E03-1068,0,0.01913,"ad, CoNLL has many names of form A. Aaa, e.g. S. Waugh, while BBN and MUC have none. We can therefore predict incompatibilities between systems trained on BBN and evaluated on CoNLL or vice-versa. Corpus and error analysis approaches To evaluate the performance impact of a corpus we may analyse (a) the annotations themselves; or (b) the model built on those annotations and its performance. A corpus can be considered in isolation or by comparison with other corpora. We use three methods to explore intra- and inter-corpus consistency in MUC, CoNLL, and BBN in Section 4. 3.1 N-gram tag variation Dickinson and Meurers (2003) present a clever method for finding inconsistencies within POS annotated corpora, which we apply to NER corpora. Their approach finds all n-grams in a corpus which appear multiple times, albeit with variant tags for some sub-sequence, the nucleus (see e.g. Table 3). To remove valid ambiguity, they suggest using (a) a minimum n-gram length; (b) a minimum margin of invariant terms around the nucleus. For example, the BBN TRAIN corpus includes eight occurrences of the 6-gram the San Francisco Bay area ,. Six instances of area are tagged as nonentities, but two instances are tagged as part of the"
E09-1070,W02-2024,0,0.433848,"hic, linguistic and external knowledge features. However, they rely heavily on large annotated training corpora. This need for costly expert annotation hinders the creation of more task-adaptable, highperformance named entity recognisers. In acquiring new sources for annotated corpora, we require an analysis of training data as a variable in NER. This paper compares the three main goldstandard corpora. We found that tagging mod2 NER and annotated corpora Research into NER has rarely considered the impact of training corpora. The CoNLL evaluations focused on machine learning methods (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) while more recent work has often involved the use of external knowledge. Since many tagging systems utilise gazetteers of known entities, some research has focused on their automatic extraction from the web (Etzioni et al., 2005) or Wikipedia (Toral et al., 2008), although Mikheev et al. (1999) and others have shown that larger NE lists do not necessarily correspond to increased NER performance. Nadeau et al. (2006) use such lists in an unsupervised NE recogniser, outperforming some entrants of the MUC Named Entity Task. Unlike statistical approaches whic"
E09-1070,toral-etal-2008-named,0,0.0255137,"Missing"
E09-1070,D07-1073,0,0.313006,"Missing"
E09-1070,E99-1001,0,0.0420908,"ata as a variable in NER. This paper compares the three main goldstandard corpora. We found that tagging mod2 NER and annotated corpora Research into NER has rarely considered the impact of training corpora. The CoNLL evaluations focused on machine learning methods (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) while more recent work has often involved the use of external knowledge. Since many tagging systems utilise gazetteers of known entities, some research has focused on their automatic extraction from the web (Etzioni et al., 2005) or Wikipedia (Toral et al., 2008), although Mikheev et al. (1999) and others have shown that larger NE lists do not necessarily correspond to increased NER performance. Nadeau et al. (2006) use such lists in an unsupervised NE recogniser, outperforming some entrants of the MUC Named Entity Task. Unlike statistical approaches which learn Proceedings of the 12th Conference of the European Chapter of the ACL, pages 612–620, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 612 patterns associated with a particular type of entity, these unsupervised approaches are limited to identifying common entities present in lists or"
E09-1070,W03-0419,0,\N,Missing
E09-1070,P02-1062,0,\N,Missing
J07-4004,J97-4005,0,0.0199829,"lla Pietra, Della Pietra, and Lafferty (1997), Ratnaparkhi motivates log-linear models from the perspective of maximizing entropy, subject to certain constraints. Ratnaparkhi models the various decisions made by a shift-reduce parser, using log-linear distributions defined over features of the local context in which a decision is made. The probabilities of each decision are multiplied together to give a score for the complete sequence of decisions, and beam search is used to find the most probable sequence, which corresponds to the most probable derivation. A different approach is proposed by Abney (1997), who develops log-linear models for attribute-value grammars, such as Head-driven Phrase Structure Grammar (HPSG). Rather than define a model in terms of parser moves, Abney defines a model directly over the syntactic structures licensed by the grammar. Another difference is that Abney uses a global model, in which a single log-linear model is defined over the complete space of attribute–value structures. Abney’s motivation for using log-linear models is to overcome various problems in applying models based on PCFGs directly to attributevalue grammars. A further motivation for using global mo"
J07-4004,E03-1036,0,0.136917,"Missing"
J07-4004,J99-2004,0,0.894269,"from CCGbank. A key component of the parsing system is a Maximum Entropy CCG supertagger (Ratnaparkhi 1996; Curran and Clark 2003) which assigns lexical categories to words in a sentence. The role of the supertagger is twofold. First, it makes discriminative estimation feasible by limiting the number of incorrect derivations for each training sentence; the supertagger can be thought of as supplying a number of incorrect but plausible lexical categories for each word in the sentence. Second, it greatly increases the efficiency of the parser, which was the original motivation for supertagging (Bangalore and Joshi 1999). One possible criticism of CCG has been that highly efficient parsing is not possible because of the additional “spurious” derivations. In fact, we show that a novel method which tightly integrates the supertagger and parser leads to parse times significantly faster than those reported for comparable parsers in the literature. The parser is evaluated on CCGbank (available through the Linguistic Data Consortium). In order to facilitate comparisons with parsers using different formalisms, we also evaluate on the publicly available DepBank (King et al. 2003), using the Briscoe and Carroll annota"
J07-4004,C04-1180,1,0.78349,"tical parsing with CCG will be described in Section 3. 3. Combinatory Categorial Grammar Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000) is a type-driven lexicalized theory of grammar based on Categorial Grammar (Wood 1993). CCG lexical 498 Clark and Curran Wide-Coverage Efficient Statistical Parsing entries consist of a syntactic category, which defines valency and directionality, and a semantic interpretation. In this article we are concerned with the syntactic component; see Steedman (2000) for how a semantic interpretation can be composed during a syntactic derivation, and also Bos et al. (2004) for how semantic interpretations can be built for newspaper text using the wide-coverage parser described in this article. Categories can be either basic or complex. Examples of basic categories are S (sentence), N (noun), NP (noun phrase), and PP (prepositional phrase). Complex categories are built recursively from basic categories, and indicate the type and directionality of arguments (using slashes), and the type of the result. For example, the following category for the transitive verb bought specifies its first argument as a noun phrase to its right, its second argument as a noun phrase"
J07-4004,briscoe-carroll-2002-robust,0,0.00406905,"et al. 2003). Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank. Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005). Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the LFG parser is more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins and Charniak, the grammatical relations finder of Buchholz, Veenstra, and Daelemans (1999), and the Briscoe and Carroll (2002) parser, using the gold-standard grammatical relations (GRs) from Carroll, Briscoe, and Sanfilippo (1998). The Penn Treebank trees of the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into the required grammatical relations, with the result that the GR finder of Buchholz is the most accurate. There are a number of problems with such evaluations. The first is that, when converting the output of the Collins parser, for example, into the output of another parser, the Collins parser is at an immediate disadvantage. This is especially true if the alternative output is"
J07-4004,P06-2006,0,0.177279,"ent grammar formalisms. One question we are often asked is whether the CCG derivations 530 Clark and Curran Wide-Coverage Efficient Statistical Parsing output by the parser could be converted to Penn Treebank–style trees to enable a comparison with, for example, the Collins and Charniak parsers. The difficulty is that CCG derivations often have a different shape to the Penn Treebank analyses (coordination being a prime example) and reversing the mapping used by Hockenmaier to create CCGbank is a far from trivial task. There is some existing work comparing parser performance across formalisms. Briscoe and Carroll (2006) evaluate the RASP parser on the Parc Dependency Bank (DepBank; King et al. 2003). Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank. Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005). Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the LFG parser is more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins and Charniak, the grammatical re"
J07-4004,P06-4020,0,0.321388,"Missing"
J07-4004,W99-0629,0,0.0609457,"Missing"
J07-4004,P04-1047,0,0.0178952,"Missing"
J07-4004,P04-1041,0,0.0227422,"Missing"
J07-4004,A00-2018,0,0.30964,"article is to incorporate the multi-modal approach; Baldridge suggests that, as well as having theoretical motivation, a multi-modal approach can improve the efficiency of CCG parsing. 3.1 Why Use CCG for Statistical Parsing? CCG was designed to deal with the long-range dependencies inherent in certain constructions, such as coordination and extraction, and arguably provides the most linguistically satisfactory account of these phenomena. Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000). This has led to a number of proposals for post-processing the output of the Collins and Charniak parsers, in which trace sites are located and the antecedent of the trace determined (Johnson 2002; Dienes and Dubey 2003; Levy and Manning 2004). An advantage of using CCG is that 501 Computational Linguistics Volume 33, Number 4 the recovery of long-range dependencies can be integrated into the parsing process in a straightforward manner, rather than be relegated to such a post-processing phase (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and Curran 2004). Another"
J07-4004,W02-2236,0,0.0645173,"Missing"
J07-4004,2000.iwpt-1.9,0,0.0787687,"Missing"
J07-4004,P00-1058,0,0.135957,"Missing"
J07-4004,W03-1013,1,0.6045,"vidual entry at the root of the subderivation which has the highest score for the class. The equivalence classes were defined so that any other individual entry cannot be part  of the highest scoring derivation for the sentence. The score for a subderivation d is i λi fi (d) where fi (d) is the number of times the ith feature occurs in the subderivation. The highest-scoring subderivations can be calculated recursively using the highest-scoring equivalence classes that were combined to create the individual entry. For the dependency model, the highest scoring dependency structure is required. Clark and Curran (2003) outline an algorithm for finding the most probable dependency structure, which keeps track of the highest scoring set of dependencies for each node in the chart. For a set of equivalent entries in the chart (a disjunctive node), this involves summing over all conjunctive node daughters which head sub-derivations leading to the same set of high scoring dependencies. In practice large numbers of such conjunctive nodes lead to very long parse times. As an alternative to finding the most probable dependency structure, we have developed an algorithm which maximizes the expected labeled recall over"
J07-4004,C04-1041,1,0.740435,"h is required to produce reasonable parse times. Thus the reduced accuracy could be due to implementation difficulties rather than the model itself. The use of conditional log-linear models in this article is designed to overcome some of the weaknesses identified in the approach of Clark, Hockenmaier, and Steedman 503 Computational Linguistics Volume 33, Number 4 (2002), and to offer a more flexible framework for including features than the generative models of Hockenmaier (2003a). For example, adding long-range dependency features to the log-linear model is straightforward. We also showed in Clark and Curran (2004b) that, in contrast with Hockenmaier (2003a), adding distance to the dependency features in the log-linear model does improve parsing accuracy. Another feature of conditional log-linear models is that they are trained discriminatively, by maximizing the conditional probability of each gold-standard parse relative to the incorrect parses for the sentence. Generative models, in contrast, are typically trained by maximizing the joint probability of the training sentence, parse pairs, even though the sentence does not need to be inferred. 3.3 CCGbank The treebank used in this article performs t"
J07-4004,P04-1014,1,0.852435,"h is required to produce reasonable parse times. Thus the reduced accuracy could be due to implementation difficulties rather than the model itself. The use of conditional log-linear models in this article is designed to overcome some of the weaknesses identified in the approach of Clark, Hockenmaier, and Steedman 503 Computational Linguistics Volume 33, Number 4 (2002), and to offer a more flexible framework for including features than the generative models of Hockenmaier (2003a). For example, adding long-range dependency features to the log-linear model is straightforward. We also showed in Clark and Curran (2004b) that, in contrast with Hockenmaier (2003a), adding distance to the dependency features in the log-linear model does improve parsing accuracy. Another feature of conditional log-linear models is that they are trained discriminatively, by maximizing the conditional probability of each gold-standard parse relative to the incorrect parses for the sentence. Generative models, in contrast, are typically trained by maximizing the joint probability of the training sentence, parse pairs, even though the sentence does not need to be inferred. 3.3 CCGbank The treebank used in this article performs t"
J07-4004,N06-1019,1,0.710915,"Missing"
J07-4004,P07-1032,1,0.715316,"Missing"
J07-4004,P02-1042,1,0.84187,"Missing"
J07-4004,W04-3215,1,0.736675,"Missing"
J07-4004,P96-1025,0,0.191085,"Parsing The work in this article began as part of the Edinburgh wide-coverage CCG parsing project (2000–2004). There has been some other work on defining stochastic categorial grammars, but mainly in the context of grammar learning (Osborne and Briscoe 1997; Watkinson and Manandhar 2001; Zettlemoyer and Collins 2005). An early attempt from the Edinburgh project at wide-coverage CCG parsing is presented in Clark, Hockenmaier, and Steedman (2002). In order to deal with the problem of the additional, nonstandard CCG derivations, a conditional model of dependency structures is presented, based on Collins (1996), in which the dependencies are modeled directly and derivations are not modeled at all. The conditional probability of a dependency structure π, given a sentence S, is factored into two parts. The first part is the probability of the lexical category sequence, C, and the second part is the dependency structure, D, giving P(π|S) = P(C|S)P(D|C, S). Intuitively, the category sequence is gen502 Clark and Curran Wide-Coverage Efficient Statistical Parsing erated first, conditioned on the sentence, and then attachment decisions are made to form the dependency links. The probability of the category"
J07-4004,J03-4003,0,0.685283,"described in this article is to incorporate the multi-modal approach; Baldridge suggests that, as well as having theoretical motivation, a multi-modal approach can improve the efficiency of CCG parsing. 3.1 Why Use CCG for Statistical Parsing? CCG was designed to deal with the long-range dependencies inherent in certain constructions, such as coordination and extraction, and arguably provides the most linguistically satisfactory account of these phenomena. Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000). This has led to a number of proposals for post-processing the output of the Collins and Charniak parsers, in which trace sites are located and the antecedent of the trace determined (Johnson 2002; Dienes and Dubey 2003; Levy and Manning 2004). An advantage of using CCG is that 501 Computational Linguistics Volume 33, Number 4 the recovery of long-range dependencies can be integrated into the parsing process in a straightforward manner, rather than be relegated to such a post-processing phase (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and C"
J07-4004,W95-0103,0,0.0326873,"t. There is considerable flexibility in defining the features for a parsing model in our log-linear framework, as the long-range dependency example demonstrates, but the need for dynamic programming for both estimation and decoding reduces the range of features which can be used. Any extension to the “locality” of the features would reduce the effectiveness of the chart packing and any dynamic programming performed over the chart. Two possible extensions, which we have not investigated, include defining dependency features which account for all three elements of the triple in a PP-attachment (Collins and Brooks 1995), and defining a rule feature which includes the grandparent node (Johnson 1998). Another alternative for future work is to compare the dynamic programming approach taken here with the beam-search approach of Collins and Roark (2004), which allows more “global” features. 5.3 Calculating Feature Expectations For estimating both the normal-form model and the dependency model, the following expectation of each feature fi , with respect to some model Λ, is required: EΛ fi =  S 1 ZS  e λ · f ( ω ) fi ( ω ) (20) ω∈ρ(S) where ρ(S) is the set of all parses for sentence S, and λ is the vector of weig"
J07-4004,J05-1003,0,0.0949806,"hly one half. Future work will also look at maintaing the POS tag ambiguity through to the parsing stage. Currently we do not use the probabilities assigned to the lexical categories by the supertagger as part of the parse selection process. These scores could be incorporated as real-valued features, or as auxiliary functions, as in Johnson and Riezler (2000). We would also like to investigate using the generative model of Hockenmaier and Steedman (2002b) in a similar way. Using a generative model’s score as a feature in a discriminative framework has been beneficial for reranking approaches (Collins and Koo 2005). Because the generative model uses local features similar to those in our log-linear models, it could be incorporated into the estimation and decoding processes without the need for reranking. One way of improving the accuracy of a supertagger is to use the parser to provide large amounts of additional training data, by taking the lexical categories chosen by the parser as gold-standard training data. If enough unlabeled data is parsed, then the large volume can overcome the noise in the data (Steedman et al. 2002; Prins and van Noord 2003). We plan to investigate this idea in the context of"
J07-4004,P04-1015,0,0.0512372,"educes the range of features which can be used. Any extension to the “locality” of the features would reduce the effectiveness of the chart packing and any dynamic programming performed over the chart. Two possible extensions, which we have not investigated, include defining dependency features which account for all three elements of the triple in a PP-attachment (Collins and Brooks 1995), and defining a rule feature which includes the grandparent node (Johnson 1998). Another alternative for future work is to compare the dynamic programming approach taken here with the beam-search approach of Collins and Roark (2004), which allows more “global” features. 5.3 Calculating Feature Expectations For estimating both the normal-form model and the dependency model, the following expectation of each feature fi , with respect to some model Λ, is required: EΛ fi =  S 1 ZS  e λ · f ( ω ) fi ( ω ) (20) ω∈ρ(S) where ρ(S) is the set of all parses for sentence S, and λ is the vector of weights for Λ. This is essentially the same calculation for both models, even though for the dependency model, features can be defined in terms of dependencies as well as the derivations. Dependencies can be stored as part of the individ"
J07-4004,E03-1071,1,0.616549,"umerating all derivations is infeasible. To solve this problem, we have adapted the dynamic programming method of Miyao and Tsujii (2002) to packed CCG charts. A packed chart efficiently represents all derivations for a sentence. The dynamic programming method uses inside and outside scores to calculate expectations, similar to the inside–outside algorithm for estimating the parameters of a PCFG from unlabeled data (Lari and Young 1990). Generalized Iterative Scaling (Darroch and Ratcliff 1972) is a common choice in the NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curran and Clark 2003). Initially we used generalized iterative scaling (GIS) for the parsing models described here, but found that convergence was extremely slow; Sha and Pereira (2003) present a similar finding for globally optimized log-linear models for sequences. As an alternative to GIS, we use the limited-memory BFGS algorithm (Nocedal and Wright 1999). As Malouf (2002) demonstrates, general purpose numerical optimization algorithms such as BFGS can converge much faster than iterative scaling algorithms (including Improved Iterative Scaling; Della Pietra, Della Pietra, and Lafferty 1997). Despite the use of"
J07-4004,P06-1088,1,0.562802,"Missing"
J07-4004,P03-1055,0,0.00870733,"al Parsing? CCG was designed to deal with the long-range dependencies inherent in certain constructions, such as coordination and extraction, and arguably provides the most linguistically satisfactory account of these phenomena. Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000). This has led to a number of proposals for post-processing the output of the Collins and Charniak parsers, in which trace sites are located and the antecedent of the trace determined (Johnson 2002; Dienes and Dubey 2003; Levy and Manning 2004). An advantage of using CCG is that 501 Computational Linguistics Volume 33, Number 4 the recovery of long-range dependencies can be integrated into the parsing process in a straightforward manner, rather than be relegated to such a post-processing phase (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and Curran 2004). Another advantage of CCG is that providing a compositional semantics for the grammar is relatively straightforward. It has a completely transparent interface between syntax and semantics and, because CCG is a lexicalized gramma"
J07-4004,P96-1011,0,0.730648,"literature to deal with certain linguistic phenomena, but we chose not to implement them. The reason is that adding new combinatory rules reduces the efficiency of the parser, and we felt that, in the case of substitution, for example, the small gain in grammatical coverage was not worth the reduction in speed. Section 9.3 discusses some of the choices we made when implementing the grammar. One way of dealing with the additional ambiguity in CCG is to only consider normal-form derivations. Informally, a normal-form derivation is one which uses typeraising and composition only when necessary. Eisner (1996) describes a technique for eliminating spurious ambiguity entirely, by defining exactly one normal-form derivation for each semantic equivalence class of derivations. The idea is to restrict the combination of categories produced by composition; more specifically, any constituent which is the result of a forward composition cannot serve as the primary (left) functor in another forward composition or forward application. Similarly, any constituent which is the result of a backward composition cannot serve as the primary (right) functor in another backward composition or backward application. Ei"
J07-4004,P02-1036,0,0.0208432,"to enumerate the analyses for each sentence in the training data. Osborne (2000) investigates training on a sample of the analyses for each sentence, for example the top-n most probable according to some other probability model, or simply a random sample. The CCG grammar used in this article is automatically extracted, has wide coverage, and can produce an extremely large number of derivations for some sentences, far too many to enumerate. We adapt the feature-forest method of Miyao and Tsujii (2002), which involves using dynamic programming to efficiently calculate the feature expectations. Geman and Johnson (2002) propose a similar method in the context of LFG parsing; an implementation is described in Kaplan et al. (2004). Miyao and Tsujii have carried out a number of investigations similar to the work in this article. In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG). One of Miyao and Tsujii’s motivations is to model predicate–argument dependencies, including long-range dependencies, which was one of the original motivations of the wide-coverage CCG parsi"
J07-4004,W01-0521,0,0.101766,"Missing"
J07-4004,P96-1024,0,0.449382,"Missing"
J07-4004,1997.iwpt-1.13,0,0.151441,"Missing"
J07-4004,P03-1046,0,0.119668,"yao and Tsujii (2002) to a packed chart; however, because the grammar is automatically extracted, the packed charts require a considerable amount of memory: up to 25 GB. We solve this massive estimation problem by developing a parallelized version of the estimation algorithm which runs on a Beowulf cluster. The lexicalized grammar formalism we use is Combinatory Categorial Grammar (CCG; Steedman 2000). A number of statistical parsing models have recently been developed for CCG and used in parsers applied to newspaper text (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002b; Hockenmaier 2003b). In this article we extend existing parsing techniques by developing log-linear models for CCG, as well as a new model and efficient parsing algorithm which exploits all CCG’s derivations, including the nonstandard ones. Estimating a log-linear model involves computing expectations of feature values. For the conditional log-linear models used in this article, computing expectations requires a sum over all derivations for each sentence in the training data. Because there can be a massive number of derivations for some sentences, enumerating all derivations is infeasible. To solve this proble"
J07-4004,hockenmaier-steedman-2002-acquiring,0,0.052539,"babilities to decide which tags to maintain. We were able to reduce the drop 540 Clark and Curran Wide-Coverage Efficient Statistical Parsing in supertagger accuracy by roughly one half. Future work will also look at maintaing the POS tag ambiguity through to the parsing stage. Currently we do not use the probabilities assigned to the lexical categories by the supertagger as part of the parse selection process. These scores could be incorporated as real-valued features, or as auxiliary functions, as in Johnson and Riezler (2000). We would also like to investigate using the generative model of Hockenmaier and Steedman (2002b) in a similar way. Using a generative model’s score as a feature in a discriminative framework has been beneficial for reranking approaches (Collins and Koo 2005). Because the generative model uses local features similar to those in our log-linear models, it could be incorporated into the estimation and decoding processes without the need for reranking. One way of improving the accuracy of a supertagger is to use the parser to provide large amounts of additional training data, by taking the lexical categories chosen by the parser as gold-standard training data. If enough unlabeled data is pa"
J07-4004,P02-1043,0,0.825906,"dynamic programming method of Miyao and Tsujii (2002) to a packed chart; however, because the grammar is automatically extracted, the packed charts require a considerable amount of memory: up to 25 GB. We solve this massive estimation problem by developing a parallelized version of the estimation algorithm which runs on a Beowulf cluster. The lexicalized grammar formalism we use is Combinatory Categorial Grammar (CCG; Steedman 2000). A number of statistical parsing models have recently been developed for CCG and used in parsers applied to newspaper text (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002b; Hockenmaier 2003b). In this article we extend existing parsing techniques by developing log-linear models for CCG, as well as a new model and efficient parsing algorithm which exploits all CCG’s derivations, including the nonstandard ones. Estimating a log-linear model involves computing expectations of feature values. For the conditional log-linear models used in this article, computing expectations requires a sum over all derivations for each sentence in the training data. Because there can be a massive number of derivations for some sentences, enumerating all derivations is infeasible. T"
J07-4004,J98-4004,0,0.173189,"tion; hence the parser can produce non-normal-form derivations. However, because the parsing model is estimated over normal-form derivations, any non-normal-form derivations will receive low probabilities and are unlikely to be returned as the most probable parse. Hockenmaier (2003a) compares a number of generative models, starting with a baseline model based on a PCFG. Various extensions to the baseline are considered: increasing the amount of lexicalization; generating a lexical category at its maximal projection; conditioning the probability of a rule instantiation on the grandparent node (Johnson 1998); adding features designed to deal with coordination; and adding distance to the dependency features. Some of these extensions, such as increased lexicalization and generating a lexical category at its maximal projection, improved performance, whereas others, such as the coordination and distance features, reduced performance. Hockenmaier (2003a) conjectures that the reduced performance is due to the problem of data sparseness, which becomes particularly severe for the generative model when the number of features is increased. The best performing model outperforms that of Clark, Hockenmaier, a"
J07-4004,P02-1018,0,0.109019,"ate the analyses for each sentence in the training data. Osborne (2000) investigates training on a sample of the analyses for each sentence, for example the top-n most probable according to some other probability model, or simply a random sample. The CCG grammar used in this article is automatically extracted, has wide coverage, and can produce an extremely large number of derivations for some sentences, far too many to enumerate. We adapt the feature-forest method of Miyao and Tsujii (2002), which involves using dynamic programming to efficiently calculate the feature expectations. Geman and Johnson (2002) propose a similar method in the context of LFG parsing; an implementation is described in Kaplan et al. (2004). Miyao and Tsujii have carried out a number of investigations similar to the work in this article. In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG). One of Miyao and Tsujii’s motivations is to model predicate–argument dependencies, including long-range dependencies, which was one of the original motivations of the wide-coverage CCG parsi"
J07-4004,P99-1069,0,0.393766,"ark@comlab.ox.ac.uk. ∗∗ School of Information Technologies, University of Sydney, NSW 2006, Australia. E-mail: james@it.usyd.edu.au. Submission received: 27 April 2006; revised submission received: 30 November 2006; accepted for publication: 16 March 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 1. Introduction Log-linear models have been applied to a number of problems in NLP, for example, POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999). Log-linear models are also referred to as maximum entropy models and random fields in the NLP literature. They are popular because of the ease with which complex discriminating features can be included in the model, and have been shown to give good performance across a range of NLP tasks. Log-linear models have previously been applied to statistical parsing (Johnson et al. 1999; Toutanova et al. 2002; Riezler et al. 2002; Malouf and van Noord 2004), but typically under the assumption that all possible parses for a sentence can be enumerated. For manually constructed grammars, this assumption"
J07-4004,A00-2021,0,0.0207646,"to maintain some POS tag ambiguity for later parts of the parsing process, using the tag probabilities to decide which tags to maintain. We were able to reduce the drop 540 Clark and Curran Wide-Coverage Efficient Statistical Parsing in supertagger accuracy by roughly one half. Future work will also look at maintaing the POS tag ambiguity through to the parsing stage. Currently we do not use the probabilities assigned to the lexical categories by the supertagger as part of the parse selection process. These scores could be incorporated as real-valued features, or as auxiliary functions, as in Johnson and Riezler (2000). We would also like to investigate using the generative model of Hockenmaier and Steedman (2002b) in a similar way. Using a generative model’s score as a feature in a discriminative framework has been beneficial for reranking approaches (Collins and Koo 2005). Because the generative model uses local features similar to those in our log-linear models, it could be incorporated into the estimation and decoding processes without the need for reranking. One way of improving the accuracy of a supertagger is to use the parser to provide large amounts of additional training data, by taking the lexica"
J07-4004,N04-1013,0,0.0809683,"of the analyses for each sentence, for example the top-n most probable according to some other probability model, or simply a random sample. The CCG grammar used in this article is automatically extracted, has wide coverage, and can produce an extremely large number of derivations for some sentences, far too many to enumerate. We adapt the feature-forest method of Miyao and Tsujii (2002), which involves using dynamic programming to efficiently calculate the feature expectations. Geman and Johnson (2002) propose a similar method in the context of LFG parsing; an implementation is described in Kaplan et al. (2004). Miyao and Tsujii have carried out a number of investigations similar to the work in this article. In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG). One of Miyao and Tsujii’s motivations is to model predicate–argument dependencies, including long-range dependencies, which was one of the original motivations of the wide-coverage CCG parsing project. Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted LTAG which"
J07-4004,W00-0729,0,0.0293961,"3QD, UK. E-mail: stephen.clark@comlab.ox.ac.uk. ∗∗ School of Information Technologies, University of Sydney, NSW 2006, Australia. E-mail: james@it.usyd.edu.au. Submission received: 27 April 2006; revised submission received: 30 November 2006; accepted for publication: 16 March 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 1. Introduction Log-linear models have been applied to a number of problems in NLP, for example, POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999). Log-linear models are also referred to as maximum entropy models and random fields in the NLP literature. They are popular because of the ease with which complex discriminating features can be included in the model, and have been shown to give good performance across a range of NLP tasks. Log-linear models have previously been applied to statistical parsing (Johnson et al. 1999; Toutanova et al. 2002; Riezler et al. 2002; Malouf and van Noord 2004), but typically under the assumption that all possible parses for a sentence can be enumerated. For manually co"
J07-4004,I05-1006,0,0.131553,"Missing"
J07-4004,P04-1042,0,0.00540724,"igned to deal with the long-range dependencies inherent in certain constructions, such as coordination and extraction, and arguably provides the most linguistically satisfactory account of these phenomena. Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000). This has led to a number of proposals for post-processing the output of the Collins and Charniak parsers, in which trace sites are located and the antecedent of the trace determined (Johnson 2002; Dienes and Dubey 2003; Levy and Manning 2004). An advantage of using CCG is that 501 Computational Linguistics Volume 33, Number 4 the recovery of long-range dependencies can be integrated into the parsing process in a straightforward manner, rather than be relegated to such a post-processing phase (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and Curran 2004). Another advantage of CCG is that providing a compositional semantics for the grammar is relatively straightforward. It has a completely transparent interface between syntax and semantics and, because CCG is a lexicalized grammar formalism, providing a"
J07-4004,W02-2018,0,0.164791,"imating the parameters of a PCFG from unlabeled data (Lari and Young 1990). Generalized Iterative Scaling (Darroch and Ratcliff 1972) is a common choice in the NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curran and Clark 2003). Initially we used generalized iterative scaling (GIS) for the parsing models described here, but found that convergence was extremely slow; Sha and Pereira (2003) present a similar finding for globally optimized log-linear models for sequences. As an alternative to GIS, we use the limited-memory BFGS algorithm (Nocedal and Wright 1999). As Malouf (2002) demonstrates, general purpose numerical optimization algorithms such as BFGS can converge much faster than iterative scaling algorithms (including Improved Iterative Scaling; Della Pietra, Della Pietra, and Lafferty 1997). Despite the use of a packed representation, the complete set of derivations for the sentences in the training data requires up to 25 GB of RAM for some of the models in this article. There are a number of ways to solve this problem. Possibilities include using a subset of the training data; repeatedly parsing the training data for each iteration of the estimation algorithm;"
J07-4004,J93-2004,0,0.0402583,"Missing"
J07-4004,W03-0401,0,0.0135774,"y a random sample. The CCG grammar used in this article is automatically extracted, has wide coverage, and can produce an extremely large number of derivations for some sentences, far too many to enumerate. We adapt the feature-forest method of Miyao and Tsujii (2002), which involves using dynamic programming to efficiently calculate the feature expectations. Geman and Johnson (2002) propose a similar method in the context of LFG parsing; an implementation is described in Kaplan et al. (2004). Miyao and Tsujii have carried out a number of investigations similar to the work in this article. In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG). One of Miyao and Tsujii’s motivations is to model predicate–argument dependencies, including long-range dependencies, which was one of the original motivations of the wide-coverage CCG parsing project. Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted LTAG which uses a simple unigram model of the elementary trees together with a loglinear model of the attachments. Miyao and Tsujii (20"
J07-4004,C04-1204,0,0.0242163,"Missing"
J07-4004,P05-1011,0,0.0315053,"yao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG). One of Miyao and Tsujii’s motivations is to model predicate–argument dependencies, including long-range dependencies, which was one of the original motivations of the wide-coverage CCG parsing project. Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted LTAG which uses a simple unigram model of the elementary trees together with a loglinear model of the attachments. Miyao and Tsujii (2005) address the issue of practical estimation using an automatically extracted HPSG grammar. A simple unigram model of lexical categories is used to limit the size of the charts for training, in a similar way to how we use a CCG supertagger to restrict the size of the charts. The main differences between Miyao and Tsujii’s work and ours, aside from the different grammar formalisms, are as follows. The CCG supertagger is a key component of our parsing system. It allows practical estimation of the log-linear models as well as highly efficient parsing. The Maximum Entropy supertagger we use could al"
J07-4004,W04-3308,0,0.0202605,"Missing"
J07-4004,C00-1085,0,0.123783,"alizes models whose weights get too large in absolute value. This smoothing method for log-linear models is also proposed by Chen and Rosenfeld (1999). Calculating the conditional feature expectations can still be problematic if the grammar licenses a large number of analyses for some sentences. This is not a problem for Johnson et al. (1999) because their grammars are hand-written and constraining enough to allow the analyses for each sentence to be enumerated. However, for grammars with wider coverage it is often not possible to enumerate the analyses for each sentence in the training data. Osborne (2000) investigates training on a sample of the analyses for each sentence, for example the top-n most probable according to some other probability model, or simply a random sample. The CCG grammar used in this article is automatically extracted, has wide coverage, and can produce an extremely large number of derivations for some sentences, far too many to enumerate. We adapt the feature-forest method of Miyao and Tsujii (2002), which involves using dynamic programming to efficiently calculate the feature expectations. Geman and Johnson (2002) propose a similar method in the context of LFG parsing;"
J07-4004,W97-1010,0,0.0561094,"he CCG grammars used in this article, the automatically extracted LTAG grammars have, as yet, been too large to enable effective supertagging (as discussed in the previous section). We are not aware of any other work which has demonstrated the parsing efficiency benefits of supertagging using an automatically extracted grammar. 3.2 Previous Work on CCG Statistical Parsing The work in this article began as part of the Edinburgh wide-coverage CCG parsing project (2000–2004). There has been some other work on defining stochastic categorial grammars, but mainly in the context of grammar learning (Osborne and Briscoe 1997; Watkinson and Manandhar 2001; Zettlemoyer and Collins 2005). An early attempt from the Edinburgh project at wide-coverage CCG parsing is presented in Clark, Hockenmaier, and Steedman (2002). In order to deal with the problem of the additional, nonstandard CCG derivations, a conditional model of dependency structures is presented, based on Collins (1996), in which the dependencies are modeled directly and derivations are not modeled at all. The conditional probability of a dependency structure π, given a sentence S, is factored into two parts. The first part is the probability of the lexical"
J07-4004,J05-1004,0,0.113316,"Missing"
J07-4004,E03-1025,0,0.0157022,"ork comparing parser performance across formalisms. Briscoe and Carroll (2006) evaluate the RASP parser on the Parc Dependency Bank (DepBank; King et al. 2003). Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank. Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005). Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the LFG parser is more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins and Charniak, the grammatical relations finder of Buchholz, Veenstra, and Daelemans (1999), and the Briscoe and Carroll (2002) parser, using the gold-standard grammatical relations (GRs) from Carroll, Briscoe, and Sanfilippo (1998). The Penn Treebank trees of the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into the required grammatical relations, with the result that the GR finder of Buchholz is the most accurate. There are a number of problems with such evaluations. The first is that, when converting the output of the Collins p"
J07-4004,P02-1035,0,0.396294,"in NLP, for example, POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999). Log-linear models are also referred to as maximum entropy models and random fields in the NLP literature. They are popular because of the ease with which complex discriminating features can be included in the model, and have been shown to give good performance across a range of NLP tasks. Log-linear models have previously been applied to statistical parsing (Johnson et al. 1999; Toutanova et al. 2002; Riezler et al. 2002; Malouf and van Noord 2004), but typically under the assumption that all possible parses for a sentence can be enumerated. For manually constructed grammars, this assumption is usually sufficient for efficient estimation and decoding. However, for wide-coverage grammars extracted from a treebank, enumerating all parses is infeasible. In this article we apply the dynamic programming method of Miyao and Tsujii (2002) to a packed chart; however, because the grammar is automatically extracted, the packed charts require a considerable amount of memory: up to 25 GB. We solve this massive estimation"
J07-4004,W05-1513,0,0.0101864,"ample by changing the beam parameter in the Collins (2003) parser, but that any increase in speed is typically associated with a reduction in accuracy. For the CCG parser, the accuracy did not degrade when using the new adaptive parsing strategy. Thus the accuracy and efficiency of the parser were not tuned separately: The configuration used to obtain the speed results was also used to obtain the accuracy results in Sections 10.2 and 11. To give some idea of how these parsing speeds compare with existing parsers, Table 12 gives the parse times on Section 23 for a number of well-known parsers. Sagae and Lavie (2005) is a classifier-based linear time parser. The times for the Sagae, Collins, and Charniak parsers were taken from the Sagae and Lavie paper, and were obtained using a 1.8 GHz P4, compared to a 3.2 GHz P4 for the CCG numbers. Comparing parser speeds is especially problematic because of implementation differences and the fact that the accuracy of the parsers is not being controlled. Thus we are not making any strong claims about the efficiency of parsing with CCG compared to other formalisms. However, the results in Table 12 add considerable weight to one of our main claims in this article, name"
J07-4004,N03-1028,0,0.0458489,"cked chart efficiently represents all derivations for a sentence. The dynamic programming method uses inside and outside scores to calculate expectations, similar to the inside–outside algorithm for estimating the parameters of a PCFG from unlabeled data (Lari and Young 1990). Generalized Iterative Scaling (Darroch and Ratcliff 1972) is a common choice in the NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curran and Clark 2003). Initially we used generalized iterative scaling (GIS) for the parsing models described here, but found that convergence was extremely slow; Sha and Pereira (2003) present a similar finding for globally optimized log-linear models for sequences. As an alternative to GIS, we use the limited-memory BFGS algorithm (Nocedal and Wright 1999). As Malouf (2002) demonstrates, general purpose numerical optimization algorithms such as BFGS can converge much faster than iterative scaling algorithms (including Improved Iterative Scaling; Della Pietra, Della Pietra, and Lafferty 1997). Despite the use of a packed representation, the complete set of derivations for the sentences in the training data requires up to 25 GB of RAM for some of the models in this article."
J07-4004,W04-3201,0,0.0262102,"estimation, could be applied to log-linear parsing models using other grammar formalisms. Despite memory requirements of up to 25 GB we have shown how a parallelized version of the estimation process can limit the estimation time to under three hours, resulting in a practical framework for parser development. One of the problems with modeling approaches which require very long estimation times is that it is difficult to test different configurations of the system, for example different feature sets. It may also not be possible to train or run the system on anything other than short sentences (Taskar et al. 2004). The supertagger is a key component in our parsing system. It reduces the size of the charts considerably compared with naive methods for assigning lexical categories, which is crucial for practical discriminative training. The tight integration of the supertagger and parser enables highly efficient as well as accurate parsing. The parser is significantly faster than comparable parsers in the NLP literature. The supertagger we have developed can be applied to other lexicalized grammar formalisms. Another contribution of the article is the development of log-linear parsing models for CCG. In p"
J07-4004,W04-3222,0,0.0151497,"Missing"
J07-4004,W05-1517,0,0.00904801,"ive node daughters which head sub-derivations leading to the same set of high scoring dependencies. In practice large numbers of such conjunctive nodes lead to very long parse times. As an alternative to finding the most probable dependency structure, we have developed an algorithm which maximizes the expected labeled recall over dependencies. Our algorithm is based on Goodman’s (1996) labeled recall algorithm for the phrase-structure PARSEVAL measures. As far as we know, this is the first application of Goodman’s approach to finding highest scoring dependency structures. Watson, Carroll, and Briscoe (2005) have also applied our algorithm to the grammatical relations output by the RASP parser. The dependency structure, πmax , which maximizes the expected recall is: πmax = argmax π  P(πi |S)|π ∩ πi | (29) πi where πi ranges over the dependency structures for S. The expectation for a single dependency structure π is realized as a weighted intersection over all possible dependency structures πi for S. The intuition is that, if πi is the gold standard, then the number of dependencies recalled in π is |π ∩ πi |. Because we do not know which πi is the gold 517 Computational Linguistics Volume 33, Num"
J07-4004,W00-1307,0,0.042199,"Missing"
J07-4004,W03-2401,0,\N,Missing
J11-4006,J07-4002,0,0.0615978,"Missing"
J11-4006,D07-1086,0,0.186146,"Missing"
J11-4006,H91-1060,0,0.0452822,". We use the Bikel (2004) implementation of the Collins (2003) models, as it is a widely used and well-known parser with state-of-the-art performance. It is important to make the distinction between Collins’s and Bikel’s parsers, as they are not identical. The same is true for their underlying models, which again have slight differences. We use Bikel’s parser in all of our experiments, but will still refer to Collins’s models for the most part. We compare the parser’s performance on the original Penn Treebank and the new NML and JJP bracketed version. We report the standard Parseval measures (Black et al. 1991) labeled bracket precision, recall, and F-scores over all sentences. Sections 02–21 are used for training, Section 00 for development, and testing is carried out on Section 23. 5.1 Initial Experiments Table 6 shows the results of Section 00. The first row comes from training and evaluating on the original Penn Treebank, and the next three are all using the extended NP corpus. The first of these, Original structure, evaluates only the brackets that existed before the NP augmentation. That is, the NML and JJP brackets are removed before calculating these figures, in the same way that the NPB bra"
J11-4006,P06-2006,0,0.0609129,"Missing"
J11-4006,A00-2018,0,0.413551,"Missing"
J11-4006,C02-1126,0,0.0291812,"ould then be quite likely, as the set of possible modifiers is restricted by the probability distribution. However, the reverse (conditioning the head cancer on the modifier lung) would also be informative, as the set of heads is likewise restricted. An NML’s left-most token is rarely the or another uninformative token, and thus the uncorrected head-finding rules are also quite effective. Furthermore, for NMLs such as Judge Curry or Mr Vinken, the left-most token is actually a much better generalization to pass up the tree and base probabilistic actions upon. Finally, Bikel (2004, §6.1.1) and Chiang and Bikel (2002) note that head-finding rules do not affect Collins’s models to a large degree. Using a much simpler set of rules degrades performance by only a small amount, whereas an optimal set of rules derived using Expectation Maximization (EM) does not perform significantly better than the standard ones. For these reasons, choosing the left- or right-most token as the head achieves similar performance. 5.4 The Base-NP Submodel The next alteration to the parser is to turn off the base-NP submodel. Collins (1999, page 179) explains that this separate model is used because the Penn Treebank does not fully"
J11-4006,P07-1032,1,0.900825,"Missing"
J11-4006,P96-1025,0,0.220978,"Missing"
J11-4006,P97-1003,0,0.230288,"Missing"
J11-4006,J03-4003,0,0.296336,"ed entity data to improve annotation quality. We check the correctness of the corpus by measuring inter-annotator agreement and by comparing against DepBank (King et al. 2003). We also analyze our extended Treebank, quantifying how much structure we have added, and how it is distributed across NPs. This new resource will allow any system or corpus developed from the Penn Treebank to represent noun phrase structure more accurately. Our next contribution is to conduct the first large-scale experiments on NP parsing. We use the newly augmented Treebank with the Bikel (2004) implementation of the Collins (2003) model. Through a number of experiments, we determine what effect various aspects of Collins’s model, and the data itself, have on parsing performance. Finally, we perform a comprehensive error analysis which identifies the many difficulties in parsing NPs. This shows that the primary difficulty in bracketing NP structure is a lack of lexical information in the training data. In order to increase the amount of information included in the NP parsing model, we turn to NP bracketing. This task has typically been approached with unsupervised methods, using statistics from unannotated corpora (Laue"
J11-4006,W04-3233,0,0.060878,"Missing"
J11-4006,1997.iwpt-1.13,0,0.0567614,"valent, and we have already seen problems caused by this approach in Section 5.2. 5.5 Bracket Structure We have now seen how a Collins-style parser performs on internal NP structure, but the question remains about whether the structure itself is optimal. Treebank structure can have a large effect on parser performance, as has been studied by many researchers. Collins (2003, page 621) notes that binary trees would be a poor choice, as the parser loses some context sensitivity, and the distance measures (§3.1.1) become ineffective. He advocates one level of bracketing structure per X-bar level. Goodman (1997) on the other hand, explicitly converts trees to a binary branching format as a preprocessing step, in order to avoid problems from varying structures. Johnson (1998) finds that the performance of simple PCFGs can be improved through tree transformations, whereas Klein and Manning (2001) observe that some simple tree transformations can increase parsing speed. Petrov et al. (2006) perform automatic tree transformations by splitting nonterminal symbols, creating a smaller grammar that achieves state-of-the-art performance. The variation shown in these approaches, all for the same task, highligh"
J11-4006,P89-1015,0,0.29181,"Missing"
J11-4006,J93-1005,0,0.272706,"nd that 35 out of the 279 non-error NPs in his data set fit this category, for example, city sewerage systems and government policy decisions. It is the government policy in question in the latter example, but also policy decisions and government decisions, resulting in all three possible dependencies. Marcus, Santorini, and Marcinkiewicz (1993) make some mention of indeterminate NPs, calling them permanent predictable ambiguities, a term they ascribe to Martin Kay. The example a boatload of warriors blown ashore is given, which is similar to the prepositional phrase attachment ambiguities in Hindle and Rooth (1993). The meanings of both attachments are true in cases like this: the boatload was blown ashore, and so were the warriors. Marcus et al. (1994) describe the *PPA* trace used in the Penn Treebank, which is applied to these permanent predictable ambiguities, or as we have called them, indeterminates. However *PPA* is also applied to cases of general ambiguity (those described in the following paragraphs), whereas we would separate the two. The final category that we suggest is for ambiguous NPs. These NPs do have a leftor right-branching structure, although the annotator has no hope of determining"
J11-4006,J98-4004,0,0.178783,"Missing"
J11-4006,P01-1044,0,0.0374381,"have a large effect on parser performance, as has been studied by many researchers. Collins (2003, page 621) notes that binary trees would be a poor choice, as the parser loses some context sensitivity, and the distance measures (§3.1.1) become ineffective. He advocates one level of bracketing structure per X-bar level. Goodman (1997) on the other hand, explicitly converts trees to a binary branching format as a preprocessing step, in order to avoid problems from varying structures. Johnson (1998) finds that the performance of simple PCFGs can be improved through tree transformations, whereas Klein and Manning (2001) observe that some simple tree transformations can increase parsing speed. Petrov et al. (2006) perform automatic tree transformations by splitting nonterminal symbols, creating a smaller grammar that achieves state-of-the-art performance. The variation shown in these approaches, all for the same task, highlights the difficulty in identifying optimal tree structure. ¨ Kubler (2005) investigates two German treebanks with different annotation schemes, and finds that certain properties, such as having unary nodes and flatter clauses, increase performance. Rehbein and van Genabith (2007) suggest t"
J11-4006,W04-3111,0,0.40213,"nts in Section 6. Extending the Penn Treebank annotation scheme and corpus is one of the major contributions of this article. There are a handful of corpora annotated with NP structure already, although these do not meet our requirements. DepBank (King et al. 2003) fully annotates NPs, as does the Briscoe and Carroll (2006) reannotation of DepBank. This corpus consists of only 700 sentences, however. The Redwoods Treebank (Oepen et al. 2002) also includes NP structure, but is again comparatively small and not widely used in the parsing community. The Biomedical Information Extraction Project (Kulick et al. 2004) introduces the use of NML nodes to mark internal NP structure in its Addendum to the Penn Treebank Bracketing Guidelines (Warner et al. 2004). This corpus is specifically focused on biomedical text, however, rather than newspaper text. We still base our approach to bracketing NP structure on these biomedical guidelines, as the grammatical structure being annotated remains similar. We chose to augment the WSJ section of the Penn Treebank with the necessary NP structure, as it is the corpus most widely used in the parsing field for English. This also meant that the NP information would not need"
J11-4006,N04-1016,0,0.043278,"Missing"
J11-4006,P95-1037,0,0.3885,"Missing"
J11-4006,H94-1020,0,0.201952,"t is the government policy in question in the latter example, but also policy decisions and government decisions, resulting in all three possible dependencies. Marcus, Santorini, and Marcinkiewicz (1993) make some mention of indeterminate NPs, calling them permanent predictable ambiguities, a term they ascribe to Martin Kay. The example a boatload of warriors blown ashore is given, which is similar to the prepositional phrase attachment ambiguities in Hindle and Rooth (1993). The meanings of both attachments are true in cases like this: the boatload was blown ashore, and so were the warriors. Marcus et al. (1994) describe the *PPA* trace used in the Penn Treebank, which is applied to these permanent predictable ambiguities, or as we have called them, indeterminates. However *PPA* is also applied to cases of general ambiguity (those described in the following paragraphs), whereas we would separate the two. The final category that we suggest is for ambiguous NPs. These NPs do have a leftor right-branching structure, although the annotator has no hope of determining which is correct. This may be because of technical jargon (e.g., senior subordinated debentures), or simply an ambiguity that cannot be reso"
J11-4006,J93-2004,0,0.0408046,"Missing"
J11-4006,W07-1002,0,0.0309666,"Missing"
J11-4006,P04-1084,0,0.0268235,"Missing"
J11-4006,W05-0603,0,0.489321,"ious data sets. Previous researchers have typically used Lauer’s set (244 NPs) or created their own small set (∼500 NPs at most). This new, much larger data set means that we can carry out large-scale machine learning effectively, rather than using unsupervised methods. Statistics comparing our new data set to those used by other researchers are shown in Table 13. As can be seen, the Penn Treebank-based corpus is significantly larger than all other data sets. The distribution of left- and right-branching NPs also appears to vary greatly, which may be affected by the content of the corpus. The Nakov and Hearst (2005) biomedical and Barker (1998) small engines data sets are both very technical texts, and the Buckeridge and Sutcliffe (2002) AmiPro software manual and Buckeridge and Sutcliffe (2002) Time magazine articles are probably aimed at a more general audience. 6.1.2 Complex NPs. We have also extracted another even larger data set of complex NPs for the experiments in Section 6.4. For this set we retrieve an example for each NP of length three or more in the Penn Treebank. We will only be identifying the structure in NML and JJP brackets and so NPs with other child nodes (e.g., prepositional phrases)"
J11-4006,C02-2025,0,0.0447086,"describe the process of manually annotating such a corpus of NP structure. The data will then be used in the parsing experiments of Section 5 and the NP Bracketing experiments in Section 6. Extending the Penn Treebank annotation scheme and corpus is one of the major contributions of this article. There are a handful of corpora annotated with NP structure already, although these do not meet our requirements. DepBank (King et al. 2003) fully annotates NPs, as does the Briscoe and Carroll (2006) reannotation of DepBank. This corpus consists of only 700 sentences, however. The Redwoods Treebank (Oepen et al. 2002) also includes NP structure, but is again comparatively small and not widely used in the parsing community. The Biomedical Information Extraction Project (Kulick et al. 2004) introduces the use of NML nodes to mark internal NP structure in its Addendum to the Penn Treebank Bracketing Guidelines (Warner et al. 2004). This corpus is specifically focused on biomedical text, however, rather than newspaper text. We still base our approach to bracketing NP structure on these biomedical guidelines, as the grammatical structure being annotated remains similar. We chose to augment the WSJ section of th"
J11-4006,P02-1040,0,0.0823096,"Missing"
J11-4006,P06-1055,0,0.160024,"Missing"
J11-4006,W95-0107,0,0.15117,"rmance. For this reason, and to implement the separate base-NP submodel, a preprocessing step is taken wherein NP brackets that do not dominate any other non-possessive NP nodes are relabeled as NPB. For consistency, an extra NP bracket is inserted around NPB nodes not already dominated by an NP. These NPB nodes are removed before evaluation. An example of this transformation can be seen here: (S (NP (DT The) (NN dog) ) (VP (VBZ barks) ) ) (S (NP (NPB (DT The) (NN dog) ) ) (VP (VBZ barks) ) ) 2.3 NP Bracketing Many approaches to identifying noun phrases have been explored as part of chunking (Ramshaw and Marcus 1995), but determining internal NP structure is rarely addressed. Recursive NP bracketing—as in the CoNLL 1999 shared task and as performed by Daum´e III and Marcu (2004)—is closer, but still less difficult than full NP bracketing. Neither of these tasks require the recovery of full sub-NP structure, which is in part because gold-standard annotations for this task have not been available in the past. Instead, we turn to the NP bracketing task as framed by Marcus (1980, page 253) and Lauer (1995), described as follows: given a three-word noun phrase like those here, decide whether it is left branchi"
J11-4006,W97-0301,0,0.138448,"Missing"
J11-4006,D07-1066,0,0.0584423,"Missing"
J11-4006,P02-1035,0,0.114114,"Missing"
J11-4006,P07-1031,1,0.881596,"of accuracy that can be exploited by many practical applications. As a result, we have made it possible to increase performance on question answering, anaphora resolution, and many other downstream NLP tasks. Appendix A: Annotation Guidelines This document describes guidelines for bracketing NP structure in the Penn Treebank. These guidelines are in addition to the Treebank II Guidelines (Bies et al. 1995). They are also based on, and overlap with, the Addendum for BioMedical Annotation (Warner et al. 2004). An earlier version (0.9) of these guidelines was used in the annotation described in Vadas and Curran (2007), whereas this version was used in a subsequent pass over the data. A.1. Bracketing NPs The goal of our annotation is to identify and bracket multi-token premodifiers in NPs. Quirk et al. (1985, page 1321) describe such premodifiers, which include adjectives, participles, nouns, genitives, and adverbs. All of these items are modifiable themselves, and this is precisely the behavior that we have annotated. Indeed, NPs with multiple premodifiers can be recursive to an arbitrary depth (though more than three or four levels is unusual), and the underlying structure is by no means always right-bran"
J11-4006,D07-1078,0,0.04569,"Missing"
J11-4006,N06-1033,0,0.079257,"Missing"
N06-1019,P04-1041,0,0.0413665,"Missing"
N06-1019,C02-1013,0,0.0613012,"Missing"
N06-1019,P00-1058,0,0.0604462,"Missing"
N06-1019,W03-1013,1,0.882703,"tions leading to each gold-standard dependency structure, and the second is over all derivations for each sentence in the training data. The estimation process attempts to make the expectations in (5) equal (ignoring the Gaussian prior term). Another way to think of the estimation process is that it attempts to put as much mass as possible on the derivations leading to the gold-standard structures (Riezler et al., 2002). Calculation of the feature expectations requires summing over all derivations for a sentence, and summing over all derivations leading to a goldstandard dependency structure. Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. 146 The partial data we use for training the dependency model is derived from CCG lexical category sequences only. Figure 1 gives an example sentence adapted from CCGbank (Hockenmaier, 2003) together with its lexical category sequence. Note that, although the attachment of the prepositional phrase to the noun phrase is not explicitly"
N06-1019,C04-1041,1,0.249881,"parsers for new domains and languages. Previous work has attempted parser adaptation without relying on treebank data from the new domain (Steedman et al., 2003; Lease and Charniak, 2005). In this paper we propose the use of annotated data in the new domain, but only partially annotated data, which reduces the annotation effort required (Hwa, 1999). We develop a parsing model which can be trained using partial data, by exploiting the properties of lexicalized grammar formalisms. The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned to the words in the sentence, and then the categories are combined by the parser (Clark and Curran, 2004a). The lexical categories can be thought of as detailed part of speech tags and typically express subcategorization information. We exploit the fact that CCG lexical categories contain a lot of syntactic information, and can therefore be used for training a full parser, even though attachment information is not explicitly repres"
N06-1019,P04-1014,1,0.906989,"Missing"
N06-1019,W04-3215,1,0.878626,"he parser (Clark and Curran, 2004a). The lexical categories can be thought of as detailed part of speech tags and typically express subcategorization information. We exploit the fact that CCG lexical categories contain a lot of syntactic information, and can therefore be used for training a full parser, even though attachment information is not explicitly represented in a category sequence. Our partial training regime only requires sentences to be annotated with lexical categories, rather than full parse trees; therefore the data can be produced much more quickly for a new domain or language (Clark et al., 2004). The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument de144 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 144–151, c New York, June 2006. 2006 Association for Computational Linguistics pendencies, rather than derivations, for training. Our novel idea is that, since there is so much information in the lexical category sequence, most of the correct dependencies can be easily inferred from the categories alone. More specifically, for a given sentence"
N06-1019,W97-1505,0,0.0545329,"Missing"
N06-1019,P02-1043,0,0.0924825,"Missing"
N06-1019,P99-1010,0,0.0471612,". The labour-intensive nature of the treebank development process, which can take many James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au years, creates a significant barrier for the development of parsers for new domains and languages. Previous work has attempted parser adaptation without relying on treebank data from the new domain (Steedman et al., 2003; Lease and Charniak, 2005). In this paper we propose the use of annotated data in the new domain, but only partially annotated data, which reduces the annotation effort required (Hwa, 1999). We develop a parsing model which can be trained using partial data, by exploiting the properties of lexicalized grammar formalisms. The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned to the words in the sentence, and then the categories are combined by the parser (Clark and Curran, 2004a). The lexical categories can be thought of as detailed pa"
N06-1019,I05-1006,0,0.064504,"Clark and Curran, 2004b; Hockenmaier, 2003), HPSG (Miyao et al., 2004) and LFG (Riezler et al., 2002; Cahill et al., 2004), often use training data derived from the Penn Treebank. The labour-intensive nature of the treebank development process, which can take many James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au years, creates a significant barrier for the development of parsers for new domains and languages. Previous work has attempted parser adaptation without relying on treebank data from the new domain (Steedman et al., 2003; Lease and Charniak, 2005). In this paper we propose the use of annotated data in the new domain, but only partially annotated data, which reduces the annotation effort required (Hwa, 1999). We develop a parsing model which can be trained using partial data, by exploiting the properties of lexicalized grammar formalisms. The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned"
N06-1019,W02-2018,0,0.0210346,"ing a log-linear form: P (ω|S) = P 1 λ.f (ω) e ZS (3) where λ.f (ω) = i λi fi (ω). The function fi is the integer-valued frequency function of the ith feature; λi is the weight of the ith feature; and ZS is a normalising constant. Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al., 2002). The optimisation of the objective function is performed using the limited-memory BFGS numerical optimisation algorithm (Nocedal and Wright, 1999; Malouf, 2002), which requires calculation of the objective function and the gradient of the objective function at each iteration. The objective function is defined below, where L(Λ) is the likelihood and G(Λ) is a Gaussian prior term for smoothing. He anticipates growth NP (S [dcl ]NP )/NP NP for the auto maker (NP NP )/NP NP [nb]/N N /N N Figure 1: Example sentence with CCG lexical categories 3 Partial Training L0 (Λ) = L(Λ) − G(Λ) = m X j=1 − X log m X (4) eλ.f (d,πj ) d∈∆(πj ) log j=1 n X λ2i eλ.f (ω) − 2σ 2 i=1 ω∈ρ(S ) X j S1 , . . . , Sm are the sentences in the training data; π1 , . . . , πm are th"
N06-1019,P92-1017,0,0.265797,"ncy F-score than the fulldata model, despite the fact that the partial data does not contain any explicit attachment information. 2 The CCG Parsing Model Clark and Curran (2004b) describes two log-linear parsing models for CCG: a normal-form derivation model and a dependency model. In this paper we use the dependency model, which requires sets of predicate-argument dependencies for training.1 1 Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations; one possibility for training this model on partial data, which has not been explored, is to use the EM algorithm (Pereira and Schabes, 1992). 145 The predicate-argument dependencies are represented as 5-tuples: hhf , f, s, ha , li, where hf is the lexical item of the lexical category expressing the dependency relation; f is the lexical category; s is the argument slot; ha is the head word of the argument; and l encodes whether the dependency is non-local. For example, the dependency encoding company as the object of bought (as in IBM bought the company) is represented as follows: hbought2 , (S NP1 )/NP2 , 2, company4 , −i (1) CCG dependency structures are sets of predicateargument dependencies. We define the probability of a depe"
N06-1019,P02-1035,0,0.358514,": P (π|S) = X P (d, π|S) (2) d∈∆(π) where ∆(π) is the set of derivations which lead to π. The probability of a hd, πi pair, ω, conditional on a sentence S, is defined using a log-linear form: P (ω|S) = P 1 λ.f (ω) e ZS (3) where λ.f (ω) = i λi fi (ω). The function fi is the integer-valued frequency function of the ith feature; λi is the weight of the ith feature; and ZS is a normalising constant. Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al., 2002). The optimisation of the objective function is performed using the limited-memory BFGS numerical optimisation algorithm (Nocedal and Wright, 1999; Malouf, 2002), which requires calculation of the objective function and the gradient of the objective function at each iteration. The objective function is defined below, where L(Λ) is the likelihood and G(Λ) is a Gaussian prior term for smoothing. He anticipates growth NP (S [dcl ]NP )/NP NP for the auto maker (NP NP )/NP NP [nb]/N N /N N Figure 1: Example sentence with CCG lexical categories 3 Partial Training L0 (Λ) = L(Λ) − G(Λ) = m X j=1 − X"
N06-1019,E03-1008,1,0.846865,"G (Chiang, 2000), CCG (Clark and Curran, 2004b; Hockenmaier, 2003), HPSG (Miyao et al., 2004) and LFG (Riezler et al., 2002; Cahill et al., 2004), often use training data derived from the Penn Treebank. The labour-intensive nature of the treebank development process, which can take many James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au years, creates a significant barrier for the development of parsers for new domains and languages. Previous work has attempted parser adaptation without relying on treebank data from the new domain (Steedman et al., 2003; Lease and Charniak, 2005). In this paper we propose the use of annotated data in the new domain, but only partially annotated data, which reduces the annotation effort required (Hwa, 1999). We develop a parsing model which can be trained using partial data, by exploiting the properties of lexicalized grammar formalisms. The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexic"
N12-1030,P11-1048,0,0.0279411,"10: Analysis of the 8,414 false positive dependencies from C&C on  6 dev set caused by supertagging errors (the bottom row), but most of these are not a result of incorrect  tags, demonstrating that supertagging and parsing are difﬁcult even with correct  tags. The sensitivity of C&C to tagging errors, and the higher performance of the P&K parser, which does not directly use  tags, calls into question whether  tagging yields a net gain in a language where distinctions such as the noun/verb ambiguity are often difﬁcult to resolve using local tagging approaches. The approach of Auli and Lopez (2011), which achieves superior results in English  parsing with a joint supertagging/parsing model, may be promising in light of the performance difference between P&K and C&C. 6.1 Non-local dependencies Table 9 shows how well the best models of each parser recovered selected local and non-local dependencies. The slot represented by each row appears in boldface. While C&C and P&K perform similarly recovering NP-internal structure, the ability of P&K to recover verbal arguments, unbounded long-range dependencies such as subject and object extraction, and bounded long-range dependencies such as co"
N12-1030,W00-1201,0,0.14494,"Missing"
N12-1030,C02-1126,0,0.0873261,"Missing"
N12-1030,C04-1041,1,0.713925,"which generate s, including the 把 ba/被 bei constructions, topicalisation and extraction receive natural  analyses in Chinese 296 (b) Lexical .S/.S .N.P . .S/.S . .(S/S.)/N . N .. .NP./N . N .. Figure 2: Two types of ambiguity CCGbank. Figure 1 shows the CCGbank analysis of passivisation, topicalisation and extraction, creating s between 公主 princess and each of 被 , 困 trap and 解救 rescue respectively. We take two state-of-the-art parsers and train them to establish the difﬁculty of parsing Chinese with . The ﬁrst is the Clark and Curran (C&C; 2007) parser, which uses supertagging (Clark and Curran, 2004), a local, linear-time tagging technique which drastically prunes the space of lexical categories which the polynomial-time parsing algorithm later considers. The second is the coarse-to-ﬁne parser of Petrov and Klein (2007) which iteratively reﬁnes its grammar by splitting production rules to uncover latent distinctions. Fowler and Penn (2010) demonstrate that the English CCGbank grammar is strongly context-free, allowing them to treat it as a  and train the Petrov and Klein (2007) parser directly. 2.1 Derivational vs. lexical ambiguity The designer of a CCGbank must frequently choose betw"
N12-1030,J07-3004,0,0.0712726,"(C&C) parser, uncovering a surprising performance gap between them not observed in English — 72.73 (P&K) and 67.09 (C&C) F -score on  6. We explore the challenges of Chinese  parsing through three novel ideas: developing corpus variants rather than treating the corpus as ﬁxed; controlling noun/verb and other  ambiguities; and quantifying the impact of constructions like pro-drop. 1 Introduction Automatic corpus conversions from the Penn Treebank (Marcus et al., 1994) have driven research in lexicalised grammar formalisms, such as  (Xia, 1999),  (Miyao et al., 2004) and  (Hockenmaier and Steedman, 2007), producing the lexical resources key to wide-coverage statistical parsing. The Chinese Penn Treebank (; Xue et al., 2005) has ﬁlled a comparable niche, enabling the development of a Chinese  (Xia et al., 2000), a wide-coverage  parser (Yu et al., 2011), and recently Chinese CCGbank (Tse and Curran, 2010), a 750 000-word corpus of Combinatory Categorial Grammar (; Steedman, 2000) derivations. We train two  parsers, Clark and Curran (C&C; 2007), and the Petrov and Klein (P&K; 2007)  parser, on Chinese CCGbank. We follow Fowler and Penn (2010), who treat the English CCGbank"
N12-1030,D09-1127,0,0.0301358,"Missing"
N12-1030,P03-1056,0,0.145334,"007)  parser, on Chinese CCGbank. We follow Fowler and Penn (2010), who treat the English CCGbank (Hockenmaier and Steedman, 2007) grammar as a  and train and evaluate the P&K parser directly on it. We obtain the ﬁrst Chinese  parsing results: F -scores of 72.73 (P&K) and 67.09 (C&C) on labelled dependencies computed over the  6 test set. While the state-of-the-art in Chinese syntactic parsing has always lagged behind English, this large gap is surprising, given that Fowler and Penn (2010) found only a small margin separated the two parsers on English CCGbank (86.0 versus 85.8). Levy and Manning (2003) established that properties of Chinese such as noun/verb ambiguity contribute to the difﬁculty of Chinese parsing. We focus on two factors within our control: annotation decisions and parser architecture. Existing research has varied parsers whilst keeping the corpus ﬁxed. We vary the corpus whilst keeping the parsers ﬁxed by exploring multiple design choices for particular constructions. By exploiting the fully automatic CCGbank extraction process, we can immediately implement these choices and assess their impact on parsing performance. Secondly, we contrast the performance of C&C, with its"
N12-1030,J07-4004,1,0.953125,"te a version of Chinese CCGbank (corpus C) which neutralises the distinction. This eliminates the atomic category N, as well as the promotion rule N → NP. 4 Experiments While a standard split of  5 exists, as deﬁned by Zhang and Clark (2008), we are not aware of a consistently used split for  6. We present a new split in Table 1 which adds data from the  broadcast section of  6, maintaining the same train/dev/test set proportions as the  5 split. We train C&C using the hybrid model, the bestperforming model for English, which extracts features from the dependency structure (Clark and Curran, 2007). We use β = ⟨0.055, 0.01, 0.05, 0.1⟩ during training with a Gaussian smoothing parameter α = 2.4 (optimised on the corpus A dev set). We use β = ⟨0.15, 0.075, 0.03, 0.01, 0.005, 0.001⟩ during parsing, with the maximum number of supercats (chart entries) set to 5,000,000, reﬂecting the greater supertagging ambiguity of Chinese parsing. The P&K parser is used “off-the-shelf” and trained with its default parameters, only varying the number of split-merge iterations and enabling the Chinesespeciﬁc lexicon features. The P&K parser involves no explicit  tagging step, as the (super)tags correspon"
N12-1030,N06-1020,0,0.0677468,"Missing"
N12-1030,E03-1071,1,0.633834,"rd Clark and Curran (2007) dependencybased evaluation from the  literature: labelled F score (LF ) over dependency tuples, as used for  parser evaluation in English. Critically, this metric is also -sensitive. We also report labelled sentence accuracy (Lsa), the proportion of sentences for which the parser returned all and only the gold standard dependencies. Supertagger accuracy compares leaf categories against the gold standard (stag). For C&C, we report on two conﬁgurations: , evaluated using gold standard  tags; and , with automatic  tags provided by the C&C tagger (Curran and Clark, 2003). For P&K, we vary the number of split-merge iterations from one to six (following Fowler and Penn (2010), the k -iterations model is called I-k ). Because the P&K parser does not use  tags, the most appropriate comparison is against the  conﬁguration of C&C. For C&C, we use the average of the logarithm of the chart size (log C ) as a measure of ambiguity, that is, the number of alternative analyses the parser must choose between. Following Fowler and Penn (2010), we perform two sets of experiments: one evaluated over all sentences in a section, and another evaluated only over sentences"
N12-1030,P10-1035,0,0.0802681,"t al., 2004) and  (Hockenmaier and Steedman, 2007), producing the lexical resources key to wide-coverage statistical parsing. The Chinese Penn Treebank (; Xue et al., 2005) has ﬁlled a comparable niche, enabling the development of a Chinese  (Xia et al., 2000), a wide-coverage  parser (Yu et al., 2011), and recently Chinese CCGbank (Tse and Curran, 2010), a 750 000-word corpus of Combinatory Categorial Grammar (; Steedman, 2000) derivations. We train two  parsers, Clark and Curran (C&C; 2007), and the Petrov and Klein (P&K; 2007)  parser, on Chinese CCGbank. We follow Fowler and Penn (2010), who treat the English CCGbank (Hockenmaier and Steedman, 2007) grammar as a  and train and evaluate the P&K parser directly on it. We obtain the ﬁrst Chinese  parsing results: F -scores of 72.73 (P&K) and 67.09 (C&C) on labelled dependencies computed over the  6 test set. While the state-of-the-art in Chinese syntactic parsing has always lagged behind English, this large gap is surprising, given that Fowler and Penn (2010) found only a small margin separated the two parsers on English CCGbank (86.0 versus 85.8). Levy and Manning (2003) established that properties of Chinese such as"
N12-1030,N07-1051,0,0.0644901,"biguity CCGbank. Figure 1 shows the CCGbank analysis of passivisation, topicalisation and extraction, creating s between 公主 princess and each of 被 , 困 trap and 解救 rescue respectively. We take two state-of-the-art parsers and train them to establish the difﬁculty of parsing Chinese with . The ﬁrst is the Clark and Curran (C&C; 2007) parser, which uses supertagging (Clark and Curran, 2004), a local, linear-time tagging technique which drastically prunes the space of lexical categories which the polynomial-time parsing algorithm later considers. The second is the coarse-to-ﬁne parser of Petrov and Klein (2007) which iteratively reﬁnes its grammar by splitting production rules to uncover latent distinctions. Fowler and Penn (2010) demonstrate that the English CCGbank grammar is strongly context-free, allowing them to treat it as a  and train the Petrov and Klein (2007) parser directly. 2.1 Derivational vs. lexical ambiguity The designer of a CCGbank must frequently choose between derivational and lexical ambiguity (Hockenmaier, 2003; Tse and Curran, 2010). Derivational ambiguity analyses special constructions through arbitrary label-rewriting phrase structure rules, while lexical ambiguity assign"
N12-1030,D07-1027,0,0.295876,"Missing"
N12-1030,D09-1085,0,0.180649,"tagging step, as the (super)tags correspond directly to non-terminals in a . Fowler and Penn (2010) use the C&C tool generate to convert P&K output to the C&C evaluation dependency format. generate critically does not depend on the C&C parsing model, permitting a fair comparison of the parsers’ output. 298  5 Train Test Dev 1–815, 1001–1136 816–885, 1137–1147 900–931, 1148–1151 Table 1: 4.1 + 6 2000–2980 3030–3145 2981–3029 #sents 22033 2758 1101  5 and 6 dev/train/test splits Evaluation Carroll et al. (1998) argued against  in favour of a dependency-based evaluation. Rimell et al. (2009) focus on evaluating  recovery, proposing a dependency-based evaluation and a  mapping procedure for inter-parser comparison. Since the P&K parser plus generate produce dependencies in the same format as C&C, we can use the standard Clark and Curran (2007) dependencybased evaluation from the  literature: labelled F score (LF ) over dependency tuples, as used for  parser evaluation in English. Critically, this metric is also -sensitive. We also report labelled sentence accuracy (Lsa), the proportion of sentences for which the parser returned all and only the gold standard dependen"
N12-1030,C10-1122,1,0.841861,"ities; and quantifying the impact of constructions like pro-drop. 1 Introduction Automatic corpus conversions from the Penn Treebank (Marcus et al., 1994) have driven research in lexicalised grammar formalisms, such as  (Xia, 1999),  (Miyao et al., 2004) and  (Hockenmaier and Steedman, 2007), producing the lexical resources key to wide-coverage statistical parsing. The Chinese Penn Treebank (; Xue et al., 2005) has ﬁlled a comparable niche, enabling the development of a Chinese  (Xia et al., 2000), a wide-coverage  parser (Yu et al., 2011), and recently Chinese CCGbank (Tse and Curran, 2010), a 750 000-word corpus of Combinatory Categorial Grammar (; Steedman, 2000) derivations. We train two  parsers, Clark and Curran (C&C; 2007), and the Petrov and Klein (P&K; 2007)  parser, on Chinese CCGbank. We follow Fowler and Penn (2010), who treat the English CCGbank (Hockenmaier and Steedman, 2007) grammar as a  and train and evaluate the P&K parser directly on it. We obtain the ﬁrst Chinese  parsing results: F -scores of 72.73 (P&K) and 67.09 (C&C) on labelled dependencies computed over the  6 test set. While the state-of-the-art in Chinese syntactic parsing has alwa"
N12-1030,P06-1054,0,0.33288,"Missing"
N12-1030,W00-1208,0,0.0304094,"pus variants rather than treating the corpus as ﬁxed; controlling noun/verb and other  ambiguities; and quantifying the impact of constructions like pro-drop. 1 Introduction Automatic corpus conversions from the Penn Treebank (Marcus et al., 1994) have driven research in lexicalised grammar formalisms, such as  (Xia, 1999),  (Miyao et al., 2004) and  (Hockenmaier and Steedman, 2007), producing the lexical resources key to wide-coverage statistical parsing. The Chinese Penn Treebank (; Xue et al., 2005) has ﬁlled a comparable niche, enabling the development of a Chinese  (Xia et al., 2000), a wide-coverage  parser (Yu et al., 2011), and recently Chinese CCGbank (Tse and Curran, 2010), a 750 000-word corpus of Combinatory Categorial Grammar (; Steedman, 2000) derivations. We train two  parsers, Clark and Curran (C&C; 2007), and the Petrov and Klein (P&K; 2007)  parser, on Chinese CCGbank. We follow Fowler and Penn (2010), who treat the English CCGbank (Hockenmaier and Steedman, 2007) grammar as a  and train and evaluate the P&K parser directly on it. We obtain the ﬁrst Chinese  parsing results: F -scores of 72.73 (P&K) and 67.09 (C&C) on labelled dependencies"
N12-1030,W11-2907,0,0.178446,"ﬁxed; controlling noun/verb and other  ambiguities; and quantifying the impact of constructions like pro-drop. 1 Introduction Automatic corpus conversions from the Penn Treebank (Marcus et al., 1994) have driven research in lexicalised grammar formalisms, such as  (Xia, 1999),  (Miyao et al., 2004) and  (Hockenmaier and Steedman, 2007), producing the lexical resources key to wide-coverage statistical parsing. The Chinese Penn Treebank (; Xue et al., 2005) has ﬁlled a comparable niche, enabling the development of a Chinese  (Xia et al., 2000), a wide-coverage  parser (Yu et al., 2011), and recently Chinese CCGbank (Tse and Curran, 2010), a 750 000-word corpus of Combinatory Categorial Grammar (; Steedman, 2000) derivations. We train two  parsers, Clark and Curran (C&C; 2007), and the Petrov and Klein (P&K; 2007)  parser, on Chinese CCGbank. We follow Fowler and Penn (2010), who treat the English CCGbank (Hockenmaier and Steedman, 2007) grammar as a  and train and evaluate the P&K parser directly on it. We obtain the ﬁrst Chinese  parsing results: F -scores of 72.73 (P&K) and 67.09 (C&C) on labelled dependencies computed over the  6 test set. While the s"
N12-1030,D08-1059,0,0.06057,"vel of NP – also argues against importing the English analysis. In contrast, the English CCGbank determiner category NP/N reﬂects the fact that determiners ‘close off’ NP — further modiﬁcation by noun modiﬁers is blocked after combining with a determiner. (4) 共和党 这 举动 Republican Party this act this action by the Republican Party To test its impact on Chinese parsing, we create a version of Chinese CCGbank (corpus C) which neutralises the distinction. This eliminates the atomic category N, as well as the promotion rule N → NP. 4 Experiments While a standard split of  5 exists, as deﬁned by Zhang and Clark (2008), we are not aware of a consistently used split for  6. We present a new split in Table 1 which adds data from the  broadcast section of  6, maintaining the same train/dev/test set proportions as the  5 split. We train C&C using the hybrid model, the bestperforming model for English, which extracts features from the dependency structure (Clark and Curran, 2007). We use β = ⟨0.055, 0.01, 0.05, 0.1⟩ during training with a Gaussian smoothing parameter α = 2.4 (optimised on the corpus A dev set). We use β = ⟨0.15, 0.075, 0.03, 0.01, 0.005, 0.001⟩ during parsing, with the maximum num"
N12-1030,W09-3825,0,0.681953,"Missing"
N12-1030,J93-2004,0,\N,Missing
P02-1030,W00-1427,0,\N,Missing
P02-1030,W02-0908,1,\N,Missing
P02-1030,C92-2082,0,\N,Missing
P02-1030,N01-1013,0,\N,Missing
P02-1030,P99-1016,0,\N,Missing
P02-1030,J92-4003,0,\N,Missing
P02-1030,P01-1005,0,\N,Missing
P02-1030,P93-1024,0,\N,Missing
P04-1014,J96-1002,0,0.0131687,"he gradient of the objective function to be computed at each iteration. The components of the gradient vector are as follows: ∂L0 (Λ) ∂λi = m X X eλ. f (d,π j ) fi (d, π j ) P λ. f (d,π j ) j=1 d∈∆(π j ) d∈∆(π j ) e − (5) m X X λi eλ. f (ω) fi (ω) − 2 P λ. f (ω) σi j=1 ω∈ρ(S j ) ω∈ρ(S j ) e The first two terms in (5) are expectations of feature fi : the first expectation is over all derivations leading to each gold standard dependency structure; the second is over all derivations for each sentence in the training data. Setting the gradient to zero yields the usual maximum entropy constraints (Berger et al., 1996), except that in this case the empirical values are themselves expectations (over all derivations leading to each gold standard dependency structure). The estimation process attempts to make the expectations equal, by putting as much mass as possible on the derivations leading to the gold standard structures.1 The Gaussian prior term penalises any model whose weights get too large in absolute value. Calculation of the feature expectations requires summing over all derivations for a sentence, and summing over all derivations leading to a gold standard dependency structure. In both cases there c"
P04-1014,W03-1013,1,0.539725,"evelop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including nonstandard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers. 1 Introduction A number of statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG; Steedman, 2000) and used in parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002; Hockenmaier, 2003b). In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG. However, estimating a log-linear model for a widecoverage CCG grammar is very computationally expensive. Following Miyao and Tsujii (2002), we showed how the estimation can be performed efficiently by applying the inside-outside algorithm to a packed chart. We also showed how the complete WSJ Penn Treebank can be used for training by developing a parallel version of Generalised Iterative Scaling (GIS) to perform the estimation. This paper significantly extends our earlier work in a number of ways. First, we evaluate a number of log-l"
P04-1014,C04-1041,1,0.564233,"Missing"
P04-1014,P02-1042,1,0.918306,"ter allowing the complete Penn Treebank to be used for estimation. We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including nonstandard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers. 1 Introduction A number of statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG; Steedman, 2000) and used in parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002; Hockenmaier, 2003b). In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG. However, estimating a log-linear model for a widecoverage CCG grammar is very computationally expensive. Following Miyao and Tsujii (2002), we showed how the estimation can be performed efficiently by applying the inside-outside algorithm to a packed chart. We also showed how the complete WSJ Penn Treebank can be used for training by developing a parallel version of Generalised Iterative Scaling (GIS) to perform the estimation. This paper significantly e"
P04-1014,P96-1025,0,0.0485102,"CG is unusual among grammar formalisms in that, for each derived structure for a sentence, there can be many derivations leading to that structure. The presence of such ambiguity, sometimes referred to as spurious ambiguity, enables CCG to produce elegant analyses of coordination and extraction phenomena (Steedman, 2000). However, the introduction of extra derivations increases the complexity of the modelling and parsing problem. Clark et al. (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. They use a conditional model, based on Collins (1996), which, as the authors acknowledge, has a number of theoretical deficiencies; thus the results of Clark et al. provide a useful baseline for the new models presented here. Hockenmaier (2003a) uses a model which favours only one of the derivations leading to a derived structure, namely the normal-form derivation (Eisner, 1996). In this paper we compare the normal-form approach with a dependency model. For the dependency model, we define the probability of a dependency structure as follows: X P(π|S ) = P(d, π|S ) (1) d∈∆(π) where π is a dependency structure, S is a sentence and ∆(π) is the set"
P04-1014,E03-1071,1,0.264655,"Missing"
P04-1014,P96-1011,0,0.135802,"er, the introduction of extra derivations increases the complexity of the modelling and parsing problem. Clark et al. (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. They use a conditional model, based on Collins (1996), which, as the authors acknowledge, has a number of theoretical deficiencies; thus the results of Clark et al. provide a useful baseline for the new models presented here. Hockenmaier (2003a) uses a model which favours only one of the derivations leading to a derived structure, namely the normal-form derivation (Eisner, 1996). In this paper we compare the normal-form approach with a dependency model. For the dependency model, we define the probability of a dependency structure as follows: X P(π|S ) = P(d, π|S ) (1) d∈∆(π) where π is a dependency structure, S is a sentence and ∆(π) is the set of derivations which lead to π. This extends the approach of Clark et al. (2002) who modelled the dependency structures directly, not using any information from the derivations. In contrast to the dependency model, the normal-form model simply defines a distribution over normalform derivations. The dependency structures consid"
P04-1014,W01-0521,0,0.0802913,"Missing"
P04-1014,P96-1024,0,0.155575,"rt for CCG parsing. We also compare log-linear models which use all CCG derivations, including non-standard derivations, with normal-form models. Second, we find that GIS is unsuitable for estimating a model of the size being considered, and develop a parallel version of the L - BFGS algorithm (Nocedal and Wright, 1999). And finally, we show that the parsing algoJames R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au rithm described in Clark and Curran (2003) is extremely slow in some cases, and suggest an efficient alternative based on Goodman (1996). The development of parsing and estimation algorithms for models which use all derivations extends existing CCG parsing techniques, and allows us to test whether there is useful information in the additional derivations. However, we find that the performance of the normal-form model is at least as good as the all-derivations model, in our experiments todate. The normal-form approach allows the use of additional constraints on rule applications, leading to a smaller model, reducing the computational resources required for estimation, and resulting in an extremely efficient parser. This paper a"
P04-1014,P02-1043,0,0.4146,"plete Penn Treebank to be used for estimation. We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including nonstandard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers. 1 Introduction A number of statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG; Steedman, 2000) and used in parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002; Hockenmaier, 2003b). In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG. However, estimating a log-linear model for a widecoverage CCG grammar is very computationally expensive. Following Miyao and Tsujii (2002), we showed how the estimation can be performed efficiently by applying the inside-outside algorithm to a packed chart. We also showed how the complete WSJ Penn Treebank can be used for training by developing a parallel version of Generalised Iterative Scaling (GIS) to perform the estimation. This paper significantly extends our earlier work in a num"
P04-1014,P03-1046,0,0.78231,"or estimation. We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including nonstandard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers. 1 Introduction A number of statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG; Steedman, 2000) and used in parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002; Hockenmaier, 2003b). In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG. However, estimating a log-linear model for a widecoverage CCG grammar is very computationally expensive. Following Miyao and Tsujii (2002), we showed how the estimation can be performed efficiently by applying the inside-outside algorithm to a packed chart. We also showed how the complete WSJ Penn Treebank can be used for training by developing a parallel version of Generalised Iterative Scaling (GIS) to perform the estimation. This paper significantly extends our earlier work in a number of ways. First,"
P04-1014,P02-1035,0,0.652293,"as in the company that IBM bought) is represented as the following 5-tuple: hbought, (S[dcl]NP1 )/NP2 , 2, company, ∗i where ∗ is the category (NPNP)/(S[dcl]/NP) assigned to the relative pronoun. For local dependencies l is assigned a null value. A dependency structure is a multiset of these dependencies. 3 Log-Linear Parsing Models Log-linear models (also known as Maximum Entropy models) are popular in NLP because of the ease with which discriminating features can be included in the model. Log-linear models have been applied to the parsing problem across a range of grammar formalisms, e.g. Riezler et al. (2002) and Toutanova et al. (2002). One motivation for using a log-linear model is that long-range dependencies which CCG was designed to handle can easily be encoded as features. A conditional log-linear model of a parse ω ∈ Ω, given a sentence S , is defined as follows: P(ω|S ) = 1 λ. f (ω) e ZS (2) P where λ. f (ω) = i λi fi (ω). The function fi is a feature of the parse which can be any real-valued function over the space of parses Ω. Each feature fi has an associated weight λi which is a parameter of the model to be estimated. ZS is a normalising constant which ensures that P(ω|S ) is a probabi"
P04-1014,N03-1028,0,0.0466571,"Missing"
P05-1004,W98-0719,0,0.0210771,"Missing"
P05-1004,A00-1031,0,0.0191055,"r, w0 )p(w, ∗, ∗) (2) where ∗ indicates a global sum over that element of the relation tuple. JACCARD and TT EST produced better quality synonyms than existing measures in the literature, so we use Curran and Moen’s configuration for our supersense tagging experiments. 6.1 Part of Speech Tagging and Chunking Our implementation of S EXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994). The only similar performing tool is the Trigrams ‘n’ Tags tagger (Brants, 2000) which uses a much simpler statistical model. Our implementation uses a maximum entropy chunker which has similar feature types to Koeling (2000) and is also trained on chunks extracted from the entire Penn Treebank using the CoNLL 2000 script. Since the Penn Treebank separates PPs and conjunctions from NPs, they are concatenated to match Grefenstette’s table-based results, i.e. the S EXTANT always prefers noun attachment. 6.2 Morphological Analysis Our implementation uses morpha, the Sussex morphological analyser (Minnen et al., 2001), which is implemented using lex grammars for both affix sp"
P05-1004,W99-0609,0,0.0419857,"Missing"
P05-1004,W93-0106,0,0.686293,"Missing"
P05-1004,W00-0729,0,0.0136388,"Missing"
P05-1004,W03-1022,0,0.330392,"s using Semantic Similarity James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Abstract W ORD N ET with the UMLS medical resource and found only a very small degree of overlap. Also, lexicalsemantic resources suffer from: The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into W ORD N ET. Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples. We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger. We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity. 1 bias towards concepts and senses from particular topics. Some specialist topics are better covered in W ORD N ET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in"
P05-1004,W02-0903,0,0.0252994,"fine-grained senses; and state, which covers companionship. The names and descriptions of the noun lex-files are shown in Table 1. Some lex-files map directly to the top level nodes in the hierarchy, called unique beginners, while others are grouped together as hyponyms of a unique beginner (Fellbaum, 1998, page 30). For example, abstraction subsumes the lex-files attribute, quantity, relation, communication and time. Ciaramita and Johnson (2003) call the noun lex-file classes supersenses. There are 11 unique beginners in the W ORD N ET noun hierarchy which could also be used as supersenses. Ciaramita (2002) has produced a miniW ORD N ET by manually reducing the W ORD N ET hierarchy to 106 broad categories. Ciaramita et al. (2003) describe how the lex-files can be used as root nodes in a two level hierarchy with the W ORD N ET synsets appear27 Previous Work A considerable amount of research addresses structurally and statistically manipulating the hierarchy of W ORD N ET and the construction of new wordnets using the concept structure from English. For lexical FreeNet, Beeferman (1998) adds over 350 000 collocation pairs (trigger pairs) extracted from a 160 million word corpus of broadcast news u"
P05-1004,J02-2003,0,0.041037,"curred every 8 sentences in the BLLIP corpus. By W ORD N ET 2.0, coverage has improved but the problem of keeping up with language evolution remains difficult. consistency when classifying similar words into categories. For instance, the W ORD N ET lexicographer file for ionosphere (location) is different to exosphere and stratosphere (object), two other layers of the earth’s atmosphere. Introduction Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001). In particular, W ORD N ET (Fellbaum, 1998) has significantly influenced research in NLP. Unfortunately, these resource are extremely timeconsuming and labour-intensive to manually develop and maintain, requiring considerable linguistic and domain expertise. Lexicographers cannot possibly keep pace with language evolution: sense distinctions are continually made and merged, words are coined or become obsolete, and technical terms migrate into the vernacular. Technical domains, such as medicin"
P05-1004,E03-1071,1,0.665304,". Here, the t-test compares the joint and product probability distributions of the headword and context: p(w, r, w0 ) − p(∗, r, w0 )p(w, ∗, ∗) p p(∗, r, w0 )p(w, ∗, ∗) (2) where ∗ indicates a global sum over that element of the relation tuple. JACCARD and TT EST produced better quality synonyms than existing measures in the literature, so we use Curran and Moen’s configuration for our supersense tagging experiments. 6.1 Part of Speech Tagging and Chunking Our implementation of S EXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994). The only similar performing tool is the Trigrams ‘n’ Tags tagger (Brants, 2000) which uses a much simpler statistical model. Our implementation uses a maximum entropy chunker which has similar feature types to Koeling (2000) and is also trained on chunks extracted from the entire Penn Treebank using the CoNLL 2000 script. Since the Penn Treebank separates PPs and conjunctions from NPs, they are concatenated to match Grefenstette’s table-based results, i.e. the S EXTANT always prefers noun attachment. 6.2 Morphological Analysis Our im"
P05-1004,W02-0908,1,0.73044,"ntic Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts. This hypothesis suggests that semantic similarity can be measured by comparing the contexts each word appears in. In vector-space models each headword is represented by a vector of frequency counts recording the contexts that it appears in. The key parameters are the context extraction method and the similarity measure used to compare context vectors. Our approach to vector-space similarity is based on the S EXTANT system described in Grefenstette (1994). Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in S EXTANT was both extremely fast and produced high-quality results. S EXTANT extracts relation tuples (w, r, w0 ) for each noun, where w is the headword, r is the relation type and w0 is the other word. The efficiency of the S EXTANT approach makes the extraction of contextual information from over 2 billion words of raw text feasible. We describe the shallow pipeline in detail below. Curran and Moens (2002a) compared several different similarity measures and found tha"
P05-1004,P02-1030,1,0.7413,"ntic Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts. This hypothesis suggests that semantic similarity can be measured by comparing the contexts each word appears in. In vector-space models each headword is represented by a vector of frequency counts recording the contexts that it appears in. The key parameters are the context extraction method and the similarity measure used to compare context vectors. Our approach to vector-space similarity is based on the S EXTANT system described in Grefenstette (1994). Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in S EXTANT was both extremely fast and produced high-quality results. S EXTANT extracts relation tuples (w, r, w0 ) for each noun, where w is the headword, r is the relation type and w0 is the other word. The efficiency of the S EXTANT approach makes the extraction of contextual information from over 2 billion words of raw text feasible. We describe the shallow pipeline in detail below. Curran and Moens (2002a) compared several different similarity measures and found tha"
P05-1004,A97-1004,0,0.0105749,"Text Corpus (NANTC); the NANTC Supplement (NANTS); and the ACQUAINT Corpus. The components and their sizes including punctuation are given in Table 3. The LDC has recently released the English Gigaword corpus which includes most of the corpora listed above. CORPUS BNC RCV 1 CSR - III NANTC NANTS ACQUAINT DOCS . SENTS . WORDS 4 124 806 791 491 349 930 367 942 167 1 033 461 6.2M 8.1M 9.3M 23.2M 25.2M 21.3M 114M 207M 226M 559M 507M 491M Table 3: 2 billion word corpus statistics We have tokenized the text using the Grok-OpenNLP tokenizer (Morton, 2002) and split the sentences using MXTerminator (Reynar and Ratnaparkhi, 1997). Any sentences less than 3 words or more than 100 words long were rejected, along with sentences containing more than 5 numbers or more than 4 brackets, to reduce noise. The rest of the pipeline is described in the next section. 6 Semantic Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts. This hypothesis suggests that semantic similarity can be measured by comparing the contexts each word appears in. In vector-space models each headword is represented by a vector of frequency counts recording the contexts tha"
P05-1004,N03-1036,0,0.0614446,"rst and Sch¨utze (1993) flatten W ORD N ET into 726 categories using an algorithm which attempts to minimise the variance in category size. These categories are used to label paragraphs with topics, effectively repeating Yarowsky’s (1992) experiments using the their categories rather than Roget’s thesaurus. Sch¨utze’s (1992) WordSpace system was used to add topical links, such as between ball, racquet and game (the tennis problem). Further, they also use the same vector-space techniques to label previously unseen words using the most common class assigned to the top 20 synonyms for that word. Widdows (2003) uses a similar technique to insert words into the W ORD N ET hierarchy. He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms. This same technique as is used in our approach to supersense tagging. Ciaramita and Johnson (2003) implement a supersense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems. Thei"
P05-1004,C92-2070,0,0.311957,"d drinks groupings of people or objects spatial position goals natural objects (not man-made) people natural phenomena plants possession and transfer of possession natural processes quantities and units of measure relations between people/things/ideas two and three dimensional shapes stable states of affairs substances time and temporal relations Table 1: 25 noun lexicographer files in W ORD N ET 2 ing directly underneath. Other alternative sets of supersenses can be created by an arbitrary cut through the W ORD N ET hierarchy near the top, or by using topics from a thesaurus such as Roget’s (Yarowsky, 1992). These topic distinctions are coarser-grained than W ORD N ET senses, which have been criticised for being too difficult to distinguish even for experts. Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses. They suggest that supersense tagging is similar to named entity recognition, which also has a very small set of categories with similar granularity (e.g. location and person) for labelling predominantly unseen terms. Supersense tagging can provide automated or semiautomated assistance to lexicographers adding words to the W ORD N ET hier"
P05-1004,J93-2004,0,\N,Missing
P06-1046,U05-1009,1,0.90176,"l containing n2 nodes and the top containing a single root node. Each level, except the top, will contain half as many nodes as the level below. • Edges between nodes are linked to consecutive levels. Each node will have at most p parent nodes in the level above, and c child nodes in the level below. 364 A C B F E 1 2 D G H 3 I J 4 5 Figure 1: K A SASH, L where p = 2, c = 3 and k = 2 • Every node must have at least one parent so that all nodes are reachable from the root. This changes our search complexity to: k + pc2 log n 2 (9) 1 −1 k log n We use this geometric function in our experiments. Gorman and Curran (2005a; 2005b) found the performance of SASH for distributional similarity could be improved by replacing the initial random ordering with a frequency based ordering. In accordance with Zipf’s law, the majority of terms have low frequencies. Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002). Creating SASH with high frequency terms near the root produces more reliable initial paths, but comparisons against these terms are more expensive. The best accuracy/efficiency trade-off was found when using more reliable initial paths rather than the most reliable. This is"
P06-1046,P05-1077,0,0.255834,"nce to the terms referencing it. Each term is then only compared with the terms with which it shares attributes. We will give a theoretically comparison against other techniques. 5.1 the list, the radius is updated to the distance from the target to the new k th closest term. Construction complexity is O(n log n). Search complexity is claimed to be O(log n) for small radius searches. This does not hold for our decreasing radius search, whose worst case complexity is O(n). 5.2 Point Location in Equal Balls is a randomised structure that uses the bit signatures generated by LSH . It was used by Ravichandran et al. (2005) to improve the efficiency of distributional similarity calculations. Having generated our d length bit signatures for each of our n terms, we take these signatures and randomly permute the bits. Each vector has the same permutation applied. This is equivalent to a column reordering in a matrix where the rows are the terms and the columns the bits. After applying the permutation, the list of terms is sorted lexicographically based on the bit signatures. The list is scanned sequentially, and each term is compared to its B nearest neighbours in the list. The choice of B will effect the accuracy/"
P06-1046,W02-0908,1,0.473318,"balance. 1 Introduction It is a general property of Machine Learning that increasing the volume of training data increases the accuracy of results. This is no more evident than in Natural Language Processing (NLP), where massive quantities of text are required to model rare language events. Despite the rapid increase in computational power available for NLP systems, the volume of raw data available still outweighs our ability to process it. Unsupervised learning, which does not require the expensive and timeconsuming human annotation of data, offers an opportunity to use this wealth of data. Curran and Moens (2002) show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases. Extracting synonymy relations using distributional similarity is based on the distributional hypothesis that similar words appear in similar contexts. Terms are described by collating informa2 Distributional Similarity Measuring distributional similarity first requires the extraction of context information for each of the vocabulary terms from raw text. These terms are then compared for similarity using a nearestneighbour sea"
P06-1046,J05-4002,0,0.0216441,"rs may improve its accuracy. RI MI produces similar result using L ARGE to SASH using BNC. This does not include the cost of extracting context relations from the raw text, so the true comparison is much worse. SASH allows the free use of weight and measure functions, but RI is constrained by having to transform any context space into a RI space. This is important when CUT- OFF NAIVE SASH INDEX 0 541,721 10,599 5,844 L ARGE 5 184,493 8,796 13,187 100 35,617 6,231 32,663 Table 4: Average number of comparisons per term considering that different tasks may require different weights and measures (Weeds and Weir, 2005). RI also suffers n2 complexity, where as SASH is n log n. Taking these into account, and that the improvements are barely significant, SASH is a better choice. The results for LSH are disappointing. It performs consistently worse than the other methods except VPT. This could be improved by using larger bit vectors, but there is a limit to the size of these as they represent a significant memory overhead, particularly as the vocabulary increases. Table 4 presents the theoretical analysis of attribute indexing. The average number of comparisons made for various cut-offs of L ARGE are shown. NAI"
P06-1046,W05-1011,1,\N,Missing
P06-1088,W02-2018,0,0.0139263,"crease in POS tagging accuracy can be achieved with only a tiny increase in ambiguity; and second that maintaining some POS ambiguity can significantly improve the accuracy of the supertagger. The parser uses the CCG lexical categories to build syntactic structure, and the POS tags are used by the supertagger and parser as part of their statisical models. We show that using a multitagger for supertagging results in an effective preprocessor for CCG parsing, and that using a multitagger for POS tagging results in more accurate CCG supertagging. 2 timisation algorithm (Nocedal and Wright, 1999; Malouf, 2002) to perform the estimation. MLE has a tendency to overfit the training data. We adopt the standard approach of Chen and Rosenfeld (1999) by introducing a Gaussian prior term to the objective function which penalises feature weights with large absolute values. A parameter defined in terms of the standard deviation of the Gaussian determines the degree of smoothing. The conditional probability of a sequence of tags, y1 , . . . , yn , given a sentence, w1 , . . . , wn , is defined as the product of the individual probabilities for each tag: Maximum Entropy Tagging P (y1 , . . . , yn |w1 , . . . ,"
P06-1088,J99-2004,0,0.790823,"Missing"
P06-1088,briscoe-carroll-2002-robust,0,0.0156975,"nough to serve as a front-end to a CCG parser, and we retain some POS ambiguity since POS tags are used as features in the statistical models of the supertagger and parser. Charniak et al. (1996) investigated multi-POS tagging in the context of PCFG parsing. It was found that multi-tagging provides only a minor improvement in accuracy, with a significant loss in efficiency; hence it was concluded that, given the particular parser and tagger used, a single-tag POS tagger is preferable to a multi-tagger. More recently, Watson (2006) has revisited this question in the context of the RASP parser (Briscoe and Carroll, 2002) and found that, similar to Charniak et al. (1996), multi-tagging at the POS level results in a small increase in parsing accuracy but at some cost in efficiency. For lexicalized grammars, such as CCG and TAG , the motivation for using a multi-tagger to assign the elementary structures (supertags) is more compelling. Since the set of supertags is typically much larger than a standard POS tag set, the tagging problem becomes much harder. In where Z(x) is a normalisation constant which ensures a proper probability distribution for each context x. The feature functions fi (x, y) are binaryvalued,"
P06-1088,W04-2401,0,0.0219275,"Missing"
P06-1088,C04-1041,1,0.6649,"Missing"
P06-1088,N03-1033,0,0.052147,"Missing"
P06-1088,P04-1014,1,0.81675,"Missing"
P06-1088,U05-1007,1,0.823674,"om the optimisation algorithm; for example, GIS only allows non-negative values. Real-valued features are commonly used with other machine learning algorithms. Binary features suffer from certain limitations of the representation, which make them unsuitable for modelling some properties. For example, POS taggers have difficulty determining if capitalised, sentence initial words are proper nouns. A useful way to model this property is to determine the ratio of capitalised and non-capitalised instances of a particular word in a large corpus and use a realvalued feature which encodes this ratio (Vadas and Curran, 2005). The only way to include this feature in a binary representation is to discretize (or bin) the feature values. For this type of feature, choosing appropriate bins is difficult and it may be hard to find a discretization scheme that performs optimally. Another problem with discretizing feature values is that it imposes artificial boundaries to define the bins. For the example above, we may choose the bins 0 ≤ x < 1 and 1 ≤ x < 2, which separate the values 0.99 and 1.01 even though they are close in value. At the same time, the model does not distinguish between 0.01 and 0.99 even though they a"
P06-1088,W02-1001,0,0.102537,"Missing"
P06-1088,E03-1071,1,0.792415,"Missing"
P06-1088,P02-1043,0,0.0515943,"Missing"
P07-1031,J93-2004,0,\N,Missing
P07-1031,J03-4003,0,\N,Missing
P07-1031,N04-1016,0,\N,Missing
P07-1031,W04-3111,0,\N,Missing
P07-1031,P06-2006,0,\N,Missing
P07-1031,W05-0603,0,\N,Missing
P07-1032,A00-2018,0,0.094588,"Missing"
P07-1032,C04-1041,1,0.858706,"from the same resource used to develop the parser. This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this approach. First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot be compared with the dependency F-scores obtained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formali"
P07-1032,P04-1014,1,0.354702,"from the same resource used to develop the parser. This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this approach. First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot be compared with the dependency F-scores obtained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formali"
P07-1032,J03-4003,0,0.0376609,"Missing"
P07-1032,E03-1071,1,0.521697,"Missing"
P07-1032,P02-1043,0,0.336984,"Missing"
P07-1032,E03-1005,0,0.0158967,"Missing"
P07-1032,N04-1013,0,0.202683,"dependency F-scores obtained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting the Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 248–255, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics output of a parser from one representation to another. Second, we develop a method for measuring how"
P07-1032,P06-2006,0,0.609096,"parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this approach. First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot be compared with the dependency F-scores obtained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting th"
P07-1032,P06-4020,0,0.0691834,"against our results; the GRs making up the annotation share some similarities with the predicateargument dependencies output by the CCG parser; and we can directly compare our parser against a non-CCG parser, namely the RASP parser. We chose not to use the corpus based on the Susanne corpus (Carroll et al., 1998) because the GRs are less like the CCG dependencies; the corpus is not based on the Penn Treebank, making comparison more difficult because of tokenisation differences, for example; and the latest results for RASP are on DepBank. The GRs are described in Briscoe and Carroll (2006) and Briscoe et al. (2006). Table 1 lists the GR s used in the evaluation. As an example, the sentence The parent sold Imperial produces three GRs: (det parent The), (ncsubj sold parent ) and (dobj sold Imperial). Note that some GRs — in this example ncsubj — have a subtype slot, giving extra information. The subtype slot for ncsubj is used to indicate passive subjects, with the null value “ ” for active subjects and obj for passive subjects. Other subtype slots are discussed in Section 4.2. The CCG dependencies were transformed into GR s in two stages. The first stage was to create a mapping between the CCG dependenci"
P07-1032,W99-0629,0,0.122663,"Missing"
P07-1032,P04-1041,0,0.0597084,"Missing"
P07-1032,P05-1022,0,0.0133165,"Missing"
P07-1032,C04-1010,0,0.00561439,"e HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000). Different parsers produce different output, for ex248 James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au ample phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b). This variety of formalisms and output creates a challenge for parser evaluation. The majority of parser evaluations have used test sets drawn from the same resource used to develop the parser. This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this approach. First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot b"
P07-1032,E03-1025,0,0.222492,"btained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting the Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 248–255, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics output of a parser from one representation to another. Second, we develop a method for measuring how effective the c"
P07-1032,W03-2401,0,\N,Missing
P07-2009,P04-1014,1,0.880545,"vels of robustness and efficiency in NLP systems and made linguistically motivated large-scale language processing a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a breakthrough in NLP technology. The system is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inherent in constructions such as extraction and coordination. CCG is a lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads"
P07-2009,P07-1032,1,0.634558,"teedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algorithm. Clark and Curran (2004b) describes log-linear parsing models for CCG. The features in the models are defined over local parts of CCG derivations and include word-word dependencies. A disadvantage of the log-linear models is that they require cluster computing resources for practical training (Clark and Curran, 2004b). We have also investigated perceptron training for the parser (Clark and Curran, 2007b), obtaining comparable accuracy scores and similar training times (a few hours) compared with the log-linear models. The significant advantage of Proceedings of the ACL 2007 Demo and Poster Sessions, pages 33–36, c Prague, June 2007. 2007 Association for Computational Linguistics the perceptron training is that it only requires a single processor. The training is online, updating the model parameters one sentence at a time, and it converges in a few passes over the CCGbank data. A packed chart representation allows efficient decoding, with the same algorithm — the Viterbi algorithm — finding"
P07-2009,W07-1202,1,0.65614,"teedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algorithm. Clark and Curran (2004b) describes log-linear parsing models for CCG. The features in the models are defined over local parts of CCG derivations and include word-word dependencies. A disadvantage of the log-linear models is that they require cluster computing resources for practical training (Clark and Curran, 2004b). We have also investigated perceptron training for the parser (Clark and Curran, 2007b), obtaining comparable accuracy scores and similar training times (a few hours) compared with the log-linear models. The significant advantage of Proceedings of the ACL 2007 Demo and Poster Sessions, pages 33–36, c Prague, June 2007. 2007 Association for Computational Linguistics the perceptron training is that it only requires a single processor. The training is online, updating the model parameters one sentence at a time, and it converges in a few passes over the CCGbank data. A packed chart representation allows efficient decoding, with the same algorithm — the Viterbi algorithm — finding"
P07-2009,W04-3215,1,0.743379,"Missing"
P07-2009,J03-4003,0,0.0233363,"g a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a breakthrough in NLP technology. The system is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inherent in constructions such as extraction and coordination. CCG is a lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a"
P07-2009,E03-1071,1,0.503429,"xicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a). The C & C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 bos@di.uniroma1.it ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001), are combined into a single program. The output from this program — a CCG derivation, POS tags, lemmas, and named entity tags — is used by the module Boxer (Bos, 2005) to produce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extract"
P07-2009,W03-0424,1,0.650833,"xicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a). The C & C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 bos@di.uniroma1.it ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001), are combined into a single program. The output from this program — a CCG derivation, POS tags, lemmas, and named entity tags — is used by the module Boxer (Bos, 2005) to produce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extract"
P07-2009,P06-1088,1,0.722633,"is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inherent in constructions such as extraction and coordination. CCG is a lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a). The C & C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 bos@di.uniroma1.it ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001),"
P07-2009,N04-1013,0,0.00471736,"School of Information Technologies Computing Laboratory Dipartimento di Informatica University of Sydney Oxford University Universit`a di Roma “La Sapienza” NSW 2006, Australia Wolfson Building, Parks Road via Salaria 113 james@it.usyd.edu.au Oxford, OX1 3QD, UK 00198 Roma, Italy stephen.clark@comlab.ox.ac.uk 1 Introduction The statistical modelling of language, together with advances in wide-coverage grammar development, have led to high levels of robustness and efficiency in NLP systems and made linguistically motivated large-scale language processing a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a breakthrough in NLP technology. The system is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inheren"
P07-2009,P06-4020,0,0.0246004,"Missing"
P07-2009,C04-1041,1,\N,Missing
P08-1039,J99-2004,0,0.0732986,"nk (Palmer et al., 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. (a) (b) N N /N ??? N N N /N ??? N /N N ??? ??? cancer deaths lung cancer lung (c) N deaths N (N /N )/(N /N ) N /N lung cancer deaths Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags 2.2 CCG parsing The C&C CCG parser (Clark and Curran, 2007b) is used to perform our experiments, and to evaluate the effect of the changes to CCGbank. The parser uses a two-stage system, first employing a supertagger (Bangalore and Joshi, 1999) to propose lexical categories for each word, and then applying the CKY chart parsing algorithm. A log-linear model is used to identify the most probable derivation, which makes it possible to add the novel features we describe in Section 4, unlike a PCFG. The C&C parser is evaluated on predicateargument dependencies derived from CCGbank. These dependencies are represented as 5-tuples: hhf , f , s, ha , li, where hf is the head of the predicate; f is the supertag of hf ; s describes which argument of f is being filled; ha is the head of the argument; and l encodes whether the dependency is loc"
P08-1039,P06-2006,0,0.054579,"Missing"
P08-1039,P07-1032,1,0.92717,"d removes the need for the grammar rule in (3). Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. PropBank (Palmer et al., 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. (a) (b) N N /N ??? N N N /N ??? N /N N ??? ??? cancer deaths lung cancer lung (c) N deaths N (N /N )/(N /N ) N /N lung cancer deaths Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags 2.2 CCG parsing The C&C CCG parser (Clark and Curran, 2007b) is used to perform our experiments, and to evaluate the effect of the changes to CCGbank. The parser uses a two-stage system, first employing a supertagger (Bangalore and Joshi, 1999) to propose lexical categories for each word, and then applying the CKY chart parsing algorithm. A log-linear model is used to identify the most probable derivation, which makes it possible to add the novel features we describe in Section 4, unlike a PCFG. The C&C parser is evaluated on predicateargument dependencies derived from CCGbank. These dependencies are represented as 5-tuples: hhf , f , s, ha , li, whe"
P08-1039,J07-4004,1,0.957308,"d removes the need for the grammar rule in (3). Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. PropBank (Palmer et al., 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. (a) (b) N N /N ??? N N N /N ??? N /N N ??? ??? cancer deaths lung cancer lung (c) N deaths N (N /N )/(N /N ) N /N lung cancer deaths Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags 2.2 CCG parsing The C&C CCG parser (Clark and Curran, 2007b) is used to perform our experiments, and to evaluate the effect of the changes to CCGbank. The parser uses a two-stage system, first employing a supertagger (Bangalore and Joshi, 1999) to propose lexical categories for each word, and then applying the CKY chart parsing algorithm. A log-linear model is used to identify the most probable derivation, which makes it possible to add the novel features we describe in Section 4, unlike a PCFG. The C&C parser is evaluated on predicateargument dependencies derived from CCGbank. These dependencies are represented as 5-tuples: hhf , f , s, ha , li, whe"
P08-1039,J03-4003,0,0.117377,"Missing"
P08-1039,J07-3004,0,0.0614646,"NN lung) (NN cancer) ) (NNS deaths) ) The corresponding lowest spanning node, which incorrectly has cancer deaths as a constituent, is shown in Figure 2(a). To flatten the node, we recursively remove brackets that partially overlap the NML bracket. Nodes that don’t overlap at all are left intact. This process results in a list of nodes (which may or may not be leaves), which in our example is [lung, cancer, deaths]. We then insert the correct left-branching structure, shown in Figure 2(b). At this stage, the supertags are still incomplete. Heads are then assigned using heuristics adapted from Hockenmaier and Steedman (2007). Since we are applying these to CCGbank NP structures rather than the Penn Treebank, the POS tag based heuristics are sufficient to determine heads accurately. Finally, we assign supertags to the new structure. We want to make the minimal number of changes to the entire sentence derivation, and so the supertag of the dominating node is fixed. Categories are then propagated recursively down the tree. For a node with category X , its head child is also given the category X . The non-head child is always treated as an adjunct, and given the category X /X or X X as appropriate. Figure 2(c) shows"
P08-1039,P95-1007,0,0.206977,"Missing"
P08-1039,J93-2004,0,0.032787,"Missing"
P08-1039,W05-0603,0,0.0605086,"Missing"
P08-1039,J05-1004,0,0.0772897,"res in CCGbank, but a further problem that highlights the need to improve NP derivations is shown in Figure 1. When a conjunction occurs in an NP, a non-CCG rule is required in order to reach a parse: conj N ⇒ N (3) This rule treats the conjunction in the same manner as a modifier, and results in the incorrect derivation shown in Figure 1(a). Our work creates the correct CCG derivation, shown in Figure 1(b), and removes the need for the grammar rule in (3). Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. PropBank (Palmer et al., 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. (a) (b) N N /N ??? N N N /N ??? N /N N ??? ??? cancer deaths lung cancer lung (c) N deaths N (N /N )/(N /N ) N /N lung cancer deaths Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags 2.2 CCG parsing The C&C CCG parser (Clark and Curran, 2007b) is used to perform our experiments, and to evaluate the effect of the changes to CCGbank. The parser uses a two-stage system, first employing a supertagger (Bangalore and Joshi, 199"
P08-1039,P07-1031,1,0.932499,"When a conjunction occurs in an NP, a non-CCG rule is required in order to reach a parse: conj N ⇒ N (3) This rule treats the conjunction in the same manner as a modifier, and results in the incorrect derivation shown in Figure 1(a). Our work creates the correct CCG derivation, shown in Figure 1(b), and removes the need for the grammar rule in (3). Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. PropBank (Palmer et al., 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. (a) (b) N N /N ??? N N N /N ??? N /N N ??? ??? cancer deaths lung cancer lung (c) N deaths N (N /N )/(N /N ) N /N lung cancer deaths Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags 2.2 CCG parsing The C&C CCG parser (Clark and Curran, 2007b) is used to perform our experiments, and to evaluate the effect of the changes to CCGbank. The parser uses a two-stage system, first employing a supertagger (Bangalore and Joshi, 1999) to propose lexical categories for each word, and then applying the CKY chart parsing algorithm. A log-linear"
P08-1039,W07-1022,0,\N,Missing
P09-1045,N03-1023,0,0.0129503,"omparisons using only one seed set are unreliable. Table 3 summarises the results on Sgold , including the minimum and maximum averages over the 10 categories. At only 100 terms, lexicon 5 Supervised Bagging While the wide variation we reported in the previous section is an impediment to reliable evaluation, it presents an opportunity to improve the performance of bootstrapping algorithms. In the next section, we present a novel unsupervised bagging approach to reducing semantic drift. In this section, we consider the standard bagging approach introduced by Breiman (1996). Bagging was used by Ng and Cardie (2003) to create committees of classifiers for labelling unseen data for retraining. Here, a bootstrapping algorithm is instantiated n = 50 times with random seed sets selected from the UNION evaluation cache. This generates n new lexicons L1 , L2 , . . . , Ln for each category. The next phase involves aggregating the predictions in L1−n to form the final lexicon for each category, using a weighted voting function. 400 1-200 401-600 801-1000 1-1000 BAGGING Shand BASILISK WMEB Sgold BAG BASILISK WMEB 1-200 401-600 801-1000 1-1000 72.3 90.2 63.5 78.5 58.8 66.3 65.1 78.5 70.7 91.0 60.7 78.4 45.5 62.2 5"
P09-1045,P06-1102,0,0.0444635,"Missing"
P09-1045,H05-1050,0,0.0200447,"ach pair of terms, to construct features for lexical entailment. Pas¸ca et al. (2006) used distributional similarity to find similar terms for verifying the names in date-of-birth facts for their tera-scale bootstrapping system. DISE DRUG FUNC MUTN 2.3 Selecting seeds PROT For the majority of bootstrapping tasks, there is little or no guidance on how to select seeds which will generate the most accurate lexicons. Most previous works used seeds selected based on a user’s or domain expert’s intuition (Curran et al., 2007), which may then have to meet a frequency criterion (Riloff et al., 2003). Eisner and Karakos (2005) focus on this issue by considering an approach called strapping for word sense disambiguation. In strapping, semisupervised bootstrapping instances are used to train a meta-classifier, which given a bootstrapping instance can predict the usefulness (fertility) of its seeds. The most fertile seeds can then be used in place of hand-picked seeds. The design of a strapping algorithm is more complex than that of a supervised learner (Eisner and Karakos, 2005), and it is unclear how well strapping will generalise to other bootstrapping tasks. In our work, we build upon bootstrapping using unsupervi"
P09-1045,W06-3909,0,0.0324006,"pection. To improve the seeds, the frequency of the potential seeds in the corpora is often considered, on the assumption that highly frequent seeds are better (Thelen and Riloff, 2002). Unfortunately, these seeds may be too general and extract many nonspecific patterns. Another approach is to identify seeds using hyponym patterns like, * is a [NAMED ENTITY ] (Meij and Katrenko, 2007). This leads us to our first investigation of seed variability and the methodology used to compare bootstrapping algorithms. Typically algorithms are compared using one set of hand-picked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008). This approach does not provide a fair comparison or any detailed analysis of the algorithms under investigation. As we shall see, it is possible that the seeds achieve the maximum precision for one algorithm and the minimum for another, and thus the single comparison is inappropriate. Even evaluating on multiple categories does not ensure the robustness of the evaluation. Secondly, it provides no insight into the sensitivity of an algorithm to different seeds. 3.3 Lexicon evaluation The evaluation involves manually inspecting each extracted term and judging whethe"
P09-1045,W06-2703,0,0.0617008,"an that of a supervised learner (Eisner and Karakos, 2005), and it is unclear how well strapping will generalise to other bootstrapping tasks. In our work, we build upon bootstrapping using unsupervised approaches. 3 SIGN TUMR Table 2: The MEDLINE semantic categories. t2 , t4 , t5 ). Unlike Riloff and Jones (1999) and Yangarber (2003), we do not use syntactic knowledge, as we aim to take a language independent approach. The 5-grams were extracted from the MEDLINE abstracts following McIntosh and Curran (2008). The abstracts were tokenised and split into sentences using bio-specific NLP tools (Grover et al., 2006). The 5-grams were filtered to remove patterns appearing with less than 7 terms4 . The statistics of the resulting dataset are shown in Table 1. 3.2 Semantic Categories The semantic categories we extract from MED LINE are shown in Table 2. These are a subset of the TREC Genomics 2007 entities (Hersh et al., 2007). Categories which are predominately multiterm entities, e.g. Pathways and Toxicities, were excluded.5 Genes and Proteins were merged into PROT as they have a high degree of metonymy, particularly out of context. The Cell or Tissue Type category was split into two fine grained classes,"
P09-1045,W03-0404,0,0.0105806,"Missing"
P09-1045,C92-2082,0,0.243883,"and AFNLP 2 Background sion Bootstrapping (MEB) which forces stricter boundaries between the competing categories than BASILISK . In MEB , the key assumptions are that terms only belong to a category and that patterns only extract terms of a single category. Semantic drift is reduced by eliminating patterns that collide with multiple categories in an iteration and by ignoring colliding candidate terms (for the current iteration). This excludes generic patterns that can occur frequently with multiple categories, and reduces the chance of assigning ambiguous terms to their less dominant sense. Hearst (1992) exploited patterns for information extraction, to acquire is-a relations using manually devised patterns like such Z as X and/or Y where X and Y are hyponyms of Z. Riloff and Jones (1999) extended this with an automated bootstrapping algorithm, Multi-level Bootstrapping (MLB), which iteratively extracts semantic lexicons from text. In MLB, bootstrapping alternates between two stages: pattern extraction and selection, and term extraction and selection. MB is seeded with a small set of user selected seed terms. These seeds are used to identify contextual patterns they appear in, which in turn i"
P09-1045,W02-1028,0,0.941896,"entually extracts polysemous terms and patterns which weakly constrain the semantic class, causing the lexicon’s meaning to shift, called semantic drift by Curran et al. (2007). For example, female firstnames may drift into flowers when Iris and Rose are extracted. Many variations on bootstrapping have been developed to reduce semantic drift.1 One approach is to extract multiple semantic categories simultaneously, where the individual bootstrapping instances compete with one another in an attempt to actively direct the categories away from each other. Multi-category algorithms outperform MLB (Thelen and Riloff, 2002), and we focus on these algorithms in our experiments. In BASILISK, MEB, and WMEB, each competing category iterates simultaneously between the term and pattern extraction and selection stages. These algorithms differ in how terms and patterns selected by multiple categories are handled, and their scoring metrics. In BASILISK (Thelen and Riloff, 2002), candidate terms are ranked highly if they have strong evidence for a category and little or no evidence for other categories. This typically favours less frequent terms, as they will match far fewer patterns and are thus more likely to belong to"
P09-1045,D08-1106,0,0.0305636,"se their extracting influence. In WMEB , the pattern pool accumulates all top-k patterns from previous iterations, to ensure previous patterns can contribute. 2.2 Distributional Similarity Distributional similarity has been used to extract semantic lexicons (Grefenstette, 1994), based on the distributional hypothesis that semantically similar words appear in similar contexts (Harris, 1954). Words are represented by context vectors, and words are considered similar if their context vectors are similar. Patterns and distributional methods have been combined previously. Pantel and Ravichandran 1 Komachi et al. (2008) used graph-based algorithms to reduce semantic drift for Word Sense Disambiguation. 2 In BASILISK, k is increased by one in each iteration, to ensure at least one new pattern is introduced. 397 TYPE (#) Terms Contexts 5-grams Unfiltered tokens MEDLINE CAT DESCRIPTION 1 347 002 4 090 412 72 796 760 6 642 802 776 ANTI Antibodies: Immunoglobulin molecules that react with a specific antigen that induced its synthesis MAb IgG IgM rituximab infliximab (κ1 :0.89, κ2 :1.0) Cells: A morphological or functional form of a cell RBC HUVEC BAEC VSMC SMC (κ1 :0.91, κ2 :1.0) Cell lines: A population of cells"
P09-1045,P07-1067,0,0.0299277,"Missing"
P09-1045,P03-1044,0,0.183896,"sed bootstrapping instances are used to train a meta-classifier, which given a bootstrapping instance can predict the usefulness (fertility) of its seeds. The most fertile seeds can then be used in place of hand-picked seeds. The design of a strapping algorithm is more complex than that of a supervised learner (Eisner and Karakos, 2005), and it is unclear how well strapping will generalise to other bootstrapping tasks. In our work, we build upon bootstrapping using unsupervised approaches. 3 SIGN TUMR Table 2: The MEDLINE semantic categories. t2 , t4 , t5 ). Unlike Riloff and Jones (1999) and Yangarber (2003), we do not use syntactic knowledge, as we aim to take a language independent approach. The 5-grams were extracted from the MEDLINE abstracts following McIntosh and Curran (2008). The abstracts were tokenised and split into sentences using bio-specific NLP tools (Grover et al., 2006). The 5-grams were filtered to remove patterns appearing with less than 7 terms4 . The statistics of the resulting dataset are shown in Table 1. 3.2 Semantic Categories The semantic categories we extract from MED LINE are shown in Table 2. These are a subset of the TREC Genomics 2007 entities (Hersh et al., 2007)."
P09-1045,U08-1013,1,0.916542,"er patterns and are thus more likely to belong to one category. Patterns are selected similarly, however patterns may also be selected by different categories in later iterations. Curran et al. (2007) introduced Mutual Exclu2.1 Weighted MEB The scoring of candidate terms and patterns in MEB is na¨ıve. Candidates which 1) match the most input instances; and 2) have the potential to generate the most new candidates, are preferred (Curran et al., 2007). This second criterion aims to increase recall. However, the selected instances are highly likely to introduce drift. Our Weighted MEB algorithm (McIntosh and Curran, 2008), extends MEB by incorporating term and pattern weighting, and a cumulative pattern pool. WMEB uses the χ2 statistic to identify patterns and terms that are strongly associated with the growing lexicon terms and their patterns respectively. The terms and patterns are then ranked first by the number of input instances they match (as in MEB), but then by their weighted score. In MEB and BASILISK2 , the top-k patterns for each iteration are used to extract new candidate terms. As the lexicons grow, general patterns can drift into the top-k and as a result the earlier precise patterns lose their e"
P09-1045,P06-2075,0,0.010539,"and processes kinase ligase acetyltransferase helicase binding (κ1 :0.87, κ2 :0.99) Mutations: Gene and protein mutations, and mutants Leiden C677T C282Y 35delG null (κ1 :0.89, κ2 :1.0) Proteins and genes p53 actin collagen albumin IL-6 (κ1 :0.99, κ2 :1.0) Signs and symptoms of diseases anemia hypertension hyperglycemia fever cough (κ1 :0.96, κ2 :0.99) Tumors: Types of tumors lymphoma sarcoma melanoma neuroblastoma osteosarcoma (κ1 :0.89, κ2 :0.95) CELL Table 1: Filtered 5-gram dataset statistics. CLNE (2004) used lexical-syntactic patterns to label clusters of distributionally similar terms. Mirkin et al. (2006) used 11 patterns, and the distributional similarity score of each pair of terms, to construct features for lexical entailment. Pas¸ca et al. (2006) used distributional similarity to find similar terms for verifying the names in date-of-birth facts for their tera-scale bootstrapping system. DISE DRUG FUNC MUTN 2.3 Selecting seeds PROT For the majority of bootstrapping tasks, there is little or no guidance on how to select seeds which will generate the most accurate lexicons. Most previous works used seeds selected based on a user’s or domain expert’s intuition (Curran et al., 2007), which may"
P09-1045,N04-1041,0,\N,Missing
P09-2014,P06-2006,0,0.0191376,"ences of the WSJ, and evaluating accuracy according to the Parseval metrics. Collins (1999) is a seminal example. The second approach is to apply statistical methods to parsers based on linguistic formalisms, such as HPSG, LFG, TAG, and CCG, with the grammar being defined manually or extracted from a formalism-specific treebank. Evaluation is typically performed by comparing against predicate-argument structures extracted from the treebank, or against a test set of manually annotated grammatical relations (GRs). Examples of this approach include Riezler et al. (2002), Miyao and Tsujii (2005), Briscoe and Carroll (2006), and Clark and Curran (2007).1 Despite the many examples from both approaches, there has been little comparison across the two groups, which we refer to as PTB parsing and formalism-based parsing, respectively. The 2 The CCG to PTB Conversion There has been much recent work in attempting to convert native parser output into alternative representations for evaluation purposes, e.g. (Clark and Curran, 2007; Matsuzaki and Tsujii, 2008). The conclusion is that such conversions are surprisingly difficult. Clark and Curran (2007) 2 Since this short paper reports a small, focused research contributi"
P09-2014,J07-4004,1,0.958666,"difficulty of mapping from a grammatical resource based on the PTB back to the PTB , and we also comment on the (non-)suitability of the PTB as a general formalism-independent evaluation resource. A second contribution is to provide the first accuracy comparison of the CCG parser with a PTB parser, obtaining competitive scores for the CCG parser on a representative subset of the PTB test sections. It is important to note that the purpose of this evaluation is comparison with a PTB parser, rather than evaluation of the CCG parser per se. The CCG parser has been extensively evaluated elsewhere (Clark and Curran, 2007), and arguably GRs or predicate-argument structures provide a more suitable test set for the CCG parser than PTB phrase-structure trees. We compare the CCG parser of Clark and Curran (2007) with a state-of-the-art Penn Treebank (PTB) parser. An accuracy comparison is performed by converting the CCG derivations into PTB trees. We show that the conversion is extremely difficult to perform, but are able to fairly compare the parsers on a representative subset of the PTB test section, obtaining results for the CCG parser that are statistically no different to those for the Berkeley parser. 1 James"
P09-2014,P05-1011,0,0.0552886,"ure trees for unseen sentences of the WSJ, and evaluating accuracy according to the Parseval metrics. Collins (1999) is a seminal example. The second approach is to apply statistical methods to parsers based on linguistic formalisms, such as HPSG, LFG, TAG, and CCG, with the grammar being defined manually or extracted from a formalism-specific treebank. Evaluation is typically performed by comparing against predicate-argument structures extracted from the treebank, or against a test set of manually annotated grammatical relations (GRs). Examples of this approach include Riezler et al. (2002), Miyao and Tsujii (2005), Briscoe and Carroll (2006), and Clark and Curran (2007).1 Despite the many examples from both approaches, there has been little comparison across the two groups, which we refer to as PTB parsing and formalism-based parsing, respectively. The 2 The CCG to PTB Conversion There has been much recent work in attempting to convert native parser output into alternative representations for evaluation purposes, e.g. (Clark and Curran, 2007; Matsuzaki and Tsujii, 2008). The conclusion is that such conversions are surprisingly difficult. Clark and Curran (2007) 2 Since this short paper reports a small,"
P09-2014,N07-1051,0,0.163768,"Missing"
P09-2014,P02-1035,0,0.0729183,"skeletal phrase-structure trees for unseen sentences of the WSJ, and evaluating accuracy according to the Parseval metrics. Collins (1999) is a seminal example. The second approach is to apply statistical methods to parsers based on linguistic formalisms, such as HPSG, LFG, TAG, and CCG, with the grammar being defined manually or extracted from a formalism-specific treebank. Evaluation is typically performed by comparing against predicate-argument structures extracted from the treebank, or against a test set of manually annotated grammatical relations (GRs). Examples of this approach include Riezler et al. (2002), Miyao and Tsujii (2005), Briscoe and Carroll (2006), and Clark and Curran (2007).1 Despite the many examples from both approaches, there has been little comparison across the two groups, which we refer to as PTB parsing and formalism-based parsing, respectively. The 2 The CCG to PTB Conversion There has been much recent work in attempting to convert native parser output into alternative representations for evaluation purposes, e.g. (Clark and Curran, 2007; Matsuzaki and Tsujii, 2008). The conclusion is that such conversions are surprisingly difficult. Clark and Curran (2007) 2 Since this sho"
P09-2014,C08-1069,0,\N,Missing
P09-2014,J03-4003,0,\N,Missing
P09-2014,J07-3004,0,\N,Missing
P10-1022,J07-3004,0,0.679984,"ditional information. That is, sometimes the existing trees allow transformation rules to be written that improve the quality of the grammar. Linguistic theories are constantly changing, which means that there is a substantial lag between what we (think we) understand of grammar and the annotations in our corpora. The grammar engineering process we describe, which we dub rebanking, is intended to reduce this gap, tightening the feedback loop between formal and computational linguistics. X /Y Y ⇒ X Y X Y ⇒ X X /Y Y /Z ⇒ X /Z Y  X Y ⇒ X  Y /Z X Y ⇒ X /Z (&gt;) (<) (&gt;B) (< B) (< B×) CCGbank (Hockenmaier and Steedman, 2007) extends this compact set of combinatory rules with a set of type-changing rules, designed to strike a better balance between sparsity in the category set and ambiguity in the grammar. We mark typechanging rules TC in our derivations. In wide-coverage descriptions, categories are generally modelled as typed-feature structures (Shieber, 1986), rather than atomic symbols. This allows the grammar to include a notion of headedness, and to unify under-specified features. We occasionally must refer to these additional details, for which we employ the following notation. Features are annotated in squ"
P10-1022,J93-2004,0,0.0434038,"epresentations from surface strings, which is why they are sometimes referred to as deep grammars. Analyses produced by these formalisms can be more detailed than those produced by skeletal phrasestructure parsers, because they produce fully specified predicate-argument structures. Unfortunately, statistical parsers do not take advantage of this potential detail. Statistical parsers induce their grammars from corpora, and the corpora for linguistically motivated formalisms currently do not contain high quality predicateargument annotation, because they were derived from the Penn Treebank (PTB Marcus et al., 1993). Manually written grammars for these formalisms, such as the ERG HPSG grammar (Flickinger, 2000) and the XLE LFG grammar (Butt et al., 2006) produce far more detailed and linguistically correct analyses than any English statistical parser, due to the comparatively coarse-grained annotation schemes of the corpora statistical parsers are trained on. While rule-based parsers use grammars that are carefully engineered (e.g. Oepen et al., 2004), and can be updated to reflect the best linguistic analyses, statistical parsers have so far had to take what they are given. What we suggest in this paper"
P10-1022,boxwell-white-2008-projecting,0,0.0297208,"vation is shown so that instantiated variables can be seen. Carthage 0s NP (NPy /(Ny /PPz )y )NPz (NPy /(Ny /PPCarthage )y )0 s destruction N /PPy < &gt; NPdestruction In this analysis, we regard the genitive clitic as a case-marker that performs a movement operation roughly analogous to WH-extraction. Its category is therefore similar to the one used in object extraction, (N N )/(S /NP ). Figure 1 shows an example with multiple core arguments. This analysis allows recovery of verbal arguments of nominalised raising and control verbs, a construction which both Gildea and Hockenmaier (2003) and Boxwell and White (2008) identify as a problem case when aligning Propbank and CCGbank. Our analysis accommodates this construction effortlessly, as shown in Figure 2. The category assigned to decision can coindex the missing NP argument of buy with its own PP argument. When that argument is supplied by the genitive, it is also supplied to the verb, buy, filling its dependency with its agent, Google. This argument would be quite difficult to recover using a shallow syntactic analysis, as the path would be quite long. There are 494 such verb arguments mediated by nominal predicates in Sections 02-21. These analyses al"
P10-1022,W04-2705,0,0.31047,"s University of Sydney NSW 2006, Australia {mhonn,james}@it.usyd.edu.au Abstract mation from existing resources. We chose to work on CCGbank (Hockenmaier and Steedman, 2007), a Combinatory Categorial Grammar (Steedman, 2000) treebank acquired from the Penn Treebank (Marcus et al., 1993). This work is equally applicable to the corpora described by Miyao et al. (2004), Shen et al. (2008) or Cahill et al. (2008). Our first changes integrate four previously suggested improvements to CCGbank. We then describe a novel CCG analysis of NP predicateargument structure, which we implement using NomBank (Meyers et al., 2004). Our analysis allows the distinction between core and peripheral arguments to be represented for predicate nouns. With this distinction, an entailment recognition system could recognise that Google’s acquisition of YouTube entailed Google acquired YouTube, because equivalent predicate-argument structures are built for both. Our analysis also recovers nonlocal dependencies mediated by nominal predicates; for instance, Google is the agent of acquire in Google’s decision to acquire YouTube. The rebanked corpus extends CCGbank with: Once released, treebanks tend to remain unchanged despite any sh"
P10-1022,J08-1003,0,0.0439007,"Missing"
P10-1022,J07-4004,1,0.928685,"Missing"
P10-1022,J05-1004,0,0.0628466,"parser’s maximum entropy features or hyperparameters, which are tuned for CCGbank. C AT 94.4 93.9 94.0 94.0 93.8 92.2 Table 3: Parser evaluation on the rebanked corpora. Corpus +NP brackets +Quotes +Propbank +Particles All Rebanking Rebanked CCGbank LF UF LF UF 86.45 86.57 87.76 87.50 87.23 92.36 92.40 92.96 92.77 92.71 86.52 86.52 87.74 87.67 88.02 92.35 92.35 92.99 92.93 93.51 10 Conclusion Research in natural language understanding is driven by the datasets that we have available. The most cited computational linguistics work to date is the Penn Treebank (Marcus et al., 1993)1 . Propbank (Palmer et al., 2005) has also been very influential since its release, and NomBank has been used for semantic dependency parsing in the CoNLL 2008 and 2009 shared tasks. This paper has described how these resources can be jointly exploited using a linguistically motivated theory of syntax and semantics. The semantic annotations provided by Propbank and NomBank allowed us to build a corpus that takes much greater advantage of the semantic transparency of a deep grammar, using careful analyses and phenomenon-specific conversion rules. The major areas of CCGbank’s grammar left to be improved are the analysis of comp"
P10-1022,U09-1017,1,0.908881,"Missing"
P10-1022,C94-2149,0,0.085401,"nd 2009 shared tasks. This paper has described how these resources can be jointly exploited using a linguistically motivated theory of syntax and semantics. The semantic annotations provided by Propbank and NomBank allowed us to build a corpus that takes much greater advantage of the semantic transparency of a deep grammar, using careful analyses and phenomenon-specific conversion rules. The major areas of CCGbank’s grammar left to be improved are the analysis of comparatives, and the analysis of named entities. English comparatives are diverse and difficult to analyse. Even the XTAG grammar (Doran et al., 1994), which deals with the major constructions of English in enviable detail, does not offer a full analysis of these phenomena. Named entities are also difficult to analyse, as many entity types obey their own specific grammars. This is another example of a phenomenon that could be analysed much better in CCGbank using an existing resource, the BBN named entity corpus. Our rebanking has substantially improved CCGbank, by increasing the granularity and linguistic fidelity of its analyses. We achieved this by exploiting existing resources and crafting novel analyses. The process we have demonstrate"
P10-1022,W03-1008,0,0.0379571,"recovered. A non-normal form derivation is shown so that instantiated variables can be seen. Carthage 0s NP (NPy /(Ny /PPz )y )NPz (NPy /(Ny /PPCarthage )y )0 s destruction N /PPy < &gt; NPdestruction In this analysis, we regard the genitive clitic as a case-marker that performs a movement operation roughly analogous to WH-extraction. Its category is therefore similar to the one used in object extraction, (N N )/(S /NP ). Figure 1 shows an example with multiple core arguments. This analysis allows recovery of verbal arguments of nominalised raising and control verbs, a construction which both Gildea and Hockenmaier (2003) and Boxwell and White (2008) identify as a problem case when aligning Propbank and CCGbank. Our analysis accommodates this construction effortlessly, as shown in Figure 2. The category assigned to decision can coindex the missing NP argument of buy with its own PP argument. When that argument is supplied by the genitive, it is also supplied to the verb, buy, filling its dependency with its agent, Google. This argument would be quite difficult to recover using a shallow syntactic analysis, as the path would be quite long. There are 494 such verb arguments mediated by nominal predicates in Sect"
P10-1022,U08-1019,1,0.879565,"Missing"
P10-1022,P07-1031,1,0.90971,"Missing"
P10-1022,P08-1039,1,0.907886,"Missing"
P10-1022,hockenmaier-steedman-2002-acquiring,0,0.0891872,"Missing"
P10-1022,P02-1043,0,\N,Missing
P10-1036,J99-2004,0,0.71339,"racy and speed improvements for Wikipedia and biomedical text. 1 Introduction In many NLP tasks and applications, e.g. distributional similarity (Curran, 2004) and question answering (Dumais et al., 2002), large volumes of text and detailed syntactic information are both critical for high performance. To avoid a tradeoff between these two, we need to increase parsing speed, but without losing accuracy. Parsing with lexicalised grammar formalisms, such as Lexicalised Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG; Steedman, 2000), can be made more efficient using a supertagger. Bangalore and Joshi (1999) call supertagging almost parsing because of the significant reduction in ambiguity which occurs once the supertags have been assigned. In this paper, we focus on the CCG parser and supertagger described in Clark and Curran (2007). 1 We use supertag and lexical category interchangeably. 345 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345–355, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics I Using the parser to generate training data also has the advantage that it is not a domain specific process. Previous wo"
P10-1036,E99-1025,0,0.096565,"Missing"
P10-1036,W02-2236,0,0.0204483,"NP S NP &gt; < &gt; < Figure 1: Two CCG derivations with PP ambiguity. can be used to find the most probable supertag sequence. Alternatively the Forward-Backward algorithm can be used to efficiently sum over all sequences, giving a probability distribution over supertags for each word which is conditional only on the input sentence. Background Supertaggers can be made accurate enough for wide coverage parsing using multi-tagging (Chen et al., 1999), in which more than one supertag can be assigned to a word; however, as more supertags are supplied by the supertagger, parsing efficiency decreases (Chen et al., 2002), demonstrating the influence of lexical ambiguity on parsing complexity (Sarkar et al., 2000). Many statistical parsers use two stages: a tagging stage that labels each word with its grammatical role, and a parsing stage that uses the tags to form a parse tree. Lexicalised grammars typically contain a much smaller set of rules than phrase-structure grammars, relying on tags (supertags) that contain a more detailed description of each word’s role in the sentence. This leads to much larger tag sets, and shifts a large proportion of the search for an optimal derivation to the tagging component o"
P10-1036,M95-1004,0,0.192466,"idering a word correct if the correct tag is amongst any of the assigned tags. For the biomedical parser evaluation we have used the parsing model and grammatical relation conversion script from Rimell and Clark (2009). Our timing measurements are calculated in two ways. Overall times were measured using the C&C parser’s timers. Individual sentence measurements were made using the Intel timing registers, since standard methods are not accurate enough for the short time it takes to parse a single sentence. To check whether changes were statistically significant we applied the test described by Chinchor (1995). This measures the probability that two sets of responses are drawn from the same distribution, where a score below 0.05 is considered significant. Models were trained on an Intel Core2Duo 3GHz with 4GB of RAM. The evaluation was performed on a dual quad-core Intel Xeon 2.27GHz with 16GB of RAM. 5.1 6 Results We have performed four primary sets of experiments to explore the ability of an adaptive supertagger to improve parsing speed or accuracy. In the first two experiments, we explore performance on the newswire domain, which is the source of training data for the parsing model and the basel"
P10-1036,C04-1041,1,0.923426,"ng component of the parser. Figure 1 gives two sentences and their CCG derivations, showing how some of the syntactic ambiguity is transferred to the supertagging component in a lexicalised grammar. Note that the lexical category assigned to with is different in each case, reflecting the fact that the prepositional phrase attaches differently. Either we need a tagging model that can resolve this ambiguity, or both lexical categories must be supplied to the parser which can then attempt to resolve the ambiguity by eventually selecting between them. 2.1 ate NP (S NP)/NP NP ((S NP)(S NP))/NP Clark and Curran (2004) applied supertagging to CCG, using a flexible multi-tagging approach. The supertagger assigns to a word all lexical categories whose probabilities are within some factor, β, of the most probable category for that word. When the supertagger is integrated with the C&C parser, several progressively lower β values are considered. If a sentence is not parsed on one pass then the parser attempts to parse the sentence again with a lower β value, using a larger set of categories from the supertagger. Since most sentences are parsed at the first level (in which the average number of supertags assigned"
P10-1036,J07-4004,1,0.901182,"etailed syntactic information are both critical for high performance. To avoid a tradeoff between these two, we need to increase parsing speed, but without losing accuracy. Parsing with lexicalised grammar formalisms, such as Lexicalised Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG; Steedman, 2000), can be made more efficient using a supertagger. Bangalore and Joshi (1999) call supertagging almost parsing because of the significant reduction in ambiguity which occurs once the supertags have been assigned. In this paper, we focus on the CCG parser and supertagger described in Clark and Curran (2007). 1 We use supertag and lexical category interchangeably. 345 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345–355, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics I Using the parser to generate training data also has the advantage that it is not a domain specific process. Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). Using a newspapertrained parser, we constructed new training sets for Wikipedia and biomedical text. These were used to create new"
P10-1036,P07-1120,0,0.0547606,"Missing"
P10-1036,W03-0407,1,0.787353,"ng analysis cannot be found by the parser, the number of lexical categories supplied by the supertagger is increased. The supertagger-parser interaction influences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence. Our goal is to increase parsing speed without loss of accuracy. The technique we use is a form of self-training, in which the output of the parser is used to train the supertagger component. The existing literature on self-training reports mixed results. Clark et al. (2003) were unable to improve the accuracy of POS tagging using self-training. In contrast, McClosky et al. (2006a) report improved accuracy through self-training for a twostage parser and re-ranker. Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adaptive supertagger. The adaptive supertagger produces lexical categories that the parser would have used in the final derivation when using the baseline model. However, it does so with much lower ambiguity levels, and potentially during an earlier pass, which means sentences are parsed faster. By increasing the"
P10-1036,W03-2401,0,0.0235798,"ed as they would be most similar in style to the evaluation corpus. In all experiments the sentences from 1989 were excluded to ensure no overlap occurred with CCGbank. As Wikipedia text we have used 794,024,397 tokens (51,673,069 sentences) from Wikipedia articles. This text was processed in the same way as the NANC data to produce parser-annotated training data. For supertagger evaluation, one thousand sentences were manually annotated with CCG lexical categories and POS tags. For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the same post-processing process as Rimell and Clark (2009) to convert th"
P10-1036,W02-1001,0,0.197012,"Missing"
P10-1036,J93-2004,0,0.0350434,"-20 14.04 17.41 39.2 21-40 28.76 29.27 49.4 41-250 49.73 86.73 10.2 All 24.83 152.15 100.0 0-4 2.81 0.60 22.4 5-20 11.64 21.56 48.9 21-40 28.02 28.48 24.3 41-250 49.69 77.70 4.5 All 15.33 154.57 100.0 0-4 2.98 0.75 0.9 5-20 14.54 15.14 41.3 21-40 28.49 29.34 48.0 41-250 49.17 68.34 9.8 All 24.53 139.35 100.0 Table 1: Statistics for sentences in the supertagger training data. Sentences containing more than 250 tokens were not included in our data sets. Training and accuracy evaluation We have used Sections 02-21 of CCGbank (Hockenmaier and Steedman, 2007), the CCG version of the Penn Treebank (Marcus et al., 1993), as training data for the newspaper domain. Sections 00 and 23 were used for development and test evaluation. A further 113,346,430 tokens (4,566,241 sentences) of raw data from the Wall Street Journal section of the North American News Corpus (Graff, 1995) were parsed to produce the training data for adaptation. This text was tokenised using the C&C tools tokeniser and parsed using our baseline models. For the smaller training sets, sentences from 1988 were used as they would be most similar in style to the evaluation corpus. In all experiments the sentences from 1989 were excluded to ensure"
P10-1036,N06-1020,0,0.0472281,"s increased. The supertagger-parser interaction influences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence. Our goal is to increase parsing speed without loss of accuracy. The technique we use is a form of self-training, in which the output of the parser is used to train the supertagger component. The existing literature on self-training reports mixed results. Clark et al. (2003) were unable to improve the accuracy of POS tagging using self-training. In contrast, McClosky et al. (2006a) report improved accuracy through self-training for a twostage parser and re-ranker. Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adaptive supertagger. The adaptive supertagger produces lexical categories that the parser would have used in the final derivation when using the baseline model. However, it does so with much lower ambiguity levels, and potentially during an earlier pass, which means sentences are parsed faster. By increasing the ambiguity level of the adaptive models to match the baseline system, we can also slightly increase supertag"
P10-1036,de-marneffe-etal-2006-generating,0,0.0116686,"Missing"
P10-1036,P06-1043,0,0.0232405,"s increased. The supertagger-parser interaction influences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence. Our goal is to increase parsing speed without loss of accuracy. The technique we use is a form of self-training, in which the output of the parser is used to train the supertagger component. The existing literature on self-training reports mixed results. Clark et al. (2003) were unable to improve the accuracy of POS tagging using self-training. In contrast, McClosky et al. (2006a) report improved accuracy through self-training for a twostage parser and re-ranker. Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adaptive supertagger. The adaptive supertagger produces lexical categories that the parser would have used in the final derivation when using the baseline model. However, it does so with much lower ambiguity levels, and potentially during an earlier pass, which means sentences are parsed faster. By increasing the ambiguity level of the adaptive models to match the baseline system, we can also slightly increase supertag"
P10-1036,U08-1013,1,0.808259,"r the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the same post-processing process as Rimell and Clark (2009) to convert the C&C parser output to Stanford format grammatical relations (de Marneffe et al., 2006). For adaptive training we have used 1,900,618,859 tokens (76,739,723 sentences) from the MEDLINE abstracts tokenised by McIntosh and Curran (2008). These sentences were POS -tagged and parsed twice, once as for the newswire and Wikipedia data, and then again, using the bio-specific models developed by Rimell and Clark (2009). Statistics for the sentences in the training sets are given in Table 1. 4.2 Speed evaluation data For speed evaluation we held out three sets of sentences from each domain-specific corpus. Specifically, we used 30,000, 4,000 and 2,000 unique sentences of length 5-20, 21-40 and 41-250 tokens respectively. Speeds on these length controlled sets were combined to calculate an overall parsing speed for the text in each"
P10-1036,W01-0521,0,0.0241195,"ery case the new models perform worse than the baseline on domains other than the one they were trained on. In some cases the models in Table 7 are less accurate than those in Table 5. This is because as well as optimising the β levels we have changed training methods. All of the training methods were tried, but only the method with the best results in newswire is included here, which for F-score when trained on 400,000 sentences was GIS. The accuracy presented so far for the biomediCross-domain speed improvement When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). In this experiment, we attempt to increase speed on out-of-domain data. Note that for some of the results presented here it may appear that the C&C parser does not lose speed when out of domain, since the Wikipedia and biomedical corpora contain shorter sentences on average than the news corpus. However, by testing on balanced sets it is clear that speed does decrease, particularly for longer sentences, as shown in Table 9. For our domain adaptation development experiments, we considered a collection of different models; here we only present results for the best set of models. For speed impr"
P10-1036,P07-1037,0,0.0368961,"Missing"
P10-1036,W07-1004,0,0.0185156,"red of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the same post-processing process as Rimell and Clark (2009) to convert the C&C parser output to Stanford format grammatical relations (de Marneffe et al., 2006). For adaptive training we have used 1,900,618,859 tokens (76,739,723 sentences) from the MEDLINE abstracts tokenised by McIntosh and Curran (2008). These sentences were POS -tagged and parsed twice, once as for the newswire and Wikipedia data, and then again, using the bio-specific models developed by Rimell and Clark (2009). Statistics for the sentences in the training sets are given in Table 1. 4.2 Speed evaluation data For sp"
P10-1036,J07-3004,0,0.343689,"Missing"
P10-1036,D08-1050,1,0.880391,"ntences were manually annotated with CCG lexical categories and POS tags. For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the same post-processing process as Rimell and Clark (2009) to convert the C&C parser output to Stanford format grammatical relations (de Marneffe et al., 2006). For adaptive training we have used 1,900,618,859 tokens (76,739,723 sentences) from the MEDLINE abstracts tokenised by McIntosh and Curran (2008). These sentences were POS -tagged and parsed twice, once as for the newswire and Wikipedia data, and then again, using the bio-specific models developed by Rimell and Clark (2009)."
P10-1036,W00-1605,0,0.0316057,"ost probable supertag sequence. Alternatively the Forward-Backward algorithm can be used to efficiently sum over all sequences, giving a probability distribution over supertags for each word which is conditional only on the input sentence. Background Supertaggers can be made accurate enough for wide coverage parsing using multi-tagging (Chen et al., 1999), in which more than one supertag can be assigned to a word; however, as more supertags are supplied by the supertagger, parsing efficiency decreases (Chen et al., 2002), demonstrating the influence of lexical ambiguity on parsing complexity (Sarkar et al., 2000). Many statistical parsers use two stages: a tagging stage that labels each word with its grammatical role, and a parsing stage that uses the tags to form a parse tree. Lexicalised grammars typically contain a much smaller set of rules than phrase-structure grammars, relying on tags (supertags) that contain a more detailed description of each word’s role in the sentence. This leads to much larger tag sets, and shifts a large proportion of the search for an optimal derivation to the tagging component of the parser. Figure 1 gives two sentences and their CCG derivations, showing how some of the"
P10-1036,N01-1023,0,0.0924336,"Missing"
P10-1036,E03-1008,1,0.886743,"Missing"
P10-1036,E09-1093,0,0.063093,"Missing"
P10-1036,W09-3832,0,0.0147488,"age (Clark and Curran, 2004). Supertagging Supertaggers typically use standard linear-time tagging algorithms, and only consider words in the local context when assigning a supertag. The C&C supertagger is similar to the Ratnaparkhi (1996) tagger, using features based on words and POS tags in a five-word window surrounding the target word, and defining a local probability distribution over supertags for each word in the sentence, given the previous two supertags. The Viterbi algorithm Supertagging has since been effectively applied to other formalisms, such as HPSG (Blunsom and Baldwin, 2006; Zhang et al., 2009), and as an information source for tasks such as Statistical Machine Translation (Hassan et al., 2007). The use of parser output for supertagger training has been explored for LTAG by Sarkar (2007). However, the focus of that work was on improving parser and supertagger accuracy rather than speed. 346 Previously , watch imports S/S N , N /N N were N S [adj ]NP denied (S [dcl ]NP )/(S [pss]NP ) (S [pss]NP )/NP (S[dcl]NP)/NP S [pss]NP (S [dcl ]NP )/(S [adj ]NP ) (S [pss]NP )/NP such NP/NP duty-free treatment N/N N (N /N )/(N /N ) N /N (S [pt]NP )/NP (S[dcl]NP)/NP Figure 2: An example"
P10-1036,W96-0213,0,\N,Missing
P10-1036,W06-1620,0,\N,Missing
P10-1036,J05-1003,0,\N,Missing
P10-1036,P06-2006,0,\N,Missing
P11-2046,W10-0907,0,0.0252375,"Missing"
P11-2046,J07-4004,1,0.788933,"The MEDLINE semantic categories within person. This guidance reduces semantic drift. 3 Experimental Setup To compare the effectiveness of RGB we consider the task of extracting biomedical semantic lexicons, building on the work of McIntosh and Curran (2008). Note however the method is equally applicable to any corpus and set of semantic categories. The corpus consists of approximately 18.5 million MEDLINE abstracts (up to Nov 2009). The text was tokenised and POS-tagged using bio-specific NLP tools (Grover et al., 2006), and parsed using the biomedical C&C CCG parser (Rimell and Clark, 2009; Clark and Curran, 2007). The term extraction data is formed from the raw 5-grams (t1 , t2 , t3 , t4 , t5 ), where the set of candidate terms correspond to the middle tokens (t3 ) and the patterns are formed from the surrounding tokens (t1 , t2 , t4 , t5 ). The relation extraction data is also formed from the 5-grams. The candidate tuples correspond to the tokens (t1 , t5 ) and the patterns are formed from the intervening tokens (t2 , t3 , t4 ). The second relation dataset (5gm + 4gm), also includes length 2 patterns formed from 4-grams. The final relation dataset (5gm + DC) includes dependency chains up to length 5"
P11-2046,W06-2703,0,0.012205,"coma melanoma neuroblastoma osteosarcoma CELL CLNE DISE DRUG FUNC MUTN PROT SIGN TUMR Table 2: The MEDLINE semantic categories within person. This guidance reduces semantic drift. 3 Experimental Setup To compare the effectiveness of RGB we consider the task of extracting biomedical semantic lexicons, building on the work of McIntosh and Curran (2008). Note however the method is equally applicable to any corpus and set of semantic categories. The corpus consists of approximately 18.5 million MEDLINE abstracts (up to Nov 2009). The text was tokenised and POS-tagged using bio-specific NLP tools (Grover et al., 2006), and parsed using the biomedical C&C CCG parser (Rimell and Clark, 2009; Clark and Curran, 2007). The term extraction data is formed from the raw 5-grams (t1 , t2 , t3 , t4 , t5 ), where the set of candidate terms correspond to the middle tokens (t3 ) and the patterns are formed from the surrounding tokens (t1 , t2 , t4 , t5 ). The relation extraction data is also formed from the 5-grams. The candidate tuples correspond to the tokens (t1 , t5 ) and the patterns are formed from the intervening tokens (t2 , t3 , t4 ). The second relation dataset (5gm + 4gm), also includes length 2 patterns form"
P11-2046,U08-1013,1,0.959359,"ries. 1 Timothy Baldwin ♠ Introduction Many approaches to extracting semantic lexicons extend the unsupervised bootstrapping framework (Riloff and Shepherd, 1997). These use a small set of seed examples from the target lexicon to identify contextual patterns which are then used to extract new lexicon items (Riloff and Jones, 1999). Bootstrappers are prone to semantic drift, caused by selection of poor candidate terms or patterns (Curran et al., 2007), which can be reduced by semantically constraining the candidates. Multicategory bootstrappers, such as NOMEN (Yangarber et al., 2002) and WMEB (McIntosh and Curran, 2008), reduce semantic drift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative categories to multi-category bootstrappers achieves the best results, by clarifying the boundaries between categories (Yangarber et al., 2002). For example, female names are often bootstrapped with 266 The University of Sydney the negative categories flowers (e.g. Rose, Iris) and gem stones (e.g. Ruby, Pearl) (Curran et al., 2007). Unfortunately, negative categories are difficult to design, introducing a substantial amount of human expertise into an otherwise unsu"
P11-2046,P09-1045,1,0.875441,"endency chains up to length 5 as the patterns between terms (Greenwood et al., 2005). These chains are formed using the Stanford dependencies generated by the Rimell and Clark (2009) parser. All candidates occurring less than 10 times were filtered. The sizes of the resulting datasets are shown in Table 1. WMEB +negative intra-RGB +negative inter-RGB +negative mixed-RGB +negative 1-500 76.1 86.9 75.7 87.4 80.5 87.7 74.7 87.9 501-1000 56.4 68.7 62.7 72.4 69.9 76.4 69.9 73.5 1-1000 66.3 77.8 69.2 79.9 75.1 82.0 72.3 80.7 INTER - RGB 5gm +negative 5gm + 4gm +negative 5gm + DC +negative We follow McIntosh and Curran (2009) in using the 10 biomedical semantic categories and their hand-picked seeds in Table 2, and manually crafted negative categories: amino acid, animal, body part and organism. Our evaluation process involved manually judging each extracted term and we calculate the average precision of the top-1000 terms over the 10 target categories. We do not calculate recall, due to the open-ended nature of the categories. Results and Discussion Table 3 compares the performance of WMEB and RGB , with and without the negative categories. For RGB , we compare intra-, inter- and mixed relation types, and use the"
P11-2046,D10-1035,1,0.76419,"ift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative categories to multi-category bootstrappers achieves the best results, by clarifying the boundaries between categories (Yangarber et al., 2002). For example, female names are often bootstrapped with 266 The University of Sydney the negative categories flowers (e.g. Rose, Iris) and gem stones (e.g. Ruby, Pearl) (Curran et al., 2007). Unfortunately, negative categories are difficult to design, introducing a substantial amount of human expertise into an otherwise unsupervised framework. McIntosh (2010) made some progress towards automatically learning useful negative categories during bootstrapping. In this work we identify an unsupervised source of semantic constraints inspired by the Coupled Pattern Learner (CPL, Carlson et al. (2010)). In CPL, relation bootstrapping is coupled with lexicon bootstrapping in order to control semantic drift in the target relation’s arguments. Semantic constraints on categories and relations are manually crafted in CPL . For example, a candidate of the relation I S C EO O F will only be extracted if its arguments can be extracted into the ceo and company lex"
P11-2046,W97-0313,0,0.051701,"fted semantic constraints such as negative categories to reduce semantic drift. Unfortunately, their use introduces a substantial amount of supervised knowledge. We present the Relation Guided Bootstrapping (RGB) algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift. This removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories. 1 Timothy Baldwin ♠ Introduction Many approaches to extracting semantic lexicons extend the unsupervised bootstrapping framework (Riloff and Shepherd, 1997). These use a small set of seed examples from the target lexicon to identify contextual patterns which are then used to extract new lexicon items (Riloff and Jones, 1999). Bootstrappers are prone to semantic drift, caused by selection of poor candidate terms or patterns (Curran et al., 2007), which can be reduced by semantically constraining the candidates. Multicategory bootstrappers, such as NOMEN (Yangarber et al., 2002) and WMEB (McIntosh and Curran, 2008), reduce semantic drift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative cate"
P11-2046,P10-1013,0,0.0996479,"Missing"
P11-2046,C02-1154,0,0.0389977,"anually generating negative categories. 1 Timothy Baldwin ♠ Introduction Many approaches to extracting semantic lexicons extend the unsupervised bootstrapping framework (Riloff and Shepherd, 1997). These use a small set of seed examples from the target lexicon to identify contextual patterns which are then used to extract new lexicon items (Riloff and Jones, 1999). Bootstrappers are prone to semantic drift, caused by selection of poor candidate terms or patterns (Curran et al., 2007), which can be reduced by semantically constraining the candidates. Multicategory bootstrappers, such as NOMEN (Yangarber et al., 2002) and WMEB (McIntosh and Curran, 2008), reduce semantic drift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative categories to multi-category bootstrappers achieves the best results, by clarifying the boundaries between categories (Yangarber et al., 2002). For example, female names are often bootstrapped with 266 The University of Sydney the negative categories flowers (e.g. Rose, Iris) and gem stones (e.g. Ruby, Pearl) (Curran et al., 2007). Unfortunately, negative categories are difficult to design, introducing a substantial amount of h"
P12-1052,D11-1031,0,0.0248402,"Missing"
P12-1052,P06-2006,0,0.192029,"Missing"
P12-1052,P05-1022,0,0.277509,"Missing"
P12-1052,P04-1014,1,0.958153,"ine them into a derivation yielding the correct dependencies, or it may not recognise the gold standard category at all. There is an additional constraint in the parser that only allows two categories to combine if they have been seen to combine in the training data. This seen rules constraint is used to reduce the size of the chart and improve parsing speed, at the cost of only permitting category combinations seen in CCGbank 0221 (Clark and Curran, 2007). Supertagger error: The supertagger uses a restricted set of 425 categories determined by a frequency cutoff of 10 over the training data (Clark and Curran, 2004b). Words with gold categories that are not in this set cannot be tagged correctly. 502 The β parameter restricts the categories to within a probability beam, and the tag dictionary restricts the set of categories that can be considered for each word. Supertagger model error occurs when the supertagger can assign a word its correct category, but the statistical model does not assign the correct tag enough probability for it to fall within the β. Model error: The parser model features may be rich enough to capture certain characteristics of parses, causing it to select a suboptimal parse. 4.1 S"
P12-1052,C04-1041,1,0.913352,"ine them into a derivation yielding the correct dependencies, or it may not recognise the gold standard category at all. There is an additional constraint in the parser that only allows two categories to combine if they have been seen to combine in the training data. This seen rules constraint is used to reduce the size of the chart and improve parsing speed, at the cost of only permitting category combinations seen in CCGbank 0221 (Clark and Curran, 2007). Supertagger error: The supertagger uses a restricted set of 425 categories determined by a frequency cutoff of 10 over the training data (Clark and Curran, 2004b). Words with gold categories that are not in this set cannot be tagged correctly. 502 The β parameter restricts the categories to within a probability beam, and the tag dictionary restricts the set of categories that can be considered for each word. Supertagger model error occurs when the supertagger can assign a word its correct category, but the statistical model does not assign the correct tag enough probability for it to fall within the β. Model error: The parser model features may be rich enough to capture certain characteristics of parses, causing it to select a suboptimal parse. 4.1 S"
P12-1052,J07-4004,1,0.858305,"different evaluation schemes, it is difficult to directly compare these numbers, but whether there is further room for improvement in CCG n-best parsing is an open question. We analyse three main classes of errors in the C & C parser in order to answer this question: grammar error, supertagger error, and model error. Furthermore, insights from this analysis will prove useful in evaluating tradeoffs made in parsers. Grammar error: the parser implements a subset of the grammar and unary type-changing rules in CCGbank for efficiency, with some rules, such as substitution, omitted for efficiency (Clark and Curran, 2007). This means that, given the correct categories for words in a sentence, the parser may be unable to combine them into a derivation yielding the correct dependencies, or it may not recognise the gold standard category at all. There is an additional constraint in the parser that only allows two categories to combine if they have been seen to combine in the training data. This seen rules constraint is used to reduce the size of the chart and improve parsing speed, at the cost of only permitting category combinations seen in CCGbank 0221 (Clark and Curran, 2007). Supertagger error: The supertagge"
P12-1052,P02-1042,0,0.316698,"Missing"
P12-1052,P96-1011,0,0.251502,"Missing"
P12-1052,P03-1046,0,0.0791396,"Missing"
P12-1052,P06-1064,0,0.0308661,"Missing"
P12-1052,J07-3004,0,0.0461143,"Missing"
P12-1052,P08-1067,0,0.0406554,"Missing"
P12-1052,W05-1506,0,0.165705,"Missing"
P12-1052,2006.amta-papers.8,0,0.0265588,"Missing"
P12-1052,W03-2401,0,0.262359,"Missing"
P12-1052,W96-0213,0,0.60623,"Missing"
P12-1052,C10-1122,1,0.893144,"Missing"
P12-1052,W02-2033,0,0.0152444,"Missing"
P12-2021,H91-1060,0,0.0702437,"Missing"
P12-2021,P11-1048,0,0.0449345,"Missing"
P12-2021,J08-1003,0,0.0563225,"Missing"
P12-2021,P05-1022,0,0.10656,"Missing"
P12-2021,P00-1058,0,0.0132874,"se errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics S NP JJ VP NNS VBD S NP PRP$ NP NN NN DT Italian magistrates labeled his death"
P12-2021,J07-4004,1,0.831639,"Missing"
P12-2021,P09-2014,1,0.921488,"re not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics S NP JJ VP NNS VBD S NP PRP$ NP NN NN DT Italian magistrates labeled his death a suicide N /N N ((S [dcl ]NP )/NP )/NP NP [nb]/N N NP [nb]/N N &gt; N NP &gt; NP &gt;"
P12-2021,P97-1003,0,0.0734998,"Missing"
P12-2021,P10-1035,0,0.0303575,"Missing"
P12-2021,P03-1054,1,0.0215858,"Missing"
P12-2021,J93-2004,0,0.0443419,"Missing"
P12-2021,C08-1069,0,0.386976,"ch as QPs, NXs, and NACs. Many of these errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics S NP JJ VP NNS VBD S NP PRP$ NP NN NN DT Italian magistr"
P12-2021,N07-1051,1,0.802091,"Missing"
P12-2021,N01-1023,0,0.0298084,"inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics S NP JJ VP NNS VBD S NP PRP$ NP NN NN DT Italian magistrates labeled his death a suicide N /N"
P12-2021,H01-1014,0,0.0421712,"he most common errors our approach makes involve nodes for clauses and rare spans such as QPs, NXs, and NACs. Many of these errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Associa"
P12-2045,bejan-harabagiu-2008-linguistic,0,0.0306006,"nably handle novel events. 1 For binary sentence classification, we calculate an interquartile range of κ ∈ [0.46, 0.64] over the 33 sub-types. Coarse event type classification ranges from κ = 0.47 for business to κ = 0.69 for conflict. 229 Identity Event coreference is complicated by partitive (sub-event) and logical (e.g. causation) relationships between events, in addition to lexicalsemantic and syntactic issues. When considering the relationship between another carjacking and grabbed, drove or stabbed, ACE05 would apply the policy: “When in doubt, do not mark any coreference” (LDC, 2005). Bejan and Harabagiu (2008) consider event coreference across documents, marking the “most important events” (Bejan, 2010), albeit within Google News clusters, where multiple articles reporting the same event are likely to use similar language. Similar challenges apply to identifying event causality and other relations: Bejan and Harabagiu (2008) suggest arcs such as feeding precedes walking enables grabbed – akin to instantia−−−→ −−−−→ tions of FrameNet’s frame relations (Fillmore et al., 2003). However, these too are semantically subtle. Explicit reference By considering events through topical document clusters, TDT a"
P12-2045,E06-1002,0,0.0418495,"Missing"
P12-2045,P11-1098,0,0.0258945,"vent typology, dividing eight thematic event types (business, justice, etc.) into 33 subtypes such as attack, die and declare bankruptcy (LDC, 2005). Most subtypes suffer from few annotated instances, while others are impractically broad: sexual abuse, gunfire and the Holocaust each constitute attack instances (is told considered an attack in Figure 1?). Inter-annotator agreement is low for most types.1 While ACE05 would mark the various attack events in our story, police warned would be unrecognised. Despite template adaptation (Yangarber et al., 2000; Filatova et al., 2006; Li et al., 2010; Chambers and Jurafsky, 2011), event types are brittle to particular tasks and domains, such as bio-text mining (e.g. Kim et al., 2009); they cannot reasonably handle novel events. 1 For binary sentence classification, we calculate an interquartile range of κ ∈ [0.46, 0.64] over the 33 sub-types. Coarse event type classification ranges from κ = 0.47 for business to κ = 0.69 for conflict. 229 Identity Event coreference is complicated by partitive (sub-event) and logical (e.g. causation) relationships between events, in addition to lexicalsemantic and syntactic issues. When considering the relationship between another carja"
P12-2045,D07-1074,0,0.0210692,"e newspapers link past event mentions to relevant news stories, but currently do so with low coverage and consistency; an event linker can add referentially-precise hyperlinks to news. The event linking task parallels entity linking (NEL; Ji and Grishman, 2011), considering a news archive as a knowledge base (KB) of events, where each article exclusively represents the zero or more events that it first reports. Coupled with an appropriate event extractor, event linking may be performed for all events mentioned in a document, like the named entity disambiguation task (Bunescu and Pa¸sca, 2006; Cucerzan, 2007). We have annotated and analysed 150 news and opinion articles, marking references to past, newsworthy events, and linking where possible to canonical articles in a 13-year news archive. 2 The events in a news story Approaches to news event processing are subsumed within broader notions of topics, scenario templates, or temporal entities, among others. We illustrate key challenges in processing news events and motivate event linking through the example story in Figure 1. Salience Our story highlights carjackings and a police warning as newsworthy, alongside events like feeding, drove and told"
P12-2045,P06-2027,0,0.0139541,"heim, 1996). ACE05 considers a broader event typology, dividing eight thematic event types (business, justice, etc.) into 33 subtypes such as attack, die and declare bankruptcy (LDC, 2005). Most subtypes suffer from few annotated instances, while others are impractically broad: sexual abuse, gunfire and the Holocaust each constitute attack instances (is told considered an attack in Figure 1?). Inter-annotator agreement is low for most types.1 While ACE05 would mark the various attack events in our story, police warned would be unrecognised. Despite template adaptation (Yangarber et al., 2000; Filatova et al., 2006; Li et al., 2010; Chambers and Jurafsky, 2011), event types are brittle to particular tasks and domains, such as bio-text mining (e.g. Kim et al., 2009); they cannot reasonably handle novel events. 1 For binary sentence classification, we calculate an interquartile range of κ ∈ [0.46, 0.64] over the 33 sub-types. Coarse event type classification ranges from κ = 0.47 for business to κ = 0.69 for conflict. 229 Identity Event coreference is complicated by partitive (sub-event) and logical (e.g. causation) relationships between events, in addition to lexicalsemantic and syntactic issues. When con"
P12-2045,C96-1079,0,0.692627,"g. We introduce event linking, which canonically labels an event reference with the article where it was first reported. This implicitly relaxes coreference to co-reporting, and will practically enable augmenting news archives with semantic hyperlinks. We annotate and analyse a corpus of 150 documents, extracting 501 links to a news archive with reasonable inter-annotator agreement. 1 Introduction Interpreting news requires identifying its constituent events. Information extraction (IE) makes this feasible by considering only events of a specified type, such as personnel succession or arrest (Grishman and Sundheim, 1996; LDC, 2005), an approach not extensible to novel events, or the same event types in sub-domains, e.g. sport. On the other hand, topic detection and tracking (TDT; Allan, 2002) disregards individual event mentions, clustering together articles that share a topic. Between these fine and coarse-grained approaches, event identification requires grouping references to the same event. However, strict coreference is hampered by the complexity of event semantics: poison, murder and die may indicate the same effective event. The solution is to tag mentions with a canonical identifier for each news-tri"
P12-2045,P11-1115,0,0.0299279,"urder and die may indicate the same effective event. The solution is to tag mentions with a canonical identifier for each news-triggering event. This paper introduces event linking: given a past event reference in context, find the article in a news archive that first reports that the event happened. The task has an immediate practical application: some online newspapers link past event mentions to relevant news stories, but currently do so with low coverage and consistency; an event linker can add referentially-precise hyperlinks to news. The event linking task parallels entity linking (NEL; Ji and Grishman, 2011), considering a news archive as a knowledge base (KB) of events, where each article exclusively represents the zero or more events that it first reports. Coupled with an appropriate event extractor, event linking may be performed for all events mentioned in a document, like the named entity disambiguation task (Bunescu and Pa¸sca, 2006; Cucerzan, 2007). We have annotated and analysed 150 news and opinion articles, marking references to past, newsworthy events, and linking where possible to canonical articles in a 13-year news archive. 2 The events in a news story Approaches to news event proce"
P12-2045,R09-1032,0,0.0304175,"Rawson Street, Auburn, about 1.20am. The next day, a 25-year-old woman was stabbed in her lower back as she got into her car on Liverpool Road. . . N B N B Figure 1: Possible event mentions marked in an article from SMH, segmented into news (N) and background (B) event portions. ishman and Sundheim, 1996) selects an event type of which all instances are salient; TDT (Allan, 2002) operates at the document level, which avoids differentiating event mentions; and TimeML (Pustejovsky et al., 2003) marks the main event in each sentence. Critiquing ACE05 event detection for not addressing salience, Ji et al. (2009) harness cross-document frequencies for event ranking. Similarly, reference to a previously-reported event implies it is newsworthy. Diversity IE traditionally targets a selected event type (Grishman and Sundheim, 1996). ACE05 considers a broader event typology, dividing eight thematic event types (business, justice, etc.) into 33 subtypes such as attack, die and declare bankruptcy (LDC, 2005). Most subtypes suffer from few annotated instances, while others are impractically broad: sexual abuse, gunfire and the Holocaust each constitute attack instances (is told considered an attack in Figure"
P12-2045,W09-1401,0,0.0286072,"declare bankruptcy (LDC, 2005). Most subtypes suffer from few annotated instances, while others are impractically broad: sexual abuse, gunfire and the Holocaust each constitute attack instances (is told considered an attack in Figure 1?). Inter-annotator agreement is low for most types.1 While ACE05 would mark the various attack events in our story, police warned would be unrecognised. Despite template adaptation (Yangarber et al., 2000; Filatova et al., 2006; Li et al., 2010; Chambers and Jurafsky, 2011), event types are brittle to particular tasks and domains, such as bio-text mining (e.g. Kim et al., 2009); they cannot reasonably handle novel events. 1 For binary sentence classification, we calculate an interquartile range of κ ∈ [0.46, 0.64] over the 33 sub-types. Coarse event type classification ranges from κ = 0.47 for business to κ = 0.69 for conflict. 229 Identity Event coreference is complicated by partitive (sub-event) and logical (e.g. causation) relationships between events, in addition to lexicalsemantic and syntactic issues. When considering the relationship between another carjacking and grabbed, drove or stabbed, ACE05 would apply the policy: “When in doubt, do not mark any corefer"
P12-2045,C00-2136,0,0.0496989,"type (Grishman and Sundheim, 1996). ACE05 considers a broader event typology, dividing eight thematic event types (business, justice, etc.) into 33 subtypes such as attack, die and declare bankruptcy (LDC, 2005). Most subtypes suffer from few annotated instances, while others are impractically broad: sexual abuse, gunfire and the Holocaust each constitute attack instances (is told considered an attack in Figure 1?). Inter-annotator agreement is low for most types.1 While ACE05 would mark the various attack events in our story, police warned would be unrecognised. Despite template adaptation (Yangarber et al., 2000; Filatova et al., 2006; Li et al., 2010; Chambers and Jurafsky, 2011), event types are brittle to particular tasks and domains, such as bio-text mining (e.g. Kim et al., 2009); they cannot reasonably handle novel events. 1 For binary sentence classification, we calculate an interquartile range of κ ∈ [0.46, 0.64] over the 33 sub-types. Coarse event type classification ranges from κ = 0.47 for business to κ = 0.69 for conflict. 229 Identity Event coreference is complicated by partitive (sub-event) and logical (e.g. causation) relationships between events, in addition to lexicalsemantic and syn"
P12-2045,Y10-1027,0,\N,Missing
P13-2018,E09-1031,0,0.17099,"h error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 James R. Curran‡ Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modiﬁcations to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, covering only part of th"
P13-2018,D07-1027,0,0.0730843,"Missing"
P13-2018,P09-1059,0,0.08056,"lso investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 James R. Curran‡ Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modiﬁcations to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, c"
P13-2018,P03-1054,1,0.0191192,"g some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, covering only part of the parser output, and was unable to characterize the relative impact of the issues they uncovered. 2 Background The closest previous work is the detailed manual analysis performed by Levy and Manning (2003). While their focus was on issues faced by their factored PCFG parser (Klein and Manning, 2003b), the error types they identiﬁed are general issues presented by Chinese syntax in the PCTB. They presented several Chinese error types that are rare or absent in English, including noun/verb ambiguity, NP-internal structure and coordination ambiguity due to pro-drop, suggesting that closing the English-Chinese parsing gap demands techniques 1 The system described in this paper is available from http://code.google.com/p/berkeley-parser-analyser/ 98 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 98–103, c Sofia, Bulgaria, August 4-9 2013. 2013 A"
P13-2018,D12-1096,1,0.875836,"Missing"
P13-2018,P03-1056,0,0.0337462,"only addresses certain error types, leaving substantial outstanding challenges. 1 James R. Curran‡ Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modiﬁcations to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, covering only part of the parser output, and was unable to characterize the relative impact of the issues they uncovered. 2 Bac"
P13-2018,J93-2004,0,0.0433648,"Missing"
P13-2018,W09-3825,0,0.0367848,"ach Span Label Sense Edge Attach Attach Attach Other 0.76 0.72 0.21 0.30 0.05 0.21 0.26 0.22 0.18 1.87 1.48 1.68 1.06 1.02 0.88 0.55 0.50 0.44 0.44 4.11 Table 2: Error breakdown for the development set of PCTB 6. The area ﬁlled in for each bar indicates the average number of bracket errors per sentence attributed to that error type, where an empty bar is no errors and a full bar has the value indicated in the bottom row. The parsers are: the Berkeley parser with gold POS tags as input (Berk-G), the Berkeley product parser with two grammars (Berk-2), the Berkeley parser (Berk-1), the parser of Zhang and Clark (2009) (ZPAR), the Bikel parser (Bikel), the Stanford Factored parser (Stan-F), and the Stanford Unlexicalized PCFG parser (Stan-P). two categories (e.g. between Verb taking wrong args and NP Attachment). Differences in treebank annotations also present a challenge for cross-language error comparison. The most common error type in Chinese, NPinternal structure, is rare in the results of Kummerfeld et al. (2012), but the datasets are not comparable because the PTB has very limited NP-internal structure annotated. Further characterization of the impact of annotation differences on errors is beyond the"
P13-2018,P06-1055,1,0.686979,"Missing"
P13-2018,D12-1046,0,0.0984765,"se parsers, covering a broad range of error types for large sets of sentences, enabling the ﬁrst empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 James R. Curran‡ Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modiﬁcations to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis t"
P13-2018,N12-1030,1,0.873947,"as been a major part of several recent papers (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009). The Berk-G row of Table 2 shows the performance of the Berkeley parser when given gold POS tags.5 While the F1 improvement is unsurprising, for the ﬁrst time we can clearly show that the gains are only in a subset of the error types. In particular, tagging improvement will not help for two of the most signiﬁcant challenges: coordination scope errors, and verb argument selection. To see which tagging confusions contribute to which error reductions, we adapt the POS ablation approach of Tse and Curran (2012). We consider the POS tag pairs shown in Table 3. To isolate the effects of each confusion we start from the gold tags and introduce the output of the Stanford tagger whenever it returns one of the two tags being considered.6 We then feed these “semi-gold” tags Cross-parser analysis The previous section described the error types and their distribution for a single Chinese parser. Here we conﬁrm that these are general trends, by showing that the same pattern is observed for several different parsers on the PCTB 6 dev set.3 We include results for a transition-based parser (ZPAR; Zhang and Clark,"
P13-2018,I05-1007,0,0.0330528,"Missing"
P13-2018,N07-1051,1,\N,Missing
P13-2018,W00-1201,0,\N,Missing
P13-2018,N10-1003,0,\N,Missing
P13-2092,P12-1042,0,0.0594818,"Missing"
P13-2092,W06-0301,0,0.0395286,"ducts that are discussed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classification that can be combined to form an opinion summary. These approaches assume that each document has a single source (the document’s author), whose communicative goal is to evaluate a well-defined target, such as a product or a movie. However this does not hold in news articles, where the goal of the journalist is to present the viewpoints of potentially many people. Several studies (Wiebe et al., 2005; Wilson et al., 2005; Kim and Hovy, 2006; Godbole et al., 2007) have looked at sentiment in news text, with some (Balahur and Steinberger, 2009; Balahur et al., 2009, 2010) focusing on quotes. In all of these studies the authors have textually-anchored the target of the sentiment. While this makes sense for targets that can be resolved back to named entities, it does not apply as obviously when the quote is arguing for a particular viewpoint in a debate, as the topic may not be mentioned explicitly and polarity labels may not align to sides of the debate. Work on debate summarisation and subgroup detection (Somasundaran and Wiebe, 2"
P13-2092,D12-1072,1,0.88906,"Missing"
P13-2092,W02-1011,0,0.0146226,"f which 1,183 were marked invalid, leaving 2,228 that were marked as supporting, neutral, or opposing the relevant topic statement. All quotes in our corpus were annotated by three annotators, with Fleiss’ κ values of between 0.43 and 0.45, which is moderate. 516 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 516–520, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Background Topic Abortion Carbon tax Immigration Reconcil. Republic Same-sex m. Work choices Total Early work in sentiment analysis (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Blitzer et al., 2007) focused on product and movie reviews, where the text under analysis discusses a single product or movie. In these cases, labels like positive and negative are appropriate as they align well with the overall communicative goal of the text. Later work established aspect-oriented opinion mining (Hu and Liu, 2004), where the aim is to find features or aspects of products that are discussed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classification that can be comb"
P13-2092,P07-1056,0,0.0829521,"ving 2,228 that were marked as supporting, neutral, or opposing the relevant topic statement. All quotes in our corpus were annotated by three annotators, with Fleiss’ κ values of between 0.43 and 0.45, which is moderate. 516 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 516–520, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Background Topic Abortion Carbon tax Immigration Reconcil. Republic Same-sex m. Work choices Total Early work in sentiment analysis (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Blitzer et al., 2007) focused on product and movie reviews, where the text under analysis discusses a single product or movie. In these cases, labels like positive and negative are appropriate as they align well with the overall communicative goal of the text. Later work established aspect-oriented opinion mining (Hu and Liu, 2004), where the aim is to find features or aspects of products that are discussed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classification that can be combined to form an opinion summary. These app"
P13-2092,W10-0214,0,0.0833236,"Missing"
P13-2092,P02-1053,0,0.0157859,",428 quotes, of which 1,183 were marked invalid, leaving 2,228 that were marked as supporting, neutral, or opposing the relevant topic statement. All quotes in our corpus were annotated by three annotators, with Fleiss’ κ values of between 0.43 and 0.45, which is moderate. 516 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 516–520, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Background Topic Abortion Carbon tax Immigration Reconcil. Republic Same-sex m. Work choices Total Early work in sentiment analysis (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Blitzer et al., 2007) focused on product and movie reviews, where the text under analysis discusses a single product or movie. In these cases, labels like positive and negative are appropriate as they align well with the overall communicative goal of the text. Later work established aspect-oriented opinion mining (Hu and Liu, 2004), where the aim is to find features or aspects of products that are discussed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classificati"
P13-2092,H05-2018,0,0.0525555,"res or aspects of products that are discussed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classification that can be combined to form an opinion summary. These approaches assume that each document has a single source (the document’s author), whose communicative goal is to evaluate a well-defined target, such as a product or a movie. However this does not hold in news articles, where the goal of the journalist is to present the viewpoints of potentially many people. Several studies (Wiebe et al., 2005; Wilson et al., 2005; Kim and Hovy, 2006; Godbole et al., 2007) have looked at sentiment in news text, with some (Balahur and Steinberger, 2009; Balahur et al., 2009, 2010) focusing on quotes. In all of these studies the authors have textually-anchored the target of the sentiment. While this makes sense for targets that can be resolved back to named entities, it does not apply as obviously when the quote is arguing for a particular viewpoint in a debate, as the topic may not be mentioned explicitly and polarity labels may not align to sides of the debate. Work on debate summarisation and subgroup detection (Somas"
P13-2092,D12-1006,0,0.0554377,"looked at sentiment in news text, with some (Balahur and Steinberger, 2009; Balahur et al., 2009, 2010) focusing on quotes. In all of these studies the authors have textually-anchored the target of the sentiment. While this makes sense for targets that can be resolved back to named entities, it does not apply as obviously when the quote is arguing for a particular viewpoint in a debate, as the topic may not be mentioned explicitly and polarity labels may not align to sides of the debate. Work on debate summarisation and subgroup detection (Somasundaran and Wiebe, 2010; AbuJbara et al., 2012; Hassan et al., 2012) has often used data from online debate forums, particularly those forums where users are asked to select whether they support or oppose a given proposition before they can participate. This is similar to our aim with news text, where instead of a textually-anchored target, we have a proposition, against which we can evaluate quotes. 3 Quotes 343 278 249 513 347 246 269 2,245 No cont. AA κ .77 .57 .71 .42 .58 .18 .66 .37 .68 .51 .72 .51 .72 .45 .69 .43 Context AA κ .73 .53 .57 .34 .58 .25 .68 .44 .71 .58 .71 .55 .65 .44 .66 .45 Table 2: Average Agreement (AA) and Fleiss’ κ over the valid quote"
P13-2092,balahur-etal-2010-sentiment,0,\N,Missing
P13-2118,D08-1031,0,0.0325972,"Missing"
P13-2118,C10-2012,0,0.0370738,"Missing"
P13-2118,W03-0424,1,0.728568,". This paper contributes three complementary techniques for more faithfully modelling apposition. Any adjacent NPs, disregarding intervening punctuation, could be considered candidates, however stronger syntactic constraints that only allow sibling NP children provide higher precision candidate sets. Semantic compatibility features encoding that an ATTR provides consistent information for its HEAD. A joint classifier models the complete apposition rather than combining separate phrase-wise decisions. Taggers and parsers are trained on TRAIN and evaluated on DEV or TEST . We use the C&C tools (Curran and Clark, 2003) for POS and NE tagging and the and the Berkeley Parser (Petrov and Klein, 2007), trained with default parameters. Rule We only consider HEADs whose syntactic head is a PER, ORG, LOC or GPE NE. We formalise semantic compatibility by requiring the ATTR head to match a gazetteer dependent on the HEAD’s NE type. To create PER, ORG and LOC gazetteers, we identified common ATTR heads in TRAIN and looked for matching WordNet synsets, selecting the most general hypernym that was still semantically compatible with the HEAD’s NE type. Gazetteer words are pluralized using pattern.en (De Smedt and Daelem"
P13-2118,H05-1031,0,0.0368631,"Missing"
P13-2118,D12-1096,1,0.821088,"Missing"
P13-2118,D08-1068,0,0.060477,"Missing"
P13-2118,W11-1902,0,0.0551298,"Missing"
P13-2118,W11-1901,0,0.0330576,".4 93.3 532 14.1 63.0 205 5.4 81.3 16 0.4 85.6 23 0.6 88.7 35 0.9 91.4 21 0.6 92.8 Table 2: Apposition forms in TRAIN with abstract (top) and actual (bottom) tokens, e.g., H t A indicates an HEAD, one token then an ATTR. 3 Data We use apposition-annotated documents from the English section of OntoNotes 4 (Weischedel et al., 2011). We manually adjust appositions that do not have exactly one HEAD and one or more ATTR1 . Some appositions are nested, and we keep only “leaf” appositions, removing the higher-level appositions. We follow the CoNLL-2011 scheme to select TRAIN , DEV and TEST datasets (Pradhan et al., 2011). OntoNotes 4 is made up of a wide variety of sources: broadcast conversation and news, magazine, newswire and web text. Appositions are most frequent in newswire (one per 192 words) and least common in broadcast conversation (one per 645 words) with the others in between (around one per 315 words). We also replicate the OntoNotes 2.9 BN data used by FHT, selecting the same sentences from OntoNotes 4 (TRAINF /DEVF /TESTF ). We do not “speechify” our data and take a different approach to nested apposition. Table 1 shows the distribution of sentences and appositions (HEAD-ATTR pairs). 3.1 Analys"
P13-2118,H05-1083,0,0.0353123,"typically adjacent coreferent noun phrases (NP) that often add information about named entities (NEs). The apposition in Figure 1 consists of three comma-separated NPs – the first NP ( HEAD ) names an entity and the others ( ATTR s) supply age and profession attributes. Attributes can be difficult to identify despite characteristic punctuation cues, as punctuation plays many roles and attributes may have rich substructure. While linguists have studied apposition in detail (Quirk et al., 1985; Meyer, 1992), most apposition extraction has been within other tasks, such as coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007) and textual entailment (Roth and Sammons, 2007). Extraction has rarely been intrinsically evaluated, with Favre and HakkaniT¨ur’s work a notable exception. We analyze apposition distribution in OntoNotes 4 (Pradhan et al., 2007) and compare rule-based, classification and parsing extraction systems. Our best system uses a joint model to classify pairs of NPs with features that faithfully encode syntactic and semantic restrictions on appositions, using parse trees and WordNet synsets. Background Apposition is widely studied, but “grammarians vary in the freedom with which"
P13-2118,C10-1089,0,0.0250434,"Missing"
P13-2118,W07-1418,0,0.0249098,"ation about named entities (NEs). The apposition in Figure 1 consists of three comma-separated NPs – the first NP ( HEAD ) names an entity and the others ( ATTR s) supply age and profession attributes. Attributes can be difficult to identify despite characteristic punctuation cues, as punctuation plays many roles and attributes may have rich substructure. While linguists have studied apposition in detail (Quirk et al., 1985; Meyer, 1992), most apposition extraction has been within other tasks, such as coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007) and textual entailment (Roth and Sammons, 2007). Extraction has rarely been intrinsically evaluated, with Favre and HakkaniT¨ur’s work a notable exception. We analyze apposition distribution in OntoNotes 4 (Pradhan et al., 2007) and compare rule-based, classification and parsing extraction systems. Our best system uses a joint model to classify pairs of NPs with features that faithfully encode syntactic and semantic restrictions on appositions, using parse trees and WordNet synsets. Background Apposition is widely studied, but “grammarians vary in the freedom with which they apply the term ‘apposition”’ (Quirk et al., 1985). They are usual"
P13-2118,P08-1117,0,0.0746461,"Missing"
P13-2118,U07-1016,1,0.770486,", trained with default parameters. Rule We only consider HEADs whose syntactic head is a PER, ORG, LOC or GPE NE. We formalise semantic compatibility by requiring the ATTR head to match a gazetteer dependent on the HEAD’s NE type. To create PER, ORG and LOC gazetteers, we identified common ATTR heads in TRAIN and looked for matching WordNet synsets, selecting the most general hypernym that was still semantically compatible with the HEAD’s NE type. Gazetteer words are pluralized using pattern.en (De Smedt and Daelemans, 2012) and normalised. We use partitive and NML-aware rules (Collins, 1999; Vadas and Curran, 2007) to extract syntactic heads from ATTRs. These must match the typeappropriate gazetteer, with ORG and LOC/GPE falling back to PER (e.g., “the champion, Apple”). Extracted tuples are post-processed as for Pattern and reranked by the OntoNotes specificity scale (i.e., NNP &gt; PRO &gt; Def. NP &gt; Indef. NP &gt; NP), and the more specific unit is assigned HEAD . Possible ATTR s further to the left or right are checked, allowing for cases such as Figure 1. Pattern POS, NE and lexical patterns are used to extract appositions avoiding parsing’s computational overhead. Rules are applied independently to tokeniz"
P13-2118,W08-1703,0,0.071039,"Missing"
P13-2118,N07-1011,0,\N,Missing
P13-2118,J03-4003,0,\N,Missing
P13-2118,W08-1700,0,\N,Missing
P13-2118,W09-2105,0,\N,Missing
P15-1111,P14-2131,0,0.0173918,"the parse, but ignore them during evaluation. We run the modified parsers over WSJ 22 with and without each set of constraints. We examine the overall unlabeled and labeled attachment scores (UAS and LAS), as well as identifying the contribution to the overall UAS improvement from directly (constrained) and indirectly corrected arcs. MSTparser uses coarse-grained tags and finegrained POS tags in its features, both of which were provided by the CoNLL-X Shared Task. We approximate the coarse-grained POS tags by taking the first character of the MXPOST-assigned POS tag, a technique also used by Bansal et al. (2014)3 . 6 Results Figure 2 and Table 2 show the impact of applying constraints on tokens with various labels to MSTparser for the sentence in Figure 1. Enforcing the gold nn arc between decline and LME repairs that noun phrase error, but does not affect any of the other errors. Conversely, enforcing the gold root arc does not affect the noun phrase error, but repairs nearly every other error in the parse. Unfortunately, the constrained root arc introduces 3 nlp.stanford.edu/software/lex-parser.shtml 1152 Mohit Bansal, p.c. Error class Baseline NP attachment NP internal Modifier attachment PP attac"
P15-1111,W06-2920,0,0.646363,"tion. The corpus has been converted to basic Stanford dependencies using the Stanford Parser v2.0,2 and part-of-speech 2 tagged using MXPOST (Ratnaparkhi, 1996). A model trained on WSJ sections 2-21 was used to tag the development set, and 10-fold jackknife training was used to tag the training data. We implement a custom evaluation script to facilitate a straightforward comparative analysis between the unconstrained and constrained output. The script is based on and produces identical scores to eval06.pl, the official evaluation for the CoNLL-X Shared Task on Multilingual Dependency Parsing (Buchholz and Marsi, 2006). We ignore punctuation as defined by eval06.pl in our evaluation; experiments with constraints over punctuation tokens constrain those tokens in the parse, but ignore them during evaluation. We run the modified parsers over WSJ 22 with and without each set of constraints. We examine the overall unlabeled and labeled attachment scores (UAS and LAS), as well as identifying the contribution to the overall UAS improvement from directly (constrained) and indirectly corrected arcs. MSTparser uses coarse-grained tags and finegrained POS tags in its features, both of which were provided by the CoNLL-"
P15-1111,cer-etal-2010-parsing,0,0.0605016,"Missing"
P15-1111,P05-1022,0,0.0145801,"luation, attempting to identify important sources of error in dependency parsers. Our constraint-based approach shares similarities to oracle training and decoding methods, where an external source of truth is used to verify parser decisions. An oracle source of parser actions is a necessary component for training transition-based parsers (Nivre, 2009). Oracle decoding, where a system is forced to produce correct output if possible, can be used to assess its upper performance bounds (Ng and Curran, 2012). Constraining the parser’s internal search space is akin to an optimal pruning operation. Charniak and Johnson (2005) use a coarse-to-fine, iterative pruning approach for efficiently generating high-quality n-best parses for a discriminative reranker. Rush and Petrov (2012) use a similar coarse-to-fine algorithm with vine grammars (Eisner and Smith, 2005) to accelerate graph-based de1151 root det amod punct conj cc punct pobj nn nsubj The LME stocks decline prep det advmod was about as nn expected , nsubj but the Comex gain neg was n’t . Figure 2: MSTparser output for the sentence in Figure 1, where the root dependency is forced to its correct value. The incorrect noun phrase error is not affected by the con"
P15-1111,P07-1032,1,0.66594,", red), six attachment errors are repaired (solid, blue), and two new errors are introduced (dotted, purple). Constraints None nn root 3 8 9 14 7 9 8 3 punct 14 3 ccomp 16 1 Remaining Errors see Figure 1 All except decline → LME decline → LME about → expected was → about decline → LME about → expected was → about decline → LME Table 2: Correct and incorrect arcs, and the remaining errors after applying various sets of constraints to the sentence in Figure 1. pendency parsing, achieving parsing speeds close to linear-time transition parsers despite encoding more complex features. Supertagging (Clark and Curran, 2007) and chart pruning (Zhang et al., 2010) have been used to constrain the search space of a CCG parser, and to remove unlikely or forbidden spans from repeated consideration. In our work, we use pruning not for parsing speed, but evaluation, and so we prune items based on goldstandard constraints rather than heuristics. 5 Evaluation We use the training (sections 2-21) and development (section 22) data from the OntoNotes 4.0 release of the Penn Treebank WSJ data (Marcus et al., 1993), as supplied by the SANCL 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012). OntoNotes annotates enr"
P15-1111,C96-1058,0,0.114107,", expl, mark, mwe, neg, parataxis, prt, ref, and rel. For example, Root constraints specify sentence roots, while PP constraints specify heads of prepositional phrases. One deficiency of our implementation is that we apply constraints to all arcs of a particular error type in each sentence, and do not isolate multiple instances of the same error class in a sentence. We do this since applying single constraints to a sentence at a time would require substantial modifications to the standard evaluation regime. 3.1 MSTparser implementation MSTparser is a graph-based, second-order parser that uses Eisner (1996)’s algorithm for projective decoding (McDonald and Pereira, 2006).1 Eisner’s algorithm constructs and caches subtrees which span progressively larger sections of the sentence. These spans are marked either as complete, consisting of a head, a dependent, and all of the descendants of that head to one side, or incomplete, consisting of a head, a dependent, and an unfilled region where additional tokens may be attached. Dependencies are formed between the head and dependent in each complete span, while label assignment occurs as a separate process. We enforce constraints by allowing complete span"
P15-1111,W05-1504,0,0.0385274,"n oracle source of parser actions is a necessary component for training transition-based parsers (Nivre, 2009). Oracle decoding, where a system is forced to produce correct output if possible, can be used to assess its upper performance bounds (Ng and Curran, 2012). Constraining the parser’s internal search space is akin to an optimal pruning operation. Charniak and Johnson (2005) use a coarse-to-fine, iterative pruning approach for efficiently generating high-quality n-best parses for a discriminative reranker. Rush and Petrov (2012) use a similar coarse-to-fine algorithm with vine grammars (Eisner and Smith, 2005) to accelerate graph-based de1151 root det amod punct conj cc punct pobj nn nsubj The LME stocks decline prep det advmod was about as nn expected , nsubj but the Comex gain neg was n’t . Figure 2: MSTparser output for the sentence in Figure 1, where the root dependency is forced to its correct value. The incorrect noun phrase error is not affected by the constraint (dashed, red), six attachment errors are repaired (solid, blue), and two new errors are introduced (dotted, purple). Constraints None nn root 3 8 9 14 7 9 8 3 punct 14 3 ccomp 16 1 Remaining Errors see Figure 1 All except decline →"
P15-1111,W07-2416,0,0.0256275,"re is adapting each parser to accept a set of constraints. Following Kummerfeld et al. (2012), we define meaningful error classes grouped with the operations that repair them. In dependency parsing, error classes are groups of Stanford dependency labels, rather than groups of node repair operations. The Stanford labels provide a rich distinction in NP internal structure, clauses, and modifiers, and map well to the error categories of Kummerfeld et al. (2012), allowing us to avoid excessive heuristics in the mapping process. Our technique can be applied to other dependency schemes such as LTH (Johansson and Nugues, 2007) by defining new mappings from labels to error types. The difficulty of the mapping task depends on the intricacies of each formalism. The major challenge with LTH dependencies is the enormous skew towards the nominal modifier NMOD label. This label occurs 11,335 times in WSJ 22, more than twice as frequently as the next most frequent punctuation P. By contrast, the most common Stanford label is punctuation, at 4,731 occurrences. The NMOD label is split into many smaller, but more informative nominal labels in the Stanford scheme, making it better suited for our goal of error analysis. The lab"
P15-1111,D12-1096,1,0.929029,"Missing"
P15-1111,J93-2004,0,0.0503786,"ng parsing speeds close to linear-time transition parsers despite encoding more complex features. Supertagging (Clark and Curran, 2007) and chart pruning (Zhang et al., 2010) have been used to constrain the search space of a CCG parser, and to remove unlikely or forbidden spans from repeated consideration. In our work, we use pruning not for parsing speed, but evaluation, and so we prune items based on goldstandard constraints rather than heuristics. 5 Evaluation We use the training (sections 2-21) and development (section 22) data from the OntoNotes 4.0 release of the Penn Treebank WSJ data (Marcus et al., 1993), as supplied by the SANCL 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012). OntoNotes annotates enriched NP structure compared to the Penn Treebank (Weischedel et al., 2011), meaning that determining NP attachments is less trivial. We changed all marker tokens in the corpus (e.g. -LRB- and -LCB-) to their equivalent unescaped punctuation marks to ensure correct evaluation. The corpus has been converted to basic Stanford dependencies using the Stanford Parser v2.0,2 and part-of-speech 2 tagged using MXPOST (Ratnaparkhi, 1996). A model trained on WSJ sections 2-21 was used to tag"
P15-1111,J11-1007,0,0.0201691,"or classes, and identify the tree operations that repair these error types, such as the insertion, deletion, or substitution of nodes in the parse tree. The error classes in a particular parser’s output are identified through a heuristic procedure that repeatedly applies the operation repairing the largest number of bracket errors. This approach differs from our methodology as it is a static post-process that assumes the parser would respond perfectly to each repair, when it is possible that the parser may not perform the repair in full, or even be incapable of constructing the repaired tree. McDonald and Nivre (2011) perform an indepth comparison of the graph-based MSTparser and transition-based MaltParser. However, MaltParser uses support vector machines to deterministically predict the next transition, rather than storing the most probable options in a beam like ZPar. Additionally, they do not focus on the cascading impact of errors, and instead concentrate on higher-level error classification (e.g. by POS tag, labels and dependency lengths) in lieu of examining how the parsers respond to forced corrections. Nivre et al. (2014) describe several uses for arclevel constraints in transition-based parsing."
P15-1111,E06-1011,0,0.205131,"h error class in dependency parsing without making assumptions about how the parser will behave. We define error classes based on dependency labels, and use the dependencies in each class as arc constraints specifying the correct head and label for particular words in each sentence. We adapt parsers to apply these constraints, whilst otherwise proceeding with decoding under their grammar and model. By evaluating performance with and without constraints, we can directly observe the cascading impact of each error class on each the parser. We implement our procedure for the graphbased MSTparser (McDonald and Pereira, 2006) and the transition-based ZPar (Zhang and Clark, 2011) using basic Stanford dependencies over the OntoNotes 4.0 release of the WSJ Penn Treebank data. Our results show that erroneously attaching NPs, PPs, modifiers, and punctuation have the largest overall impact on UAS. Of those, NPs and punctuation have the most substantial cascading impact, indicating that these errors have the most effect on the remainder of the parse. Enforcing correct punctuation arcs has a particularly large impact on accuracy, even though most evaluation scripts ignore punctuation. We find that punctuation arcs are com"
P15-1111,P12-1052,1,0.861712,"ied, e.g. imperatives at the beginning of a sentence are likely to be the root. We focus our constraints on evaluation, attempting to identify important sources of error in dependency parsers. Our constraint-based approach shares similarities to oracle training and decoding methods, where an external source of truth is used to verify parser decisions. An oracle source of parser actions is a necessary component for training transition-based parsers (Nivre, 2009). Oracle decoding, where a system is forced to produce correct output if possible, can be used to assess its upper performance bounds (Ng and Curran, 2012). Constraining the parser’s internal search space is akin to an optimal pruning operation. Charniak and Johnson (2005) use a coarse-to-fine, iterative pruning approach for efficiently generating high-quality n-best parses for a discriminative reranker. Rush and Petrov (2012) use a similar coarse-to-fine algorithm with vine grammars (Eisner and Smith, 2005) to accelerate graph-based de1151 root det amod punct conj cc punct pobj nn nsubj The LME stocks decline prep det advmod was about as nn expected , nsubj but the Comex gain neg was n’t . Figure 2: MSTparser output for the sentence in Figure 1"
P15-1111,P09-1040,0,0.05044,"el constraints in transition-based parsing. However, these applications focus on improving parsing accuracy when constraints can be readily identified, e.g. imperatives at the beginning of a sentence are likely to be the root. We focus our constraints on evaluation, attempting to identify important sources of error in dependency parsers. Our constraint-based approach shares similarities to oracle training and decoding methods, where an external source of truth is used to verify parser decisions. An oracle source of parser actions is a necessary component for training transition-based parsers (Nivre, 2009). Oracle decoding, where a system is forced to produce correct output if possible, can be used to assess its upper performance bounds (Ng and Curran, 2012). Constraining the parser’s internal search space is akin to an optimal pruning operation. Charniak and Johnson (2005) use a coarse-to-fine, iterative pruning approach for efficiently generating high-quality n-best parses for a discriminative reranker. Rush and Petrov (2012) use a similar coarse-to-fine algorithm with vine grammars (Eisner and Smith, 2005) to accelerate graph-based de1151 root det amod punct conj cc punct pobj nn nsubj The L"
P15-1111,J14-2001,0,0.0939495,"traints have no impact on the parser’s coverage as all possible head selections are considered. 3.2 ZPar implementation ZPar is an arc-eager transition-based parser (Zhang and Clark, 2011) that uses an incremental process with a stack storing partial parse states (Nivre et al., 2004). Each state represents tokens that may accept further arcs. The tokens of a sentence are initially stored in a buffer, and at each point during parsing, the parser decides whether or not to create an arc between the front token of the buffer and the top token on the stack. We apply constraints in a similar way to Nivre et al. (2014). Arc creation actions are factored on the dependency label to be assigned to the arc. ZPar scores each possible action using a perceptron model over features from the front of the buffer and the top of the stack (as well as some additional context features which refer to previously created states). The highest scoring actions and their resulting states are kept in a beam; during parsing, ZPar finds the optimal action for all items in the beam, and retains the highest scoring new states at each step. We disallow any arc creation action that would create an arc that conflicts with any constrain"
P15-1111,W04-2407,0,0.108046,"Missing"
P15-1111,W96-0213,0,0.5462,"ntoNotes 4.0 release of the Penn Treebank WSJ data (Marcus et al., 1993), as supplied by the SANCL 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012). OntoNotes annotates enriched NP structure compared to the Penn Treebank (Weischedel et al., 2011), meaning that determining NP attachments is less trivial. We changed all marker tokens in the corpus (e.g. -LRB- and -LCB-) to their equivalent unescaped punctuation marks to ensure correct evaluation. The corpus has been converted to basic Stanford dependencies using the Stanford Parser v2.0,2 and part-of-speech 2 tagged using MXPOST (Ratnaparkhi, 1996). A model trained on WSJ sections 2-21 was used to tag the development set, and 10-fold jackknife training was used to tag the training data. We implement a custom evaluation script to facilitate a straightforward comparative analysis between the unconstrained and constrained output. The script is based on and produces identical scores to eval06.pl, the official evaluation for the CoNLL-X Shared Task on Multilingual Dependency Parsing (Buchholz and Marsi, 2006). We ignore punctuation as defined by eval06.pl in our evaluation; experiments with constraints over punctuation tokens constrain those"
P15-1111,N12-1054,0,0.0130619,"g methods, where an external source of truth is used to verify parser decisions. An oracle source of parser actions is a necessary component for training transition-based parsers (Nivre, 2009). Oracle decoding, where a system is forced to produce correct output if possible, can be used to assess its upper performance bounds (Ng and Curran, 2012). Constraining the parser’s internal search space is akin to an optimal pruning operation. Charniak and Johnson (2005) use a coarse-to-fine, iterative pruning approach for efficiently generating high-quality n-best parses for a discriminative reranker. Rush and Petrov (2012) use a similar coarse-to-fine algorithm with vine grammars (Eisner and Smith, 2005) to accelerate graph-based de1151 root det amod punct conj cc punct pobj nn nsubj The LME stocks decline prep det advmod was about as nn expected , nsubj but the Comex gain neg was n’t . Figure 2: MSTparser output for the sentence in Figure 1, where the root dependency is forced to its correct value. The incorrect noun phrase error is not affected by the constraint (dashed, red), six attachment errors are repaired (solid, blue), and two new errors are introduced (dotted, purple). Constraints None nn root 3 8 9 1"
P15-1111,W13-3733,0,0.0208464,"d especially ZPar) stem from incorrectly identified coordination markers such as commas. Punctuation is commonly ignored in dependency parser evaluation (Yamada and Matsumoto, 2003; Buchholz and Marsi, 2006), and they are inconsistently treated across different grammars. Our results show that enforcing the correct punctuation attachments in a sentence has a substantial cascading impact, suggesting that punctuation errors are highly correlated with errors elsewhere in the analysis. Given the broad similarities between Stanford dependencies and other dependency schemes commonly used in parsing (Søgaard, 2013), we anticipate that the problems with roots and punctuation will carry across different treebanks and schemes. Punctuation is often placed at phrasal boundaries and serves to split sentences into smaller sec7 Conclusion We have developed a procedure to classify the importance of errors in dependency parsers without any assumptions on how the parser will respond to attachment repairs. Our approach constrains the parser to allow only correct arcs for certain tokens, whilst allowing it to otherwise form the parse that it thinks is best. Compared to Kummerfeld et al. (2012), we can observe exactl"
P15-1111,W03-3023,0,0.193888,"non-crossing arcs. Table 6 summarises the error classes of the repaired cascaded arcs when punctuation constraints are applied. MSTparser has a more even distribution of repairs, while ZPar’s repairs are concentrated in coordination attachment. This shows that MSTparser is relatively better at coordination as a proportion of its overall performance compared to ZPar. It also indicates that the majority of punctuation errors in both parsers (and especially ZPar) stem from incorrectly identified coordination markers such as commas. Punctuation is commonly ignored in dependency parser evaluation (Yamada and Matsumoto, 2003; Buchholz and Marsi, 2006), and they are inconsistently treated across different grammars. Our results show that enforcing the correct punctuation attachments in a sentence has a substantial cascading impact, suggesting that punctuation errors are highly correlated with errors elsewhere in the analysis. Given the broad similarities between Stanford dependencies and other dependency schemes commonly used in parsing (Søgaard, 2013), we anticipate that the problems with roots and punctuation will carry across different treebanks and schemes. Punctuation is often placed at phrasal boundaries and"
P15-1111,C10-2168,1,0.906225,"Missing"
P15-1111,W08-1301,0,\N,Missing
P15-1111,J11-1005,0,\N,Missing
P19-1510,W04-2705,0,\N,Missing
P19-1510,J93-2004,0,\N,Missing
P19-1510,sekine-etal-2002-extended,0,\N,Missing
P19-1510,D09-1015,0,\N,Missing
P19-1510,W07-1009,0,\N,Missing
P19-1510,W03-0419,0,\N,Missing
P19-1510,J96-2004,0,\N,Missing
P19-1510,P07-1031,1,\N,Missing
P19-1510,D15-1102,0,\N,Missing
P19-1510,N16-1030,0,\N,Missing
P19-1510,P17-1114,0,\N,Missing
P19-1510,D17-1276,0,\N,Missing
P19-1510,N18-1079,0,\N,Missing
P19-1510,N18-1131,0,\N,Missing
P19-1510,P18-3006,1,\N,Missing
P19-1510,D18-1124,0,\N,Missing
P19-1510,D18-1309,0,\N,Missing
P19-1510,D18-1019,0,\N,Missing
P19-1510,E12-2021,0,\N,Missing
S10-1069,P09-2014,1,0.894261,"Missing"
S10-1069,P03-1046,0,0.0328256,"07a) demonstrate the use of techniques like adaptive supertagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al., 2009). We use this parser for the PETE task. The performance of statistical parsers is largely a function of the quality of the corpora they are trained on. For this task, we used models derived from the CCGbank corpus – a transformation of the Penn Treebank (Marcus et al., 1993) including CCG derivations and dependencies (Hockenmaier, 2003a). It was created to further CCG research by providing a large corpus of appropriately annotated data, and has been shown to be suitable for the training of high-performance parsers (Hockenmaier, 2003b; Clark and Curran, 2004). Combinatory Categorial Grammar (CCG, Steedman, 2000) is a lexicalised grammar formalism based on combinatory logic. The grammar is directly encoded in the lexicon in the form of combinatory categories that govern how each word combines with its neighbours. The parsing process determines the most likely assignment of categories to words, and finds a sequence of combinat"
S10-1069,J07-3004,0,0.072655,"Missing"
S10-1069,W09-3306,1,0.897466,"Missing"
S10-1069,J93-2004,0,0.0353382,"formance parsers built on the CCG formalism. Clark and Curran (2007a) demonstrate the use of techniques like adaptive supertagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al., 2009). We use this parser for the PETE task. The performance of statistical parsers is largely a function of the quality of the corpora they are trained on. For this task, we used models derived from the CCGbank corpus – a transformation of the Penn Treebank (Marcus et al., 1993) including CCG derivations and dependencies (Hockenmaier, 2003a). It was created to further CCG research by providing a large corpus of appropriately annotated data, and has been shown to be suitable for the training of high-performance parsers (Hockenmaier, 2003b; Clark and Curran, 2004). Combinatory Categorial Grammar (CCG, Steedman, 2000) is a lexicalised grammar formalism based on combinatory logic. The grammar is directly encoded in the lexicon in the form of combinatory categories that govern how each word combines with its neighbours. The parsing process determines the most likely assig"
S10-1069,P04-1014,1,0.814347,"parsers built on different formalisms (Rimell et al., 2009). We use this parser for the PETE task. The performance of statistical parsers is largely a function of the quality of the corpora they are trained on. For this task, we used models derived from the CCGbank corpus – a transformation of the Penn Treebank (Marcus et al., 1993) including CCG derivations and dependencies (Hockenmaier, 2003a). It was created to further CCG research by providing a large corpus of appropriately annotated data, and has been shown to be suitable for the training of high-performance parsers (Hockenmaier, 2003b; Clark and Curran, 2004). Combinatory Categorial Grammar (CCG, Steedman, 2000) is a lexicalised grammar formalism based on combinatory logic. The grammar is directly encoded in the lexicon in the form of combinatory categories that govern how each word combines with its neighbours. The parsing process determines the most likely assignment of categories to words, and finds a sequence of combinators that allows them to form a sentence. A sample CCG derivation for a sentence from the test set is shown in Figure 1. The category for each word is indicated beneath it. It can be seen that some categories take other categori"
S10-1069,D09-1085,0,0.223878,"Missing"
S10-1069,J07-4004,1,0.9112,"Missing"
S10-1069,P07-1032,1,0.853662,"Evaluation, ACL 2010, pages 313–316, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Totals NP include only vehicle sales reported (SN P )/N P (SN P )(SN P ) N/N &lt;B× (SN P )/N P N N ⇒ NP SN P &gt; in period. ((SN P )(SN P ))/N P NP (SN P )(SN P ) SN P ⇒ N P N P NP SN P S &gt; &lt; &lt; &gt; &lt; Figure 1: An example CCG derivation, showing how the categories assigned to words are combined to form a sentence. The arrows indicate the direction of application. 2 Background Recent work has seen the development of highperformance parsers built on the CCG formalism. Clark and Curran (2007a) demonstrate the use of techniques like adaptive supertagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al., 2009). We use this parser for the PETE task. The performance of statistical parsers is largely a function of the quality of the corpora they are trained on. For this task, we used models derived from the CCGbank corpus – a transformation of the Penn Treebank (Marcus et al., 1993) including CCG derivations and dependencies (Ho"
S10-1069,S10-1009,0,0.126128,"Missing"
U05-1007,J96-1002,0,0.0102522,"rate additional sources of information from a larger unannotated corpus. 3 Maximum Entropy modelling A Maximum Entropy model is defined in terms of a number of constraints on the expected occurrences of features that represent the training data. Once these constraints on the model are met, the model assumes nothing further, giving a uniform distribution to all unknowns, that is, the model with maxi33 mum entropy (Ratnaparkhi, 1996). In this way, the model makes use of all the information available, but does not favour any further unfounded hypothesis, giving equal chance to all possibilities (Berger et al., 1996). The empirical expectation of these features, as observed in the training data, is calculated by: p˜(f ) ≡ X p˜(x, y)f (x, y) (1) x,y We attempt to make our model’s estimated value: p(f ) ≡ X p˜(x)p(y|x)f (x, y) (2) x,y an accurate reflection of the training data, so that, p(f ) = p˜(f ) (3) and therefore, X p˜(x)p(y|x)f (x, y) = X p˜(x, y)f (x, y) (4) x,y x,y The training algorithm we use to achieve this is Generalised Iterative Scaling (GIS) (Darroch and Ratcliff, 1972). Each iteration of the algorithm involves updating all λi as follows: p˜(f ) 1 (5) log (t) C p (f ) where C is the maximum"
U05-1007,E03-1071,1,0.949822,"ese reasons, it is quite important that unknown words are POS tagged correctly, so that the information carried by them can be extracted properly in future stages of an NLP system. Previous work on tagging unknown words has focused on morphological features, and using common affixes to better identify the correct tag. This has been done using manually created, common English endings (Weischedel et al., 1993), with Transformation Based Learning (TBL) (Brill, 1994), and by comparing pairs of words in a lexicon for differences in their beginnings and endings (Mikheev, 1997). Our existing tagger (Curran and Clark, 2003) already makes use of such features, while we aim to incorporate additional sources of information from a larger unannotated corpus. 3 Maximum Entropy modelling A Maximum Entropy model is defined in terms of a number of constraints on the expected occurrences of features that represent the training data. Once these constraints on the model are met, the model assumes nothing further, giving a uniform distribution to all unknowns, that is, the model with maxi33 mum entropy (Ratnaparkhi, 1996). In this way, the model makes use of all the information available, but does not favour any further unfo"
U05-1007,J93-2004,0,0.0277254,"Missing"
U05-1007,J97-3003,0,0.0461628,"Missing"
U05-1007,J93-2006,0,0.132985,"Missing"
U05-1009,J92-4003,0,0.0385608,"context vectors extracted from large corpora scales poorly. The Spatial Approximation Sample Hierarchy (SASH) is a data-structure for performing approximate nearest-neighbour queries, and has been previously used to improve the scalability of distributional similarity searches. We add lexical semantic information from WordNet to the SASH in an attempt to improve the accuracy and efficiency of similarity searches. 1 Introduction Lexical semantic resources and electronic thesauri are regularly used to solve NLP problems, including collocation discovery (Pearce, 2001), smoothing and estimation (Brown et al., 1992; Clark and Weir, 2001) and question answering (Pasca and Harabagiu, 2001). These use similarity relationships between words, as given in the resources, to enhance corpus-based statistics. It is difficult to account for the needs of the many domains in which NLP techniques are now being applied and for rapid change in language use. Manual creation is expensive and time consuming, and open to the problems of bias, inconsistency and limited coverage. The assisted or automatic creation and maintenance of these resources would be of great advantage. Much of the existing work on automatically extra"
U05-1009,N01-1013,0,0.172483,"racted from large corpora scales poorly. The Spatial Approximation Sample Hierarchy (SASH) is a data-structure for performing approximate nearest-neighbour queries, and has been previously used to improve the scalability of distributional similarity searches. We add lexical semantic information from WordNet to the SASH in an attempt to improve the accuracy and efficiency of similarity searches. 1 Introduction Lexical semantic resources and electronic thesauri are regularly used to solve NLP problems, including collocation discovery (Pearce, 2001), smoothing and estimation (Brown et al., 1992; Clark and Weir, 2001) and question answering (Pasca and Harabagiu, 2001). These use similarity relationships between words, as given in the resources, to enhance corpus-based statistics. It is difficult to account for the needs of the many domains in which NLP techniques are now being applied and for rapid change in language use. Manual creation is expensive and time consuming, and open to the problems of bias, inconsistency and limited coverage. The assisted or automatic creation and maintenance of these resources would be of great advantage. Much of the existing work on automatically extracting lexical semantic"
U05-1009,P02-1030,1,0.860642,"ibed by collating information about their contexts in a corpus into a vector. These context vectors are then compared for similarity. Existing approaches differ primarily in their definition of “context”, e.g. the surrounding words or the entire document, and their choice of distance metric for calculating similarity between the context vectors representing each term. Finding synonyms using distributional similarity requires a nearest-neighbour search over the context vectors of each term. This is computationally intensive, scaling to the number of terms and the size of their context vectors. Curran and Moens (2002) have demonstrated that dramatically increasing the volume of raw input text used to extract context information significantly improves the quality of extracted synonyms. This will increase the size of the vocabulary, decreasing the efficiency of a na¨ıve nearest-neighbour approach. Using a data-structure such as the Spatial Approximation Sample Hierarchy (SASH; Houle and  Sakuma, 2005) allows  us to reduce the original  complexity (for an term vocabulary) to (Gorman and Curran, 2005). The SASH represents the distributional space as a hierarchical directed graph in which each node is"
U05-1009,W05-1011,1,0.796405,"s is computationally intensive, scaling to the number of terms and the size of their context vectors. Curran and Moens (2002) have demonstrated that dramatically increasing the volume of raw input text used to extract context information significantly improves the quality of extracted synonyms. This will increase the size of the vocabulary, decreasing the efficiency of a na¨ıve nearest-neighbour approach. Using a data-structure such as the Spatial Approximation Sample Hierarchy (SASH; Houle and  Sakuma, 2005) allows  us to reduce the original  complexity (for an term vocabulary) to (Gorman and Curran, 2005). The SASH represents the distributional space as a hierarchical directed graph in which each node is connected to several near-neighbour children, deriving its structure from the distribution of the space it represents. The SASH is searched by traversing these edges. WordNet (Fellbaum, 1998) is an electronic lexical database. The main unit of organisation within WordNet is the synset, which is a collection of synonymous words. In the case of nouns, there is a secondary organisation based on hyponymy. The structure of WordNet was derived from a model of how humans understand language. WordNet"
U05-1009,O97-1002,0,0.0437934,"Missing"
U05-1009,W00-1427,0,0.0146212,"lculations between the statistical descriptions of their contexts. 2.1 Extraction Method      A context relation is defined as a tuple where is a term, which occurs in some grammat in some senical relation with another word     tence. We refer to the tuple as an attribute of . For example, (dog, direct-obj, walk) indicates that dog was the direct object of walk in a sentence. Context extraction begins with a Maximum Entropy POS tagger and chunker (Ratnaparkhi, 1996). The S EXTANT relation extractor (Grefenstette, 1994) produces context relations that are then lemmatised using the Minnen et al. (2000) morphological analyser. The relations for each term are collected together and counted, producing a vector of attributes and their frequencies in the corpus. The syntactic contexts that are extracted by S EX TANT are: 1. term is the subject of a verb 2. term is the (direct/indirect) object of a verb 3. term is modified by a noun or adjective 4. term is modified by a prepositional phrase 2.2 Measures and Weights Both nearest-neighbour and cluster analysis methods require a distance measure to calculate the similarity between context vectors. Curran (2004) decomposes this into measure and weigh"
U05-1024,E03-1071,1,0.749012,"ction system developed by Curran (2004). It is based on the distributional hypothesis that similar words appear in similar contexts. The system extracts one-word noun synonyms (i.e. not multi-word expressions). The extraction process is divided into two main parts. First, all target noun contexts are represented as relations and compiled into one context vector for each noun. Second, a comparison between all context vectors is made to identify the closest (i.e. most similar) terms. Contexts are extracted from raw sentences using a maximum entropy pos tagger, chunker, and a relation extractor (Curran and Clark, 2003). Six different types of relationship are identified: • • • • • • Between a noun and a modifying adjective. Between a noun and a noun modifier. Between a verb and a subject. Between a verb and a direct object. Between a verb and an indirect object. Between a noun and the head of a modifying prepositional phrase. The nouns in each case (including the subjects and objects) are the target headword. All context relations for a particular headword are aggregated into the headword’s context vector. Words are identified as synonyms on the basis of the number of context vectors they have in common. 7."
U05-1024,halacsy-etal-2004-creating,0,0.0260802,"., 1994). The British National Corpus (bnc) is a collection of British English, consisting of 90 million words of written text and 10 million words of transcribed speech (Burnard, 2000). At almost one hundred times the size of the Brown Corpus and more than twenty times the size of the Penn Treebank, it is too large to be manually annotated and so the bnc is automatically tagged with pos tags. While languages such as English are rich with language resources, minority languages often resort to using freely available web text. One of the first web-collected corpora was the Hungarian Web Corpus (Halacsy et al., 2004), created by downloading pages from the .hu domain. It has about 1 billion words of text after removal of duplicates and non-Hungarian text. 2.2 The Gigaword Corpus The English Gigaword Corpus consists of over 4 million documents and 1.75 billion words (Graff, 2003), with more than 2 billion tokens when the text is tokenised, including punctuation. It is the next progression up in size from the bnc. The Gigaword is the large single collection of English news text available to-date. It consists of newspaper text from the Associated Press, Agence France Press (English Service), the New York Time"
U05-1024,J93-2004,0,\N,Missing
U05-1027,C04-1180,1,0.815811,"Missing"
U05-1027,P04-1014,1,0.86042,"formalism to easily capture long-range dependencies. This is particularly important for our system, as the constructions used to describe programming instructions often contain non-standard constituents such as extraction, relativization, and coordination. These possiblities result in a large number of 194 interpretations, as a single word can be assigned a different category depending on how it is used, and the words that surround it. However, the application of statistical parsing techniques for CCG have shown that it is capable of performing widecoverage parsing at state-of-the-art levels (Clark and Curran, 2004). 5 English Code Corpus In order to investigate the way that people would use English to describe a programming task, we ellicited responses from programmers, asking them to describe how they would solve sample tasks. These tasks included finding the smallest number in a list, splitting a string on a character and finding all primes less than 100. The respondents were all experienced programmers, since computer science staff were all that were easily available. As a result of this, they tended to impose typical programming constructs on what they wanted to do, rather than using a simpler Engli"
U05-1027,W04-3215,1,0.825359,"y the original model. We can see that Initialise not been identified as a verb, but is instead tagged as a proper noun. min is also misclassified as a verb, when it is the noun. This highlights the fact that the parser does not expect the first word in a sentence to be a verb. We could not use this parse and expect to perform adequately in the following stages of the system. For this reason we created and annotated the English code corpus, in order to provide training data and allow us to build a new, and better performing model. A similar process had been followed in Question Answering (QA) (Clark et al., 2004), because questions also show quite different syntactic properties to newspaper text. This technique produced a significant improvement for QA, and so we have reused this idea. Following Clark et al., we used multiples of the English corpus, as it is quite small in comparison to the entire WSJ. These results are shown in Figure 8, for training with just the WSJ (original), with the WSJ and the English code corpus (1x code), and with the WSJ and multiples of the English corpus (5x, 10x, 20x). We show results for POS tagging and supertagging, on a word-by-word basis, and also the proportion of w"
U05-1027,H89-2010,0,0.0925619,", and answering questions entered by the user. It also uses discourse in order to better interpret sentences from the user. There were also a handful of systems that attempted to build a system similar to what we describe in this paper (Heidorn, 1976; Biermann et al., 1983). Most of these used a restricted syntax, or defined a specific domain over which they could be used. Our system should have much greater coverage, and be able to interpret most instructions from the user in some way. More generally, we can look at a system that interprets natural language utterances about planetary bodies (Frost and Launchbury, 1989). This system processes queries about its knowledge base, but is restricted to sentences that are covered by its vocabulary and grammar. It deals with ambiguous questions by providing answers to each possible reading, even when those readings would be easily dismissed by humans. With our system, we will determine the most likely reading, and process the sentence accordingly. 193 Another point of difference comes in the way people use logical connectives such as AND and OR, which are not neccesarily meant in the strictly logical way that is the case when using a programming language. There are"
U05-1027,J82-2002,0,0.680751,"Missing"
U05-1027,H89-1033,0,0.0955315,"es the highest paid female manager have any degrees from Harvard? Yes, 1. &gt; How about MIT? No, none. Figure 2: An example of anaphora and an elliptical sentence Understanding Natural Language • AGGREGATE sum up all the values in the list • I TERATION start the sum at 0 tem. This would also allow us to resolve much of the ambiguity involved in natural language by asking the user which possibility they actually meant. 3.2 for each in value in the list add this value to the sum Figure 3: Finding the sum of the values in a list Early Systems One of the first natural language interfaces is SHRDLU (Winograd, 1972), which allows users to interact with a number of objects in what was called Blocksworld. This system is capable of discriminating between objects, fulfilling goals, and answering questions entered by the user. It also uses discourse in order to better interpret sentences from the user. There were also a handful of systems that attempted to build a system similar to what we describe in this paper (Heidorn, 1976; Biermann et al., 1983). Most of these used a restricted syntax, or defined a specific domain over which they could be used. Our system should have much greater coverage, and be able to"
U05-1029,W04-3202,0,0.0413195,"Missing"
U05-1029,J99-2004,0,0.0361423,"Steedman (2000). The combinatory rules used by the parser are functional application (forward and backward), generalised forward composition, backward composition, generalised backward-crossed composition, and type raising. There is also a coordination rule which conjoins categories of the same type. 3.2 CCG Supertagging Lexicalised grammar formalisms, such as ltag and ccg, assign one or more syntactic structures to each word which are then manipulated by the parser. Supertagging was introduced for ltag to increase parsing efficiency by reducing the number of structures assigned to each word (Bangalore and Joshi, 1999). The parser model parameters are estimated using a discriminative method, that is, one which requires statistics across all incorrect parses for a sentence as well as the correct parse. Since an automatically extracted ccg grammar can produce an extremely large number of parses, the use of a supertagger is crucial in limiting the total number of parses for the training data to a computationally manageable number. The supertagger is also crucial for increasing the speed of the parser. We have shown that spectacular increases in speed can be obtained, without affecting accuracy or coverage, by"
U05-1029,W97-1502,0,0.0706899,"Missing"
U05-1029,C04-1041,1,0.895399,"NP ) or complex functor categories (e.g. (S [dcl]NP )/NP a transitive declarative verb). An example sentence with lexical categories is shown in Figure 1. These categories are combined together according to a small number of combinatory rules. The set of these lexical categories is obtained from CCGbank (Hockenmaier and Steedman, 2002; Hockenmaier, 2003), a corpus of ccg normal-form derivations derived semiautomatically from the Penn Treebank. The category set consists of those category types which occur at least 10 times in sections 2-21 of CCGbank, which results in a set of 409 categories. Clark and Curran (2004a) demonstrates that this relatively small set has high coverage on unseen data and can be used to create a robust and accurate parser. In order to obtain semantic representations for a particular formalism, only 409 categories have to be annotated. 3.1 CCG Parsing In our system we are using the c&c ccg parser (Clark and Curran, 2004b), which uses a loglinear model over normal-form derivations to select an analysis. The parser takes a pos tagged sentence as input with a set of one ore more categories assigned to each word. A ccg supertagger (Clark and Curran, 2004a) assigns the lexical categor"
U05-1029,P04-1014,1,0.89309,"NP ) or complex functor categories (e.g. (S [dcl]NP )/NP a transitive declarative verb). An example sentence with lexical categories is shown in Figure 1. These categories are combined together according to a small number of combinatory rules. The set of these lexical categories is obtained from CCGbank (Hockenmaier and Steedman, 2002; Hockenmaier, 2003), a corpus of ccg normal-form derivations derived semiautomatically from the Penn Treebank. The category set consists of those category types which occur at least 10 times in sections 2-21 of CCGbank, which results in a set of 409 categories. Clark and Curran (2004a) demonstrates that this relatively small set has high coverage on unseen data and can be used to create a robust and accurate parser. In order to obtain semantic representations for a particular formalism, only 409 categories have to be annotated. 3.1 CCG Parsing In our system we are using the c&c ccg parser (Clark and Curran, 2004b), which uses a loglinear model over normal-form derivations to select an analysis. The parser takes a pos tagged sentence as input with a set of one ore more categories assigned to each word. A ccg supertagger (Clark and Curran, 2004a) assigns the lexical categor"
U05-1029,W04-3215,1,0.903528,"mplex functor categories (e.g. (S [dcl]NP )/NP a transitive declarative verb). An example sentence with lexical categories is shown in Figure 1. These categories are combined together according to a small number of combinatory rules. The set of these lexical categories is obtained from CCGbank (Hockenmaier and Steedman, 2002; Hockenmaier, 2003), a corpus of ccg normal-form derivations derived semiautomatically from the Penn Treebank. The category set consists of those category types which occur at least 10 times in sections 2-21 of CCGbank, which results in a set of 409 categories. Clark and Curran (2004a) demonstrates that this relatively small set has high coverage on unseen data and can be used to create a robust and accurate parser. In order to obtain semantic representations for a particular formalism, only 409 categories have to be annotated. 3.1 CCG Parsing In our system we are using the c&c ccg parser (Clark and Curran, 2004b), which uses a loglinear model over normal-form derivations to select an analysis. The parser takes a pos tagged sentence as input with a set of one ore more categories assigned to each word. A ccg supertagger (Clark and Curran, 2004a) assigns the lexical categor"
U05-1029,W03-0806,1,0.845345,"introduced in Day et al. (1997), mixed initiative annotation (where the division of labour between computational facilities and human effort is coordinated for increased efficiency) has become an increasingly common methodology for the preparation of large corpora. Typically however, mixed initiative approaches have largely decoupled human and machine effort, even for larger scale tasks. Extending the mixed initiative model specifically to a distributed environment, Hughes and Bird (2003) offer a model for the type of solution we implement here. Additionally, the ar208 chitecture advocated by Curran (2003) allows us flexibility in designing individual components of this system independently, and then marshalling them into a single application instance. Experiments with distributed NLP tasks of building n-gram language models (Hughes et al., 2004a) and generalised textual indexing and linguistically motivated retrieval (Hughes et al., 2004b) are broadly indicative of other work in this area. To date, however we are not aware of any work in this vein specifically involving mixed initiative annotation. 3 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2000) is a type-drive"
U05-1029,A97-1051,0,0.0946618,"Missing"
U05-1029,W01-0521,0,0.0280961,"sult there are only a small number of such corpora, including the Penn Treebank (Marcus et al., 1994), the German TiGer Corpus (Skut et al., 1997) and more recently the LinGO Redwoods Treebank (Oepen et al., 2002). These corpora are also limited in size, typically around one million words. Unfortunately, the statistical approaches to parsing which have been most successful rely heavily on both the quality and quantity of annotated resources. Also, these approaches are very sensitive to the statistical properties of the corpus, and so a parser trained on one genre may perform badly on another (Gildea, 2001). Another major problem with parsed corpora is that they must, at least to some extent, follow a particular syntactic theory or formalism. This is a major difficulty for two reasons: firstly, it means we need separate annotated corpora for each formalism; and secondly, it means that comparing parser evaluations across formalisms is difficult. Fully automated conversion of parse trees between formalisms is difficult because each analyses certain constructs in idiosyncratic ways. An example is CCGbank (Hockenmaier and Steedman, 2002), a treebank of Combinatory Categorial Grammar (Steedman, 2000)"
U05-1029,hockenmaier-steedman-2002-acquiring,0,0.121092,"f the corpus, and so a parser trained on one genre may perform badly on another (Gildea, 2001). Another major problem with parsed corpora is that they must, at least to some extent, follow a particular syntactic theory or formalism. This is a major difficulty for two reasons: firstly, it means we need separate annotated corpora for each formalism; and secondly, it means that comparing parser evaluations across formalisms is difficult. Fully automated conversion of parse trees between formalisms is difficult because each analyses certain constructs in idiosyncratic ways. An example is CCGbank (Hockenmaier and Steedman, 2002), a treebank of Combinatory Categorial Grammar (Steedman, 2000) derivations which were converted semi-automatically from the Penn Treebank trees. The result still required laborious editing to produce idiomatic ccg derivations (Hockenmaier, 2003). We intend to create a new corpus of ccg derivations on a wide range of text. We face three key problems: 1) selecting sentence to annotate which creates the most useful corpus for statistical parsers. 2) maximising the annotator efficiency and minimising error; 3) allowing distributed annotators to share expertise. The selection problem is addressed"
U05-1029,W03-0805,1,0.854797,"Missing"
U05-1029,C02-2025,0,0.101936,"annotator are selected by an active learning framework to maximise the value of the annotated corpus for machine learners. We report on an initial implementation based on a distributed workflow architecture. 1 Introduction Annotating sentences with parse trees is perhaps the most complex and intensive linguistic annotation. The time and expense of developing parsed corpora is almost prohibitive. As a result there are only a small number of such corpora, including the Penn Treebank (Marcus et al., 1994), the German TiGer Corpus (Skut et al., 1997) and more recently the LinGO Redwoods Treebank (Oepen et al., 2002). These corpora are also limited in size, typically around one million words. Unfortunately, the statistical approaches to parsing which have been most successful rely heavily on both the quality and quantity of annotated resources. Also, these approaches are very sensitive to the statistical properties of the corpus, and so a parser trained on one genre may perform badly on another (Gildea, 2001). Another major problem with parsed corpora is that they must, at least to some extent, follow a particular syntactic theory or formalism. This is a major difficulty for two reasons: firstly, it means"
U05-1029,A97-1014,0,0.0462801,"servers through an annotation gui. The examples presented to each annotator are selected by an active learning framework to maximise the value of the annotated corpus for machine learners. We report on an initial implementation based on a distributed workflow architecture. 1 Introduction Annotating sentences with parse trees is perhaps the most complex and intensive linguistic annotation. The time and expense of developing parsed corpora is almost prohibitive. As a result there are only a small number of such corpora, including the Penn Treebank (Marcus et al., 1994), the German TiGer Corpus (Skut et al., 1997) and more recently the LinGO Redwoods Treebank (Oepen et al., 2002). These corpora are also limited in size, typically around one million words. Unfortunately, the statistical approaches to parsing which have been most successful rely heavily on both the quality and quantity of annotated resources. Also, these approaches are very sensitive to the statistical properties of the corpus, and so a parser trained on one genre may perform badly on another (Gildea, 2001). Another major problem with parsed corpora is that they must, at least to some extent, follow a particular syntactic theory or forma"
U05-1029,P02-1016,0,0.0818517,"Missing"
U05-1029,J93-2004,0,\N,Missing
U06-1003,P81-1022,0,0.686341,"dimensional array indexed by pos and span. The valid pairs correspond to pos + span ≤ n, that is, to spans that do not extend beyond the end of the sentence. The squares represent valid cells in Figure 2. The location of cell(3, 4) is marked with a diamond. cell(3, 4) stores the derivations whose yield is the four word sequence indicated. The CKY (also called CYK or Cocke-YoungerKasami) algorithm used in the C&C parser has an O(n3 ) worst case time complexity. Sikkel and Nijholt (1997) give a formal description of CKY (Younger, 1967) and similar parsing algorithms, such as the Earley parser (Earley, 1970). The CKY algorithm is a bottom up algorithm and works by combining adjacent words to give a span of size two (second row from the bottom in Figure 2). It then combines adjacent spans in the first two rows to create all allowable spans of size three in the row above. This process is continued until a phrase that spans the whole sentence (top row) is reached. The (lexical) categories in the bottom row (on the lexical items themselves) are assigned by the supertagger (Clark and Curran, 2004). The number of categories assigned to each word can be varied dynamically. Assigning a small number of ca"
U06-1003,N03-1016,0,0.221418,"Missing"
U06-1003,J99-2004,0,0.276512,"Missing"
U06-1003,P90-1001,0,0.113273,"p (ALTW2006), pages 3–10. 3 The cat ate the hat that I NP /N N (S NP )/NP NP /N N (NP NP )/(S /NP ) NP > NP > made (S NP )/NP NP >T S /(S NP ) S /NP NP NP S > < NP S NP >B > < Figure 1: A Combinatory Categorial Grammar derivation 2 Combinatory Categorial Grammar Context free grammars (CFGs) have traditionally been used for parsing natural language. However, some constructs in natural language require more expressive grammars. Mildly context sensitive grammars, e.g. HPSG and TAG, are powerful enough to describe natural language but like CFGs (Younger, 1967) are polynomial time parseable (Vijay-Shanker and Weir, 1990). Combinatory Categorial Grammar (CCG) is another mildy context sensitive grammar (Steedman, 2000) that has significant advantages over CFGs, especially for analysing constructions involving coordination and long range dependencies. Consider the following sentence: Give a teacher an apple and a policeman a flower. There are local dependencies between give and teacher, and between give and apple. The additional dependencies between give, and policeman and flower are long range, but are extracted easily using CCG. a policeman a flower is a non-standard constituent that CCG deals with very elegan"
U06-1003,J96-1002,0,0.00368183,"r. 3 C&C CCG Parser The C&C parser takes one or more syntactic structures (categories) assigned to each word and attempts to build a spanning analysis of the sentence. Typically every category that a word was seen with in the training data is assigned. Supertagging (Bangalore and Joshi, 1999) was introduced for Lexicalized Tree Adjoining Grammar (LTAG) as a way of assigning fewer categories to each word thus reducing the search space of the parser and improving parser efficiency. Clark (2002) introduced supertagging for CCG parsing. The supertagger used in the C&C parser is a maximum entropy (Berger et al., 1996) sequence tagger that uses words and part of speech (POS) tags in a five word window as features. The label set consists of approximately 500 categories (or supertags) so the task is significantly harder than other NLP sequence tagging tasks. The supertagger assigns one or more possible categories to each word together with the probability for each of the guesses. Clark and Curran (2004) discovered that initially assigning a very small number of categories per word and then attempting to parse was not only faster but more accurate than assigning many categories. If no spanning analysis could b"
U06-1003,C04-1041,1,0.946253,"gories to each word thus reducing the search space of the parser and improving parser efficiency. Clark (2002) introduced supertagging for CCG parsing. The supertagger used in the C&C parser is a maximum entropy (Berger et al., 1996) sequence tagger that uses words and part of speech (POS) tags in a five word window as features. The label set consists of approximately 500 categories (or supertags) so the task is significantly harder than other NLP sequence tagging tasks. The supertagger assigns one or more possible categories to each word together with the probability for each of the guesses. Clark and Curran (2004) discovered that initially assigning a very small number of categories per word and then attempting to parse was not only faster but more accurate than assigning many categories. If no spanning analysis could be found the parser requested more categories from the supertagger, and the parsing process was repeated until the number of attempts exceeded a limit (typically 5 levels of supertagger ambiguity). This tight integration of the supertagger and the parser resulted in state of the art accuracy and a massive improvement in efficiency, reaching up to 25 sentences a second. Up until now, when"
U06-1003,J07-3004,0,\N,Missing
U06-1010,W05-0619,0,0.0427385,"Missing"
U06-1010,W03-0424,1,0.874757,"4: Baseline contextual predicates 6 Contextual predicate wi contains period/punctuation wi is only digits wi is a number wi is {upper,lower,title,mixed} case wi is alphanumeric length of wi wi has only Roman numerals wi is an initial (x.) wi is an acronym (abc, a.b.c.) memory ne tag for wi unigram tag of wi+1 , wi+2 wi , wi−1 or wi+1 in a gazetteer wi not lowercase and flc > fuc uni-, bi- and tri-grams of word type Maximum Entropy Tagger The purpose of creating this annotated corpus is to develop a named entity tagger for astronomy literature. In these experiments we adapt the C&C ne tagger (Curran and Clark, 2003) to astronomy literature by investigating which feature types improve the performance of the tagger. However, as we shall see below, the tagger can also be used to test and improve the quality of the annotation. It can also be used to speed up the annotation process by pre-annotating sentences with their most likely tag. We were also interested to see whether > 40 named-entity categories could be distinguished successfully with this quantity of data. 63 The C&C ne tagger feature types are shown in Tables 4 and 5. The feature types in Table 4 are the same as used in MXPost (Ratnaparkhi, 1996) w"
U06-1010,A97-1004,0,0.126601,"the corresponding Unicode character is difficult. For example, acirc, ^{a} and hat{a} are all used to produce ˆa, which in Unicode is 0x0174. Several systems attempt to convert LATEX to other formats e.g. xml (Grimm, 2003). No existing system rendered the mathematics faithfully enough or with high enough coverage for our purposes. Currently our coverage of mathematics is very good but there are still some expressions that cannot be translated, e.g. complex nested expressions, rare symbols and non-Latin/Greek/Hebrew alphabetic characters. 4.4 Sentences and Tokenisation We used MXTerminator (Reynar and Ratnaparkhi, 1997) as the sentence boundary detector with an additional Python script to fix common errors, e.g. mistaken boundaries on Sect. and et al. We used the Penn Treebank (Marcus et al., 1994) sed script to tokenize the text, again with a Python script to fix common errors, e.g. splitting numbers like 1,000 on the Our FUSE|tel spectrum of HD|sta 73882|sta is derived from time-tagged observations over the course of 8 orbits on 1999|dat Oct|dat 30|dat . Several “ burst ” events occurred during the observation ( Sahnow|per et al. 2000|dat ) . We excluded all photon|part events that occurred during the burs"
U06-1010,J93-2004,0,\N,Missing
U06-1010,E03-1068,0,\N,Missing
U06-1010,P02-1062,0,\N,Missing
U07-1011,C92-2082,0,0.113869,"two novel lexical-semantic extraction tasks: extracting bigram named entities and WordNet lexical ﬁle classes (Fellbaum, 1998) from the Google Web 1T 5-grams. 1 Introduction Extracting lexical semantic resources from text with minimal supervision is critical to overcoming the knowledge bottleneck in Natural Language Processing (NLP) tasks ranging from Word Sense Disambiguation to Question Answering. Template-based extraction is attractive because it is reasonably efﬁcient, works on small and large datasets, and requires minimal linguistic preprocessing, making it fairly language independent. Hearst (1992) proposed template-based extraction for identifying hyponyms using templates like X, Y, and/or other Z where X and Y are hyponyms of Z. Riloff and Shepherd (1997) proposed iterative bootstrapping where frequent neighbours to terms from a given semantic class are extracted in multiple bootstrap iterations. Roark and Charniak (1998) 66 improved its accuracy by optimising the bootstrapping parameters. In mutual bootstrapping (Riloff and Jones, 1999) the terms, and the contexts they occur in, are extracted. Similar approaches have been used in Information Extraction (IE) for identifying company he"
U07-1011,W97-0313,0,0.111309,"T 5-grams. 1 Introduction Extracting lexical semantic resources from text with minimal supervision is critical to overcoming the knowledge bottleneck in Natural Language Processing (NLP) tasks ranging from Word Sense Disambiguation to Question Answering. Template-based extraction is attractive because it is reasonably efﬁcient, works on small and large datasets, and requires minimal linguistic preprocessing, making it fairly language independent. Hearst (1992) proposed template-based extraction for identifying hyponyms using templates like X, Y, and/or other Z where X and Y are hyponyms of Z. Riloff and Shepherd (1997) proposed iterative bootstrapping where frequent neighbours to terms from a given semantic class are extracted in multiple bootstrap iterations. Roark and Charniak (1998) 66 improved its accuracy by optimising the bootstrapping parameters. In mutual bootstrapping (Riloff and Jones, 1999) the terms, and the contexts they occur in, are extracted. Similar approaches have been used in Information Extraction (IE) for identifying company headquarters (Agichtein et al., 2000) and acronym expansions (Sundaresan and Yi, 2000). In Mutual Exclusion Bootstrapping (MEB), we assume the semantic classes part"
U07-1011,P98-2182,0,0.019275,"rocessing (NLP) tasks ranging from Word Sense Disambiguation to Question Answering. Template-based extraction is attractive because it is reasonably efﬁcient, works on small and large datasets, and requires minimal linguistic preprocessing, making it fairly language independent. Hearst (1992) proposed template-based extraction for identifying hyponyms using templates like X, Y, and/or other Z where X and Y are hyponyms of Z. Riloff and Shepherd (1997) proposed iterative bootstrapping where frequent neighbours to terms from a given semantic class are extracted in multiple bootstrap iterations. Roark and Charniak (1998) 66 improved its accuracy by optimising the bootstrapping parameters. In mutual bootstrapping (Riloff and Jones, 1999) the terms, and the contexts they occur in, are extracted. Similar approaches have been used in Information Extraction (IE) for identifying company headquarters (Agichtein et al., 2000) and acronym expansions (Sundaresan and Yi, 2000). In Mutual Exclusion Bootstrapping (MEB), we assume the semantic classes partition terms into disjoint sets, that is, the classes are mutually exclusive (Curran et al., 2007). Each class is extracted in parallel using separate bootstrapping loops"
U07-1011,C98-2177,0,\N,Missing
U07-1016,J98-4004,0,\N,Missing
U07-1016,J93-2004,0,\N,Missing
U07-1016,P97-1003,0,\N,Missing
U07-1016,N03-1022,0,\N,Missing
U07-1016,J03-4003,0,\N,Missing
U07-1016,D07-1066,0,\N,Missing
U07-1016,P01-1044,0,\N,Missing
U07-1016,P03-1029,0,\N,Missing
U07-1016,P07-1031,1,\N,Missing
U07-1021,W02-2001,0,0.0303924,"2, also improved. The next set of experiments extracted synonyms for 300 bigram headwords drawn from the M AC QUARIE thesaurus. The best results for bigram headwords was achieved when unigram and bigram data UNI UNI + BI UNI + BI + VPC 3UNI 4UNI 3UNI+ 4BI 4UNI+ 5BI 3UNI+ 4BI+ 4VPC 4UNI+ 5BI+ 5VPC 3UNI+ 4BI 4UNI+ 5BI 3UNI+ 4BI+ 4VPC 4UNI+ 5BI+ 5VPC ATOMIC BOMB DINING TABLE nuclear bomb atom bomb nuclear explosion atomic explosion nuclear weapon coffee table dining room cocktail table dining chair bedroom furniture Table 3: Sample bigram synonyms was extracted from W EB 1T and the VPC resource (Baldwin and Villavicencio, 2002) was included. Table 3 shows the top 5 synonyms (as ranked by the Jaccard measure) for atomic bomb and dining table. 6 Conclusion We have integrated the identification of simple multi-word expressions (MWEs) with a state-of-theart distributional similarity system. We evaluated extracted synonyms for both unigram and bigram headwords against a gold standard consisting of the union of multiple thesauri. The main difficulties are the sparsity of distributional evidence for MWEs and their low coverage in the gold standard. These preliminary experiments show the potential of distributional similari"
U07-1021,briscoe-carroll-2002-robust,0,0.0232918,"gold-standard thesauri and observe a slight decrease in overall performance when the bigram MWEs were included. This is unsurprising since the larger vocabulary and sparser contextual information for bigrams makes 146 Background Distributional similarity relies on the distributional hypothesis that similar terms appear in similar contexts (Harris, 1954). Here we extend the SEXTANT parser (Grefenstette, 1994) to include multi-word terms and syntactic contexts. Curran (2004) experiments with different parsers for extracting contextual information, including SEXTANT , MINIPAR (Lin, 1994), RASP (Briscoe and Carroll, 2002), and CASS (Abney, 1996). Lin (1998) used MINIPAR and Weeds (2003) used RASP for distributional similarity calculations. MINIPAR is the only parser to identify a range of MWEs that has been used for distributional similarity. Weeds (2003) and Curran (2004) evaluate measures for calculating distributional similarity. We follow (Curran, 2004) in using the weighted Jaccard measure with truncated t-test relation weighting for our experiments. 3 Detecting MWEs The initial step in creating a thesaurus for MWEs is to identify potential MWE headwords using collocation statistics. We used various stati"
U07-1021,C94-1079,0,0.0274282,"te against several gold-standard thesauri and observe a slight decrease in overall performance when the bigram MWEs were included. This is unsurprising since the larger vocabulary and sparser contextual information for bigrams makes 146 Background Distributional similarity relies on the distributional hypothesis that similar terms appear in similar contexts (Harris, 1954). Here we extend the SEXTANT parser (Grefenstette, 1994) to include multi-word terms and syntactic contexts. Curran (2004) experiments with different parsers for extracting contextual information, including SEXTANT , MINIPAR (Lin, 1994), RASP (Briscoe and Carroll, 2002), and CASS (Abney, 1996). Lin (1998) used MINIPAR and Weeds (2003) used RASP for distributional similarity calculations. MINIPAR is the only parser to identify a range of MWEs that has been used for distributional similarity. Weeds (2003) and Curran (2004) evaluate measures for calculating distributional similarity. We follow (Curran, 2004) in using the weighted Jaccard measure with truncated t-test relation weighting for our experiments. 3 Detecting MWEs The initial step in creating a thesaurus for MWEs is to identify potential MWE headwords using collocation"
U07-1021,P98-2127,0,0.0743697,"e in overall performance when the bigram MWEs were included. This is unsurprising since the larger vocabulary and sparser contextual information for bigrams makes 146 Background Distributional similarity relies on the distributional hypothesis that similar terms appear in similar contexts (Harris, 1954). Here we extend the SEXTANT parser (Grefenstette, 1994) to include multi-word terms and syntactic contexts. Curran (2004) experiments with different parsers for extracting contextual information, including SEXTANT , MINIPAR (Lin, 1994), RASP (Briscoe and Carroll, 2002), and CASS (Abney, 1996). Lin (1998) used MINIPAR and Weeds (2003) used RASP for distributional similarity calculations. MINIPAR is the only parser to identify a range of MWEs that has been used for distributional similarity. Weeds (2003) and Curran (2004) evaluate measures for calculating distributional similarity. We follow (Curran, 2004) in using the weighted Jaccard measure with truncated t-test relation weighting for our experiments. 3 Detecting MWEs The initial step in creating a thesaurus for MWEs is to identify potential MWE headwords using collocation statistics. We used various statistical tests, e.g. the t-test and th"
U07-1021,C98-2122,0,\N,Missing
U07-1022,J07-3004,0,0.0605065,"Missing"
U07-1022,J93-2004,0,\N,Missing
U08-1002,breck-etal-2000-evaluate,0,0.0231716,"from the text which are considered likely to contain answers. Short phrases which match the expect answer type of the question are selected in the answer extraction stage, these answers are then ranked according to how well they match or answer the original question. 2.1 QA Evaluation The methods for evaluating and analysing QA systems can be divided into three categories: black-box whole system evaluation, component specific evaluation and system-wide component analysis. Black-box Evaluation The results of a human assessment of an answer can be approximated using human generated answer keys (Breck et al., 2000). This method remains the most widely used form of evaluation of QA systems. However, this method should be used with caution (Lin, 2005). Lin’s work calls into question the reliability of the use of these answer keys for system evaluation and comparison. Firstly, the answer keys do not take into account the support of the source document. Similarly, the answers are limited to those returned by existing systems. Any new QA system will not be evaluated as better than the sum of preceding systems. Consequently, Lin finds that these evaluation resources underestimate answer accuracy. Lin proposes"
U08-1002,W02-1033,0,0.02927,"ex et al., 2003). Also, human assessment of QA document retrieval has formed an additional part of the TREC QA Track (Voorhees, 2003) and the results are then used for automated evaluation (Monz, 2003). These individual analyses are useful, yet their application is limited to specific components and implementations. Our approach is not component specific. Whole System Component Analysis There are two approaches to whole system component analysis. The first approach is assessing each component’s usefulness or contribution to overall system performance. This can be done by ablation experiments (Brill et al., 2002), where each component is replaced by a baseline component which performs the same function in a minimal way. The second approach is to look at cases where the system does not perform correctly, and identify the components which are causing these failures. Moldovan et al. (2003) manually traced each incorrectly answered question and decided which component was the cause. Each component was given a score in terms of the percentage of failures that it caused. This was used to determine which components of the system should be focused on to improve overall system performance. While useful, this a"
U08-1002,P04-1014,1,0.876953,"Missing"
U08-1006,P01-1005,0,0.0656794,"ised approaches, e.g. Bod (2006) and Seginer (2007), operate directly on raw text. While supervised approaches have generally proven more successful, the need for annotated training data is a major bottleneck. Although the emergence of the Penn Treebank as a standard resource has been beneficial in parser development and evaluation, parsing performance drops when analysing text from domains other than that represented in the training data (Sekine, 1997; Gildea, 2001). In addition, there is evidence that language processing performance can still benefit from orders of magnitude more data (e.g. Banko and Brill (2001)). However, the cost of manually annotating the necessary amount of data is prohibitive. We investigate a method of automatically creating annotated data to supplement existing training corpora. We constructed a list of facts based on factoid questions from the TREC 2004 Question Answering track (Voorhees, 2004) and the ISI Question Answer Typology (Hovy et al., 2002). For each of these facts, we extracted sentences from Web text that contained all the keywords of the fact. These sentences were then parsed using a state-of-the-art parser (Clark and Curran, 2007). By assuming that the same gram"
U08-1006,J93-2001,0,0.0803857,"rithms and find that with a training corpus of 1 billion words, there is little difference between the algorithms, and performance appears not to be asymptoting. This result suggests that for other language processing tasks, perhaps including parsing, performance can still be improved by large quantities of additional training data, and that the amount of data is more important than the particular learning algorithm used. Quantity of data is not the only consideration, however. It has been shown that there is considerable variation in the syntactic structures used in different genres of text (Biber, 1993), suggesting that a parser trained solely on newswire text will show reduced performance when parsing other genres. Evaluating parser performance on the Brown corpus, Sekine (1997) found that the best performance was achieved when the grammar was extracted from a corpus of the same domain as the test set, followed by one extracted from an equivalent size corpus containing texts from the same class (fiction or nonfiction), while parsers trained on a different class or different domain performed noticeably worse. Comparing parsers trained on the Penn Treebank and Brown corpora, Gildea (2001) fou"
U08-1006,P06-1109,0,0.182702,"sentences and increasing coverage incurs significant development costs (Cahill et al., 2008). This has led to interest in automatic acquisition of grammars from raw text or automatically annotated data such as the Penn Treebank (Marcus et al., 1993). Automatically-acquired grammars may be classified according to whether the learning algorithm used to estimate the language model is supervised or unsupervised. Supervised algorithms, e.g. Collins (1999) and Charniak (2000), require a large number of sentences already parsed according to the desired formalism, while unsupervised approaches, e.g. Bod (2006) and Seginer (2007), operate directly on raw text. While supervised approaches have generally proven more successful, the need for annotated training data is a major bottleneck. Although the emergence of the Penn Treebank as a standard resource has been beneficial in parser development and evaluation, parsing performance drops when analysing text from domains other than that represented in the training data (Sekine, 1997; Gildea, 2001). In addition, there is evidence that language processing performance can still benefit from orders of magnitude more data (e.g. Banko and Brill (2001)). However"
U08-1006,P06-2006,0,0.0148996,"evaluation, however, does not allow easy comparison with non-CCG systems. Evaluating a CCG parser using the traditional metrics of precision, recall and F-score over Penn Treebank bracketings is problematic since CCG derivations, being binary-branching, can have a very different shape from the trees found in the Penn Treebank. This is particularly true of the spurious derivations, which will be heavily penalised even though they are correct analyses that lead to the correct predicateargument dependencies. To address this problem, Clark and Curran (2007) also evaluate their parser against the Briscoe and Carroll (2006) re-annotation of the PARC Dependency Bank (DepBank; King et al. (2003)), which contains 700 sentences from section 23 of the Penn Treebank, represented in the form of grammatical relations, e.g. ncsubj (non-clausal subject) and dobj (direct object). To do this, they convert the predicate-argument dependency output of the parser into grammatical relations, a non-trivial, many-tomany mapping. The conversion process puts the C & C parser at a disadvantage, however the its performance still rivals that of the RASP parser (Briscoe et al., 2006) that returns DepBank grammatical relations natively."
U08-1006,P06-4020,0,0.0407778,"Missing"
U08-1006,J08-1003,0,0.0234187,"Missing"
U08-1006,A00-2018,0,0.255365,"ake use of hand-crafted grammars, e.g. Riezler et al. (2002) and Briscoe et al. (2006), these typically cannot accommodate a wide variety of sentences and increasing coverage incurs significant development costs (Cahill et al., 2008). This has led to interest in automatic acquisition of grammars from raw text or automatically annotated data such as the Penn Treebank (Marcus et al., 1993). Automatically-acquired grammars may be classified according to whether the learning algorithm used to estimate the language model is supervised or unsupervised. Supervised algorithms, e.g. Collins (1999) and Charniak (2000), require a large number of sentences already parsed according to the desired formalism, while unsupervised approaches, e.g. Bod (2006) and Seginer (2007), operate directly on raw text. While supervised approaches have generally proven more successful, the need for annotated training data is a major bottleneck. Although the emergence of the Penn Treebank as a standard resource has been beneficial in parser development and evaluation, parsing performance drops when analysing text from domains other than that represented in the training data (Sekine, 1997; Gildea, 2001). In addition, there is ev"
U08-1006,N06-1019,1,0.787187,"csubj (non-clausal subject) and dobj (direct object). To do this, they convert the predicate-argument dependency output of the parser into grammatical relations, a non-trivial, many-tomany mapping. The conversion process puts the C & C parser at a disadvantage, however the its performance still rivals that of the RASP parser (Briscoe et al., 2006) that returns DepBank grammatical relations natively. Figure 3 illustrates the grammatical relations obtained from the analysis in Figure 1. 5 Getting Past the Bottleneck Although some work has aimed at reducing the cost of training the C & C parser (Clark and Curran, 2006), the question remains of whether annotated training data can be obtained for free. In their first entry to the TREC QuestionAnswering task, Brill et al. (2001) reasoned that while finding the correct answer in the given corpus may sometimes require sophisticated linguistic or logical processing, a preliminary answer found on the Web can help to identify the final answer in the corpus. They argue that the sheer size of the Web means that an answer can often be found using simple or shallow processing. We exploit the same idea of the redundancy of information on the Web here. Mozart was aux bor"
U08-1006,J07-4004,1,0.861489,"ers of magnitude more data (e.g. Banko and Brill (2001)). However, the cost of manually annotating the necessary amount of data is prohibitive. We investigate a method of automatically creating annotated data to supplement existing training corpora. We constructed a list of facts based on factoid questions from the TREC 2004 Question Answering track (Voorhees, 2004) and the ISI Question Answer Typology (Hovy et al., 2002). For each of these facts, we extracted sentences from Web text that contained all the keywords of the fact. These sentences were then parsed using a state-of-the-art parser (Clark and Curran, 2007). By assuming that the same grammatical relations always hold between the keywords, we use the analyses of simple sentences to constrain the analysis of more complex sentences expressing the same fact. The constrained parses then form additional training data for the parser. The results here show that parser performance has not been adversely affected; when the scale of the data collection is increased, we expect to see a corresponding increase in performance. 2 Background The grammars used in parsing systems may be classified as either hand-crafted or automatically acquired. Hand-crafted gram"
U08-1006,W01-0521,0,0.214212,"e.g. Collins (1999) and Charniak (2000), require a large number of sentences already parsed according to the desired formalism, while unsupervised approaches, e.g. Bod (2006) and Seginer (2007), operate directly on raw text. While supervised approaches have generally proven more successful, the need for annotated training data is a major bottleneck. Although the emergence of the Penn Treebank as a standard resource has been beneficial in parser development and evaluation, parsing performance drops when analysing text from domains other than that represented in the training data (Sekine, 1997; Gildea, 2001). In addition, there is evidence that language processing performance can still benefit from orders of magnitude more data (e.g. Banko and Brill (2001)). However, the cost of manually annotating the necessary amount of data is prohibitive. We investigate a method of automatically creating annotated data to supplement existing training corpora. We constructed a list of facts based on factoid questions from the TREC 2004 Question Answering track (Voorhees, 2004) and the ISI Question Answer Typology (Hovy et al., 2002). For each of these facts, we extracted sentences from Web text that contained"
U08-1006,W03-2401,0,0.0273371,"uating a CCG parser using the traditional metrics of precision, recall and F-score over Penn Treebank bracketings is problematic since CCG derivations, being binary-branching, can have a very different shape from the trees found in the Penn Treebank. This is particularly true of the spurious derivations, which will be heavily penalised even though they are correct analyses that lead to the correct predicateargument dependencies. To address this problem, Clark and Curran (2007) also evaluate their parser against the Briscoe and Carroll (2006) re-annotation of the PARC Dependency Bank (DepBank; King et al. (2003)), which contains 700 sentences from section 23 of the Penn Treebank, represented in the form of grammatical relations, e.g. ncsubj (non-clausal subject) and dobj (direct object). To do this, they convert the predicate-argument dependency output of the parser into grammatical relations, a non-trivial, many-tomany mapping. The conversion process puts the C & C parser at a disadvantage, however the its performance still rivals that of the RASP parser (Briscoe et al., 2006) that returns DepBank grammatical relations natively. Figure 3 illustrates the grammatical relations obtained from the analys"
U08-1006,J06-4003,0,0.0145651,"ument processing. Each unique page returned was saved. Prior to splitting the documents into sentences, HTML tags were stripped from the text and HTML character entities replaced with their character equivalents. In order to incorporate some HTML markup information in the sentence boundary identification process, each page was split into a number of chunks by dividing at certain manually-identified HTML tags, such as heading and paragraph markers, which are unlikely to occur mid-sentence. Each of these chunks was then passed through the sentence boundary identifier available in NLTK v.0.9.31 (Kiss and Strunk, 2006). Ideally, the process would use a boundary identifier trained on HTML text including markup, to avoid these heuristic divisions. To further simplify processing, sentences were discarded if they contained characters other than standard ASCII alphanumeric characters or a small number of additional punctuation characters. We also regarded as noisy and discarded sentences containing whitespace-delimited tokens that contained fewer alphanumeric characters than other characters. Sentences were then tokenised using a tokeniser developed the C & C parser for the TREC competition (Bos et al., 2007). S"
U08-1006,J93-2004,0,0.0450841,"anguage processing tasks such as Question Answering (Harabagiu et al., 2000) and Machine Translation (Melamed, 2004). The structures that a parsing system assigns to sentences are governed by the grammar used. While some parsers make use of hand-crafted grammars, e.g. Riezler et al. (2002) and Briscoe et al. (2006), these typically cannot accommodate a wide variety of sentences and increasing coverage incurs significant development costs (Cahill et al., 2008). This has led to interest in automatic acquisition of grammars from raw text or automatically annotated data such as the Penn Treebank (Marcus et al., 1993). Automatically-acquired grammars may be classified according to whether the learning algorithm used to estimate the language model is supervised or unsupervised. Supervised algorithms, e.g. Collins (1999) and Charniak (2000), require a large number of sentences already parsed according to the desired formalism, while unsupervised approaches, e.g. Bod (2006) and Seginer (2007), operate directly on raw text. While supervised approaches have generally proven more successful, the need for annotated training data is a major bottleneck. Although the emergence of the Penn Treebank as a standard reso"
U08-1006,P04-1083,0,0.0198585,"a corpus of sentences extracted from the Web that contain the fact keywords. We use a state-of-the-art parser to parse these sentences, constraining the analysis of the more complex sentences using information from the simpler sentences. This allows us to automatically create additional annotated sentences which we then use to augment our existing training data. 1 Introduction Determining the syntactic structure of sentences is a necessary step in analysing the content of text for a range of language processing tasks such as Question Answering (Harabagiu et al., 2000) and Machine Translation (Melamed, 2004). The structures that a parsing system assigns to sentences are governed by the grammar used. While some parsers make use of hand-crafted grammars, e.g. Riezler et al. (2002) and Briscoe et al. (2006), these typically cannot accommodate a wide variety of sentences and increasing coverage incurs significant development costs (Cahill et al., 2008). This has led to interest in automatic acquisition of grammars from raw text or automatically annotated data such as the Penn Treebank (Marcus et al., 1993). Automatically-acquired grammars may be classified according to whether the learning algorithm"
U08-1006,P02-1035,0,0.118222,"e more complex sentences using information from the simpler sentences. This allows us to automatically create additional annotated sentences which we then use to augment our existing training data. 1 Introduction Determining the syntactic structure of sentences is a necessary step in analysing the content of text for a range of language processing tasks such as Question Answering (Harabagiu et al., 2000) and Machine Translation (Melamed, 2004). The structures that a parsing system assigns to sentences are governed by the grammar used. While some parsers make use of hand-crafted grammars, e.g. Riezler et al. (2002) and Briscoe et al. (2006), these typically cannot accommodate a wide variety of sentences and increasing coverage incurs significant development costs (Cahill et al., 2008). This has led to interest in automatic acquisition of grammars from raw text or automatically annotated data such as the Penn Treebank (Marcus et al., 1993). Automatically-acquired grammars may be classified according to whether the learning algorithm used to estimate the language model is supervised or unsupervised. Supervised algorithms, e.g. Collins (1999) and Charniak (2000), require a large number of sentences already"
U08-1006,P07-1049,0,0.118749,"increasing coverage incurs significant development costs (Cahill et al., 2008). This has led to interest in automatic acquisition of grammars from raw text or automatically annotated data such as the Penn Treebank (Marcus et al., 1993). Automatically-acquired grammars may be classified according to whether the learning algorithm used to estimate the language model is supervised or unsupervised. Supervised algorithms, e.g. Collins (1999) and Charniak (2000), require a large number of sentences already parsed according to the desired formalism, while unsupervised approaches, e.g. Bod (2006) and Seginer (2007), operate directly on raw text. While supervised approaches have generally proven more successful, the need for annotated training data is a major bottleneck. Although the emergence of the Penn Treebank as a standard resource has been beneficial in parser development and evaluation, parsing performance drops when analysing text from domains other than that represented in the training data (Sekine, 1997; Gildea, 2001). In addition, there is evidence that language processing performance can still benefit from orders of magnitude more data (e.g. Banko and Brill (2001)). However, the cost of manua"
U08-1006,A97-1015,0,0.204162,"d algorithms, e.g. Collins (1999) and Charniak (2000), require a large number of sentences already parsed according to the desired formalism, while unsupervised approaches, e.g. Bod (2006) and Seginer (2007), operate directly on raw text. While supervised approaches have generally proven more successful, the need for annotated training data is a major bottleneck. Although the emergence of the Penn Treebank as a standard resource has been beneficial in parser development and evaluation, parsing performance drops when analysing text from domains other than that represented in the training data (Sekine, 1997; Gildea, 2001). In addition, there is evidence that language processing performance can still benefit from orders of magnitude more data (e.g. Banko and Brill (2001)). However, the cost of manually annotating the necessary amount of data is prohibitive. We investigate a method of automatically creating annotated data to supplement existing training corpora. We constructed a list of facts based on factoid questions from the TREC 2004 Question Answering track (Voorhees, 2004) and the ISI Question Answer Typology (Hovy et al., 2002). For each of these facts, we extracted sentences from Web text"
U08-1006,H93-1052,0,0.0231056,"simple sentence such as Mozart was born in 1756 (Figure 3). Given the size of the Web, many such sentences should be easily found. A longer sentence, such as Wolfgang Amadeus Mozart (baptized Johannes Chrysostomus Wolfgangus Theophilus) was born in Salzburg in 1756, the second survivor out of six children, is more complex and thus contains more opportunities for the parser to produce an incorrect analysis. However, this more complex sentence contains the same grammatical relations between the same words as in the simple sentence (Figure 4). Similar to the one sense per collocation constraint (Yarowsky, 1993), we assume that any sentence containing the words Mozart, born and 1756 will contain the same relationships between these words. We hypothesise that by constraining the parser to output an analysis consistent with these relationships, the correct analysis of the complex sentences can be found without manual intervention. These complex sentences can then be used as additional training data, allowing the parser to learn the general pattern of the sentence. For this process, we use grammatical relations rather than CCG dependencies as the former generalise across the latter and are therefore mor"
U08-1006,J03-4003,0,\N,Missing
U08-1008,W02-2001,0,0.1172,"ns accurately. Also, because these systems will be running during parsing, they need to be fast to prevent the creation of an additional bottleneck. Here we focus on Verb-Particle Constructions (VPCs), a type of MWE composed of a verb and a particle. Villavicencio (2003a) showed that VPCs are poorly covered by corpus data and constantly growing in number, making them a suitable candidate for an automatic classification system. Previous work on VPCs has mainly focused either on their compositionality (McCarthy et al., 2003), or on using sophisticated parsers to perform extraction from corpora (Baldwin and Villavicencio, 2002). While parser based methods have been very successful (Kim and Baldwin, 2006) they rely on contextual knowledge and complex processing. The Web has been used as a corpus previously by Villavicencio (2003b), who used search engines as a source of statistics for classification, but her aim was to create new resources, rather than a tool that can be used for identification. We have constructed a high-throughput query system for the Google Web1T data, which we use to collect information to perform the fronting linguistic constituency test. The results of this process are used to train a classifie"
U08-1008,W03-1809,0,0.0302881,"on compositionality as the two areas are closely linked. 2.1 Compositionality Determining how much the simplex meaning of individual words in a MWE contribute to the overall meaning is challenging, and important for producing semantically correct analysis of text. A range of methods have been considered to differentiate examples based on their degree of semantic idiosyncrasy. Initial work by Lin (1999) considered the compositionality of MWEs by comparing the distributional characteristics for a given MWE and potentially similar expressions formed by synonym substitution. This was followed by Bannard et al. (2003), who used human non-experts to construct a gold standard dataset of VPCs and their compositionality, which was used to construct a classifier that could judge whether the two words used contributed their simplex meaning. McCarthy et al. (2003) used an automatically acquired thesaurus in the calculation of statistics regarding the compositionality of VPCs and compared their results with statistics commonly used for extracting multiwords, such as latent semantic analysis. Bannard (2005) compared the lexical contexts of VPC s and their component words across a corpus to determine which words are"
U08-1008,P08-1002,0,0.0348507,"Missing"
U08-1008,W06-1207,0,0.0491799,"ing multiwords, such as latent semantic analysis. Bannard (2005) compared the lexical contexts of VPC s and their component words across a corpus to determine which words are contributing an independent meaning. Light Verb Constructions (LVCs) are another example of an MWE that has been considered in compositionality studies. The challenge of distinguishing LVCs from idioms was considered by Fazly et al. (2005), who proposed a set of statistical measures to quantify properties that relate to the compositionality of an MWE, ideas that were then extended in Fazly and Stevenson (2007). Recently, Cook and Stevenson (2006) addressed the question of which sense of the component words is being used in a particular VPC. They focused on the contribution by particles and constructed a feature set based on the properties of VPC s and compared their effectiveness with standard co-occurrence measurements. 2.2 Classification A range of methods for automatic classification of MWE s have been studied previously, in particular the use of parsers, applying heuristics to n-grams, and search engine queries. Possibly the earliest attempt at classification, was by Smadja (1993), who considered verbparticle pairs, separated by u"
U08-1008,W07-1102,0,0.0117343,"h statistics commonly used for extracting multiwords, such as latent semantic analysis. Bannard (2005) compared the lexical contexts of VPC s and their component words across a corpus to determine which words are contributing an independent meaning. Light Verb Constructions (LVCs) are another example of an MWE that has been considered in compositionality studies. The challenge of distinguishing LVCs from idioms was considered by Fazly et al. (2005), who proposed a set of statistical measures to quantify properties that relate to the compositionality of an MWE, ideas that were then extended in Fazly and Stevenson (2007). Recently, Cook and Stevenson (2006) addressed the question of which sense of the component words is being used in a particular VPC. They focused on the contribution by particles and constructed a feature set based on the properties of VPC s and compared their effectiveness with standard co-occurrence measurements. 2.2 Classification A range of methods for automatic classification of MWE s have been studied previously, in particular the use of parsers, applying heuristics to n-grams, and search engine queries. Possibly the earliest attempt at classification, was by Smadja (1993), who consider"
U08-1008,W05-1005,0,0.0135417,"McCarthy et al. (2003) used an automatically acquired thesaurus in the calculation of statistics regarding the compositionality of VPCs and compared their results with statistics commonly used for extracting multiwords, such as latent semantic analysis. Bannard (2005) compared the lexical contexts of VPC s and their component words across a corpus to determine which words are contributing an independent meaning. Light Verb Constructions (LVCs) are another example of an MWE that has been considered in compositionality studies. The challenge of distinguishing LVCs from idioms was considered by Fazly et al. (2005), who proposed a set of statistical measures to quantify properties that relate to the compositionality of an MWE, ideas that were then extended in Fazly and Stevenson (2007). Recently, Cook and Stevenson (2006) addressed the question of which sense of the component words is being used in a particular VPC. They focused on the contribution by particles and constructed a feature set based on the properties of VPC s and compared their effectiveness with standard co-occurrence measurements. 2.2 Classification A range of methods for automatic classification of MWE s have been studied previously, in"
U08-1008,U07-1008,0,0.0653415,"71 120 47 60 52 54 45 63 Table 1: N-grams in the Web1T corpus for the VPC ferret out. alphabetical characters. One approach to this dataset is to simply treat it as a normal collection of n-gram frequencies, scanning the relevant sections for answers, such as in Bergsma et al. (2008) and Yuret (2007). Another approach is used by Talbot and Brants (2008), pruning the corpus to a third of its size and quantising the rest, reducing the space used by frequency counts to eight bits each. These methods have the disadvantages of slow execution and only providing approximate frequencies respectively. Hawker et al. (2007) considered two methods for making practical queries possible, preprocessing of the data, and pre-processing of queries. The first approach is to reduce the size of the dataset by decreasing the resolution of measurements, and by accessing n-grams by implicit information based on their location in the compressed data, rather than their actual representation. While this avoids the cost of processing the entire dataset for each query, it does produce less accurate results. The second method described is intelligent batching queries, then performing a single pass through the data to answer them a"
U08-1008,W06-2110,0,0.0928112,"tra information about the context of the verb-particle pair being considered. In particular, the information needed to identify the head noun phrase (NP) for each potential VPC. Early work by Blaheta and Johnson (2001) used a parsed corpus and log-linear models to identify VPC s. This was followed by Baldwin and Villavicencio (2002) who used a range of parser outputs and other features to produce a better informed classifier. Other forms of linguistic and statistical unsupervised methods were considered by Baldwin (2005), such as Pointwise Mutual Information. This work was further extended by Kim and Baldwin (2006) to utilise the sentential context of verb-particle pairs and their associated NP to improve results. The closest work to our own in terms of the corpus used is that of Villavicencio (2003b), in which the web is used to test the validity of candidate VPC s. Also, like our work, Villavicencio does not consider the context of the verb and particle, unlike the parser based methods described above. A collection of verb-particle pairs was generated by combining verbs from Levin’s classes (Levin, 1993) with the particle up. Each potential VPC was passed to the search engine Google to obtain an appro"
U08-1008,P99-1041,0,0.0376084,"s work concerning VPCs can be broadly divided into two groups, compositionality analysis, and classification. Our work is entirely concerned with classification, but we will briefly describe previous work on compositionality as the two areas are closely linked. 2.1 Compositionality Determining how much the simplex meaning of individual words in a MWE contribute to the overall meaning is challenging, and important for producing semantically correct analysis of text. A range of methods have been considered to differentiate examples based on their degree of semantic idiosyncrasy. Initial work by Lin (1999) considered the compositionality of MWEs by comparing the distributional characteristics for a given MWE and potentially similar expressions formed by synonym substitution. This was followed by Bannard et al. (2003), who used human non-experts to construct a gold standard dataset of VPCs and their compositionality, which was used to construct a classifier that could judge whether the two words used contributed their simplex meaning. McCarthy et al. (2003) used an automatically acquired thesaurus in the calculation of statistics regarding the compositionality of VPCs and compared their results"
U08-1008,W03-1810,0,0.0939327,"Missing"
U08-1008,J93-1007,0,0.0648401,"Fazly and Stevenson (2007). Recently, Cook and Stevenson (2006) addressed the question of which sense of the component words is being used in a particular VPC. They focused on the contribution by particles and constructed a feature set based on the properties of VPC s and compared their effectiveness with standard co-occurrence measurements. 2.2 Classification A range of methods for automatic classification of MWE s have been studied previously, in particular the use of parsers, applying heuristics to n-grams, and search engine queries. Possibly the earliest attempt at classification, was by Smadja (1993), who considered verbparticle pairs, separated by up to four other words, but did not perform a rigorous evaluation, which prevents us from performing a comparison. Recent work has focused on using parsers over raw text to gain extra information about the context of the verb-particle pair being considered. In particular, the information needed to identify the head noun phrase (NP) for each potential VPC. Early work by Blaheta and Johnson (2001) used a parsed corpus and log-linear models to identify VPC s. This was followed by Baldwin and Villavicencio (2002) who used a range of parser outputs"
U08-1008,P08-1058,0,0.0163202,"ferret you out out a ferret out of ferret out the ferret ferret it all out ferret these people out ferret these projects out out of the ferret ferret lovers can ferret out out a needing shelter ferret Frequency 79728 74 52 342 43 54 1562 58 180 1582 232 58 148 100 63 71 120 47 60 52 54 45 63 Table 1: N-grams in the Web1T corpus for the VPC ferret out. alphabetical characters. One approach to this dataset is to simply treat it as a normal collection of n-gram frequencies, scanning the relevant sections for answers, such as in Bergsma et al. (2008) and Yuret (2007). Another approach is used by Talbot and Brants (2008), pruning the corpus to a third of its size and quantising the rest, reducing the space used by frequency counts to eight bits each. These methods have the disadvantages of slow execution and only providing approximate frequencies respectively. Hawker et al. (2007) considered two methods for making practical queries possible, preprocessing of the data, and pre-processing of queries. The first approach is to reduce the size of the dataset by decreasing the resolution of measurements, and by accessing n-grams by implicit information based on their location in the compressed data, rather than the"
U08-1008,W03-1808,0,0.675799,"es are crucial for a range of Natural Language Processing (NLP) tasks, such as accurate parsing. Identification of MWEs does not prevent the production of syntactically accurate parses, but the extra information can improve results. Since manually creating these resources is a challenge, systems are needed that can automatically classify expressions accurately. Also, because these systems will be running during parsing, they need to be fast to prevent the creation of an additional bottleneck. Here we focus on Verb-Particle Constructions (VPCs), a type of MWE composed of a verb and a particle. Villavicencio (2003a) showed that VPCs are poorly covered by corpus data and constantly growing in number, making them a suitable candidate for an automatic classification system. Previous work on VPCs has mainly focused either on their compositionality (McCarthy et al., 2003), or on using sophisticated parsers to perform extraction from corpora (Baldwin and Villavicencio, 2002). While parser based methods have been very successful (Kim and Baldwin, 2006) they rely on contextual knowledge and complex processing. The Web has been used as a corpus previously by Villavicencio (2003b), who used search engines as a s"
U08-1008,S07-1044,0,0.0309763,"hese out ferret things out ferret this out ferret you out out a ferret out of ferret out the ferret ferret it all out ferret these people out ferret these projects out out of the ferret ferret lovers can ferret out out a needing shelter ferret Frequency 79728 74 52 342 43 54 1562 58 180 1582 232 58 148 100 63 71 120 47 60 52 54 45 63 Table 1: N-grams in the Web1T corpus for the VPC ferret out. alphabetical characters. One approach to this dataset is to simply treat it as a normal collection of n-gram frequencies, scanning the relevant sections for answers, such as in Bergsma et al. (2008) and Yuret (2007). Another approach is used by Talbot and Brants (2008), pruning the corpus to a third of its size and quantising the rest, reducing the space used by frequency counts to eight bits each. These methods have the disadvantages of slow execution and only providing approximate frequencies respectively. Hawker et al. (2007) considered two methods for making practical queries possible, preprocessing of the data, and pre-processing of queries. The first approach is to reduce the size of the dataset by decreasing the resolution of measurements, and by accessing n-grams by implicit information based on"
U08-1013,W06-2703,0,0.143235,"jectival) American European French British Western FOG Facilities and Organisations Ford Microsoft Sony Disneyland Google PLCE Place: Geo-political entities and locations Africa America Washington London Pacific DAT Reference to a date or period January May December October June LANG Any named language English Chinese Japanese Spanish Russian FEM Table 2: Web 1T semantic categories and seeds. Limited preprocessing was required to extract the 5-grams from MEDLINE and TREC. The TREC documents were converted from HTML to raw text, and both collections were tokenised using bio-specific NLP tools (Grover et al., 2006). We did not exclude lowercase terms or templates containing numbers. Templates appearing with less than 7 (MEDLINE) or 3 (TREC) terms were removed. These frequencies were selected to permit the largest number of templates and terms loadable by BASILISK 3 , to allow a fair comparison. The size of the resulting datasets are shown in Table 1. Note that, Web 1T has far fewer terms but many more templates than the biomedical sets, and TREC articles result in more templates than MEDLINE for a similar number of terms. 4.2 Semantic Categories & Stop Categories In the Web 1T experiments, we are extrac"
U08-1013,C92-2082,0,0.0663427,"t two state-of-theart mutual bootstrapping algorithms, MEB (Curran et al., 2007) and BASILISK (Thelen and Riloff, 2002). We have evaluated the terms and templates these algorithms extract under a range of conditions from three raw text collections: noisy web text, biomedical abstracts, and full-text articles. We demonstrate that WMEB outperforms these existing algorithms in extracting precise lexicons and templates from all three datasets. WMEB is significantly less susceptible to semantic drift and so can produce large lexicons accurately and efficiently across multiple domains. 2 Background Hearst (1992) pioneered the use of templates for information extraction, focussing on acquiring isa relations using manually devised templates like such W as X, ..., Y and/or Z where X, ..., Y, Z are hyponyms of W. Various automated template-based bootstrapping algorithms have since been developed to iteratively build semantic lexicons from texts. Riloff and Shepherd (1997) proposed Iterative Bootstrapping (IB) where seed instances of a semantic category are used to identify related terms that frequently co-occur. In Mutual Bootstrapping (MB) (Riloff and Jones, 1999) seed instances of a desired type are us"
U08-1013,U07-1011,1,0.839388,"ore scores cannot be pre-calculated. 4 Experimental Setting 4.1 Data We evaluated the performance of BASILISK, MEB and WMEB using 5-grams from three raw text resources: the Google Web 1T corpus (Brants and Franz, 2006), MEDLINE abstracts2 and the TREC Genomics Track 2007 full-text articles (Hersh et al., 2007). In our experiments, the term is the middle token of each 5-gram and the template is the two tokens on either side. Unlike Riloff and Jones (1999) and Yangarber (2003), we do not use syntactic knowledge. Although we only extract unigrams, each algorithm can identify multi-term entities (Murphy and Curran, 2007). The Web 1T 5-grams were filtered by removing templates appearing with only one term and templates containing numbers. All 5-gram contexts with a non-titlecase term were also filtered as we are extracting proper nouns. 2 The set contains all MEDLINE abstracts available up to Oct 2007 (16 140 000 abstracts) CAT DESCRIPTION Person: female first name Mary Patricia Linda Barbara Elizabeth MALE Person: male first name James John Robert Michael William LAST Person: last name Smith Johnson Williams Jones Brown TTL Honorific title General President Director King Doctor NORP Nationality, Religion, Pol"
U08-1013,P06-1015,0,0.0879736,"most input instances, and 2) have the potential to generate the most new candidates, are preferred (Curran et al., 2007). This second criteria aims to increase recall, however the selected instances are highly likely to introduce drift. We introduce a new weighting scheme to effectively overcome this. Template-based bootstrapping algorithms have also been used in various Information Extraction (IE) tasks. Agichtein and Gravano (2000) developed the SNOWBALL system to identify the locations of companies, and Yu and Agichtein (2003) applied SNOWBALL to extract synonymous gene and protein terms. Pantel and Pennacchiotti (2006) used bootstrapping to identify numerous semantic relationships, such as is-a and part-of relationships. They incorporate the pointwise mutual information (MI) measure between the templates and instances to determine template reliability, as well as exploiting generic templates and the Web for filtering incorrect instances. We evaluate the effectiveness of MI as a weighting function for selecting terms and templates in WMEB. In the biomedical domain, there is an increased interest in automatically extracting lexicons of biomedical entities such as antibodies and mutations, and the templates wh"
U08-1013,P02-1006,0,0.0758111,"ew weighting functions and a cumulative template pool while still enforcing mutual exclusion between the categories. We compare WMEB and two state-of-theart approaches on the Web 1T corpus and two large biomedical literature collections. WMEB is more efficient and scalable, and we demonstrate that it significantly outperforms the other approaches on the noisy web corpus and biomedical text. 1 Introduction Automatically acquiring semantic lexicons and templates from raw text is essential for overcoming the knowledge bottleneck in many natural language processing tasks, e.g. question answering (Ravichandran and Hovy, 2002). These tasks typically involve identifying named entity (NE) classes which are not found in annotated corpora and thus supervised NE recognition models are not always available. This issue becomes even more evident in new domains, such as biomedicine, where new semantic categories are often poorly represented in linguistic resources, if at all (Hersh et al., 2007). There are two common approaches to extract semantic lexicons: distributional similarity and template-based bootstrapping . In template-based bootstrapping algorithms, templates that express a particular semantic type are used to re"
U08-1013,W97-0313,0,0.198319,"ese existing algorithms in extracting precise lexicons and templates from all three datasets. WMEB is significantly less susceptible to semantic drift and so can produce large lexicons accurately and efficiently across multiple domains. 2 Background Hearst (1992) pioneered the use of templates for information extraction, focussing on acquiring isa relations using manually devised templates like such W as X, ..., Y and/or Z where X, ..., Y, Z are hyponyms of W. Various automated template-based bootstrapping algorithms have since been developed to iteratively build semantic lexicons from texts. Riloff and Shepherd (1997) proposed Iterative Bootstrapping (IB) where seed instances of a semantic category are used to identify related terms that frequently co-occur. In Mutual Bootstrapping (MB) (Riloff and Jones, 1999) seed instances of a desired type are used to infer new templates, which in turn identify new lexicon entries. This process is repeated with the new terms identifying new templates. In each iteration, new terms and templates are selected based on a metric scoring their suitability for extracting additional templates and terms for the category. Unfortunately, if a term with multiple senses or a templa"
U08-1013,W02-1028,0,0.853882,") developed Mutual Exclusion Bootstrapping (MEB) to reduce semantic drift by forcing semantic classes to be mutually exclusive. We introduce a new algorithm, Weighted Mutual Exclusion Bootstrapping (WMEB), that automatically acquires multiple semantic lexicons and their templates simultaneously. It extends on the Curran et al. (2007) assumption of mutual exclusion between categories by incorporating a novel cumulative template pool and new term and template weighting functions. We compare WMEB against two state-of-theart mutual bootstrapping algorithms, MEB (Curran et al., 2007) and BASILISK (Thelen and Riloff, 2002). We have evaluated the terms and templates these algorithms extract under a range of conditions from three raw text collections: noisy web text, biomedical abstracts, and full-text articles. We demonstrate that WMEB outperforms these existing algorithms in extracting precise lexicons and templates from all three datasets. WMEB is significantly less susceptible to semantic drift and so can produce large lexicons accurately and efficiently across multiple domains. 2 Background Hearst (1992) pioneered the use of templates for information extraction, focussing on acquiring isa relations using man"
U08-1013,C02-1154,0,0.401903,"cted based on a metric scoring their suitability for extracting additional templates and terms for the category. Unfortunately, if a term with multiple senses or a template which weakly constrains the semantic class is selected, semantic drift of the lexicon and templates occurs – the semantic class drifts into another category (Curran et al., 2007). Extracting multiple semantic categories simultaneously has been proposed to reduce semantic drift. The bootstrapping instances compete with one another in an attempt to actively direct the categories away from each other (Thelen and Riloff, 2002; Yangarber et al., 2002; Curran et al., 2007). This strategy is similar to the one sense per discourse assumption (Yarowsky, 1995). In BASILISK (Thelen and Riloff, 2002), candidate terms for a category are ranked highly if they have strong evidence for the category and little or no evidence for another. It is possible for an ambiguous term to be assigned to the less dominant sense, and in turn less precise templates will be selected, causing semantic drift. Drift may also be introduced as templates can be selected by different categories in different iterations. NOMEN (Yangarber et al., 2002) was developed to extrac"
U08-1013,P03-1044,0,0.735827,"tationally expensive. In BASILISK, each individual calculation is dependent on the current state of the bootstrapping process, and therefore scores cannot be pre-calculated. 4 Experimental Setting 4.1 Data We evaluated the performance of BASILISK, MEB and WMEB using 5-grams from three raw text resources: the Google Web 1T corpus (Brants and Franz, 2006), MEDLINE abstracts2 and the TREC Genomics Track 2007 full-text articles (Hersh et al., 2007). In our experiments, the term is the middle token of each 5-gram and the template is the two tokens on either side. Unlike Riloff and Jones (1999) and Yangarber (2003), we do not use syntactic knowledge. Although we only extract unigrams, each algorithm can identify multi-term entities (Murphy and Curran, 2007). The Web 1T 5-grams were filtered by removing templates appearing with only one term and templates containing numbers. All 5-gram contexts with a non-titlecase term were also filtered as we are extracting proper nouns. 2 The set contains all MEDLINE abstracts available up to Oct 2007 (16 140 000 abstracts) CAT DESCRIPTION Person: female first name Mary Patricia Linda Barbara Elizabeth MALE Person: male first name James John Robert Michael William LAS"
U08-1013,P95-1026,0,0.156966,"nfortunately, if a term with multiple senses or a template which weakly constrains the semantic class is selected, semantic drift of the lexicon and templates occurs – the semantic class drifts into another category (Curran et al., 2007). Extracting multiple semantic categories simultaneously has been proposed to reduce semantic drift. The bootstrapping instances compete with one another in an attempt to actively direct the categories away from each other (Thelen and Riloff, 2002; Yangarber et al., 2002; Curran et al., 2007). This strategy is similar to the one sense per discourse assumption (Yarowsky, 1995). In BASILISK (Thelen and Riloff, 2002), candidate terms for a category are ranked highly if they have strong evidence for the category and little or no evidence for another. It is possible for an ambiguous term to be assigned to the less dominant sense, and in turn less precise templates will be selected, causing semantic drift. Drift may also be introduced as templates can be selected by different categories in different iterations. NOMEN (Yangarber et al., 2002) was developed to extract generalized names such as diseases and drugs, with no capitalisation cues. NOMEN, like BASILISK , identif"
U08-1014,S07-1085,0,0.0504984,"Missing"
U08-1014,S07-1084,0,0.0305426,"oncepts which encompass the nominals. These features allow us to broaden the coverage given by the nominals over less specific entities. We exhaustively mined all hypernyms of the marked nouns to a height of two levels, and encoded the two levels as separate features; • lex: Lexical file numbers, which correspond to a number of abstract semantic classes in WordNet, including noun.artifact, noun.event, and noun.process. This allows for nominal relations which do not make sense to be identified, e.g. a noun.process should not be able to contain a noun.event, but the process may cause the event (Bedmar et al., 2007); • cont: Container - a binary feature indicating whether the marked nouns are hyponyms (more specific concepts) of the container synset. This feature was included mainly for the benefit of the Content-Container relation; however, we hypothesised that their inclusion may also assist in classifying other relations; e.g. the ‘effect’ in Cause-Effect should not be a physical entity. 4.4 Grammatical Relations Features Syntactic features representing the path between nominals are a useful complement for semantic and lexical features because they account for the way in which words are commonly used"
U08-1014,W04-3205,0,0.0463675,"Missing"
U08-1014,P04-1014,1,0.713837,"of a classifier is a measure of how predictable that classifier’s decisions are. The lower the entropy, the more biased a classifier is, i.e. a relation classifier has zero entropy if it always assigns the same relation to any input. The theory underpinning ME modelling is that the distribution chosen to fit the specified constraints will eliminate biases by being as uniform as possible. Such models are useful in NLP applications because they can effectively incorporate diverse and overlapping features whilst also addressing statistical dependencies. We used the ME implementation described in Clark and Curran (2004). The ME models used have the following form: ! n X 1 p(y|x, λ) = exp λk fk (x, y) Z(x|λ) k=1 where Z(x|λ) is the normalisation function and the fk are features with associated weights λk . The system uses Gaussian smoothing on the parameters of the model. The features are binaryvalued functions which pair a relation y with various observations x from the context provided, e.g. fj (x, y) =   1   0 if word(x) = damage & y = Cause-Effect-True otherwise 4 Features and Methodology We focused our efforts on finding features which aggressively generalise the initial material over as broad a sea"
U08-1014,J07-4004,1,0.734624,"nks them) could be used to find a path between the two nominals in each sentence. This path would compare favourably to a naive concatenation of the words between the nominals as it considers the actual dependencies in the sentence rather than just the positions of the words, although in many cases at least one of the words between the marked nominals in the sentence will be represented in the dependency path. Table 2 gives a list of GRs used in this process. To extract the grammatical relations from the provided data we parsed each training and test example with the C & C parser developed by Clark and Curran (2007). Figure 1 gives an example of the GRs for the sentence A man does not talk or every woman walks. A dependency graph was generated from this output and the shortest path between the nominals found. In the example in Figure 1, the path between man and woman is talk does or walks. Features were extracted from this path output in two formats: a generalised version (labelled with a ‘g’ prefix), whereby the two nominals in question were replaced whenever they appeared with the marker tags e1 and e2 , and the actual version, where this extra generalisation step was not applied. We reasoned that the"
U08-1014,P08-1027,0,0.0234826,"Missing"
U08-1014,S07-1003,0,0.0319307,"Missing"
U08-1014,C92-2082,0,0.0368709,"s, 2-, 3-, 4-, and 5-grams in the 1 trillion word tokens, discarding unigrams appearing less than 200 times in the tokens (1 in 5 billion) and n-grams appearing less than 40 times (1 in 25 billion) in the tokens. This resource captures many lexical patterns used in common English, though there are some inconsistencies due to the permissive nature of the web: some commonly misspelt words are included and some text in languages other than English are also present. The idea of searching a large corpus for specific lexical patterns to indicate semantic relations of interest was first described by Hearst (1992). As previously mentioned, we postulated that certain patterns of words would associate with certain relations, but a naive concatenation of words located between the nominals would be unhelpful with such a small data set. This problem can be avoided by examining the frequencies of lexical patterns within a much larger dataset such as Web 1T, where the problem of data sparseness is offset by the size of the corpus. This pattern information would complement the semantic and syntactic information already used by incorporating evidence regarding word use in real-world text. We chose to conduct st"
U08-1014,S07-1039,0,0.0514771,"Missing"
U08-1014,S07-1101,0,0.0212838,"iated with certain patterns of words, e.g. the pattern e1 is inside e2 is a strong indicator for the ContentContainer relation for many general combinations of e1 and e2 . However, these patterns can be expressed in many different ways - inside e2 e1 is or inside e2 is e1 are other ways of expressing a Content-Container relationship – and while the words are essentially the same between the examples the changed ordering creates difficulties in designing good features. This problem can be alleviated by considering syntactic dependencies in a sentence rather than a naive concatenation of words (Nicolae et al., 2007). Grammatical relations (GRs) represent the syntactic dependencies that hold between a head and a dependent in text. Initially proposed by Carroll et al. (1998) as a framework-independent metric GR s conj aux det ncmod xmod cmod pmod ncsubj xsubj csubj dobj obj2 iobj pcomp xcomp ccomp ta Description coordinator auxiliary determiner non-clausal modifier unsaturated predicative modifier saturated clausal modifier PP modifier with a PP complement non-clausal subject unsaturated predicative subject saturated clausal subject direct object second object indirect object PP which is a PP complement un"
U08-1014,S07-1082,0,0.0336368,"Missing"
U08-1014,W01-0511,0,0.074414,"Missing"
U08-1014,C08-1082,0,0.0406214,"Missing"
U08-1016,P03-2031,0,0.337073,"Missing"
U08-1016,M98-1001,0,0.0690231,"anisation and location). Comparing to MUC, C O NLL and BBN corpora, Wikipedia generally performs better than other cross-corpus train/test pairs. 1 Introduction Named Entity Recognition (NER), the task of identifying and classifying the names of people, organisations, locations and other entities within text, is central to many NLP tasks. The task developed from information extraction in the Message Understanding Conferences (MUC) of the 1990s. By the final two MUC evaluations, NER had become a distinct task: tagging the aforementioned proper names and some temporal and numerical expressions (Chinchor, 1998). The C O NLL NER evaluations of 2002 and 2003 (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) focused on determining superior machine learning algorithms and feature models for multilingual NER, marking tags for person (PER), organisation (ORG), location (LOC) and miscellaneous (MISC; broadly including e.g. events, artworks and nationalities). Brunstein (2002) and Sekine et al. (2002) expanded this into fine-grained categorical hierarchies; others have utilised the WordNet noun hierarchy (Miller, 1998) in a similar manner (e.g. Toral et al. (2008)). For some applications, such as"
U08-1016,W03-0424,1,0.571285,"entities. Hence we unlink the following strings when found at the end of link text: parenthesised expressions; text following a comma for LOC, ORG and PER; possessive ’s; or other punctuation. 6 Evaluation We evaluate our corpora by training the C&C tagger4 to build separate models (a) when trained with Wikipedia data; (b) when trained with handannotated training data; (c) when trained with both combined, and comparing the tagging results on gold-standard test data. We use the C&C Maximum Entropy NER tagger with default orthographic, contextual, in-document and first name gazetteer features (Curran and Clark, 2003). Our results are given as per-category and microaveraged phrasal precision, recall and F1 -score. 6.1 Gold-standard corpora We evaluate our generated corpora against three sets of manually-annotated data from (a) the MUC -7 Named Entity Task (MUC, 2001); (b) the English C O NLL -03 Shared Task (Tjong Kim Sang and De Meulder, 2003); (c) the BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005). Stylistic and genre differences between the source texts affect compatibility for NER, e.g. the C O NLL corpus formats headlines in all-caps, and includes much nonsentential da"
U08-1016,I08-1071,0,0.0559107,"Missing"
U08-1016,D07-1073,0,0.0939179,"Missing"
U08-1016,J06-4003,0,0.0257373,"e 2. Our largest failures are in recall for MISC and NON, by far the broadest classes and hence difficult to capture completely. 5 Extracting and selecting sentences Wikipedia’s articles are composed using a structural markup language specific to its software. While marked-up data is available, it requires cleaning, separation into sentences and tokenisation in order to be transformed into a NER training corpus. We produce a parse tree of the markup using mwlib2 , remove most non-sentential data and all markup other than inter-article links, and split article texts into sentences using Punkt (Kiss and Strunk, 2006)—an unsupervised algorithm for sentence boundary detection, trained here on Wikipedia data—before tokenising. We need to select sentences for inclusion in our training corpus for which we are confident of having correctly labelled all named entities. For the generic NER task in English, this depends highly on capitalisation information. For instance, we simply might accept only sentences where all capitalised words have links to articles of known classification (not UNK or DAB). This criterion is overly restrictive: (a) it provides a low recall of sentences per article; (b) it is biased toward"
U08-1016,E99-1001,0,0.0261,"Missing"
U08-1016,U06-1010,1,0.824205,"Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) focused on determining superior machine learning algorithms and feature models for multilingual NER, marking tags for person (PER), organisation (ORG), location (LOC) and miscellaneous (MISC; broadly including e.g. events, artworks and nationalities). Brunstein (2002) and Sekine et al. (2002) expanded this into fine-grained categorical hierarchies; others have utilised the WordNet noun hierarchy (Miller, 1998) in a similar manner (e.g. Toral et al. (2008)). For some applications, such as biotextmining (Kim et al., 2003) or astroinformatics (Murphy et al., 2006), domain-specific entity classification schemes are more appropriate. Statistical machine learning systems have proved successful for NER. These learn terms and patterns commonly associated with particular entity classes, making use of many contextual, orthographic, linguistic and external knowledge features. They rely on annotated training corpora of newswire text, each typically smaller than a million words. The need for costly, low-yield, expert annotation therefore hinders the creation of more task-adaptable, high-performance named entity (NE) taggers. This paper presents the use of Wikipe"
U08-1016,P08-1001,0,0.261453,"Thunderball_(novel) is the ninth novel in Ian_Fleming|Ian_Fleming &apos;s James_Bond|James_Bond series. Linked article texts: Article classifications: misc. person person NE-tagged sentences: [MISC Thunderball] is the ninth novel in [PER Ian Fleming]&apos;s [PER James Bond] series. Figure 1: Deriving training sentences from Wikipedia text: sentences are extracted from articles; links to other articles are then translated to NE categories. tracted sentences containing listed entities from the web, and produced a 1.8 million word Korean corpus that gave similar results to manuallyannotated training data. Richman and Schone (2008) used a method similar to that presented here in order to derive NE-annotated corpora in languages other than English. Their approach involves classifying English Wikipedia articles and using Wikipedia’s inter-language links to infer classifications in other languages’ articles. With these classifications they automatically annotate entire articles for NER training, and suggest that their results with a 340k-word Spanish corpus are comparable to 20k-40k words of gold-standard training data. 3 From Wikipedia to NE corpora Wikipedia is a multilingual online encyclopedia written by many thousands"
U08-1016,sekine-etal-2002-extended,0,0.0202036,"Message Understanding Conferences (MUC) of the 1990s. By the final two MUC evaluations, NER had become a distinct task: tagging the aforementioned proper names and some temporal and numerical expressions (Chinchor, 1998). The C O NLL NER evaluations of 2002 and 2003 (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) focused on determining superior machine learning algorithms and feature models for multilingual NER, marking tags for person (PER), organisation (ORG), location (LOC) and miscellaneous (MISC; broadly including e.g. events, artworks and nationalities). Brunstein (2002) and Sekine et al. (2002) expanded this into fine-grained categorical hierarchies; others have utilised the WordNet noun hierarchy (Miller, 1998) in a similar manner (e.g. Toral et al. (2008)). For some applications, such as biotextmining (Kim et al., 2003) or astroinformatics (Murphy et al., 2006), domain-specific entity classification schemes are more appropriate. Statistical machine learning systems have proved successful for NER. These learn terms and patterns commonly associated with particular entity classes, making use of many contextual, orthographic, linguistic and external knowledge features. They rely on an"
U08-1016,W02-2024,0,0.0758806,"edia generally performs better than other cross-corpus train/test pairs. 1 Introduction Named Entity Recognition (NER), the task of identifying and classifying the names of people, organisations, locations and other entities within text, is central to many NLP tasks. The task developed from information extraction in the Message Understanding Conferences (MUC) of the 1990s. By the final two MUC evaluations, NER had become a distinct task: tagging the aforementioned proper names and some temporal and numerical expressions (Chinchor, 1998). The C O NLL NER evaluations of 2002 and 2003 (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) focused on determining superior machine learning algorithms and feature models for multilingual NER, marking tags for person (PER), organisation (ORG), location (LOC) and miscellaneous (MISC; broadly including e.g. events, artworks and nationalities). Brunstein (2002) and Sekine et al. (2002) expanded this into fine-grained categorical hierarchies; others have utilised the WordNet noun hierarchy (Miller, 1998) in a similar manner (e.g. Toral et al. (2008)). For some applications, such as biotextmining (Kim et al., 2003) or astroinformatics (Murphy et al.,"
U08-1016,toral-etal-2008-named,0,0.0267446,"Missing"
U08-1016,W03-0419,0,\N,Missing
U08-1019,P04-1014,1,0.80607,"2 59995 Figure 9: Number of comma rule instances before and after normalisation preventing the parser from hypothesising the attachment of commas to the left in the absorption case, reserving left-attachment analyses for the special cases of coordination and type-change structures. 6 Evaluation We wish to determine the impact of our changes in two respects: their effect on the accuracy of the resulting parser model and on parser ambiguity, relative to the unmodified parser and corpus. To measure the impact of the modifications on parser accuracy, we performed the dependencybased evaluation of Clark and Curran (2004) on the standard development and test sets, sections 00 and 23 of CCGbank. We evaluate the performance of a parser and model by computing the F-score over the obtained set of predicate-argument dependencies, relative to a gold standard. In the labelled CCG evaluation, a dependency consists of a tuple ⟨H,C, i, D, l⟩, where H is the head lexical item, C is its category with its arguments annotated with indices, i is the index of the argument satisfied by this dependency, and D is the lexical item satisfying this dependency. l is a flag distinguishing local from long-range dependencies. The unlab"
U08-1019,J07-4004,1,0.932252,"two categories into another category. Combinatory rules enable succinct, natural CCG analyses for difficult constructions such as argument cluster coordination and parasitic gaps (Steedman, 2000). I quickly grilled steak . the NP (SNP)/(SNP) (SNP)/NP NP/N NP SNP SNP N . &gt; &gt; < < S S Figure 1: CCG bank CCG analysis is a wide-coverage CCG corpus, generated from the Wall Street Journal section of Penn Treebank through an automatic conversion procedure described by Hockenmaier and Steedman (2007). CCGbank enabled the development of widecoverage statistical CCG parsers such as the C & C parser Clark and Curran (2007), on which we will evaluate this work. Parsing in the C & C parser proceeds in two broad stages: a maximum entropy supertagger assigns categories to lexical items, which the parser then attempts to combine using the CCG rules. The corpus, as a collection of CCG derivations, implicitly encodes the set of CCG rules used in its derivations. These rules must be explicitly encoded in a parser trained on CCGbank, since the training process requires it to reproduce the derivations represented in the corpus. The goal of this work is to drastically reduce the number of rules implicitly encoded by CCGba"
U08-1019,W07-2206,1,0.929642,"o inform its chunking decisions (in this context, the segmentation of input text into sentences, robustly handling non-terminal uses of periods, for example). In Briscoe’s work, an implementation of text grammar can eliminate incorrect parses based on punctuation. In Example 1, a variation on the well-known PP attachment problem, a parser which simply ignores punctuation in the input will encounter ambiguity which a punctuation-aware parser will not. (1) I saw the girl on the hill with the telescope, from the store. (2) I saw the girl on the hill with the telescope from the store. The work of Djordjevic et al. (2007) for the C & C parser constrains phrases delimited by hyphens, colons and semicolons to be complete constituents, allowing the parser to avoid great amounts of attachment ambiguity when parsing long sentences. However, while Djordjevic et al. modify the parser to derive information from punctuation in the corpus, this work changes the representation of commas in the corpus to convey information on each comma’s role to the parser. The corpus we obtain assigns a uniform structure to each of the comma’s syntactic roles. 3 CCG and CCGbank In Combinatory Categorial Grammar (Steedman, 2000), each le"
U08-1019,J07-3004,0,0.285932,"plying combinatory rules. In Figure 1, each line denotes the application of a particular combinatory rule, which combines at most two categories into another category. Combinatory rules enable succinct, natural CCG analyses for difficult constructions such as argument cluster coordination and parasitic gaps (Steedman, 2000). I quickly grilled steak . the NP (SNP)/(SNP) (SNP)/NP NP/N NP SNP SNP N . &gt; &gt; < < S S Figure 1: CCG bank CCG analysis is a wide-coverage CCG corpus, generated from the Wall Street Journal section of Penn Treebank through an automatic conversion procedure described by Hockenmaier and Steedman (2007). CCGbank enabled the development of widecoverage statistical CCG parsers such as the C & C parser Clark and Curran (2007), on which we will evaluate this work. Parsing in the C & C parser proceeds in two broad stages: a maximum entropy supertagger assigns categories to lexical items, which the parser then attempts to combine using the CCG rules. The corpus, as a collection of CCG derivations, implicitly encodes the set of CCG rules used in its derivations. These rules must be explicitly encoded in a parser trained on CCGbank, since the training process requires it to reproduce the derivations"
U08-1019,U07-1022,1,0.813215,"ing decisions (in this context, the segmentation of input text into sentences, robustly handling non-terminal uses of periods, for example). In Briscoe’s work, an implementation of text grammar can eliminate incorrect parses based on punctuation. In Example 1, a variation on the well-known PP attachment problem, a parser which simply ignores punctuation in the input will encounter ambiguity which a punctuation-aware parser will not. (1) I saw the girl on the hill with the telescope, from the store. (2) I saw the girl on the hill with the telescope from the store. The work of Djordjevic et al. (2007) for the C & C parser constrains phrases delimited by hyphens, colons and semicolons to be complete constituents, allowing the parser to avoid great amounts of attachment ambiguity when parsing long sentences. However, while Djordjevic et al. modify the parser to derive information from punctuation in the corpus, this work changes the representation of commas in the corpus to convey information on each comma’s role to the parser. The corpus we obtain assigns a uniform structure to each of the comma’s syntactic roles. 3 CCG and CCGbank In Combinatory Categorial Grammar (Steedman, 2000), each le"
U08-1019,J93-2004,0,\N,Missing
U09-1003,P91-1022,0,0.545163,"Missing"
U09-1003,J96-2004,0,0.116027,"(135,537 pairs, lagmax = 7 days). Percentage of linked documents for each source. should pick out the individual stages of the overarching process. Moreover, the minimality constraint encourages conceptual clarity and mirrors the way information is released piecemeal while still allowing later aggregation of clusters if required. Inter-annotator agreement for our five annotators is assessed using Cohen’s Kappa over the shared task of 42 screens. Table 4 shows that acceptable Kappa scores are achieved for LINK and FIRST with borderline Kappa scores for BACK– relative to the threshold at 0.67 (Carletta, 1996). ANLY was annotated with lower agreement and is consistent with annotator feedback during scheme development, where ANLY links were the most difficult to disambiguate from BACK since the distinction between existing and new information proved subjective. We placed an upper bound of a week on the time lag between ASX and RNA publishing time. Though primarily an optimisation step to reduce the number of pairs for comparison, it is also consistent with the cluster minimality constraint; annotators were encouraged to split clusters that spanned too long a time. Table 5 shows the count of links (w"
U09-1003,M95-1004,0,0.0624352,"s were used as baseline features and those scores were exceeded for all link types. While higher F-scores were achieved, for the most part, in the tasks with higher prior link probabilities, scores in ANLY were surprisingly high given its low prior of 4.9. Table 8 shows the best performing (by F-score) feature combinations for each link type. To test the contribution of each feature, subtractive analysis was performed on the best performing feature set for each link type. An experiment is conducted that uses all but one feature and the results compared to best using approximate randomisation (Chinchor, 1995) to assess whether adding the omitted feature results in a statistically significant improvement.7 Features used are marked with ·, while features are marked with ? or ?? if their removal results in significantly worse F-score (at p<0.05 and p<0.01 respectively). The first observation to make from the table is that the tasks can be separated into two groups on the set of features that was most successful: LINK/BACK and FIRST / ANLY , though this may also be related to the different prior link probabilities, higher and lower for each group in this case. Features based on the text play perhaps t"
U09-1003,P02-1020,0,0.731086,"Missing"
U09-1003,P91-1023,0,0.379225,"Missing"
U09-1003,W99-0625,0,0.054621,"Missing"
U09-1003,J06-4003,0,0.0307294,"bution link type appear without another (they all co-occur with LINK) , indicating that the annotators applied them with a high degree of overlap. 6 Features We model the information flow problems using a variety of text similarity and temporal features extracted from the ASX - RNA pairs. Each feature value is binary and real-valued features are placed into equally sized bins (with the exception of lag as mentioned below). The text and title of the announcement and story were both tokenised using the NLTK’s word tokeniser (Bird et al., 2009) and implementation of the Punkt sentence tokeniser (Kiss and Strunk, 2006). Unigram and bigram features are extracted, ignoring punctuation and any n-grams that include a token in NLTK’s list of 127 English stopwords. To model fine-grained textual similarity, we define three set-theoretic classes of bag-of-words features depending on where content is found: intersection (ASX ∩ RNA), only in the announcement (ASX  RNA) and only in the story (RNA  ASX). These methods are applied to unigrams and bigrams in the title and body text of the ASX - RNA pair. The intersection text/title features are used for a baseline approach. The settheoretic features are mainly designed"
U09-1004,biemann-etal-2008-asv,0,0.0223259,"Missing"
U09-1004,W03-0424,1,0.829737,"row), river, saints and various British tanks from WW1 of the same name. Most disambiguation pages are children of a category of D ISAMBIGUATION, many ¨ have the word Disambiguation or Begriffsklarung in the title, and further information is available in the form of disambiguation templates. 4.3 Bootstrapped features For general classification, we extracted features from articles, which may each be mapped to an entity class. These mappings are produced by the bootstrapping process. Category nouns Head nouns from Wikipedia category titles for both English and German, extracted using C&C tools (Curran and Clark, 2003) in English and the Tree-Tagger in German (Schmid, 1995) to POS-tag and chunk the sentences. In English, the category feature only applied to plural head nouns (and bigrams thereof) following Suchanek et al.’s (2007) suggestion that these best represent ontology. Differences in both language and the structure of the German Wikipedia project invalidate this approach in German: conceputal categories are not plural, and forms that are bigrams in English are generally compound nouns. Hence we experimented with ASV toolbox (Chris Biemann and Holz, 2008) to extract a head morpheme. This allows P REM"
U09-1004,I08-1071,0,0.476716,"methods of language-resource generation which is independent of existing data sets is an important and challenging goal. We work towards generating high-coverage training corpora which can be used for a range of German NLP. 3 Data To learn a classification of German Wikipedia articles, we labelled a corpus of English Wikipedia articles. Wikipedia’s inter-language links allow us to then develop classifiers for all articles in English and German (or other language) Wikipedias. We use XML dumps of Wikipedia from March 2009 for both languages. 3.1 Article selection Both Nothman et al. (2008) and Dakka and Cucerzan (2008) have labelled collections of Wikipedia articles with gold standard classificaRank 1 2 3 4 5 6 7 8 9 10 Article 2008 Summer Olympics Wiki Sarah Palin Michael Phelps YouTube Bernie Mac Olympic Games Joe Biden Georgia (country) The Dark Knight (film) Pageviews 4 437 251 4 030 068 4 004 853 3 476 803 2 685 316 2 013 775 2 003 678 1 966 877 1 757 967 1 427 277 Table 1: Most frequently viewed Wikipedia articles from August 2008, retrieved from http://stats.grok.se Rank 1 2 3 4 5 Title United States Australia Wikipedia Association Football France Inlinks 543 995 344 969 272 073 241 514 227 464 Table"
U09-1004,W03-0425,0,0.109506,"Missing"
U09-1004,D07-1073,0,0.199109,"Missing"
U09-1004,X96-1049,0,0.135686,"Missing"
U09-1004,U08-1016,1,0.928108,"Developing more automated methods of language-resource generation which is independent of existing data sets is an important and challenging goal. We work towards generating high-coverage training corpora which can be used for a range of German NLP. 3 Data To learn a classification of German Wikipedia articles, we labelled a corpus of English Wikipedia articles. Wikipedia’s inter-language links allow us to then develop classifiers for all articles in English and German (or other language) Wikipedias. We use XML dumps of Wikipedia from March 2009 for both languages. 3.1 Article selection Both Nothman et al. (2008) and Dakka and Cucerzan (2008) have labelled collections of Wikipedia articles with gold standard classificaRank 1 2 3 4 5 6 7 8 9 10 Article 2008 Summer Olympics Wiki Sarah Palin Michael Phelps YouTube Bernie Mac Olympic Games Joe Biden Georgia (country) The Dark Knight (film) Pageviews 4 437 251 4 030 068 4 004 853 3 476 803 2 685 316 2 013 775 2 003 678 1 966 877 1 757 967 1 427 277 Table 1: Most frequently viewed Wikipedia articles from August 2008, retrieved from http://stats.grok.se Rank 1 2 3 4 5 Title United States Australia Wikipedia Association Football France Inlinks 543 995 344 969"
U09-1004,E09-1070,1,0.870507,"Missing"
U09-1004,U09-1015,1,0.744683,"Missing"
U09-1004,W02-2024,0,0.0314124,"Missing"
U09-1004,W06-2809,0,0.0843115,"Missing"
U09-1004,D07-1068,0,0.0626911,"Missing"
U09-1004,P08-1001,0,\N,Missing
U09-1004,W03-0419,0,\N,Missing
U09-1009,E99-1025,0,0.0913013,"Missing"
U09-1009,W02-2236,0,0.060454,"Missing"
U09-1009,W03-1013,1,0.677757,"te in particular the change of tag for ‘with’ in the two examples and its affect on the subsequent rule applications. The decision made by the supertagger effectively decides which analysis will be found, or if both are provided the parser must consider more possible derivations. I ate pizza with cutlery NP (S NP )/NP NP ((S NP )(S NP ))/NP S NP > NP > (S NP )(S NP ) < S NP < S I ate pizza with anchovies NP (S NP )/NP NP (NP NP )/NP NP NP NP S NP S NP > < > < Figure 1: Two CCG derivations with PP ambiguity. The CCG parser and associated supertagger we have used is the C&C parser (Clark and Curran, 2003; Clark and Curran, 2007b). The supertagger applies categories to words using the forward backward algorithm, and the parser forms a derivation by applying the Cocke–Younger– Kasami (CKY) chart parsing algorithm (Younger, 1967; Kasami, 1967) and dynamic programming. 2.1 Supertagging Supertags were first proposed by Joshi and Bangalore (1994) for Lexicalized Tree-Adjoining Grammar (LTAG). Like POS tags, supertags are assigned to each word in the sentence prior to parsing, but supertags contain much more detailed syntactic information. This leads to tag sets that are up to two orders of magnitud"
U09-1009,W07-1202,1,0.831586,"nge of tag for ‘with’ in the two examples and its affect on the subsequent rule applications. The decision made by the supertagger effectively decides which analysis will be found, or if both are provided the parser must consider more possible derivations. I ate pizza with cutlery NP (S NP )/NP NP ((S NP )(S NP ))/NP S NP > NP > (S NP )(S NP ) < S NP < S I ate pizza with anchovies NP (S NP )/NP NP (NP NP )/NP NP NP NP S NP S NP > < > < Figure 1: Two CCG derivations with PP ambiguity. The CCG parser and associated supertagger we have used is the C&C parser (Clark and Curran, 2003; Clark and Curran, 2007b). The supertagger applies categories to words using the forward backward algorithm, and the parser forms a derivation by applying the Cocke–Younger– Kasami (CKY) chart parsing algorithm (Younger, 1967; Kasami, 1967) and dynamic programming. 2.1 Supertagging Supertags were first proposed by Joshi and Bangalore (1994) for Lexicalized Tree-Adjoining Grammar (LTAG). Like POS tags, supertags are assigned to each word in the sentence prior to parsing, but supertags contain much more detailed syntactic information. This leads to tag sets that are up to two orders of magnitude larger. The first supe"
U09-1009,J07-4004,1,0.74932,"nge of tag for ‘with’ in the two examples and its affect on the subsequent rule applications. The decision made by the supertagger effectively decides which analysis will be found, or if both are provided the parser must consider more possible derivations. I ate pizza with cutlery NP (S NP )/NP NP ((S NP )(S NP ))/NP S NP > NP > (S NP )(S NP ) < S NP < S I ate pizza with anchovies NP (S NP )/NP NP (NP NP )/NP NP NP NP S NP S NP > < > < Figure 1: Two CCG derivations with PP ambiguity. The CCG parser and associated supertagger we have used is the C&C parser (Clark and Curran, 2003; Clark and Curran, 2007b). The supertagger applies categories to words using the forward backward algorithm, and the parser forms a derivation by applying the Cocke–Younger– Kasami (CKY) chart parsing algorithm (Younger, 1967; Kasami, 1967) and dynamic programming. 2.1 Supertagging Supertags were first proposed by Joshi and Bangalore (1994) for Lexicalized Tree-Adjoining Grammar (LTAG). Like POS tags, supertags are assigned to each word in the sentence prior to parsing, but supertags contain much more detailed syntactic information. This leads to tag sets that are up to two orders of magnitude larger. The first supe"
U09-1009,W03-0407,1,0.837372,"re accurate it can further constrain the set of possible derivations by supplying fewer categories, leaving the parser with less to do. One means of improving the supertagger’s statistical model of language is to provide more evidence, in this case, more annotated text. However, creating a significant amount of extra gold standard annotated text is not feasible. An alternative approach is ‘semi-supervised training’, in which a small set of annotated data and a much larger set of unannotated data is used. Training a system directly on its own output, ‘self-training’, is not normally effective (Clark et al., 2003), but recently McClosky et al. (2006) demonstrated that parser output can be made useful for retraining by the application of a reranker. To enable the use of more training data we have parallelised the C&C parser’s supertagger training process and implemented perceptron–based algorithms for parameter estimation. In the process of this work we also modified the C&C parser’s use of a particular CCG rule, based on observations of its behaviour. Our unlabeled training data was part of the English section of Wikipedia, consisting of 47 million sentences. We used the C&C parser to label the data wi"
U09-1009,W02-2203,0,0.055938,"Missing"
U09-1009,P04-1015,0,0.0563999,"Missing"
U09-1009,W02-1001,0,0.152541,"Missing"
U09-1009,P96-1011,0,0.0690988,"xcess Backward Composition In the process of debugging the parser, we investigated the number of times particular pairs of categories were combined. We were surprised to discover that a very large number of backward compositions were being performed in the chart, even though backward composition rarely occurred in the parser output (or in the gold standard itself). Backward composition is normally used for non-constituent coordination between pairs of type-raised categories, but the parser was also using it for combining non-type-raised and typeraised categories. This is an instance where the Eisner (1996) normal form constraints have failed to stop non-normal form derivations, because Eisner’s constraints were not designed to work with type-raising. We added a constraint that only allows backward composition to occur if both children are type-raised. 4 Methodology 4.1 Data Evaluation has been performed using Section 00 of CCGBank, a translation of the Penn Treebank to CCG (Hockenmaier, 2003). Sections 02-21 were used as training data and are simply referred to as WSJ in the following section. The raw Wikipedia data was tokenised using Punkt (Kiss and Strunk, 2006) and the NLTK tokeniser (Bird"
U09-1009,C94-1024,0,0.0714572,"re of sentences. However, when they are the bottleneck in the data acquisition phase of a system simple solutions are to use less data, or not use a parser at all. If we can improve the speed of parsers this will be unnecessary. For lexicalised grammars such as Combinatory Categorial Grammar (CCG) (Steedman, 2000) the step in which words are labelled with lexical categories has great influence on parsing speed and accuracy. In these formalisms, the labels chosen constrain the set of possible derivations so much that the process of choosing them, supertagging, is described as ‘almost parsing’ (Joshi and Bangalore, 1994). If the supertagger is more accurate it can further constrain the set of possible derivations by supplying fewer categories, leaving the parser with less to do. One means of improving the supertagger’s statistical model of language is to provide more evidence, in this case, more annotated text. However, creating a significant amount of extra gold standard annotated text is not feasible. An alternative approach is ‘semi-supervised training’, in which a small set of annotated data and a much larger set of unannotated data is used. Training a system directly on its own output, ‘self-training’, i"
U09-1009,J06-4003,0,0.0133171,"egories. This is an instance where the Eisner (1996) normal form constraints have failed to stop non-normal form derivations, because Eisner’s constraints were not designed to work with type-raising. We added a constraint that only allows backward composition to occur if both children are type-raised. 4 Methodology 4.1 Data Evaluation has been performed using Section 00 of CCGBank, a translation of the Penn Treebank to CCG (Hockenmaier, 2003). Sections 02-21 were used as training data and are simply referred to as WSJ in the following section. The raw Wikipedia data was tokenised using Punkt (Kiss and Strunk, 2006) and the NLTK tokeniser (Bird et al., 2009), and parsed using the C&C parser and models version 1.022 . The WSJ sentences had an average length of 23.5 words and a variance of 122.0 while the Wikipedia sentences had an average length of 21.7 words and a variance of 151.0. 2 http://svn.ask.it.usyd.edu.au/trac/candc 4.2 Evaluation For a given beta level the number of categories assigned to each word by the supertagger will vary greatly between models. This presents a problem because as described in Section 3 the number of categories assigned has a large influence on parsing speed and accuracy. T"
U09-1009,N06-1020,0,0.0325602,"in the set of possible derivations by supplying fewer categories, leaving the parser with less to do. One means of improving the supertagger’s statistical model of language is to provide more evidence, in this case, more annotated text. However, creating a significant amount of extra gold standard annotated text is not feasible. An alternative approach is ‘semi-supervised training’, in which a small set of annotated data and a much larger set of unannotated data is used. Training a system directly on its own output, ‘self-training’, is not normally effective (Clark et al., 2003), but recently McClosky et al. (2006) demonstrated that parser output can be made useful for retraining by the application of a reranker. To enable the use of more training data we have parallelised the C&C parser’s supertagger training process and implemented perceptron–based algorithms for parameter estimation. In the process of this work we also modified the C&C parser’s use of a particular CCG rule, based on observations of its behaviour. Our unlabeled training data was part of the English section of Wikipedia, consisting of 47 million sentences. We used the C&C parser to label the data with supertags, producing training data"
U09-1009,W00-1605,0,0.0711674,"Missing"
U09-1009,N01-1023,0,0.0607919,"Missing"
U09-1009,P95-1026,0,0.35424,"Missing"
U09-1009,W09-3832,0,0.0404103,"Missing"
U09-1010,J99-2004,0,0.163036,"e slashes indicate the directionality of arguments, stating that an NP object is expected to the right, and an NP subject is expected to the left. An example CCG derivation containing the transitive verb like is: I like the cat NP (S [dcl ]NP )/NP NP [nb]/N N NP [nb] S [dcl ]NP S [dcl ] > < < This derivation uses the rules of forward and backward application to build the representation of the sentence. Most of the information is contained in the lexical categories. 2.2 The C&C Parser The C&C parser makes use of this property of CCG by dividing the parsing problem into two phases, following (Bangalore and Joshi, 1999). First, a supertagger proposes a set of likely categories for each token in the sentence. The parser then attempts to build a spanning analysis from the proposed categories, using the modified CKY algorithm described in Steedman (2000). The supertagging phase dramatically reduces the search space the parser must explore, making the C&C parser very efficient. 3 Motivation It is important to note that the concepts motivating this paper could be applied to any grammar formalism. However, our experiments were conducted using CCG and the C&C parser for a number of reasons, which are outlined throu"
U09-1010,P04-1014,1,0.814539,"F-score nor parse time was observed. 1 Introduction Natural language parsing is the task of assigning syntactic structure to text. Initial parsing research mostly relied on manually constructed grammars. Statistical parsers have been able to achieve high accuracy since the creation of the Penn Treebank (Marcus et al., 1993); a corpus of Wall Street Journal text used for training. Statistical parsers are typically inefficient, parsing only a few sentences per second on standard hardware (Kaplan et al., 2004). There has been substantial progress on addressing this issue over the last few years. Clark and Curran (2004) presented a statistical CCG parser, C&C, which was an order of magnitude faster than those analysed in Kaplan et al. (2004). However the C&C parser is still limited to around 25 sentences per second. This paper investigates whether the speed of statistical parsers can be improved using a novel form of caching. Currently, parsers treat each sentence independently, despite the fact that some phrases are constantly reused. We propose to store analyses for common phrases, instead of re-computing their syntactic structure each time the parser encounters them. Our first idea was to store a single,"
U09-1010,J07-4004,1,0.85331,"ssful when the caching process is performed using a data set. 2 Background We are interested in storing the parse structure for common n-grams, so that the analysis can be reused across multiple sentences. In a way, this is an extension of an important innovation in parsing: the CKY chart parsing algorithm (Younger, 1967). Our proposal is an attempt to memoise sections of the chart across multiple sentences. Most constituency parsers use some form of chart for constructing a derivation, so our investigation could have begun with a number of different parsers. We decided to use the C&C parser (Clark and Curran, 2007) for the following reasons. First, the aim of the caching we are proposing is to improve the speed of a parser. It makes sense to look at a parser that has already been optimised, to ensure that we do not demonstrate an improvement that could have been achieved using a much simpler solution. Secondly, there are aspects of the parser’s grammar formalism, Combinatory Categorial Grammar, that are relevant to the issues we want to consider. 2.1 Chart Parsing The chart is a triangular hierarchical structure used for storing the nodes in a parse tree, as seen in Figure 1. A chart for a sentence cons"
U09-1010,P96-1011,0,0.053197,"ame forwards composed derivation of the bigram of the as described earlier for the bigram on the, the wrong analysis would be constructed. X NP on the (NP NP )/N NP NP NP king of England N NP NP > < < NP While an NP was still the resultant overall category assigned to the phrase, the internal noun phrases are incorrect; the named entity the king of England is not represented within this incorrect derivation. From an implementation point of view, being able to construct and use this forward composed parse structure for of the involves violating one of the normal-form constraints proposed in Eisner (1996) to eliminate CCG’s “spurious ambiguity”. The constraint which was violated states that the left child of forward application cannot be the result of forward composition, as is the case in our previous example. The C&C parser implements these Eisner constraints, and as such a special rule was added to the parser to allow any chart structures which were loaded from a pre-constructed database to violate the Eisner constraints. 4.2.2 Coordination In CCG parsing, commas can be parsed in one of two ways depending on their semantic role in the sentence. They are either used for coordination or they"
U09-1010,J07-3004,0,0.0369865,"l parsing time, as well as the accuracy of the resultant derivations. The accuracy is measured in terms of F-score values for both labelled and unlabelled dependencies when evaluated against the predicate-argument dependencies in CCGbank (Clark and Hockenmaier, 2002). The parsing times reported do not include the time to load the grammar, statistical models, or our database. 6 Implementation 6.1 Data The models used by the C&C parser for our experiments were trained using two different corpora. The WSJ models were trained using the CCG version of the Penn Treebank, CCGbank (Hockenmaier, 2003; Hockenmaier and Steedman, 2007), which is available from the Linguistic Data Consortium1 . The second corpus is a version of CCGbank where the noun phrase bracketing has been corrected (Vadas and Curran, 2008; Vadas, 2009). 6.2 Tokyo Cabinet Tokyo Cabinet2 is an open source, lightweight database API which provides a number of different database implementations, including a hash database, B+ tree, and a fixed-length key database. Our experiments used Tokyo Cabinet to store the pre-constructed n-grams because of its ease of use, speed, and maximum database size (8EB). A large 1 2 http://ldc.upenn.edu/ http://tokyocabinet.sour"
U09-1010,N04-1013,0,0.031573,"s over WSJ sections 02 to 21 and evaluating on section 00, a preliminary result of no significant change in F-score nor parse time was observed. 1 Introduction Natural language parsing is the task of assigning syntactic structure to text. Initial parsing research mostly relied on manually constructed grammars. Statistical parsers have been able to achieve high accuracy since the creation of the Penn Treebank (Marcus et al., 1993); a corpus of Wall Street Journal text used for training. Statistical parsers are typically inefficient, parsing only a few sentences per second on standard hardware (Kaplan et al., 2004). There has been substantial progress on addressing this issue over the last few years. Clark and Curran (2004) presented a statistical CCG parser, C&C, which was an order of magnitude faster than those analysed in Kaplan et al. (2004). However the C&C parser is still limited to around 25 sentences per second. This paper investigates whether the speed of statistical parsers can be improved using a novel form of caching. Currently, parsers treat each sentence independently, despite the fact that some phrases are constantly reused. We propose to store analyses for common phrases, instead of re-c"
U09-1010,J93-2004,0,0.0335081,"parser does not have to re-derive the parse structure for these n-grams when they occur. Instead, their preconstructed analysis can be reused. By generating these pre-constructed databases over WSJ sections 02 to 21 and evaluating on section 00, a preliminary result of no significant change in F-score nor parse time was observed. 1 Introduction Natural language parsing is the task of assigning syntactic structure to text. Initial parsing research mostly relied on manually constructed grammars. Statistical parsers have been able to achieve high accuracy since the creation of the Penn Treebank (Marcus et al., 1993); a corpus of Wall Street Journal text used for training. Statistical parsers are typically inefficient, parsing only a few sentences per second on standard hardware (Kaplan et al., 2004). There has been substantial progress on addressing this issue over the last few years. Clark and Curran (2004) presented a statistical CCG parser, C&C, which was an order of magnitude faster than those analysed in Kaplan et al. (2004). However the C&C parser is still limited to around 25 sentences per second. This paper investigates whether the speed of statistical parsers can be improved using a novel form o"
U09-1010,P08-1039,1,0.797687,"inst the predicate-argument dependencies in CCGbank (Clark and Hockenmaier, 2002). The parsing times reported do not include the time to load the grammar, statistical models, or our database. 6 Implementation 6.1 Data The models used by the C&C parser for our experiments were trained using two different corpora. The WSJ models were trained using the CCG version of the Penn Treebank, CCGbank (Hockenmaier, 2003; Hockenmaier and Steedman, 2007), which is available from the Linguistic Data Consortium1 . The second corpus is a version of CCGbank where the noun phrase bracketing has been corrected (Vadas and Curran, 2008; Vadas, 2009). 6.2 Tokyo Cabinet Tokyo Cabinet2 is an open source, lightweight database API which provides a number of different database implementations, including a hash database, B+ tree, and a fixed-length key database. Our experiments used Tokyo Cabinet to store the pre-constructed n-grams because of its ease of use, speed, and maximum database size (8EB). A large 1 2 http://ldc.upenn.edu/ http://tokyocabinet.sourceforge.net/ George , N the of N (NP NP )/NP , NP [nb]/N NP England , king NP [nb] , (S NP )/NP N > owned the company NP [nb]/N N NP NP [nb] > NP NP S NP > > < NP [nb] <Φ> N"
U09-1012,W02-1011,0,0.0167808,"Missing"
U09-1015,P08-1092,0,0.0408615,"Missing"
U09-1015,I08-1071,0,0.130969,"e due to fundamental conflicts in the category hierarchy that could not be resolved. This set of handlabelled articles will be released after publication. 4 Features for text categorisation Our baseline system used a simple bag-of-words including tokens from the entire article body and the article title. This did not include tokens that appear in templates used in the generation of an article. We then experimented with a number of different feature extraction methods, focusing primarily on the document structure for identifying useful features. Tokens in the first paragraph were identified by Dakka and Cucerzan (2008) as useful features for a machine learner, an idea stemming from the fact that most human annotators will recognise an article’s category after reading just the first paragraph. We extended this idea by also marking the first sentence and title tokens as separate from other tokens, as we found that often the first sentence was all that was required for a human annotator to classify an article. We ran experiments limiting the feature space to these smaller portions of the document. Wikipedia articles often have a large amount of metadata that helps in identifying an article’s category, in parti"
U09-1015,J06-4003,0,0.0205082,"ube, 2007; Hu et al., 2008; Biadsy et al., 2008), focusing on the extraction of features from Wikipedia’s rich metadata. 3 Data Our annotation and experiments were all run on a March 2009 dump of Wikipedia. The mwlib1 library 1 http://code.pediapress.com Example PER Fictional Animal Popeye Chupacabra ORG Band Blink-182 LOC Geological Himalayas MISC Franchise Product → Software Star Wars Python Table 1: Extensions to the BBN categories with examples was used to parse the Mediawiki markup and perform tasks such as expanding Wikipedia templates and extracting article categories and links. Punkt (Kiss and Strunk, 2006) and the NLTK (Loper and Bird, 2002) were used to tokenise the corpus. 3.1 Annotation scheme Annotation was performed under a slightly modified BBN category hierarchy (Brunstein, 2002). During annotation we discovered the need for a number of additional categories due to the large number of articles Wikipedia contains relating to popular culture, for example the new categories Organisation → Band and M isc → W ork of Art → T V Series were quite common. We map these categories back to the “Other” subcategory of their parent category to allow accurate comparison with the original BBN scheme. Tab"
U09-1015,W02-0109,0,0.055583,"al., 2008), focusing on the extraction of features from Wikipedia’s rich metadata. 3 Data Our annotation and experiments were all run on a March 2009 dump of Wikipedia. The mwlib1 library 1 http://code.pediapress.com Example PER Fictional Animal Popeye Chupacabra ORG Band Blink-182 LOC Geological Himalayas MISC Franchise Product → Software Star Wars Python Table 1: Extensions to the BBN categories with examples was used to parse the Mediawiki markup and perform tasks such as expanding Wikipedia templates and extracting article categories and links. Punkt (Kiss and Strunk, 2006) and the NLTK (Loper and Bird, 2002) were used to tokenise the corpus. 3.1 Annotation scheme Annotation was performed under a slightly modified BBN category hierarchy (Brunstein, 2002). During annotation we discovered the need for a number of additional categories due to the large number of articles Wikipedia contains relating to popular culture, for example the new categories Organisation → Band and M isc → W ork of Art → T V Series were quite common. We map these categories back to the “Other” subcategory of their parent category to allow accurate comparison with the original BBN scheme. Table 1 lists some of our new categorie"
U09-1015,U06-1010,1,0.882575,"Missing"
U09-1015,E09-1070,1,0.74499,"and the travel industry (Vijayakrishna and Sobha, 2008). We also extend the broad scheme with a DAB category for Wikipedia “disambiguation” pages — pages used to group articles with identical titles. NER systems that categorise NEs under these schemes require a large amount of highly accurate training data to perform well at the task. Expert annotation is time consuming and expensive, so there is an imperative to generate this data automatically. Wikipedia is emerging as a significant resource due to its immense size and rich structural information, such as its link structure. Nothman et al. (2009) introduced a novel approach to exploiting Wikipedia’s internal structure to produce training data for NER systems. Their process involved an initial step of categorising all Wikipedia articles using a simple heuristic-based bootstrapping algorithm. Potential NEs were then identified as the words in an article’s text that served as links to other Wikipedia articles. To label a NE they then used the category assigned to the article that it linked to. We have explored the use of Na¨ıve Bayes (NB) and support vector machines (SVMs) as replacements for the text categorisation approach taken by Not"
U09-1015,U09-1004,1,0.743769,"Missing"
U09-1015,sekine-etal-2002-extended,0,0.601675,"Missing"
U09-1015,I08-5009,0,0.0325546,"Missing"
U09-1015,P06-4018,0,\N,Missing
U09-1017,W02-2001,0,0.109497,"sound. 1 Introduction Multiword expressions (MWEs), compound lexemes made up of two or more words that together form a complete semantic unit, are one of the problems facing natural language processing systems. Verb-particle constructions (VPCs) are a common type of MWE, comprising a verb and a particle, most often a preposition. The meaning of some VPCs can be logically attributed to the component parts (e.g., picked out), but many are idiomatic and semantically opaque (e.g., make out). Previous research into VPCs has focussed much attention on their automatic extraction and classification (Baldwin and Villavicencio, 2002; Villavicencio, 2003). However, research into how they should be handled by parsers is noticeably lacking. Their unusual ability to manifest in both a ‘joined’ and ‘split’ configuration (‘gunned down the man’ versus ‘gunned the man down’) prevents parsers from treating them as a single unit, and demands a system that is able to maintain the semantic bond between the components, even when they are nonadjacent. To compound the problem, existing corpora are not consistent in their handling of these constructions. The Penn Treebank (Marcus et al., 1993, 1994) has an RP tag for particles, but some"
U09-1017,J07-4004,1,0.828937,"particles varies, but leans towards treating all particles as adverbial modifiers. This is in itself problematic, since it fails to take into account the fact that particles are a core part of the construct, whereas adverbs are optional. This lack of quality corpora for VPC-related work limits the power of corpustrained parsers. In this paper we draw on the Penn Treebank and PropBank (Kingsbury and Palmer, 2003) to repair CCGbank’s representation of VPCs, and demonstrate how our approach is able to satisfactorily account for most VPC-related phenomena. Retraining the Clark and Curran parser (Clark and Curran, 2007) on our modified corpus, we observe a very slight decrease in parser F-score, although this is balanced by the fact that the parses now make structural sense. 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (CCG, Steedman (2000)) is a lexicalised grammar formalism based on combinatory logic. One of the features that makes CCG so appealing to NLP researchers is its high degree of lexicalisation (i.e., the degree to which the grammar is built into the lexicon). Every word is assigned a category, and parsing is simply a matter of finding the right sequence of combinators to form a"
U09-1017,J07-3004,0,0.071188,"nto how they should be handled by parsers is noticeably lacking. Their unusual ability to manifest in both a ‘joined’ and ‘split’ configuration (‘gunned down the man’ versus ‘gunned the man down’) prevents parsers from treating them as a single unit, and demands a system that is able to maintain the semantic bond between the components, even when they are nonadjacent. To compound the problem, existing corpora are not consistent in their handling of these constructions. The Penn Treebank (Marcus et al., 1993, 1994) has an RP tag for particles, but sometimes labels them as adverbs. The CCGbank (Hockenmaier and Steedman, 2007) analysis of particles varies, but leans towards treating all particles as adverbial modifiers. This is in itself problematic, since it fails to take into account the fact that particles are a core part of the construct, whereas adverbs are optional. This lack of quality corpora for VPC-related work limits the power of corpustrained parsers. In this paper we draw on the Penn Treebank and PropBank (Kingsbury and Palmer, 2003) to repair CCGbank’s representation of VPCs, and demonstrate how our approach is able to satisfactorily account for most VPC-related phenomena. Retraining the Clark and Cur"
U09-1017,kingsbury-palmer-2002-treebank,0,0.0920914,"tage in our process was modifying CCGbank to accommodate the changes. This involved changing both the syntactic derivations and the word-word dependencies in the predicateargument structure. The details of the structure of CCGbank can be found in Hockenmaier and Steedman (2005). To simplify the manipulation of the CCG structures, we first read them into Python as treestructures, and then wrote these to an external database1 . This gave us quite a lot of flexibility in querying, retrieving and modifying the structures. To locate VPCs within the corpus, we relied on a combination of PropBank’s (Kingsbury and Palmer, 2002) argument structure labeling and the tags in the Penn Treebank. PropBank provides a listing of every verb (relation) in the corpus, along with its arguments. The word positions for each relation and its arguments are also given, making multiword relations (such as VPCs) readily identifiable. Whenever a multiword relation was found that also contained an RP tag in the Penn Treebank (RP being the Penn Treebank’s tag for particles), we took that set of words as being a VPC. This approach errs on the side of caution — there are some valid VPCs in the Penn Treebank that do not have the particle tag"
U09-1017,J93-2004,0,0.036849,"c extraction and classification (Baldwin and Villavicencio, 2002; Villavicencio, 2003). However, research into how they should be handled by parsers is noticeably lacking. Their unusual ability to manifest in both a ‘joined’ and ‘split’ configuration (‘gunned down the man’ versus ‘gunned the man down’) prevents parsers from treating them as a single unit, and demands a system that is able to maintain the semantic bond between the components, even when they are nonadjacent. To compound the problem, existing corpora are not consistent in their handling of these constructions. The Penn Treebank (Marcus et al., 1993, 1994) has an RP tag for particles, but sometimes labels them as adverbs. The CCGbank (Hockenmaier and Steedman, 2007) analysis of particles varies, but leans towards treating all particles as adverbial modifiers. This is in itself problematic, since it fails to take into account the fact that particles are a core part of the construct, whereas adverbs are optional. This lack of quality corpora for VPC-related work limits the power of corpustrained parsers. In this paper we draw on the Penn Treebank and PropBank (Kingsbury and Palmer, 2003) to repair CCGbank’s representation of VPCs, and demo"
U09-1017,H94-1020,0,0.287814,"Missing"
U09-1017,W03-1808,0,0.180514,"expressions (MWEs), compound lexemes made up of two or more words that together form a complete semantic unit, are one of the problems facing natural language processing systems. Verb-particle constructions (VPCs) are a common type of MWE, comprising a verb and a particle, most often a preposition. The meaning of some VPCs can be logically attributed to the component parts (e.g., picked out), but many are idiomatic and semantically opaque (e.g., make out). Previous research into VPCs has focussed much attention on their automatic extraction and classification (Baldwin and Villavicencio, 2002; Villavicencio, 2003). However, research into how they should be handled by parsers is noticeably lacking. Their unusual ability to manifest in both a ‘joined’ and ‘split’ configuration (‘gunned down the man’ versus ‘gunned the man down’) prevents parsers from treating them as a single unit, and demands a system that is able to maintain the semantic bond between the components, even when they are nonadjacent. To compound the problem, existing corpora are not consistent in their handling of these constructions. The Penn Treebank (Marcus et al., 1993, 1994) has an RP tag for particles, but sometimes labels them as a"
U10-1014,W05-1506,0,0.248895,"s to the gold standard, allowing us to explore both classification and regression as frameworks for the task. In classification, the closest sentence(s) to the gold standard with respect to F-score are labeled as positive, while all other sentences are labeled as negative. If there are multiple parses with the highest F-score, they are all labeled as positive. In regression, the F-score of each parse is used as the target value. Both classification and regression approaches were implemented using MEGAM1 . n-best lists of parses were generated using the n-best C & C parser using Algorithm 3 of Huang and Chiang (2005). We used the normal-form model for the C & C parser as described in Clark and Curran (2007) for all experiments in this paper. Reranker training data was created using nbest parses of each sentence in Sections 02-21 of CCGbank. As this is also the parser’s training data, care must be taken to avoid generating training data where the parser’s confidence level is different to that at run-time (caused by parsing the training data). We constructed ten folds of Sections 02-21, training the POS tagger, supertagger, and parser on nine of the folds and producing nbest parses over the remaining fold."
U10-1014,P08-1067,0,0.0769539,"Missing"
U10-1014,N10-1095,0,0.0462472,"tional benefit of improving the Charniak parser’s performance on out-of-domain data – a known weakness of supervised parsing. More recently, the Charniak reranking system has been adapted for the Berkeley parser (Petrov et al., 2006). Unlike the Collins and Charniak parsers, which are broadly similar and heavily based on lexicalised models, the Berkeley parser baked a cake with raisins N (NP NP )/NP N (S NP )/NP NP /N N NP NP > NP NP NP NP S NP S > < > < Figure 1: A simple CCG derivation. uses a split-merge technique to acquire a much smaller, unlexicalised grammar from its training data. Johnson and Ural (2010) report that reranking leads to negligible performance improvements for the Berkeley parser, and acknowledge that the reranker’s feature set, adapted from Charniak and Johnson (2005), may be implicitly tailored to the Charniak parser over the Berkeley parser. In particular, the feature pruning process for reranking was conducted over output from the Charniak parser, which may have prevented useful features for the Berkeley parser from being chosen. 3 Combinatory Categorial Grammar Combinatory Categorial Grammar (CCG, Steedman (2000)) is a lexicalised grammar formalism based on combinatory logi"
U10-1014,C04-1180,1,0.813026,"We experimented with values of 10 and 50 for n to balance between the potential accuracy improvement and the efficiency of the reranker. n was kept constant between the training data and the final test data (i.e. a reranker trained on 50best parses was then tested over 50-best parses). Following Charniak and Johnson (2005) we implemented feature pruning for the reranker trainThe C & C parser The C & C parser (Clark and Curran, 2007) is a fast, highly accurate parser based on the CCG formalism. The parser is used in question answering systems (Bos et al., 2007), computational semantics tools (Bos et al., 2004), and has been shown to perform well in recovering unbounded dependencies (Rimell et al., 2009). The parser divides the parsing process into two main phases: supertagging and parsing. First, the supertagger assigns a small set of initial categories to each word in the sentence. Then, the parser attempts to find a spanning analysis using the proposed categories using the modified CKY algorithm described in Steedman (2000). If the parser cannot find an analysis (i.e. there is no sequence of combinators that can combine the proposed categories) the supertagger is run again at a higher ambiguity l"
U10-1014,P05-1022,0,0.707841,"n adapted for the Berkeley parser (Petrov et al., 2006). Unlike the Collins and Charniak parsers, which are broadly similar and heavily based on lexicalised models, the Berkeley parser baked a cake with raisins N (NP NP )/NP N (S NP )/NP NP /N N NP NP > NP NP NP NP S NP S > < > < Figure 1: A simple CCG derivation. uses a split-merge technique to acquire a much smaller, unlexicalised grammar from its training data. Johnson and Ural (2010) report that reranking leads to negligible performance improvements for the Berkeley parser, and acknowledge that the reranker’s feature set, adapted from Charniak and Johnson (2005), may be implicitly tailored to the Charniak parser over the Berkeley parser. In particular, the feature pruning process for reranking was conducted over output from the Charniak parser, which may have prevented useful features for the Berkeley parser from being chosen. 3 Combinatory Categorial Grammar Combinatory Categorial Grammar (CCG, Steedman (2000)) is a lexicalised grammar formalism based on combinatory logic. The grammar is directly encoded in the lexicon in the form of categories that govern the syntactic behaviour of each word. A small number of generic rules combine categories toget"
U10-1014,M92-1003,0,0.0570338,"LR LF AF 87.19 91.98 93.43 86.32 90.89 92.26 86.75 91.43 92.84 84.80 89.47 90.96 Table 1: Baseline and oracle n-best parser performance over Section 00 of CCGbank. LexDep: CCG dependencies can be partially captured via the children of non-terminals in the tree. This feature is active for non-terminals with two children and encodes the heads of the children in terms of lexical items, POS tags, categories, and depth from the non-terminal. Dependencies involving punctuation are ignored as they are not assessed in the evaluation. Statistical significance was calculated using the test described in Chinchor (1992), which measures the probability that the two sets of responses are drawn from the same distribution. A score below 0.05 is considered significant. We report labeled precision (LP), labeled recall (LR), and labeled F-score (LF) results over gold standard POS tags and labeled F-score over automatically assigned POS tags (AF). NumDeps: distinguishes between parses based on the log number of dependencies that they yield ignoring punctuation. Dependencies are located using the same heuristic as the LexDep feature. 8 8.1 TypeRaising: indicates the presence of unary type-raising in the tree. While t"
U10-1014,J07-4004,1,0.961612,"rks for the task. In classification, the closest sentence(s) to the gold standard with respect to F-score are labeled as positive, while all other sentences are labeled as negative. If there are multiple parses with the highest F-score, they are all labeled as positive. In regression, the F-score of each parse is used as the target value. Both classification and regression approaches were implemented using MEGAM1 . n-best lists of parses were generated using the n-best C & C parser using Algorithm 3 of Huang and Chiang (2005). We used the normal-form model for the C & C parser as described in Clark and Curran (2007) for all experiments in this paper. Reranker training data was created using nbest parses of each sentence in Sections 02-21 of CCGbank. As this is also the parser’s training data, care must be taken to avoid generating training data where the parser’s confidence level is different to that at run-time (caused by parsing the training data). We constructed ten folds of Sections 02-21, training the POS tagger, supertagger, and parser on nine of the folds and producing nbest parses over the remaining fold. Features were generated over the n-best parses of the folded training data and the appropria"
U10-1014,D09-1076,0,0.0510937,"Missing"
U10-1014,P03-1003,0,0.0920353,"Missing"
U10-1014,J02-3001,0,0.187955,"Missing"
U10-1014,J07-3004,0,0.0349349,"cess is repeated. The supertagging phase dramatically reduces the number of derivations for the parser to consider, making the system highly efficient. An n-best version of the C & C parser has recently been developed (Brennan, 2008), incorporating the algorithms described in Huang and Chiang (2005). The n-best parser is almost as efficient as the baseline 1-best version, and we use it as the basis for all experiments presented in this paper. CCGbank is the standard corpus for English parsing with CCG. It is a transformation of the Penn Treebank WSJ data into CCG derivations and dependencies (Hockenmaier and Steedman, 2007). Sections 02-21 are the standard training data for the C & C parser, with Section 00 used for development and Section 23 for evaluation. The supertagger requires part-of-speech information for each word as part of its feature set, so a POS tagger is also included with the C & C parser. Both the supertagger and the POS tagger are trained over tags extracted from Sections 02-21 of CCGbank. 1 92 http://www.umiacs.umd.edu/˜hal/megam heuristics used by Charniak and Johnson (2005) unnecessary for our purposes. The features adapted from Charniak and Johnson (2005) are described in Sections 6.1 and 6"
U10-1014,W01-1812,0,0.0153889,"Missing"
U10-1014,J93-2004,0,0.0391638,"Missing"
U10-1014,N06-1020,0,0.135121,"Missing"
U10-1014,P06-1055,0,0.286312,"Missing"
U10-1014,D09-1085,0,0.0307715,"provement and the efficiency of the reranker. n was kept constant between the training data and the final test data (i.e. a reranker trained on 50best parses was then tested over 50-best parses). Following Charniak and Johnson (2005) we implemented feature pruning for the reranker trainThe C & C parser The C & C parser (Clark and Curran, 2007) is a fast, highly accurate parser based on the CCG formalism. The parser is used in question answering systems (Bos et al., 2007), computational semantics tools (Bos et al., 2004), and has been shown to perform well in recovering unbounded dependencies (Rimell et al., 2009). The parser divides the parsing process into two main phases: supertagging and parsing. First, the supertagger assigns a small set of initial categories to each word in the sentence. Then, the parser attempts to find a spanning analysis using the proposed categories using the modified CKY algorithm described in Steedman (2000). If the parser cannot find an analysis (i.e. there is no sequence of combinators that can combine the proposed categories) the supertagger is run again at a higher ambiguity level, giving each word a larger set of possible categories, and the process is repeated. The su"
U10-1014,W09-3839,0,0.0370477,"Missing"
U10-1014,N04-1023,0,0.0252936,"Missing"
U10-1014,D09-1043,0,0.045747,"Missing"
U10-1014,J03-4003,0,\N,Missing
U10-1014,P03-1046,0,\N,Missing
U11-1010,J99-2004,0,0.0694072,"s of CCG derivations with lexical categories assigned to each word. Both examples also provide the word saw with the (S NP )/NP category. Lexicalised grammars typically have a small set of rules (the combinatory rules in CCG) and instead rely on categories that describe a word’s syntactic role in a sentence. In Figure 1, the word with contains two separate categories indicating whether it modifies saw (first example) or John (second example). In a highly lexicalised grammar, a parser may need to explore a large search space of categories in order to select the correct category for each word. Bangalore and Joshi (1999) proposed supertagging, where each word is assigned a reduced set of categories by a sequence tagger, rather than all of the categories previously seen with that word. Our supertags are CCG categories, and so are much more detailed than POS tags. By limiting the number of supertags for each word, there is a massive reduction in the number of derivations. The effectiveness of supertagging (Clark and Curran, 2004) demonstrates the influence of lexical ambiguity on parsing complexity for lexicalised grammars. Hockenmaier and Steedman (2007) developed CCGbank, a semi-automated conversion of the Pe"
U11-1010,P05-1022,0,0.169619,"Missing"
U11-1010,C04-1041,1,0.92569,"r with a small accuracy penalty. 1 Introduction Parsing is a vital component of sophisticated natural language processing (NLP) systems that require deep and accurate semantic interpretation, including question answering and summarisation. Unfortunately, the complexity of natural languages results in substantial ambiguity. For even a typical sentence, thousands of potential analyses may be considered by a wide-coverage parser, making parsing impractical for large-scale applications. Several methods have been proposed to improve parsing speed, including supertagging (Bangalore and Joshi, 1999; Clark and Curran, 2004; Kummerfeld et al., 2010), coarse-to-fine parsing (Charniak and Johnson, 2005; Pauls and Klein, 2009), chart repair (Djordjevic, 2006), chart constraints (Roark and Hollingshead, 2009), structure caching (Dawborn and Curran, 2009) and chart pruning (Zhang et al., 2010). These heuristic methods offer a tradeoff between accuracy and speed. A* parsing (Klein and Manning, 2003) offers speed increases with no reduction in accuracy. For parsers optimised for speed, the overhead required by additional efficiency techniques can exceed the speed gains they provide (Dawborn and Curran, 2009). As mistak"
U11-1010,J07-4004,1,0.946972,"009), structure caching (Dawborn and Curran, 2009) and chart pruning (Zhang et al., 2010). These heuristic methods offer a tradeoff between accuracy and speed. A* parsing (Klein and Manning, 2003) offers speed increases with no reduction in accuracy. For parsers optimised for speed, the overhead required by additional efficiency techniques can exceed the speed gains they provide (Dawborn and Curran, 2009). As mistakes made in the parsing phase propagate to later stages, high speed but low accuracy parsers may not be useful in NLP systems (Chang et al., 2006). In this paper, we modify the C&C (Clark and Curran, 2007) Combinatory Categorial Grammar (CCG) parser to enable shift-reduce (SR) parsing. The Cocke-Kasami-Younger (CKY) algorithm (Kasami, 1965; Younger, 1967) is replaced with the shiftreduce algorithm (Aho and Ullman, 1972). However, back-tracking in shift-reduce parsers make them exponential in the worst case. To eliminate this duplication of work, a graphstructured stack (GSS; Tomita, 1988) is employed. This is the equivalent, for shift-reduce parsing, of the chart in CKY parsing, which stores all possible parse states compactly and enables polynomial time worst-case complexity. Due to the increm"
U11-1010,P02-1042,0,0.0254203,"the categories previously seen with that word. Our supertags are CCG categories, and so are much more detailed than POS tags. By limiting the number of supertags for each word, there is a massive reduction in the number of derivations. The effectiveness of supertagging (Clark and Curran, 2004) demonstrates the influence of lexical ambiguity on parsing complexity for lexicalised grammars. Hockenmaier and Steedman (2007) developed CCGbank, a semi-automated conversion of the Penn Treebank (Marcus et al., 1993) to the CCG formalism. A number of statistical parsers (Hockenmaier and Steedman, 2002; Clark et al., 2002) have been created for CCG parsing using CCGbank. > NP > (S NP )(S NP ) < S NP < S I saw John with binoculars NP (S NP )/NP NP (NP NP )/NP NP NP NP S NP S NP > < > < Figure 1: Two CCG derivations with PP ambiguity. supertagger provides each word with a set of likely categories, reducing the search space considerably. Second, the parser combines the categories, using the CKY chart-parsing algorithm and CCG’s combinatory rules, to produce all derivations that can be constructed with the given categories. Finally, the decoder finds the best derivation from amongst the spanning analyses in"
U11-1010,W02-1001,0,0.268733,"Missing"
U11-1010,U09-1010,1,0.896661,"Missing"
U11-1010,U06-1003,1,0.889998,"Missing"
U11-1010,P02-1043,0,0.0413937,"ence tagger, rather than all of the categories previously seen with that word. Our supertags are CCG categories, and so are much more detailed than POS tags. By limiting the number of supertags for each word, there is a massive reduction in the number of derivations. The effectiveness of supertagging (Clark and Curran, 2004) demonstrates the influence of lexical ambiguity on parsing complexity for lexicalised grammars. Hockenmaier and Steedman (2007) developed CCGbank, a semi-automated conversion of the Penn Treebank (Marcus et al., 1993) to the CCG formalism. A number of statistical parsers (Hockenmaier and Steedman, 2002; Clark et al., 2002) have been created for CCG parsing using CCGbank. > NP > (S NP )(S NP ) < S NP < S I saw John with binoculars NP (S NP )/NP NP (NP NP )/NP NP NP NP S NP S NP > < > < Figure 1: Two CCG derivations with PP ambiguity. supertagger provides each word with a set of likely categories, reducing the search space considerably. Second, the parser combines the categories, using the CKY chart-parsing algorithm and CCG’s combinatory rules, to produce all derivations that can be constructed with the given categories. Finally, the decoder finds the best derivation from amongst the"
U11-1010,J07-3004,0,0.032769,"gories in order to select the correct category for each word. Bangalore and Joshi (1999) proposed supertagging, where each word is assigned a reduced set of categories by a sequence tagger, rather than all of the categories previously seen with that word. Our supertags are CCG categories, and so are much more detailed than POS tags. By limiting the number of supertags for each word, there is a massive reduction in the number of derivations. The effectiveness of supertagging (Clark and Curran, 2004) demonstrates the influence of lexical ambiguity on parsing complexity for lexicalised grammars. Hockenmaier and Steedman (2007) developed CCGbank, a semi-automated conversion of the Penn Treebank (Marcus et al., 1993) to the CCG formalism. A number of statistical parsers (Hockenmaier and Steedman, 2002; Clark et al., 2002) have been created for CCG parsing using CCGbank. > NP > (S NP )(S NP ) < S NP < S I saw John with binoculars NP (S NP )/NP NP (NP NP )/NP NP NP NP S NP S NP > < > < Figure 1: Two CCG derivations with PP ambiguity. supertagger provides each word with a set of likely categories, reducing the search space considerably. Second, the parser combines the categories, using the CKY chart-parsing algo"
U11-1010,P10-1110,0,0.0218759,"d tagging accuracy is possible. 68 3.2 Graph-Structured Stack stored in the GSS. This means that only a single shift action is necessary for the GSS instead of one for each possible derivation. Back-tracking shift-reduce parsers are worst case exponential, preventing a full exploration of the search space. A graph-structured stack (GSS) is a general structure that allows for the efficient handling of non-determinism in shift-reduce parsing (Tomita, 1988). The GSS allows for polynomial time non-deterministic shift-reduce parsing and has been shown to be highly effective for dependency parsing (Huang and Sagae, 2010). The use of GSS allows for the incremental construction of the parse tree without being forced to discard large segments of the search space. Here we will show an example of using a GSS to augment shift-reduce parsing and then show how it can be applied to CCG parsing. In the example grammar below, all three reduction rules are possible on the given stack. By performing backtracking and pursuing all possible reductions, shift-reduce parsing becomes worst-case exponential as previous results must be re-computed. ∅ A B F G H C D F G ∅ C D F ∅ D A B C D G J E I H Reduction Rules J ← F I J ← G I"
U11-1010,N03-1016,0,0.108465,"Missing"
U11-1010,P10-1036,1,0.924281,"penalty. 1 Introduction Parsing is a vital component of sophisticated natural language processing (NLP) systems that require deep and accurate semantic interpretation, including question answering and summarisation. Unfortunately, the complexity of natural languages results in substantial ambiguity. For even a typical sentence, thousands of potential analyses may be considered by a wide-coverage parser, making parsing impractical for large-scale applications. Several methods have been proposed to improve parsing speed, including supertagging (Bangalore and Joshi, 1999; Clark and Curran, 2004; Kummerfeld et al., 2010), coarse-to-fine parsing (Charniak and Johnson, 2005; Pauls and Klein, 2009), chart repair (Djordjevic, 2006), chart constraints (Roark and Hollingshead, 2009), structure caching (Dawborn and Curran, 2009) and chart pruning (Zhang et al., 2010). These heuristic methods offer a tradeoff between accuracy and speed. A* parsing (Klein and Manning, 2003) offers speed increases with no reduction in accuracy. For parsers optimised for speed, the overhead required by additional efficiency techniques can exceed the speed gains they provide (Dawborn and Curran, 2009). As mistakes made in the parsing pha"
U11-1010,J93-2004,0,0.0469829,"ertagging, where each word is assigned a reduced set of categories by a sequence tagger, rather than all of the categories previously seen with that word. Our supertags are CCG categories, and so are much more detailed than POS tags. By limiting the number of supertags for each word, there is a massive reduction in the number of derivations. The effectiveness of supertagging (Clark and Curran, 2004) demonstrates the influence of lexical ambiguity on parsing complexity for lexicalised grammars. Hockenmaier and Steedman (2007) developed CCGbank, a semi-automated conversion of the Penn Treebank (Marcus et al., 1993) to the CCG formalism. A number of statistical parsers (Hockenmaier and Steedman, 2002; Clark et al., 2002) have been created for CCG parsing using CCGbank. > NP > (S NP )(S NP ) < S NP < S I saw John with binoculars NP (S NP )/NP NP (NP NP )/NP NP NP NP S NP S NP > < > < Figure 1: Two CCG derivations with PP ambiguity. supertagger provides each word with a set of likely categories, reducing the search space considerably. Second, the parser combines the categories, using the CKY chart-parsing algorithm and CCG’s combinatory rules, to produce all derivations that can be constructed with"
U11-1010,C04-1010,0,0.0340902,"of the input sentence, selecting one or more actions at each step. The current state of the parser is stored in a stack, where the partial derivation is stored and the parsing operations are performed. For the actions, either we shift the current word onto the stack or reduce the top two (or more) items at the top of the stack (Aho and Ullman, 1972). As the scoring model can be defined over actions, this can allow for highly efficient parsing through greedy search (Sagae and Lavie, 2005). This has made shift-reduce parsing popular for high-speed dependency parsers (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). Unfortunately, a deterministic shift-reduce parser cannot handle ambiguity because it only considers a single derivation. A simple extension is to eliminate determinism and perform a best-first search, backtracking if the parser reaches a dead end. This backtracking leads to duplicate construction of substructures and complete exploration is exponential in the worst case. Beam search has been used to handle this exponential explosion by discarding a large portion of the search space. In Zhang and Clark (2011), a direct comparison is made between their shift-reduce CCG parser and the chart-ba"
U11-1010,P09-1108,0,0.040092,"Missing"
U11-1010,N09-1073,0,0.0551798,"Missing"
U11-1010,W05-1513,0,0.0233792,"ccuracies compared to the C&C chart parser. In its deterministic form, a shift-reduce parser performs a single left-to-right scan of the input sentence, selecting one or more actions at each step. The current state of the parser is stored in a stack, where the partial derivation is stored and the parsing operations are performed. For the actions, either we shift the current word onto the stack or reduce the top two (or more) items at the top of the stack (Aho and Ullman, 1972). As the scoring model can be defined over actions, this can allow for highly efficient parsing through greedy search (Sagae and Lavie, 2005). This has made shift-reduce parsing popular for high-speed dependency parsers (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). Unfortunately, a deterministic shift-reduce parser cannot handle ambiguity because it only considers a single derivation. A simple extension is to eliminate determinism and perform a best-first search, backtracking if the parser reaches a dead end. This backtracking leads to duplicate construction of substructures and complete exploration is exponential in the worst case. Beam search has been used to handle this exponential explosion by discarding a large portion"
U11-1010,P88-1031,0,0.618098,"n, 2009). As mistakes made in the parsing phase propagate to later stages, high speed but low accuracy parsers may not be useful in NLP systems (Chang et al., 2006). In this paper, we modify the C&C (Clark and Curran, 2007) Combinatory Categorial Grammar (CCG) parser to enable shift-reduce (SR) parsing. The Cocke-Kasami-Younger (CKY) algorithm (Kasami, 1965; Younger, 1967) is replaced with the shiftreduce algorithm (Aho and Ullman, 1972). However, back-tracking in shift-reduce parsers make them exponential in the worst case. To eliminate this duplication of work, a graphstructured stack (GSS; Tomita, 1988) is employed. This is the equivalent, for shift-reduce parsing, of the chart in CKY parsing, which stores all possible parse states compactly and enables polynomial time worst-case complexity. Due to the incremental nature of shift-reduce parsing, we can perform pruning of the parse state in the process of considering the next word (the frontier). Our frontier pruning model is an averaged perceptron trained to recognise the highest-scoring derivation that the C&C parser would have selected. By eliminating unlikely derivations , we substantially decrease the amount of ambiguity that the parser"
U11-1010,W03-3023,0,0.0531256,"a single left-to-right scan of the input sentence, selecting one or more actions at each step. The current state of the parser is stored in a stack, where the partial derivation is stored and the parsing operations are performed. For the actions, either we shift the current word onto the stack or reduce the top two (or more) items at the top of the stack (Aho and Ullman, 1972). As the scoring model can be defined over actions, this can allow for highly efficient parsing through greedy search (Sagae and Lavie, 2005). This has made shift-reduce parsing popular for high-speed dependency parsers (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). Unfortunately, a deterministic shift-reduce parser cannot handle ambiguity because it only considers a single derivation. A simple extension is to eliminate determinism and perform a best-first search, backtracking if the parser reaches a dead end. This backtracking leads to duplicate construction of substructures and complete exploration is exponential in the worst case. Beam search has been used to handle this exponential explosion by discarding a large portion of the search space. In Zhang and Clark (2011), a direct comparison is made between their shift-reduce CC"
U11-1010,P11-1069,0,0.0119889,"uce parsing popular for high-speed dependency parsers (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). Unfortunately, a deterministic shift-reduce parser cannot handle ambiguity because it only considers a single derivation. A simple extension is to eliminate determinism and perform a best-first search, backtracking if the parser reaches a dead end. This backtracking leads to duplicate construction of substructures and complete exploration is exponential in the worst case. Beam search has been used to handle this exponential explosion by discarding a large portion of the search space. In Zhang and Clark (2011), a direct comparison is made between their shift-reduce CCG parser and the chart-based C&C parser. As CCG allows for a limited number of unary rules, specifically typechanging and type-raising, Zhang and Clark extend 3.1 Advantages of Semi-Incremental Parsing Shift-reduce parsing allows for fully incremental parsing that does not require the full sentence. Whilst the C&C parser could be modified to perform in this fashion, POS tagging and supertagging accuracy would likely decrease, leading to lower overall parsing accuracy as mistakes propagate up the parsing pipeline. Semi-incremental parsi"
U11-1010,C10-2168,1,0.878271,"Missing"
U13-1007,D08-1031,0,0.0208936,"ejudice. While their work outperformed the previous work on literature by EM2010, their system was very slow, so they did not provide a full comparison. The first competitive learning based system is described in Soon et al. (2001). A binary classifier was trained to determine whether pairs of mentions were coreferential, based on 12 features which considered surface level details such as string matching and heuristically determined morphosyntatics. Its feature set was expanded in Ng and Cardie (2002) to include the role of syntactic constraints and modification on coreference. Various works (Bengtson and Roth, 2008; Stoyanov et al., 2010; Stoyanov and Eisner, 2012) have expanded this feature set further. Ng and Cardie (2002) also proposed ranking potential coreference links. Where Soon et al. assigned the closest positively classified mention as the antecedent of an active mention, ranking approaches define a window for candidate selection and return the most probable candidate within the window. Systems can either incorporate ranking as a post-processing stage which forms clusters based on pairwise probabilities (Ng and Cardie, 2002; Stoyanov et al., 2010; Denis and Baldridge, 2008), or they can rank d"
U13-1007,P02-1014,0,0.0420054,"king problem. As part of their work they introduced a new corpus which covers the entirety of the novel Pride & Prejudice. While their work outperformed the previous work on literature by EM2010, their system was very slow, so they did not provide a full comparison. The first competitive learning based system is described in Soon et al. (2001). A binary classifier was trained to determine whether pairs of mentions were coreferential, based on 12 features which considered surface level details such as string matching and heuristically determined morphosyntatics. Its feature set was expanded in Ng and Cardie (2002) to include the role of syntactic constraints and modification on coreference. Various works (Bengtson and Roth, 2008; Stoyanov et al., 2010; Stoyanov and Eisner, 2012) have expanded this feature set further. Ng and Cardie (2002) also proposed ranking potential coreference links. Where Soon et al. assigned the closest positively classified mention as the antecedent of an active mention, ranking approaches define a window for candidate selection and return the most probable candidate within the window. Systems can either incorporate ranking as a post-processing stage which forms clusters based"
U13-1007,D08-1069,0,0.0206396,"erence. Various works (Bengtson and Roth, 2008; Stoyanov et al., 2010; Stoyanov and Eisner, 2012) have expanded this feature set further. Ng and Cardie (2002) also proposed ranking potential coreference links. Where Soon et al. assigned the closest positively classified mention as the antecedent of an active mention, ranking approaches define a window for candidate selection and return the most probable candidate within the window. Systems can either incorporate ranking as a post-processing stage which forms clusters based on pairwise probabilities (Ng and Cardie, 2002; Stoyanov et al., 2010; Denis and Baldridge, 2008), or they can rank during clustering (Rahman and Ng, 2009). Stanford’s system (Lee et al., 2011) achieved the best result in the CoNLL 2011 shared task and remained competitive in CoNLL2012 using a simple, unsupervised classifier. It captures global consistency constraints by having cluster level modelling, which it achieves by having a series of sieves that each read the document and expand clusters. The sieves are arranged in order of decreasing precision, such that mentions with a high chance of being coreferential are clustered first, which allows more difficult mentions to use more inform"
U13-1007,D12-1072,1,0.896041,"Missing"
U13-1007,pareti-2012-database,0,0.0308437,"h their own method. They do not identify pronouns and only perform coreference on the NEs, using a simple system. Following on from EM2010, was the work of O’Keefe et al. (2012). They note that EM2010 had used some features that relied on gold standard information about previous decisions, which O’Keefe et al. replaced with features using predicted information and a sequence decoding step. They also evaluated their method on two other corpora, one that they build from Sydney Morning Herald1 news articles (SMHC), and another over Wall Street Journal2 news articles (PARC) that was introduced in Pareti (2012). They found that removing the gold standard features had a large impact on accuracy, and that their sequence labelling approaches could recover some of that lost accuracy. Later work by Pareti et al. (2013) extended the SMHC to include indirect and mixed quotes, though their focus was on quote extraction. While the work of O’Keefe et al. (2012) and Pareti et al. (2013) was mainly focused on news articles, He et al. (2013) focused on literature. They developed a model that treated the task similarly to EM2010, though they considered it to be a ranking problem. As part of their work they introd"
U13-1007,P13-1129,0,0.142841,"od on two other corpora, one that they build from Sydney Morning Herald1 news articles (SMHC), and another over Wall Street Journal2 news articles (PARC) that was introduced in Pareti (2012). They found that removing the gold standard features had a large impact on accuracy, and that their sequence labelling approaches could recover some of that lost accuracy. Later work by Pareti et al. (2013) extended the SMHC to include indirect and mixed quotes, though their focus was on quote extraction. While the work of O’Keefe et al. (2012) and Pareti et al. (2013) was mainly focused on news articles, He et al. (2013) focused on literature. They developed a model that treated the task similarly to EM2010, though they considered it to be a ranking problem. As part of their work they introduced a new corpus which covers the entirety of the novel Pride & Prejudice. While their work outperformed the previous work on literature by EM2010, their system was very slow, so they did not provide a full comparison. The first competitive learning based system is described in Soon et al. (2001). A binary classifier was trained to determine whether pairs of mentions were coreferential, based on 12 features which consider"
U13-1007,D13-1101,1,0.874384,"Missing"
U13-1007,D13-1027,0,0.0204455,"Missing"
U13-1007,W11-1901,0,0.0166041,"rst, which allows more difficult mentions to use more information from the expanded clusters. Research into quote attribution has ignored the impact that these different approaches could have, and the four large-scale corpora that exist for quote attribution all include some gold-standard information about either the mentions or the coreference chains. Thus the goal of our work is to use consistent coreference methods across the different corpora, in order to evaluate the effect of coreference on quote attribution. This also allows us to 2.1 Coreference resolution Coreference resolution (e.g. Pradhan et al. (2011)) is the task of partitioning mentions (typically noun phrases) into equivalence classes which refer to the same real world entity. It has largely been 1 2 http://www.smh.com.au http://www.wsj.com 44 Corpus SMHC PARC LIT P&P Documents 965 2,280 11 1 Tokens 601k 1,139k 407k 144k Quotations 6,705 9,961 3,486 1,692 Proper Gold Gold Auto Auto Pronouns Gold Gold Common Gold Auto Proper Gold - Auto Gold Pronouns Gold Gold Common Coref Entities Street Journal. Pareti’s work includes more general forms of attributable text than we are interested in, so we use just the assertions, as they correspond to"
U13-1007,W11-1902,0,0.126791,"med entities with limited gold-standard coreference, but do not include pronouns or common nouns. The SMHC (Pareti et al., 2013) includes gold-standard named entities and pronouns, as well as gold-standard coreference, but does not include common noun candidates. Finally the PARC (Pareti, 2012) is intended to cover attribution more generally, and so does not include any candidate speakers except for those that have attributed text. Our work addresses the problem of inconsistent candidates within these corpora by separately aligning the output of three coreference resolution systems, Stanford (Lee et al., 2011), Reconcile (Stoyanov et al., 2010), and a naive baseline system, with the gold-standard speaker annotations. We can then evaluate the quote attribution methods from O’Keefe et al. (2012) with a set of speakers that have been identified in a more consistent manner across attribution methods and corpora. O’Keefe et al. note that one of the primary factors confounding their evaluation was that the set of candidates was not consistent, which our work addresses. Our second main contribution is that we use quote attribution as an extrinsic evaluation for coreference resolution. Intrinsic evaluation"
U13-1007,W06-0305,0,0.0450379,"Missing"
U13-1007,H05-1004,0,0.0761816,"Missing"
U13-1007,D10-1048,0,0.0227075,"ates, though they do use them as features. Note that the set of characters that they can attribute quotes to is closed, and does not include any unnamed characters. consistent, with the SMHC including gold-standard coreference for the two categories of candidates it contains and P & P including gold-standard coreference for its automatically identified named entities. LIT includes only automatic coreference of named entities, while PARC only includes goldstandard coreference of pronouns. 4 Coreference Systems The three coreference resolution systems that we use are Stanford’s CoreNLP package (Raghunathan et al., 2010), Reconcile (Stoyanov et al., 2010), and a naive baseline system. By using Stanford and Reconcile we can evaluate the two main types of systems, as they are unsupervised and supervised respectively. The naive system is included for comparison. It performs NE coreference using simple string-matching of NEs found with Stanford’s NE tagger, and coreference of pronouns by linking them to the most recent gendermatching antecedent. The naive baseline does not include common noun mentions. We experimented with a fourth system, CherryPicker (Rahman and Ng, 2009), but are unable to include results usin"
U13-1007,D09-1101,0,0.154244,"2010; Stoyanov and Eisner, 2012) have expanded this feature set further. Ng and Cardie (2002) also proposed ranking potential coreference links. Where Soon et al. assigned the closest positively classified mention as the antecedent of an active mention, ranking approaches define a window for candidate selection and return the most probable candidate within the window. Systems can either incorporate ranking as a post-processing stage which forms clusters based on pairwise probabilities (Ng and Cardie, 2002; Stoyanov et al., 2010; Denis and Baldridge, 2008), or they can rank during clustering (Rahman and Ng, 2009). Stanford’s system (Lee et al., 2011) achieved the best result in the CoNLL 2011 shared task and remained competitive in CoNLL2012 using a simple, unsupervised classifier. It captures global consistency constraints by having cluster level modelling, which it achieves by having a series of sieves that each read the document and expand clusters. The sieves are arranged in order of decreasing precision, such that mentions with a high chance of being coreferential are clustered first, which allows more difficult mentions to use more information from the expanded clusters. Research into quote attr"
U13-1007,J01-4004,0,0.13604,"us was on quote extraction. While the work of O’Keefe et al. (2012) and Pareti et al. (2013) was mainly focused on news articles, He et al. (2013) focused on literature. They developed a model that treated the task similarly to EM2010, though they considered it to be a ranking problem. As part of their work they introduced a new corpus which covers the entirety of the novel Pride & Prejudice. While their work outperformed the previous work on literature by EM2010, their system was very slow, so they did not provide a full comparison. The first competitive learning based system is described in Soon et al. (2001). A binary classifier was trained to determine whether pairs of mentions were coreferential, based on 12 features which considered surface level details such as string matching and heuristically determined morphosyntatics. Its feature set was expanded in Ng and Cardie (2002) to include the role of syntactic constraints and modification on coreference. Various works (Bengtson and Roth, 2008; Stoyanov et al., 2010; Stoyanov and Eisner, 2012) have expanded this feature set further. Ng and Cardie (2002) also proposed ranking potential coreference links. Where Soon et al. assigned the closest posit"
U13-1007,P10-2029,0,0.208327,"-standard coreference, but do not include pronouns or common nouns. The SMHC (Pareti et al., 2013) includes gold-standard named entities and pronouns, as well as gold-standard coreference, but does not include common noun candidates. Finally the PARC (Pareti, 2012) is intended to cover attribution more generally, and so does not include any candidate speakers except for those that have attributed text. Our work addresses the problem of inconsistent candidates within these corpora by separately aligning the output of three coreference resolution systems, Stanford (Lee et al., 2011), Reconcile (Stoyanov et al., 2010), and a naive baseline system, with the gold-standard speaker annotations. We can then evaluate the quote attribution methods from O’Keefe et al. (2012) with a set of speakers that have been identified in a more consistent manner across attribution methods and corpora. O’Keefe et al. note that one of the primary factors confounding their evaluation was that the set of candidates was not consistent, which our work addresses. Our second main contribution is that we use quote attribution as an extrinsic evaluation for coreference resolution. Intrinsic evaluation of coreference is known to be prob"
U13-1007,C12-1154,0,0.0126231,"ous work on literature by EM2010, their system was very slow, so they did not provide a full comparison. The first competitive learning based system is described in Soon et al. (2001). A binary classifier was trained to determine whether pairs of mentions were coreferential, based on 12 features which considered surface level details such as string matching and heuristically determined morphosyntatics. Its feature set was expanded in Ng and Cardie (2002) to include the role of syntactic constraints and modification on coreference. Various works (Bengtson and Roth, 2008; Stoyanov et al., 2010; Stoyanov and Eisner, 2012) have expanded this feature set further. Ng and Cardie (2002) also proposed ranking potential coreference links. Where Soon et al. assigned the closest positively classified mention as the antecedent of an active mention, ranking approaches define a window for candidate selection and return the most probable candidate within the window. Systems can either incorporate ranking as a post-processing stage which forms clusters based on pairwise probabilities (Ng and Cardie, 2002; Stoyanov et al., 2010; Denis and Baldridge, 2008), or they can rank during clustering (Rahman and Ng, 2009). Stanford’s"
U13-1007,P09-1074,0,0.0410098,"Missing"
U13-1007,M95-1005,0,0.468798,"Missing"
U13-1007,D08-1067,0,\N,Missing
U14-1005,P98-1012,0,0.206824,"over entire documents. Named entities (NEs) in natural language are often difficult to resolve; one entity can be referred to by many different mention strings (synonymy) or multiple distinct entities referred to by the same mention string (polysemy). While the task of resolving this ambiguity is automatically and subconsciously performed by most people when they read text, this is a much more difficult task for automated systems to perform. The NE ambiguity problem has been approached within the field of computational linguistics as three related tasks: Cross-document Coreference Resolution (Bagga and Baldwin, 1998), Wikification (Mihalcea and Csomai, 2007), and named entity linking (NEL) (Bunescu and Pasca, 2006). NEL aims to link in-text mentions of NEs to a knowledge base (KB) using the context of the mention and the vast amount of structured and unstructured information held in the KB. Approaches to NEL vary (Hachey et al., 2013; Ji and Grishman, 2011), however many systems share some core components. Radford et al. (2012) combines three seminal approaches (Cucerzan, 2007; Varma et al., 2009; Bunescu and Pasca, 2006) to produce the NEL system on which this paper is based. Almost all approaches can be"
U14-1005,E06-1002,0,0.0679743,"entity can be referred to by many different mention strings (synonymy) or multiple distinct entities referred to by the same mention string (polysemy). While the task of resolving this ambiguity is automatically and subconsciously performed by most people when they read text, this is a much more difficult task for automated systems to perform. The NE ambiguity problem has been approached within the field of computational linguistics as three related tasks: Cross-document Coreference Resolution (Bagga and Baldwin, 1998), Wikification (Mihalcea and Csomai, 2007), and named entity linking (NEL) (Bunescu and Pasca, 2006). NEL aims to link in-text mentions of NEs to a knowledge base (KB) using the context of the mention and the vast amount of structured and unstructured information held in the KB. Approaches to NEL vary (Hachey et al., 2013; Ji and Grishman, 2011), however many systems share some core components. Radford et al. (2012) combines three seminal approaches (Cucerzan, 2007; Varma et al., 2009; Bunescu and Pasca, 2006) to produce the NEL system on which this paper is based. Almost all approaches can be split into 3 stages: mention extraction, candidate generation and candidate disambiguation. The men"
U14-1005,P05-1077,0,0.0982809,"Missing"
U14-1005,D07-1074,0,0.279336,"approached within the field of computational linguistics as three related tasks: Cross-document Coreference Resolution (Bagga and Baldwin, 1998), Wikification (Mihalcea and Csomai, 2007), and named entity linking (NEL) (Bunescu and Pasca, 2006). NEL aims to link in-text mentions of NEs to a knowledge base (KB) using the context of the mention and the vast amount of structured and unstructured information held in the KB. Approaches to NEL vary (Hachey et al., 2013; Ji and Grishman, 2011), however many systems share some core components. Radford et al. (2012) combines three seminal approaches (Cucerzan, 2007; Varma et al., 2009; Bunescu and Pasca, 2006) to produce the NEL system on which this paper is based. Almost all approaches can be split into 3 stages: mention extraction, candidate generation and candidate disambiguation. The mention extraction stage involves chaining together mentions in a document that refer to the same entity, and candidate generation stage involves retrieving entities from the KB that have similar names to mentions in the query’s chain. The candidate disambiguation stage compares each candidate with the query and ranks them by the aggregate of similarity scores. Core to"
U14-1005,P11-1115,0,0.0219295,"people when they read text, this is a much more difficult task for automated systems to perform. The NE ambiguity problem has been approached within the field of computational linguistics as three related tasks: Cross-document Coreference Resolution (Bagga and Baldwin, 1998), Wikification (Mihalcea and Csomai, 2007), and named entity linking (NEL) (Bunescu and Pasca, 2006). NEL aims to link in-text mentions of NEs to a knowledge base (KB) using the context of the mention and the vast amount of structured and unstructured information held in the KB. Approaches to NEL vary (Hachey et al., 2013; Ji and Grishman, 2011), however many systems share some core components. Radford et al. (2012) combines three seminal approaches (Cucerzan, 2007; Varma et al., 2009; Bunescu and Pasca, 2006) to produce the NEL system on which this paper is based. Almost all approaches can be split into 3 stages: mention extraction, candidate generation and candidate disambiguation. The mention extraction stage involves chaining together mentions in a document that refer to the same entity, and candidate generation stage involves retrieving entities from the KB that have similar names to mentions in the query’s chain. The candidate"
U14-1006,W97-0710,0,0.152716,"ories have been used to create an ontology (Suchanek et al., 2007) and infoboxes (keyvalue pairs of facts) have been used to provide additional context to information in text (Wu and Weld, 2010). Wikipedia’s revision history is exploited less frequently, but has proven useful to train a model of sentence compression (Yamangil and Nelken, 2008). We know of no prior work that aligns page traffic to text in Wikipedia. ing documents about an entity, selecting the most representative content of the text while adhering to length constraints. Early approaches to the task train a sentence classifier (Teufel and Moens, 1997) on a corpus of sentences which are in some way biographical. This corpus is typically existing biographies, or manually selected sentences from a larger corpus. Previous work has used Wikipedia as large, alternate source of biographical sentences (Biadsy et al., 2008), hypothesising that most sentences in Wikipedia’s articles about people are biographical. Zhou et al. (2004) experiment with non-binary sentence classification, requiring a summary to have at least one sentence of each category in a “biographical checklist”, with categories such as work, scandal, and nationality. Training a clas"
U14-1006,P08-1092,0,0.163658,"oach to biographical summarisation, by extracting important sentences from an entity’s Wikipedia page based on internet traffic to the page over time. Using a pilot data set, we found that it is feasible to extract key sentences about people’s notability without the need for a large annotated corpus. 1 Introduction “What is Julian Assange known for?” is a question which can be answered in many ways. Previous computational approaches to answering questions like these have focused on summarisation: selecting a subset of sentences from a group of documents relating to a person and ordering them (Biadsy et al., 2008; Zhou et al., 2004). Full text summaries do provide some insight into the notability of their subject, but can also contain superfluous information. To pinpoint the notoriety of individuals, we aim to extract the sentences from a document which show how the document’s subject is notable. We provide an alternate, unsupervised approach to the broader task of biography abstraction, which exploits external information about text, rather then extracting textual features directly. In this paper, we respond to “What is Julian Assange known for?” with sentences mentioning important events which have"
U14-1006,P06-4018,0,0.0275099,"Missing"
U14-1006,P10-1013,0,0.0227429,"approach to biographical abstraction is to summarise existAlexander Hogue, Joel Nothman and James R. Curran. 2014. Unsupervised Biographical Event Extraction Using Wikipedia Traffic. In Proceedings of Australasian Language Technology Association Workshop, pages 41−49. massive source of human-written semi-structured information. Plain text has been used to assist named entity recognition (Nothman et al., 2013), page categories have been used to create an ontology (Suchanek et al., 2007) and infoboxes (keyvalue pairs of facts) have been used to provide additional context to information in text (Wu and Weld, 2010). Wikipedia’s revision history is exploited less frequently, but has proven useful to train a model of sentence compression (Yamangil and Nelken, 2008). We know of no prior work that aligns page traffic to text in Wikipedia. ing documents about an entity, selecting the most representative content of the text while adhering to length constraints. Early approaches to the task train a sentence classifier (Teufel and Moens, 1997) on a corpus of sentences which are in some way biographical. This corpus is typically existing biographies, or manually selected sentences from a larger corpus. Previous"
U14-1006,W04-1017,0,0.0420441,"facts to extract not only the patterns they represent, (e.g.@ [person] WORKS FOR [organisation]) but also to extract additional patterns. Liu et al. (2010) presented BIOSNOWBALL for the biographical fact extraction domain, which extracts biographical key-value pairs. It is the wide range of reasons for notoriety (which would require a large number of potential patterns to fill) motivating our novel source of measures of importance — Wikipedia page traffic over time. Rather than the traditional approach of classifying sentences via textual features (Schiffman et al., 2001) or locating events (Filatova and Hatzivassiloglou, 2004), we explore the use of an extrinsic source of information indicating what is interesting. Motivating this approach is our hypothesis that many people are most well-known for the events they were involved in. These events have previously been ordered temporally by supervised learning from textual features, (Filatova and Hovy, 2001), and our extrinsic information may assist with the temporal location of events with little temporal information mentioned in text. Various features of Wikipedia have been previously exploited in NLP, since they provide a 2.1 Timeseries Analysis To exploit the Wikipe"
U14-1006,W01-1313,0,0.060308,"umber of potential patterns to fill) motivating our novel source of measures of importance — Wikipedia page traffic over time. Rather than the traditional approach of classifying sentences via textual features (Schiffman et al., 2001) or locating events (Filatova and Hatzivassiloglou, 2004), we explore the use of an extrinsic source of information indicating what is interesting. Motivating this approach is our hypothesis that many people are most well-known for the events they were involved in. These events have previously been ordered temporally by supervised learning from textual features, (Filatova and Hovy, 2001), and our extrinsic information may assist with the temporal location of events with little temporal information mentioned in text. Various features of Wikipedia have been previously exploited in NLP, since they provide a 2.1 Timeseries Analysis To exploit the Wikipedia page traffic data, we need to extract peaks from timeseries data. There are many definitions of peaks in the literature on timeseries peak extraction, and many approaches to detecting them. Motivating much of this research is the need to automatically detect spikes in Electroencephalography results (EEG) (Wilson and Emerson, 20"
U14-1006,P08-2035,0,0.179752,"ent Extraction Using Wikipedia Traffic. In Proceedings of Australasian Language Technology Association Workshop, pages 41−49. massive source of human-written semi-structured information. Plain text has been used to assist named entity recognition (Nothman et al., 2013), page categories have been used to create an ontology (Suchanek et al., 2007) and infoboxes (keyvalue pairs of facts) have been used to provide additional context to information in text (Wu and Weld, 2010). Wikipedia’s revision history is exploited less frequently, but has proven useful to train a model of sentence compression (Yamangil and Nelken, 2008). We know of no prior work that aligns page traffic to text in Wikipedia. ing documents about an entity, selecting the most representative content of the text while adhering to length constraints. Early approaches to the task train a sentence classifier (Teufel and Moens, 1997) on a corpus of sentences which are in some way biographical. This corpus is typically existing biographies, or manually selected sentences from a larger corpus. Previous work has used Wikipedia as large, alternate source of biographical sentences (Biadsy et al., 2008), hypothesising that most sentences in Wikipedia’s ar"
U14-1006,J06-4003,0,0.061379,"Missing"
U14-1006,W04-3256,0,0.405085,"summarisation, by extracting important sentences from an entity’s Wikipedia page based on internet traffic to the page over time. Using a pilot data set, we found that it is feasible to extract key sentences about people’s notability without the need for a large annotated corpus. 1 Introduction “What is Julian Assange known for?” is a question which can be answered in many ways. Previous computational approaches to answering questions like these have focused on summarisation: selecting a subset of sentences from a group of documents relating to a person and ordering them (Biadsy et al., 2008; Zhou et al., 2004). Full text summaries do provide some insight into the notability of their subject, but can also contain superfluous information. To pinpoint the notoriety of individuals, we aim to extract the sentences from a document which show how the document’s subject is notable. We provide an alternate, unsupervised approach to the broader task of biography abstraction, which exploits external information about text, rather then extracting textual features directly. In this paper, we respond to “What is Julian Assange known for?” with sentences mentioning important events which have occurred in his life"
U14-1006,P01-1059,0,0.0436296,"m which bootstraps using a small set of seed facts to extract not only the patterns they represent, (e.g.@ [person] WORKS FOR [organisation]) but also to extract additional patterns. Liu et al. (2010) presented BIOSNOWBALL for the biographical fact extraction domain, which extracts biographical key-value pairs. It is the wide range of reasons for notoriety (which would require a large number of potential patterns to fill) motivating our novel source of measures of importance — Wikipedia page traffic over time. Rather than the traditional approach of classifying sentences via textual features (Schiffman et al., 2001) or locating events (Filatova and Hatzivassiloglou, 2004), we explore the use of an extrinsic source of information indicating what is interesting. Motivating this approach is our hypothesis that many people are most well-known for the events they were involved in. These events have previously been ordered temporally by supervised learning from textual features, (Filatova and Hovy, 2001), and our extrinsic information may assist with the temporal location of events with little temporal information mentioned in text. Various features of Wikipedia have been previously exploited in NLP, since the"
W02-0908,W00-1427,0,\N,Missing
W02-0908,C92-2082,0,\N,Missing
W02-0908,N01-1013,0,\N,Missing
W02-0908,P99-1016,0,\N,Missing
W02-0908,J92-4003,0,\N,Missing
W02-0908,P02-1030,1,\N,Missing
W02-0908,P98-2127,0,\N,Missing
W02-0908,C98-2122,0,\N,Missing
W02-0908,P93-1024,0,\N,Missing
W02-1029,P01-1005,0,0.0708851,"Missing"
W02-1029,P98-1029,0,0.0274894,"Missing"
W02-1029,J92-4003,0,0.00842723,"Missing"
W02-1029,N01-1013,0,0.0342489,"Missing"
W02-1029,W02-0908,1,0.890451,"Missing"
W02-1029,P02-1030,1,0.809906,"Missing"
W02-1029,W99-0623,0,0.0148289,"Missing"
W02-1029,W00-1427,0,0.0181119,"Missing"
W02-1029,A00-2009,0,0.0172358,"Missing"
W02-1029,P93-1024,0,0.0795556,"Missing"
W02-1029,A00-2007,0,0.0196678,"Missing"
W02-1029,P98-1081,0,0.0143748,"Missing"
W02-1029,C98-1078,0,\N,Missing
W02-1029,C98-1029,0,\N,Missing
W02-2008,rose-etal-2002-reuters,0,\N,Missing
W02-2008,P97-1048,0,\N,Missing
W02-2008,P02-1030,1,\N,Missing
W02-2008,W02-1030,0,\N,Missing
W02-2008,P01-1005,0,\N,Missing
W03-0407,A00-1031,0,0.703418,"data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model T N T tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the T N T tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labell"
W03-0407,W99-0613,0,0.226777,"ment-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy"
W03-0407,W02-2018,0,0.0219854,"d to define the conditional probabilities of a tag given some context. The advantage of ME models over the Markov model used by T N T is that arbitrary features can easily be included in the context; so as well as considering the target word and the previous two tags (which is the information T N T uses), the ME models also consider the words either side of the target word and, for unknown and infrequent words, various properties of the string of the target word. A disadvantage is that the training times for ME models are usually relatively slow, especially with iterative scaling methods (see Malouf (2002) for alternative methods). Here we use Generalised Iterative Scaling (Darroch and Ratcliff, 1972), but our implementation is much faster than Ratnaparkhi’s publicly available tagger. The C&C tagger trains in less than 7 minutes on the 1 million words of the Penn Treebank, and tags slightly faster than T N T. Since the taggers share many common features, one might think they are not different enough for effective co-training to be possible. In fact, both taggers are sufficiently different for co-training to be effective. Section 4 shows that both taggers can benefit significantly from the infor"
W03-0407,J94-2001,0,0.329951,"Missing"
W03-0407,W01-0501,0,0.305384,"gging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS tagge"
W03-0407,N01-1023,0,0.328731,"results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We i"
W03-0407,E03-1008,1,0.830279,"that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-tr"
W03-0407,W02-2006,0,0.0259342,"a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model T N T tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the T N T tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data should be accurate, but also useful, providing the tagger with new information. Our work differs from the Blum and Mitchell (1998)"
W03-0407,P95-1026,0,0.0370871,"ing literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly la"
W03-0407,E03-1071,1,0.88395,"ly used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model T N T tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the T N T tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data"
W03-0407,zavrel-daelemans-2000-bootstrapping,0,0.10034,"example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model T N T tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the T N T tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data should be accurate, but also useful, providing the tagger with new information. Our work differs f"
W03-0407,A94-1009,0,0.124495,"Missing"
W03-0407,P02-1046,0,\N,Missing
W03-0424,W02-2003,0,0.0166223,"entify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch. 1 Introduction Named Entity Recognition1 (NER) can be treated as a tagging problem where each word in a sentence is assigned a label indicating whether it is part of a named entity and the entity type. Thus methods used for part of speech (POS) tagging and chunking can also be used for NER. The papers from the CoNLL-2002 shared task which used such methods (e.g. Malouf (2002), Burger et al. (2002)) reported results significantly lower than the best system (Carreras et al., 2002). However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. We demonstrate this to be the case by improving on the best Dutch results from CoNLL-2002 using a maximum entropy (ME) tagger. We report reasonable precision and recall (84.9"
W03-0424,E99-1025,0,0.0103392,". yn given a sentence w1 . . . wn is approximated as follows: p(y1 . . . yn |w1 . . . wn ) ≈ n Y p(yi |xi ) (2) i=1 where xi is the context for word wi . The tagger uses beam search to find the most probable sequence given the sentence. The features are binary valued functions which pair a tag with various elements of the context; for example:  1 if word(x) = Moody & y = I-PER fj (x, y) = 0 otherwise (3) word(x) = Moody is an example of a contextual predicate. Generalised Iterative Scaling (GIS) is used to estimate the values of the weights. The tagger uses a Gaussian prior over the weights (Chen et al., 1999) which allows a large number of rare, but informative, features to be used without overfitting. Condition freq(wi ) < 5 ∀wi ∀wi ∀wi Contextual predicate X is prefix of wi , |X |≤ 4 X is suffix of wi , |X |≤ 4 wi contains a digit wi contains uppercase character wi contains a hyphen wi = X wi−1 = X, wi−2 = X wi+1 = X, wi+2 = X POS i = X POS i−1 = X, POS i−2 = X POS i+1 = X, POS i+2 = X NEi−1 = X NEi−2 NEi−1 = XY Condition freq(wi ) < 5 ∀wi ∀wi Table 1: Contextual predicates in baseline system 3 The Data We used three data sets: the English and German data for the CoNLL-2003 shared task (Tjong Ki"
W03-0424,E03-1071,1,0.458487,"t data, and an F-score of 68.4 for the CoNLL-2003 German test data. 1 We assume that NER involves assigning the correct label to an entity as well as identifying its boundaries. Incorporating a diverse set of overlapping features in a HMM-based tagger is difficult and complicates the smoothing typically used for such taggers. In contrast, a ME tagger can easily deal with diverse, overlapping features. We also use a Gaussian prior on the parameters for effective smoothing over the large feature space. 2 The ME Tagger The ME tagger is based on Ratnaparkhi (1996)’s POS tagger and is described in Curran and Clark (2003) . The tagger uses models of the form: ! n X 1 exp λi fi (x, y) (1) p(y|x) = Z(x) i=1 where y is the tag, x is the context and the fi (x, y) are the features with associated weights λi . The probability of a tag sequence y1 . . . yn given a sentence w1 . . . wn is approximated as follows: p(y1 . . . yn |w1 . . . wn ) ≈ n Y p(yi |xi ) (2) i=1 where xi is the context for word wi . The tagger uses beam search to find the most probable sequence given the sentence. The features are binary valued functions which pair a tag with various elements of the context; for example:  1 if word(x) = Moody & y"
W03-0424,W02-2024,0,0.0213805,"tting. Condition freq(wi ) < 5 ∀wi ∀wi ∀wi Contextual predicate X is prefix of wi , |X |≤ 4 X is suffix of wi , |X |≤ 4 wi contains a digit wi contains uppercase character wi contains a hyphen wi = X wi−1 = X, wi−2 = X wi+1 = X, wi+2 = X POS i = X POS i−1 = X, POS i−2 = X POS i+1 = X, POS i+2 = X NEi−1 = X NEi−2 NEi−1 = XY Condition freq(wi ) < 5 ∀wi ∀wi Table 1: Contextual predicates in baseline system 3 The Data We used three data sets: the English and German data for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) and the Dutch data for the CoNLL2002 shared task (Tjong Kim Sang, 2002). Each word in the data sets is annotated with a named entity tag plus POS tag, and the words in the German and English data also have a chunk tag. Our system does not currently exploit the chunk tags. There are 4 types of entities to be recognised: persons, locations, organisations, and miscellaneous entities not belonging to the other three classes. The 2002 data uses the IOB-2 format in which a B - XXX tag indicates the first word of an entity of type XXX and I - XXX is used for subsequent words in an entity of type XXX. The tag O indicates words outside of a named entity. The 2003 data use"
W03-0424,W02-2008,1,0.652247,"name and last name gazetteers as shown in Table 6. These gazetteers are used for predicates applied to the current, previous and next word in the window. Collins (2002) includes a number of interesting contextual predicates for NER. One feature we have adapted encodes whether the current word is more frequently seen lowercase than uppercase in a large external corpus. This feature is useful for disambiguating beginning of sentence capitalisation and tagging sentences which are all capitalised. The frequency counts have been obtained from 1 billion words of English newspaper text collected by Curran and Osborne (2002). Collins (2002) also describes a mapping from words to word types which groups words with similar orthographic forms into classes. This involves mapping characters to classes and merging adjacent characters of the same type. For example, Moody becomes Aa, A.B.C. becomes A.A.A. and 1,345.05 becomes 0,0.0. The classes are used to define unigram, bigram and trigram contextual predicates over the window. We have also defined additional composite features which are a combination of atomic features; for example, a feature which is active for mid-sentence titlecase words seen more frequently as lowe"
W03-0424,W02-2019,0,0.134021,"ormation and identify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch. 1 Introduction Named Entity Recognition1 (NER) can be treated as a tagging problem where each word in a sentence is assigned a label indicating whether it is part of a named entity and the entity type. Thus methods used for part of speech (POS) tagging and chunking can also be used for NER. The papers from the CoNLL-2002 shared task which used such methods (e.g. Malouf (2002), Burger et al. (2002)) reported results significantly lower than the best system (Carreras et al., 2002). However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. We demonstrate this to be the case by improving on the best Dutch results from CoNLL-2002 using a maximum entropy (ME) tagger. We report reasonable prec"
W03-0424,W02-2031,0,0.0155258,"o words with mixed lower- and uppercase (e.g. CityBank). The length predicates encode the number of characters in the word from 1 to 15, with a single predicate for lengths greater than 15. The next set of contextual predicates encode extra information about NE tags in the current context. The memory NE tag predicate (see e.g. Malouf (2002)) records the NE tag that was most recently assigned to the current word. The use of beam-search tagging means that tags can only be recorded from previous sentences. This memory is cleared at the beginning of each document. The unigram predicates (see e.g. Tsukamoto et al. (2002)) encode the most probable tag for the next words in the window. The unigram probabilities are relative frequencies obtained from the training data. This feature enables us to know something about the likely NE tag of the next word before reaching it. Most systems use gazetteers to encode information about personal and organisation names, locations and trigger words. There is considerable variation in the size of the gazetteers used. Some studies found that gazetteers did not improve performance (e.g. Malouf (2002)) whilst others gained significant improvement using gazetteers and triggers (e."
W03-0424,P02-1060,0,0.0610272,"guages and works effectively not only for English, but also for other languages such as German and Dutch. 1 Introduction Named Entity Recognition1 (NER) can be treated as a tagging problem where each word in a sentence is assigned a label indicating whether it is part of a named entity and the entity type. Thus methods used for part of speech (POS) tagging and chunking can also be used for NER. The papers from the CoNLL-2002 shared task which used such methods (e.g. Malouf (2002), Burger et al. (2002)) reported results significantly lower than the best system (Carreras et al., 2002). However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. We demonstrate this to be the case by improving on the best Dutch results from CoNLL-2002 using a maximum entropy (ME) tagger. We report reasonable precision and recall (84.9 F-score) for the CoNLL-2003 English test data, and an F-score of 68.4 for the CoNLL-2003 German test data. 1 We"
W03-0424,W02-2004,0,\N,Missing
W03-0424,W03-0419,0,\N,Missing
W03-0424,P02-1062,0,\N,Missing
W03-0806,P01-1005,0,0.0158163,". NLP is experiencing an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example, 10 million words of the American National Corpus (Ide et al., 2002) will have manually corrected POS tags, a tenfold increase over the Penn Treebank (Marcus et al., 1993), currently used for training POS taggers. This will require more efficient learning algorithms and implementations. However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work (Banko and Brill, 2001; Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data. Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web. Other potential applications must process text online or in realtime. For example, Google currently answers 250 million queries per day, thus processing time must be minimised. Clearly, efficient NLP comp"
W03-0806,H01-1017,0,0.0298311,"edure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP. Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI. This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed. There have already been several attempts to develop distributed NLP systems for dialogue systems (Bayer et al., 2001) and speech recognition (Hacioglu and Pellom, 2003). Web services will allow components developed by different researchers in different locations to be composed to build larger systems. Because web services are of great commercial interest they are already being supported strongly by many programming languages. For instance, web services can be accessed with very little code in Java, Python, Perl, C, C++ and Prolog. This allows us to provide NLP services to many systems that we could not otherwise support using a single interface definition. Since the service arguments and results are primaril"
W03-0806,A00-1031,0,0.022545,"omplex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The T N T POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second. The Weka package (Witten and Frank, 1999) provides a common framework for several existing machine learning methods including decision trees and support vector machines. This library has been very popular because it allows researchers to experiment with different methods without having to modify code or reformat data. Finally, the Natural Language Toolkit (NLTK) is a package of NLP components implemented in Python (Loper and Bird, 2002). Python scripting is extremely simple to learn, read"
W03-0806,W03-0407,1,0.902042,"back into memory between each step in the process, which will provide a significant performance increase. Further, we can use techniques for making string matching and other text processing very fast such as making only one copy of each lexical item or annotation in memory. We can also load a lexicon into memory that is shared between all of the components, reducing the memory use. The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and Clark, 2003; Clark et al., 2003). We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than T N T, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999). We expect even faster training times when we move to conjugate gradient methods. The next step of the process will be to add different statistical models and machine learning methods. We first plan to ad"
W03-0806,A97-2017,0,0.0214808,"uding a script language and GUI interfaces, and web services for distributed NLP system development. We seek feedback on the overall design and implementation of our proposed infrastructure and to promote discussion about software engineering best practice in NLP. 1 We use high performance to refer to both state of the art performance and high runtime efficiency. 2 Existing Systems 3 Performance Requirements There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI . For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002). GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system. A number of stand-alone tools have also been developed. For example, the suite of LT tools (Mikheev et"
W03-0806,E03-1071,1,0.891738,"ata to disk, and load it back into memory between each step in the process, which will provide a significant performance increase. Further, we can use techniques for making string matching and other text processing very fast such as making only one copy of each lexical item or annotation in memory. We can also load a lexicon into memory that is shared between all of the components, reducing the memory use. The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and Clark, 2003; Clark et al., 2003). We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than T N T, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999). We expect even faster training times when we move to conjugate gradient methods. The next step of the process will be to add different statistical models and machine learning methods"
W03-0806,P02-1030,1,0.903829,"n explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example, 10 million words of the American National Corpus (Ide et al., 2002) will have manually corrected POS tags, a tenfold increase over the Penn Treebank (Marcus et al., 1993), currently used for training POS taggers. This will require more efficient learning algorithms and implementations. However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work (Banko and Brill, 2001; Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data. Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web. Other potential applications must process text online or in realtime. For example, Google currently answers 250 million queries per day, thus processing time must be minimised. Clearly, efficient NLP components will need to be de"
W03-0806,A97-1051,0,0.154189,"services for distributed NLP system development. We seek feedback on the overall design and implementation of our proposed infrastructure and to promote discussion about software engineering best practice in NLP. 1 We use high performance to refer to both state of the art performance and high runtime efficiency. 2 Existing Systems 3 Performance Requirements There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI . For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002). GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system. A number of stand-alone tools have also been developed. For example, the suite of LT tools (Mikheev et al., 1999; Grover et al., 2000) perform token"
W03-0806,grover-etal-2000-lt,0,0.0241628,"embic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI . For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002). GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system. A number of stand-alone tools have also been developed. For example, the suite of LT tools (Mikheev et al., 1999; Grover et al., 2000) perform tokenization, tagging and chunking on XML marked-up text directly. These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997; Mohri et al., 1998). However, the source code for these tools is not freely available, so they cannot be extended. Efficiency has not been a focus for NLP research in general. However, it will"
W03-0806,ide-etal-2002-american,0,0.0183789,"xt-to-speech technology has made complete spoken dialogue systems feasible. Developing these complex NLP systems involves composing many different NLP tools. Unfortunately, this is difficult because many implementations have not been designed as components and only recently has input/output standardisation been considered. Finally, these tools can be difficult to customise and tune for a particular task. NLP is experiencing an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example, 10 million words of the American National Corpus (Ide et al., 2002) will have manually corrected POS tags, a tenfold increase over the Penn Treebank (Marcus et al., 1993), currently used for training POS taggers. This will require more efficient learning algorithms and implementations. However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work (Banko and Brill, 2001; Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data. Also, many potential applications of NLP will involve processing very large text data"
W03-0806,W02-0109,0,0.227902,"ast tagging (Roche and Schabes, 1997). The T N T POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second. The Weka package (Witten and Frank, 1999) provides a common framework for several existing machine learning methods including decision trees and support vector machines. This library has been very popular because it allows researchers to experiment with different methods without having to modify code or reformat data. Finally, the Natural Language Toolkit (NLTK) is a package of NLP components implemented in Python (Loper and Bird, 2002). Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple. As discussed earlier, there are two main requirements of the system that are covered by “high performance”: speed and state of the art accuracy. Efficiency is required both in training and processing. Efficient training is required because the amount of data available for training will increase significantly. Also, advanced methods often require many training iterations, for example active learning (Dagan and Engelson, 1995) and co-training (Blum and Mit"
W03-0806,W02-2018,0,0.042193,"icular techniques, such as finite state machines (Karttunen et al., 1997; Mohri et al., 1998). However, the source code for these tools is not freely available, so they cannot be extended. Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The T N T POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second. The Weka package (Witten and Frank, 1999) provides a common framework for several existing machine learning methods including decision trees and support vector machines. Thi"
W03-0806,J93-2004,0,0.0246806,"NLP systems involves composing many different NLP tools. Unfortunately, this is difficult because many implementations have not been designed as components and only recently has input/output standardisation been considered. Finally, these tools can be difficult to customise and tune for a particular task. NLP is experiencing an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example, 10 million words of the American National Corpus (Ide et al., 2002) will have manually corrected POS tags, a tenfold increase over the Penn Treebank (Marcus et al., 1993), currently used for training POS taggers. This will require more efficient learning algorithms and implementations. However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work (Banko and Brill, 2001; Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data. Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biolo"
W03-0806,N01-1006,0,0.0196776,"code for these tools is not freely available, so they cannot be extended. Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The T N T POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second. The Weka package (Witten and Frank, 1999) provides a common framework for several existing machine learning methods including decision trees and support vector machines. This library has been very popular because it allows researchers to experiment with different methods without having to modify"
W03-0806,J01-2002,0,0.0320837,"Missing"
W03-0806,P06-4018,0,\N,Missing
W03-0806,J95-2004,0,\N,Missing
W03-1013,J97-4005,0,0.203373,"ebank to be used for estimation. 1 Introduction Statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG, Steedman (2000)) and used in wide-coverage parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002). An attraction of CCG is its elegant treatment of coordination and extraction, allowing recovery of the long-range dependencies inherent in these constructions. We would like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars; hence Hockenmaier and Steedman do not include such dependencies in their model, and Clark et al. include the dependencies but use an inconsistent model. Following Abney, we propose a loglinear framework which incorporates long-range dependencies as features without loss of consistency. Log-linear models have previously been applied to statistical parsing (Johnson et al., 1999; Toutanova et al., 2002; Riezler et al., 2002; Osborne, 2000). Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse. For g"
W03-1013,P02-1042,1,0.863843,"stical parsing, under the assumption that all possible parses for a sentence can be enumerated. Enumerating all parses is infeasible for large grammars; however, dynamic programming over a packed chart can be used to efficiently estimate the model parameters. We describe a parellelised implementation which runs on a Beowulf cluster and allows the complete WSJ Penn Treebank to be used for estimation. 1 Introduction Statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG, Steedman (2000)) and used in wide-coverage parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002). An attraction of CCG is its elegant treatment of coordination and extraction, allowing recovery of the long-range dependencies inherent in these constructions. We would like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars; hence Hockenmaier and Steedman do not include such dependencies in their model, and Clark et al. include the dependencies but use an inconsistent model. Following Abney, we propose a loglinear framework wh"
W03-1013,W02-2018,0,0.0497924,"obability mass for the outside (at least for the feature forests we have defined). 8 {d |d ∈δ(c),d ,d} i The normalisation constant ZS is the sum of the inside scores for the root disjunctive nodes: X ZS = φdr (16) dr ∈R In order to calculate inside scores, the scores for daughter nodes need to be calculated before the scores for mother nodes (and vice versa for the outside scores). This can easily be achieved by ordering the nodes in the bottom-up CKY parsing order. Note that the inside-outside approach can be combined with any maximum entropy estimation procedure, such as those evaluated by Malouf (2002). Finally, in order to avoid overfitting, we use a Gaussian prior on the parameters of the model (Chen and Rosenfeld, 1999), which requires a slight modification to the update rule in (10). A Gaussian prior also handles the problem of “pseudo-maximal” features (Johnson et al., 1999). 6 The Parser The parser is based on Clark et al. (2002) and takes as input a POS-tagged sentence with a set of possible lexical categories assigned to each word. The supertagger of Clark (2002) provides the lexical categories, with a parameter setting which assigns around 4 categories per word on average. The pars"
W03-1013,C00-1085,0,0.205358,"model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars; hence Hockenmaier and Steedman do not include such dependencies in their model, and Clark et al. include the dependencies but use an inconsistent model. Following Abney, we propose a loglinear framework which incorporates long-range dependencies as features without loss of consistency. Log-linear models have previously been applied to statistical parsing (Johnson et al., 1999; Toutanova et al., 2002; Riezler et al., 2002; Osborne, 2000). Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse. For grammars extracted from the Penn Treebank (in our case CCGbank (Hockenmaier, 2003)), enumerating all parses is infeasible. One approach to this problem is to sample the parse space for estimation, e.g. Osborne (2000). In this paper we use a dynamic programming technique applied to a packed chart, similar to those proposed by Geman and Johnson (2002) and Miyao and Tsujii (2002), which efficiently estimates the model parameters over the complete space without enumeratin"
W03-1013,E03-1071,1,0.193226,"value of fi for a parse is then the sum of the values of fi for each conjunctive node in the parse. 5 Estimation using GIS GIS is a very simple algorithm for estimating the parameters of a log-linear model. The parameters are initialised to some arbitrary constant and the following update rule is applied until convergence: µ(t+1) i = µ(t) i E p˜ fi E p(t) fi ! C1 (10) where (t) is the iteration index and the constant C P is defined as maxω,S i fi (ω). In practice C is maximised over the sentences in the training data. Implementations of GIS typically use a “correction feature”, but following Curran and Clark (2003) we do not use such a feature, which simplifies the algorithm. Calculating E p(t) fi requires summing over all derivations which include fi for each packed chart in the training data. The key to performing this sum efficiently is to write the sum in terms of inside and outside scores for each conjunctive node. The inside and outside scores can be defined recursively, as in the inside-outside algorithm for PCFGs. If the inside score for a conjunctive node c is denoted φc , and the 5 Miyao and Tsujii have a single root conjunctive node; the disjunctive root nodes we define correspond to the root"
W03-1013,P02-1035,0,0.0917698,"ould like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars; hence Hockenmaier and Steedman do not include such dependencies in their model, and Clark et al. include the dependencies but use an inconsistent model. Following Abney, we propose a loglinear framework which incorporates long-range dependencies as features without loss of consistency. Log-linear models have previously been applied to statistical parsing (Johnson et al., 1999; Toutanova et al., 2002; Riezler et al., 2002; Osborne, 2000). Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse. For grammars extracted from the Penn Treebank (in our case CCGbank (Hockenmaier, 2003)), enumerating all parses is infeasible. One approach to this problem is to sample the parse space for estimation, e.g. Osborne (2000). In this paper we use a dynamic programming technique applied to a packed chart, similar to those proposed by Geman and Johnson (2002) and Miyao and Tsujii (2002), which efficiently estimates the model parameters over the complete space wi"
W03-1013,P02-1036,0,0.314858,"ar models have previously been applied to statistical parsing (Johnson et al., 1999; Toutanova et al., 2002; Riezler et al., 2002; Osborne, 2000). Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse. For grammars extracted from the Penn Treebank (in our case CCGbank (Hockenmaier, 2003)), enumerating all parses is infeasible. One approach to this problem is to sample the parse space for estimation, e.g. Osborne (2000). In this paper we use a dynamic programming technique applied to a packed chart, similar to those proposed by Geman and Johnson (2002) and Miyao and Tsujii (2002), which efficiently estimates the model parameters over the complete space without enumerating parses. The estimation method is similar to the inside-outside algorithm used for estimating a PCFG (Lari and Young, 1990). Miyao and Tsujii (2002) apply their estimation technique to an automatically extracted Tree Adjoining Grammar using Improved Iterative Scaling (IIS, Della Pietra et al. (1997)). However, their model has significant memory requirements which limits them to using 868 sentences as training data. We use a parallelised version of Generalised Iterative Scal"
W03-1013,P02-1043,0,0.51676,"r the assumption that all possible parses for a sentence can be enumerated. Enumerating all parses is infeasible for large grammars; however, dynamic programming over a packed chart can be used to efficiently estimate the model parameters. We describe a parellelised implementation which runs on a Beowulf cluster and allows the complete WSJ Penn Treebank to be used for estimation. 1 Introduction Statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG, Steedman (2000)) and used in wide-coverage parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002). An attraction of CCG is its elegant treatment of coordination and extraction, allowing recovery of the long-range dependencies inherent in these constructions. We would like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars; hence Hockenmaier and Steedman do not include such dependencies in their model, and Clark et al. include the dependencies but use an inconsistent model. Following Abney, we propose a loglinear framework which incorporates long-range depen"
W03-1013,P99-1069,0,0.571643,"ndencies inherent in these constructions. We would like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars; hence Hockenmaier and Steedman do not include such dependencies in their model, and Clark et al. include the dependencies but use an inconsistent model. Following Abney, we propose a loglinear framework which incorporates long-range dependencies as features without loss of consistency. Log-linear models have previously been applied to statistical parsing (Johnson et al., 1999; Toutanova et al., 2002; Riezler et al., 2002; Osborne, 2000). Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse. For grammars extracted from the Penn Treebank (in our case CCGbank (Hockenmaier, 2003)), enumerating all parses is infeasible. One approach to this problem is to sample the parse space for estimation, e.g. Osborne (2000). In this paper we use a dynamic programming technique applied to a packed chart, similar to those proposed by Geman and Johnson (2002) and Miyao and Tsujii (2002), which efficiently estimates t"
W03-1013,J96-1002,0,\N,Missing
W04-3215,J99-2004,0,\N,Missing
W04-3215,A00-2018,0,\N,Missing
W04-3215,C04-1180,1,\N,Missing
W04-3215,C04-1041,1,\N,Missing
W04-3215,J03-4003,0,\N,Missing
W04-3215,P02-1043,1,\N,Missing
W04-3215,P02-1018,0,\N,Missing
W04-3215,briscoe-carroll-2002-robust,0,\N,Missing
W04-3215,P02-1042,1,\N,Missing
W04-3215,P03-1055,0,\N,Missing
W04-3215,P03-1046,0,\N,Missing
W04-3215,P04-1014,1,\N,Missing
W04-3215,W97-1505,0,\N,Missing
W05-1011,J92-4003,0,0.0374172,"scalability. The Spatial Approximation Sample Hierarchy (SASH), proposed by Houle (2003b), is a data structure for approximate nearestneighbour queries that balances the efficiency/approximation trade-off. We have intergrated this into an existing distributional similarity system, tripling efficiency with a minor accuracy penalty. 1 Introduction With the development of WordNet (Fellbaum, 1998) and large electronic thesauri, information from lexical semantic resources is regularly used to solve NLP problems. These problems include collocation discovery (Pearce, 2001), smoothing and estimation (Brown et al., 1992; Clark and Weir, 2001) and question answering (Pasca and Harabagiu, 2001). Unfortunately, these resources are expensive and time-consuming to create manually, and tend to suffer from problems of bias, inconsistency, and limited coverage. In addition, lexicographers cannot keep up with constantly evolving language use and cannot afford to build new resources for the many subdomains that NLP techniques are being applied to. There is a clear need for methods to extract lexical semantic resources automatically or tools that assist in their manual creation and maintenance. Much of the existing wor"
W05-1011,N01-1013,0,0.094233,"tial Approximation Sample Hierarchy (SASH), proposed by Houle (2003b), is a data structure for approximate nearestneighbour queries that balances the efficiency/approximation trade-off. We have intergrated this into an existing distributional similarity system, tripling efficiency with a minor accuracy penalty. 1 Introduction With the development of WordNet (Fellbaum, 1998) and large electronic thesauri, information from lexical semantic resources is regularly used to solve NLP problems. These problems include collocation discovery (Pearce, 2001), smoothing and estimation (Brown et al., 1992; Clark and Weir, 2001) and question answering (Pasca and Harabagiu, 2001). Unfortunately, these resources are expensive and time-consuming to create manually, and tend to suffer from problems of bias, inconsistency, and limited coverage. In addition, lexicographers cannot keep up with constantly evolving language use and cannot afford to build new resources for the many subdomains that NLP techniques are being applied to. There is a clear need for methods to extract lexical semantic resources automatically or tools that assist in their manual creation and maintenance. Much of the existing work on automatically extr"
W05-1011,W02-0908,1,0.880244,"resources is based on the distributional hypothesis that similar words appear in similar contexts. Existing approaches differ primarily in their definition of “context”, e.g. the surrounding words or the entire document, and their choice of distance metric for calculating similarity between the vector of contexts representing each term. Finding synonyms using distributional similarity involves performing a nearest-neighbour search over the context vectors for each term. This is very computationally intensive and scales according to the vocabulary size and the number of contexts for each term. Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. Unfortunately, this also increases the vocabulary size and the number of contexts for each term, making the use of huge datasets infeasible. There have been many data structures and approximation algorithms proposed to reduce the computational complexity of nearest-neighbour search (Ch´avez et al., 2001). Many of these approaches reduce the search space by using clustering techniques to generate an index of near-neighbours. We use the Spacial Approximation Sam"
W05-1011,P02-1030,1,0.896223,"resources is based on the distributional hypothesis that similar words appear in similar contexts. Existing approaches differ primarily in their definition of “context”, e.g. the surrounding words or the entire document, and their choice of distance metric for calculating similarity between the vector of contexts representing each term. Finding synonyms using distributional similarity involves performing a nearest-neighbour search over the context vectors for each term. This is very computationally intensive and scales according to the vocabulary size and the number of contexts for each term. Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. Unfortunately, this also increases the vocabulary size and the number of contexts for each term, making the use of huge datasets infeasible. There have been many data structures and approximation algorithms proposed to reduce the computational complexity of nearest-neighbour search (Ch´avez et al., 2001). Many of these approaches reduce the search space by using clustering techniques to generate an index of near-neighbours. We use the Spacial Approximation Sam"
W05-1011,W00-1427,0,0.0619711,"small attribute vectors making the use of huge datasets infeasible. 2.1 3.1 Extraction Method (w, r, w′ ) A context relation is defined as a tuple where w is a term, which occurs in some grammatical relation r with another word w′ in some sentence. We refer to the tuple (r, w′ ) as an attribute of w. For example, (dog, diect-obj, walk) indicates that dog was the direct object of walk in a sentence. Context extraction begins with a Maximum Entropy POS tagger and chunker (Ratnaparkhi, 1996). The Grefenstette (1994) relation extractor produces context relations that are then lemmatised using the Minnen et al. (2000) morphological analyser. The relations for each term are collected together and counted, producing a context vector of attributes and their frequencies in the corpus. 2.2 Measures and Weights Both nearest-neighbour and cluster analysis methods require a distance measure that calculates the similarity between context vectors. Curran (2004) decomposes this measure into measure and weight functions. The measure function calculates the similarity between two weighted context vectors and the weight function calculates a weight from the raw frequency information for each context relation. The SASH r"
W05-1011,W96-0213,0,\N,Missing
W06-1654,2005.mtsummit-papers.11,0,0.0120969,"Missing"
W06-1654,C88-1016,0,0.0578589,"Missing"
W06-1654,P02-1030,1,0.829964,"ese relationships from a corpus representative of the task. Manually created resources are expensive and time-consuming to create, and tend to suffer from problems of bias, inconsistency, and limited coverage. These problems may result in an inappropriate vocabulary, where some terms are not present or an unbalanced set of synonyms. In a medical context it is more likely that administration will refer to the giving of medicine than to paper work, whereas in a business context the converse is more likely. The most common method for automatically creating these resources uses distributional simiCurran and Moens (2002) found that dramatically increasing the volume of raw input data for distributional similarity tasks increases the accuracy of synonyms extracted. Random Indexing performs poorly on these volumes of data. Noting that in many NLP tasks, including distributional similarity, statistical weighting is used to improve performance, we modify the Random Indexing algorithm to allow for weighted contexts. We test the performance of the original and our modified system using existing evaluation metrics. We further evaluate against bilingual lexicon extraction using distributional similarity (Sahlgren and"
W06-1654,A97-1011,0,0.111043,"Missing"
W06-1654,P06-1046,1,0.838407,"Missing"
W07-0610,P06-1046,1,0.808424,"ludes the most similar words. 5 k=5 *k=5 k=50 *k=50 Distributional Similarity Networks Lexical semantic resources can be automatically extracted using distributional similarity. Here words are projected into a vector space using the contexts in which they appear as axes. Contexts can be as 78 1e-05 1 10 100 k 1000 10000 Figure 3: Degree distributions of Jaccard wide as document (Landauer and Dumais, 1997) or close as grammatical dependencies (Grefenstette, 1994). The distance between words in this space approximates the similarity measured by synonymy. We use the noun similarities produced by Gorman and Curran (2006) using the weighted Jaccard measure and the t-test weight and grammatical relations extracted from their L ARGE corpus, the method found to perform best against their goldstandard evaluation. Only words with a corpus frequency higher than 100 are included. This method is comparable to that used in LSA, although using grammatical relations as context produces similarity much more like synonymy than those taken at a document level (Kilgarriff and Yallop, 2000). Distributional similarity produces a list of vocabulary words, their similar neighbours and the similarity to the neighbours. These list"
W07-0610,kilgarriff-yallop-2000-whats,0,0.142109,"rse (in cognitive science). When the words share a root or meanings are close, the relationship is called polysemy. This distinction is significant in language acquisition, but as yet little research has been performed on the learning of polysemes (Casenhiser, 2005). It is also significant for Natural Language Processing. The effect of disambiguating homonyms is markedly different from polysemes in Information Retrieval (Stokoe, 2005). We do not have access to these distinctions, as they are not available in most resources, nor are there techniques to automatically acquire these distinctions (Kilgarriff and Yallop, 2000). For simplicity, will conflate the categories under homonymy. There have been several studies into synonymy and homonymy acquisition in children, and these have shown that it lags behind vocabulary growth (Doherty and Perner, 1998; Garnham et al., 2000). A child will associate both rabbit and bunny with the same concept, but before the age of four, most children have difficulty in choosing the word bunny if they have already been presented with the word rabbit. Similarly, a young child asked to point to two pictures that have the same name but mean different things will have difficulty, despi"
W07-0610,H05-1051,0,0.0229101,"s when a word has multiple meanings. Formally, homonymy is occurs when words do not share an etymological root (in linguistics) or when the distinction between meanings is coarse (in cognitive science). When the words share a root or meanings are close, the relationship is called polysemy. This distinction is significant in language acquisition, but as yet little research has been performed on the learning of polysemes (Casenhiser, 2005). It is also significant for Natural Language Processing. The effect of disambiguating homonyms is markedly different from polysemes in Information Retrieval (Stokoe, 2005). We do not have access to these distinctions, as they are not available in most resources, nor are there techniques to automatically acquire these distinctions (Kilgarriff and Yallop, 2000). For simplicity, will conflate the categories under homonymy. There have been several studies into synonymy and homonymy acquisition in children, and these have shown that it lags behind vocabulary growth (Doherty and Perner, 1998; Garnham et al., 2000). A child will associate both rabbit and bunny with the same concept, but before the age of four, most children have difficulty in choosing the word bunny i"
W07-0610,W96-0213,0,\N,Missing
W07-1023,W97-1311,0,0.0316736,"fy the importance of coreference expressions, instances in our corpus are annotated with pronominal, sortal and event anaphoric, and cataphoric expressions, including those extending beyond one sentence. Instances 4–6 in Tables 4– 5, each contain annotated pronominal or sortal anaphoric expressions. Instance 5 also involves a cataphoric expression, where suppressor proteins refers to p16INK4a and p19ARF Event anaphora refer to processes and are quite common in biomedical text. We have annotated these separately to pronominal and sortal anaphora. Our event anaphora annotations are different to Humphreys et al. (1997). They associate sequential events, while we only refer to the same event. An example is shown in instance 6 (Table 5) where the additional sortal anaphor complicates resolving the event anaphor. The third this refers to the phosphorylation event, phosphorylated, and not the protein cdc25-C like the second this. 8 Locating Facts The key facts and results are generally repeated and reworded in various contexts within an article. This redundancy can be used in two ways to improve system precision and recall. Firstly, the redundancy increases the chance of an imperfect system identifying at least"
W07-1023,P06-2083,0,0.0152274,"he fact to be derived from the original instance. Dependencies may occur elsewhere in the document or may not be mentioned at all. We consider two types of dependencies: synonym facts and extra facts. 5.1 Synonym Facts The frequent use of synonyms, abbreviations and acronyms in biomedical text is a common source of ambiguity that is often hard to resolve (Sehgal et al., 2004). Furthermore, synonym lists are difficult to maintain in rapidly moving fields like biology (Lussier et al., 2006). There has been recent interest in developing systems to identify and extract these (Ao and Takagi, 2005; Okazaki and Ananiadou, 2006). In our corpus we group all of these synonyms, abbreviations, acronyms and other orthographic variations as synonym facts. For example, the synonyms (1) E2F4, (2) E2F-4 and (3) E2F1-4 in our corpus refer to the same entity E2F4, however term (3) also includes the entities E2F1, E2F2 and E2F3. In Table 2, an instance supporting subfact 1. is shown in 1.a). The bio-entity pRb mentioned in the subfact does not appear in this instance. Thus 1.a) depends on knowing that pRb is equivalent to RB, and so we form a new synonym fact. This synonym 174 is undefined in the article and cannot be assumed as"
W07-1023,W04-1212,0,0.0295992,"P researchers, who have begun investigating how specific sections and structures can be mined in various information extraction tasks. Regev et al. (2002) developed the first bioIR system specifically focusing on limited text sections. Their performance in the KDD Cup Challenge, primarily using Figure legends, showed the importance of considering document structure. Yu et al. (2002) showed that the Introduction defines the majority of synonyms, while Schuemie et al. (2004) and Shah et al. (2003) showed that the Results and Methods are the most and least informative, respectively. In contrast, Sinclair and Webber (2004) found the Methods useful in assigning Gene Ontology codes to articles. These section specific results highlight the information loss resulting from restricting searches to individual sections, as sections often provide unique information. Furthermore, facts appearing in different contexts across various sections, will be lost. This redundancy has been used for passage validation and ranking (Clarke et al., 2001). There are limited training resources for biomedical full-text systems. The majority of corpora consist of abstracts annotated for bio-entity recognition and Relationship Extraction,"
W07-1023,P06-4005,0,\N,Missing
W07-1202,P96-1011,0,0.11745,"s. In summary, the perceptron learner appears highly robust to the order of the training examples, at least for this parsing task. 6 Comparison with Other Work Taskar et al. (2004) investigate discriminative training methods for a phrase-structure parser, and also use dynamic programming for the decoder. The key difference between our work and theirs is that they are only able to train on sentences of 15 words or less, because of the expense of the decoding. There is work on discriminative models for dependency parsing (McDonald, 2006); since there are efficient decoding algorithms available (Eisner, 1996b), complete resources such as the Penn Treebank can used for estimation, leading to accurate parsers. There is also work on discriminative models for parse reranking (Collins and Koo, 2005). The main drawback with this approach is that the correct 15 parse may get lost in the first phase. The existing work most similar to ours is Collins and Roark (2004). They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation. The main difference is that we are able to store the complete forests for training, and can guarantee that the forest contains the correct der"
W07-1202,C96-1058,0,0.0283044,"s. In summary, the perceptron learner appears highly robust to the order of the training examples, at least for this parsing task. 6 Comparison with Other Work Taskar et al. (2004) investigate discriminative training methods for a phrase-structure parser, and also use dynamic programming for the decoder. The key difference between our work and theirs is that they are only able to train on sentences of 15 words or less, because of the expense of the decoding. There is work on discriminative models for dependency parsing (McDonald, 2006); since there are efficient decoding algorithms available (Eisner, 1996b), complete resources such as the Penn Treebank can used for estimation, leading to accurate parsers. There is also work on discriminative models for parse reranking (Collins and Koo, 2005). The main drawback with this approach is that the correct 15 parse may get lost in the first phase. The existing work most similar to ours is Collins and Roark (2004). They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation. The main difference is that we are able to store the complete forests for training, and can guarantee that the forest contains the correct der"
W07-1202,P02-1043,0,0.03414,"s for the complete CCGbank require over 20 GB of RAM . Reading the training instances into memory one at a time and keeping a record of the relevant feature counts would be too slow for practical development, since the log-linear model requires hundreds of iterations to converge. Hence the packed charts need to be stored in memory. In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. The need for cluster computing resources presents a barrier to the development of further CCG parsing models. Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. In this paper we propose the perceptron algorithm as a solution. The perceptron is an online learning algorithm, and so the parameters are updated one training instance at a time. However, the key difference compared with the loglinear training is that the perceptron converges in many fewer iterations, and so it is practical to read the training instances into memory one"
W07-1202,J99-2004,0,0.580946,"me training criterion. Hence, for efficient training, these methods require an efficient decoder; in fact, for methods like the perceptron, the update procedure is so trivial that the training algorithm essentially is decoding. This paper describes a decoder for a lexicalizedgrammar parser which is efficient enough for practical discriminative training. We use a lexicalized phrase-structure parser, the CCG parser of Clark and Curran (2004b), together with a DP-based decoder. The key idea is to exploit the properties of lexicalized grammars by using a finite-state supertagger prior to parsing (Bangalore and Joshi, 1999; Clark and Curran, 2004a). The decoder still uses the CKY algorithm, so the worst case complexity of 9 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 9–16, c Prague, Czech Republic, June, 2007. 2007 Association for Computational Linguistics the parsing is unchanged; however, by allowing the supertagger to do much of the parsing work, the efficiency of the decoder is greatly increased in practice. We chose the perceptron for the training algorithm because it has shown good performance on other NLP tasks; in particular, Collins (2002) reported good performance for a p"
W07-1202,P06-4020,0,0.0508906,"Missing"
W07-1202,C04-1041,1,0.0464384,"of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, the memory requirements can be prohibitive, especially for automatically extracted, wide-coverage grammars. In Clark and Curran (2004b) we u"
W07-1202,P04-1014,1,0.920905,"Missing"
W07-1202,P07-1032,1,0.881951,"Missing"
W07-1202,J05-1003,0,0.02199,"investigate discriminative training methods for a phrase-structure parser, and also use dynamic programming for the decoder. The key difference between our work and theirs is that they are only able to train on sentences of 15 words or less, because of the expense of the decoding. There is work on discriminative models for dependency parsing (McDonald, 2006); since there are efficient decoding algorithms available (Eisner, 1996b), complete resources such as the Penn Treebank can used for estimation, leading to accurate parsers. There is also work on discriminative models for parse reranking (Collins and Koo, 2005). The main drawback with this approach is that the correct 15 parse may get lost in the first phase. The existing work most similar to ours is Collins and Roark (2004). They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation. The main difference is that we are able to store the complete forests for training, and can guarantee that the forest contains the correct derivation (assuming the grammar is able to generate it given the correct lexical categories). The downside of our approach is the restriction on the locality of the features, to allow dynamic"
W07-1202,P04-1015,0,0.333524,"h of the parsing work, resulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the feat"
W07-1202,W02-1001,0,0.914374,"gger prior to parsing (Bangalore and Joshi, 1999; Clark and Curran, 2004a). The decoder still uses the CKY algorithm, so the worst case complexity of 9 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 9–16, c Prague, Czech Republic, June, 2007. 2007 Association for Computational Linguistics the parsing is unchanged; however, by allowing the supertagger to do much of the parsing work, the efficiency of the decoder is greatly increased in practice. We chose the perceptron for the training algorithm because it has shown good performance on other NLP tasks; in particular, Collins (2002) reported good performance for a perceptron tagger compared to a Maximum Entropy tagger. Like Collins (2002), the decoder is the same for both the perceptron and the log-linear parsing models; the only change is the method for setting the weights. The perceptron model performs as well as the loglinear model, but is considerably easier to train. Another contribution of this paper is to advance wide-coverage CCG parsing. Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. In this paper we reduce the memory requirements from 20 GB of RAM"
W07-1202,P99-1069,0,0.0551292,"chine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, the memory requirements can be prohibitive, especially for automatically extracted, wide-cover"
W07-1202,P05-1011,0,0.063773,"Missing"
W07-1202,P02-1035,0,0.0246402,"using a finite-state supertagger to do much of the parsing work, resulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be u"
W07-1202,N06-1021,0,0.0186896,"n Standard order Shortest first Longest first 1 86.14 85.98 86.25 2 86.30 86.41 86.48 3 86.53 86.57 86.66 4 86.61 86.56 86.72 5 86.69 86.54 86.74 6 86.72 86.53 86.75 Table 5: F-score of the averaged perceptron on the development data for different data orderings (β = 0.002) perceptron model LP LR F CAT standard order 87.50 86.62 87.06 94.08 best random order 87.52 86.72 87.12 94.12 averaged 87.53 86.67 87.10 94.09 Table 6: Comparison of various perceptron models on the test data Finally, we used the 10 models (including the model from the original training set) to investigate model averaging. Corston-Oliver et al. (2006) motivate model averaging for the perceptron in terms of Bayes Point Machines. The averaged perceptron weights resulting from each permutation of the training data were simply averaged to produce a new model. Table 6 shows that the averaged model again performs only marginally better than the original model, and not as well as the best-performing “random” model, which is perhaps not surprising given the small variation among the performances of the component models. In summary, the perceptron learner appears highly robust to the order of the training examples, at least for this parsing task. 6"
W07-1202,W04-3201,0,0.476772,"supertagger to do much of the parsing work, resulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the"
W07-1202,P06-1088,1,0.931104,"he model are defined over local parts of the derivation and include wordword dependencies. A packed chart representation allows efficient decoding, with the Viterbi algorithm finding the most probable derivation. The supertagger is a key part of the system. It uses a log-linear model to define a distribution over the lexical category set for each word and the previous two categories (Ratnaparkhi, 1996) and the forward backward algorithm efficiently sums over all histories to give a distibution for each word. These distributions are then used to assign a set of lexical categories to each word (Curran et al., 2006). Supertagging was first defined for LTAG (Bangalore and Joshi, 1999), and was designed to increase parsing speed for lexicalized grammars by allowing a finite-state tagger to do some of the parsing work. Since the elementary syntactic units in a lexicalized grammar — in LTAG’s case elementary trees and in CCG’s case lexical categories – contain a significant amount of grammatical information, combining them together is easier than the parsing typically performed by phrase-structure parsers. Hence Bangalore and Joshi (1999) refer to supertagging as almost parsing. Supertagging has been especia"
W07-1202,P06-1110,0,0.0177231,"sulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local"
W07-1212,U04-1020,1,0.814859,"s for NLP applications to be explored, and support current research using SFG for applied linguistics. The conversion process relies on a set of manually coded rules. The first step of the process is to collect SFG clauses and their constituents from parses in the Penn Treebank. Each clause constituent is then assigned up to three function labels, for the three simultaneous semantic and pragmatic structures Halliday (1970) describes. Finally, the system features are calculated, using rules referring to the function labels assigned in the previous step. This paper extends the work described in Honnibal (2004). 89 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 89–96, c Prague, Czech Republic, June, 2007. 2007 Association for Computational Linguistics 2 Related Work Converting the Penn Treebank is the standard approach to creating a corpus annotated according to a specific linguistic theory. This has been the method used to create LTAG (Frank, 2001), LFG (Frank et al., 2003) and CCG (Hockenmaier and Steedman, 2005) corpora, among others. We employ a similar methodology, converting the corpus using manually specified rules. Since the SFG annotation is semantically oriented,"
W07-1212,P83-1012,0,0.549628,"functions are Subject and Finite, since the relative position of the constituents bearing these labels largely determines whether the clause will be a question, statement or command. The textual structure of the clause includes the functions Theme and Rheme, following Halliday’s (1970) theory of information structure. Finally, the experiential function of a constituent is its semantic role, described in terms of a small set of labels that are only minimally sensitive to the semantics of the predicate. 4 Annotation Implemented We base our annotation on the clause network in the Nigel grammar (Mann and Matthiessen, 1983), as it is freely available and discussed at length in Matthiessen (1995). It is difficult to include annotation from the group and phrase networks, because of the flat bracketing of constituents in the Penn Treebank. The converted corpus has full coverage over all sections of the Penn Treebank 3 corpus. We implement features from 41 systems from the clause network, out of a possible 62. The most prominent missing features relate to process type. The process type system classifies clauses as one of four broad semantic types: material, mental, verbal or relational, with subsequent systems makin"
W07-1212,J05-1004,0,0.0367324,"inguistic Processing, pages 89–96, c Prague, Czech Republic, June, 2007. 2007 Association for Computational Linguistics 2 Related Work Converting the Penn Treebank is the standard approach to creating a corpus annotated according to a specific linguistic theory. This has been the method used to create LTAG (Frank, 2001), LFG (Frank et al., 2003) and CCG (Hockenmaier and Steedman, 2005) corpora, among others. We employ a similar methodology, converting the corpus using manually specified rules. Since the SFG annotation is semantically oriented, the work also bears some resemblance to Propbank (Palmer et al., 2005). However, Propbank is concerned with manually adding information to the Penn Treebank, rather than automatically reinterpreting the same information through the lens of a different linguistic theory. We chose not to base our conversion on the Propbank annotation, as it does not currently cover the Brown or Switchboard sections of the Treebank. The wider variety of genres provided by these sections makes the corpus much more useful for SFG, since the theory devotes significant attention to pragmatic phenomena and stylistic variation. 3 Systemic Functional Grammar Generating a constituent using"
W07-1212,J93-2004,0,\N,Missing
W07-2206,J99-2004,0,0.158496,"an, 2004b). The attraction of linguistically motivated parsers is the potential to produce rich output, in particular the predicate-argument structure representing the underlying meaning of a sentence. The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al., 2004). The C&C CCG parser (Clark and Curran, 2004b) is an order of magnitude faster, but is still limited to around 25 sentences per second. The key to efficient CCG parsing is a finite-state supertagger which performs much of the parsing work (Bangalore and Joshi, 1999). CCG is a lexicalised grammar formalism, in which elementary syntactic structures — in CCG’s case lexical categories expressing subcategorisation information — are assigned to the words in a sentence. CCG supertagging can be performed accurately and efficiently by a Maximum Entropy tagger (Clark and Curran, 2004a). Since the lexical categories contain so much grammatical information, assigning them with low average ambiguity leaves the parser, which combines them together, with much less work to do at parse time. Hence Bangalore and Joshi (1999), in the context of LTAG parsing, refer to super"
W07-2206,P04-1041,0,0.0286954,"Missing"
W07-2206,C04-1041,1,0.950093,"mar formalism, in which elementary syntactic structures — in CCG’s case lexical categories expressing subcategorisation information — are assigned to the words in a sentence. CCG supertagging can be performed accurately and efficiently by a Maximum Entropy tagger (Clark and Curran, 2004a). Since the lexical categories contain so much grammatical information, assigning them with low average ambiguity leaves the parser, which combines them together, with much less work to do at parse time. Hence Bangalore and Joshi (1999), in the context of LTAG parsing, refer to supertagging as almost parsing. Clark and Curran (2004a) presents a novel method of integrating the supertagger and parser: initially only a small number of categories, on average, is assigned to each word, and the parser attempts to find a spanning analysis using the CKY chart-parsing algorithm. If one cannot be found, the parser requests more categories from the supertagger and builds the chart again from scratch. This process repeats until the parser is able to build a chart containing a spanning analysis.1 1 Tsuruoka and Tsujii (2004) investigate a similar idea in the context of the CKY algorithm for a PCFG. 39 Proceedings of the 10th Confere"
W07-2206,P04-1014,1,0.932691,"mar formalism, in which elementary syntactic structures — in CCG’s case lexical categories expressing subcategorisation information — are assigned to the words in a sentence. CCG supertagging can be performed accurately and efficiently by a Maximum Entropy tagger (Clark and Curran, 2004a). Since the lexical categories contain so much grammatical information, assigning them with low average ambiguity leaves the parser, which combines them together, with much less work to do at parse time. Hence Bangalore and Joshi (1999), in the context of LTAG parsing, refer to supertagging as almost parsing. Clark and Curran (2004a) presents a novel method of integrating the supertagger and parser: initially only a small number of categories, on average, is assigned to each word, and the parser attempts to find a spanning analysis using the CKY chart-parsing algorithm. If one cannot be found, the parser requests more categories from the supertagger and builds the chart again from scratch. This process repeats until the parser is able to build a chart containing a spanning analysis.1 1 Tsuruoka and Tsujii (2004) investigate a similar idea in the context of the CKY algorithm for a PCFG. 39 Proceedings of the 10th Confere"
W07-2206,J03-4003,0,0.0181837,"le, e.g. punctuation constraints. We find that the punctuation constraints are particularly effective while the gold standard chunks are required to gain any benefit for the NP constraints. Adding constraints also has the potential to increase coverage because the reduced search space means that longer sentences can 43 be parsed without exceeding the pre-defined limits on chart size. 5 Selective Beam Search Beam search involves greedy elimination of low probability partial derivations before they can form complete derivations. It is used in many parsers to reduce the search space, for example Collins (2003). We use a variable width beam where all categories c in a particular cell C that satisfy score(c) &lt; max{score(x)|x ∈ C} − B, for some beam cutoff B, are removed. The category scores score(c) are log probabilities. In the C&C parser, the entire packed chart is constructed first and then the spanning derivations are marked. Only the partial derivations that form part of spanning derivations are scored to select the best parse, which is a small fraction of the categories in the chart. Because the categories are scored with a complex statistical model with a large number of features, the time spe"
W07-2206,E03-1071,1,0.862477,"straints How can we know in advance that the correct derivation must yield specific spans, since this appears to require knowledge of the parse itself? We have explored constraints derived from shallow parsing and from the raw sentence. Our results demonstrate that simple constraints can reduce parsing time significantly without loss of coverage or accuracy. Chunk tags were used to create constraints. We experimented with both gold standard chunks from the Penn Treebank and also chunker output from the C&C chunk tagger. The tagger is very similar to the Maximum Entropy POS tagger described in Curran and Clark (2003). Only NP chunks were used because the accuracy of the tagger for other chunks is lower. The Penn Treebank chunks required modification because CCGbank analyses some constructions differently. We also created longer NPs by concatenating adjacent base NPs, for example in the case of possessives. A number of punctuation constraints were used and had a significant impact especially for longer sentences. There are a number of punctuation rules in CCGbank which absorb a punctuation mark by combining it with a category and returning a category of the same type. These rules are very productive, combi"
W07-2206,P06-1088,1,0.807349,"Missing"
W07-2206,P02-1043,0,0.0756201,"Missing"
W07-2206,N04-1013,0,0.0682503,"Missing"
W07-2206,N03-1016,0,0.0643485,"Missing"
W09-3302,E06-1002,0,0.0705156,"Missing"
W09-3302,M98-1001,0,0.0351267,"2006) estimated on Wikipedia text to perform sentence boundary detection, and tokenise the resulting text using regular expressions. Nothman et al. (2009) infer additional NEs not provided by existing links, and apply rules to adjust link boundaries and classifications to closer match BBN annotations. 2.2 NER evaluation Meaningful automatic evaluation of NER is difficult and a number of metrics have been proposed (Nadeau and Sekine, 2007). Ambiguity leads to entities correctly delimited but misclassified, or boundaries mismatched despite correct classification. Although the MUC-7 evaluation (Chinchor, 1998) defined a metric which was less sensitive to often-meaningless boundary errors, we consider only exact entity matches as correct, following the standard C O NLL evaluation (Tjong Kim Sang, 2002). We report precision, recall and F -score for each entity type. Automatic Wikipedia annotation Wikipedia, a collaboratively-written online encyclopedia, is readily exploited in NLP, because it is large, semi-structured and multilingual. Its articles often correspond to NEs, so it has been used for NE recognition (Kazama and Torisawa, 2007) and disambiguation (Bunescu and Pas¸ca, 2006; Cucerzan, 2007)."
W09-3302,D07-1074,0,0.0512064,"(Chinchor, 1998) defined a metric which was less sensitive to often-meaningless boundary errors, we consider only exact entity matches as correct, following the standard C O NLL evaluation (Tjong Kim Sang, 2002). We report precision, recall and F -score for each entity type. Automatic Wikipedia annotation Wikipedia, a collaboratively-written online encyclopedia, is readily exploited in NLP, because it is large, semi-structured and multilingual. Its articles often correspond to NEs, so it has been used for NE recognition (Kazama and Torisawa, 2007) and disambiguation (Bunescu and Pas¸ca, 2006; Cucerzan, 2007). Wikipedia links often span NEs, which may be exploited to automatically create annotated NER training data by determining the entity class of the linked article and then labelling the link text with it. Richman and Schone (2008) use article classification knowledge from English Wikipedia to produce NE-annotated corpora in other languages (evaluated against NE gold standards for French, Spanish, and Ukrainian). Mika et al. (2008) explored the use of tags from a C O NLL-trained tagger to seed the labelling of entities and evaluate the performance of a Wikipedia-trained model by hand. We make u"
W09-3302,E03-1071,1,0.656146,"gs were mapped to the four C O NLL tags before the final corrections were made. The final WG corpus consists of the body text of 145 Wikipedia articles tagged with the four C O NLL-03 tags. marketed the Sea Hawker 4 NER on the Wikipedia gold-standard Nothman et al. (2009) have previously shown that that an NER system trained on automatically annotated Wikipedia corpora performs reasonably well on non-Wikipedia text. Having created our WG corpus of gold-standard annotations, we are able to evaluate the performance of these models on Wikipedia text. We compare the C&C NE maximum-entropy tagger (Curran and Clark, 2003b) trained on gold-standard newswire corpora (MUC-7, BBN and C O NLL -03) with the same tagger trained on automatically annotated Wikipedia text, WP 2. WG is 12 WG Tokens Sentences Articles NEs Test 39 007 1 696 145 3 558 WP 2 Train 3 500 032 146 543 — 288 545 BBN Train 901 849 37 843 1 775 49 999 Test 129 654 5 462 238 7 307 C O NLL -03 Train 203 621 14 987 946 23 498 Test 46 435 3 453 231 5 648 MUC -7 Train 83 601 3 485 102 4 315 Test 60 436 2 419 99 3 540 Table 1: Corpus sizes. too small to train a reasonable NER model on goldstandard Wikipedia annotations. Part-of-speech tags are added to"
W09-3302,W03-0424,1,0.611777,"gs were mapped to the four C O NLL tags before the final corrections were made. The final WG corpus consists of the body text of 145 Wikipedia articles tagged with the four C O NLL-03 tags. marketed the Sea Hawker 4 NER on the Wikipedia gold-standard Nothman et al. (2009) have previously shown that that an NER system trained on automatically annotated Wikipedia corpora performs reasonably well on non-Wikipedia text. Having created our WG corpus of gold-standard annotations, we are able to evaluate the performance of these models on Wikipedia text. We compare the C&C NE maximum-entropy tagger (Curran and Clark, 2003b) trained on gold-standard newswire corpora (MUC-7, BBN and C O NLL -03) with the same tagger trained on automatically annotated Wikipedia text, WP 2. WG is 12 WG Tokens Sentences Articles NEs Test 39 007 1 696 145 3 558 WP 2 Train 3 500 032 146 543 — 288 545 BBN Train 901 849 37 843 1 775 49 999 Test 129 654 5 462 238 7 307 C O NLL -03 Train 203 621 14 987 946 23 498 Test 46 435 3 453 231 5 648 MUC -7 Train 83 601 3 485 102 4 315 Test 60 436 2 419 99 3 540 Table 1: Corpus sizes. too small to train a reasonable NER model on goldstandard Wikipedia annotations. Part-of-speech tags are added to"
W09-3302,W02-2024,0,0.0189761,"ng links, and apply rules to adjust link boundaries and classifications to closer match BBN annotations. 2.2 NER evaluation Meaningful automatic evaluation of NER is difficult and a number of metrics have been proposed (Nadeau and Sekine, 2007). Ambiguity leads to entities correctly delimited but misclassified, or boundaries mismatched despite correct classification. Although the MUC-7 evaluation (Chinchor, 1998) defined a metric which was less sensitive to often-meaningless boundary errors, we consider only exact entity matches as correct, following the standard C O NLL evaluation (Tjong Kim Sang, 2002). We report precision, recall and F -score for each entity type. Automatic Wikipedia annotation Wikipedia, a collaboratively-written online encyclopedia, is readily exploited in NLP, because it is large, semi-structured and multilingual. Its articles often correspond to NEs, so it has been used for NE recognition (Kazama and Torisawa, 2007) and disambiguation (Bunescu and Pas¸ca, 2006; Cucerzan, 2007). Wikipedia links often span NEs, which may be exploited to automatically create annotated NER training data by determining the entity class of the linked article and then labelling the link text"
W09-3302,W01-0521,0,0.0190797,"Missing"
W09-3302,D07-1073,0,0.0206488,"mismatched despite correct classification. Although the MUC-7 evaluation (Chinchor, 1998) defined a metric which was less sensitive to often-meaningless boundary errors, we consider only exact entity matches as correct, following the standard C O NLL evaluation (Tjong Kim Sang, 2002). We report precision, recall and F -score for each entity type. Automatic Wikipedia annotation Wikipedia, a collaboratively-written online encyclopedia, is readily exploited in NLP, because it is large, semi-structured and multilingual. Its articles often correspond to NEs, so it has been used for NE recognition (Kazama and Torisawa, 2007) and disambiguation (Bunescu and Pas¸ca, 2006; Cucerzan, 2007). Wikipedia links often span NEs, which may be exploited to automatically create annotated NER training data by determining the entity class of the linked article and then labelling the link text with it. Richman and Schone (2008) use article classification knowledge from English Wikipedia to produce NE-annotated corpora in other languages (evaluated against NE gold standards for French, Spanish, and Ukrainian). Mika et al. (2008) explored the use of tags from a C O NLL-trained tagger to seed the labelling of entities and evaluate t"
W09-3302,J06-4003,0,0.0131127,"orpus performance to be due to tokenisation and annotation scheme mismatch, missing frequent lexical items, and naming conventions. They then compared automaticallyannotated Wikipedia text as training data and found it also differs in otherwise inconsequential ways from the newswire corpora, in particular lacking abbreviations necessary to tag news text. 2.1 3. Label expanded links according to target NEs 4. Select sentences for inclusion in a corpus To prepare the text, they use mwlib (PediaPress, 2007) to parse Wikipedia’s native markup retaining only paragraph text with links, apply Punkt (Kiss and Strunk, 2006) estimated on Wikipedia text to perform sentence boundary detection, and tokenise the resulting text using regular expressions. Nothman et al. (2009) infer additional NEs not provided by existing links, and apply rules to adjust link boundaries and classifications to closer match BBN annotations. 2.2 NER evaluation Meaningful automatic evaluation of NER is difficult and a number of metrics have been proposed (Nadeau and Sekine, 2007). Ambiguity leads to entities correctly delimited but misclassified, or boundaries mismatched despite correct classification. Although the MUC-7 evaluation (Chinch"
W09-3302,W00-0904,0,0.216228,"on answering. Machine learning approaches now dominate NER , learning patterns associated with individual entity classes from annotated training data. This training data, including English newswire from the MUC-6, MUC-7 (Chinchor, 1998), and C O NLL03 (Tjong Kim Sang and De Meulder, 2003) competitive evaluation tasks, and the BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005), is critical to the success of these approaches. This data dependence has impeded the adaptation or porting of existing NER systems to new domains, such as scientific or biomedical text, e.g. Nobata et al. (2000). Similar domain sensitivity is exhibited by most tasks across NLP, e.g. 10 Proceedings of the 2009 Workshop on the People’s Web Meets NLP, ACL-IJCNLP 2009, pages 10–18, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP 2 Background 2. Split the articles into tokenised sentences Traditional evaluations of NER have considered the performance of a tagger on test data from the same source as its training data. Although the majority of annotated corpora available consist of newswire text, recent practical applications cover a far wider range of genres, including Wikipedia, blogs, RSS feeds, a"
W09-3302,U08-1016,1,0.858379,"Missing"
W09-3302,E09-1070,1,0.768779,"s, which may be exploited to automatically create annotated NER training data by determining the entity class of the linked article and then labelling the link text with it. Richman and Schone (2008) use article classification knowledge from English Wikipedia to produce NE-annotated corpora in other languages (evaluated against NE gold standards for French, Spanish, and Ukrainian). Mika et al. (2008) explored the use of tags from a C O NLL-trained tagger to seed the labelling of entities and evaluate the performance of a Wikipedia-trained model by hand. We make use of an approach described by Nothman et al. (2009) which is engineered to perform well on BBN data with a reduced tag-set (LOC, MISC , ORG , PER ). They derive an annotated corpus with the following steps: 3 Creating the Wikipedia gold standard We created a corpus by manually annotating the text of 149 articles from the May 22, 2008 dump of English Wikipedia. The articles were selected at random from all articles describing named entities, with a roughly equal proportion of article topics from each of the four C O NLL-03 classes (LOC, MISC, ORG, PER). We adopted Nothman et al.’s (2008) preprocessing described above to produce tokenised senten"
W09-3302,P08-1001,0,0.00850989,"rt precision, recall and F -score for each entity type. Automatic Wikipedia annotation Wikipedia, a collaboratively-written online encyclopedia, is readily exploited in NLP, because it is large, semi-structured and multilingual. Its articles often correspond to NEs, so it has been used for NE recognition (Kazama and Torisawa, 2007) and disambiguation (Bunescu and Pas¸ca, 2006; Cucerzan, 2007). Wikipedia links often span NEs, which may be exploited to automatically create annotated NER training data by determining the entity class of the linked article and then labelling the link text with it. Richman and Schone (2008) use article classification knowledge from English Wikipedia to produce NE-annotated corpora in other languages (evaluated against NE gold standards for French, Spanish, and Ukrainian). Mika et al. (2008) explored the use of tags from a C O NLL-trained tagger to seed the labelling of entities and evaluate the performance of a Wikipedia-trained model by hand. We make use of an approach described by Nothman et al. (2009) which is engineered to perform well on BBN data with a reduced tag-set (LOC, MISC , ORG , PER ). They derive an annotated corpus with the following steps: 3 Creating the Wikiped"
W09-3302,W03-0419,0,\N,Missing
W09-3302,P02-1062,0,\N,Missing
W09-3306,J99-2004,0,0.0703893,"lengths before (and after) length filter. 2 CCG Parsing Combinatory Categorial Grammar (CCG) (Steedman, 2000) is a linguistically motivated grammar formalism with several advantages for NLP. Like HPSG , LFG and LTAG , a CCG parse recovers the semantic structure of a sentence, including longrange dependencies and complement/adjunct distinctions, providing substantially more information than skeletal brackets. Clark and Curran (2007) describe how a fast and accurate CCG parser can be trained from CCGbank (Hockenmaier and Steedman, 2007). One of the keys to the system’s success is supertagging (Bangalore and Joshi, 1999). Supertagging is the assignment of lexical categories before parsing. The parser is given only tags assigned a high probability, greatly restricting the search space it must explore. We use this system, referred to as C&C, for our parsing experiments. 3 Self-training Methodology Processing Wikipedia Data We began by processing all articles from the March 2009 dump of Simple English Wikipedia (SEW) and the matching Full English Wikipedia (FEW) articles. SEW is an online encyclopedia written in basic English. It has stylistic guidelines that instruct contributors to use basic vocabulary and syn"
W09-3306,J07-4004,1,0.93112,"he accuracy with which the data can be parsed, which determines how noisy the new training data will be; and the utility of the examples, which determines how informative the examples will be. We experimented with a novel source of data to balance these two concerns. Simple English Wikipedia imposes editorial guidelines on the length and syntactic style authors can use. This text should be easier to parse, lowering the noise, but the syntactic restrictions might mean its examples have lower utility for adapting the parser to the full English Wikipedia. We train the C&C supertagger and parser (Clark and Curran, 2007) on sections 02-21 of the Wall Street Journal (WSJ) marked up with CCG annotations (Hockenmaier and Steedman, 2007) in the standard way. We then parse all of the Simple English Wikipedia remaining after our preprocessing. We discard the 826 sentences the parser could not find an analysis for, and set aside 1,486 randomly selected sentences as a future development set, leaving a corpus of 185,000 automatically parsed sentences (2.6 million words). We retrain the supertagger on a simple concatenation of the 39,607 WSJ training sentences and the Wikipedia sentences, and then use it with the norma"
W09-3306,P96-1011,0,0.0672979,"n. There were, however, some systematic differences from CCGbank, due to the faulty noun phrase bracketing and complement/adjunct distinctions inherited from the Penn Treebank. 6 Results The results in this section refer to precision, recall and F -Score over labelled CCG dependencies, which are 5-tuples (head, child, category, slot, range). Speed is reported as words per second, using a single core 2.6 GHz Pentium 4 Xeon. 6.1 Out-of-the-Box Performance Our experiments were performed using two models provided with v1.02 of the C&C parser. The derivs model is calculated using features from the Eisner (1996) normal form derivation. This is the model C&C recommend for general use, because it is simpler and faster to train. The hybrid model achieves the best published results for CCG parsing (Clark and Curran, 2007), so we also experimented with this model. The models’ performance is shown in the WSJ rows of Table 2. We report accuracy using automatic POS tags, since we did not correct the POS tags in the Wikipedia data. 6.2 Domain Adaptation Experiments The inclusion of parsed data from Wikipedia articles in the supertagger’s training data improves its accuracy on Wikipedia data, with the FEW enha"
W09-3306,W01-0521,0,0.189117,"Missing"
W09-3306,J06-4003,0,0.0193737,"ia (SEW) and the matching Full English Wikipedia (FEW) articles. SEW is an online encyclopedia written in basic English. It has stylistic guidelines that instruct contributors to use basic vocabulary and syntax, to improve the articles’ readability. This might make SEW text easier to parse, making it useful for our self-training experiments. mwlib (PediaPress, 2007) was used to parse the MediaWiki markup. We did not expand templates, and retained only paragraph text tokenized according to the WSJ, after it was split into sentences using the NLTK (Loper and Bird, 2002) implementation of Punkt (Kiss and Strunk, 2006) parameterised on Wikipedia text. Finally, we discarded incorrectly parsed markup and other noise. We also introduced a sentence length filter for the domain adaptation data (but not the evaluation data), discarding sentences longer than 25 words or shorter than 3 words. The length filter was used to gather sentences that would be easier to parse. The effect of this filter is shown in Table 1. 1 39 http://svn.ask.it.usyd.edu.au/trac/candc Model P WSJ derivs 85.51 SEW derivs 85.06 FEW derivs 85.24 WSJ hybrid 86.20 SEW hybrid 85.80 FEW hybrid 85.94 WSJ R 84.62 84.11 84.32 84.80 84.30 84.46 Secti"
W09-3306,W02-0109,0,0.00852498,"m the March 2009 dump of Simple English Wikipedia (SEW) and the matching Full English Wikipedia (FEW) articles. SEW is an online encyclopedia written in basic English. It has stylistic guidelines that instruct contributors to use basic vocabulary and syntax, to improve the articles’ readability. This might make SEW text easier to parse, making it useful for our self-training experiments. mwlib (PediaPress, 2007) was used to parse the MediaWiki markup. We did not expand templates, and retained only paragraph text tokenized according to the WSJ, after it was split into sentences using the NLTK (Loper and Bird, 2002) implementation of Punkt (Kiss and Strunk, 2006) parameterised on Wikipedia text. Finally, we discarded incorrectly parsed markup and other noise. We also introduced a sentence length filter for the domain adaptation data (but not the evaluation data), discarding sentences longer than 25 words or shorter than 3 words. The length filter was used to gather sentences that would be easier to parse. The effect of this filter is shown in Table 1. 1 39 http://svn.ask.it.usyd.edu.au/trac/candc Model P WSJ derivs 85.51 SEW derivs 85.06 FEW derivs 85.24 WSJ hybrid 86.20 SEW hybrid 85.80 FEW hybrid 85.94"
W09-3306,N03-1031,0,0.0288617,"g it and other collaborative semantic resources. 38 Proceedings of the 2009 Workshop on the People’s Web Meets NLP, ACL-IJCNLP 2009, pages 38–41, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP Corpus WSJ 02-21 FEW SEW Sentences 39,607 889,027 (586,724) 224,251 (187,321) Mean length 23.5 22.4 (16.6) 16.5 (14.1) 4 To investigate how the parser could be improved on Wikipedia text, we experimented with semisupervised learning. We chose a simple method, self-training. Unlabelled data is annotated by the system, and the predictions are taken as truth and integrated into the training system. Steedman et al. (2003) showed that the selection of sentences for semi-supervised parsing is very important. There are two issues: the accuracy with which the data can be parsed, which determines how noisy the new training data will be; and the utility of the examples, which determines how informative the examples will be. We experimented with a novel source of data to balance these two concerns. Simple English Wikipedia imposes editorial guidelines on the length and syntactic style authors can use. This text should be easier to parse, lowering the noise, but the syntactic restrictions might mean its examples have"
W09-3306,D07-1112,0,\N,Missing
W09-3306,J07-3004,0,\N,Missing
W09-3306,D07-1096,0,\N,Missing
W09-3603,E03-1071,1,0.876177,"l ∈ x, that is, the word goal is part of the context of the sentence, is a contextual predicate. The central idea in maximum entropy modelling is that the model chosen should satisfy all of the constraints imposed by the training data (in the 20 first The first four words of a sentence, added individually. form of empirical feature counts from the training data) whilst remaining as unbiased as possible. This is achieved by selecting the model with the maximum entropy, i.e. the most uniform distribution, given the constraints. Our classifier uses the maximum entropy implementation described in Curran and Clark (2003). Generalised Iterative Scaling (GIS) is used to estimate the values of the weights and we use a Gaussian prior over the weights (Chen and Rosenfeld, 1999) which allows many rare, but informative, features to be used without overfitting. This will be an important property when we use sparse features like bigrams in the models below. 4 4.1 Sections, positions, and lengths section A section counter which increments on each heading to measure the distance into the document. It does not take into consideration whether they are sub-headings or similar. There are two versions of this feature. The fi"
W09-3603,E99-1015,0,0.186563,"Missing"
W09-3603,W06-1613,0,0.0118428,"thor’s use and opinion of other authors they cite in their work and also to create Rhetorical Document Profiles (RDP), a type of summarization used to provide typical information that a new reader may need in a systematic manner. For the use of Argumentative Zoning in RDPs Teufel (1999) points out that due to the redundancy in language that near perfect accuracy is not required as important pieces of information will be repeated in the paper. Recognising these salient points once is enough for them to be included in the RDP. In further tasks, such as the analysis of the function of citations (Teufel et al., 2006) and automatic summarization, higher levels of accuracy are more critical. i=1 where y is the zone label, x is the context (the sentence) and the fi (x, y) are the features with associated weights λi . The probability of a sequence of zone labels y1 . . . yn given a sequence of sentences is s1 . . . sn is approximated as follows: p(y1 . . . yn |s1 . . . sn ) ≈ n Y p(yi |xi ) (2) i=1 where xi is the context for sentence si . In our experiments that treat argumentative zoning as a sequence labelling task, the context xi incorporates history information – i.e. the previous labelling decisions of"
W09-3603,P99-1069,0,0.0157584,"Missing"
W09-3603,J93-2004,0,0.0315016,"Missing"
W09-3603,U06-1010,1,0.846518,"annotations (e.g. named entities). Where feasible we have reimplemented the features described in Teufel (1999). In other cases, our features are somewhat simpler. Since the Curran and Clark (2003) classifier only accepts binary features, any numerical features had to be bucketed into smaller sets of alternatives to reduce sparseness, either by integer division or through reducing the number by scaling to a small integer range. The features we implemented are described below. Named entity features Our astronomy corpus has been manually annotated with domain-specific named entity information (Murphy et al., 2006). There are 12 coarsegrained categories and 43 fine-grained categories including star, galaxy, telescope, as well as a number of the usual categories including person, organisation and location. Both the coarse-grained and fine-grained categories were used as features. 4.2 Teufel (1999)’s features To compare with previous work, we also implemented most of the features that gave Teufel (1999) the best performance. We list all of the feature types in Table 2, indicating which ones have and have not been implemented. Teufel’s unigram features (cont-1) are filtered using TF - IDF to select the top"
W09-3603,A97-1004,0,0.0501161,"Missing"
W09-3603,J02-4002,0,0.532993,"(1999) from the Computation and Language E-Print Archive 1 . The LATEX source was converted to HTML with Latex2HTML then transformed into XML with custom PERL scripts. This text was then tokenized using the TTT (Text Tokenization) System into Penn Treebank format. The result is a corpus of 12,000 annotated sentences, containing 333,000 word tokens, in XML format. We attempted to recreate Teufel’s original experiments by emulating the features she used with the same type of classifier. We used Weka’s (Frank et al., 2005) implementation of the NB classifier. Table 3 reproduces the results from Teufel and Moens (2002) alongside our reimplementation of History features and Viterbi In order to take advantage of the predictability of tags given prior sequences (for example, A IM commonly following itself) we used history features and treated Argumentative Zoning as a sequence labelling task. Since each prediction now relies on the previous decisions we used the Viterbi algorithm to find the optimal sequence. Given the small number of labelling alternatives, we experimented with several history lengths ranging from previous label to the previous four labels. To determine the impact of this 1 22 http://xxx.lanl"
W09-3603,P99-1071,0,\N,Missing
W10-0515,U09-1003,1,0.872364,"Missing"
W10-0515,P02-1020,0,\N,Missing
W10-0515,J96-2004,0,\N,Missing
W14-5207,C14-1072,1,0.931958,"lities for managing and exploring annotated corpora Joel Nothman and Tim Dawborn and James R. Curran -lab, School of Information Technologies University of Sydney NSW 2006, Australia {joel.nothman,tim.dawborn,james.r.curran}@sydney.edu.au e Abstract Users of annotated corpora frequently perform basic operations such as inspecting the available annotations, filtering documents, formatting data, and aggregating basic statistics over a corpus. While these may be easily performed over flat text files with stream-processing UNIX tools, similar tools for structured annotation require custom design. Dawborn and Curran (2014) have developed a declarative description and storage for structured annotation, on top of which we have built generic command-line utilities. We describe the most useful utilities – some for quick data exploration, others for high-level corpus management – with reference to comparable UNIX utilities. We suggest that such tools are universally valuable for working with structured corpora; in turn, their utility promotes common storage and distribution formats for annotated text. 1 Introduction Annotated corpora are a mainstay of language technology, but are often stored or transmitted in a var"
W14-5207,W09-1501,0,0.0302962,"form for tool delivery and is comparable to the UNIX command-line, albeit more graphical, while providing further opportunities for integration. For example, UIMA employs Java Logical Structures to yield corpus inspection within the Eclipse debugger (Lally et al., 2009). Generic processors in these frameworks include those for combining or splitting documents, or copying annotations from one document to another. The community has further built tools to export corpora to familiar query environments, such as a relational database or Lucene search engine (Hahn et al., 2008). The uimaFIT library (Ogren and Bethard, 2009) simplifies the creation and deployment of UIMA processors, but to produce and execute a processor for mere data exploration still has some overhead. Other related work includes utilities for querying or editing treebanks (e.g. Kloosterman, 2009), among specialised annotation formats; and for working with binary-encoded structured data such as Protocol Buffers (e.g. protostuff1 ). Like these tools and those within UIMA and GATE, the utilities presented in this work reduce development effort, with an orientation towards data management and evaluation of arbitrary functions from the command-line"
W14-5207,rehm-etal-2008-ontology,0,0.0324054,"d providing means to operate over them, frameworks such as GATE and UIMA promise savings in development and data management costs. These frameworks store annotated corpora with XML, so users may exploit standard infrastructure (e.g. XQuery) for basic transformation and aggregation over the data. Generic XML tools are limited in their ability to exploit the semantics of a particular XML language, such that expressing queries over annotations (which include pointers, spatial relations, etc.) can be cumbersome. LT- XML (Thompson et al., 1997) implements annotators using standard XML tools, while Rehm et al. (2008) present extensions to an XQuery implementation specialised to annotated text. Beyond generic XML transformation, UIMA and GATE and their users provide utilities with broad application for data inspection and management, ultimately leading to quality assurance and rapid development within those frameworks. Both GATE and UIMA provide sophisticated graphical tools for viewing and modifying annotations; for comparing parallel annotations; and for displaying a concordance of contexts for a term across a document collection (Cunningham et al., 2002; Lally et al., 2009). Both also provide means of p"
