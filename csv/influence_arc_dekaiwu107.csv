1994.amta-1.26,J90-2002,0,0.131675,"Missing"
1994.amta-1.26,J93-2003,0,0.210416,"Missing"
1994.amta-1.26,P91-1022,0,0.43288,"Missing"
1994.amta-1.26,P93-1002,0,0.46355,"Missing"
1994.amta-1.26,P93-1001,0,0.50083,"Missing"
1994.amta-1.26,W93-0301,0,0.761483,"Missing"
1994.amta-1.26,C94-2178,0,0.539151,"Missing"
1994.amta-1.26,P91-1023,0,0.0460761,"Missing"
1994.amta-1.26,C88-2142,0,0.081678,"Missing"
1994.amta-1.26,H94-1027,0,0.481468,"Missing"
1994.amta-1.26,A94-1030,1,0.784164,"Missing"
1994.amta-1.26,J93-1004,0,\N,Missing
1994.amta-1.26,J93-1006,0,\N,Missing
1994.amta-1.26,P94-1012,1,\N,Missing
1995.tmi-1.18,J93-2003,0,0.00798376,"Missing"
1995.tmi-1.18,1994.amta-1.3,0,0.0223545,"Missing"
1995.tmi-1.18,A94-1006,0,0.0292693,"Missing"
1995.tmi-1.18,W93-0301,0,0.0187203,"Missing"
1995.tmi-1.18,P95-1032,1,0.839504,"Missing"
1995.tmi-1.18,1994.amta-1.11,1,0.834197,"Missing"
1995.tmi-1.18,P94-1051,0,0.0217946,"Missing"
1995.tmi-1.18,P94-1012,1,0.859572,"Missing"
1995.tmi-1.18,1995.tmi-1.28,1,0.842079,"Missing"
1995.tmi-1.18,A94-1030,1,0.841127,"Missing"
1995.tmi-1.18,1994.amta-1.26,1,0.843615,"Missing"
1995.tmi-1.28,P91-1022,0,0.0354373,"Missing"
1995.tmi-1.28,P93-1002,0,0.0300825,"Missing"
1995.tmi-1.28,P93-1001,0,0.0374631,"Missing"
1995.tmi-1.28,C94-1014,0,0.0453306,"Missing"
1995.tmi-1.28,W93-0301,0,0.389703,"Missing"
1995.tmi-1.28,C94-2178,0,0.0903904,"Missing"
1995.tmi-1.28,1994.amta-1.11,0,0.364482,"Missing"
1995.tmi-1.28,P91-1023,0,0.0309513,"Missing"
1995.tmi-1.28,C92-2101,0,0.638476,"Missing"
1995.tmi-1.28,C88-2142,0,0.0565484,"Missing"
1995.tmi-1.28,P93-1003,0,0.0640279,"Missing"
1995.tmi-1.28,P93-1004,0,0.116088,"Missing"
1995.tmi-1.28,C90-3101,0,0.195533,"Missing"
1995.tmi-1.28,P94-1012,1,0.697046,"Missing"
1995.tmi-1.28,A94-1030,1,0.637599,"Missing"
1995.tmi-1.28,1994.amta-1.26,1,0.57957,"Missing"
1995.tmi-1.28,J93-1004,0,\N,Missing
1995.tmi-1.28,J93-1006,0,\N,Missing
2006.iwslt-evaluation.5,W02-2020,0,0.0420419,"classifier is trained sequentially, increasingly focusing more heavily on the instances that the previous classifiers found difficult to classify. Our system uses AdaBoost.MH (Freund and Schapire, 1997), an n-ary classification variant of the original binary AdaBoost algorithm. As demonstrated by Wu et al. (2002) and Carreras et al. (2002), boosting can be used to build language independent NER models that perform exceptionally well. Support Vector Machines: Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992). Sassano and Utsuro (2000) and McNamee and Mayfield (2002) have demonstrated that SVMs show promise when applied to named entity recognition, though performance appears • Lexical (words and lemmas) and syntactic (partof-speech) information within a window of 2 words surrounding the current word • Prefixes and suffixes of up to a length of 4 characters from the current word • Capitalization: whether the word starts with a capital letter and/or the entire word is capitalized • A small set of conjunctions of POS tags and words within a window of 2 words of the current word • Previous history: the chunk tags (gold standard during training; assigned for e"
2006.iwslt-evaluation.5,N01-1006,0,0.0178429,"8992 13337 13337 18992 quite sensitive to parameter choices. Transformation-based learning: Transformationbased learning (TBL) is a rule-based machine learning algorithm that was first introduced by Brill (1995) and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. Our system uses the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. patterns: for instance, many location and person names can typically be transliterated, while some components of organization names should be translated with a standard bilexicon instead. After identifying NE boundaries and types, a rulebased translation approach based on name gazetteers and transliteration schemes is used to obtain one or more translations for each identified NE. The decoder integrates the NE translation candidates as additional translation candidates for the NE phr"
2006.iwslt-evaluation.5,P02-1038,0,0.0119,"be well suited to handling features that are highly interdependent. 2.1. Decoder For the experiments here, we used the Pharaoh decoder (Koehn, 2004), which implements a heuristic beam search for phrase based translation. While the phrase reordering model used in Pharaoh is weaker than in other proposed models, Pharaoh was chosen for the advantages of being freely available and widely used, and therefore constitutes an appropriate point of reference. 2.2. Phrasal bilexicon The core phrasal bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2002). The intersection is augmented using growing heuristics proposed by Och and Ney (2002) in order to improve recall. Following Koehn (2003), each entry in the phrasal bilexicon is scored using phrase translation conditional probabilities for both translation directions, as well as lexical weights which combine word translation probabilities according to the word alignment observed within the phrase pair during training. 2.3. Language model The language model is a standard trigram model with Kneser-Ney smoothing trained using the SRI language modeling toolkit (Stolcke, 2002). 3.2. WSD features T"
2006.iwslt-evaluation.5,J95-4004,0,0.00644608,"translation disambiguation task faced by the SMT system. • Instead of using predefined senses drawn from 39 Table 3: IWSLT-06 Training data statistics computed for the 4 language pairs Training Data Statistics Chinese-English Arabic-English Italian-English Japanese-English Number of bisentences 39953 19972 19972 39953 Vocabulary size (input lang) 11178 25152 17917 12535 Vocabulary size (English) 18992 13337 13337 18992 quite sensitive to parameter choices. Transformation-based learning: Transformationbased learning (TBL) is a rule-based machine learning algorithm that was first introduced by Brill (1995) and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. Our system uses the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. patterns: for instance, many location and person names can typically be tran"
2006.iwslt-evaluation.5,P02-1040,0,0.0769139,"): ïå ÷ Š ` ( å, „ 0@ ™ e } Output: Could you please write down the address in Japan, please. Input (read): ïå ÷î ¨ ( å, „ 0@ ™ e } Output: Could you please write down the address in Japan, please. Input (spontaneous): ïå ÷ ž Ù º „ 0@ ™ † } Output: May handle, deal with the address of the please. Example 2 Input (text): ` Z A åM Å{ {° Output: You must check in by ten o’clock in the evening. Input (read): ¨ ©?A åM Å{ {° Output: You must check in at ten before. Input (spontaneous): ¨ Z A åM Å{ {° Output: You must check in by ten o’clock in the evening. mon automatic evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), as well as word error rate (WER) and position-independent word error rate (PER) (Tillmann et al., 1997). The HKUST system achieves reasonable performance, with evaluation scores situated in the middle range, compared to all systems evaluated on the open track. syntactic units, is the first step of processing in ASVMT. This is followed by lemmatization which, in ASVMT, refers to a normalization step where the tokens coming from stems that were modified when agglutinated are converted back to their original form. Italian: We preproces"
2006.iwslt-evaluation.5,I05-2021,1,0.860642,"ed and therefore not designed and tuned to tackle the challenges of speech translation. We also find that the system achieves reasonable results on a wide range of languages, by evaluating on read speech transcriptions from Arabic, Italian, and Japanese into English. 1. Introduction The role and usefulness of semantic processing for Statistical Machine Translation (SMT) has recently been much debated. In previous work, we reported surprisingly disappointing results when using the predictions of a Senseval word sense disambiguation (WSD) system in conjunction with SMT using an IBM-style model (Carpuat and Wu, 2005b). Nevertheless, error analysis leaves little doubt that the performance of SMT systems still suffers from inaccurate lexical choice. Other empirical studies have shown that SMT systems perform much more poorly than dedicated WSD models, both 2. Machine translation engine ∗ This work was supported in part by DARPA GALE contract HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. The core MT engine as used in the experiments here is an off-the-shelf phrase-based statistical machine trans37 3.1. WSD classifiers lation"
2006.iwslt-evaluation.5,W05-0822,0,0.0114512,"oth 2. Machine translation engine ∗ This work was supported in part by DARPA GALE contract HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. The core MT engine as used in the experiments here is an off-the-shelf phrase-based statistical machine trans37 3.1. WSD classifiers lation model. This is a useful engine since the approach has been shown to achieve competitive translation quality and is commonly used. Many state-of-the-art systems employ phrase-based approaches (e.g., Zens et al. (2005), Koehn et al. (2005), Sadat et al. (2005)). All phrase-based models make use of a phrasal bilexicon, but essentially differ in the bilexicon extraction and parameter estimation strategies, and the phrase reordering method. The model consists of an ensemble of four voting models combined by majority vote. The first voting model is a na¨ıve Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1979), since Klein and Manning (2002) found that this model yie"
2006.iwslt-evaluation.5,P05-1048,1,0.935692,"ed and therefore not designed and tuned to tackle the challenges of speech translation. We also find that the system achieves reasonable results on a wide range of languages, by evaluating on read speech transcriptions from Arabic, Italian, and Japanese into English. 1. Introduction The role and usefulness of semantic processing for Statistical Machine Translation (SMT) has recently been much debated. In previous work, we reported surprisingly disappointing results when using the predictions of a Senseval word sense disambiguation (WSD) system in conjunction with SMT using an IBM-style model (Carpuat and Wu, 2005b). Nevertheless, error analysis leaves little doubt that the performance of SMT systems still suffers from inaccurate lexical choice. Other empirical studies have shown that SMT systems perform much more poorly than dedicated WSD models, both 2. Machine translation engine ∗ This work was supported in part by DARPA GALE contract HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. The core MT engine as used in the experiments here is an off-the-shelf phrase-based statistical machine trans37 3.1. WSD classifiers lation"
2006.iwslt-evaluation.5,C00-2102,0,0.0176483,"e strong classifier. Each weak classifier is trained sequentially, increasingly focusing more heavily on the instances that the previous classifiers found difficult to classify. Our system uses AdaBoost.MH (Freund and Schapire, 1997), an n-ary classification variant of the original binary AdaBoost algorithm. As demonstrated by Wu et al. (2002) and Carreras et al. (2002), boosting can be used to build language independent NER models that perform exceptionally well. Support Vector Machines: Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992). Sassano and Utsuro (2000) and McNamee and Mayfield (2002) have demonstrated that SVMs show promise when applied to named entity recognition, though performance appears • Lexical (words and lemmas) and syntactic (partof-speech) information within a window of 2 words surrounding the current word • Prefixes and suffixes of up to a length of 4 characters from the current word • Capitalization: whether the word starts with a capital letter and/or the entire word is capitalized • A small set of conjunctions of POS tags and words within a window of 2 words of the current word • Previous history: the chunk tags (gold standard"
2006.iwslt-evaluation.5,W04-0822,1,0.916544,"tracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonlinearly mapped from their original space Rn to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space. We have showed that KPCAbased WSD models achieve close accuracies to the best individual WSD models, while having a significantly different bias (Carpuat et al., 2004). All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant. Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent. 2.1. Decoder For the experiments here, we used the Pharaoh decoder (Koehn, 2004), which implements a heuristic beam search for phrase based translation. While the phrase reordering model used in Pharaoh is weaker than in other proposed models, Pharaoh was chosen for the advantages of being freely available and widely used, and therefore constitutes"
2006.iwslt-evaluation.5,W02-2004,0,0.08139,"Missing"
2006.iwslt-evaluation.5,C04-1190,1,0.836588,"ankings and are more discriminative than the baseline translation probabilities, yielding improved translations as can be seen in Table 2. Table 2: Translation examples with and without WSD for SMT Example 1 Input © ÜU } Ref. May I see the menu ? Baseline Let me see the menu ? + WSD May I see the menu ? Example 2 Input ý Š „ §M Ù Ref. Would you show me to my seat ? Baseline Can you change my seat finger for me ? + WSD Can you direct me to my seat ? 4. Named-entity translation cal sample disambiguation tasks both on Senseval-2 and Senseval-3 data (e.g., Carpuat et al. (2004), Wu et al. (2004), Su et al. (2004)). Recognizing, disambiguating, and translating entities is a special case of word sense disambiguation for translation lexical choice, where the words or phrases in question are entities of various sorts. Translating names correctly is particularly important to translation quality and usefulness, but does present some distinct challenges from regular phrase translation. First, the vast majority of names are rare and often never seen in training, and, with the exception of names of well-known persons or other entities, are typically not recorded in lexicons. Second, whether a phrase is a named"
2006.iwslt-evaluation.5,W02-2035,1,0.875051,"iers As NER can be framed as a classification task, we use an ensemble of three relatively high performing machine learning classifiers: Boosting: The main idea behind boosting algorithms is that a set of many weak classifiers can be effectively combined to yield a single strong classifier. Each weak classifier is trained sequentially, increasingly focusing more heavily on the instances that the previous classifiers found difficult to classify. Our system uses AdaBoost.MH (Freund and Schapire, 1997), an n-ary classification variant of the original binary AdaBoost algorithm. As demonstrated by Wu et al. (2002) and Carreras et al. (2002), boosting can be used to build language independent NER models that perform exceptionally well. Support Vector Machines: Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992). Sassano and Utsuro (2000) and McNamee and Mayfield (2002) have demonstrated that SVMs show promise when applied to named entity recognition, though performance appears • Lexical (words and lemmas) and syntactic (partof-speech) information within a window of 2 words surrounding the current word • Prefixes and suffixes of up to a length of 4 cha"
2006.iwslt-evaluation.5,W03-0433,1,0.809241,", and vice versa. Third, unlike European languages, Chinese allows an open vocabulary for proper names of persons, eliminating another major source of explicit clues used by European language NER models. Based on these observations, we use character-level features instead of word-level features; this prevents committing to a given word segmentation, which might be incorrect at NE boundaries. Several versions of this NER system were extensively evaluated on NER shared tasks for Chinese at SIGHAN 2006 (Yu et al., 2006) and for several European languages at CoNLL 2002 (Wu et al., 2002) and 2003 (Wu et al., 2003). English sentence. All training data was clean text, representing a mismatch to the test data used in the evaluation, which was noisy output from automatic speech recognition. In addition to recognition errors, automatic speech transcriptions do not contain punctuation, and use digits to represent numbers. Performance could be improved by eliminating the mismatch between training and test data. For each Chinese sentence, we are given correct speech transcriptions as well as automatic read speech transcriptions and automatic spontaneous speech transcriptions. For the other languages, we only t"
2006.iwslt-evaluation.5,W02-1002,0,0.0223543,". (2005), Koehn et al. (2005), Sadat et al. (2005)). All phrase-based models make use of a phrasal bilexicon, but essentially differ in the bilexicon extraction and parameter estimation strategies, and the phrase reordering method. The model consists of an ensemble of four voting models combined by majority vote. The first voting model is a na¨ıve Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1979), since Klein and Manning (2002) found that this model yielded higher accuracy than na¨ıve Bayes in a subsequent comparison of WSD performance. The third voting model is a boosting model (Freund and Schapire, 1997), since has consistently turned in very competitive scores on related tasks such as named entity classification, as described in Section 4.1.1. We also use the Adaboost.MH algorithm for WSD, just like for NER. The fourth voting model is a model based on Kernel PCA (Wu et al., 2004). Kernel Principal Component Analysis (KPCA) is a nonlinear kernel method for extracting nonlinear principal components from vector sets"
2006.iwslt-evaluation.5,P04-1081,1,0.795225,"on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1979), since Klein and Manning (2002) found that this model yielded higher accuracy than na¨ıve Bayes in a subsequent comparison of WSD performance. The third voting model is a boosting model (Freund and Schapire, 1997), since has consistently turned in very competitive scores on related tasks such as named entity classification, as described in Section 4.1.1. We also use the Adaboost.MH algorithm for WSD, just like for NER. The fourth voting model is a model based on Kernel PCA (Wu et al., 2004). Kernel Principal Component Analysis (KPCA) is a nonlinear kernel method for extracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonlinearly mapped from their original space Rn to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space. We have showed that KPCAbased WSD models achieve close accuracies"
2006.iwslt-evaluation.5,2005.iwslt-1.8,0,0.0189194,"dicated WSD models, both 2. Machine translation engine ∗ This work was supported in part by DARPA GALE contract HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. The core MT engine as used in the experiments here is an off-the-shelf phrase-based statistical machine trans37 3.1. WSD classifiers lation model. This is a useful engine since the approach has been shown to achieve competitive translation quality and is commonly used. Many state-of-the-art systems employ phrase-based approaches (e.g., Zens et al. (2005), Koehn et al. (2005), Sadat et al. (2005)). All phrase-based models make use of a phrasal bilexicon, but essentially differ in the bilexicon extraction and parameter estimation strategies, and the phrase reordering method. The model consists of an ensemble of four voting models combined by majority vote. The first voting model is a na¨ıve Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1979), since Klein and Manning (2002) foun"
2006.iwslt-evaluation.5,koen-2004-pharaoh,0,0.0488484,"pf et al., 1998). WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space. We have showed that KPCAbased WSD models achieve close accuracies to the best individual WSD models, while having a significantly different bias (Carpuat et al., 2004). All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant. Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent. 2.1. Decoder For the experiments here, we used the Pharaoh decoder (Koehn, 2004), which implements a heuristic beam search for phrase based translation. While the phrase reordering model used in Pharaoh is weaker than in other proposed models, Pharaoh was chosen for the advantages of being freely available and widely used, and therefore constitutes an appropriate point of reference. 2.2. Phrasal bilexicon The core phrasal bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2002). The intersection is augmented using growing heuristics proposed by Och and Ney (2002) in order to improve recall. Following Koeh"
2006.iwslt-evaluation.5,2005.iwslt-1.20,0,0.0338571,"more poorly than dedicated WSD models, both 2. Machine translation engine ∗ This work was supported in part by DARPA GALE contract HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. The core MT engine as used in the experiments here is an off-the-shelf phrase-based statistical machine trans37 3.1. WSD classifiers lation model. This is a useful engine since the approach has been shown to achieve competitive translation quality and is commonly used. Many state-of-the-art systems employ phrase-based approaches (e.g., Zens et al. (2005), Koehn et al. (2005), Sadat et al. (2005)). All phrase-based models make use of a phrasal bilexicon, but essentially differ in the bilexicon extraction and parameter estimation strategies, and the phrase reordering method. The model consists of an ensemble of four voting models combined by majority vote. The first voting model is a na¨ıve Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1979), since Klein an"
2006.iwslt-evaluation.5,W06-0124,1,\N,Missing
2007.iwslt-1.12,2006.iwslt-evaluation.5,1,0.846504,"d on Chinese to English translation, but we also report results on the Arabic to English, Italian to English, and Japanese to English tasks. 1. Introduction We describe experiments conducted at HKUST during the IWSLT 2007 evaluation campaign on spoken language translation. For our second participation in the IWSLT evaluation, our focus was on experimenting with Moses (Koehn et al., 2007), the new open-source toolkit for phrase-based Statistical Machine Translation (SMT), and on comparing it against its closed-source predecessor Pharaoh (Koehn, 2004) which we used in our IWSLT 2006 submission (Carpuat et al., 2006). Our main focus was on the Chinese-English task, which, this year, used clean text as opposed to the other tasks where speech transcriptions were to be translated. We also report results on all the language pairs, although we did not do any tuning or any language-specific processing for the Arabic to English, Japanese to English and Italian to English tasks.  This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC625"
2007.iwslt-1.12,2006.iwslt-evaluation.19,0,0.0290995,"under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 2. Machine translation engine We focus on phrase-based statistical machine translation since this approach has been shown to achieve competitive translation quality in many state-of-the-art systems (e.g., Zens et al. (2005), Shen et al. (2006), Eck et al. (2006)), and therefore is a good benchmark to evaluate decoder architectures. 2.1. Decoder We use the Moses decoder (Koehn et al., 2007), which is an open source toolkit for statistical machine translation. Just like the Pharaoh decoder, Moses uses a log-linear model, which combines several knowledge sources in translation decisions. Moses differs from Pharaoh by its represention of each input word as a factor as opposed to the word suface form only. Factors can include additional information such as part-ofspeech, class, morphology and allow the phrase-based model to incorporate richer linguistic i"
2007.iwslt-1.12,P07-2045,0,0.015333,"he HKUST experiments in the IWSLT 2007 evaluation campaign on spoken language translation. Our primary objective was to compare the open-source phrase-based statistical machine translation toolkit Moses against Pharaoh. We focused on Chinese to English translation, but we also report results on the Arabic to English, Italian to English, and Japanese to English tasks. 1. Introduction We describe experiments conducted at HKUST during the IWSLT 2007 evaluation campaign on spoken language translation. For our second participation in the IWSLT evaluation, our focus was on experimenting with Moses (Koehn et al., 2007), the new open-source toolkit for phrase-based Statistical Machine Translation (SMT), and on comparing it against its closed-source predecessor Pharaoh (Koehn, 2004) which we used in our IWSLT 2006 submission (Carpuat et al., 2006). Our main focus was on the Chinese-English task, which, this year, used clean text as opposed to the other tasks where speech transcriptions were to be translated. We also report results on all the language pairs, although we did not do any tuning or any language-specific processing for the Arabic to English, Japanese to English and Italian to English tasks.  This"
2007.iwslt-1.12,E06-1031,0,0.0938417,"(DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 2. Machine translation engine We focus on phrase-based statistical machine translation since this approach has been shown to achieve competitive translation quality in many state-of-the-art systems (e.g., Zens et al. (2005), Shen et al. (2006), Eck et al. (2006)), and therefore is a good benchmark to evaluate decoder architectures. 2.1. Decoder We use the Moses decoder (Koehn et al., 2007), which is an open source toolkit for statistical machine translation. Just like the Pharaoh decoder, Moses uses a log-linear model, which combines several knowledge sources in translation decisions. Moses differs from Pharaoh by its represention of each input word as a factor as opposed to the word suface form only. Factors can include additional information such as part-ofspeech, class, morphology and allow the phrase-based model to incorporate"
2007.iwslt-1.12,P02-1038,0,0.0434894,"n decisions. Moses differs from Pharaoh by its represention of each input word as a factor as opposed to the word suface form only. Factors can include additional information such as part-ofspeech, class, morphology and allow the phrase-based model to incorporate richer linguistic information. However, we do not use the factored representation in this first set of experiments, and use the surface form of words, just like in Pharaoh. 2.2. Phrasal bilexicon The core phrasal bilexicon is obtained by collecting phrase pairs that are consistent with the IBM model 4 alignments obtained with GIZA++ (Och and Ney, 2002). During phrase extraction, we tried two different methods to get the final word alignment from the bi-directional GIZA++ alignments: (1) intersect and (2) grow-diagfinal. Intersect uses the strict intersection of the bidirectional word alignments, while grow-diag-final expands the alignment by adding directly neighboring alignment points, and alignment points in the diagonal neighborhood. We found that using grow-diag-final improves IWSLT-07 data set CE devtest1 CE devtest2 CE devtest3 Table 1: Resegmenting test sentences improves BLEU score. # original # sentences after BLEU with original BL"
2007.iwslt-1.12,P02-1040,0,0.0935189,"not optimal, as Italian presents more morphological inflexions than English, as suggested by the larger vocabulary size on the Italian side of the training data than on the English side (Table 2.) Japanese: We used the provided word segmentation and did not perform any additional processing. 5. Experimental results The official BLEU scores for HKUST’s submitted runs, which were buggy due to accidental errors in combining the models and parameters used in the experiments, are shown in Table 3 for all four language pairs. The official results were only automatically evaluated using BLEU score (Papineni et al., 2002). We achieved a BLEU score of 34.26 on Chinese to English read speech translation. There were 9 primary submissions to that task, with BLEU scores ranging from 19.34 to 40.77. Our subsequent debugged runs yielded higher translation accuracy. Updated results for our debugged runs on the development sets are reported in Table 4 for the Chinese-English task. For running the submitted buggy model on the official IWSLT-07 test set, there is a slight difference between the official BLEU score of 34.26 and our own measurement of 34.04. This difference appears to be caused by slight differences betwee"
2007.iwslt-1.12,2006.amta-papers.25,0,0.0178306,"on the development sets are reported in Table 4 for the Chinese-English task. For running the submitted buggy model on the official IWSLT-07 test set, there is a slight difference between the official BLEU score of 34.26 and our own measurement of 34.04. This difference appears to be caused by slight differences between BLEU scoring tools and settings (the tool we are using appears to give lower scores).We also computed the other most commonly used automatic evaluation metrics for translation quality: NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and Translation Edit Rate (TER) (Snover et al., 2006), Word Error Rate (WER), Position-Independent Word Error Rate (PER) and CDER (Leusch et al., 2006). 6. Comparing Moses results with Pharaoh Using the same phrasal bilexicon and language model as with Moses, we performed several contrastive runs using Pharaoh, all other settings being identical. Results are reported in Table 5 for three different baseline experimental settings. We performed many experimental runs in which we vary the experimental settings and pre or post processing steps, e.g. phrase tables, language models, to compare the translation quality produced by Pharoah and Moses. The"
2007.iwslt-1.12,P96-1021,1,0.764787,"Missing"
2007.iwslt-1.12,J97-3002,1,0.656199,"Missing"
2007.iwslt-1.12,2005.iwslt-1.20,0,0.0197801,"vanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 2. Machine translation engine We focus on phrase-based statistical machine translation since this approach has been shown to achieve competitive translation quality in many state-of-the-art systems (e.g., Zens et al. (2005), Shen et al. (2006), Eck et al. (2006)), and therefore is a good benchmark to evaluate decoder architectures. 2.1. Decoder We use the Moses decoder (Koehn et al., 2007), which is an open source toolkit for statistical machine translation. Just like the Pharaoh decoder, Moses uses a log-linear model, which combines several knowledge sources in translation decisions. Moses differs from Pharaoh by its represention of each input word as a factor as opposed to the word suface form only. Factors can include additional information such as part-ofspeech, class, morphology and allow the phrase-based m"
2007.iwslt-1.12,2006.iwslt-evaluation.20,0,0.0201845,"previous evaluations where manual and automatic transcriptions of speech had to be translated. The IWSLT 2007 test set therefore matches more closely with the training data and the first three development test data, as opposed to the more recent Chinese-English tasks where automatic transcriptions of read and spontaneous speech were used. 3.2. Training data preprocessing For the training data, we used the same basic preprocessing as in our IWSLT 2006 submission, which consists in performing tokenization and case normalization. The case normalization method is the same as the one described in Zollmann et al. (2006), where the first word of the sentence is normalized to its most frequent form. English: The English was simply tokenized and case-normalized in the same manner for all languages. Chinese: We use the LDC segmenter to re-segment the Chinese side of the corpus to get a better segmentation. 3.3. English text normalization For all language pairs, in addition to training data normalization, we use simple heuristics to normalize punctuation, capitalization and contractions in the English output. 3.4. Improved sentence segmentation Since the training data is drawn from clean text as opposed to speech"
2007.iwslt-1.12,koen-2004-pharaoh,0,\N,Missing
2007.iwslt-1.12,W05-0909,0,\N,Missing
2007.iwslt-1.12,D07-1007,1,\N,Missing
2007.iwslt-1.12,D08-1064,0,\N,Missing
2007.iwslt-1.12,C08-1141,0,\N,Missing
2007.iwslt-1.12,2005.iwslt-1.16,0,\N,Missing
2007.mtsummit-papers.11,W05-0909,0,0.036251,"xicon is still a pure WSD model. Just as in any Senseval/SemEval multilingual lexical sample task (e.g., Chklovski et al. (2004)), the task consists of disambiguating between semantic distinctions made by another language. 4 Evaluation on full-scale translation We have conducted a comprehensive evaluation on two standard Chinese to English translation tasks, using all eight of the most commonly employed automated evaluation metrics. For every task, we evaluate translation quality with both BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores along with the recently proposed METEOR (Banerjee and Lavie, 2005) with and without WordNet synonyms. In addition, we report for each task four edit-distance style metrics: Word Error Rate (WER), Position-independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al., 2006), and Translation Edit Rate (TER) (Snover et al., 2006). Since our goal is to evaluate actual translation quality, we restrict ourselves to standard MT evaluation methodology. We do not evaluate the disambiguation accuracy of the embedded WSD models independently, as we cannot safely assume that higher WSD evaluation scores necessarily lead to"
2007.mtsummit-papers.11,P91-1034,0,0.0893691,"glish translation and allows for rich expressive context features, but this system was also only evaluated on WSD accuracy, and not integrated in a full-scale machine translation system. 2.2 Context-dependent SMT To the best of our knowledge, our model represents the first attempt at integrating a fully phrasal context-dependent translation lexicon into SMT, where evaluation is conducted by measuring the accuracy of the resulting SMT system on a translation task (as opposed to, for example, measures of word sense disambiguation accuracy as discussed in the preceding section). In contrast with Brown et al. (1991), our approach incorporates the predictions of state-of-the-art WSD models that generalize across rich contextual features for any phrase in the input vocabulary. In Brown et al.’s early study of contextual features on SMT performance, the authors reported improved translation quality on a French to English task, by choosing an English translation for a French word based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize t"
2007.mtsummit-papers.11,I05-2021,1,0.941639,"e that for languages that do not contain space characters, such as Chinese (as considered in this paper), it is not even clear what “single word” means. Any string of characters could be considered as either a word or a phrase, if we insist on forcing an analogy to European languages. 2.3 WSD vs. SMT In previous work, we have obtained seemingly conflicting empirical evidence on the usefulness of WSD in SMT. When we integrated the WSD predictions of Senseval-style WSD models into a word-based SMT system in a number of ways, for the first time, we surprisingly obtained a decrease in BLEU score (Carpuat and Wu, 2005b). However, we also showed that SMT systems alone perform much worse that WSD systems on a WSD task (Carpuat and Wu, 2005a), which suggests that WSD should have something to offer to SMT. Taken together, these results suggest that a better framework for integrating contextual evidence in SMT is needed. In this paper, we argue that such a framework is provided by context-dependent phrasal translation lexicons. 3 Context-dependent phrasal translation lexicons As mentioned earlier, there are two main open issues in moving toward context-dependent phrasal translation lexicons. First, which dynami"
2007.mtsummit-papers.11,W04-0822,1,0.927777,"tual evidence into a translation or sense prediction. In particular, the Senseval/SemEval series of workshops have extensively evaluated systems with different feature sets, as well as different machine learning models for combining contextual evidence. Recent work on WSD for SMT also provides interesting insights. Following this approach, we propose to exploit WSD insights to build context-dependent translation lexicons for SMT. We use context features and WSD models that were designed and evaluated on several Senseval-2 lexical sample tasks (Yarowsky and Florian, 2002) and Senseval-3 tasks (Carpuat et al., 2004). On the one hand, these tasks included monolingual lexical choice tasks, where word senses are defined according to some manually built ontology or semantic network such as WordNet, HowNet, or the like. More relevant to the task at hand, however, are also the multilingual lexical choice tasks, where word senses are directly defined as the semantic distinctions made by another language (e.g., Chklovski et al. (2004)). In many ways, the multilingual lexical choice tasks of Senseval/SemEval embody a more empirically justifiable approach to defining the sense inventory for WSD than the monolingua"
2007.mtsummit-papers.11,2006.iwslt-evaluation.5,1,0.831432,"e NIST and METEOR metrics are slightly improved while the BLEU and WER score gets worse, which shows that the results are not consistent across the most widely used automated evaluation metrics. This confirms that the full sentential context and the syntactic features used by WSD models are necessary to translate long phrases as well as single words, and therefore that WSD is an appropriate framework for integrating contextual information into traditional phrase-based SMT. Note that we also reported small improvements in BLEU score by using single-word WSD predictions in a Pharaoh baseline in Carpuat et al. (2006). These small improvements were obtained on a slightly weaker SMT baseline. On the contrary, Table 5 shows that BLEU scores now actually slightly decrease with our stronger baseline. This restricted lexicon approach is similar to the proposal by Cabezas and Resnik (2005) who used the XML input scheme to provide word-based WSD predictions in the Pharaoh decoder. They obtained small gains in BLEU score on the Spanish-English Europarl task. However, their report does not check consistency of this improvement using other evaluation metrics and other data sets. 11 Conclusion We have described a new"
2007.mtsummit-papers.11,W07-0403,0,0.0152676,"aim was to study this approach for the broadest possible class of models, we chose one of the most widely used SMT models as the baseline, namely flat phrasebased SMT. In light of the encouraging results, dynamic context-dependent phrasal translation lexicons might also be integrated into other current SMT models such as treestructured SMT models employing various kinds of stochastic transduction grammars (e.g., Wu (1997), Wu and Chiang (2007)). For example, the context-dependent predictions might be utilized by a Bracketing ITG based decoder such as that of Wu (1996), Zens et al. (2004), or Cherry and Lin (2007), or alternatively a more grammatically structured statistical MT model that is less reliant on n-gram language modeling, such as the syntactic ITG based “grammatical channel” translation model of (Wu and Wong, 1998). The question remains open as to which type of SMT model could make most effective use of contextdependent phrasal translation lexicons. Acknowledgements This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E,"
2007.mtsummit-papers.11,W04-0802,0,0.162256,"exicons for SMT. We use context features and WSD models that were designed and evaluated on several Senseval-2 lexical sample tasks (Yarowsky and Florian, 2002) and Senseval-3 tasks (Carpuat et al., 2004). On the one hand, these tasks included monolingual lexical choice tasks, where word senses are defined according to some manually built ontology or semantic network such as WordNet, HowNet, or the like. More relevant to the task at hand, however, are also the multilingual lexical choice tasks, where word senses are directly defined as the semantic distinctions made by another language (e.g., Chklovski et al. (2004)). In many ways, the multilingual lexical choice tasks of Senseval/SemEval embody a more empirically justifiable approach to defining the sense inventory for WSD than the monolingual ontology-based lexical choice tasks. Sense inventories constructed on the basis of manually-built ontologies inherit an enormous variety of arbitrary choices made by the ontology builder, that can damage prediction and generalization accuracy. Since ontologies are not directly observable, as the saying goes, there are as many different ontologies as there are ontology builders. Unlike manually built ontologies, on"
2007.mtsummit-papers.11,P01-1027,0,0.0231678,"quality on a French to English task, by choosing an English translation for a French word based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize to more recent SMT architectures and full phrasal translation lexicons. It is also necessary to focus directly on translation accuracy rather than other measures such as alignment error rate, which may not actually lead to improved translation quality; in contrast, for example, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based contextdependent lexical choice model, but not improved translation accuracy. Our evaluation in this paper is conducted on the decoding task, rather than intermediate tasks such as word alignment. Moreover, in the present work, all commonly available automated MT evaluation metrics are used. Another problem in the context-dependent SMT models of Garcia Varea et al. is that their feature set is insufficiently rich to make much better predictions than the SMT model itself. In contrast, our dynamic con"
2007.mtsummit-papers.11,garcia-varea-etal-2002-efficient,0,0.0236878,"task, by choosing an English translation for a French word based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize to more recent SMT architectures and full phrasal translation lexicons. It is also necessary to focus directly on translation accuracy rather than other measures such as alignment error rate, which may not actually lead to improved translation quality; in contrast, for example, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based contextdependent lexical choice model, but not improved translation accuracy. Our evaluation in this paper is conducted on the decoding task, rather than intermediate tasks such as word alignment. Moreover, in the present work, all commonly available automated MT evaluation metrics are used. Another problem in the context-dependent SMT models of Garcia Varea et al. is that their feature set is insufficiently rich to make much better predictions than the SMT model itself. In contrast, our dynamic context-dependent phrasal lexicons"
2007.mtsummit-papers.11,koen-2004-pharaoh,0,0.0337202,"re corpora, and therefore of a much wider domain than the IWSLT data set. The training set consists of about 1 million sentence pairs in the news domain. Basic preprocessing was applied to the corpus. The English side was simply tokenized and case-normalized. The Chinese side was word segmented using the LDC segmenter. 4.2 A standard baseline SMT system Our aim is to lay out an approach that can be expected to work in any reasonably common phrase-based SMT implementation. Since our focus is not on a specific SMT architecture, we chose the widely-used off-the-shelf phrasebased decoder Pharaoh (Koehn, 2004). Pharaoh implements a beam search decoder for phrase-based statistical models, and presents the advantages of being freely available and widely used. Table 3: Translation examples with and without the WSDbased dynamic context-dependent phrasal translation lexicon, drawn from IWSLT data sets. Input ÷ l X -. ¿ Ref. Please transfer to the Central train line . SMT Please turn to the Central Line . +WSD Please transfer to Central Line . Input fh ( f p Ref. Do I pay on the bus ? SMT Please get on the bus ? +WSD I buy a ticket on the bus ? Input • „¢ Ref. Do I need a reservation ? SMT I need a reser"
2007.mtsummit-papers.11,E06-1031,0,0.0159085,"n We have conducted a comprehensive evaluation on two standard Chinese to English translation tasks, using all eight of the most commonly employed automated evaluation metrics. For every task, we evaluate translation quality with both BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores along with the recently proposed METEOR (Banerjee and Lavie, 2005) with and without WordNet synonyms. In addition, we report for each task four edit-distance style metrics: Word Error Rate (WER), Position-independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al., 2006), and Translation Edit Rate (TER) (Snover et al., 2006). Since our goal is to evaluate actual translation quality, we restrict ourselves to standard MT evaluation methodology. We do not evaluate the disambiguation accuracy of the embedded WSD models independently, as we cannot safely assume that higher WSD evaluation scores necessarily lead to higher translation accuracies. 4.1 Two very different tasks with 4 test sets One set of experiments was conducted using training and evaluation data drawn from the multilingual BTEC corpus, which contains sentences used in conversations in the travel dom"
2007.mtsummit-papers.11,J03-1002,0,0.00340523,"ation lexicons (e.g. “turn” vs. “transfer” in the first example, “get” vs. “buy” in the second example, “open” vs. “have” in the last example). Across all the IWSLT test sets, an average of 19 features per occurrence of a Chinese phrase are observed and used to build the dynamic context-dependent lexicon. This confirms that the rich WSD-style context features are indeed used for SMT translation lexicons, even in the single domain IWSLT corpus where sentences are quite short. 7 The phrase bilexicon was derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2003), augmented to improve recall. The language model was trained on the English side of the corpus using the SRI language modeling toolkit (Stolcke, 2002). The loglinear model weights were learned using Chiang’s implementation of the maximum BLEU training algorithm (Och, 2003), both for the baseline and for the WSD-augmented system. In the remaining sections, we discuss a number of different analyses of the experimental results. 5 Context-dependent modeling consistently improves translation The most obvious observation on the experimental results, as shown in Table 1 for the IWSLT task and Table"
2007.mtsummit-papers.11,P03-1021,0,0.0328371,"ntext-dependent lexicon. This confirms that the rich WSD-style context features are indeed used for SMT translation lexicons, even in the single domain IWSLT corpus where sentences are quite short. 7 The phrase bilexicon was derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2003), augmented to improve recall. The language model was trained on the English side of the corpus using the SRI language modeling toolkit (Stolcke, 2002). The loglinear model weights were learned using Chiang’s implementation of the maximum BLEU training algorithm (Och, 2003), both for the baseline and for the WSD-augmented system. In the remaining sections, we discuss a number of different analyses of the experimental results. 5 Context-dependent modeling consistently improves translation The most obvious observation on the experimental results, as shown in Table 1 for the IWSLT task and Table 2 for the NIST task, is that making the phrasal translation lexicons context-dependent produces higher translation quality on all test sets, as measured by all eight commonly used automated evaluation metrics. Paired bootstrap resampling shows that the improvements on the m"
2007.mtsummit-papers.11,P02-1040,0,0.0846811,"n. Despite these differences, the WSD model supporting our context-dependent phrasal translation lexicon is still a pure WSD model. Just as in any Senseval/SemEval multilingual lexical sample task (e.g., Chklovski et al. (2004)), the task consists of disambiguating between semantic distinctions made by another language. 4 Evaluation on full-scale translation We have conducted a comprehensive evaluation on two standard Chinese to English translation tasks, using all eight of the most commonly employed automated evaluation metrics. For every task, we evaluate translation quality with both BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores along with the recently proposed METEOR (Banerjee and Lavie, 2005) with and without WordNet synonyms. In addition, we report for each task four edit-distance style metrics: Word Error Rate (WER), Position-independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al., 2006), and Translation Edit Rate (TER) (Snover et al., 2006). Since our goal is to evaluate actual translation quality, we restrict ourselves to standard MT evaluation methodology. We do not evaluate the disambiguation accuracy of the embedded WSD"
2007.mtsummit-papers.11,2006.amta-papers.25,0,0.0279322,"tandard Chinese to English translation tasks, using all eight of the most commonly employed automated evaluation metrics. For every task, we evaluate translation quality with both BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores along with the recently proposed METEOR (Banerjee and Lavie, 2005) with and without WordNet synonyms. In addition, we report for each task four edit-distance style metrics: Word Error Rate (WER), Position-independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al., 2006), and Translation Edit Rate (TER) (Snover et al., 2006). Since our goal is to evaluate actual translation quality, we restrict ourselves to standard MT evaluation methodology. We do not evaluate the disambiguation accuracy of the embedded WSD models independently, as we cannot safely assume that higher WSD evaluation scores necessarily lead to higher translation accuracies. 4.1 Two very different tasks with 4 test sets One set of experiments was conducted using training and evaluation data drawn from the multilingual BTEC corpus, which contains sentences used in conversations in the travel domain, and their translations in several languages. A sub"
2007.mtsummit-papers.11,2005.mtsummit-papers.33,0,0.536754,"ion, which has taken place largely independently of the SMT community, has been directly targeted at the question of how to design context features and combine a wide range of contextual evidence into making a translation or sense prediction. Evaluation of WSD models is typically done on WSD accuracy only—it is implicitly assumed that better WSD models will help higher level applications such as SMT. Recently, several researchers have focused on designing WSD systems that use rich contextual information for the specific purpose of translation, instead of any sense distinctions. Vickrey et al. (2005) train a logistic regression WSD model on data extracted from automatically word aligned parallel corpora, but evaluate on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia (2006) describes an inductive logic programming-based WSD system, which was specifically designed for the purpose of Portuguese to English translation and allows for rich expressive context features, but this system was also only evaluated on WSD accuracy, and not integrated in a full-scale machine translation system. 2.2 Context-dependent SMT To the best of our knowledge, our model represents"
2007.mtsummit-papers.11,P98-2230,1,0.786317,"xt-dependent phrasal translation lexicons might also be integrated into other current SMT models such as treestructured SMT models employing various kinds of stochastic transduction grammars (e.g., Wu (1997), Wu and Chiang (2007)). For example, the context-dependent predictions might be utilized by a Bracketing ITG based decoder such as that of Wu (1996), Zens et al. (2004), or Cherry and Lin (2007), or alternatively a more grammatically structured statistical MT model that is less reliant on n-gram language modeling, such as the syntactic ITG based “grammatical channel” translation model of (Wu and Wong, 1998). The question remains open as to which type of SMT model could make most effective use of contextdependent phrasal translation lexicons. Acknowledgements This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Resear"
2007.mtsummit-papers.11,P96-1021,1,0.645726,"dependent probabilities. Since our aim was to study this approach for the broadest possible class of models, we chose one of the most widely used SMT models as the baseline, namely flat phrasebased SMT. In light of the encouraging results, dynamic context-dependent phrasal translation lexicons might also be integrated into other current SMT models such as treestructured SMT models employing various kinds of stochastic transduction grammars (e.g., Wu (1997), Wu and Chiang (2007)). For example, the context-dependent predictions might be utilized by a Bracketing ITG based decoder such as that of Wu (1996), Zens et al. (2004), or Cherry and Lin (2007), or alternatively a more grammatically structured statistical MT model that is less reliant on n-gram language modeling, such as the syntactic ITG based “grammatical channel” translation model of (Wu and Wong, 1998). The question remains open as to which type of SMT model could make most effective use of contextdependent phrasal translation lexicons. Acknowledgements This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Gra"
2007.mtsummit-papers.11,J97-3002,1,0.476098,"D scores at all stages of decoding, since the contextdependent WSD scores are defined for every phrase in the bilexicon, just like regular context-independent probabilities. Since our aim was to study this approach for the broadest possible class of models, we chose one of the most widely used SMT models as the baseline, namely flat phrasebased SMT. In light of the encouraging results, dynamic context-dependent phrasal translation lexicons might also be integrated into other current SMT models such as treestructured SMT models employing various kinds of stochastic transduction grammars (e.g., Wu (1997), Wu and Chiang (2007)). For example, the context-dependent predictions might be utilized by a Bracketing ITG based decoder such as that of Wu (1996), Zens et al. (2004), or Cherry and Lin (2007), or alternatively a more grammatically structured statistical MT model that is less reliant on n-gram language modeling, such as the syntactic ITG based “grammatical channel” translation model of (Wu and Wong, 1998). The question remains open as to which type of SMT model could make most effective use of contextdependent phrasal translation lexicons. Acknowledgements This material is based upon work s"
2007.mtsummit-papers.11,C04-1030,0,0.0382302,"Missing"
2007.mtsummit-papers.11,H05-1097,0,\N,Missing
2007.mtsummit-papers.11,C98-2225,1,\N,Missing
2007.tmi-papers.10,I05-2021,1,0.838417,"results in section 5. In section 6, we then demonstrate experimentally how ARG ALIGN outperforms a more conventional method based on semantic role projection, SYN ALIGN. 2 Problem Definition In recent years, researchers have shown that statistical machine translation models can be enhanced by incorporating structural information (Wu and Chiang, 2007). The atten76 tion, though, has thus far been largely focused on chunk or syntactic structures. Researchers only recently began seriously investigating whether incorporating semantic models can enhance statistical machine translation performance (Carpuat and Wu, 2005a; Carpuat and Wu, 2005b), and are only just beginning to show that semantic word sense disambiguation techniques can indeed improve accuracy (Carpuat et al., 2006; Carpuat and Wu, 2007). However, it remains an intriguing open question as to how semantic structures—semantic role mappings in bilingual semantic frames—can also be potentially leveraged to improve machine translation. Thus, in order to overcome the immediate obstacle to exploring this potential, we are interested in learning the bilingual semantic structure given a predicate verb pair in English and Chinese, as in Figure 1. The pr"
2007.tmi-papers.10,P05-1048,1,0.821015,"results in section 5. In section 6, we then demonstrate experimentally how ARG ALIGN outperforms a more conventional method based on semantic role projection, SYN ALIGN. 2 Problem Definition In recent years, researchers have shown that statistical machine translation models can be enhanced by incorporating structural information (Wu and Chiang, 2007). The atten76 tion, though, has thus far been largely focused on chunk or syntactic structures. Researchers only recently began seriously investigating whether incorporating semantic models can enhance statistical machine translation performance (Carpuat and Wu, 2005a; Carpuat and Wu, 2005b), and are only just beginning to show that semantic word sense disambiguation techniques can indeed improve accuracy (Carpuat et al., 2006; Carpuat and Wu, 2007). However, it remains an intriguing open question as to how semantic structures—semantic role mappings in bilingual semantic frames—can also be potentially leveraged to improve machine translation. Thus, in order to overcome the immediate obstacle to exploring this potential, we are interested in learning the bilingual semantic structure given a predicate verb pair in English and Chinese, as in Figure 1. The pr"
2007.tmi-papers.10,D07-1007,1,0.811803,"nition In recent years, researchers have shown that statistical machine translation models can be enhanced by incorporating structural information (Wu and Chiang, 2007). The atten76 tion, though, has thus far been largely focused on chunk or syntactic structures. Researchers only recently began seriously investigating whether incorporating semantic models can enhance statistical machine translation performance (Carpuat and Wu, 2005a; Carpuat and Wu, 2005b), and are only just beginning to show that semantic word sense disambiguation techniques can indeed improve accuracy (Carpuat et al., 2006; Carpuat and Wu, 2007). However, it remains an intriguing open question as to how semantic structures—semantic role mappings in bilingual semantic frames—can also be potentially leveraged to improve machine translation. Thus, in order to overcome the immediate obstacle to exploring this potential, we are interested in learning the bilingual semantic structure given a predicate verb pair in English and Chinese, as in Figure 1. The predicate verb pair “organized /举办” have the operators ARG0 “African Environmental Centre/非洲环境中心”, and the operands ARG1 “Seminar on desertification/沙漠化问题研讨 会”. In the above example, the s"
2007.tmi-papers.10,2006.iwslt-evaluation.5,1,0.813528,"ALIGN. 2 Problem Definition In recent years, researchers have shown that statistical machine translation models can be enhanced by incorporating structural information (Wu and Chiang, 2007). The atten76 tion, though, has thus far been largely focused on chunk or syntactic structures. Researchers only recently began seriously investigating whether incorporating semantic models can enhance statistical machine translation performance (Carpuat and Wu, 2005a; Carpuat and Wu, 2005b), and are only just beginning to show that semantic word sense disambiguation techniques can indeed improve accuracy (Carpuat et al., 2006; Carpuat and Wu, 2007). However, it remains an intriguing open question as to how semantic structures—semantic role mappings in bilingual semantic frames—can also be potentially leveraged to improve machine translation. Thus, in order to overcome the immediate obstacle to exploring this potential, we are interested in learning the bilingual semantic structure given a predicate verb pair in English and Chinese, as in Figure 1. The predicate verb pair “organized /举办” have the operators ARG0 “African Environmental Centre/非洲环境中心”, and the operands ARG1 “Seminar on desertification/沙漠化问题研讨 会”. In t"
2007.tmi-papers.10,P06-1146,0,0.356547,"hich , [ARG0 more than 20 thousand enterprises ] have [T ARGET received] [ARG1 loan support] [ARG2 from the Bank of China] . Chinese [ARGM −T M P 目前] ， 约 有 十五万 家 外商 投资 企业 在 中国 银行 开立 帐 户 ， 其中 [ARG0 二万多 家] [T ARGET 获 得] [ARG1 中国 银行 的 贷款 支持] 。 Gloss currently, about 150 thousand foreign merchant investment enterprise in China Bank open account, of which, 20 thousand more enterprise receive China Bank’s loan support . 6 Role Mapping from Syntactic Constituent Alignment To date, it is often casually assumed that semantic roles can be simply projected across language pairs by constituent alignment (Pado and Lapata, 2006). In such an approach, it is assumed that an English constituent is lexically translated into the Chinese constituent, in which case they must share the same role label. This sort of view is typically inspired by the many structurallybased statistical machine translation models that make use of some kind of syntactic constituent projection (Hwa et al., 2005). Therefore it is worth investigating the possibility of projecting semantic role labels across matching syntactic constituents. To accomplish this, we implement a contrastive SYN ALIGN algorithm that obtains semantic structure mapping base"
2007.tmi-papers.10,W05-0309,0,0.0696953,"Missing"
2007.tmi-papers.10,N04-1030,0,0.0610291,"ng to the unavoidable errors through POS tagging, chunking or syntactic parsing, among the bilingual sentences, some Chinese and English sentences have no identifiable predicate verb, and are eliminated from further processing. Finally, 397 sentence pairs with automatic semantic parsing results are used in our predicate-argument mapping experiment. In our proposed method, Chinese/English shallow semantic parsing is a prerequisite to achieving the task of bilingual semantic frame mapping. In recent years, there has been a lot of research on shallow semantic labeling or parsing both in English (Pradhan et al., 2004; Pradhan et al., 2005) and Chinese (Sun and Jurafsky, 2004; Xue and Palmer, 2005). In our experiments, we use the ASSERT semantic parser (Pradhan, 2005) to carry out the automatic semantic parsing on the English side and a similar SVM-based Chinese semantic parsing system (Wu et al., 2006) on the Chinese side. According to (Pradhan et al., 2005), their English semantic parser achieved 89.40 F-score with gold syntactic parse input, and 79.40 F-score with automatic syntactic parse input. Meanwhile, our SVM-based Chinese semantic parser yielded 89.89 F-score with gold syntactic parse input and 6"
2007.tmi-papers.10,N04-1032,0,0.0590976,"g or syntactic parsing, among the bilingual sentences, some Chinese and English sentences have no identifiable predicate verb, and are eliminated from further processing. Finally, 397 sentence pairs with automatic semantic parsing results are used in our predicate-argument mapping experiment. In our proposed method, Chinese/English shallow semantic parsing is a prerequisite to achieving the task of bilingual semantic frame mapping. In recent years, there has been a lot of research on shallow semantic labeling or parsing both in English (Pradhan et al., 2004; Pradhan et al., 2005) and Chinese (Sun and Jurafsky, 2004; Xue and Palmer, 2005). In our experiments, we use the ASSERT semantic parser (Pradhan, 2005) to carry out the automatic semantic parsing on the English side and a similar SVM-based Chinese semantic parsing system (Wu et al., 2006) on the Chinese side. According to (Pradhan et al., 2005), their English semantic parser achieved 89.40 F-score with gold syntactic parse input, and 79.40 F-score with automatic syntactic parse input. Meanwhile, our SVM-based Chinese semantic parser yielded 89.89 F-score with gold syntactic parse input and 69.12 Fscore with automatic syntactic parse input. Both of t"
2007.tmi-papers.6,P07-1005,0,0.225601,"all improvements were obtained on a slightly weaker SMT baseline, and subsequent evaluations showed that these gains are not consistent across metrics. Gim´enez and M`arquez (2007) also used WSD predictions in Pharaoh for the slightly more general case of very frequent phrases, which in practice essentially limits the set of WSD targets to single words or very short phrases. However, evaluation on the single Europarl Spanish-English task did not yield consistent improvements across metrics: BLEU score did not improve, while there were small improvements in the QUEEN, METEOR and ROUGE metrics. Chan et al. (2007) report an improved BLEU score for a hierarchical phrase-based SMT system on a NIST Chinese-English task, by incorporating WSD predictions only for single words and short phrases of length 1 or 2. However, no results for metrics other than BLEU were reported, and no results on other tasks, so the relia45 bility of this model is not known. What the foregoing attempts at WSD in SMT share is that (1) they focus on single words rather than full phrases, and (2) the evaluations do not show consistent improvement systematically across different tasks and metrics. In contrast, we showed in Carpuat an"
2007.tmi-papers.6,P05-1033,0,0.0629702,"do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 43 word boundaries. Linguistically, however, this oversimplistic modeling approach seems rather questionable, and recalls long-held debates over the issue of what properly constitutes a “word”. In contrast, work in statistical machine translation has for some time recognized the need to segment sentences as required by the task’s evaluation criteria, and today most systems use phrases or segments, and not single words, as the basic unit for lexical choice (e.g., Wu (1997); Och and Ney (2004); Koehn (2004); Chiang (2005)). Note that single-word based SMT architectures already perform a significant amount of sense disambiguation intrinsically, by virtue of combining a priori sense candidate likelihoods (from adequacy criteria as modeled by lexical translation probabilities) with contextual coherence preferences (from fluency criteria as modeled by language model probabilities). Phrasal SMT architectures, furthermore, integrate lexical collocation preferences into the disambiguation choices, raising the bar yet higher. This suggests that to be effective at improving disambiguation accuracy within SMT architectu"
2007.tmi-papers.6,W04-0802,0,0.0326319,"parallel corpora used to learn the SMT lexicon. Given a Chinese-English sentence pair, a WSD or PSD target in the Chinese sentence is annotated with the English phrase which is consistent with the word alignment. The definition of consistency with the word alignment should be exactly the one used for building the SMT lexicon. Despite the differences introduced by the use of phrasal targets, the disambiguation task remains in the character and spirit of WSD. The translation lexical choice problem is exactly the same task as in recent and coming Senseval Multilingual Lexical Sample tasks (e.g., Chklovski et al. (2004)), where sense inventories represent the semantic distinctions made by another language. In our SMT-driven approach to PSD rather than WSD, we are only generalizing the definition of the sense disambiguation targets, and automating the sense annotation process. 3.2 Leveraging Senseval classifiers for both WSD and PSD As in Carpuat and Wu (2007), the word sense disambiguation system is modeled after the best performing WSD system in the Chinese lexical sample task at Senseval-3 (Carpuat et al., 2004). The features employed include position-sensitive, syntactic, and local collocational features,"
2007.tmi-papers.6,P01-1027,0,0.0805489,"to English translation, but this system was also only evaluated on WSD accuracy, and not integrated in a fullscale machine translation system. Even when using automatically-aligned SMT parallel corpora to define WSD tasks, as in the SemEval-2007 English Lexical Sample Task via English-Chinese Parallel Text (Ng and Chan, 2007), WSD is still defined as a word-based task. There have been other attempts at using context information for lexical selection in SMT, but the focus was also on single words vs. multi-word phrases, and they were not evaluated in terms of translation quality. For instance, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based context-dependent lexical choice model, but do not report improved translation accuracy. Another problem in the context-sensitive SMT models of Garcia Varea et al. is that they strictly reside within the Bayesian source-channel model, which is word-based. The few recent attempts at integrating single word based WSD models into SMT have failed to obtain clear improvements in terms of translation quality. Carpuat and Wu (2005) show that using word-based Senseval trained models does not help BLEU score"
2007.tmi-papers.6,garcia-varea-etal-2002-efficient,0,0.124056,"s system was also only evaluated on WSD accuracy, and not integrated in a fullscale machine translation system. Even when using automatically-aligned SMT parallel corpora to define WSD tasks, as in the SemEval-2007 English Lexical Sample Task via English-Chinese Parallel Text (Ng and Chan, 2007), WSD is still defined as a word-based task. There have been other attempts at using context information for lexical selection in SMT, but the focus was also on single words vs. multi-word phrases, and they were not evaluated in terms of translation quality. For instance, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based context-dependent lexical choice model, but do not report improved translation accuracy. Another problem in the context-sensitive SMT models of Garcia Varea et al. is that they strictly reside within the Bayesian source-channel model, which is word-based. The few recent attempts at integrating single word based WSD models into SMT have failed to obtain clear improvements in terms of translation quality. Carpuat and Wu (2005) show that using word-based Senseval trained models does not help BLEU score when integrated in a standard"
2007.tmi-papers.6,W07-0719,0,0.0250273,"Missing"
2007.tmi-papers.6,S01-1004,0,0.0142708,"ine translation (SMT) models have met with mixed or disappointing results (e.g., Carpuat and Wu (2005), Cabezas and Resnik (2005)), suggesting that a deeper empirical exploration of the differences and consequences of the assumptions of WSD and SMT is called for. On one hand, word sense disambiguation as a standalone task consists in identifying the correct sense of a given word among a set of predefined sense candidates. In the Senseval series of evaluations, WSD targets are typically single words, both in the lexical sample tasks, where only a predefined set of targets are considered (e.g., Kilgarriff (2001); ), and in the all-words tasks, where all content word in a given corpus must be disambiguated (e.g., Kilgarriff and Rosenzweig (1999)). This focus on single words as WSD targets might be explained by the sense inventory, which is usually derived from a manually constructed dictionary or ontology, where most entries are single words. In addition, historically, as for many other tasks, work on European languages imposed whitespace as an easy way to define convenient the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Gra"
2007.tmi-papers.6,koen-2004-pharaoh,0,0.400558,"he authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 43 word boundaries. Linguistically, however, this oversimplistic modeling approach seems rather questionable, and recalls long-held debates over the issue of what properly constitutes a “word”. In contrast, work in statistical machine translation has for some time recognized the need to segment sentences as required by the task’s evaluation criteria, and today most systems use phrases or segments, and not single words, as the basic unit for lexical choice (e.g., Wu (1997); Och and Ney (2004); Koehn (2004); Chiang (2005)). Note that single-word based SMT architectures already perform a significant amount of sense disambiguation intrinsically, by virtue of combining a priori sense candidate likelihoods (from adequacy criteria as modeled by lexical translation probabilities) with contextual coherence preferences (from fluency criteria as modeled by language model probabilities). Phrasal SMT architectures, furthermore, integrate lexical collocation preferences into the disambiguation choices, raising the bar yet higher. This suggests that to be effective at improving disambiguation accuracy within"
2007.tmi-papers.6,E06-1031,0,0.0275463,"Missing"
2007.tmi-papers.6,S07-1010,0,0.0116643,"SD model on data extracted from automatically word aligned parallel corpora, and evaluate it on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia et al. (2007) use an inductive logic programming based WSD system to integrate expressive features for Portuguese to English translation, but this system was also only evaluated on WSD accuracy, and not integrated in a fullscale machine translation system. Even when using automatically-aligned SMT parallel corpora to define WSD tasks, as in the SemEval-2007 English Lexical Sample Task via English-Chinese Parallel Text (Ng and Chan, 2007), WSD is still defined as a word-based task. There have been other attempts at using context information for lexical selection in SMT, but the focus was also on single words vs. multi-word phrases, and they were not evaluated in terms of translation quality. For instance, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based context-dependent lexical choice model, but do not report improved translation accuracy. Another problem in the context-sensitive SMT models of Garcia Varea et al. is that they strictly reside within the B"
2007.tmi-papers.6,J04-4002,0,0.0121231,"erial are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 43 word boundaries. Linguistically, however, this oversimplistic modeling approach seems rather questionable, and recalls long-held debates over the issue of what properly constitutes a “word”. In contrast, work in statistical machine translation has for some time recognized the need to segment sentences as required by the task’s evaluation criteria, and today most systems use phrases or segments, and not single words, as the basic unit for lexical choice (e.g., Wu (1997); Och and Ney (2004); Koehn (2004); Chiang (2005)). Note that single-word based SMT architectures already perform a significant amount of sense disambiguation intrinsically, by virtue of combining a priori sense candidate likelihoods (from adequacy criteria as modeled by lexical translation probabilities) with contextual coherence preferences (from fluency criteria as modeled by language model probabilities). Phrasal SMT architectures, furthermore, integrate lexical collocation preferences into the disambiguation choices, raising the bar yet higher. This suggests that to be effective at improving disambiguation a"
2007.tmi-papers.6,W05-0909,0,0.0425738,"gnificant improvements on the large NIST task, while in contrast, the impact of single-word WSD on translation quality is highly unpredictable. In particular, the single-word WSD results are inconsistent across different test sets, and depend on which evaluation metric is chosen. In order to measure the impact of WSD on translation quality, the translation results were evaluated using all eight of the most commonly used automatic evaluation metrics. In addition to the widely used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with METEOR (Banerjee and Lavie, 2005), Word Error Rate (WER), Position-independent word Error Rate (PER) (Tillmann et al., 1997), CDER (Leusch 48 et al., 2006), and Translation Edit Rate (TER) (Snover et al., 2006). Note that we report METEOR scores computed both with and without using WordNet synonyms to match translation candidates and references, showing that the improvement is not due to context-independent synonym matches at evaluation time. In the sections that follow, we investigate various reasons that PSD outperforms WSD, drawing from data analysis on these comparative experiments. 7 Single-word WSD yields unreliable res"
2007.tmi-papers.6,P02-1040,0,0.104047,"Missing"
2007.tmi-papers.6,P91-1034,0,0.268585,"tegrating a generalized sense disambiguation method into SMT, such that phrasal lexical choice is dynamically influenced by context-dependent probabilities or scores. This Phrase Sense Disambiguation— as opposed to Word Sense Disambiguation— approach appears to be the only model to date that has been shown capable of consistently yielding improvements on translation quality across all 44 different test sets and automatic evaluation metrics. Other related work has all been heavily oriented toward disambiguating single words. In perhaps the earliest study of WSD potential for SMT performance by Brown et al. (1991), the authors reported improved translation quality on a French to English task, by choosing an English translation for a French word based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to single words with exactly two translation candidates, and it is far from clear that the conclusions could generalize to more recent SMT architectures. In contrast with Brown et al.’s work, our approach incorporates the predictions of state-of-the-art WSD models (generalized to PSD models) that use rich contextual features for any phrase i"
2007.tmi-papers.6,P07-1006,0,0.0123474,"T architectures. In contrast with Brown et al.’s work, our approach incorporates the predictions of state-of-the-art WSD models (generalized to PSD models) that use rich contextual features for any phrase in the input vocabulary. More recent work on WSD systems designed for the specific purpose of translation has followed the traditional word-based definition of the WSD task. Vickrey et al. (2005) train a logistic regression WSD model on data extracted from automatically word aligned parallel corpora, and evaluate it on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia et al. (2007) use an inductive logic programming based WSD system to integrate expressive features for Portuguese to English translation, but this system was also only evaluated on WSD accuracy, and not integrated in a fullscale machine translation system. Even when using automatically-aligned SMT parallel corpora to define WSD tasks, as in the SemEval-2007 English Lexical Sample Task via English-Chinese Parallel Text (Ng and Chan, 2007), WSD is still defined as a word-based task. There have been other attempts at using context information for lexical selection in SMT, but the focus was also on single word"
2007.tmi-papers.6,D07-1007,1,0.914733,"the contrastive experiments reported here show that incorporating single-word WSD into phrasal SMT leads to unpredictable and inconsistent effects on translation quality, depending on which evaluation metric one looks at. We then turn to data analysis exploring more closely how and why the multi-word PSD approach outperforms the single-word WSD approach. The analysis shows that dynamic integration of PSD prediction is crucial to this improvement, as it allows all PSD predictions to participate in the segmention of the input sentence that yields the best translation quality. 2 Previous work In Carpuat and Wu (2007), we proposed a novel general framework for integrating a generalized sense disambiguation method into SMT, such that phrasal lexical choice is dynamically influenced by context-dependent probabilities or scores. This Phrase Sense Disambiguation— as opposed to Word Sense Disambiguation— approach appears to be the only model to date that has been shown capable of consistently yielding improvements on translation quality across all 44 different test sets and automatic evaluation metrics. Other related work has all been heavily oriented toward disambiguating single words. In perhaps the earliest"
2007.tmi-papers.6,W04-0822,1,0.843534,"actly the same task as in recent and coming Senseval Multilingual Lexical Sample tasks (e.g., Chklovski et al. (2004)), where sense inventories represent the semantic distinctions made by another language. In our SMT-driven approach to PSD rather than WSD, we are only generalizing the definition of the sense disambiguation targets, and automating the sense annotation process. 3.2 Leveraging Senseval classifiers for both WSD and PSD As in Carpuat and Wu (2007), the word sense disambiguation system is modeled after the best performing WSD system in the Chinese lexical sample task at Senseval-3 (Carpuat et al., 2004). The features employed include position-sensitive, syntactic, and local collocational features, and are therefore much richer than those used in most SMT systems. 4 Integrating multi-word PSD vs. single-word WSD into phrasal SMT architectures Unlike single-word WSD, it is non-trivial to incorporate the PSD predictions into an existing phrase-based architecture such as Pharaoh (Koehn, 2004), since the decoder is not set up to easily accept multiple translation probabilities that are dynamically computed in context46 sensitive fashion. While PSD and WSD models differ in principle only by the le"
2007.tmi-papers.6,H05-1097,0,0.0369903,"which is reliably discriminative. However, this was a pilot study, which is limited to single words with exactly two translation candidates, and it is far from clear that the conclusions could generalize to more recent SMT architectures. In contrast with Brown et al.’s work, our approach incorporates the predictions of state-of-the-art WSD models (generalized to PSD models) that use rich contextual features for any phrase in the input vocabulary. More recent work on WSD systems designed for the specific purpose of translation has followed the traditional word-based definition of the WSD task. Vickrey et al. (2005) train a logistic regression WSD model on data extracted from automatically word aligned parallel corpora, and evaluate it on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia et al. (2007) use an inductive logic programming based WSD system to integrate expressive features for Portuguese to English translation, but this system was also only evaluated on WSD accuracy, and not integrated in a fullscale machine translation system. Even when using automatically-aligned SMT parallel corpora to define WSD tasks, as in the SemEval-2007 English Lexical Sample Task via E"
2007.tmi-papers.6,J97-3002,1,0.162575,"in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 43 word boundaries. Linguistically, however, this oversimplistic modeling approach seems rather questionable, and recalls long-held debates over the issue of what properly constitutes a “word”. In contrast, work in statistical machine translation has for some time recognized the need to segment sentences as required by the task’s evaluation criteria, and today most systems use phrases or segments, and not single words, as the basic unit for lexical choice (e.g., Wu (1997); Och and Ney (2004); Koehn (2004); Chiang (2005)). Note that single-word based SMT architectures already perform a significant amount of sense disambiguation intrinsically, by virtue of combining a priori sense candidate likelihoods (from adequacy criteria as modeled by lexical translation probabilities) with contextual coherence preferences (from fluency criteria as modeled by language model probabilities). Phrasal SMT architectures, furthermore, integrate lexical collocation preferences into the disambiguation choices, raising the bar yet higher. This suggests that to be effective at improv"
2007.tmi-papers.6,2006.iwslt-evaluation.5,1,\N,Missing
2009.eamt-1.30,W05-0909,0,0.0388687,"Missing"
2009.eamt-1.30,P05-1048,1,0.119242,"en increasing amounts of evidence that SMT accuracy can indeed be improved via tree-structured and syntactic models (e.g., Wu (1997); Chiang and Wu (2008); Wu and Chiang (2009)) despite numerous disappointProceedings of the 13th Annual Conference of the EAMT, pages 218–225, Barcelona, May 2009 218 ing attempts Och et al. (2004). More recently, lexical semantics models for word sense disambiguation have also finally been successfully applied to increasing SMT accuracy (e.g., Carpuat and Wu (2007), Chan et al. (2007); Gim´enez and M`arquez (2007a)) again after surprising initial failures (e.g., Carpuat and Wu (2005)). In both the syntactic and semantic cases, improving SMT accuracy ultimately required making major adaptations to the original linguistic models. We can reasonably expect it to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling. Avoiding the many potential blind alleys calls for careful analysis and evaluation of (1) the frequencies of types of SMT errors where semantic parsing and role labeling could help, (2) if and when semantic roles offer more accurate guidance to SMT than merely syntactic anno"
2009.eamt-1.30,D07-1007,1,0.262102,"l difficulty of making syntactic and semantic models contribute to improving SMT accuracy. The past decade has at last seen increasing amounts of evidence that SMT accuracy can indeed be improved via tree-structured and syntactic models (e.g., Wu (1997); Chiang and Wu (2008); Wu and Chiang (2009)) despite numerous disappointProceedings of the 13th Annual Conference of the EAMT, pages 218–225, Barcelona, May 2009 218 ing attempts Och et al. (2004). More recently, lexical semantics models for word sense disambiguation have also finally been successfully applied to increasing SMT accuracy (e.g., Carpuat and Wu (2007), Chan et al. (2007); Gim´enez and M`arquez (2007a)) again after surprising initial failures (e.g., Carpuat and Wu (2005)). In both the syntactic and semantic cases, improving SMT accuracy ultimately required making major adaptations to the original linguistic models. We can reasonably expect it to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling. Avoiding the many potential blind alleys calls for careful analysis and evaluation of (1) the frequencies of types of SMT errors where semantic parsing an"
2009.eamt-1.30,P07-1005,0,0.0516711,"syntactic and semantic models contribute to improving SMT accuracy. The past decade has at last seen increasing amounts of evidence that SMT accuracy can indeed be improved via tree-structured and syntactic models (e.g., Wu (1997); Chiang and Wu (2008); Wu and Chiang (2009)) despite numerous disappointProceedings of the 13th Annual Conference of the EAMT, pages 218–225, Barcelona, May 2009 218 ing attempts Och et al. (2004). More recently, lexical semantics models for word sense disambiguation have also finally been successfully applied to increasing SMT accuracy (e.g., Carpuat and Wu (2007), Chan et al. (2007); Gim´enez and M`arquez (2007a)) again after surprising initial failures (e.g., Carpuat and Wu (2005)). In both the syntactic and semantic cases, improving SMT accuracy ultimately required making major adaptations to the original linguistic models. We can reasonably expect it to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling. Avoiding the many potential blind alleys calls for careful analysis and evaluation of (1) the frequencies of types of SMT errors where semantic parsing and role labeling coul"
2009.eamt-1.30,J02-3001,0,0.00785905,"ntial quantitative impact of realistic semantic role guidance to SMT systems, at least in terms of scores such as BLEU and METEOR. In this paper, we present a series of four experiments designed to address each of these questions, using Chinese-English parallel resources, a typical representative SMT system based on Moses, and shallow semantic parsers for both English and Chinese. (when). For a sentence with multiple verbs, there can be multiple predicate argument structures. Shallow semantic parsing systems are mostly based on classifiers that learn from a manually annotated semantic corpus (Gildea and Jurafsky (2002), Pradhan et al. (2005)). Following the publication of the Proposional Bank (PropBank) (Palmer et al., 2005) first in English, then in Chinese, it has been possible to train these classifiers to perform semantic analysis on news wire type of texts. 2 2.2 Chinese shallow semantic parsing Related work While this is a new avenue of inquiry, the background relevant to the experiments described here includes (1) a broad body of work on shallow semantic parsing and semantic role labeling, the majority of which has been performed on English, (2) a relatively small body of work specific to semantic pa"
2009.eamt-1.30,W07-0738,0,0.0740987,"Missing"
2009.eamt-1.30,W08-0332,0,0.101108,"Missing"
2009.eamt-1.30,P07-2045,0,0.00344519,"Missing"
2009.eamt-1.30,N04-1021,0,0.0287774,"mained consistent cross-lingually across sentence translations. We approach this promise with caution, however, given the painful lessons learned through the historical difficulty of making syntactic and semantic models contribute to improving SMT accuracy. The past decade has at last seen increasing amounts of evidence that SMT accuracy can indeed be improved via tree-structured and syntactic models (e.g., Wu (1997); Chiang and Wu (2008); Wu and Chiang (2009)) despite numerous disappointProceedings of the 13th Annual Conference of the EAMT, pages 218–225, Barcelona, May 2009 218 ing attempts Och et al. (2004). More recently, lexical semantics models for word sense disambiguation have also finally been successfully applied to increasing SMT accuracy (e.g., Carpuat and Wu (2007), Chan et al. (2007); Gim´enez and M`arquez (2007a)) again after surprising initial failures (e.g., Carpuat and Wu (2005)). In both the syntactic and semantic cases, improving SMT accuracy ultimately required making major adaptations to the original linguistic models. We can reasonably expect it to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing an"
2009.eamt-1.30,P02-1040,0,0.103507,"Missing"
2009.eamt-1.30,2007.iwslt-1.12,1,0.846107,".1 Experimental setup To assess the above sorts of phenomena quantitatively, we designed an experiment making use of 745 bi-sentences extracted from the Parallel PropBank with gold standard annotations of both syntactic and semantic roles. We use the Chinese sentences as system input and their corresponding English translations as the reference translations. We use the open source statistical machine translation decoder Moses (?) for the experiments, translating the PropBank Chinese sentences into English with the same model trained for our participation in the IWSLT 2007 evaluation campaign (Shen et al., 2007).The English translations generated by the decoder are the system output. Based on the system input and the reference Table 1: Accuracy of predicate-argument structure in Chinese-English SMT output for data set A. P-A Precision Recall F-measure Structure Predicate 0.98 0.57 0.72 ARG0 0.74 0.38 0.50 ARG1 0.73 0.41 0.53 ARG2 0.82 0.32 0.46 ARG3 1.00 0.67 0.80 ARG4 1.00 0.33 0.51 All ARGs 0.74 0.39 0.51 translation, we intend to investigate whether the predicate verbs are correctly translated and their predicate-argument structures preserved in the system output. We first randomly select 50 bi-se"
2009.eamt-1.30,N04-1032,0,0.0146506,"ho did what to whom, for whom or what, how, where, when, and why.” Shallow semantic parsing extracts the predicateargument structure of verbs in a sentence based on the syntactic tree of that sentence. For example, the predicate argument structure of the verb hold in Figure 1 specifies a “holding” relation between both sides (who) and meeting (what) on Sunday Figure 1: Chinese shallow semantic parsing example. Systems that perform shallow semantic parsing on Chinese texts are likewise based on classifiers and trained on the Chinese PropBank and the bilingual Chinese-English Parallel PropBank (Sun and Jurafsky (2004), Xue (2006), Fung et al. (2006)). It is interesting to note that, despite the very different characteristics of Chinese verbs (Xue and Palmer, 2005) from those in English, the core algorithm of a shallow semantic parser remains the same. As was found to be the case in English, SVM classifiers have been found to outperform maximum entropy classifiers for this task (Fung et al., 2006). The primary difference lies in the feature set chosen to represent semantic information. In experiments carried out on PropBank data using gold standard syntactic parse trees, extended syntactic features such as"
2009.eamt-1.30,J97-3002,1,0.271172,"Missing"
2009.eamt-1.30,N06-1055,0,0.0117212,"whom or what, how, where, when, and why.” Shallow semantic parsing extracts the predicateargument structure of verbs in a sentence based on the syntactic tree of that sentence. For example, the predicate argument structure of the verb hold in Figure 1 specifies a “holding” relation between both sides (who) and meeting (what) on Sunday Figure 1: Chinese shallow semantic parsing example. Systems that perform shallow semantic parsing on Chinese texts are likewise based on classifiers and trained on the Chinese PropBank and the bilingual Chinese-English Parallel PropBank (Sun and Jurafsky (2004), Xue (2006), Fung et al. (2006)). It is interesting to note that, despite the very different characteristics of Chinese verbs (Xue and Palmer, 2005) from those in English, the core algorithm of a shallow semantic parser remains the same. As was found to be the case in English, SVM classifiers have been found to outperform maximum entropy classifiers for this task (Fung et al., 2006). The primary difference lies in the feature set chosen to represent semantic information. In experiments carried out on PropBank data using gold standard syntactic parse trees, extended syntactic features such as Path Trigram"
2009.eamt-1.30,J05-1004,0,\N,Missing
2011.eamt-1.42,N10-1028,0,0.321955,"d anywhere in the training corpus. Naturally, the number of rules explodes when we need to consider all the possible phrasal translations that could have occurred somewhere in the training corpus as individual rules in our grammar. Our solution to this problem is to start with a token based grammar, and then extract larger biterminals that occur in the parse forest of the simpler grammar. This allows us to avoid enumerating phrasal biterminals that would likely be weeded out by EM anyway, and thus saves considerable amounts of space (Section 5). Another approach to this problem is reported in Blunsom and Cohn (2010), involving sampling. Up until the point that a rule is actually sampled, it can be implicitly represented with the probability distribution it is to be drawn from. 3 Linear Transduction Grammars Linear transduction grammars constitute the natural bilingualization of linear grammars (LGs). This class of grammar has received relatively little attention, mostly because there is no obvious use for it. LGs lie between finite-state grammars and context-free grammars in computational complexity, but all you get is palindromes. Another way to look at LGs is that they relate the beginning of a string"
2011.eamt-1.42,J93-2003,0,0.0407826,"Missing"
2011.eamt-1.42,P09-1104,0,0.176076,"from parallel corpora intractable. One way to make induction tractable is to restrict the search to a subset of the syntaxdirected transductions. This approach was pioneered in a series of papers by Wu (1995a; 1995b; 1996; 1997), where inversion transductions and the corresponding inversion transduction grammars (ITGs) were introduced. This restricted set of grammars can parse a sentence pair in O(n6 ) time, making it tractable but not practical to induce grammars from data. Several attempts have been made to approximate biparsing, see for example Zhang et al. (2008), Saers et al. (2009) and Haghighi et al. (2009). An even more aggressive restriction to SDTGs was proposed in Saers et al. (2010b). Where the original ITGs allow branching, the introduced linear ITGs (LITGs) do not, allowing for biparsing in O(n4 ) time. By approximating the search, linear time complexity is attainable. Saers (2011) later introduced the more general class of linear transduction grammars (LTG), which are equivalent to LITG s in terms of generative capacity. Searching for linear transductions rather than inversion transductions or full syntax-directed transductions makes the grammar induction tractable. It does, however, als"
2011.eamt-1.42,2005.iwslt-1.8,0,0.0280376,"Missing"
2011.eamt-1.42,P07-2045,0,0.00420266,"g data to sentence pairs where none of the two sentences were longer than 20 tokens (see Table 1). Our train–tune–test pipeline includes: 1. Preprocessing tools: tokenizer, corpus cleaner and case folder (supplied by the organizers). 2. Language model: 5-gram model using SRILM (Stolcke, 2002). 3. Translation model: Phrase-based model extracted and scored with with the Moses 318 Corpus French–English (train) French–English (tune) French–English (test) English (LM) Size 381,780 sent. pairs 2,000 sent. pairs 2,000 sent. pairs 1,412,546 sentences Table 1: Corpora used in the experiments. toolkit (Koehn et al., 2007) using the grow-diag-final-and heuristic (Koehn et al., 2005) on bidirectional word alignments obtained through IBM-models (Brown et al., 1993) and HMM-alignment (Vogel et al., 1996) using GIZA ++ (Och and Ney, 2000). 4. Tuning: Minimum error-rate training (Och and Ney, 2002). 5. Decoder: Moses (Koehn et al., 2007). 6. Postprocessing: recaser (trained with the Moses toolkit) and detokenization (supplied by the organizers). 7. Evaluation: NIST (Doddington, 2002) and BLEU (Papineni et al., 2002). This constitutes the baseline. Our system replaces the translation model with a phrasal bilexicon fr"
2011.eamt-1.42,P00-1056,0,0.212615,"Missing"
2011.eamt-1.42,P02-1038,0,0.0719414,"(Stolcke, 2002). 3. Translation model: Phrase-based model extracted and scored with with the Moses 318 Corpus French–English (train) French–English (tune) French–English (test) English (LM) Size 381,780 sent. pairs 2,000 sent. pairs 2,000 sent. pairs 1,412,546 sentences Table 1: Corpora used in the experiments. toolkit (Koehn et al., 2007) using the grow-diag-final-and heuristic (Koehn et al., 2005) on bidirectional word alignments obtained through IBM-models (Brown et al., 1993) and HMM-alignment (Vogel et al., 1996) using GIZA ++ (Och and Ney, 2000). 4. Tuning: Minimum error-rate training (Och and Ney, 2002). 5. Decoder: Moses (Koehn et al., 2007). 6. Postprocessing: recaser (trained with the Moses toolkit) and detokenization (supplied by the organizers). 7. Evaluation: NIST (Doddington, 2002) and BLEU (Papineni et al., 2002). This constitutes the baseline. Our system replaces the translation model with a phrasal bilexicon from PLITG induction. To isolate the effect of the bilexicon (which is the main focus of this paper), we refrained from using the more advanced reordering model that the Moses toolkit can build from alignments. This is a concept that is completely orthogonal to the bilexicon, a"
2011.eamt-1.42,P02-1040,0,0.0835989,"000 sent. pairs 2,000 sent. pairs 1,412,546 sentences Table 1: Corpora used in the experiments. toolkit (Koehn et al., 2007) using the grow-diag-final-and heuristic (Koehn et al., 2005) on bidirectional word alignments obtained through IBM-models (Brown et al., 1993) and HMM-alignment (Vogel et al., 1996) using GIZA ++ (Och and Ney, 2000). 4. Tuning: Minimum error-rate training (Och and Ney, 2002). 5. Decoder: Moses (Koehn et al., 2007). 6. Postprocessing: recaser (trained with the Moses toolkit) and detokenization (supplied by the organizers). 7. Evaluation: NIST (Doddington, 2002) and BLEU (Papineni et al., 2002). This constitutes the baseline. Our system replaces the translation model with a phrasal bilexicon from PLITG induction. To isolate the effect of the bilexicon (which is the main focus of this paper), we refrained from using the more advanced reordering model that the Moses toolkit can build from alignments. This is a concept that is completely orthogonal to the bilexicon, and rather than simulating it when converting the PLITG to a bilexicon, we chose to leave it out. The PLITG bilexicon was induced by combining existing biterminals three times, with five iterations of expectation maximizati"
2011.eamt-1.42,W09-3804,1,0.816584,"e, which makes induction from parallel corpora intractable. One way to make induction tractable is to restrict the search to a subset of the syntaxdirected transductions. This approach was pioneered in a series of papers by Wu (1995a; 1995b; 1996; 1997), where inversion transductions and the corresponding inversion transduction grammars (ITGs) were introduced. This restricted set of grammars can parse a sentence pair in O(n6 ) time, making it tractable but not practical to induce grammars from data. Several attempts have been made to approximate biparsing, see for example Zhang et al. (2008), Saers et al. (2009) and Haghighi et al. (2009). An even more aggressive restriction to SDTGs was proposed in Saers et al. (2010b). Where the original ITGs allow branching, the introduced linear ITGs (LITGs) do not, allowing for biparsing in O(n4 ) time. By approximating the search, linear time complexity is attainable. Saers (2011) later introduced the more general class of linear transduction grammars (LTG), which are equivalent to LITG s in terms of generative capacity. Searching for linear transductions rather than inversion transductions or full syntax-directed transductions makes the grammar induction tract"
2011.eamt-1.42,W10-3802,1,0.653028,"rminals directly from the parallel corpus using expectationmaximization (EM). When it has stabilized, we collect all biterminal pairs that could form larger biterminals and incorporate them into the grammar to produce a multi-token transduction grammar. The process is then repeated until large enough units are learned. Since maximum likelihood learning methods such as EM tends to favor longer chunks over shorter, we introduce a length penalty for multi-token terminals. Since induction of transduction grammars is very time consuming, we opt to view the parallel corpus as a linear transduction (Saers et al., 2010b; Saers, 2011). This assumption allows us to use something that is equivalent to linear transduction grammars (LTGs), which can approximate the search for a parse forest given a sentence pair in linear time. LTGs do not, however, have an explicit biterminal concept, making it non-trivial to map the grammar to a probabilistic bilexicon. To fix this, we introduce preterminalized linear inversion transduction grammars (PLITGs), which will allow the desired parameterization. Learning a stochastic bracketing PLITG from a parallel corpus is equivalent to building a probabilistic bilexicon based on"
2011.eamt-1.42,N10-1050,1,0.875306,"rminals directly from the parallel corpus using expectationmaximization (EM). When it has stabilized, we collect all biterminal pairs that could form larger biterminals and incorporate them into the grammar to produce a multi-token transduction grammar. The process is then repeated until large enough units are learned. Since maximum likelihood learning methods such as EM tends to favor longer chunks over shorter, we introduce a length penalty for multi-token terminals. Since induction of transduction grammars is very time consuming, we opt to view the parallel corpus as a linear transduction (Saers et al., 2010b; Saers, 2011). This assumption allows us to use something that is equivalent to linear transduction grammars (LTGs), which can approximate the search for a parse forest given a sentence pair in linear time. LTGs do not, however, have an explicit biterminal concept, making it non-trivial to map the grammar to a probabilistic bilexicon. To fix this, we introduce preterminalized linear inversion transduction grammars (PLITGs), which will allow the desired parameterization. Learning a stochastic bracketing PLITG from a parallel corpus is equivalent to building a probabilistic bilexicon based on"
2011.eamt-1.42,C96-2141,0,0.465776,"Missing"
2011.eamt-1.42,P95-1033,1,0.579108,"represent an NP-complete search problem that needs to be heavily pruned to be practically useful. The problem with SDTGs is that they are very time consuming to learn from parallel corpora. Whereas all context-free grammars can be reduced to an equivalent grammar in a two-normal form, SDTGs cannot. With an arbitrary-rank SDTG, parsing a sentence pair requires O(n2n+2 ) time, which makes induction from parallel corpora intractable. One way to make induction tractable is to restrict the search to a subset of the syntaxdirected transductions. This approach was pioneered in a series of papers by Wu (1995a; 1995b; 1996; 1997), where inversion transductions and the corresponding inversion transduction grammars (ITGs) were introduced. This restricted set of grammars can parse a sentence pair in O(n6 ) time, making it tractable but not practical to induce grammars from data. Several attempts have been made to approximate biparsing, see for example Zhang et al. (2008), Saers et al. (2009) and Haghighi et al. (2009). An even more aggressive restriction to SDTGs was proposed in Saers et al. (2010b). Where the original ITGs allow branching, the introduced linear ITGs (LITGs) do not, allowing for bipa"
2011.eamt-1.42,W95-0106,1,0.735405,"represent an NP-complete search problem that needs to be heavily pruned to be practically useful. The problem with SDTGs is that they are very time consuming to learn from parallel corpora. Whereas all context-free grammars can be reduced to an equivalent grammar in a two-normal form, SDTGs cannot. With an arbitrary-rank SDTG, parsing a sentence pair requires O(n2n+2 ) time, which makes induction from parallel corpora intractable. One way to make induction tractable is to restrict the search to a subset of the syntaxdirected transductions. This approach was pioneered in a series of papers by Wu (1995a; 1995b; 1996; 1997), where inversion transductions and the corresponding inversion transduction grammars (ITGs) were introduced. This restricted set of grammars can parse a sentence pair in O(n6 ) time, making it tractable but not practical to induce grammars from data. Several attempts have been made to approximate biparsing, see for example Zhang et al. (2008), Saers et al. (2009) and Haghighi et al. (2009). An even more aggressive restriction to SDTGs was proposed in Saers et al. (2010b). Where the original ITGs allow branching, the introduced linear ITGs (LITGs) do not, allowing for bipa"
2011.eamt-1.42,P96-1021,1,0.689329,"Missing"
2011.eamt-1.42,J97-3002,1,0.682699,"languages to each other, with the caveat that two of them are also related to each other. Definition 1. An LTG over languages L1 and L2 is a tuple G = hN, Σ, ∆, S, Ri where N is a finite, nonempty set of nonterminal symbols, Σ is a finite, nonempty set of L1 symbols, ∆ is a finite, nonempty set of L2 symbols, S ∈ N is the designated start symbol, and R is a finite, nonempty set of linear transduction rules on the forms: A → a/x B b/y, A → a/x where A, B ∈ N , a, b ∈ Σ∗ and x, y ∈ ∆∗ . Linear inversion transduction grammars (LITGs), on the other hand, are inversion transduction grammars, ITGs (Wu, 1997) that have been subjected to a linearity constraint. An ITG is a transduction grammar restricted to have only context-free rules, and only monotonic permutations (specifically identity or inversion permutation). By subjecting an ITG in normal form to a linearity constraint, we replace one of the nonterminal symbols with biterminal symbols, thereby significantly reducing the expressive power of the grammar, for a significant efficiency boost. LITGs were introduced in Saers et al. (2010b), and subsequently compared to full ITGs in Saers et al. (2010a). The rules in an LITG take the following for"
2011.eamt-1.42,P08-1012,0,0.299944,"equires O(n2n+2 ) time, which makes induction from parallel corpora intractable. One way to make induction tractable is to restrict the search to a subset of the syntaxdirected transductions. This approach was pioneered in a series of papers by Wu (1995a; 1995b; 1996; 1997), where inversion transductions and the corresponding inversion transduction grammars (ITGs) were introduced. This restricted set of grammars can parse a sentence pair in O(n6 ) time, making it tractable but not practical to induce grammars from data. Several attempts have been made to approximate biparsing, see for example Zhang et al. (2008), Saers et al. (2009) and Haghighi et al. (2009). An even more aggressive restriction to SDTGs was proposed in Saers et al. (2010b). Where the original ITGs allow branching, the introduced linear ITGs (LITGs) do not, allowing for biparsing in O(n4 ) time. By approximating the search, linear time complexity is attainable. Saers (2011) later introduced the more general class of linear transduction grammars (LTG), which are equivalent to LITG s in terms of generative capacity. Searching for linear transductions rather than inversion transductions or full syntax-directed transductions makes the gr"
2011.mtsummit-papers.49,J93-2003,0,0.061902,"Missing"
2011.mtsummit-papers.49,J09-4009,0,0.183874,"lignments that can be generated are perfectly straight diagonals. Given a grammar that can generate inﬁnitely long sentence pairs, the set of these token alignments (one for every possible sentence length) is also inﬁnitely large, which means that we cannot compare the absolute sizes of the sets. Instead we will observe how the number of token alignment grows as a function of their length, and for easy comparison to following grammars, we will express it as a recurrence formula: aF1 = 1, aFn = aFn−1 + 1 Moving on to inversion transduction grammars (ITGs), we know from previous work (Wu, 1997; Huang et al., 2009) that the number of token alignments up to and including length n is equal to the nth large Schr¨oder numbers (Schr¨oder, 1870), which can be expressed as: aI1 = 1, aI2 = 2, aIn = 6n−9 n aIn−1 − n−3 n aIn−2 Finally, we have the arbitrary rank syntax-directed transduction grammars (SDTGs), which are capable of generating any permutation (Lewis and Stearns, 433 1968; Aho and Ullman, 1972). The number of permutations are n!, which we can also formulate as a recurrence formula: aT1 = 1, aTn = naTn−1 It should be clear that these series grow at different paces, and that: aFn &lt; aIn &lt; aTn 4 Weak alig"
2011.mtsummit-papers.49,N03-1017,0,0.0699369,"Missing"
2011.mtsummit-papers.49,W02-1018,0,0.0540128,"Missing"
2011.mtsummit-papers.49,2011.eamt-1.42,1,0.872194,"ments that are allowed under a transduction as its weak alignment capacity. This aspect of expressivity is quantiﬁed for linear transductions using preterminalized linear inversion transduction grammars, and compared to the expressivity of ﬁnite-state transductions, inversion transductions and syntax-directed transductions. 1 Introduction We investigate the formal expressivity properties of linear transductions, the class of transductions generated by linear transduction grammars (Saers, 2011, LTGs), linear inversion transduction grammars (Saers et al., 2010, LITGs) and preterminalized LITGs (Saers and Wu, 2011, PLITGs). While empirical results such as those in previous work are of course an ultimate test of modeling adequacy for machine translation applications, it is 431 Although not always given the attention deserved, the expressivity of different grammar-based methods varies quite a lot. Unlike monolingual grammars, where all context-free grammars, for example, fall into one class of languages, the bilingual case is not as well-behaved. Syntax-directed transduction grammars (Lewis and Stearns, 1968; Aho and Ullman, 1972) form distinct transduction classes for all ranks above 3, while ranks 2 an"
2011.mtsummit-papers.49,N10-1050,1,0.938221,"s generated. We refer to the number of different alignments that are allowed under a transduction as its weak alignment capacity. This aspect of expressivity is quantiﬁed for linear transductions using preterminalized linear inversion transduction grammars, and compared to the expressivity of ﬁnite-state transductions, inversion transductions and syntax-directed transductions. 1 Introduction We investigate the formal expressivity properties of linear transductions, the class of transductions generated by linear transduction grammars (Saers, 2011, LTGs), linear inversion transduction grammars (Saers et al., 2010, LITGs) and preterminalized LITGs (Saers and Wu, 2011, PLITGs). While empirical results such as those in previous work are of course an ultimate test of modeling adequacy for machine translation applications, it is 431 Although not always given the attention deserved, the expressivity of different grammar-based methods varies quite a lot. Unlike monolingual grammars, where all context-free grammars, for example, fall into one class of languages, the bilingual case is not as well-behaved. Syntax-directed transduction grammars (Lewis and Stearns, 1968; Aho and Ullman, 1972) form distinct transd"
2011.mtsummit-papers.49,2010.eamt-1.5,0,0.0218833,"is that a PLITG is unable to generate the inside-out alignments, which is ex3 This is only valid for permutation vectors of length four, since we know that the underspeciﬁed part, being of length three, constitutes a valid permutation vector. pected since linear transductions are a proper subset of inversion transductions. The other two that cannot be generated are [1, 0, 3, 2] (which we call serial inversion) and [2, 3, 0, 1] (which we call constituent swapping). Whereas there are some evidence that the inside-out alignments are irrelevant to natural language translation (Huang et al., 2009; Søgaard, 2010), no such results exist for serial inversion and constituent swapping. On the contrary, we intuitively expect these phenomena to be frequent between natural languages. We consider this to be a serious problem with linear transductions, but empirical studies will have give the ﬁnal say on how much it hurts performance. 5 Conclusions In this paper we have presented an analysis of the weak reordering capacity of linear transductions, and compared it to that of ﬁnite-state transduction grammars, inversion transduction grammars and syntax-directed transduction grammars. We have showed that it is po"
2011.mtsummit-papers.49,C96-2141,0,0.262768,"ence and Engineering One Microsoft Way, Redmond Hong Kong University of Science and Technology Washington, USA {masaers|dekai}@cs.ust.hk chrisq@microsoft.com Abstract equally important to understand the formal theoretical properties of any such new representation. In recent years, there has been a shift away from surface-based translation method such as phrasebased statistical machine translation (phrase-based SMT) (Marcu and Wong, 2002; Koehn et al., 2003) in favor of grammar-based SMT. Although most of the grammar based methods still rely on surfacebased word alignments (Brown et al., 1993; Vogel et al., 1996) and language speciﬁc parsers, the grammar-based models themselves restrict reordering to a much higher degree than the surface-based methods, which typically allow any permutation of any segmentation of the input, and relies on heuristic search methods such as beam search, to restrict the exponential time to a tractable polynomial. We investigate the formal expressivity properties of linear transductions, the class of transductions generated by linear transduction grammars, linear inversion transduction grammars and preterminalized linear inversion transduction grammars. While empirical resul"
2011.mtsummit-papers.49,J97-3002,1,0.895993,"ork are of course an ultimate test of modeling adequacy for machine translation applications, it is 431 Although not always given the attention deserved, the expressivity of different grammar-based methods varies quite a lot. Unlike monolingual grammars, where all context-free grammars, for example, fall into one class of languages, the bilingual case is not as well-behaved. Syntax-directed transduction grammars (Lewis and Stearns, 1968; Aho and Ullman, 1972) form distinct transduction classes for all ranks above 3, while ranks 2 and 3 form a class of their own termed inversion transductions (Wu, 1997), and rank 1 forms the class termed linear transductions (Saers, 2011). In this paper we will take a closer look at the expressive powers of transduction grammars in general, noting that the concept of generative capacity fails to capture all the relevant details. Instead, we will propose a division of the expressivity into a strong and weak transductive capacity and alignCapacity Weak Transductive Strong Weak Alignment Strong Required equality Sentence pairs Biparse trees Token alignments Compositional alignments Table 1: Capacities of transduction grammars. ment capacity. The argument for th"
2012.eamt-1.64,W07-0718,0,0.2635,"Missing"
2012.eamt-1.64,2011.eamt-1.3,0,0.0287249,"Missing"
2012.eamt-1.64,W08-0309,0,0.234232,"Missing"
2012.eamt-1.64,W07-0738,0,0.354497,"Missing"
2012.eamt-1.64,W08-0332,0,0.246741,"Missing"
2012.eamt-1.64,J09-4009,0,0.0233322,"Huang et al., 2009). The linear time skeleton algorithm builds canonical binarization trees by reducing greedily but such an approach would not work for a LTG. For example, the permutation [3, 2, 0, 1] which can be parsed by an LTG reduces to 2-3, 0-1 on the stack which cannot be further reduced. 296 We propose an algorithm that makes use of a technique similar to top-down parsing of bisentences using linear transduction grammars. The algorithm is as shown in the procedure parsable. In order to prove the correctness of the algorithm, we use the deﬁnition of permuted sequence from Huang et al. (2009) but we redeﬁne proper split in the context of BLTGs. The proof is as follows: Deﬁnition 1. A permuted sequence is a permutation of consecutive integers. If a permuted sequence of sequence a can be split into the concatenation of a permuted sequence b and a single element of permutation α such that a = (b; α) or a = (α; b), then the corresponding split is called the proper split of a. The deﬁnition of a proper split implicitly imposes the constraints of a linear transduction grammar. Restricting one of the elements in a split to be a single element in the permutation is equivalent to allowing"
2012.eamt-1.64,P11-1023,1,0.855519,"Missing"
2012.eamt-1.64,J05-1004,0,0.0113465,"labels, we noticed that there were some inconsistencies in the annotation. The sentence pairs were manually annotated with the frame sets deﬁned for Chinese and English Zh/En alt. patterns [arg0:action:arg1] [arg0:action] [action:arg1] [arg1:action] [action:arg2] [arg0:action:arg2] [action:arg4] [arg1:action:arg2] [arg1:action:arg4] Sum [arg0:action:arg1] 88 0 0 0 0 3 0 3 3 97 [arg0:action 0 11 3 12 1 0 1 0 0 28 [action:arg1] 0 0 39 6 5 0 1 0 0 51 [arg1:action] 0 0 1 3 0 0 0 0 0 4 Sum 88 11 43 21 6 3 2 3 3 Table 1: Frequency of source and target alternation pattern occurrence in the Propbank (Palmer et al., 2005). We argue that it is due to the limitation of frame set deﬁnitions as they were deﬁned to be consistent within one language but not across languages. For example, in the frame set deﬁnition of 死于 (died of), the arg0 is the entity who dies, while in the frame set deﬁnition of its translation die, the deceased is arg1 and there is no arg0 deﬁned. Similar observations could be made for most of the sentence pairs which diﬀered in source and target alternation labels. As our initial analysis of cross-lingual verb frame alternation patterns suggests that patterns in one language align with only a r"
2012.eamt-1.64,2011.mtsummit-papers.49,1,0.897227,"Missing"
2012.eamt-1.64,N09-2004,1,0.901535,"Missing"
2012.eamt-1.64,J97-3002,1,0.916868,"nd Linear Transduction Grammars (Saers, 2011). As a part of our evaluation, we discuss the reordering of semantic roles within a frame and across frames within a sentence. We also present a novel algorithm to determine whether there exists a canonical parse for an alignment under Linear Transduction Grammars. © 2012 European Association for Machine Translation. 295 To fulﬁll the above requirements, we evaluate two well known syntax-based machine translation formalisms: Inversion Transduction Grammars or ITGs (Wu, 1997) and Linear Transduction Grammars or LTGs (Saers, 2011). As discussed in Wu (1997), ITGs allow nearly all possible reorderings (22 out of 24) given up to four semantic role labels within a semantic frame. Further, various forms of empirical conﬁrmation for the eﬀectiveness of ITG expressivity constraints (Zens and Ney, 2003; Zhang and Gildea, 2005, 2004) motivate us to choose it as a likely candidate. Though ITGs are far more constraining than other higher order syntax directed transduction grammars and IBM models, it would be interesting to see how far an even more constrained model is able to handle reorderings of semantic role ﬁllers. For this purpose, we choose LTGs whi"
2012.eamt-1.64,P03-1019,0,0.745834,"Missing"
2012.eamt-1.64,C04-1060,0,0.0688712,"Missing"
2012.eamt-1.64,P05-1059,0,0.658455,"Missing"
2012.eamt-1.64,H93-1040,0,\N,Missing
2012.eamt-1.64,1993.mtsummit-1.24,0,\N,Missing
2012.eamt-1.64,C00-2108,0,\N,Missing
2012.eamt-1.64,2009.eamt-1.30,1,\N,Missing
2012.eamt-1.64,W09-2300,0,\N,Missing
2012.eamt-1.64,N04-1030,0,\N,Missing
2013.iwslt-evaluation.2,P11-1023,1,0.791624,"13 German-English MT and SLT tracks which show that HMEANT provides a perspective which is different from BLEU and TER in evaluating the performance of the MT systems. The IWSLT evaluation campaign has offered a variety of speech translation tasks over the past decade but none of them included evaluation of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should"
2013.iwslt-evaluation.2,W11-1002,1,0.754734,"13 German-English MT and SLT tracks which show that HMEANT provides a perspective which is different from BLEU and TER in evaluating the performance of the MT systems. The IWSLT evaluation campaign has offered a variety of speech translation tasks over the past decade but none of them included evaluation of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should"
2013.iwslt-evaluation.2,2006.amta-papers.25,0,0.0441286,"mpaign has offered a variety of speech translation tasks over the past decade but none of them included evaluation of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tunin"
2013.iwslt-evaluation.2,P02-1040,0,0.0974466,"r the past decade but none of them included evaluation of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, sup"
2013.iwslt-evaluation.2,W05-0909,0,0.0320103,"none of them included evaluation of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper"
2013.iwslt-evaluation.2,E06-1031,0,0.0200686,"ed evaluation of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the re"
2013.iwslt-evaluation.2,niessen-etal-2000-evaluation,0,0.0515966,"tion of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the results of"
2013.iwslt-evaluation.2,E06-1032,0,0.0187295,"challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the results of one of the largest semantic MT evaluations to date, in terms of both the number of systems and the number of translations evaluated, using HMEANT as the evaluation metric. The aims of this evaluation campaign are two-fold: (1) to demonstrate feasibility o"
2013.iwslt-evaluation.2,W06-3114,0,0.0277223,"challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the results of one of the largest semantic MT evaluations to date, in terms of both the number of systems and the number of translations evaluated, using HMEANT as the evaluation metric. The aims of this evaluation campaign are two-fold: (1) to demonstrate feasibility o"
2013.iwslt-evaluation.2,P13-2067,1,0.759382,"t correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the results of one of the largest semantic MT evaluations to date, in terms of both the number of systems and the number of translations evaluated, using HMEANT as the evaluation metric. The aims of this evaluation campaign are two-fold: (1) to demonstrate feasibility of running a large-scale semantic MT evaluation campaign using humans, and (2) to provide fine-grained statistics over a large number of systems that enable a fair com"
2013.iwslt-evaluation.2,2013.mtsummit-papers.12,1,0.783852,"t correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the results of one of the largest semantic MT evaluations to date, in terms of both the number of systems and the number of translations evaluated, using HMEANT as the evaluation metric. The aims of this evaluation campaign are two-fold: (1) to demonstrate feasibility of running a large-scale semantic MT evaluation campaign using humans, and (2) to provide fine-grained statistics over a large number of systems that enable a fair com"
2013.iwslt-evaluation.2,2013.iwslt-evaluation.5,1,0.716509,"t correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the results of one of the largest semantic MT evaluations to date, in terms of both the number of systems and the number of translations evaluated, using HMEANT as the evaluation metric. The aims of this evaluation campaign are two-fold: (1) to demonstrate feasibility of running a large-scale semantic MT evaluation campaign using humans, and (2) to provide fine-grained statistics over a large number of systems that enable a fair com"
2013.iwslt-evaluation.2,W12-3129,1,0.677229,"rences between metrics gauging semantic similarity and surface based metrics, but also quantifies the robustness of HMEANT as an MT evaluation metric. In the rest of the paper, we discuss the details of the evaluation campaign and provide results on the interannotator agreement on the tasks of semantic role annotation and alignment. We also provide an analysis of the time taken for annotation and the alignment of the semantic roles. We also report the results of different participating systems according to the criterion of our semantic evaluation metric HMEANT and its automatic variant, MEANT [15]. 2. Human judges align the semantic frames between the references and the MT output by judging the correctness of the predicates. 3. For each pair of aligned semantic frames, (a) Human judges determine the translation correctness of the semantic role fillers. (b) Human judges align the semantic role fillers between the reference and the MT output according to the correctness of the semantic role fillers. 2. Participating tracks and systems To perform a full-scale semantic MT evaluation, all the systems which participated in IWSLT 2013 GermanEnglish MT and SLT tracks were evaluated. There were"
2013.iwslt-evaluation.2,W12-4206,1,0.732282,"Missing"
2013.iwslt-evaluation.5,P02-1040,0,0.101561,"nglish and English-Chinese translation with that of the baseline SMT systems tuned against BLEU. We show that the improvement of tuning a MT system against MEANT on Chinese translation output is more significant because of the nature of ambiguous word boundaries in Chinese. Our encouraging results show that using MEANT is a promising alternative to BLEU in evaluating and tuning MT systems to drive the progress of MT research across different languages. In the past decade, the progress of MT research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU [1], which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning[2, 3]. Unlike BLEU, or other n-gram based MT evaluation metrics, the MEANT family of metrics [4, 5, 6] adopt at outset the principle that a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, wher"
2013.iwslt-evaluation.5,P11-1023,1,0.938243,"valuating and tuning MT systems to drive the progress of MT research across different languages. In the past decade, the progress of MT research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU [1], which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning[2, 3]. Unlike BLEU, or other n-gram based MT evaluation metrics, the MEANT family of metrics [4, 5, 6] adopt at outset the principle that a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU"
2013.iwslt-evaluation.5,W12-4206,1,0.886653,"valuating and tuning MT systems to drive the progress of MT research across different languages. In the past decade, the progress of MT research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU [1], which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning[2, 3]. Unlike BLEU, or other n-gram based MT evaluation metrics, the MEANT family of metrics [4, 5, 6] adopt at outset the principle that a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU"
2013.iwslt-evaluation.5,W12-3129,1,0.854369,"valuating and tuning MT systems to drive the progress of MT research across different languages. In the past decade, the progress of MT research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU [1], which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning[2, 3]. Unlike BLEU, or other n-gram based MT evaluation metrics, the MEANT family of metrics [4, 5, 6] adopt at outset the principle that a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU"
2013.iwslt-evaluation.5,N04-1030,0,0.345627,"sume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning[2, 3]. Unlike BLEU, or other n-gram based MT evaluation metrics, the MEANT family of metrics [4, 5, 6] adopt at outset the principle that a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU [1], NIST [8], METEOR [9], CDER [10], WER [11], and TER [12]. We recently showed that the translation adequacy across different genres (ranging from formal news to informal web forum) is improved by replacing surface oriented metrics like BLEU or"
2013.iwslt-evaluation.5,W05-0909,0,0.788856,"inciple that a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU [1], NIST [8], METEOR [9], CDER [10], WER [11], and TER [12]. We recently showed that the translation adequacy across different genres (ranging from formal news to informal web forum) is improved by replacing surface oriented metrics like BLEU or TER with a semantic frame based objective function, MEANT, when tuning the parameters of MT systems [13, 14]. However, the question of whether the same approach of tuning MT systems against a semantic objective function might improve translation adequacy when translating into other languages, such as Chinese, is left unanswered. Although there exists no studies on correlation"
2013.iwslt-evaluation.5,E06-1031,0,0.811225,"at a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU [1], NIST [8], METEOR [9], CDER [10], WER [11], and TER [12]. We recently showed that the translation adequacy across different genres (ranging from formal news to informal web forum) is improved by replacing surface oriented metrics like BLEU or TER with a semantic frame based objective function, MEANT, when tuning the parameters of MT systems [13, 14]. However, the question of whether the same approach of tuning MT systems against a semantic objective function might improve translation adequacy when translating into other languages, such as Chinese, is left unanswered. Although there exists no studies on correlation between hu"
2013.iwslt-evaluation.5,niessen-etal-2000-evaluation,0,0.911651,"translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU [1], NIST [8], METEOR [9], CDER [10], WER [11], and TER [12]. We recently showed that the translation adequacy across different genres (ranging from formal news to informal web forum) is improved by replacing surface oriented metrics like BLEU or TER with a semantic frame based objective function, MEANT, when tuning the parameters of MT systems [13, 14]. However, the question of whether the same approach of tuning MT systems against a semantic objective function might improve translation adequacy when translating into other languages, such as Chinese, is left unanswered. Although there exists no studies on correlation between human adequa"
2013.iwslt-evaluation.5,2006.amta-papers.25,0,0.717561,"one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU [1], NIST [8], METEOR [9], CDER [10], WER [11], and TER [12]. We recently showed that the translation adequacy across different genres (ranging from formal news to informal web forum) is improved by replacing surface oriented metrics like BLEU or TER with a semantic frame based objective function, MEANT, when tuning the parameters of MT systems [13, 14]. However, the question of whether the same approach of tuning MT systems against a semantic objective function might improve translation adequacy when translating into other languages, such as Chinese, is left unanswered. Although there exists no studies on correlation between human adequacy judgement a"
2013.iwslt-evaluation.5,2013.mtsummit-papers.12,1,0.711088,"similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU [1], NIST [8], METEOR [9], CDER [10], WER [11], and TER [12]. We recently showed that the translation adequacy across different genres (ranging from formal news to informal web forum) is improved by replacing surface oriented metrics like BLEU or TER with a semantic frame based objective function, MEANT, when tuning the parameters of MT systems [13, 14]. However, the question of whether the same approach of tuning MT systems against a semantic objective function might improve translation adequacy when translating into other languages, such as Chinese, is left unanswered. Although there exists no studies on correlation between human adequacy judgement and MEANT scores on Chinese output, we hypothesize that the benefits of tuning against MEANT that we see for English: better adequacy and fluency carries over into Chinese. It is because a high MEANT score is contingent on correct lexical choices as well as getting the syntactic and semantic str"
2013.iwslt-evaluation.5,W07-0738,0,0.119299,"nd TER [12] do not correctly reflect the similarity of the basic event structure— “who did what to whom, when, where and why”— of the input sentence. In fact, a number of large scale meta-evaluations [2, 3] report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work on developing MT evaluation metrics that would outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment [16], however, it is very hard to interpret and indicate what errors the MT systems are making. ULC [17, 18] is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18] but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE [21] is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticate"
2013.iwslt-evaluation.5,W08-0332,0,0.0857312,"nd TER [12] do not correctly reflect the similarity of the basic event structure— “who did what to whom, when, where and why”— of the input sentence. In fact, a number of large scale meta-evaluations [2, 3] report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work on developing MT evaluation metrics that would outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment [16], however, it is very hard to interpret and indicate what errors the MT systems are making. ULC [17, 18] is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18] but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE [21] is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticate"
2013.iwslt-evaluation.5,W07-0718,0,0.086525,"umber of large scale meta-evaluations [2, 3] report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work on developing MT evaluation metrics that would outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment [16], however, it is very hard to interpret and indicate what errors the MT systems are making. ULC [17, 18] is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18] but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE [21] is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase tables. Like ULC, these me"
2013.iwslt-evaluation.5,W08-0309,0,0.0746276,"umber of large scale meta-evaluations [2, 3] report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work on developing MT evaluation metrics that would outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment [16], however, it is very hard to interpret and indicate what errors the MT systems are making. ULC [17, 18] is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18] but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE [21] is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase tables. Like ULC, these me"
2013.iwslt-evaluation.5,W12-3107,0,0.0975052,"ecent surge of work on developing MT evaluation metrics that would outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment [16], however, it is very hard to interpret and indicate what errors the MT systems are making. ULC [17, 18] is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18] but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE [21] is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase tables. Like ULC, these metrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive"
2013.iwslt-evaluation.5,W12-3103,0,0.067404,"with human adequacy judgment [16], however, it is very hard to interpret and indicate what errors the MT systems are making. ULC [17, 18] is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18] but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE [21] is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase tables. Like ULC, these metrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive in training and tuning due to the large number of parameters that need to be estimated. ROSE [23] is a weighted linear model of shallow linguistic features"
2013.iwslt-evaluation.5,W11-2113,0,0.0119603,"ce needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase tables. Like ULC, these metrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive in training and tuning due to the large number of parameters that need to be estimated. ROSE [23] is a weighted linear model of shallow linguistic features which is cheaper in run time but still contains several dozens of weights that need to be tuned, which makes it hard to port the metric to different domains. TINE [24] is an automatic recall-oriented evaluation metric which aims to preserve the basic event structure. However, it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. In contrast, there is very little work on designing MT evaluation metrics for evaluating Chinese or other languages with ambiguous word boundaries. For instance, stud"
2013.iwslt-evaluation.5,W11-2112,0,0.151131,"ntain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase tables. Like ULC, these metrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive in training and tuning due to the large number of parameters that need to be estimated. ROSE [23] is a weighted linear model of shallow linguistic features which is cheaper in run time but still contains several dozens of weights that need to be tuned, which makes it hard to port the metric to different domains. TINE [24] is an automatic recall-oriented evaluation metric which aims to preserve the basic event structure. However, it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. In contrast, there is very little work on designing MT evaluation metrics for evaluating Chinese or other languages with ambiguous word boundaries. For instance, studies show that simply adapting the commonly used MT evaluation metrics to evaluate Chinese on characterlevel showed a higher correlation with human judgment than the original word-level evaluation metrics [25]. Later, TESLA-CEL"
2013.iwslt-evaluation.5,P11-2028,0,0.018332,"ent domains. TINE [24] is an automatic recall-oriented evaluation metric which aims to preserve the basic event structure. However, it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. In contrast, there is very little work on designing MT evaluation metrics for evaluating Chinese or other languages with ambiguous word boundaries. For instance, studies show that simply adapting the commonly used MT evaluation metrics to evaluate Chinese on characterlevel showed a higher correlation with human judgment than the original word-level evaluation metrics [25]. Later, TESLA-CELAB is introduced as a hybrid character-level and word-level MT evaluation metric for evaluating Chinese [26]. Although TESLA-CELAB correlates significantly better with human judgment for evaluating Chinese than BLEU, no work has been done towards tuning an SMT system for translating into Chinese using it. 2.2. The MEANT family of metrics MEANT [6], which is the weighted f-score over the matched semantic role labels of the automatically aligned semantic frames and role fillers, outperforms BLEU, NIST, METEOR, WER, CDER and TER in correlating with human adequacy judgment. MEANT"
2013.iwslt-evaluation.5,P12-1097,0,0.0126949,"ever, it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. In contrast, there is very little work on designing MT evaluation metrics for evaluating Chinese or other languages with ambiguous word boundaries. For instance, studies show that simply adapting the commonly used MT evaluation metrics to evaluate Chinese on characterlevel showed a higher correlation with human judgment than the original word-level evaluation metrics [25]. Later, TESLA-CELAB is introduced as a hybrid character-level and word-level MT evaluation metric for evaluating Chinese [26]. Although TESLA-CELAB correlates significantly better with human judgment for evaluating Chinese than BLEU, no work has been done towards tuning an SMT system for translating into Chinese using it. 2.2. The MEANT family of metrics MEANT [6], which is the weighted f-score over the matched semantic role labels of the automatically aligned semantic frames and role fillers, outperforms BLEU, NIST, METEOR, WER, CDER and TER in correlating with human adequacy judgment. MEANT is easily portable to other languages requiring only an automatic semantic parser and a large monolingual corpus in the outpu"
2013.iwslt-evaluation.5,D11-1035,0,0.0282041,"labels in MEANT as defined in [27]. For MEANT, wpred and wj are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments [4]. For UMEANT, wpred and wj are estimated in an unsupervised manner using relative frequency of each semantic role label in the reference translations. UMEANT can thus be used when when human judgments on adequacy of the development set are unavailable [5]. 2.3. Tuning against better evaluation metrics Previous works show that tuning MT system against better evaluation metrics improve the translation quality [28, 29]. Recent studies [13, 14] also shows that tuning MT system against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. Therefore, we believe that tuning MT systems against MEANT would improve the adequacy on Chinese MT output. 3. Experimental setup In this section, we describe the details of our systems for the English-Chinese and Chinese-English TED talk MT tasks in terms of data, preprocesing, SMT pipeline and MEANT settings. 3."
2013.iwslt-evaluation.5,P03-1021,0,0.0739366,"was used as a part of development set other than the officially released development set. In order to test the consistency of the experimental results the test sets of IWSLT 2011 and 2012 were used in addition to the IWSLT 2013 test set. We perform minimal preprocessing on the training data running a maximum entropy Chinese segmenter [30] along with numex/timex segmenter on the Chinese data and punctuation tokenization and true casing on the English data. 3.2. SMT pipeline With the goal of improving MT utility by using MEANT as an objective function to drive minimum error rate training (MERT) [31] of state-of-the-art MT systems, we setup our baseline using Moses [32], an off-the-shelf translation toolkit. In this paper we have two baselines: a flat phrase-based MT and a hierarchical phrase-based MT [33]. This allows us to use Moses to compare the performance of MEANT-tuned systems in these two different MT paradigms. The language models are trained using the SRI language model toolkit [34]. For both translation tasks, we used a 6-gram language model. We use ZMERT [35] to tune the baseline because it is a widely used, highly competitive, robust, and reliable implementation of MERT that"
2013.iwslt-evaluation.5,P07-2045,0,0.0071028,"d development set. In order to test the consistency of the experimental results the test sets of IWSLT 2011 and 2012 were used in addition to the IWSLT 2013 test set. We perform minimal preprocessing on the training data running a maximum entropy Chinese segmenter [30] along with numex/timex segmenter on the Chinese data and punctuation tokenization and true casing on the English data. 3.2. SMT pipeline With the goal of improving MT utility by using MEANT as an objective function to drive minimum error rate training (MERT) [31] of state-of-the-art MT systems, we setup our baseline using Moses [32], an off-the-shelf translation toolkit. In this paper we have two baselines: a flat phrase-based MT and a hierarchical phrase-based MT [33]. This allows us to use Moses to compare the performance of MEANT-tuned systems in these two different MT paradigms. The language models are trained using the SRI language model toolkit [34]. For both translation tasks, we used a 6-gram language model. We use ZMERT [35] to tune the baseline because it is a widely used, highly competitive, robust, and reliable implementation of MERT that is also fully configurable and extensible with regard to incorporating"
2013.iwslt-evaluation.5,J07-2003,0,0.129562,"to the IWSLT 2013 test set. We perform minimal preprocessing on the training data running a maximum entropy Chinese segmenter [30] along with numex/timex segmenter on the Chinese data and punctuation tokenization and true casing on the English data. 3.2. SMT pipeline With the goal of improving MT utility by using MEANT as an objective function to drive minimum error rate training (MERT) [31] of state-of-the-art MT systems, we setup our baseline using Moses [32], an off-the-shelf translation toolkit. In this paper we have two baselines: a flat phrase-based MT and a hierarchical phrase-based MT [33]. This allows us to use Moses to compare the performance of MEANT-tuned systems in these two different MT paradigms. The language models are trained using the SRI language model toolkit [34]. For both translation tasks, we used a 6-gram language model. We use ZMERT [35] to tune the baseline because it is a widely used, highly competitive, robust, and reliable implementation of MERT that is also fully configurable and extensible with regard to incorporating new evaluation metrics. 3.3. MEANT for evaluating Chinese Since UMEANT is shown to be more stable when evaluating translations across diffe"
2013.iwslt-evaluation.5,W13-2254,1,0.365957,"n these two different MT paradigms. The language models are trained using the SRI language model toolkit [34]. For both translation tasks, we used a 6-gram language model. We use ZMERT [35] to tune the baseline because it is a widely used, highly competitive, robust, and reliable implementation of MERT that is also fully configurable and extensible with regard to incorporating new evaluation metrics. 3.3. MEANT for evaluating Chinese Since UMEANT is shown to be more stable when evaluating translations across different language pairs [36], we use a UMEANT framework along the lines described in [37] for evaluating both English and Chinese. However, for evaluating Chinese, MEANT has to be equipped with a Chinese shallow semantic parser in order to capture the semantic frames in the Chinese translation output. For this purpose, we used C-ASSERT [38] because of its high accuracy. Since the primary objective in this experiment is studying the feasibility of tuning MT systems against Chinese MEANT, we limited ourselves to using a window-size3 context vector model trained on the word segmented monolingual Chinese gigaword corpus, for estimating the phrasal similarity of the semantic role fille"
2013.iwslt-evaluation.5,E06-1032,0,\N,Missing
2013.iwslt-evaluation.5,W12-3102,0,\N,Missing
2013.iwslt-evaluation.5,W06-3114,0,\N,Missing
2013.iwslt-evaluation.5,P13-2067,1,\N,Missing
2013.iwslt-evaluation.5,W12-3104,0,\N,Missing
2013.iwslt-papers.15,W13-0806,1,0.512843,"of the hypotheses while (d) uses them to further generate longer transduction rules. For convenience, our implementation breaks this into two stages: one that generates a large set of short transduction rule hypotheses, and another that iteratively segments long transduction rules (initialized from the sentence pairs in the training data) by trying to reuse a minimal subset of the hypotheses while chipping away at the long sentence pair rules until the conditional description length is minimized. 2. Background Description length has been used before to drive iterative segmenting ITG learning [1]. We will use their algorithm as our baseline, but the simple mixture model we used then works poorly with our ITG with categories. Instead, we propose a tighter incorporation, where the rule segmenting learning is biased towards rules that are present in the categorized ITG. We refer to this objective as minimizing conditional description length, since technically, the length of the ITG being segmented is conditioned on the categorized ITG. Conditional description length (CDL) is detailed in Section 3. The minimum CDL (MCDL) objective differs from the simple mixture model in that it separates"
2013.iwslt-papers.15,W07-0403,0,0.0703722,"Missing"
2013.iwslt-papers.15,P08-1012,0,0.0217528,"p into the vast engineering efforts that have gone into perfecting existing decoders, it also prevents you from surpassing them in the long run. The motivation for our present series of experiments is that, as a field we are well served by tackling the fundamental questions as well, and not exclusively focusing on engineering short term incremental BLEU score boosts where the quality of an induced ITG itself is obscured because it is embedded within many other heuristic algorithms. When the structure of an ITG is induced without supervision, it is possible to get an effect that resembles MDL. [3] impose a sparsity prior over the rule probabilities to prevent the search from having to consider all the rules found in the Viterbi biparses. [4, 5, 8, 13, 14] use Gibbs sampling to learn ITGs with priors over the rule structures that serve a similar purpose to the model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing exte"
2013.iwslt-papers.15,P09-1088,0,0.0161427,"e motivation for our present series of experiments is that, as a field we are well served by tackling the fundamental questions as well, and not exclusively focusing on engineering short term incremental BLEU score boosts where the quality of an induced ITG itself is obscured because it is embedded within many other heuristic algorithms. When the structure of an ITG is induced without supervision, it is possible to get an effect that resembles MDL. [3] impose a sparsity prior over the rule probabilities to prevent the search from having to consider all the rules found in the Viterbi biparses. [4, 5, 8, 13, 14] use Gibbs sampling to learn ITGs with priors over the rule structures that serve a similar purpose to the model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints [15]. Although this constitutes a way to borrow nonterminal categories that help the translation model, it complicates the learning process by a"
2013.iwslt-papers.15,P09-1104,0,0.0524446,"Missing"
2013.iwslt-papers.15,W09-2304,1,0.905197,"Missing"
2013.iwslt-papers.15,N10-1028,0,0.0131358,"e motivation for our present series of experiments is that, as a field we are well served by tackling the fundamental questions as well, and not exclusively focusing on engineering short term incremental BLEU score boosts where the quality of an induced ITG itself is obscured because it is embedded within many other heuristic algorithms. When the structure of an ITG is induced without supervision, it is possible to get an effect that resembles MDL. [3] impose a sparsity prior over the rule probabilities to prevent the search from having to consider all the rules found in the Viterbi biparses. [4, 5, 8, 13, 14] use Gibbs sampling to learn ITGs with priors over the rule structures that serve a similar purpose to the model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints [15]. Although this constitutes a way to borrow nonterminal categories that help the translation model, it complicates the learning process by a"
2013.iwslt-papers.15,N10-1015,0,0.0263712,"Missing"
2013.iwslt-papers.15,P10-1017,0,0.0438788,"Missing"
2013.iwslt-papers.15,N10-1050,1,0.865646,"Missing"
2013.iwslt-papers.15,2011.eamt-1.42,1,0.870705,"Missing"
2013.iwslt-papers.15,P11-1064,0,0.0126129,"e motivation for our present series of experiments is that, as a field we are well served by tackling the fundamental questions as well, and not exclusively focusing on engineering short term incremental BLEU score boosts where the quality of an induced ITG itself is obscured because it is embedded within many other heuristic algorithms. When the structure of an ITG is induced without supervision, it is possible to get an effect that resembles MDL. [3] impose a sparsity prior over the rule probabilities to prevent the search from having to consider all the rules found in the Viterbi biparses. [4, 5, 8, 13, 14] use Gibbs sampling to learn ITGs with priors over the rule structures that serve a similar purpose to the model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints [15]. Although this constitutes a way to borrow nonterminal categories that help the translation model, it complicates the learning process by a"
2013.iwslt-papers.15,P12-1018,0,0.0131275,"e motivation for our present series of experiments is that, as a field we are well served by tackling the fundamental questions as well, and not exclusively focusing on engineering short term incremental BLEU score boosts where the quality of an induced ITG itself is obscured because it is embedded within many other heuristic algorithms. When the structure of an ITG is induced without supervision, it is possible to get an effect that resembles MDL. [3] impose a sparsity prior over the rule probabilities to prevent the search from having to consider all the rules found in the Viterbi biparses. [4, 5, 8, 13, 14] use Gibbs sampling to learn ITGs with priors over the rule structures that serve a similar purpose to the model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints [15]. Although this constitutes a way to borrow nonterminal categories that help the translation model, it complicates the learning process by a"
2013.iwslt-papers.15,P06-1121,0,0.0416765,"y prior over the rule probabilities to prevent the search from having to consider all the rules found in the Viterbi biparses. [4, 5, 8, 13, 14] use Gibbs sampling to learn ITGs with priors over the rule structures that serve a similar purpose to the model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints [15]. Although this constitutes a way to borrow nonterminal categories that help the translation model, it complicates the learning process by adding external constraints that are bound to match the translation model poorly. 3. Conditional description length Conditional description length (CDL) is a general method for evaluating a model and a dataset given a preexisting model. This makes it ideal for augmenting an existing model with a variant model of the same family. In this paper we will apply this to augment an existing inversion transduction grammar (ITG) with rules that are found with a diff"
2013.iwslt-papers.15,C12-1142,1,0.888267,"( ) DL D|Φ′ , Ψ − DL (D|Φ, Ψ) ( ) = −lg P D|Φ′ , Ψ − −lg P (D|Φ, Ψ) = −lg P (D|Φ′ , Ψ) P (D|Φ, Ψ) 4. Generating rule hypotheses In the first stage of our learning approach, we generate a large set of possible rules, from which the second stage will choose a small subset to keep. The goal of this stage is to keep the recall high with respect to a theoretical “optimal ITG”, precision is achieved in the second stage. We rely on chunking and category splitting to generate this large set of rule hypotheses. To generate these high-recall ITGs, we will follow the bootstrapping approach presented in [19], and start with a finite-state transduction grammar (FSTG), do the chunking and category splitting within the FSTG framework before transferring the resulting grammar to a corresponding ITG. This is likely to produce an ITG that performs poorly on its own, but may be informative in the second stage. 5. Segmenting rules In the second stage of our learning approach, we segment rules explicitly representing the entire training data, into smaller—more general—rules, reusing rules from the first stage whenever we can. By driving the segmentation-based learning with a minimum description length obj"
2013.iwslt-papers.15,W09-3804,1,0.854975,"ys of using these two stages to arrive at a final ITG, and how we intend to evaluate the quality of those ITGs. For the first stage, we will use the technique described in [19] to start with a finite-state transduction grammar (FSTG) and perform chunking before splitting the nonterminal categories and moving the FSTG into ITG form. We perform one round of chunking, and two rounds of category splitting (resulting in 4 nonterminals and 4 preterminals, which becomes 8 nonterminals in the ITG form). At each stage, we run a few iterations of expectation maximization using the algorithm detailed in [20] for biparsing. For comparison we also bootstrap a comparable ITG that has not had the categories split. Before using either of the bootstrapped ITGs, we eliminate all rules that do not have a probability above a threshold that we fixed to 10−50 . This eliminates the highly unlikely rules from the ITG. For the second stage, we use the iterative rule segmentation learning algorithm driven by minimum conditional description length that we introduced in Section 5. We will try three different variants on this algorithm: one without an ITG to condition on, one conditioned on the chunked ITG, and on"
2013.iwslt-papers.15,J07-2003,0,0.0344983,"learned ITG. All the above outlined ITGs are trained using the IWSLT07 Chinese–English data set [21], which contains 46,867 sentence pairs of training data, and 489 Chinese sentences with 6 English reference translations each as test data; all the sentences are taken from the traveling domain. Since the Chinese is written without whitespace, we use a tool that tries to clump characters together into more “word like” sequences [22]. To test the learned ITGs, we use them as translation systems with our in-house ITG decoder. The decoder uses a CKY-style parsing algorithm [23–25] and cube pruning [26] to integrate the language model scores. For language model, we use a trigram language model trained with the SRILM toolkit [27] on the English side of the training corpus. To evaluate the resulting translations, we use BLEU [28] and NIST [29]. 7. Results In this section we present the empirical results: bilingual categories help translation quality under the experimental conditions detailed in the previous section. The results are summarized in Table 1. As predicted the base chunked only ITG fares poorly, while the categories help a great deal in the chunked w/categories only ITG—though the s"
2013.iwslt-papers.15,P02-1040,0,0.0878297,"test data; all the sentences are taken from the traveling domain. Since the Chinese is written without whitespace, we use a tool that tries to clump characters together into more “word like” sequences [22]. To test the learned ITGs, we use them as translation systems with our in-house ITG decoder. The decoder uses a CKY-style parsing algorithm [23–25] and cube pruning [26] to integrate the language model scores. For language model, we use a trigram language model trained with the SRILM toolkit [27] on the English side of the training corpus. To evaluate the resulting translations, we use BLEU [28] and NIST [29]. 7. Results In this section we present the empirical results: bilingual categories help translation quality under the experimental conditions detailed in the previous section. The results are summarized in Table 1. As predicted the base chunked only ITG fares poorly, while the categories help a great deal in the chunked w/categories only ITG—though the scores are not very reliable when in this low range. The trade-off between model and data size during segmentation conditioned on the ITG with categories is illustrated in Figure 1. It starts out with most of the total description"
2013.mtsummit-papers.12,W11-2136,0,0.0199318,"difficulty of semantic parsing. Below, we describe some of the attempts to (a) improve informal text translation quality using domain adaptation techniques and (b) incorporate semantic role labeling information into the SMT pipeline and present a brief survey of evaluation metrics that focus on rewarding semantically valid translations. 2.1 Semantics in SMT There is a rising trend of work aimed at incorporating semantics into various stages of the SMT pipeline, for example, preprocessing the input (Komachi et al., 2006; Wu et al., 2011), training treeto-string MT models (Liu and Gildea, 2010; Aziz et al., 2011), training reordering model (Xiong et al., 2012) and reordering the output in the postprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as disfluencies, incomplete sentences and misspellings; and (2) only small volu"
2013.mtsummit-papers.12,W05-0909,0,0.40075,"of the discourse, through contextual modelling. Again, all these approaches are orthogonal to our approach of incorporating semantics into SMT by tuning against a semantic objective function and any of the 94 above models could potentially benefit from tuning with semantic metrics. 2.3 MT evaluation metrics Lo et al. (2013) showed that tuning against BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) does not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translati"
2013.mtsummit-papers.12,2011.mtsummit-papers.32,0,0.0267266,"Missing"
2013.mtsummit-papers.12,E06-1032,0,0.321911,"Missing"
2013.mtsummit-papers.12,W07-0718,0,0.310907,"ch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008). Lambert et al. (2006) tuned on QUEEN, a simplified version of ULC, that discards the semantic features of ULC and bases on pure lexical similarity. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to a semantic objective function. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but only performs"
2013.mtsummit-papers.12,W08-0309,0,0.535531,"Missing"
2013.mtsummit-papers.12,N10-1080,0,0.0197458,"ion metrics. In this experiment, we use a MEANT implementation along the lines described in Lo et al. (2012) and Tumuluru et al. (2012) but we incorporate a variant of the aggregation function proposed in Mihalcea et al. (2006) for phrasal similarity of role fillers because it normalizes the phrase length better than geometric mean. 4 Results Of course, tuning against any metric would maximize the performance of the SMT system on that particular metric; it would be overfitting. In the following, we avoid comparing on metrics too similar to the one that the system was tuned on. This is because Cer et al. (2010) showed that tuning on METEOR ↑ 34.63 30.85 33.08 WER ↓ 80.09 76.15 77.32 CDER ↓ 64.54 57.96 61.01 TER ↓ 76.12 74.73 74.64 MEANT ↑ 17.11 15.39 17.27 METEOR, TER and their variations would do well on metrics similar to what they were tuned on but perform particularly poorly on the other metrics. Therefore, it is less meaningful to evaluate a system on metrics similar to what they were tuned on. A far more worthwhile goal would be to bias the SMT system to produce adequate translations while achieving the best scores across all the metrics. In addition, we believe a good translation is one from"
2013.mtsummit-papers.12,W07-0738,0,0.55035,"es not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008). Lambert et al. (2006) tuned on QUEEN, a simplified version of ULC, that discards the semantic features of ULC and bases on pure lexical similarity. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any seman"
2013.mtsummit-papers.12,W08-0332,0,0.497374,"ailing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008). Lambert et al. (2006) tuned on QUEEN, a simplified version of ULC, that discards the semantic features of ULC and bases on pure lexical similarity. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to a semantic objective function. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but only performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment"
2013.mtsummit-papers.12,W06-3114,0,0.488604,"Missing"
2013.mtsummit-papers.12,2006.iwslt-evaluation.11,0,0.163443,"ility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008). Lambert et al. (2006) tuned on QUEEN, a simplified version of ULC, that discards the semantic features of ULC and bases on pure lexical similarity. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to a semantic objective function. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but only performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. In contrast, Lo et al"
2013.mtsummit-papers.12,E06-1031,0,0.570378,"tual modelling. Again, all these approaches are orthogonal to our approach of incorporating semantics into SMT by tuning against a semantic objective function and any of the 94 above models could potentially benefit from tuning with semantic metrics. 2.3 MT evaluation metrics Lo et al. (2013) showed that tuning against BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) does not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translation quality (Callison-Burch e"
2013.mtsummit-papers.12,C10-1081,0,0.015891,"ormal text due to the difficulty of semantic parsing. Below, we describe some of the attempts to (a) improve informal text translation quality using domain adaptation techniques and (b) incorporate semantic role labeling information into the SMT pipeline and present a brief survey of evaluation metrics that focus on rewarding semantically valid translations. 2.1 Semantics in SMT There is a rising trend of work aimed at incorporating semantics into various stages of the SMT pipeline, for example, preprocessing the input (Komachi et al., 2006; Wu et al., 2011), training treeto-string MT models (Liu and Gildea, 2010; Aziz et al., 2011), training reordering model (Xiong et al., 2012) and reordering the output in the postprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as disfluencies, incomplete sentences and misspellings; and"
2013.mtsummit-papers.12,C10-1079,0,0.0625948,"Missing"
2013.mtsummit-papers.12,P11-1023,1,0.86082,"where mi and ri are the weights for frame i that estimate the degree of contribution of the frame to the overall meaning of the sentence in the MT/REF respectively. Mi,j and Ri,j are the total counts of argument of type j in frame i in the MT and REF respectively. Si,pred and Si,j are the lexical/phrasal similarities of the predicates and role fillers of the arguments of type j between the MT and REF. wpred and wj are the weights of the the predicates and role fillers of the arguments of type j in the MT and REF. There are 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b) and they are determined by optimizing the correlation with human adequacy judgments using grid search (Lo and Wu, 2011a). 3 Tuning SMT against MEANT We now show that using MEANT as an objective function to drive minimum error rate training (MERT) of state-of-the-art MT systems improves MT utility in the informal genres. Aiming at improving SMT adequacy of informal genres, we set up two experiments on public speech TED talk data and web forum data. The TED talk MT system is trained on the IWSLT2012 ChineseEnglish parallel TED talk training data consisting of over 130k sentences pairs. The de"
2013.mtsummit-papers.12,W12-3129,1,0.849085,"Missing"
2013.mtsummit-papers.12,P13-2067,1,0.225645,"hereas He and Deng (2011) proposed to classify the training data into in-domain or out-of-domain for training two independent translation model and then combine the two models using a system combination approach. Mei and Kirchhoff (2010) incorporated document-level semantics, such as topic of the discourse, through contextual modelling. Again, all these approaches are orthogonal to our approach of incorporating semantics into SMT by tuning against a semantic objective function and any of the 94 above models could potentially benefit from tuning with semantic metrics. 2.3 MT evaluation metrics Lo et al. (2013) showed that tuning against BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) does not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more l"
2013.mtsummit-papers.12,C10-1138,0,0.0226934,"ering the output in the postprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as disfluencies, incomplete sentences and misspellings; and (2) only small volumes of high-quality parallel training data are available (Mei and Kirchhoff, 2010). Rao et al. (2007) and Wang et al. (2010) proposed to remove disfluency in preprocessing stage; Bertoldi et al. (2010) introduced a model to recover the misspelled words before translation; Banerjee et al. (2011) addressed the data sparsity problem by mixing data from comparable domain into the training of both the translation model and the language model whereas He and Deng (2011) proposed to classify the training data into in-domain or out-of-domain for training two independent translation model and then combine the two models using a system combination approach. Mei and Kirchhoff (2010) in"
2013.mtsummit-papers.12,niessen-etal-2000-evaluation,0,0.849474,"se approaches are orthogonal to our approach of incorporating semantics into SMT by tuning against a semantic objective function and any of the 94 above models could potentially benefit from tuning with semantic metrics. 2.3 MT evaluation metrics Lo et al. (2013) showed that tuning against BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) does not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrqu"
2013.mtsummit-papers.12,J05-1004,0,0.0591182,"Missing"
2013.mtsummit-papers.12,P02-1040,0,0.0951864,"training data into in-domain or out-of-domain for training two independent translation model and then combine the two models using a system combination approach. Mei and Kirchhoff (2010) incorporated document-level semantics, such as topic of the discourse, through contextual modelling. Again, all these approaches are orthogonal to our approach of incorporating semantics into SMT by tuning against a semantic objective function and any of the 94 above models could potentially benefit from tuning with semantic metrics. 2.3 MT evaluation metrics Lo et al. (2013) showed that tuning against BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) does not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of"
2013.mtsummit-papers.12,N04-1030,0,0.608726,"ms from different SMT approaches (such as hierarchical, phrase based, or synchronous/transduction grammar based) or those applying other techniques for informal data (such as domain adaptation from formal to informal text, or integration of linguistic features) could also benefit from the semantic information incorporated through our approach. 2 Related work Relatively little work has been done toward addressing the problem of biasing the translation decisions of an SMT system to produce adequate translations for informal text that correctly preserve who did what to whom, when, where and why (Pradhan et al., 2004). There has been a recent surge of work aimed at incorporating semantics into the SMT pipeline; however, none attempts to improve translation quality on informal text due to the difficulty of semantic parsing. Below, we describe some of the attempts to (a) improve informal text translation quality using domain adaptation techniques and (b) incorporate semantic role labeling information into the SMT pipeline and present a brief survey of evaluation metrics that focus on rewarding semantically valid translations. 2.1 Semantics in SMT There is a rising trend of work aimed at incorporating semanti"
2013.mtsummit-papers.12,2007.mtsummit-papers.51,0,0.0212218,"stprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as disfluencies, incomplete sentences and misspellings; and (2) only small volumes of high-quality parallel training data are available (Mei and Kirchhoff, 2010). Rao et al. (2007) and Wang et al. (2010) proposed to remove disfluency in preprocessing stage; Bertoldi et al. (2010) introduced a model to recover the misspelled words before translation; Banerjee et al. (2011) addressed the data sparsity problem by mixing data from comparable domain into the training of both the translation model and the language model whereas He and Deng (2011) proposed to classify the training data into in-domain or out-of-domain for training two independent translation model and then combine the two models using a system combination approach. Mei and Kirchhoff (2010) incorporated document"
2013.mtsummit-papers.12,W11-2112,0,0.232616,"c features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008). Lambert et al. (2006) tuned on QUEEN, a simplified version of ULC, that discards the semantic features of ULC and bases on pure lexical similarity. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to a semantic objective function. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but only performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. In contrast, Lo et al. (2013) show that a MT system tuned against MEANT (Lo et al., 2012) produces more adequate translations in formal news genre as evaluated both quantitatively and qualitatively. Precisely, MEANT is computed as follows: 1. Apply an automatic shallow semantic parser on both the references and MT output. 2. Apply maximum weighted bipartite matching algorithm to align the semantic frames between th"
2013.mtsummit-papers.12,2006.amta-papers.25,0,0.274257,"r out-of-domain for training two independent translation model and then combine the two models using a system combination approach. Mei and Kirchhoff (2010) incorporated document-level semantics, such as topic of the discourse, through contextual modelling. Again, all these approaches are orthogonal to our approach of incorporating semantics into SMT by tuning against a semantic objective function and any of the 94 above models could potentially benefit from tuning with semantic metrics. 2.3 MT evaluation metrics Lo et al. (2013) showed that tuning against BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) does not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. UL"
2013.mtsummit-papers.12,N09-2004,1,0.841443,"sing domain adaptation techniques and (b) incorporate semantic role labeling information into the SMT pipeline and present a brief survey of evaluation metrics that focus on rewarding semantically valid translations. 2.1 Semantics in SMT There is a rising trend of work aimed at incorporating semantics into various stages of the SMT pipeline, for example, preprocessing the input (Komachi et al., 2006; Wu et al., 2011), training treeto-string MT models (Liu and Gildea, 2010; Aziz et al., 2011), training reordering model (Xiong et al., 2012) and reordering the output in the postprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as disfluencies, incomplete sentences and misspellings; and (2) only small volumes of high-quality parallel training data are available (Mei and Kirchhoff, 2010). Rao et al. (2007) and Wang et al. (201"
2013.mtsummit-papers.12,I11-1004,0,0.0126835,"none attempts to improve translation quality on informal text due to the difficulty of semantic parsing. Below, we describe some of the attempts to (a) improve informal text translation quality using domain adaptation techniques and (b) incorporate semantic role labeling information into the SMT pipeline and present a brief survey of evaluation metrics that focus on rewarding semantically valid translations. 2.1 Semantics in SMT There is a rising trend of work aimed at incorporating semantics into various stages of the SMT pipeline, for example, preprocessing the input (Komachi et al., 2006; Wu et al., 2011), training treeto-string MT models (Liu and Gildea, 2010; Aziz et al., 2011), training reordering model (Xiong et al., 2012) and reordering the output in the postprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as"
2013.mtsummit-papers.12,P12-1095,0,0.0121004,"ibe some of the attempts to (a) improve informal text translation quality using domain adaptation techniques and (b) incorporate semantic role labeling information into the SMT pipeline and present a brief survey of evaluation metrics that focus on rewarding semantically valid translations. 2.1 Semantics in SMT There is a rising trend of work aimed at incorporating semantics into various stages of the SMT pipeline, for example, preprocessing the input (Komachi et al., 2006; Wu et al., 2011), training treeto-string MT models (Liu and Gildea, 2010; Aziz et al., 2011), training reordering model (Xiong et al., 2012) and reordering the output in the postprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as disfluencies, incomplete sentences and misspellings; and (2) only small volumes of high-quality parallel training data are a"
2013.mtsummit-papers.14,C12-1142,1,0.568939,"ces involving alignments which are not purely monotonic. We trade-off some of the initial fluency that segmental phrase-based models offer for the flexibility offered by the token based models. We induce the bracketing ITG on the corpus generated from the previous stage to identify the word associations between rhyming lines. Expectation maximization (Dempster et al., 1977) is used to estimate the model parameters for the bracketing grammar. As the corpora are fairly large, beam pruning is used to make the training faster. Further details of the transduction grammar induction can be found in (Saers et al., 2012; Saers and Wu, 2011). 3.3 Decoding: Challenge-response algorithm We translate the challenge into responses using our in-house ITG decoder Wu (1996); Wu and Wong (1998) for the task of decoding. The decoder builds the parse forest using a CKY-style parsing algorithm which is represented in an efficient hypergraph structure. The translation hypotheses are scored using the transduction grammar and the language model efficiently using cube pruning (Chiang, 2007) In our decoding algorithm, we allow only straight rules as we want to produce responses with the same rhyming order as the challenge. In"
2013.mtsummit-papers.14,W09-2006,0,0.240592,"ng responses to hip hop challenge lyrics. An SMT system was used in conjunction with stress patterns and rhymes found in a pronunciation dictionary to produce translations of poems by Genzel et al. (2010). However, it was challenging to produce translations of full verses that satisfied all the constraints enforced by classical poetry. For example, the translations could not comply with the desired meter of the line although the rhyming constraints were satisfied. These results indicate the difficulty of producing quality output via automatic methods even for very structured domains. A et al. (2009) proposed a model for automatically generating Tamil lyrics given a melody. The lyrics were represented as a sequence of labels using the KNM system where K, N and M represented the long vowels, short vowels and consonants respectively. They solved the sequence labeling problem of generating the lyrics given a melody using conditional random fields. Others have attempted to identify word-to-word relationships, stress patterns (Greene et al., 2010) and rhyming words (Reddy and Knight, 2011), mostly in the domain of poetry. Greene et al. (2010) used an FST to assign stress patterns to words give"
2013.mtsummit-papers.14,J07-2003,0,0.155234,"rly large, beam pruning is used to make the training faster. Further details of the transduction grammar induction can be found in (Saers et al., 2012; Saers and Wu, 2011). 3.3 Decoding: Challenge-response algorithm We translate the challenge into responses using our in-house ITG decoder Wu (1996); Wu and Wong (1998) for the task of decoding. The decoder builds the parse forest using a CKY-style parsing algorithm which is represented in an efficient hypergraph structure. The translation hypotheses are scored using the transduction grammar and the language model efficiently using cube pruning (Chiang, 2007) In our decoding algorithm, we allow only straight rules as we want to produce responses with the same rhyming order as the challenge. Interleaved rhyming order is harder to evaluate without the larger context of the song and we do not address that problem in our current model. Singleton rules are penalized, as successive lines in a stanza are typically of similar length. Lastly, we add a penalty to reflexive translation rules that map the same surface form to itself such as A → ”yo”/”yo”. We observed that such rules have a high probability in our learned grammar due to presence of sentence pa"
2013.mtsummit-papers.14,D10-1051,0,0.731321,"re discussed in Section 2. Sections 3 and 4 contain our system description and experimental setup respectively. Results and conclusions are presented in Sections 5 and 6. 2 Related work Although a handful of previous approaches applied SMT models and other statistical learning methods have been to unconventional domains in the past, ours is the first known work on the domain of hip hop lyrics. Most of the past work in this vein can be classified into two categories. In the first category some form of prior linguistic knowledge about the domain, such as pronunciation dictionaries Genzel et al. (2010) or phonological or morphological information is used to bootstrap the learning. While the second category uses unsupervised learning methods to identify word association probabilities, appropriate bias is provided by the inherent constraints in the domain such as a set number of words in a line (in Chinese couplets), or a set meter (in classical poetry). In this work, we present a completely unsupervised model on a domain that inherently has very few such constraints. As previously mentioned, hip hop lyrics unlike poems (especially in classical poetry where, for example, an octave has exactly"
2013.mtsummit-papers.14,P09-1104,0,0.0357094,"T model. Each selected pair generates two training instances: a challenge-response and a responsechallenge pair as the source and target languages are identical. 3.2 Unsupervised learning: Stochastic transduction grammar induction We choose to induce a token based inversion transduction grammar (ITG) model (Wu, 1997, 1995a,b) because of its expressiveness and the empirical evidence for its representational capacity across a wide spectrum of natural language tasks including textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003; Haghighi et al., 2009). We restrict the ITG to a bracketing ITG (BITG) and use it as the translation model for our SMT system as the focus of our model is to learn the token level correspondences in order to identify potential rhyming candidates. We chose an ITG as opposed to a monotonic finitestate transduction grammar model in order to be able to learn token level correspondences involving alignments which are not purely monotonic. We trade-off some of the initial fluency that segmental phrase-based models offer for the flexibility offered by the token based models. We induce the bracketing ITG on the corpus gene"
2013.mtsummit-papers.14,C08-1048,0,0.407246,". In this work, we present a completely unsupervised model on a domain that inherently has very few such constraints. As previously mentioned, hip hop lyrics unlike poems (especially in classical poetry where, for example, an octave has exactly 10 syllables per line and 8 lines per stanza) do not require a set number of syllables in a line. Also, not all words in the lyrics are required to be a part of the lexicon. Finally, rhyming is frequently achieved via intonation and assonance making it hard to apply prior phonological constraints. A brief summary of the related work is presented below. Jiang and Zhou (2008) trained a phrase-based SMT system to translate the first line of of a Chinese couplet or duilian into the second. Linguistic constraints were applied to the n best output of the SMT system to select the most suitable next line. 110 However in contrast to Chinese couplets, which adhere to strict rules requiring, for example, an identical number of characters in each line and one-toone correspondence in their metrical length, the domain of hip hop lyrics is far more unstructured and there exists no clear constraint that would ensure fluent and rhyming responses to hip hop challenge lyrics. An S"
2013.mtsummit-papers.14,P07-2045,0,0.00374336,"on lines of hip hop lyrics. A subset of 85 lines was randomly chosen as a test set to provide the hip hop challenge lyrics to the systems. In order to train the rhyme scheme detector, we extracted the end-of-line words and words before all the commas2 from each verse. We obtained a corpus containing 4.2 million tokens corresponding to potential rhyming candidates (with around 153,000 unique token types). 4.2 Phrase-based SMT baseline In order to evaluate the performance of standard SMT alignment and search strategies on this novel “translation” task, we also trained a standard Moses baseline (Koehn et al., 2007) and compared its performance to our transduction grammar based SMT system. A 4-gram language model which was trained on the entire training corpus using SRILM (Stolcke, 2002) was used in decoding both the baseline and our model. Both the baseline and our bracketing ITG model were used to decode a held out test set with a slightly higher language model weight which was empirically chosen using a small development set to produce fluent outputs. The best translation produced by both these SMT systems was used to evaluate their performance at the task of improvising fluent and rhyming responses g"
2013.mtsummit-papers.14,W11-1008,1,0.194579,"ents which are not purely monotonic. We trade-off some of the initial fluency that segmental phrase-based models offer for the flexibility offered by the token based models. We induce the bracketing ITG on the corpus generated from the previous stage to identify the word associations between rhyming lines. Expectation maximization (Dempster et al., 1977) is used to estimate the model parameters for the bracketing grammar. As the corpora are fairly large, beam pruning is used to make the training faster. Further details of the transduction grammar induction can be found in (Saers et al., 2012; Saers and Wu, 2011). 3.3 Decoding: Challenge-response algorithm We translate the challenge into responses using our in-house ITG decoder Wu (1996); Wu and Wong (1998) for the task of decoding. The decoder builds the parse forest using a CKY-style parsing algorithm which is represented in an efficient hypergraph structure. The translation hypotheses are scored using the transduction grammar and the language model efficiently using cube pruning (Chiang, 2007) In our decoding algorithm, we allow only straight rules as we want to produce responses with the same rhyming order as the challenge. Interleaved rhyming ord"
2013.mtsummit-papers.14,P95-1033,1,0.688319,"Missing"
2013.mtsummit-papers.14,J97-3002,1,0.321446,"e then segment each verse into stanzas with their respective rhyme schemes according to the Viterbi parse of the model. We then select the lines from each stanza that rhyme with each other according to its rhyme scheme and add them as training instances for our transduction grammar based SMT model. Each selected pair generates two training instances: a challenge-response and a responsechallenge pair as the source and target languages are identical. 3.2 Unsupervised learning: Stochastic transduction grammar induction We choose to induce a token based inversion transduction grammar (ITG) model (Wu, 1997, 1995a,b) because of its expressiveness and the empirical evidence for its representational capacity across a wide spectrum of natural language tasks including textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003; Haghighi et al., 2009). We restrict the ITG to a bracketing ITG (BITG) and use it as the translation model for our SMT system as the focus of our model is to learn the token level correspondences in order to identify potential rhyming candidates. We chose an ITG as opposed to a monotonic finitestate transduction gra"
2013.mtsummit-papers.14,I05-1023,1,0.779954,"them as training instances for our transduction grammar based SMT model. Each selected pair generates two training instances: a challenge-response and a responsechallenge pair as the source and target languages are identical. 3.2 Unsupervised learning: Stochastic transduction grammar induction We choose to induce a token based inversion transduction grammar (ITG) model (Wu, 1997, 1995a,b) because of its expressiveness and the empirical evidence for its representational capacity across a wide spectrum of natural language tasks including textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003; Haghighi et al., 2009). We restrict the ITG to a bracketing ITG (BITG) and use it as the translation model for our SMT system as the focus of our model is to learn the token level correspondences in order to identify potential rhyming candidates. We chose an ITG as opposed to a monotonic finitestate transduction grammar model in order to be able to learn token level correspondences involving alignments which are not purely monotonic. We trade-off some of the initial fluency that segmental phrase-based models offer for the flexibility offered by the"
2013.mtsummit-papers.14,P98-2230,1,0.651793,"y the token based models. We induce the bracketing ITG on the corpus generated from the previous stage to identify the word associations between rhyming lines. Expectation maximization (Dempster et al., 1977) is used to estimate the model parameters for the bracketing grammar. As the corpora are fairly large, beam pruning is used to make the training faster. Further details of the transduction grammar induction can be found in (Saers et al., 2012; Saers and Wu, 2011). 3.3 Decoding: Challenge-response algorithm We translate the challenge into responses using our in-house ITG decoder Wu (1996); Wu and Wong (1998) for the task of decoding. The decoder builds the parse forest using a CKY-style parsing algorithm which is represented in an efficient hypergraph structure. The translation hypotheses are scored using the transduction grammar and the language model efficiently using cube pruning (Chiang, 2007) In our decoding algorithm, we allow only straight rules as we want to produce responses with the same rhyming order as the challenge. Interleaved rhyming order is harder to evaluate without the larger context of the song and we do not address that problem in our current model. Singleton rules are penali"
2014.iwslt-evaluation.4,N04-1030,0,0.357453,"s similar lexical n-grams as the reference translation. Although such metrics tend to enforce fluency, it has been shown that these metrics generally do not emphasize meaning preservation, and thus are weak at enforcing translation adequacy (Callison-Burch et al. [7]; Koehn and Monz [8]). Unlike BLEU, or other n-gram based metrics, the MEANT family of metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly used MT evaluation metrics such as BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14]. 34 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2. Related work Surface-form oriented metrics like BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and T"
2014.iwslt-evaluation.4,W05-0909,0,0.2511,"s, the MEANT family of metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly used MT evaluation metrics such as BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14]. 34 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2. Related work Surface-form oriented metrics like BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14] do not correctly reflect the meaning similarities of the basic event structure “who did what to whom, when, where and why” of the input sentence. In fact, many studies (Callison-Bursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of w"
2014.iwslt-evaluation.4,W14-4003,1,0.71406,"gments, resulting also in translation performance gains compared to the system tuned against our previous version of MEANT from the IWSLT 2013 evaluation campaign (Lo et al. [4]). This improved variant of MEANT uses f-score to aggregate lexical similarities within role filler phrases instead of linear average. We also introduced several changes to last year’s baseline, including improved Chinese word segmentation, improved Chinese named entity recognition combined with dedicated proper name translation, and number expression handling. We also experimented with tuning against IMEANT (Wu et al. [5]), a new inversion transduction grammar (ITG) version of MEANT, that was shown this year to correlate with human adequacy judgements more closely than MEANT. Despite this fact, we observed that tuning to IMEANT is statistically indistinguishable from tuning to MEANT.In the past few years, MT research has mainly focused on evaluation using fast and cheap ngram based MT evaluation metrics such as BLEU [6] which assume that a good translation is one that has similar lexical n-grams as the reference translation. Although such metrics tend to enforce fluency, it has been shown that these metrics ge"
2014.iwslt-evaluation.4,E06-1031,0,0.383768,"T family of metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly used MT evaluation metrics such as BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14]. 34 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2. Related work Surface-form oriented metrics like BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14] do not correctly reflect the meaning similarities of the basic event structure “who did what to whom, when, where and why” of the input sentence. In fact, many studies (Callison-Bursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of work on deve"
2014.iwslt-evaluation.4,P02-1040,0,0.0900717,"gmentation, improved Chinese named entity recognition combined with dedicated proper name translation, and number expression handling. We also experimented with tuning against IMEANT (Wu et al. [5]), a new inversion transduction grammar (ITG) version of MEANT, that was shown this year to correlate with human adequacy judgements more closely than MEANT. Despite this fact, we observed that tuning to IMEANT is statistically indistinguishable from tuning to MEANT.In the past few years, MT research has mainly focused on evaluation using fast and cheap ngram based MT evaluation metrics such as BLEU [6] which assume that a good translation is one that has similar lexical n-grams as the reference translation. Although such metrics tend to enforce fluency, it has been shown that these metrics generally do not emphasize meaning preservation, and thus are weak at enforcing translation adequacy (Callison-Burch et al. [7]; Koehn and Monz [8]). Unlike BLEU, or other n-gram based metrics, the MEANT family of metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did"
2014.iwslt-evaluation.4,niessen-etal-2000-evaluation,0,0.401668,"f metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly used MT evaluation metrics such as BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14]. 34 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2. Related work Surface-form oriented metrics like BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14] do not correctly reflect the meaning similarities of the basic event structure “who did what to whom, when, where and why” of the input sentence. In fact, many studies (Callison-Bursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of work on developing MT"
2014.iwslt-evaluation.4,E06-1032,0,0.0481031,"judgements more closely than MEANT. Despite this fact, we observed that tuning to IMEANT is statistically indistinguishable from tuning to MEANT.In the past few years, MT research has mainly focused on evaluation using fast and cheap ngram based MT evaluation metrics such as BLEU [6] which assume that a good translation is one that has similar lexical n-grams as the reference translation. Although such metrics tend to enforce fluency, it has been shown that these metrics generally do not emphasize meaning preservation, and thus are weak at enforcing translation adequacy (Callison-Burch et al. [7]; Koehn and Monz [8]). Unlike BLEU, or other n-gram based metrics, the MEANT family of metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly"
2014.iwslt-evaluation.4,2006.amta-papers.25,0,0.114241,"t the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly used MT evaluation metrics such as BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14]. 34 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2. Related work Surface-form oriented metrics like BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14] do not correctly reflect the meaning similarities of the basic event structure “who did what to whom, when, where and why” of the input sentence. In fact, many studies (Callison-Bursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of work on developing MT evaluation met"
2014.iwslt-evaluation.4,W06-3114,0,0.0342993,"ely than MEANT. Despite this fact, we observed that tuning to IMEANT is statistically indistinguishable from tuning to MEANT.In the past few years, MT research has mainly focused on evaluation using fast and cheap ngram based MT evaluation metrics such as BLEU [6] which assume that a good translation is one that has similar lexical n-grams as the reference translation. Although such metrics tend to enforce fluency, it has been shown that these metrics generally do not emphasize meaning preservation, and thus are weak at enforcing translation adequacy (Callison-Burch et al. [7]; Koehn and Monz [8]). Unlike BLEU, or other n-gram based metrics, the MEANT family of metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly used MT evaluation"
2014.iwslt-evaluation.4,W12-3104,0,0.0199397,"e Translation Lake Tahoe, December 4th and 5th, 2014 2. Related work Surface-form oriented metrics like BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14] do not correctly reflect the meaning similarities of the basic event structure “who did what to whom, when, where and why” of the input sentence. In fact, many studies (Callison-Bursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of work on developing MT evaluation metrics that outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment (Callison-Burch et al. [16]); however, it is very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and"
2014.iwslt-evaluation.4,W07-0738,0,0.0244469,"ursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of work on developing MT evaluation metrics that outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment (Callison-Burch et al. [16]); however, it is very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and PDA models that predicts the edit sequence needed for the MT output to match the reference. SAGAN [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; furthermore, they typically rely on several dozens of parameters to tune and use expensive"
2014.iwslt-evaluation.4,W08-0332,0,0.0172516,"ursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of work on developing MT evaluation metrics that outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment (Callison-Burch et al. [16]); however, it is very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and PDA models that predicts the edit sequence needed for the MT output to match the reference. SAGAN [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; furthermore, they typically rely on several dozens of parameters to tune and use expensive"
2014.iwslt-evaluation.4,W07-0718,0,0.0182655,"n metrics that outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment (Callison-Burch et al. [16]); however, it is very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and PDA models that predicts the edit sequence needed for the MT output to match the reference. SAGAN [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; furthermore, they typically rely on several dozens of parameters to tune and use expensive linguistic resources, like WordNet and paraphrase tables. These metrics themselves are expensive in training and tuning due to the large number of parameters that need to be estimated"
2014.iwslt-evaluation.4,W08-0309,0,0.0220211,"n metrics that outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment (Callison-Burch et al. [16]); however, it is very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and PDA models that predicts the edit sequence needed for the MT output to match the reference. SAGAN [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; furthermore, they typically rely on several dozens of parameters to tune and use expensive linguistic resources, like WordNet and paraphrase tables. These metrics themselves are expensive in training and tuning due to the large number of parameters that need to be estimated"
2014.iwslt-evaluation.4,W12-3107,0,0.0136485,"ms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment (Callison-Burch et al. [16]); however, it is very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and PDA models that predicts the edit sequence needed for the MT output to match the reference. SAGAN [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; furthermore, they typically rely on several dozens of parameters to tune and use expensive linguistic resources, like WordNet and paraphrase tables. These metrics themselves are expensive in training and tuning due to the large number of parameters that need to be estimated, thus to tu"
2014.iwslt-evaluation.4,W12-3103,0,0.0168022,"s very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and PDA models that predicts the edit sequence needed for the MT output to match the reference. SAGAN [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; furthermore, they typically rely on several dozens of parameters to tune and use expensive linguistic resources, like WordNet and paraphrase tables. These metrics themselves are expensive in training and tuning due to the large number of parameters that need to be estimated, thus to tune against these metrics can be extremely expensive. the semantic role fillers of the reference and translation. More precisely, MEANT is computed as fo"
2014.iwslt-evaluation.4,2013.mtsummit-papers.12,1,0.792327,"arameters that need to be estimated, thus to tune against these metrics can be extremely expensive. the semantic role fillers of the reference and translation. More precisely, MEANT is computed as follows: 1. Apply an automatic shallow semantic parser to both the reference and machine translations. (Figure 1 shows examples of automatic shallow semantic parses on both reference and machine translations.) 2. Apply the maximum weighted bipartite matching algorithm to align the semantic frames between the reference and machine translations according to the lexical similarities of the predicates. ([23] proposed a backoff algorithm that evaluates the entire sentence of the MT output using the lexical similarity based on the context vector model, if the automatic shallow semantic parser fails to parse the reference or machine translations.) 3. For each pair of the aligned frames, apply the maximum weighted bipartite matching algorithm to align the arguments between the reference and machine translations according to the lexical similarity of role fillers. 4. Compute the weighted f-score over the matching role labels of these aligned predicates and role fillers as follow : 0 qi,j 1 qi,j ≡ ≡ wi"
2014.iwslt-evaluation.4,J97-3002,1,0.587069,"illers of the arguments of type j of all frame between the reference translations and the machine translations. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu [24]. For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu [1]). For UMEANT (Lo and Wu [2]), they are estimated in an unsupervised manner using relative freIMEANT (Wu et al. [5]) is an inversion transduction grammar based variant of MEANT. IMEANT uses a a length-normalized weighted BITG [25, 26, 27, 28] to constrain permissible token alignment patterns between aligned role filler phrases. More precisely, IMEANT differs from MEANT in the definition of si,pred and si,j , as follows: G R ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } ≡ p ([AA] |A) p (e/f |A) = = si,pred = si,j = p (⟨AA⟩|A) = 1 s(e, f ) ))   ( ( ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |) 36 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 where G is a brac"
2014.iwslt-evaluation.4,P03-1019,0,0.127937,"illers of the arguments of type j of all frame between the reference translations and the machine translations. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu [24]. For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu [1]). For UMEANT (Lo and Wu [2]), they are estimated in an unsupervised manner using relative freIMEANT (Wu et al. [5]) is an inversion transduction grammar based variant of MEANT. IMEANT uses a a length-normalized weighted BITG [25, 26, 27, 28] to constrain permissible token alignment patterns between aligned role filler phrases. More precisely, IMEANT differs from MEANT in the definition of si,pred and si,j , as follows: G R ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } ≡ p ([AA] |A) p (e/f |A) = = si,pred = si,j = p (⟨AA⟩|A) = 1 s(e, f ) ))   ( ( ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |) 36 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 where G is a brac"
2014.iwslt-evaluation.4,W09-2304,1,0.82476,"illers of the arguments of type j of all frame between the reference translations and the machine translations. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu [24]. For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu [1]). For UMEANT (Lo and Wu [2]), they are estimated in an unsupervised manner using relative freIMEANT (Wu et al. [5]) is an inversion transduction grammar based variant of MEANT. IMEANT uses a a length-normalized weighted BITG [25, 26, 27, 28] to constrain permissible token alignment patterns between aligned role filler phrases. More precisely, IMEANT differs from MEANT in the definition of si,pred and si,j , as follows: G R ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } ≡ p ([AA] |A) p (e/f |A) = = si,pred = si,j = p (⟨AA⟩|A) = 1 s(e, f ) ))   ( ( ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |) 36 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 where G is a brac"
2014.iwslt-evaluation.4,2012.eamt-1.64,1,0.776757,"illers of the arguments of type j of all frame between the reference translations and the machine translations. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu [24]. For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu [1]). For UMEANT (Lo and Wu [2]), they are estimated in an unsupervised manner using relative freIMEANT (Wu et al. [5]) is an inversion transduction grammar based variant of MEANT. IMEANT uses a a length-normalized weighted BITG [25, 26, 27, 28] to constrain permissible token alignment patterns between aligned role filler phrases. More precisely, IMEANT differs from MEANT in the definition of si,pred and si,j , as follows: G R ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } ≡ p ([AA] |A) p (e/f |A) = = si,pred = si,j = p (⟨AA⟩|A) = 1 s(e, f ) ))   ( ( ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |) 36 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 where G is a brac"
2014.iwslt-evaluation.4,W09-3804,1,0.906381,",j |, |fi,j |) 36 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 where G is a bracketing ITG whose only non terminal is A, and R is a set of transduction rules with e ∈ W 0 ∪ {ϵ} denoting a token in the MT output (or the null token) and f ∈ W 1 ∪ {ϵ} denoting a token in the reference translation (or the null token). The rule weight function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is defined using MEANT’s context vector model based lexical similarity measure. The Saers et al. [29] algorithm is used to (compute the ) inside probability of a pair ∗ of segments, P A ⇒ e/f|G . Given this, si,pred and si,j now represent the length normalized BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and machine translations. 4. Baseline In this section, we describe in detail our systems for the Chinese-English and English-Chinese TED talk MT tasks in terms of data, preprocessing, SMT pipeline and MEANT settings. 4.1. Data Our main goal for 2014 was to improve our MEANT tuned system and compare the results to our 2013 system. For th"
2014.iwslt-evaluation.4,P03-1021,0,0.0983889,"focus was to test our performance in comparison to 2013, we purposely targeted the IWSLT 2013 set more than the IWSLT 2014 set. However, we do present IWSLT 2014 results for our BLEU tuned system for both EnglishChinese and Chinese-English. The English sentences were normalized for punctuation, tokenization, and truecasing. Obviously, higher scores could have been obtained by training on the IWSLT 2014 data set instead of 2013. 4.2. SMT pipeline With the goal of improving MT utility by using our new improved version of MEANT as an objective function to drive minimum error rate training (MERT) [30] of state-of-the-art MT systems, we set up our baseline using the translation toolkit Moses [31]. In our experiments, we are using the flat phrase-based MT. The language models are trained using the SRI language model toolkit [32]. For both translation tasks, we used a 6gram language model. We use ZMERT [33] to tune the baseline since it is a reliable implementation of MERT and is fully configurable and extensible allowing us to easily incorporate our new evaluation metrics. 5. Experiments 5.1. MEANT improvements This year’s system incorporated new improvements to the MEANT metric, consisting"
2014.iwslt-evaluation.4,W13-2254,1,0.832613,"ince it is a reliable implementation of MERT and is fully configurable and extensible allowing us to easily incorporate our new evaluation metrics. 5. Experiments 5.1. MEANT improvements This year’s system incorporated new improvements to the MEANT metric, consisting of using f-score in order to aggregate lexical similarities within semantic role filler phrases instead of Mihalcea’s [34] method used in our last year system. We also tried to extend the window-size from 3 to 5 for the context vector model trained on the word segmented monolingual English gigaword corpus. Since UMEANT (Lo and Wu [35]) has been shown to be more stable when evaluating translations across different language pairs (Machacek and Bojar [36]), we use UMEANT for evaluating our output. 5.2. Tuning to IMEANT In this paper, we also ran preliminary experiments on tuning to IMEANT [5], the new inversion transduction grammar based variant of MEANT, that achieves higher correlation with human adequacy judgments of MT output quality than MEANT and its variants. Addanki et al. [28] showed empirically that the semantic role reordering that MEANT uses is covered by ITG constraints. 5.3. Word segmentation improvements For Ch"
2014.iwslt-evaluation.4,W13-2202,0,0.0679049,"our new evaluation metrics. 5. Experiments 5.1. MEANT improvements This year’s system incorporated new improvements to the MEANT metric, consisting of using f-score in order to aggregate lexical similarities within semantic role filler phrases instead of Mihalcea’s [34] method used in our last year system. We also tried to extend the window-size from 3 to 5 for the context vector model trained on the word segmented monolingual English gigaword corpus. Since UMEANT (Lo and Wu [35]) has been shown to be more stable when evaluating translations across different language pairs (Machacek and Bojar [36]), we use UMEANT for evaluating our output. 5.2. Tuning to IMEANT In this paper, we also ran preliminary experiments on tuning to IMEANT [5], the new inversion transduction grammar based variant of MEANT, that achieves higher correlation with human adequacy judgments of MT output quality than MEANT and its variants. Addanki et al. [28] showed empirically that the semantic role reordering that MEANT uses is covered by ITG constraints. 5.3. Word segmentation improvements For Chinese sentences, we improved the segmentation of Chinese words. We performed extensive comparisons between four word seg"
2014.iwslt-evaluation.4,W03-1730,0,0.0159284,"nary experiments on tuning to IMEANT [5], the new inversion transduction grammar based variant of MEANT, that achieves higher correlation with human adequacy judgments of MT output quality than MEANT and its variants. Addanki et al. [28] showed empirically that the semantic role reordering that MEANT uses is covered by ITG constraints. 5.3. Word segmentation improvements For Chinese sentences, we improved the segmentation of Chinese words. We performed extensive comparisons between four word segmentation approaches. The results reported this year were obtained using the ICTCLAS word segmenter [37]. 5.4. Named entity translation improvements We also used our own new implementation of Chinese named entity recognition and a dedicated proper name translation, where we use our own library translator based on Wikipedia data. We implemented an adequate library generator for our new named entity recognizer. 37 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 Table 1: Translation quality of the participated Chinese-English MT systems on the IWSLT 2013 test set: (a) 2013 MEANT-tuned system, (b) 2014 improved MEANT-tuned system."
2015.mtsummit-papers.26,2012.eamt-1.64,1,0.794263,"Missing"
2015.mtsummit-papers.26,W05-0909,0,0.303895,"ectively. The MEANT score for ITG based systems is considerably higher than the MEANT score for GIZA++ aligned model. We also observe that MEANT score for ITG with SRL constraints is better than the conventional ITG model. We believe that a better SRL-parser would yield a better system still. Tables 4, 5 and 6 give an upper bound on the results for the ten runs, we also see here that new semantically biased ITG model outperforms the baseline and ITG based alignment. In addition, we evaluated the performance of our model using surface-based metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006), and observed that both the unbiased ITG and semantically biased ITG based systems yield comparable results that are high in comparison to conventional GIZA++ alignment. Figure 1 shows an interesting example extracted from the test data. The Chinese input sentence has been pre-segmented into eight word-like units, and a word-for-word gloss reads approximately ”this one in Japan yet haven’t sell .” The translator who produce the reference translation took some liberties and introduced the actor they, who no d"
2015.mtsummit-papers.26,2014.iwslt-evaluation.4,1,0.88414,"ivated by the fact that including semantic role labeling in the SMT pipeline in a different way has already been shown to increase translation quality. The semantic frame based evaluation metric MEANT, which was shown to correlate better with human adequacy judgment than conventional surface based evaluation metrics (Lo et al., 2012), can be used as an objective function for tuning SMT. Tuning to MEANT, which attempts to optimize the degree to which a sentence’s semantic frames can be preserved across translation, was shown to improve translation quality across many metrics (Lo et al., 2013b; Beloucif et al., 2014). We show in this paper that including soft constraints based on semantic role labeling into the alignment training step yields both higher adequacy-oriented MEANT and, while still producing comparable scores on surface based and edit distance metrics. 2 2.1 Related work Alignment For most recent automatic machine translation systems, learning a good word alignment is paramount for producing meaningful translation. Unfortunately, conventional alignment algorithms such as IBM models (Brown et al., 1990) and the HMM-alignment model (Vogel et al., 1996) are flat and directed, meaning that (a) the"
2015.mtsummit-papers.26,P09-1088,0,0.212889,"complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeling in MT Our alignment method is fully compatible with the principle that a good trans"
2015.mtsummit-papers.26,J90-2002,0,0.818289,"at tuning towards preserving the shallow semantic structure across translations, robustly improves translation performance. Our approach brings the same intuition into the training phase. We show that our new alignment outperforms both conventional Moses and BITG alignment baselines in terms of the adequacy-oriented MEANT scores, while still producing comparable results in terms of edit distance metrics. 1 Introduction The quality of machine translation output relies heavily on word alignment. However, the most widespread approach to word alignment is the ad hoc method of training IBM models (Brown et al., 1990) in both directions and combining their results using various heuristics. Word alignments based on inversion transduction grammars or ITGs (Wu, 1997), on the other hand, provide a more structured model leading to efficient and optimal bidirectional alignments. In this paper we introduce an improved word aligner based on applying soft semantic role label constraints to ITG alignment. We show that both translation adequacy and fluency can be improved by replacing the conventional GIZA++ based alignment (Och and Ney, 2000) with more semantically motivated alignments obtained through training Proc"
2015.mtsummit-papers.26,W07-0403,0,0.20279,"r cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeling in MT Our alignment method is fully compatible with the prin"
2015.mtsummit-papers.26,J07-2003,0,0.22586,"omes at a higher cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeling in MT Our alignment method is fully c"
2015.mtsummit-papers.26,P09-1104,0,0.371113,"Missing"
2015.mtsummit-papers.26,P07-2045,0,0.0060811,"rd SRL constraints did not lead to any alignment at all: the constraints were too harsh and did not permit any biparses. Soft SRL constrained ITGs, on the other hand, outperformed the unbiased BITG model in term of both adequacyoriented MEANT scores. We noticed that λ0 = 1 and λ1 = 0.5 correspond to the best combination. The SRL constraints were only used during training of the probabilities of the ITG, and not when extracting the Viterbi parses and the corresponding word alignments. 4.3 SMT pipeline To test the different alignments described in this paper, we used the standard Moses toolkit (Koehn et al., 2007), with a 6-gram language model learned with the SRI language model toolkit (Stolcke, 2002), to train our baselines. We tested our approach using Moses hierarchical models. For tuning, we used ZMERT (Zaidan, 2009), a standard implementation of minimum error rate training or MERT (Och, 2003), we run each tuning task ten times for each system, then we decoded with both DEV and TEST set, we then chose the results according to what performed the best on the know developProceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 339 ment data. We also present the highes"
2015.mtsummit-papers.26,E06-1031,0,0.23652,"based systems is considerably higher than the MEANT score for GIZA++ aligned model. We also observe that MEANT score for ITG with SRL constraints is better than the conventional ITG model. We believe that a better SRL-parser would yield a better system still. Tables 4, 5 and 6 give an upper bound on the results for the ten runs, we also see here that new semantically biased ITG model outperforms the baseline and ITG based alignment. In addition, we evaluated the performance of our model using surface-based metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006), and observed that both the unbiased ITG and semantically biased ITG based systems yield comparable results that are high in comparison to conventional GIZA++ alignment. Figure 1 shows an interesting example extracted from the test data. The Chinese input sentence has been pre-segmented into eight word-like units, and a word-for-word gloss reads approximately ”this one in Japan yet haven’t sell .” The translator who produce the reference translation took some liberties and introduced the actor they, who no doubt was present in the cont"
2015.mtsummit-papers.26,P11-1023,1,0.898338,"biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeling in MT Our alignment method is fully compatible with the principle that a good translation is one where a human can successfully understand the main meaning of the output sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). The MEANT family of metrics are semantic evaluation MT evaluation metrics that correlate with human adequacy judgements more closely than most commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). MEANT compares the MT output sentence against provided reference translations, and produce a score measuring the degree of similarity between their semantic frame structures. Our new approach is encouraged by the fact that many previous studies have empirically shown that integrating semantic role labeling i"
2015.mtsummit-papers.26,W12-4206,1,0.913979,"Missing"
2015.mtsummit-papers.26,2013.mtsummit-papers.12,1,0.882039,"Missing"
2015.mtsummit-papers.26,W13-2254,1,0.858506,"onventional heuristics. 2.2 Semantic role labeling in MT Our alignment method is fully compatible with the principle that a good translation is one where a human can successfully understand the main meaning of the output sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). The MEANT family of metrics are semantic evaluation MT evaluation metrics that correlate with human adequacy judgements more closely than most commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). MEANT compares the MT output sentence against provided reference translations, and produce a score measuring the degree of similarity between their semantic frame structures. Our new approach is encouraged by the fact that many previous studies have empirically shown that integrating semantic role labeling into the training pipeline by tuning against MEANT improves the translation adequacy (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014). We show here, that soft incorporation of SRL constraints much earlier in the pipeline, at the word"
2015.mtsummit-papers.26,W12-3129,1,0.921193,"Missing"
2015.mtsummit-papers.26,P13-2067,1,0.910847,"ach is further motivated by the fact that including semantic role labeling in the SMT pipeline in a different way has already been shown to increase translation quality. The semantic frame based evaluation metric MEANT, which was shown to correlate better with human adequacy judgment than conventional surface based evaluation metrics (Lo et al., 2012), can be used as an objective function for tuning SMT. Tuning to MEANT, which attempts to optimize the degree to which a sentence’s semantic frames can be preserved across translation, was shown to improve translation quality across many metrics (Lo et al., 2013b; Beloucif et al., 2014). We show in this paper that including soft constraints based on semantic role labeling into the alignment training step yields both higher adequacy-oriented MEANT and, while still producing comparable scores on surface based and edit distance metrics. 2 2.1 Related work Alignment For most recent automatic machine translation systems, learning a good word alignment is paramount for producing meaningful translation. Unfortunately, conventional alignment algorithms such as IBM models (Brown et al., 1990) and the HMM-alignment model (Vogel et al., 1996) are flat and direc"
2015.mtsummit-papers.26,2013.iwslt-evaluation.5,1,0.888572,"ach is further motivated by the fact that including semantic role labeling in the SMT pipeline in a different way has already been shown to increase translation quality. The semantic frame based evaluation metric MEANT, which was shown to correlate better with human adequacy judgment than conventional surface based evaluation metrics (Lo et al., 2012), can be used as an objective function for tuning SMT. Tuning to MEANT, which attempts to optimize the degree to which a sentence’s semantic frames can be preserved across translation, was shown to improve translation quality across many metrics (Lo et al., 2013b; Beloucif et al., 2014). We show in this paper that including soft constraints based on semantic role labeling into the alignment training step yields both higher adequacy-oriented MEANT and, while still producing comparable scores on surface based and edit distance metrics. 2 2.1 Related work Alignment For most recent automatic machine translation systems, learning a good word alignment is paramount for producing meaningful translation. Unfortunately, conventional alignment algorithms such as IBM models (Brown et al., 1990) and the HMM-alignment model (Vogel et al., 1996) are flat and direc"
2015.mtsummit-papers.26,W13-2202,0,0.110404,"tics. 2.2 Semantic role labeling in MT Our alignment method is fully compatible with the principle that a good translation is one where a human can successfully understand the main meaning of the output sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). The MEANT family of metrics are semantic evaluation MT evaluation metrics that correlate with human adequacy judgements more closely than most commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). MEANT compares the MT output sentence against provided reference translations, and produce a score measuring the degree of similarity between their semantic frame structures. Our new approach is encouraged by the fact that many previous studies have empirically shown that integrating semantic role labeling into the training pipeline by tuning against MEANT improves the translation adequacy (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014). We show here, that soft incorporation of SRL constraints much earlier in the pipeline, at the word alignment stage of SMT trai"
2015.mtsummit-papers.26,P11-1064,0,0.385195,"Missing"
2015.mtsummit-papers.26,niessen-etal-2000-evaluation,0,0.362777,"bly higher than the MEANT score for GIZA++ aligned model. We also observe that MEANT score for ITG with SRL constraints is better than the conventional ITG model. We believe that a better SRL-parser would yield a better system still. Tables 4, 5 and 6 give an upper bound on the results for the ten runs, we also see here that new semantically biased ITG model outperforms the baseline and ITG based alignment. In addition, we evaluated the performance of our model using surface-based metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006), and observed that both the unbiased ITG and semantically biased ITG based systems yield comparable results that are high in comparison to conventional GIZA++ alignment. Figure 1 shows an interesting example extracted from the test data. The Chinese input sentence has been pre-segmented into eight word-like units, and a word-for-word gloss reads approximately ”this one in Japan yet haven’t sell .” The translator who produce the reference translation took some liberties and introduced the actor they, who no doubt was present in the context, but is not needed when"
2015.mtsummit-papers.26,P00-1056,0,0.595762,"Missing"
2015.mtsummit-papers.26,P03-1021,0,0.0267747,"o the best combination. The SRL constraints were only used during training of the probabilities of the ITG, and not when extracting the Viterbi parses and the corresponding word alignments. 4.3 SMT pipeline To test the different alignments described in this paper, we used the standard Moses toolkit (Koehn et al., 2007), with a 6-gram language model learned with the SRI language model toolkit (Stolcke, 2002), to train our baselines. We tested our approach using Moses hierarchical models. For tuning, we used ZMERT (Zaidan, 2009), a standard implementation of minimum error rate training or MERT (Och, 2003), we run each tuning task ten times for each system, then we decoded with both DEV and TEST set, we then chose the results according to what performed the best on the know developProceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 339 ment data. We also present the highest score achieved by each system among all runs (an upper bound on the score that can be achieved). We compared an edit-distance based metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), and a semantic evaluation metric MEANT (Lo et al., 2012) as the tuning objective. 5 Res"
2015.mtsummit-papers.26,P02-1040,0,0.0952017,"s hierarchical models. For tuning, we used ZMERT (Zaidan, 2009), a standard implementation of minimum error rate training or MERT (Och, 2003), we run each tuning task ten times for each system, then we decoded with both DEV and TEST set, we then chose the results according to what performed the best on the know developProceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 339 ment data. We also present the highest score achieved by each system among all runs (an upper bound on the score that can be achieved). We compared an edit-distance based metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), and a semantic evaluation metric MEANT (Lo et al., 2012) as the tuning objective. 5 Results We compared the the performance of our soft SRL-constrained ITG alignment to (a) the GIZA++ baseline and (b) the unbiased BITG, for both BLEU, TER and MEANT tuned systems. We evaluated our MT output using the semantic metric MEANT (Lo et al., 2012). Tables 1, 2 and 3 show the improvment in terms of MEANT scores for the SRL ITG aligned system in comparison to conventional ITG alignment and GIZA++ alignment for BLEU, TER and MEANT tuned systems respectively. The MEANT score"
2015.mtsummit-papers.26,N04-1030,0,0.769282,"troduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeling in MT Our alignment method is fully compatible with the principle that a good translation is one where a human can successfully understand the main meaning of the output sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). The MEANT family of metrics are semantic evaluation MT evaluation metrics that correlate with human adequacy judgements more closely than most commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). MEANT compares the MT output sentence against provided reference translations, and produce a score measuring the degree of similarity between their semantic frame structures. Our new approach is encouraged by the fact that many previous studies have empirically shown that integrating semanti"
2015.mtsummit-papers.26,W09-2304,1,0.915134,"optimal. Transduction grammars, on the other hand, do provide a model that (a) is inherently structurally compositional, and (b) can provide optimal bidirectional alignments. Although this structured optimality comes at a higher cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses"
2015.mtsummit-papers.26,W09-3804,1,0.965484,"on grammars, on the other hand, do provide a model that (a) is inherently structurally compositional, and (b) can provide optimal bidirectional alignments. Although this structured optimality comes at a higher cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or"
2015.mtsummit-papers.26,N10-1050,1,0.917202,"ectional alignment that the translation system is trained on to be optimal. Transduction grammars, on the other hand, do provide a model that (a) is inherently structurally compositional, and (b) can provide optimal bidirectional alignments. Although this structured optimality comes at a higher cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased"
2015.mtsummit-papers.26,2006.amta-papers.25,0,0.525027,"g, we used ZMERT (Zaidan, 2009), a standard implementation of minimum error rate training or MERT (Och, 2003), we run each tuning task ten times for each system, then we decoded with both DEV and TEST set, we then chose the results according to what performed the best on the know developProceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 339 ment data. We also present the highest score achieved by each system among all runs (an upper bound on the score that can be achieved). We compared an edit-distance based metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), and a semantic evaluation metric MEANT (Lo et al., 2012) as the tuning objective. 5 Results We compared the the performance of our soft SRL-constrained ITG alignment to (a) the GIZA++ baseline and (b) the unbiased BITG, for both BLEU, TER and MEANT tuned systems. We evaluated our MT output using the semantic metric MEANT (Lo et al., 2012). Tables 1, 2 and 3 show the improvment in terms of MEANT scores for the SRL ITG aligned system in comparison to conventional ITG alignment and GIZA++ alignment for BLEU, TER and MEANT tuned systems respectively. The MEANT score for ITG based systems is cons"
2015.mtsummit-papers.26,C96-2141,0,0.562813,"y across many metrics (Lo et al., 2013b; Beloucif et al., 2014). We show in this paper that including soft constraints based on semantic role labeling into the alignment training step yields both higher adequacy-oriented MEANT and, while still producing comparable scores on surface based and edit distance metrics. 2 2.1 Related work Alignment For most recent automatic machine translation systems, learning a good word alignment is paramount for producing meaningful translation. Unfortunately, conventional alignment algorithms such as IBM models (Brown et al., 1990) and the HMM-alignment model (Vogel et al., 1996) are flat and directed, meaning that (a) they allow unstructured movement of words leading to weak word alignment, (b) translations in one direction are considered in isolation, and (c) two separate alignments are needed to form a single bidirectional alignment. The harmonization of two directed alignments is typically done heuristically, which means that there is no model that considers the final bidirectional alignment that the translation system is trained on to be optimal. Transduction grammars, on the other hand, do provide a model that (a) is inherently structurally compositional, and (b"
2015.mtsummit-papers.26,W95-0106,1,0.736259,"lthough this structured optimality comes at a higher cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeli"
2015.mtsummit-papers.26,J97-3002,1,0.787959,"ion into the training phase. We show that our new alignment outperforms both conventional Moses and BITG alignment baselines in terms of the adequacy-oriented MEANT scores, while still producing comparable results in terms of edit distance metrics. 1 Introduction The quality of machine translation output relies heavily on word alignment. However, the most widespread approach to word alignment is the ad hoc method of training IBM models (Brown et al., 1990) in both directions and combining their results using various heuristics. Word alignments based on inversion transduction grammars or ITGs (Wu, 1997), on the other hand, provide a more structured model leading to efficient and optimal bidirectional alignments. In this paper we introduce an improved word aligner based on applying soft semantic role label constraints to ITG alignment. We show that both translation adequacy and fluency can be improved by replacing the conventional GIZA++ based alignment (Och and Ney, 2000) with more semantically motivated alignments obtained through training Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 333 ITGs (Saers and Wu, 2009) under soft SRL constraints. The n"
2015.mtsummit-papers.26,P05-1059,0,0.384966,"s structured optimality comes at a higher cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeling in MT Our alignment me"
A94-1030,W93-0305,0,0.100106,"Missing"
A94-1030,O92-1003,0,0.13173,"Missing"
A94-1030,O93-1004,0,0.144584,"Missing"
A94-1030,P94-1012,1,0.657309,"Missing"
A94-1030,P94-1010,0,\N,Missing
addanki-wu-2014-evaluating,I05-1023,1,\N,Missing
addanki-wu-2014-evaluating,C08-1048,0,\N,Missing
addanki-wu-2014-evaluating,E06-1031,0,\N,Missing
addanki-wu-2014-evaluating,P11-2014,0,\N,Missing
addanki-wu-2014-evaluating,P02-1040,0,\N,Missing
addanki-wu-2014-evaluating,P95-1033,1,\N,Missing
addanki-wu-2014-evaluating,P07-2045,0,\N,Missing
addanki-wu-2014-evaluating,W09-2006,0,\N,Missing
addanki-wu-2014-evaluating,W11-1008,1,\N,Missing
addanki-wu-2014-evaluating,P96-1021,1,\N,Missing
addanki-wu-2014-evaluating,P09-1104,0,\N,Missing
addanki-wu-2014-evaluating,C12-1142,1,\N,Missing
addanki-wu-2014-evaluating,W95-0106,1,\N,Missing
addanki-wu-2014-evaluating,J07-2003,0,\N,Missing
addanki-wu-2014-evaluating,2011.iwslt-evaluation.1,0,\N,Missing
addanki-wu-2014-evaluating,D10-1051,0,\N,Missing
addanki-wu-2014-evaluating,2013.mtsummit-papers.14,1,\N,Missing
addanki-wu-2014-evaluating,P03-1021,0,\N,Missing
addanki-wu-2014-evaluating,D10-1016,0,\N,Missing
addanki-wu-2014-evaluating,P03-1019,0,\N,Missing
C04-1058,J95-4004,0,0.0775075,"typeGaz 0=not-inGaz wcaptype 0=allupper =&gt; ne=I-MISC ne 0=I-LOC word:[-3,-1]=universidad =&gt; ne=I-ORG ne 1=O ne 2=O word 0=de captypeLex 0=not-inLex captypeGaz 0=inGaz wcaptype 0=alllower =&gt; ne=O The AdaBoost.MH base model’s high accuracy sets a high bar for error correction. Aside from brute-force en masse voting of the sort at CoNLL-2002 described above, we do not know of any existing post-boosting models that improve rather than degrade accuracy. We aim to further improve performance, and propose using a piped error corrector. 4.2 Transformation-based Learning Transformation-based learning (Brill, 1995), or TBL, is one of the most successful rule-based machine learning algorithms. The central idea of TBL is to learn an ordered list of rules, each of which evaluates on the results of those preceding it. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. Transformation-based learning has been used to tackle a wide range of NLP problems, ranging from part-ofspeech tagging (Brill, 1995) to parsing (Brill, 1996) to segmentation and message understanding (Day et al., 1997). In general, it achi"
C04-1058,W02-2004,0,0.278193,"Missing"
C04-1058,A97-1051,0,0.0818433,"formation-based learning (Brill, 1995), or TBL, is one of the most successful rule-based machine learning algorithms. The central idea of TBL is to learn an ordered list of rules, each of which evaluates on the results of those preceding it. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. Transformation-based learning has been used to tackle a wide range of NLP problems, ranging from part-ofspeech tagging (Brill, 1995) to parsing (Brill, 1996) to segmentation and message understanding (Day et al., 1997). In general, it achieves state-of-the-art performances and is fairly resistant to overtraining. 5 Conclusion We have investigated frequently raised questions about N-fold Templated Piped Correction (NTPC), a generalpurpose, conservative error correcting model, which has been shown to reliably deliver small but consistent gains on the accuracy of even high-performing base models on high-dimensional NLP tasks, with little risk of accidental degradation. Experimental evidence shows that when error-correcting high-accuracy base models, simple models and hypotheses are more beneficial than complex"
C04-1058,W03-0419,0,0.0665735,"Missing"
C04-1058,W02-2024,0,0.016439,"forming three teams in the CoNLL-2002 Named Entity Recognition shared task evaluation used boosting as their base system (Carreras et al., 2002)(Wu et al., 2002). However, precedents for improving performance after boosting are few. At the CoNLL-2002 shared task session, Tjong Kim Sang (unpublished) described an experiment using voting to combine the NER outputs from the shared task participants which, predictably, produced better results than the individual systems. A couple of the individual systems were boosting models, so in some sense this could be regarded as an example. Tsukamoto et al.(2002) used piped AdaBoost.MH models for NER. Their experimental results were somewhat disappointing, but this could perhaps be attributable to various reasons including the feature engineering or not using cross-validation sampling in the stacking. Appendix The following examples show the top 10 rules learned for English and Spanish on the bracketing + classification task. (Models M6 and M8) English ne -2=ZZZ ne -1=ZZZ word:[1,3]=21 nonnevocab 0=inNonNeVocab nevocab 0=inNeVocab captype 0=firstword-firstupper =&gt; ne=I-ORG ne 1=O ne 2=O word -1=ZZZ nonnevocab 0=inNonNeVocab nevocab 0=not-inNeVocab cap"
C04-1058,W02-2031,0,0.0638367,"Missing"
C04-1058,W02-2035,1,0.564651,"Missing"
C04-1058,wu-etal-2004-raising,1,0.898058,"imization techniques. Thus the results were surprising: the simplest models kept outperforming the “sophisticated” models. This paper attempts to investigate some of the key reasons why. To avoid reinventing the wheel, we originally considered adapting an existing error-driven method, transformation-based learning (TBL) for this purpose. TBL seems well suited to the problem as it is inherently an error corrector and, on its own, has been shown to achieve high accuracies on a variety of problems (see Section 4). Our original goal was to adapt TBL for error correction of high-performing models (Wu et al., 2004a), with two main principles: (1) since it is not clear that the usual assumptions made about the distribution of the training/test data are valid in such extreme operating ranges, empirical observations would take precedence over theoretical models, which implies that (2) any model would have to be empirically justified by testing on a diverse range of data. Experimental observations, however, increasingly drove us toward different goals. Our resulting error corrector, NTPC, was instead constructed on the principle of making as few assumptions as possible in order to robustly generalize over"
C04-1190,S01-1004,0,0.0654158,"tically trained methods; space only permits us to summarize a few key points here. Na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. The Senseval series of evaluations facilitates comparing the strengths and weaknesses of various WSD models on common data sets, with Senseval-1 (Kilgarriff and Rosenzweig, 1999), Senseval-2 (Kilgarriff, 2001), and Senseval-3 held in 1998, 2001, and 2004 respectively. 3 To do this requires solving the equation λv = Cv for eigenvalues λ ≥ 0 and eigenvectors v ∈ F . Because Supervised KPCA baseline model Our baseline WSD model is a supervised learning model that also makes use of Kernel Principal Component Analysis (KPCA), proposed by (Sch¨olkopf et al., 1998) as a generalization of PCA. KPCA has been successfully applied in many areas such as de-noising of images of hand-written digits (Mika et al., 1999) and modeling the distribution of non-linear data sets in the context of shape modelling for rea"
C04-1190,W02-1002,0,0.124954,"affects the model, and propose a new semi-supervised model that takes advantage of unlabeled data, along with a composite model that combines both the supervised and semi-supervised models. Finally, details of the experimental setup and comparative results are given. 2 Related work The long history of WSD research includes numerous statistically trained methods; space only permits us to summarize a few key points here. Na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. The Senseval series of evaluations facilitates comparing the strengths and weaknesses of various WSD models on common data sets, with Senseval-1 (Kilgarriff and Rosenzweig, 1999), Senseval-2 (Kilgarriff, 2001), and Senseval-3 held in 1998, 2001, and 2004 respectively. 3 To do this requires solving the equation λv = Cv for eigenvalues λ ≥ 0 and eigenvectors v ∈ F . Because Supervised KPCA baseline model Our baseline WSD model is a supervised learning model that also makes"
C04-1190,W96-0208,0,0.0268833,"s. After a brief look at related work, we review the baseline supervised WSD model, which is based on Kernel PCA. We then discuss how data sparseness affects the model, and propose a new semi-supervised model that takes advantage of unlabeled data, along with a composite model that combines both the supervised and semi-supervised models. Finally, details of the experimental setup and comparative results are given. 2 Related work The long history of WSD research includes numerous statistically trained methods; space only permits us to summarize a few key points here. Na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. The Senseval series of evaluations facilitates comparing the strengths and weaknesses of various WSD models on common data sets, with Senseval-1 (Kilgarriff and Rosenzweig, 1999), Senseval-2 (Kilgarriff, 2001), and Senseval-3 held in 1998, 2001, and 2004 respectively. 3 To do this requires solving the equation λv"
C04-1190,S01-1005,0,0.0396358,"Missing"
C04-1190,S01-1034,0,0.0305888,"we review the baseline supervised WSD model, which is based on Kernel PCA. We then discuss how data sparseness affects the model, and propose a new semi-supervised model that takes advantage of unlabeled data, along with a composite model that combines both the supervised and semi-supervised models. Finally, details of the experimental setup and comparative results are given. 2 Related work The long history of WSD research includes numerous statistically trained methods; space only permits us to summarize a few key points here. Na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. The Senseval series of evaluations facilitates comparing the strengths and weaknesses of various WSD models on common data sets, with Senseval-1 (Kilgarriff and Rosenzweig, 1999), Senseval-2 (Kilgarriff, 2001), and Senseval-3 held in 1998, 2001, and 2004 respectively. 3 To do this requires solving the equation λv = Cv for eigenvalues λ ≥ 0 and eigenvecto"
C04-1190,P04-1081,1,0.689727,"d individual models. Although empirical results with supervised KPCA models demonstrate significantly better accuracy compared to the state-of-the-art achieved by either na¨ıve Bayes or maximum entropy models on Senseval-2 data, we identify specific sparse data conditions under which supervised KPCA models deteriorate to essentially a most-frequent-sense predictor. We discuss the potential of KPCA for leveraging unannotated data for partially-unsupervised training to address these issues, leading to a composite model that combines both the supervised and semi-supervised models. 1 Introduction Wu et al. (2004) propose an efficient and accurate new supervised learning model for word sense disambiguation (WSD), that exploits a nonlinear Kernel Principal Component Analysis (KPCA) technique to make predictions implicitly based on generalizations over feature combinations. Experiments performed on the Senseval2 English lexical sample data show that KPCA-based word sense disambiguation method is capable of outperforming other widely used WSD models including na¨ıve Bayes, maximum entropy, and SVM models. Despite the excellent performance of the supervised KPCA-based WSD model on average, though, our furt"
C04-1190,W02-0813,0,\N,Missing
C04-1190,P03-1004,0,\N,Missing
C12-1142,P05-1033,0,0.111335,"hat a transduction grammar (or synchronous grammar) is learned without using any external resources such as parses, POS tags, or dictionaries. In contrast, many, if not most, tree-based statistical MT approaches rely on monolingually parsed, chunked, and/or tagged parallel corpora (e.g., Galley et al. (2006)), from which a transduction grammar is extracted. Such approaches must compensate for monolingual analyses that often are not designed optimally for expressing the relationship between, say, English and Chinese. Exceptions include the hierarchical phrase-based SMT method of learning ITGs (Chiang, 2005), which does not rely on external resources. However, unlike Chiang (2005) where huge numbers of (linguistically questionable) phrase translations are essentially memorized, our present work aims at inducing syntactic categories at an early stage in the learning, as occurs in child language acquisition. While only time will tell whether our more cognitively motivated approach to the induction of structural and lexical relationships between two languages will lead to better machine translation models, it is our belief that ultimately, learning the correct categories as early as humans do will p"
C12-1142,P06-1121,0,0.227971,"ach to bilingual grammar induction that is cognitively oriented toward early category formation and phrasal chunking in the bootstrapping process up the expressiveness hierarchy from finite-state to linear to inversion transduction grammars. In the context of bilingual grammar induction, “unsupervised” means that a transduction grammar (or synchronous grammar) is learned without using any external resources such as parses, POS tags, or dictionaries. In contrast, many, if not most, tree-based statistical MT approaches rely on monolingually parsed, chunked, and/or tagged parallel corpora (e.g., Galley et al. (2006)), from which a transduction grammar is extracted. Such approaches must compensate for monolingual analyses that often are not designed optimally for expressing the relationship between, say, English and Chinese. Exceptions include the hierarchical phrase-based SMT method of learning ITGs (Chiang, 2005), which does not rely on external resources. However, unlike Chiang (2005) where huge numbers of (linguistically questionable) phrase translations are essentially memorized, our present work aims at inducing syntactic categories at an early stage in the learning, as occurs in child language acqu"
C12-1142,P02-1040,0,0.092651,"ments on using our trained models for the task of SMT. Table 3 shows the BLEU scores obtained using only token based models (without any sort of chunking) on the IWSLT07_CE test set. These scores gives a gist of how translation quality changes with the grammar formalism, but a token-based model will naturally perform poorly compared to the state of the art. We used an in-house decoder for the purpose of decoding using our models. We used a trigram LM trained using SRILM (Stolcke, 2002) on the IWSLT dataset and a part of the gigaword data set. 2337 We can observe significant gains in the BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores after training on LTGs and a further improvement after training on ITGs. Further, these scores seem to be reflecting our estimates about the goodness of our models using crossentropy. Due to time constraints, we could not perform extensive experiments on the quality of translations produced with other models discussed in the rest of the paper. We also want to emphasize that an exhaustive evaluation of the performance of these models on translation tasks would result in a combinatoric explosion of models where chunking and splitting could be applied at variou"
C12-1142,W09-3804,1,0.734216,"g promoted. In this round of experiments, β was uniform over all nonterminals. Naturally, we want to keep the new ITG in normal form, so we will actually not insert the unary rules, but rather anything the nonterminal can expand into. This gives us the following new probability function (p′ ) over the new and old rules: p′ (x → φ; π) = αp(x → φ; π) + (1 − α) ∑ y∈N  β y p( y → φ; π) Where x is the preterminal being promoted, N is the set of nonterminals, φ is a sequence of nonterminals and biterminals, and π is a permutation over φ. To train at the ITG stage, we use the algorithm presented in Saers et al. (2009), which we generalize to handle multiple nonterminals rather than being restricted to bracketing ITGs. It is clear from Table 1 that the ITG models explain the data better than any other kind of model, and that more induction steps are better (the best model is the most heavily processed one, with two chunking steps and three splitting steps before moving on to ITGs). In the cases where we move to ITGs via PLITGs, we also see some improvements, which is encouraging. 2335 7 Qualitative analysis and translation assessments In this section, we present a qualitative analysis of an example to illus"
C12-1142,N10-1050,1,0.774451,"iform probability, and the lexical rules will have a portion of the probability mass relative to their cooccurrence in the training corpus. Specifically, we will initialize a stochastic bracketing FSTG such that: p (S → A) = 1, = 0.5, p (A → ε/ε)  p Q → e/ f = 0.5, = ∑ p (A → QA) where c e/ f {ε/ε}.  c e/ f e′ ∈Σ, f ′ ∈∆ c  e′ / f ′  is the cooccurrence count for the biterminal e/ f ∈ ((Σ ∪ {ε}) × (∆ ∪ {ε})) − To calculate the expectations for the expectation maximization, we will parse the training corpus with the grammar we have, using an adaptation of the parsing algorithm described in Saers et al. (2010). The adaptation consists of allowing multiple categories (not simply bracketing grammars), and to handle preterminals. Although this algorithm was designed for the linear family of transduction grammars, the PFSTGs are merely a restricted form of PLITGs, which means that the algorithm applies to them as well. Training this grammar on IWSLT07 ChineseEnglish data (Fordyce, 2007) gave us a sentencelevel cross-entropy of 110.2. The results of all training runs can be found in Table 1, where the baseline model is designated fstg. 2329 3 Chunking helps We can induce new lexical rules by allowing le"
C12-1142,2011.eamt-1.42,1,0.74365,"orithm applies to them as well. Training this grammar on IWSLT07 ChineseEnglish data (Fordyce, 2007) gave us a sentencelevel cross-entropy of 110.2. The results of all training runs can be found in Table 1, where the baseline model is designated fstg. 2329 3 Chunking helps We can induce new lexical rules by allowing lexical entities to combine in order to form larger lexical entities. The end results are similar to phrase-based machine translation, but the method of arriving at the segments (chunks or “phrases”) is very different. To apply chunking to our PFSTG, we use the method described in Saers and Wu (2011). The method was originally developed for PLITGs, which is a superset of PFSTGs, so it can be applied as is. The gist of the method is to allow two bitokens that are observed next to each other to combine into a new, larger bitoken. The process is thus limited to produce bitokens of twice the length of existing bitokens. There is, however, nothing stopping us from applying the method several times, getting larger and larger chunks. The maximum bitoken length of the baseline grammar is 2 (one input token and one output token), which we can double by chunking. By applying chunking to the baselin"
C12-1142,2011.mtsummit-papers.49,1,0.814142,"grammar that can possibly be afforded. PLITGs are wedged between PFSTGs and ITGs in terms of expressiveness. They do allow for some structured reordering. Like PFSTGs, the parse trees are chains rather than trees, but unlike PFSTGs, the links in these chains do not have to be physically adjacent. Allowing the first input token to be lexically associated with the last output token is an example of the kind of reordering that PLITGs allow, which make them more expressive than PFSTGs. The fact that these reorderings are limited to lexical units only is what makes them less expressive than ITGs (Saers et al., 2011) . The rest of the paper is structured so that we begin by describing the initial finite-state transduction grammar that we will start our bootstrapping sequence (Section 2). We then describe how the lexical chunking (Section 3) and category splitting (Section 4) is carried out before moving on to expressivity expansion: first from finite-state to linear transduction grammars (Section 5), and then from linear to inversion transduction grammar (Section 6). We then move on to an illustrative example of what the bootstrapping process actually entails (Section 7), along with some tentative decodin"
C12-1142,J97-3002,1,0.680696,"shing the foundations for understanding category formation in bottom-up bilingual grammar induction, it is more important to compare more directly how well the relationships between the two languages are learned by the very many possible various combinations of bootstrapping between transduction expressivity levels, phrasal chunking, and category formation. Therefore, we focus more on the cross entropy of the induced bilingual grammars—which is exactly the bilingual form of the standard way to evaluate how well monolingual grammar induction captures monolingual data. Note that, as observed in Wu (1997), this way of analyzing bilingual grammar induction can also be viewed as the problem of bilingual language modeling—modeling two languages simultaneously. We attack this problem of bilingual language modeling by extending a surprisingly effective finite-state baseline in two different ways: adding reordering capabilities and adding context information in the form of categories. The ultimate aim of this research direction is to have a grammar-based end-to-end statistical machine translation system, which would enable the usage of the same model in training and decoding instead of relying on a"
C12-1142,2007.iwslt-1.1,0,\N,Missing
C90-2071,P88-1012,0,0.296277,"Missing"
C90-2071,C90-2035,0,0.0107936,"rs (Wilensky & Arens 1980; Wilensky et al. 1988). Figure 3 shows a sample construction containing both syntactic and semantic feature-structures. 2 Typed feature-structures are used: the value of the special feature TYPE is a type in a multiple-inheritance type hierarchy, and two TYPE values unify only if they are not disjoint. This allows (1) easy transformation from semantic feature-structures to more convenient frame-based semantic network representations, and (2) efficient encoding of partially redundant lexical/syntactic categories using inheritance (see, for example, Pollard & Sag 1987; Jurafsky 1990). Our notation is chosen for generality; the exact encoding of signification relationships is inessential to our purpose here. TYPE: NN.constrl [ TYPE: NN ] S YN: CONS T1 : 1 CONST~: 2 SEM: 1[ TYPE: thing ] TYPE: composlte-thlng ] FRAME: ROLE1: 3 ROLE2: ¢ TYPE: N-constr ] SUBI: SYN: 1 SEM: a TYPE: N-constr ] SUB~: SYN: 2 SEM: 4 Figure 3. A nominal compound construction. Unificatlon-Based Formulation We are primarily concerned here with the following p,:oblem: given a nominal compound, determine the ranking of its possible interpretations from most to least likely. The problem can be formulated"
C98-2225,J90-2002,0,0.0221118,"er rules or semantic ontologies), monolingual syntactic grammars are relatively easy to acquire or construct. We use the grammar in the SITG channel, while retaining the bigram language model. The new model facilitates explicit coding of grammatical knowledge and finer control over channel probabilities. Like Wu's SBTG model, the translation hypothesis space can be exhaustively searched in polynomial time, as shown in Section 5. The experiments discussed in Section 6 show promising results for these directions. 2 Review: Noisy Channel Model The statistical translation model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process. The underlying generative model contains a stochastic Chinese (input) sentence generator whose output is &quot;corrupted&quot; by the translation channel to produce English (output) sentences. Assume, as we do throughout this paper, that the input language is English and the task is to translate into Chinese. In the IBM system, the language model employs simple 'n-grams, while the translation model employs several sets of parameters as discussed below. Estimation of the parameters has been described elsewhere (Brown et al., 1993). Translation is performed i"
C98-2225,J93-2003,0,0.0155698,"on model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process. The underlying generative model contains a stochastic Chinese (input) sentence generator whose output is &quot;corrupted&quot; by the translation channel to produce English (output) sentences. Assume, as we do throughout this paper, that the input language is English and the task is to translate into Chinese. In the IBM system, the language model employs simple 'n-grams, while the translation model employs several sets of parameters as discussed below. Estimation of the parameters has been described elsewhere (Brown et al., 1993). Translation is performed in the reverse direction from generation, as usual for recognition under generative models. For each English sentence e to be translated, the system attempts to find the Chinese sentence e . such that: c* == argtnax Pr(e[e) = argmax l'r(e[e)l'r(c) (1) c c In the IBM model, the search for the optimal e. is performed using a best-first heuristic &quot;stack search&quot; similar to A* methods. One of the primary obstacles to making the statistical translation approach practical is slow speed of translation, as performed in A* l'ashion. This price is paid for the robustness that i"
C98-2225,P81-1022,0,0.0698964,"within a certain distance, see Section 5) to the dot position of the item. The the located on the adjacent to the dot position of the item is skipped. Word-skipping provides us the flexibility to parse the source input by skipping possible singleton(s), if when we doing so, the source input can be parsed with the highest likelihood, and grammatical output can be produced. 5 Translation Algorithm The translation search algorithm differs from that of Wu's SBTG model in that it handles arbitrary grammars rather than binary bracketing grammars. As such it is more similar to active chart parsing (Earley, 1970) rather than CYK parsing (Kasami, 1965; Younger, 1967). We take the standard notion of items (Aho and Ulhnan, 1972), and use the term anticipation to mean an item which still has symbols right of its dot. Items that don't have any symbols right of the dot are called subtree. As with Wu's SBTG model, the algorithm maximizes a probabilistic objective function, Equation (2), u s i n g d y n a m i c p r o g r a m m i n g similar to that for H M M r e c o g n i t i o n (Viterbi, 1967). T h e p r e s e n c e o f the b i g r a m m o d e l in the objective function necessitates indexes in the recurren"
C98-2225,A94-1030,1,0.844151,"Missing"
C98-2225,P94-1012,1,0.584848,"Missing"
C98-2225,P95-1033,1,0.911082,"stic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &quot;stack search&quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions of the space of 1408 word alignments. The SBTG can be regarded as a model of the language-universal hypothesis that closely related arguments tend to stay together (Wu, 1995a; Wu, 1995b). In this paper we introduce a generalization of Wu's method with the objectives of 1. increasing translation speed flmher, 2. improving meaning-preservation accuracy, 3. improving grammaticality of the output, and 4. seeding a natural transition toward transduction role models, under the constraint of • employing no additional knowledge resources except a grammar for the target language. To achieve these objectives, we: • replace Wn's SBTG channel with a full stochastic inversion transduction grammar or SITG channel, discussed in Section 3, and • (mis-)use the target language gra"
C98-2225,1995.tmi-1.28,1,0.765319,"stic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &quot;stack search&quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions of the space of 1408 word alignments. The SBTG can be regarded as a model of the language-universal hypothesis that closely related arguments tend to stay together (Wu, 1995a; Wu, 1995b). In this paper we introduce a generalization of Wu's method with the objectives of 1. increasing translation speed flmher, 2. improving meaning-preservation accuracy, 3. improving grammaticality of the output, and 4. seeding a natural transition toward transduction role models, under the constraint of • employing no additional knowledge resources except a grammar for the target language. To achieve these objectives, we: • replace Wn's SBTG channel with a full stochastic inversion transduction grammar or SITG channel, discussed in Section 3, and • (mis-)use the target language gra"
C98-2225,W95-0106,1,0.89352,"stic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &quot;stack search&quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions of the space of 1408 word alignments. The SBTG can be regarded as a model of the language-universal hypothesis that closely related arguments tend to stay together (Wu, 1995a; Wu, 1995b). In this paper we introduce a generalization of Wu's method with the objectives of 1. increasing translation speed flmher, 2. improving meaning-preservation accuracy, 3. improving grammaticality of the output, and 4. seeding a natural transition toward transduction role models, under the constraint of • employing no additional knowledge resources except a grammar for the target language. To achieve these objectives, we: • replace Wn's SBTG channel with a full stochastic inversion transduction grammar or SITG channel, discussed in Section 3, and • (mis-)use the target language gra"
C98-2225,P96-1021,1,0.406026,"wever, unlike pure statistical translation models, the generated output string is guaranteed to conform to a given target grammar. The model employs only (1) a translation lexicon, (2) a context-free grammar for the target language, and (3) a bigram language model. The fact that no explicit bilingual translation roles are used makes the model easily portable to a variety of source languages. Initial experiments show that it also achieves significant speed gains over our earlier model. 1 Motivation Speed of statistical machine translation methods has long been an issue. A step was taken by Wu (Wu, 1996) who introduced a polynomial-time algorithm for the mntime search for an optimal translation. To achieve this, Wu's method substituted a language-independent stochastic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &quot;stack search&quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions o"
C98-2225,J97-3002,1,0.491105,"position i of a length-V Chinese sentence. So V g alignments are possible, yielding an exponential space with correspondingly slow search times. 'Various models have been constructed by the IBM team (Brown et al., 1993). This description correspondsto one of the simplest ones, &quot;Model 2&quot;; search costs for the more complex models are corrcspondinglyhigher. 1409 3 A SITG ChannelModel The translation channel we propose is based on the recently introduced bilingual language modeling approach. The model employs a stochastic version of an inversion transduction grammar or ITG (Wu, 1995c; Wu, 1995d; Wu, 1997). This formalism was originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. Subsequently, a method was developed to use a special case of the ITG--the aforementioned B T G - - f o r the translation task itself (Wu, 1996). The next few paragraphs briefly review the main properties of ITGs, before we describe the SITG channel. An ITG consists of context-free productions where terminal symbols come in couples, for example :c/9, where x is a English word and U is an Chinese translation of z, with singletons of the form x/~"
carpuat-wu-2008-evaluation,koen-2004-pharaoh,0,\N,Missing
carpuat-wu-2008-evaluation,2007.tmi-papers.28,0,\N,Missing
carpuat-wu-2008-evaluation,A94-1030,1,\N,Missing
carpuat-wu-2008-evaluation,J96-1002,0,\N,Missing
carpuat-wu-2008-evaluation,P02-1040,0,\N,Missing
carpuat-wu-2008-evaluation,P01-1027,0,\N,Missing
carpuat-wu-2008-evaluation,W05-0909,0,\N,Missing
carpuat-wu-2008-evaluation,P05-1048,1,\N,Missing
carpuat-wu-2008-evaluation,D07-1007,1,\N,Missing
carpuat-wu-2008-evaluation,P07-1005,0,\N,Missing
carpuat-wu-2008-evaluation,J03-1002,0,\N,Missing
carpuat-wu-2008-evaluation,J97-3002,1,\N,Missing
carpuat-wu-2008-evaluation,2007.tmi-papers.6,1,\N,Missing
carpuat-wu-2008-evaluation,garcia-varea-etal-2002-efficient,0,\N,Missing
carpuat-wu-2008-evaluation,W04-0822,1,\N,Missing
D07-1007,W05-0909,0,0.038279,"Missing"
D07-1007,P91-1034,0,0.141512,"ation direction, and take full advantage of not residing strictly within the Bayesian sourcechannel model in order to benefit from the much richer Senseval-style feature set this facilitates. Garcia Varea et al. found that the best results are obtained when the training of the context-dependent translation model is fully incorporated with the EM training of the SMT system. As described below, the training of our new WSD model, though not incorporated within the EM training, is also far more closely tied to the SMT model than is the case with traditional standalone WSD models. In contrast with Brown et al. (1991), our approach incorporates the predictions of state-of-theart WSD models that use rich contextual features for any phrase in the input vocabulary. In Brown et al.’s early study of WSD impact on SMT performance, the authors reported improved translation quality on a French to English task, by choosing an English translation for a French word based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize to more recent SMT archit"
D07-1007,I05-2021,1,0.881308,"em, that performs fully phrasal multi-word disambiguation. Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems. Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary. Common assumptions about the role and usefulness of word sense disambiguation (WSD) models in full-scale statistical machine translation (SMT) systems have recently been challenged. On the one hand, in previous work (Carpuat and Wu, 2005b) we obtained disappointing results when using the predictions of a Senseval WSD system in conjunction with a standard word-based SMT system: we reported slightly lower BLEU scores despite trying to incorporate WSD using a number of apparently sensible methods. These results cast doubt on the assumption that sophisticated dedicated WSD systems that were developed independently from any particular NLP application can easily be integrated into a SMT system so as to improve translation quality through stronger models of context and rich linguistic information. Rather, it has been argued, SMT sys"
D07-1007,P05-1048,1,0.924497,"em, that performs fully phrasal multi-word disambiguation. Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems. Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary. Common assumptions about the role and usefulness of word sense disambiguation (WSD) models in full-scale statistical machine translation (SMT) systems have recently been challenged. On the one hand, in previous work (Carpuat and Wu, 2005b) we obtained disappointing results when using the predictions of a Senseval WSD system in conjunction with a standard word-based SMT system: we reported slightly lower BLEU scores despite trying to incorporate WSD using a number of apparently sensible methods. These results cast doubt on the assumption that sophisticated dedicated WSD systems that were developed independently from any particular NLP application can easily be integrated into a SMT system so as to improve translation quality through stronger models of context and rich linguistic information. Rather, it has been argued, SMT sys"
D07-1007,W04-0822,1,0.64632,"o extract the WSD training data therefore depends on the one used by the SMT system. This presents the advantage of training WSD and SMT models on exactly the same data, thus eliminating domain mismatches between Senseval data and parallel corpora. But most importantly, this allows WSD training data to be generated entirely automatically, since the parallel corpus is automatically phrase-aligned in order to learn the SMT phrase bilexicon. 4.4 The WSD system The word sense disambiguation subsystem is modeled after the best performing WSD system in the Chinese lexical sample task at Senseval-3 (Carpuat et al., 2004). The features employed are typical of WSD and are therefore far richer than those used in most SMT systems. The feature set consists of positionsensitive, syntactic, and local collocational features, since these features yielded the best results when combined in a na¨ıve Bayes model on several Senseval-2 lexical sample tasks (Yarowsky and Florian, 2002). These features scale easily to the bigger vocabulary and sense candidates to be considered in a SMT task. The Senseval system consists of an ensemble of four combined WSD models: The first model is a na¨ıve Bayes model, since Yarowsky and Flo"
D07-1007,2006.iwslt-evaluation.5,1,0.848993,"In this paper, we present first results with a new architecture that integrates a state-of-the-art WSD model into phrase-based SMT so as to perform multi-word phrasal lexical disambiguation, and show that this new WSD approach not only produces gains across all available Chinese-English IWSLT06 test sets for all eight commonly used automated MT evaluation metrics, but also produces statistically significant gains on the much larger NIST Chinese-English task. The main difference between this approach and several of our earlier approaches as described in Carpuat and Wu (2005b) and subsequently Carpuat et al. (2006) lies in the fact that we focus on repurposing the WSD system for multi-word phrase-based SMT. Rather than using a generic Senseval WSD model as we did in Carpuat and Wu (2005b), here both the WSD training and the WSD predictions are integrated into the phrase-based SMT framework. Furthermore, rather than using a single word based WSD approach to augment a phrase-based SMT model as we did in Carpuat et al. (2006) to improve BLEU and NIST scores, here the WSD training and predictions operate on full multi-word phrasal units, resulting in significantly more reliable and consistent gains as evalu"
D07-1007,W07-0403,0,0.0489086,"Missing"
D07-1007,W04-0802,0,0.151868,"ion candidates seen during SMT training. • Instead of learning from manually annotated training data, our WSD system is trained on the same corpora as the SMT system. However, despite these adaptations to the SMT task, the core sense disambiguation task remains pure WSD: • The rich context features are typical of WSD and almost never used in SMT. • The dynamic integration of context-sensitive translation probabilities is not typical of SMT. • Although it is embedded in a real SMT system, the WSD task is exactly the same as in recent and coming Senseval Multilingual Lexical Sample tasks (e.g., Chklovski et al. (2004)), where sense inventories represent the semantic distinctions made by another language. We begin by presenting the WSD module and the SMT integration technique. We then show that incorporating it into a standard phrase-based SMT baseline system consistently improves translation quality across all three different test sets from the Chinese-English IWSLT text translation evaluation, as well as on the larger NIST Chinese-English translation task. Depending on the metric, the individual gains are sometimes modest, but remarkably, incorporating WSD never hurts, and helps enough to always make it a"
D07-1007,P04-1039,0,0.0176286,"based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize to more recent SMT architectures. 3 Problems in translation-oriented WSD The close relationship between WSD and SMT has been emphasized since the emergence of WSD as an independent task. However, most of previous research has focused on using multilingual resources typically used in SMT systems to improve WSD accuracy, e.g., Dagan and Itai (1994), Li and Li (2002), Diab (2004). In contrast, this paper focuses on the converse goal of using WSD models to improve actual translation quality. Recently, several researchers have focused on designing WSD systems for the specific purpose of translation. Vickrey et al. (2005) train a logistic regression WSD model on data extracted from automatically word aligned parallel corpora, but evaluate on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia (2006) describes an inductive logic programming-based WSD system, which was specifically designed for the purpose of Portuguese to English translation,"
D07-1007,P01-1027,0,0.207028,"babilities versus the WSD model predictions for single words. In addition, the single-word model does not generalize to WSD for phrasal lexical choice, as overlapping spans cannot be specified with the XML markup scheme. Providing WSD predictions for phrases would require committing to a phrase segmentation of the input sentence before decoding, which is likely to hurt translation quality. It is also necessary to focus directly on translation accuracy rather than other measures such as alignment error rate, which may not actually lead to improved translation quality; in contrast, for example, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based context-dependent lexical 63 choice model, but not improved translation accuracy. In contrast, our evaluation in this paper is conducted on the actual decoding task, rather than intermediate tasks such as word alignment. Moreover, in the present work, all commonly available automated MT evaluation metrics are used, rather than only BLEU score, so as to maintain a more balanced perspective. Another problem in the context-sensitive lexical choice in SMT models of Garcia Varea et al. is that their feat"
D07-1007,garcia-varea-etal-2002-efficient,0,0.0665362,"predictions for single words. In addition, the single-word model does not generalize to WSD for phrasal lexical choice, as overlapping spans cannot be specified with the XML markup scheme. Providing WSD predictions for phrases would require committing to a phrase segmentation of the input sentence before decoding, which is likely to hurt translation quality. It is also necessary to focus directly on translation accuracy rather than other measures such as alignment error rate, which may not actually lead to improved translation quality; in contrast, for example, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based context-dependent lexical 63 choice model, but not improved translation accuracy. In contrast, our evaluation in this paper is conducted on the actual decoding task, rather than intermediate tasks such as word alignment. Moreover, in the present work, all commonly available automated MT evaluation metrics are used, rather than only BLEU score, so as to maintain a more balanced perspective. Another problem in the context-sensitive lexical choice in SMT models of Garcia Varea et al. is that their feature set is insufficiently rich"
D07-1007,S01-1004,0,0.00733519,"incorporating WSD never hurts, and helps enough to always make it a worthwile additional component in an SMT system. Finally, we analyze the reasons for the improvement. 2 Problems in context-sensitive lexical choice for SMT To the best of our knowledge, there has been no previous attempt at integrating a state-of-the-art WSD system for fully phrasal multi-word lexical choice into phrase-based SMT, with evaluation of the resulting system on a translation task. While there are many evaluations of WSD quality, in particular the Senseval series of shared tasks (Kilgarriff and Rosenzweig (1999), Kilgarriff (2001), Mihalcea et al. (2004)), very little work has been done to address the actual integration of WSD in realistic SMT applications. To fully integrate WSD into phrase-based SMT, it is necessary to perform lexical disambiguation on multi-word phrasal lexical units; in contrast, the model reported in Cabezas and Resnik (2005) can only perform lexical disambiguation on single words. Like the model proposed in this paper, Cabezas and Resnik attempted to integrate phrasebased WSD models into decoding. However, although they reported that incorporating these predictions via the Pharaoh XML markup sche"
D07-1007,W02-1002,0,0.00644799,"ed the best results when combined in a na¨ıve Bayes model on several Senseval-2 lexical sample tasks (Yarowsky and Florian, 2002). These features scale easily to the bigger vocabulary and sense candidates to be considered in a SMT task. The Senseval system consists of an ensemble of four combined WSD models: The first model is a na¨ıve Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second model is a maximum entropy model (Jaynes, 1978), since Klein and Manning (Klein and Manning, 2002) found that this model yielded higher accuracy than na¨ıve Bayes in a subsequent comparison of WSD performance. The third model is a boosting model (Freund and Schapire, 1997), since boosting has consistently turned in very competitive scores on related tasks such as named entity classification. We also use the Adaboost.MH algorithm. The fourth model is a Kernel PCA-based model (Wu et al., 2004). Kernel Principal Component Analysis or KPCA is a nonlinear kernel method for 65 extracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonl"
D07-1007,N03-1017,0,0.0269456,"(Carpuat and Wu, 2007). 4.2 WSD uses the same sense definitions as the SMT system Instead of using pre-defined sense inventories, the WSD models disambiguate between the SMT translation candidates. In order to closely integrate WSD predictions into the SMT system, we need to formulate WSD models so that they produce features that can directly be used in translation decisions taken by the SMT system. It is therefore necessary for the WSD and SMT systems to consider exactly the same translation candidates for a given word in the input language. Assuming a standard phrase-based SMT system (e.g., Koehn et al. (2003)), WSD senses are thus either words or phrases, as learned in the SMT phrasal translation lexicon. Those “sense” candidates are very different from those typically used even in dedicated WSD tasks, even in the multilingual Senseval tasks. Each candidate is a phrase that is not necessarily a syntactic noun or verb phrase as in manually compiled dictionaries. It is quite possible that distinct “senses” in our WSD for SMT system could be considered synonyms in a traditional WSD framework, especially in monolingual WSD. In addition to the consistency requirements for integration, this requirement"
D07-1007,koen-2004-pharaoh,0,0.0645239,"input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space. All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant. Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent. 4.5 Integrating WSD predictions in phrase-based SMT architectures It is non-trivial to incorporate WSD into an existing phrase-based architecture such as Pharaoh (Koehn, 2004), since the decoder is not set up to easily accept multiple translation probabilities that are dynamically computed in context-sensitive fashion. For every phrase in a given SMT input sentence, the WSD probabilities can be used as additional feature in a loglinear translation model, in combination with typical context-independent SMT bilexicon probabilities. We overcome this obstacle by devising a calling architecture that reinitializes the decoder with dynamically generated lexicons on a per-sentence basis. Unlike a n-best reranking approach, which is limited by the lexical choices made by th"
D07-1007,P02-1044,0,0.0298416,"for a French word based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize to more recent SMT architectures. 3 Problems in translation-oriented WSD The close relationship between WSD and SMT has been emphasized since the emergence of WSD as an independent task. However, most of previous research has focused on using multilingual resources typically used in SMT systems to improve WSD accuracy, e.g., Dagan and Itai (1994), Li and Li (2002), Diab (2004). In contrast, this paper focuses on the converse goal of using WSD models to improve actual translation quality. Recently, several researchers have focused on designing WSD systems for the specific purpose of translation. Vickrey et al. (2005) train a logistic regression WSD model on data extracted from automatically word aligned parallel corpora, but evaluate on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia (2006) describes an inductive logic programming-based WSD system, which was specifically designed for the purpose of Portuguese to English"
D07-1007,W04-0807,0,0.0596988,"never hurts, and helps enough to always make it a worthwile additional component in an SMT system. Finally, we analyze the reasons for the improvement. 2 Problems in context-sensitive lexical choice for SMT To the best of our knowledge, there has been no previous attempt at integrating a state-of-the-art WSD system for fully phrasal multi-word lexical choice into phrase-based SMT, with evaluation of the resulting system on a translation task. While there are many evaluations of WSD quality, in particular the Senseval series of shared tasks (Kilgarriff and Rosenzweig (1999), Kilgarriff (2001), Mihalcea et al. (2004)), very little work has been done to address the actual integration of WSD in realistic SMT applications. To fully integrate WSD into phrase-based SMT, it is necessary to perform lexical disambiguation on multi-word phrasal lexical units; in contrast, the model reported in Cabezas and Resnik (2005) can only perform lexical disambiguation on single words. Like the model proposed in this paper, Cabezas and Resnik attempted to integrate phrasebased WSD models into decoding. However, although they reported that incorporating these predictions via the Pharaoh XML markup scheme yielded a small impro"
D07-1007,P03-1058,0,0.241453,"ly, several researchers have focused on designing WSD systems for the specific purpose of translation. Vickrey et al. (2005) train a logistic regression WSD model on data extracted from automatically word aligned parallel corpora, but evaluate on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia (2006) describes an inductive logic programming-based WSD system, which was specifically designed for the purpose of Portuguese to English translation, but this system was also only evaluated on WSD accuracy, and not integrated in a full-scale machine translation system. Ng et al. (2003) show that it is possible to use automatically word aligned parallel corpora to train accurate supervised WSD models. The purpose of the study was to lower the annotation cost for supervised WSD, as suggested earlier by Resnik and Yarowsky (1999). However this result is also encouraging for the integration of WSD in SMT, since it suggests that accurate WSD can be achieved using training data of the kind needed for SMT. 4 4.1 Building WSD models for phrase-based SMT WSD models for every phrase in the input vocabulary Just like for the baseline phrase translation model, WSD models are defined fo"
D07-1007,J03-1002,0,0.00760388,"ized. The Chinese side was word segmented using the LDC segmenter. 5.2 Baseline SMT system Since our focus is not on a specific SMT architecture, we use the off-the-shelf phrase-based decoder 66 translation predictions improves WER PER CDER 88.26 83.87 61.71 57.29 70.32 67.38 Pharaoh (Koehn, 2004) trained on the IWSLT training set. Pharaoh implements a beam search decoder for phrase-based statistical models, and presents the advantages of being freely available and widely used. The phrase bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2003), augmented to improve recall using the grow-diag-final heuristic. The language model is trained on the English side of the corpus using the SRI language modeling toolkit (Stolcke, 2002). The loglinear model weights are learned using Chiang’s implementation of the maximum BLEU training algorithm (Och, 2003), both for the baseline, and the WSD-augmented system. Due to time constraints, this optimization was only conducted on the IWSLT task. The weights used in the WSD-augmented NIST model are based on the best IWSLT model. Given that the two tasks are quite different, we expect further improvem"
D07-1007,P03-1021,0,0.0252698,"SLT training set. Pharaoh implements a beam search decoder for phrase-based statistical models, and presents the advantages of being freely available and widely used. The phrase bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2003), augmented to improve recall using the grow-diag-final heuristic. The language model is trained on the English side of the corpus using the SRI language modeling toolkit (Stolcke, 2002). The loglinear model weights are learned using Chiang’s implementation of the maximum BLEU training algorithm (Och, 2003), both for the baseline, and the WSD-augmented system. Due to time constraints, this optimization was only conducted on the IWSLT task. The weights used in the WSD-augmented NIST model are based on the best IWSLT model. Given that the two tasks are quite different, we expect further improvements on the WSD-augmented system after running maximum BLEU optimization for the NIST task. 6 Results and discussion Using WSD predictions in SMT yields better translation quality on all test sets, as measured by all eight commonly used automatic evaluation metrics. Table 3: Translation examples with and wi"
D07-1007,P02-1040,0,0.107212,"Missing"
D07-1007,2006.iwslt-evaluation.1,0,0.00627121,"WSD 51.05 9.142 74.13 71.44 34.68 39.75 31.71 34.58 Table 2: Evaluation results on the NIST test set: integrating the WSD BLEU, NIST, METEOR, WER, PER, CDER and TER Exper. BLEU NIST METEOR METEOR TER (no syn) SMT 20.41 7.155 60.21 56.15 76.76 SMT+WSD 20.92 7.468 60.30 56.79 71.34 5.1 Data set Preliminary experiments are conducted using training and evaluation data drawn from the multilingual BTEC corpus, which contains sentences used in conversations in the travel domain, and their translations in several languages. A subset of this data was made available for the IWSLT06 evaluation campaign (Paul, 2006); the training set consists of 40000 sentence pairs, and each test set contains around 500 sentences. We used only the pure text data, and not the speech transcriptions, so that speech-specific issues would not interfere with our primary goal of understanding the effect of integrating WSD in a fullscale phrase-based model. A larger scale evaluation is conducted on the standard NIST Chinese-English test set (MT-04), which contains 1788 sentences drawn from newswire corpora, and therefore of a much wider domain than the IWSLT data set. The training set consists of about 1 million sentence pairs"
D07-1007,2006.amta-papers.25,0,0.0330913,"Missing"
D07-1007,W06-2505,0,0.337401,"int Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 61–72, Prague, June 2007. 2007 Association for Computational Linguistics and unsupervised, on a Senseval WSD task (Carpuat and Wu, 2005a), and therefore suggest that WSD should have a role to play in state-of-the-art SMT systems. In addition to the Senseval shared tasks, which have provided standard sense inventories and data sets, WSD research has also turned increasingly to designing specific models for a particular application. For instance, Vickrey et al. (2005) and Specia (2006) proposed WSD systems designed for French to English, and Portuguese to English translation respectively, and present a more optimistic outlook for the use of WSD in MT, although these WSD systems have not yet been integrated nor evaluated in full-scale machine translation systems. Taken together, these seemingly contradictory results suggest that improving SMT lexical choice accuracy remains a key challenge to improve current SMT quality, and that it is still unclear what is the most appropriate integration framework for the WSD models in SMT. In this paper, we present first results with a ne"
D07-1007,H05-1097,0,0.67277,". ∗ 61 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 61–72, Prague, June 2007. 2007 Association for Computational Linguistics and unsupervised, on a Senseval WSD task (Carpuat and Wu, 2005a), and therefore suggest that WSD should have a role to play in state-of-the-art SMT systems. In addition to the Senseval shared tasks, which have provided standard sense inventories and data sets, WSD research has also turned increasingly to designing specific models for a particular application. For instance, Vickrey et al. (2005) and Specia (2006) proposed WSD systems designed for French to English, and Portuguese to English translation respectively, and present a more optimistic outlook for the use of WSD in MT, although these WSD systems have not yet been integrated nor evaluated in full-scale machine translation systems. Taken together, these seemingly contradictory results suggest that improving SMT lexical choice accuracy remains a key challenge to improve current SMT quality, and that it is still unclear what is the most appropriate integration framework for the WSD models in SMT. In this paper, we present first"
D07-1007,P98-2230,1,0.61049,"Missing"
D07-1007,P04-1081,1,0.558695,"e the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second model is a maximum entropy model (Jaynes, 1978), since Klein and Manning (Klein and Manning, 2002) found that this model yielded higher accuracy than na¨ıve Bayes in a subsequent comparison of WSD performance. The third model is a boosting model (Freund and Schapire, 1997), since boosting has consistently turned in very competitive scores on related tasks such as named entity classification. We also use the Adaboost.MH algorithm. The fourth model is a Kernel PCA-based model (Wu et al., 2004). Kernel Principal Component Analysis or KPCA is a nonlinear kernel method for 65 extracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonlinearly mapped from their original space Rn to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space. All these classifiers have the ability to handle large numbers"
D07-1007,P96-1021,1,0.501543,"Missing"
D07-1007,C04-1030,0,0.59701,"en during SMT training. • Instead of learning from manually annotated training data, our WSD system is trained on the same corpora as the SMT system. However, despite these adaptations to the SMT task, the core sense disambiguation task remains pure WSD: • The rich context features are typical of WSD and almost never used in SMT. • The dynamic integration of context-sensitive translation probabilities is not typical of SMT. • Although it is embedded in a real SMT system, the WSD task is exactly the same as in recent and coming Senseval Multilingual Lexical Sample tasks (e.g., Chklovski et al. (2004)), where sense inventories represent the semantic distinctions made by another language. We begin by presenting the WSD module and the SMT integration technique. We then show that incorporating it into a standard phrase-based SMT baseline system consistently improves translation quality across all three different test sets from the Chinese-English IWSLT text translation evaluation, as well as on the larger NIST Chinese-English translation task. Depending on the metric, the individual gains are sometimes modest, but remarkably, incorporating WSD never hurts, and helps enough to always make it a"
D07-1007,E06-1031,0,\N,Missing
D07-1007,J94-4003,0,\N,Missing
D07-1007,J04-1001,0,\N,Missing
D07-1007,P06-3010,0,\N,Missing
D07-1007,J97-3002,1,\N,Missing
D07-1007,C98-2225,1,\N,Missing
D07-1007,2007.tmi-papers.6,1,\N,Missing
D13-1011,W09-2006,0,0.195947,"Chinese couplets, which adhere to strict rules requiring, for example, an identical number of characters in each line and one-to-one correspondence in their metrical length, the domain of hip hop lyrics is far more unstructured and there exists no clear constraint that would ensure fluent and rhyming responses to hip hop challenge lyrics. Barbieri et al. (2012) use controlled Markov processes to semi-automatically generate lyrics that satisfy the structural constraints of rhyme and meter. 104 Tamil lyrics were automatically generated given a melody using conditional random fields by A. et al. (2009). The lyrics were represented as a sequence of labels using the KNM system where K, N and M represented the long vowels, short vowels and consonants respectively. Genzel et al. (2010) used SMT in conjunction with stress patterns and rhymes found in a pronunciation dictionary to produce translations of poems. Although many constraints were applied in translating full verses of poems, it was challenging to satisfy all the constraints. Stress patterns were assigned to words given the meter of a line in Shakespeare’s sonnets by Greene et al. (2010), which were then combined with a language model t"
D13-1011,J07-2003,0,0.187334,"Missing"
D13-1011,D10-1016,0,0.0135278,"gth, the domain of hip hop lyrics is far more unstructured and there exists no clear constraint that would ensure fluent and rhyming responses to hip hop challenge lyrics. Barbieri et al. (2012) use controlled Markov processes to semi-automatically generate lyrics that satisfy the structural constraints of rhyme and meter. 104 Tamil lyrics were automatically generated given a melody using conditional random fields by A. et al. (2009). The lyrics were represented as a sequence of labels using the KNM system where K, N and M represented the long vowels, short vowels and consonants respectively. Genzel et al. (2010) used SMT in conjunction with stress patterns and rhymes found in a pronunciation dictionary to produce translations of poems. Although many constraints were applied in translating full verses of poems, it was challenging to satisfy all the constraints. Stress patterns were assigned to words given the meter of a line in Shakespeare’s sonnets by Greene et al. (2010), which were then combined with a language model to generate poems. Sonderegger (2011) attempted to infer the pronunciation of words in old English by identifying the rhyming patterns using graph theory. However, their heuristic of c"
D13-1011,D10-1051,0,0.196029,"ted given a melody using conditional random fields by A. et al. (2009). The lyrics were represented as a sequence of labels using the KNM system where K, N and M represented the long vowels, short vowels and consonants respectively. Genzel et al. (2010) used SMT in conjunction with stress patterns and rhymes found in a pronunciation dictionary to produce translations of poems. Although many constraints were applied in translating full verses of poems, it was challenging to satisfy all the constraints. Stress patterns were assigned to words given the meter of a line in Shakespeare’s sonnets by Greene et al. (2010), which were then combined with a language model to generate poems. Sonderegger (2011) attempted to infer the pronunciation of words in old English by identifying the rhyming patterns using graph theory. However, their heuristic of clustering words with similar IPA endings resulted in large clusters of false positives such as bloom and numb. A language-independent generative model for stanzas in poetry was proposed by Reddy and Knight (2011) via which they could discover rhyme schemes in French and English poetry. 3 Experimental conditions Before introducing our Freestyle models, we first deta"
D13-1011,C08-1048,0,0.644324,"e structured domains such as poetry. However, in hip hop lyrics it is hard to make any linguistic or structural assumptions. For example, words such as sho, flo, holla which frequently appear in the lyrics are not part of any standard lexicon and hip hop does not require a set number of syllables in a line, unlike poems. Also, surprising and unlikely rhymes in hip hop are frequently achieved via intonation and assonance, making it hard to apply prior phonological constraints. A phrase based SMT system was trained to “translate” the first line of a Chinese couplet or duilian into the second by Jiang and Zhou (2008). The most suitable next line was selected by applying linguistic constraints to the n best output of the SMT system. However in contrast to Chinese couplets, which adhere to strict rules requiring, for example, an identical number of characters in each line and one-to-one correspondence in their metrical length, the domain of hip hop lyrics is far more unstructured and there exists no clear constraint that would ensure fluent and rhyming responses to hip hop challenge lyrics. Barbieri et al. (2012) use controlled Markov processes to semi-automatically generate lyrics that satisfy the structur"
D13-1011,P07-2045,0,0.00599273,"op listeners for manual evaluation. They were asked to evaluate the system outputs according to fluency and the degree of rhyming. They were free to choose the tune to make the lyrics rhyme as the beats of the song were not used in the training data. Each evaluator was asked to score the response of each system on the criterion of fluency and rhyming as being good, acceptable or bad. 3.3 Phrase-based SMT baseline In order to evaluate the performance of an out-ofthe-box phrase-based SMT (PBSMT) system toward this novel task of generating rhyming and fluent responses, a standard Moses baseline (Koehn et al., 2007) was also trained in order to compare its performance with our transduction grammar induction model. A 4-gram language model which was trained on the entire training corpus using SRILM (Stolcke, 2002) was used to generate responses in conjunction with the phrase-based translation model. As no automatic quality evaluation metrics exist for hip hop responses analogous to BLEU for SMT, the model weights cannot be tuned in conventional ways such as MERT (Och, 2003). Instead, a slightly higher than typical language model weight was empirically chosen using a small development set to produce fluent"
D13-1011,P03-1021,0,0.00591131,"phrase-based SMT (PBSMT) system toward this novel task of generating rhyming and fluent responses, a standard Moses baseline (Koehn et al., 2007) was also trained in order to compare its performance with our transduction grammar induction model. A 4-gram language model which was trained on the entire training corpus using SRILM (Stolcke, 2002) was used to generate responses in conjunction with the phrase-based translation model. As no automatic quality evaluation metrics exist for hip hop responses analogous to BLEU for SMT, the model weights cannot be tuned in conventional ways such as MERT (Och, 2003). Instead, a slightly higher than typical language model weight was empirically chosen using a small development set to produce fluent outputs. 4 Interpolated segmenting model vs. token based model We compare the performance of transduction grammars induced via interpolated token based and rule segmenting (ISTG) versus token based transduction grammars (TG) on the task of generating a rhyming and fluent response to hip hop challenges. We use the framework of stochastic transduction grammars, specifically bracketing ITGs (inversion transduction 105 grammars) (Wu, 1997), as our translation model"
D13-1011,P02-1040,0,0.0904368,"h rule in a segmental ITG grammar can contain more than one token in both input and output languages. In machine translation applications, segmental models produce translations that are more fluent as they can capture lexical knowledge at a phrasal level. However, only a handful of purely unsupervised algorithms exist for learning segmental ITGs under matched training and testing assumptions. Most other approaches in SMT use a variety of ad hoc heuristics for extracting segments from token alignments, justified purely by short term improvements in automatic MT evaluation metrics such as BLEU (Papineni et al., 2002) which cannot be transferred to our current task. Instead, we use a completely unsupervised learning algorithm for segmental ITGs that stays strictly within the transduction grammar optimization framework for both training and testing as proposed in Saers et al. (2013). Saers et al. (2013) induce a phrasal inversion transduction grammar via interpolating the bottomup rule chunking approach proposed in Saers et al. (2012) with a top-down rule segmenting approach driven by a minimum description length objective function (Solomonoff, 1959; Rissanen, 1983) that trades off the maximum likelihood ag"
D13-1011,P11-2014,0,0.514818,"ses of poems, it was challenging to satisfy all the constraints. Stress patterns were assigned to words given the meter of a line in Shakespeare’s sonnets by Greene et al. (2010), which were then combined with a language model to generate poems. Sonderegger (2011) attempted to infer the pronunciation of words in old English by identifying the rhyming patterns using graph theory. However, their heuristic of clustering words with similar IPA endings resulted in large clusters of false positives such as bloom and numb. A language-independent generative model for stanzas in poetry was proposed by Reddy and Knight (2011) via which they could discover rhyme schemes in French and English poetry. 3 Experimental conditions Before introducing our Freestyle models, we first detail our experimental assumptions and the evaluation scheme under which the responses generated by different models are compared against one another. We describe our training data as well as a phrasebased SMT (PBSMT) contrastive baseline. We also define the evaluation scheme used to compare the responses of different systems on criteria of fluency and rhyming. 3.1 Training data We used freely available user generated hip hop lyrics on the Inte"
D13-1011,C12-1142,1,0.908351,"ges. We use the framework of stochastic transduction grammars, specifically bracketing ITGs (inversion transduction 105 grammars) (Wu, 1997), as our translation model for “transducing” any given challenge into a rhyming and fluent response. Our choice is motivated by the significant amount of empirical evidence for the representational capacity of transduction grammars across a spectrum of natural language tasks such as textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003). Further, existence of efficient learning algorithms (Saers et al., 2012; Saers and Wu, 2011) that make no language specific assumptions, make inversion transduction grammars a suitable framework for our modeling needs. Examples of lexical transduction rules can be seen in Tables 3 and 5. In addition, the grammar also includes structural transduction rules for the straight case A → [A A] and also the inverted case A → &lt;A A>. 4.1 Token based vs. segmental ITGs The degenerate case of ITGs are token based ITGs wherein each translation rule contains at most one token in input and output languages. Efficient induction algorithms with polynomial run time exist for token"
D13-1011,W13-0806,1,0.83837,"rely unsupervised algorithms exist for learning segmental ITGs under matched training and testing assumptions. Most other approaches in SMT use a variety of ad hoc heuristics for extracting segments from token alignments, justified purely by short term improvements in automatic MT evaluation metrics such as BLEU (Papineni et al., 2002) which cannot be transferred to our current task. Instead, we use a completely unsupervised learning algorithm for segmental ITGs that stays strictly within the transduction grammar optimization framework for both training and testing as proposed in Saers et al. (2013). Saers et al. (2013) induce a phrasal inversion transduction grammar via interpolating the bottomup rule chunking approach proposed in Saers et al. (2012) with a top-down rule segmenting approach driven by a minimum description length objective function (Solomonoff, 1959; Rissanen, 1983) that trades off the maximum likelihood against model size. Saers et al. (2013) report improvements in BLEU score (Papineni et al., 2002) on their translation task. In our current approach instead of using a bottom-up rule chunking approach we use a simpler token based grammar instead. Given two grammars (Ga a"
D13-1011,W11-1008,1,0.818598,"ework of stochastic transduction grammars, specifically bracketing ITGs (inversion transduction 105 grammars) (Wu, 1997), as our translation model for “transducing” any given challenge into a rhyming and fluent response. Our choice is motivated by the significant amount of empirical evidence for the representational capacity of transduction grammars across a spectrum of natural language tasks such as textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003). Further, existence of efficient learning algorithms (Saers et al., 2012; Saers and Wu, 2011) that make no language specific assumptions, make inversion transduction grammars a suitable framework for our modeling needs. Examples of lexical transduction rules can be seen in Tables 3 and 5. In addition, the grammar also includes structural transduction rules for the straight case A → [A A] and also the inverted case A → &lt;A A>. 4.1 Token based vs. segmental ITGs The degenerate case of ITGs are token based ITGs wherein each translation rule contains at most one token in input and output languages. Efficient induction algorithms with polynomial run time exist for token based ITGs and the e"
D13-1011,P96-1021,1,0.501234,"t address that problem in our current model. We also penalize singleton rules to produce responses of similar length as successive lines in a stanza are typically of similar length. Finally, we add a penalty to reflexive translation rules that map the same surface form to itself such as A → yo/yo. We obtain these rules with a high probability due to the presence of sentence pairs where both the input and output are identical strings as many stanzas in our data contain repeated chorus lines. Decoding heuristics We use our in-house ITG decoder implemented according to the algorithm mentioned in Wu (1996) for the generating responses to challenges by decoding with the trained transduction grammars. The decoder uses a CKY-style parsing algorithm (Cocke, 106 4.3 Results: Rule segmentation improves responses Results in Table 1 indicate that the ISTG outperforms the TG model towards the task of generating fluent and rhyming responses. On the criterion of fluency, Table 1: Percentage of ≥good and ≥acceptable (i.e., either good or acceptable) responses on fluency and rhyming criteria. PBSMT, TG and ISTG models trained using corpus generated from all adjacent lines in a verse. PBSMT+RS, TG+RS, ISTG+R"
D13-1011,J97-3002,1,0.198613,"ional ways such as MERT (Och, 2003). Instead, a slightly higher than typical language model weight was empirically chosen using a small development set to produce fluent outputs. 4 Interpolated segmenting model vs. token based model We compare the performance of transduction grammars induced via interpolated token based and rule segmenting (ISTG) versus token based transduction grammars (TG) on the task of generating a rhyming and fluent response to hip hop challenges. We use the framework of stochastic transduction grammars, specifically bracketing ITGs (inversion transduction 105 grammars) (Wu, 1997), as our translation model for “transducing” any given challenge into a rhyming and fluent response. Our choice is motivated by the significant amount of empirical evidence for the representational capacity of transduction grammars across a spectrum of natural language tasks such as textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003). Further, existence of efficient learning algorithms (Saers et al., 2012; Saers and Wu, 2011) that make no language specific assumptions, make inversion transduction grammars a suitable framewor"
D13-1011,2013.mtsummit-papers.14,1,0.814366,"Missing"
D13-1011,I05-1023,1,0.782388,"rsus token based transduction grammars (TG) on the task of generating a rhyming and fluent response to hip hop challenges. We use the framework of stochastic transduction grammars, specifically bracketing ITGs (inversion transduction 105 grammars) (Wu, 1997), as our translation model for “transducing” any given challenge into a rhyming and fluent response. Our choice is motivated by the significant amount of empirical evidence for the representational capacity of transduction grammars across a spectrum of natural language tasks such as textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003). Further, existence of efficient learning algorithms (Saers et al., 2012; Saers and Wu, 2011) that make no language specific assumptions, make inversion transduction grammars a suitable framework for our modeling needs. Examples of lexical transduction rules can be seen in Tables 3 and 5. In addition, the grammar also includes structural transduction rules for the straight case A → [A A] and also the inverted case A → &lt;A A>. 4.1 Token based vs. segmental ITGs The degenerate case of ITGs are token based ITGs wherein each translation rule contains at"
D13-1011,P03-1019,0,\N,Missing
I05-1023,P95-1033,1,0.878394,"e available. But while loose translations by themselves already have numerous applications, truly parallel sentence translations provide invaluable types of information for the aforementioned types of mining and induction, which cannot easily be obtained from merely loose translations or comparable sentence pairs. In particular, truly parallel bi-sentences are especially useful for extracting more precise syntactic and semantic relations within word sequences. We present a new method that exploits a novel application of Inversion Transduction Grammar or ITG expressiveness constraints (Wu 1995 [1], Wu 1997 [2]) for mining monolingual data to obtain tight sentence translation pairs, yielding accuracy signiﬁcantly higher than previous known methods. We focus here on very non-parallel quasi-comparable monolingual corpora, which are available in far larger quantities but are signiﬁcantly more diﬃcult to mine than either noisy parallel corpora or comparable corpora. The majority of previous work has concerned noisy parallel corpora (sometimes imprecisely also called “comparable corpora”), which contain non-aligned sentences that are nevertheless mostly bilingual translations of the same doc"
I05-1023,J97-3002,1,0.819269,"But while loose translations by themselves already have numerous applications, truly parallel sentence translations provide invaluable types of information for the aforementioned types of mining and induction, which cannot easily be obtained from merely loose translations or comparable sentence pairs. In particular, truly parallel bi-sentences are especially useful for extracting more precise syntactic and semantic relations within word sequences. We present a new method that exploits a novel application of Inversion Transduction Grammar or ITG expressiveness constraints (Wu 1995 [1], Wu 1997 [2]) for mining monolingual data to obtain tight sentence translation pairs, yielding accuracy signiﬁcantly higher than previous known methods. We focus here on very non-parallel quasi-comparable monolingual corpora, which are available in far larger quantities but are signiﬁcantly more diﬃcult to mine than either noisy parallel corpora or comparable corpora. The majority of previous work has concerned noisy parallel corpora (sometimes imprecisely also called “comparable corpora”), which contain non-aligned sentences that are nevertheless mostly bilingual translations of the same document. More r"
I05-1023,P03-1019,0,0.0446484,"es. Conversely, ITGs should do well at rejecting candidates where (1) too many words in one sentence ﬁnd no correspondence in the other, (2) frames do not nest in similar ways in the candidate sentence pair, or (3) too many arguments must be transposed to achieve an alignment—all of which would suggest that the sentences probably express diﬀerent ideas. Various forms of empirical conﬁrmation for the ITG Hypothesis have emerged recently, which quantitatively support the qualitative cross-linguistic characteristics just described across a variety of language pairs and tasks. Zens and Ney (2003) [3] show that ITG constraints yield signiﬁcantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using Bracketing ITGs produces signiﬁcantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. With regard to translation rather than alignment accuracy, Zens"
I05-1023,C04-1060,0,0.0325904,"eve an alignment—all of which would suggest that the sentences probably express diﬀerent ideas. Various forms of empirical conﬁrmation for the ITG Hypothesis have emerged recently, which quantitatively support the qualitative cross-linguistic characteristics just described across a variety of language pairs and tasks. Zens and Ney (2003) [3] show that ITG constraints yield signiﬁcantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using Bracketing ITGs produces signiﬁcantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. With regard to translation rather than alignment accuracy, Zens et al. (2004) [7] show that decoding under ITG constraints yields signiﬁcantly lower word error rates and BLEU scores than the IBM constraints. Chiang (2005) [8] obtains signiﬁcant BLEU score improvements via unsupervised induction of hierarchical phrasal brac"
I05-1023,P01-1067,0,0.029135,", which quantitatively support the qualitative cross-linguistic characteristics just described across a variety of language pairs and tasks. Zens and Ney (2003) [3] show that ITG constraints yield signiﬁcantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using Bracketing ITGs produces signiﬁcantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. With regard to translation rather than alignment accuracy, Zens et al. (2004) [7] show that decoding under ITG constraints yields signiﬁcantly lower word error rates and BLEU scores than the IBM constraints. Chiang (2005) [8] obtains signiﬁcant BLEU score improvements via unsupervised induction of hierarchical phrasal bracketing ITGs. Such results partly motivate the work we discuss here. We will begin by surveying recent related work and reviewing the formal properties of ITGs. Subsequently we des"
I05-1023,P05-1059,0,0.0266738,"t the qualitative cross-linguistic characteristics just described across a variety of language pairs and tasks. Zens and Ney (2003) [3] show that ITG constraints yield signiﬁcantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using Bracketing ITGs produces signiﬁcantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. With regard to translation rather than alignment accuracy, Zens et al. (2004) [7] show that decoding under ITG constraints yields signiﬁcantly lower word error rates and BLEU scores than the IBM constraints. Chiang (2005) [8] obtains signiﬁcant BLEU score improvements via unsupervised induction of hierarchical phrasal bracketing ITGs. Such results partly motivate the work we discuss here. We will begin by surveying recent related work and reviewing the formal properties of ITGs. Subsequently we describe the architecture of our"
I05-1023,C04-1030,0,0.0190595,"constraints yield signiﬁcantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using Bracketing ITGs produces signiﬁcantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. With regard to translation rather than alignment accuracy, Zens et al. (2004) [7] show that decoding under ITG constraints yields signiﬁcantly lower word error rates and BLEU scores than the IBM constraints. Chiang (2005) [8] obtains signiﬁcant BLEU score improvements via unsupervised induction of hierarchical phrasal bracketing ITGs. Such results partly motivate the work we discuss here. We will begin by surveying recent related work and reviewing the formal properties of ITGs. Subsequently we describe the architecture of our new method, which relies on multiple stages so as to balance eﬃciency and accuracy considerations. Finally we discuss experimental results on a quas"
I05-1023,P05-1033,0,0.00610294,"-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using Bracketing ITGs produces signiﬁcantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. With regard to translation rather than alignment accuracy, Zens et al. (2004) [7] show that decoding under ITG constraints yields signiﬁcantly lower word error rates and BLEU scores than the IBM constraints. Chiang (2005) [8] obtains signiﬁcant BLEU score improvements via unsupervised induction of hierarchical phrasal bracketing ITGs. Such results partly motivate the work we discuss here. We will begin by surveying recent related work and reviewing the formal properties of ITGs. Subsequently we describe the architecture of our new method, which relies on multiple stages so as to balance eﬃciency and accuracy considerations. Finally we discuss experimental results on a quasi-comparable corpus of Chinese and English from the topic detection task. 2 Recent Approaches to Mining Non-parallel Corpora Recent work (Fung a"
I05-1023,W04-3208,1,0.93986,"cant BLEU score improvements via unsupervised induction of hierarchical phrasal bracketing ITGs. Such results partly motivate the work we discuss here. We will begin by surveying recent related work and reviewing the formal properties of ITGs. Subsequently we describe the architecture of our new method, which relies on multiple stages so as to balance eﬃciency and accuracy considerations. Finally we discuss experimental results on a quasi-comparable corpus of Chinese and English from the topic detection task. 2 Recent Approaches to Mining Non-parallel Corpora Recent work (Fung and Cheung 2004 [9]; Munteanu et al. 2004 [10]; Zhao and Vogel 2002 [11]) on extracting bi-sentences from comparable corpora is largely based on ﬁnding on-topic documents ﬁrst through similarity matching and time alignment. However, Zhao and Vogel used a corpus of Chinese and English versions of news stories from the Xinhua News agency, with “roughly similar sentence order 260 D. Wu and P. Fung of content”. This corpus can be more accurately described as a noisy parallel corpus. Munteanu et al. used comparable corpora of news articles published within the same 5-day window. In both cases, the corpora contain doc"
I05-1023,N04-1034,0,0.0267443,"ts via unsupervised induction of hierarchical phrasal bracketing ITGs. Such results partly motivate the work we discuss here. We will begin by surveying recent related work and reviewing the formal properties of ITGs. Subsequently we describe the architecture of our new method, which relies on multiple stages so as to balance eﬃciency and accuracy considerations. Finally we discuss experimental results on a quasi-comparable corpus of Chinese and English from the topic detection task. 2 Recent Approaches to Mining Non-parallel Corpora Recent work (Fung and Cheung 2004 [9]; Munteanu et al. 2004 [10]; Zhao and Vogel 2002 [11]) on extracting bi-sentences from comparable corpora is largely based on ﬁnding on-topic documents ﬁrst through similarity matching and time alignment. However, Zhao and Vogel used a corpus of Chinese and English versions of news stories from the Xinhua News agency, with “roughly similar sentence order 260 D. Wu and P. Fung of content”. This corpus can be more accurately described as a noisy parallel corpus. Munteanu et al. used comparable corpora of news articles published within the same 5-day window. In both cases, the corpora contain documents on the same matching"
I05-1023,P99-1043,1,0.841703,"Missing"
I05-1023,2003.mtsummit-papers.32,0,0.0300776,"o reﬁne document matching and parallel sentence extraction. Convergence. The IBM model parameters, including sentence alignment score and word alignment scores, are computed in each iteration. The parameter values eventually stay unchanged and the set of extracted bi-sentence candidates also converges to a ﬁxed size. The iteration then terminates and returns the last set of bilingual sentence pairs as the generated candidate sentences. 5 ITG Scoring The ITG model computes scores upon the set of candidates generated in the preceding stage. A variant of the approach used by Leusch et al. (2003) [16] allows us to forego training to estimate true probabilities; instead, rules are simply given unit weights. This allows the scores computed by ITG biparsing to be interpreted as a generalization of classical Levenshtein string edit distance, where inverted block transpositions are also allowed. Even without probability estimation, Leusch et al. found excellent correlation with human judgment of similarity between translated paraphrases. As mentioned earlier, biparsing for ITGs can be accomplished eﬃciently in polynomial time, rather than the exponential time required for classical SDTGs. The b"
I05-1023,J93-2003,0,\N,Missing
I05-1023,P00-1056,0,\N,Missing
I05-2021,W04-0821,0,0.0123974,"MT models. WER is defined as the percentage of words to be inserted, deleted or replaced in the translation in order to obtain the sentence of reference. However, WER does not isolate WSD performance since it also encompasses many other types of errors. Also, since the choice of a translation for a particular word affects the translation of other words in the sentence, the effect of WSD performance on WER is unclear. In contrast, the Senseval accuracy metric counts each incorrect translation choice only once. Apart from the voted WSD system described in section 4, and the unsupervised system (Cabezas et al., 2004) mentioned in section 6.4, systems built and optimized for the Senseval-3 Chinese lexical sample task, include Niu et al. (2004). Many of the Senseval-type WSD system are not language specific and the presentation of the results in the English lexical sample task (Midhalcea et al., 2004), English-Hindi multilingual task (Chklovski et al., 2004), or any of the lexical sample tasks defined in other languages, give a good overview of the variety of approaches to WSD. Most previous work on multilingual WSD has focused on the different problem of exploiting bilingual resources (e.g., parallel or co"
I05-2021,P05-1048,1,0.736175,"t explicitly addressed. However, recent progress in machine translation and the continuous improvement on evaluation metrics such as BLEU (Papineni et al., 2002) suggest that SMT systems are already very good at choosing correct word translations. BLEU score with low order n-grams can be seen as an evaluation of the translation adequacy, which suggests that as SMT systems achieve higher BLEU score, their ability to disambiguate word translations improves. In other work, we have been conducting comparative studies testing whether state-of-the-art WSD models can improve SMT translation quality (Carpuat and Wu, 2005). Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system, we found that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone. The surprising difficulty of this challenge might suggest that SMT models are sufficiently strong at word level disambiguation on their own, and has recently encouraged speculation that SMT performs WSD as well as the dedicated WSD models. The studies described in this paper are aimed at directly testing th"
I05-2021,W04-0822,1,0.61499,"which implements an efficient greedy decoding algorithm, is used to translate the Chinese sentences, using the alignment model and language model previously described. Notice that very little contextual information is available to the IBM SMT models. Lexical choice during decoding essentially depends on the translation probabilities learned for the target word, and on the English language model scores. 4 The WSD system The WSD system used here is based on the model that achieved the best performance on the Senseval-3 Chinese lexical sample task, outperforming other systems by a large margin (Carpuat et al., 2004). The model consists of an ensemble of four highly accurate classifiers combined by majority vote: a naive Bayes classifier, a maximum entropy model (Jaynes, 1978), a boosting model (Freund and Schapire, 1997), and a Kernel PCA-based model (Wu et al., 2004), which has the advantage of having a signficantly different bias. All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant. Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent. The feature set used consists"
I05-2021,W04-0802,0,0.200134,"et senses for a set of new occurrences of the target word in context. We use the SMT sytem described in Section 3 to translate the Chinese sentences of the Senseval evaluation test set, and extract the translation chosen for each 122 of the target word occurrences. In order to evaluate the predictions of the SMT model just like any WSD model, we need to map the English translations to HowNet senses. This mapping is done using HowNet, which provides English glosses for each of the senses of every Chinese word. Note that Senseval-3 also defined a translation or multilingual lexical sample task (Chklovski et al., 2004), which is just like the English lexical sample task, except that the WSD systems are expected to predict Hindi translations instead of WordNet senses. This translation task might seem to be a more natural evaluation framework for SMT than the monolingual Chinese lexical sample task. However, in practice, there is very little data available to train an Englishto-Hindi SMT model, which would significantly hinder its performance and bias the study in favor of the dedicated WSD models. 5.2 Allowing the SMT model to exploit the Senseval data Comparing the Senseval WSD models with a regular SMT mod"
I05-2021,P04-1039,0,0.0448296,"ovski et al., 2004), or any of the lexical sample tasks defined in other languages, give a good overview of the variety of approaches to WSD. Most previous work on multilingual WSD has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full MT systems) to help WSD. For instance, Ng et al. (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a translation disambiguation WSD model, and Diab (2004) who exploited large amounts of automatically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. In all this work, the goal is to achieve accurate WSD with minimum amounts of annotated data. Again, this differs from our objective which is to directly evaluate an SMT model as a WSD model. 8 Conclusion We presented empirical results casting doubt on the increasingly common assumption that SMT models are very good at WSD, even though they do not explicitly address WSD as an independent task. Using the Senseval-3 Chinese lexical sample task as a testbed, we"
I05-2021,N03-1010,0,0.0172677,"nsists of about 1 million sentences from the United Nations Chinese-English parallel corpus from LDC. This corpus was automatically sentence-aligned, so the training data does not require as much manual annotation as for the WSD model. 3.2 Language model The English language model is a trigram model trained on the Gigaword newswire data and on the English side of the UN and Xinhua parallel corpora. The language model is also trained using a publicly available software, the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). 3.3 Decoding The ISI ReWrite decoder (Germann, 2003), which implements an efficient greedy decoding algorithm, is used to translate the Chinese sentences, using the alignment model and language model previously described. Notice that very little contextual information is available to the IBM SMT models. Lexical choice during decoding essentially depends on the translation probabilities learned for the target word, and on the English language model scores. 4 The WSD system The WSD system used here is based on the model that achieved the best performance on the Senseval-3 Chinese lexical sample task, outperforming other systems by a large margin"
I05-2021,P02-1044,0,0.046374,"n of the results in the English lexical sample task (Midhalcea et al., 2004), English-Hindi multilingual task (Chklovski et al., 2004), or any of the lexical sample tasks defined in other languages, give a good overview of the variety of approaches to WSD. Most previous work on multilingual WSD has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full MT systems) to help WSD. For instance, Ng et al. (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a translation disambiguation WSD model, and Diab (2004) who exploited large amounts of automatically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. In all this work, the goal is to achieve accurate WSD with minimum amounts of annotated data. Again, this differs from our objective which is to directly evaluate an SMT model as a WSD model. 8 Conclusion We presented empirical results casting doubt on the increasingly common assumption that SMT models are very good at WSD, even though they do not exp"
I05-2021,W04-0807,0,0.0478446,"slation for a particular word affects the translation of other words in the sentence, the effect of WSD performance on WER is unclear. In contrast, the Senseval accuracy metric counts each incorrect translation choice only once. Apart from the voted WSD system described in section 4, and the unsupervised system (Cabezas et al., 2004) mentioned in section 6.4, systems built and optimized for the Senseval-3 Chinese lexical sample task, include Niu et al. (2004). Many of the Senseval-type WSD system are not language specific and the presentation of the results in the English lexical sample task (Midhalcea et al., 2004), English-Hindi multilingual task (Chklovski et al., 2004), or any of the lexical sample tasks defined in other languages, give a good overview of the variety of approaches to WSD. Most previous work on multilingual WSD has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full MT systems) to help WSD. For instance, Ng et al. (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a tra"
I05-2021,P03-1058,0,0.0555526,"l-3 Chinese lexical sample task, include Niu et al. (2004). Many of the Senseval-type WSD system are not language specific and the presentation of the results in the English lexical sample task (Midhalcea et al., 2004), English-Hindi multilingual task (Chklovski et al., 2004), or any of the lexical sample tasks defined in other languages, give a good overview of the variety of approaches to WSD. Most previous work on multilingual WSD has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full MT systems) to help WSD. For instance, Ng et al. (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a translation disambiguation WSD model, and Diab (2004) who exploited large amounts of automatically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. In all this work, the goal is to achieve accurate WSD with minimum amounts of annotated data. Again, this differs from our objective which is to directly evaluate an SMT model as a WSD model. 8 Conclusion We pre"
I05-2021,W04-0847,0,0.022527,"entence of reference. However, WER does not isolate WSD performance since it also encompasses many other types of errors. Also, since the choice of a translation for a particular word affects the translation of other words in the sentence, the effect of WSD performance on WER is unclear. In contrast, the Senseval accuracy metric counts each incorrect translation choice only once. Apart from the voted WSD system described in section 4, and the unsupervised system (Cabezas et al., 2004) mentioned in section 6.4, systems built and optimized for the Senseval-3 Chinese lexical sample task, include Niu et al. (2004). Many of the Senseval-type WSD system are not language specific and the presentation of the results in the English lexical sample task (Midhalcea et al., 2004), English-Hindi multilingual task (Chklovski et al., 2004), or any of the lexical sample tasks defined in other languages, give a good overview of the variety of approaches to WSD. Most previous work on multilingual WSD has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full MT systems) to help WSD. For instance, Ng et al. (2003) showed that it is possible to use word al"
I05-2021,J03-1002,0,0.00440369,"y the language model. That is, the predictions benefit from the sentential context of the target language. This has the general effect of improving translation fluency. Another major difference with most lexical sample WSD models is that SMT models are always unsupervised. SMT models learn from large sets of bisentences but the correct word alignment between the two sentences is unknown. SMT models cannot therefore 121 To build a representative baseline SMT system, we restricted ourselves to making use of freely available tools. 3.1 Alignment model The alignment model was trained with GIZA++ (Och and Ney, 2003), which implements the most typical IBM and HMM alignment models. Translation quality could be improved using more advanced hybrid phrasal or tree models, but this would interfere with the questions being investigated here. The alignment model used is IBM-4, as required by our decoder. The training scheme is IBM-1, HMM, IBM-3 and IBM-4, as specified in (Och and Ney, 2003). The training corpus consists of about 1 million sentences from the United Nations Chinese-English parallel corpus from LDC. This corpus was automatically sentence-aligned, so the training data does not require as much manual"
I05-2021,P02-1040,0,0.0780761,"WSD either explicitly or implicitly. Since the Senseval models have been built and optimized specifically to address the WSD problems, they typically use richer disambiguating information than SMT systems. This, however, raises the question of whether the sophisticated WSD models are in fact needed in practice. In many machine translation architectures, in particular most current statistical machine translation (SMT) models, the WSD problem is typically not explicitly addressed. However, recent progress in machine translation and the continuous improvement on evaluation metrics such as BLEU (Papineni et al., 2002) suggest that SMT systems are already very good at choosing correct word translations. BLEU score with low order n-grams can be seen as an evaluation of the translation adequacy, which suggests that as SMT systems achieve higher BLEU score, their ability to disambiguate word translations improves. In other work, we have been conducting comparative studies testing whether state-of-the-art WSD models can improve SMT translation quality (Carpuat and Wu, 2005). Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system,"
I05-2021,P98-2230,1,0.914616,"Missing"
I05-2021,P04-1081,1,0.746599,"oice during decoding essentially depends on the translation probabilities learned for the target word, and on the English language model scores. 4 The WSD system The WSD system used here is based on the model that achieved the best performance on the Senseval-3 Chinese lexical sample task, outperforming other systems by a large margin (Carpuat et al., 2004). The model consists of an ensemble of four highly accurate classifiers combined by majority vote: a naive Bayes classifier, a maximum entropy model (Jaynes, 1978), a boosting model (Freund and Schapire, 1997), and a Kernel PCA-based model (Wu et al., 2004), which has the advantage of having a signficantly different bias. All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant. Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent. The feature set used consists of position-sensitive, syntactic, and local collocational features, as described by Yarowsky and Florian (2002). 5 Experimental method 5.1 Senseval-3 Chinese lexical sample task The Senseval-3 Chinese lexical sample task includes 20 target word types. For"
I05-2021,C98-2225,1,\N,Missing
I11-1047,1998.amta-tutorials.5,0,0.198481,"Missing"
I11-1047,C10-1054,0,0.0292649,"n the target language, the system simply returns &lt;not-found&gt;. 2.1 Representing Source Document We cannot enter documents with thousands of words directly into an online search engine. We need to convert full text into keywords to perform automated queries. A keyword may exist in multiple articles. However, several keywords cam uniquely identify a document if they are grouped together as a keyword set (Jiang et al., 2009). We then translate each keyword to target language to form the initial query. There are several reasons why using the translated keyword set as query directly, as proposed by Hong et al. (2010), does not always yields the desired target document: 1) Keyword translation might not correspond to the actual words in the target document; 2) Certain keywords in the target document might have been removed by content editors; 3) There are errors in keyword translation or selection. It is essential to select appropriate keywords to find the desired target document in a search engine. Two conditions that an appropriate keyword set should satisfy are: (1) they should represent the document exclusively (Jiang et al., 2009) (2) they should have unique or common translation in both languages. We"
I11-1047,W06-1710,0,0.0758253,"Missing"
I11-1047,ma-2006-champollion,0,0.0653889,"Missing"
I11-1047,J05-4003,0,0.0870527,"Missing"
I11-1047,J03-3002,0,0.0532487,"cuments from the web. We suggest that parallel documents can be mined with high precision from web sites that are not necessarily parallel to each other. Parallel resources reside on a diverse range of websites which can be classified into the following categories: Parallel websites: single website with structurally aligned bilingual pages. Typically they are websites of institutions, governments and commercial companies. (e.g. Financial Times Chinese/English, Wall Street Journal Chinese/English). Structure based methods were previously proposed to mine parallel documents from these websites: Resnik and Smith (2003) used (1) parent pages containing links to versions of one document in different languages and (2) sibling pages contains link to translation of the current documents. They also rely on the URL and anchor text to spot language specific version of documents. A structural alignment using DOM tree representation was proposed by Shi et al. (2006) to align parallel documents by using HTML structure. They identify the translational equivalent texts and hyperlinks between two parallel DOM trees to find parallel documents. However, the web is a heterogeneous collection of documents that extend far bey"
I11-1047,P06-1062,0,0.0236433,"ly they are websites of institutions, governments and commercial companies. (e.g. Financial Times Chinese/English, Wall Street Journal Chinese/English). Structure based methods were previously proposed to mine parallel documents from these websites: Resnik and Smith (2003) used (1) parent pages containing links to versions of one document in different languages and (2) sibling pages contains link to translation of the current documents. They also rely on the URL and anchor text to spot language specific version of documents. A structural alignment using DOM tree representation was proposed by Shi et al. (2006) to align parallel documents by using HTML structure. They identify the translational equivalent texts and hyperlinks between two parallel DOM trees to find parallel documents. However, the web is a heterogeneous collection of documents that extend far beyond bilingual and comparable pages with obvious structural features, such as similar URLs or common titles. Structural features only work for bilingual websites or document pairs that are already linked by editors. Comparable websites: websites that contain parallel content in different languages without any structural relation between docume"
I11-1047,C10-1124,0,0.0345992,"Missing"
I11-1047,P11-1133,1,0.874666,"Missing"
I11-1047,P06-1011,0,0.0608623,"Missing"
I11-1047,E09-1003,0,\N,Missing
I11-1047,P02-1040,0,\N,Missing
I13-1165,N10-1028,0,0.0806238,"on for this line of inquiry may be necessary. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Although this allows us to tap into the vast engineering efforts that have gone into tweaking existing decoders, it also prevents us from understanding the quality of the learned transduction grammar, whose characteristics become obscured by the many unrelated variables in the subsequent process"
I13-1165,P09-1088,0,0.239881,"Missing"
I13-1165,N10-1015,0,0.0136058,"ry may be necessary. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Although this allows us to tap into the vast engineering efforts that have gone into tweaking existing decoders, it also prevents us from understanding the quality of the learned transduction grammar, whose characteristics become obscured by the many unrelated variables in the subsequent processing pipeline. Our own p"
I13-1165,P95-1031,0,0.0367051,", any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. The motivation for our present series of experiments is that as a field we are well served by tackling the fundamental questions as well, and not exclusively focusing on engineering short term incremental BLEU score boosts where the quality of an induced ITG itself is obscured because it is embedded within many other heuristic algorithms. Bayesian approaches to grammar induction have a long history in computation linguistics. Starting with monolingual grammar induction (Chen, 1995; Stolcke and Omohundro, 1994), and moving on to transduction grammar induction (Blunsom et al., 2008, 2009; Blunsom and Cohn, 2010; Neubig et al., 2011, 2012). So far, the induced transduction grammar have only been used to derive Viterbi-style word alignments to feed into existing translation system, and there has been no evaluation of the grammars actually learned. In contrast, we directly evaluate the grammars that we induce. Our algorithm for learning the structure of an ITG relies on segmenting known bilingual segments, starting with the sentence pairs of the training data, and continuin"
I13-1165,W07-0403,0,0.021434,"lt into the formalism (although we will not make use of this feature in this work). A word as to the bigger-picture motivation for this line of inquiry may be necessary. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Although this allows us to tap into the vast engineering efforts that have gone into tweaking existing decoders, it also prevents us from understanding the quality of the"
I13-1165,P05-1033,0,0.0344124,"ent estimation of the model prior and the likelihood of the data given a limited change in the model—it is, in other words, possible to generate a set of possible rule segmentations and compare them using the model posteriors or description length. Our new approach differs from most other SMT approaches to unsupervised learning of phrasal translations, which (a) require enormous amounts of run-time memory, (b) contain a high degree of redundancy, and (c) do not provide an obvious basis for generalization to abstract translation schemas. The current state-of-the-art in SMT (Koehn et al., 2003; Chiang, 2005) relies on long pipelines of mismatched learning models and heuristics. There is no way for latter stages of the pipeline to recover a mistake of ommission made in an earlier stage, which forces the individual steps to massively overgenerate hypotheses. This typically manifests as massive redundancy in the phrasal lexicon, which causes significant overhead at run-time. The fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates these deficiencies well. By s"
I13-1165,J07-2003,0,0.133658,"007), which contains 46,867 sentence pairs of training data, and 489 Chinese sentences with 6 English reference translations each as test data; all the sentences are taken from the traveling domain. Since the Chinese is written without whitespace, we use a tool which tries to clump characters together into more “wordlike” sequences (Wu, 1999). After each induction iteration there is a fullyfunctional grammar that we can test as a translation system. For this, we use our in-house ITG decoder, which uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) with cube pruning (Chiang, 2007) to integrate the language model scores. We use SRILM (Stolcke, 2002) to train a trigram language model on the English side of the training data. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002), and NIST (Doddington, 2002), and compare the results against our bottom-up oriented chunking ITG induction approach (Saers et al., 2012). 1164 50 40 30 20 10 0 0 1 2 3 4 Iterations 5 6 7 (a) Rule count Probability in log domain (Mbit) Number of rules (thousands) 60 Table 1: Translation results of the baseline, the initial model and the model after n interations. 14 12 10 Sys"
I13-1165,P06-1121,0,0.0143135,"inuing with the segments learned in this way. This is similar to the Recursive Alignment Model, or MAR (Vilar, 2005; Vilar and Vidal, 2005). Our method is, however, learning a full ITG, where MAR only learns a translation lexicon; furthermore, MAR is a discriminative model, whereas ours is a generative. Transduction grammars can also be induced from treebanks instead of unannotated corpora, which cuts down the vast search space by enforcing additional, external constraints—taking it from the realm of unsupervised induction into the realm of supervised induction. This approach was pioneered by Galley et al. (2006) with numerous variants in subsequent research, usually referred to as tree-to-tree, tree-to-string and string-to-tree, depending on where the analyses are found in the training data. Our view on this line of research is that it complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. The work presented in this paper is related to our preliminary"
I13-1165,P09-1104,0,0.0190238,"his work). A word as to the bigger-picture motivation for this line of inquiry may be necessary. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Although this allows us to tap into the vast engineering efforts that have gone into tweaking existing decoders, it also prevents us from understanding the quality of the learned transduction grammar, whose characteristics become obscured by th"
I13-1165,D07-1103,0,0.0150731,"rent state-of-the-art in SMT (Koehn et al., 2003; Chiang, 2005) relies on long pipelines of mismatched learning models and heuristics. There is no way for latter stages of the pipeline to recover a mistake of ommission made in an earlier stage, which forces the individual steps to massively overgenerate hypotheses. This typically manifests as massive redundancy in the phrasal lexicon, which causes significant overhead at run-time. The fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates these deficiencies well. By staying within a single framework throughout training and testing, we do not have to overgenerate hypotheses—instead, we are able to evaluate their effect on the posterior model probability at the time they are proposed during learning. This cuts down the size of the phrasal lexicon significantly, and consequently saves the decoder a lot of run-time resources. The fact that we learn a phrasal inversion transduction grammar, or ITG (Wu, 1997) also means that the power to generalize and abstract over categories is built into the formalism (although we wil"
I13-1165,N03-1017,0,0.0348063,"Missing"
I13-1165,P12-1018,0,0.0227958,"Missing"
I13-1165,P11-1064,0,0.0257227,"Missing"
I13-1165,P02-1040,0,0.0887694,"nese is written without whitespace, we use a tool which tries to clump characters together into more “wordlike” sequences (Wu, 1999). After each induction iteration there is a fullyfunctional grammar that we can test as a translation system. For this, we use our in-house ITG decoder, which uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) with cube pruning (Chiang, 2007) to integrate the language model scores. We use SRILM (Stolcke, 2002) to train a trigram language model on the English side of the training data. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002), and NIST (Doddington, 2002), and compare the results against our bottom-up oriented chunking ITG induction approach (Saers et al., 2012). 1164 50 40 30 20 10 0 0 1 2 3 4 Iterations 5 6 7 (a) Rule count Probability in log domain (Mbit) Number of rules (thousands) 60 Table 1: Translation results of the baseline, the initial model and the model after n interations. 14 12 10 System baseline initial iteration 1 iteration 2 iteration 3 iteration 4 iteration 5 iteration 6 iteration 7 8 6 4 2 0 0 1 2 3 4 Iterations 5 6 7 (b) Impact of model structure Figure 1: Number of rules (a), and the impact of"
I13-1165,P10-1017,0,0.0181855,"insisting on the fundamental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Although this allows us to tap into the vast engineering efforts that have gone into tweaking existing decoders, it also prevents us from understanding the quality of the learned transduction grammar, whose characteristics become obscured by the many unrelated variables in the subsequent processing pipeline. Our own past work has also taken"
I13-1165,C12-1142,1,0.852343,"ach induction iteration there is a fullyfunctional grammar that we can test as a translation system. For this, we use our in-house ITG decoder, which uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) with cube pruning (Chiang, 2007) to integrate the language model scores. We use SRILM (Stolcke, 2002) to train a trigram language model on the English side of the training data. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002), and NIST (Doddington, 2002), and compare the results against our bottom-up oriented chunking ITG induction approach (Saers et al., 2012). 1164 50 40 30 20 10 0 0 1 2 3 4 Iterations 5 6 7 (a) Rule count Probability in log domain (Mbit) Number of rules (thousands) 60 Table 1: Translation results of the baseline, the initial model and the model after n interations. 14 12 10 System baseline initial iteration 1 iteration 2 iteration 3 iteration 4 iteration 5 iteration 6 iteration 7 8 6 4 2 0 0 1 2 3 4 Iterations 5 6 7 (b) Impact of model structure Figure 1: Number of rules (a), and the impact of changes in the model structure (b) during the structure induction phase. The change in model structure is broken down into the model prior"
I13-1165,W13-0806,1,0.631774,"lly referred to as tree-to-tree, tree-to-string and string-to-tree, depending on where the analyses are found in the training data. Our view on this line of research is that it complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. The work presented in this paper is related to our preliminary work with description length as learning objective (Saers et al., 2013b). A key difference lies in the added parameter training, which facilitates a completely Bayesian interpretation. The present paper aims to be self-contained, by explaining the relationships throughout. 2 Background In this section we briefly survey essential foundations for inversion transduction grammars and description length together with its Bayesian interpretation—in other words, what we search for, and how. 2.1 Inversion transduction grammars Inversion transduction grammars, or ITGs (Wu, 1997), are an expressive yet efficient way to model translation. Much like context-free grammars (C"
I13-1165,W13-2810,1,0.317916,"Missing"
I13-1165,N10-1050,1,0.833847,"ental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Although this allows us to tap into the vast engineering efforts that have gone into tweaking existing decoders, it also prevents us from understanding the quality of the learned transduction grammar, whose characteristics become obscured by the many unrelated variables in the subsequent processing pipeline. Our own past work has also taken similar approaches, b"
I13-1165,2011.eamt-1.42,1,0.868685,"Missing"
I13-1165,W05-0815,0,0.0175858,"08, 2009; Blunsom and Cohn, 2010; Neubig et al., 2011, 2012). So far, the induced transduction grammar have only been used to derive Viterbi-style word alignments to feed into existing translation system, and there has been no evaluation of the grammars actually learned. In contrast, we directly evaluate the grammars that we induce. Our algorithm for learning the structure of an ITG relies on segmenting known bilingual segments, starting with the sentence pairs of the training data, and continuing with the segments learned in this way. This is similar to the Recursive Alignment Model, or MAR (Vilar, 2005; Vilar and Vidal, 2005). Our method is, however, learning a full ITG, where MAR only learns a translation lexicon; furthermore, MAR is a discriminative model, whereas ours is a generative. Transduction grammars can also be induced from treebanks instead of unannotated corpora, which cuts down the vast search space by enforcing additional, external constraints—taking it from the realm of unsupervised induction into the realm of supervised induction. This approach was pioneered by Galley et al. (2006) with numerous variants in subsequent research, usually referred to as tree-to-tree, tree-to-st"
I13-1165,W05-0835,0,0.0312758,"nsom and Cohn, 2010; Neubig et al., 2011, 2012). So far, the induced transduction grammar have only been used to derive Viterbi-style word alignments to feed into existing translation system, and there has been no evaluation of the grammars actually learned. In contrast, we directly evaluate the grammars that we induce. Our algorithm for learning the structure of an ITG relies on segmenting known bilingual segments, starting with the sentence pairs of the training data, and continuing with the segments learned in this way. This is similar to the Recursive Alignment Model, or MAR (Vilar, 2005; Vilar and Vidal, 2005). Our method is, however, learning a full ITG, where MAR only learns a translation lexicon; furthermore, MAR is a discriminative model, whereas ours is a generative. Transduction grammars can also be induced from treebanks instead of unannotated corpora, which cuts down the vast search space by enforcing additional, external constraints—taking it from the realm of unsupervised induction into the realm of supervised induction. This approach was pioneered by Galley et al. (2006) with numerous variants in subsequent research, usually referred to as tree-to-tree, tree-to-string and string-to-tree,"
I13-1165,W95-0106,1,0.598849,"complete model P (D|ΦG , ΦS , θΦ )). The latter adjusts the model parameters θΦ to optimize the posterior (thus affecting the prior over the parameters P (θΦ |ΦS , ΦG ) and again P (D|ΦG , ΦS , θΦ )), assuming the model structure ΦS to be fixed, as well as ΦG which remains fixed as bracketing inversion transduction grammars. The prior is a symmetric Dirichlet distribution over rule right-hand sides given rule left-hand sides. To get the conditional, we have to biparse the training data, and to maximize it, we perform expectation maximization (Dempster et al., 1977), as specified for ITGs by (Wu, 1995) with the caveat that we increase all the fractional counts by one before normalizing. The biparsing is done with our in-house implementation of the cubic time biparsing algorithm described in Saers et al. (2009), with a beam width of 100. 5 Experimental setup To test the viability of the idea of starting with a very specific ITG consisting of long rules, and iteratively segmenting the rules to induce a more general ITG under a MAP or MDL objective, we have implemented the steps detailed in Sections 3 and 4; in this section we will describe in greater detail the exact experimental conditions o"
I13-1165,J97-3002,1,0.821419,"a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates these deficiencies well. By staying within a single framework throughout training and testing, we do not have to overgenerate hypotheses—instead, we are able to evaluate their effect on the posterior model probability at the time they are proposed during learning. This cuts down the size of the phrasal lexicon significantly, and consequently saves the decoder a lot of run-time resources. The fact that we learn a phrasal inversion transduction grammar, or ITG (Wu, 1997) also means that the power to generalize and abstract over categories is built into the formalism (although we will not make use of this feature in this work). A word as to the bigger-picture motivation for this line of inquiry may be necessary. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the"
I13-1165,W09-3804,1,\N,Missing
I13-1165,2007.iwslt-1.1,0,\N,Missing
I13-1165,W09-2304,1,\N,Missing
I13-1165,P08-1012,0,\N,Missing
J88-4003,J88-4003,1,0.0513221,"Missing"
J88-4003,P84-1063,0,0.285638,"e other hand, 'rm' should be connected to the goal of deleting a file by a planfor relation, since this goal is what 'rm' is typically used for. Traditional approaches to dialog understanding have focused on the process of plan inference. Under this approach, utterances are viewed as steps of plans. Such plans may themselves be parts of higher-level plans, and so on. Allen and Perrault (1980) developed a system that exemplifies this approach. Their system handled direct and indirect speech acts by plan analysis. Carberry (1983) extended this paradigm to deal more thoroughly with domain plans. Litman and Allen (1984) used the notion of metaplans (Wilensky 1983) to facilitate the comprehension of subdialogs. Grosz and Sidner (1985) pointed out the need for attentional knowledge in understanding discourse. One problem that has persisted in the literature is an inadequate representation of the relationship between goals and plans. Planfors provide such a representation. Planfors allow a goal analysis mechanism to combine certain inferences that should be kept together. First, inferences about plans may be made at the same time as those about goals. This is in contrast with systems such as Wilensky's PAM syst"
J97-3002,J90-2002,0,0.363987,"hereby sidestepping the limitations of traditional transduction grammars. In tandem with the concept of bilingual language-modeling, we propose the concept of bilingual parsing, where the input is a sentence-pair rather than a sentence. Though inversion transduction grammars remain inadequate as full-fledged translation models, bilingual parsing with simple inversion transduction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be gr"
J97-3002,J93-2003,0,0.09798,"h the concept of bilingual language-modeling, we propose the concept of bilingual parsing, where the input is a sentence-pair rather than a sentence. Though inversion transduction grammars remain inadequate as full-fledged translation models, bilingual parsing with simple inversion transduction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be grammatical, in keeping with the spirit of robust parsing. The formalism's uniform integr"
J97-3002,P91-1022,0,0.108138,"Missing"
J97-3002,W93-0305,0,0.039629,"Missing"
J97-3002,P93-1002,0,0.408463,"o it is up to the parser to decide when to break up potential collocations into individual words. The problem is particularly acute for English and Chinese because word boundaries are not orthographically marked in Chinese text, so not even a default chunking exists upon which word matchings could be postulated. (Sentences (2) and (5) demonstrate why the obvious trick of taking single characters as words is not a workable strategy.) The usual Chinese NLP architecture first preprocesses input text through a word segmentation module (Chiang et al. 1992; Lin, Chiang, and Su 1992, 1993; Chang and Chen 1993; Wu and Tseng 1993; Sproat et al. 1994; Wu and Fung 1994), but, clearly, bilingual parsing will be hampered by any errors arising from segmentation ambiguities that could not be resolved in the isolated monolingual context because even if the Chinese segmentation is acceptable monolingually, it may not agree with the words present in the English sentence. Matters are made still worse by unpredictable omissions in the translation lexicon, even for valid compounds. We therefore extend the algorithm to optimize the Chinese sentence segmentation in conjunction with the bracketing process. Note th"
J97-3002,O92-1003,0,0.0266253,"ences do not come broken into appropriately matching chunks, so it is up to the parser to decide when to break up potential collocations into individual words. The problem is particularly acute for English and Chinese because word boundaries are not orthographically marked in Chinese text, so not even a default chunking exists upon which word matchings could be postulated. (Sentences (2) and (5) demonstrate why the obvious trick of taking single characters as words is not a workable strategy.) The usual Chinese NLP architecture first preprocesses input text through a word segmentation module (Chiang et al. 1992; Lin, Chiang, and Su 1992, 1993; Chang and Chen 1993; Wu and Tseng 1993; Sproat et al. 1994; Wu and Fung 1994), but, clearly, bilingual parsing will be hampered by any errors arising from segmentation ambiguities that could not be resolved in the isolated monolingual context because even if the Chinese segmentation is acceptable monolingually, it may not agree with the words present in the English sentence. Matters are made still worse by unpredictable omissions in the translation lexicon, even for valid compounds. We therefore extend the algorithm to optimize the Chinese sentence segmentatio"
J97-3002,P93-1001,0,0.0394564,"In tandem with the concept of bilingual language-modeling, we propose the concept of bilingual parsing, where the input is a sentence-pair rather than a sentence. Though inversion transduction grammars remain inadequate as full-fledged translation models, bilingual parsing with simple inversion transduction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be grammatical, in keeping with the spirit of robust parsing. The formali"
J97-3002,C94-1014,0,0.0459478,"Missing"
J97-3002,A94-1006,0,0.0342208,"is difficult because correct matchings are not usually linearly ordered, i.e., there are crossings. Without some additional constraints, any word position in the source sentence can be matched to any position in the target sentence, an assumption that leads to high error rates. More sophisticated word alignment algorithms therefore attempt to model the intuition that proximate constituents in close relationships in one language remain proximate in the other. The later IBM models are formulated to prefer collocations (Brown et al. 1993). In the case of word_align (Dagan, Church, and Gale 1993; Dagan and Church 1994), a penalty is imposed according to the deviation from an ideal matching, as constructed by linear interpolation? From this point of view, the proposed technique is a word alignment method that imposes a more realistic distortion penalty. The tree structure reflects the assumption that crossings should not be penalized as long as they are consistent with constituent structure. Figure 7 gives theoretical upper bounds on the matching flexibility as the lengths of the sequences increase, where the constituent structure constraints are reflected by high flexibility up to length-4 sequences and a r"
J97-3002,W93-0301,0,0.0696346,"Missing"
J97-3002,P81-1022,0,0.194051,"never participate in any ML bibracketing. The running time reduction in this case depends heavily on the domain constraints. We have found this strategy to be useful for incorporating punctuation constraints. Certain punctuation characters give constituency indications with high reliability; ""perfect separators"" include colons and Chinese full stops, while ""perfect delimiters"" include parentheses and quotation marks. 10. Unrestricted-Form G r a m m a r s It is possible to construct a parser that accepts unrestricted-form, rather than normalform, grammars. In this case an Earley-style scheme (Earley 1970), employing an active chart, can be used. The time complexity remains the same as the normal-form case. We have found this to be useful in practice. For bracketing grammars of the type considered in this paper, there is no advantage. However, for more complex, linguistically structured grammars, the more flexible parser does not require the unreasonable numbers of productions that can easily arise from normal-form requirements. For most grammars, we have found performance to be comparable or faster than the normalform parser. 11. C o n c l u s i o n The twin concepts of bilingual language mode"
J97-3002,C94-2178,0,0.029819,"simple inversion transduction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be grammatical, in keeping with the spirit of robust parsing. The formalism's uniform integration of various types of bracketing and alignment constraints is one of its chief strengths. The paper is divided into two main parts. We begin in the first part below by laying out the basic formalism, then show that reduction to a normal form is possible. We then ra"
J97-3002,1994.amta-1.11,0,0.0966312,"turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be grammatical, in keeping with the spirit of robust parsing. The formalism's uniform integration of various types of bracketing and alignment constraints is one of its chief strengths. The paper is divided into two main parts. We begin in the first part below by laying out the basic formalism, then show that reduction to a normal form is possible. We then raise several desiderata for the expressive"
J97-3002,P91-1023,0,0.0183183,"the limitations of traditional transduction grammars. In tandem with the concept of bilingual language-modeling, we propose the concept of bilingual parsing, where the input is a sentence-pair rather than a sentence. Though inversion transduction grammars remain inadequate as full-fledged translation models, bilingual parsing with simple inversion transduction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be grammatical, in keeping"
J97-3002,1992.tmi-1.9,0,0.0450721,"Missing"
J97-3002,C92-2101,0,0.0152655,"Missing"
J97-3002,J94-3001,0,0.0275527,"view is more appropriate for our applications because the roles of the two languages are symmetrical, in contrast to the usual applications of syntax-directed transduction grammars. Moreover, the input-output view works better when a machine for accepting one of the languages (the input language) has a high degree of determinism, which is not the case here. Our transduction model is context-free, rather than finite-state. Finite-state transducers, or FSTs, are well known to be useful for specific tasks such as analysis of inflectional morphology (Koskenniemi 1983), text-to-speech conversion (Kaplan and Kay 1994), and nominal, number, and temporal phrase normalization (Gazdar and Mellish 1989). FSTs may also be used to parse restricted classes of context-free grammars (Pereira 1991; Roche 1994; Laporte 1996). However, the bilingual corpus analysis tasks we consider in this paper are quite different from the tasks for which FSTs are apparently well suited. Our domain is broader, and the model possesses very little a priori specific structural knowledge of the language. As a stepping stone to inversion transduction grammars, we first consider what a context-free model known as a simple transduction gram"
J97-3002,C88-2142,0,0.0439426,"Missing"
J97-3002,P93-1003,0,0.036229,"xamples are only at the sentential level. It can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy (Kay and Ri3cheisen 1988; Catizone, Russel, and Warwick 1989; Gale and Church 1991; Brown, Lai, and Mercer 1991; Chen 1993), even for languages as disparate as Chinese and English (Wu 1994). Algorithms for subsentential alignment have been developed as well as granularities of the character (Church 1993), word (Dagan, Church, and Gale 1993; Fung and Church 1994; Fung and McKeown 1994), collocation (Smadja 1992), and specially segmented (Kupiec 1993) levels. However, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial problem, due to the added complexity of dealing with constituent structure. Manual phrasal matching is feasible only for small corpora, either for toy-prototype testing or for narrowly restricted applications. Automatic approaches to identification of subsentential translation units have largely followed what we might call a ""parse-parse-match"" procedure. Each half of the parallel corpus is first parsed individually using a monolingual grammar. Subsequently, the co"
J97-3002,O92-1001,0,0.0421733,"Missing"
J97-3002,O93-1004,0,0.0378188,"Missing"
J97-3002,P93-1004,0,0.041604,"Missing"
J97-3002,P91-1032,0,0.0445706,"s. Moreover, the input-output view works better when a machine for accepting one of the languages (the input language) has a high degree of determinism, which is not the case here. Our transduction model is context-free, rather than finite-state. Finite-state transducers, or FSTs, are well known to be useful for specific tasks such as analysis of inflectional morphology (Koskenniemi 1983), text-to-speech conversion (Kaplan and Kay 1994), and nominal, number, and temporal phrase normalization (Gazdar and Mellish 1989). FSTs may also be used to parse restricted classes of context-free grammars (Pereira 1991; Roche 1994; Laporte 1996). However, the bilingual corpus analysis tasks we consider in this paper are quite different from the tasks for which FSTs are apparently well suited. Our domain is broader, and the model possesses very little a priori specific structural knowledge of the language. As a stepping stone to inversion transduction grammars, we first consider what a context-free model known as a simple transduction grammar (Lewis and Stearns 1968) would look like. Simple transduction grammars (as well as inversion transduction grammars) are restricted cases of the general class of context"
J97-3002,P92-1017,0,0.0383716,"gical cases involving coordination constructions or lexicon coverage inadequacies. The method is also straightforward to employ in tandem with other applications, such as those below. 7. Bracketing Bracketing is another intermediate corpus annotation, useful especially when a fullcoverage grammar with which to parse a corpus is unavailable (for Chinese, an even more common situation than with English). Aside from purely linguistic interest, bracket structure has been empirically shown to be highly effective at constraining subsequent training of, for example, stochastic context-free grammars (Pereira and Schabes 1992; Black, Garside, and Leech 1993). Previous algorithms for automatic bracketing operate on monolingual texts and hence require more grammatical constraints; for example, tactics employing mutual information have been applied to tagged text (Magerman and Marcus 1990). Our method based on SITGs operates on the novel principle that lexical correspondences between parallel sentences yields information from which partial bracketings for both sentences can be extracted. The assumption that no grammar is available means that constituent categories are not differentiated. Instead, a generic bracketing"
J97-3002,C94-1071,0,0.0263795,"he input-output view works better when a machine for accepting one of the languages (the input language) has a high degree of determinism, which is not the case here. Our transduction model is context-free, rather than finite-state. Finite-state transducers, or FSTs, are well known to be useful for specific tasks such as analysis of inflectional morphology (Koskenniemi 1983), text-to-speech conversion (Kaplan and Kay 1994), and nominal, number, and temporal phrase normalization (Gazdar and Mellish 1989). FSTs may also be used to parse restricted classes of context-free grammars (Pereira 1991; Roche 1994; Laporte 1996). However, the bilingual corpus analysis tasks we consider in this paper are quite different from the tasks for which FSTs are apparently well suited. Our domain is broader, and the model possesses very little a priori specific structural knowledge of the language. As a stepping stone to inversion transduction grammars, we first consider what a context-free model known as a simple transduction grammar (Lewis and Stearns 1968) would look like. Simple transduction grammars (as well as inversion transduction grammars) are restricted cases of the general class of context-free syntax"
J97-3002,C90-3101,0,0.0836919,"g Constraints. Arrangements where the matchings between subtrees cross each another are prohibited by crossing constraints, unless the subtrees' immediate parent constituents are also matched to each other. For example, given the constituent matchings depicted as solid lines in Figure 4, the dotted-line matchings corresponding to potential lexical translations would be ruled illegal. Crossing constraints are implicit in many phrasal matching approaches, both constituency-oriented (Kaji, Kida, and Morimoto 1992; Cranias, Papageorgiou, and Peperidis 1994; Grishman 1994) and dependency-oriented (Sadler and Vendelmans 1990; Matsumoto, Ishimoto, and Utsuro 1993). The theoretical cross-linguistic hypothesis here is that the core arguments of frames tend to stay together over different languages. The constraint is also useful for computational reasons, since it helps avoid exponential bilingual matching times. ITGs inherently implement a crossing constraint; in fact, the version enforced by ITGs is even stronger. This is because even within a single constituent, immediate subtrees are only permitted to cross in exact inverted order. As we shall argue below, this restriction reduces matching flexibility in a desira"
J97-3002,P94-1012,1,0.298142,"in normal form is generatively equivalent to any reasonable bracketing transduction grammar. Moreover, we also show how postprocessing using rotation and flattening operations restores the rank flexibility so that an output bracketing can hold more than two immediate constituents, as shown in Figure 11. The bq distribution actually encodes the English-Chinese translation lexicon with degrees of probability on each potential word translation. We have been using a lexicon that was automatically learned from the HKUST English-Chinese Parallel Bilingual Corpus via statistical sentence alignment (Wu 1994) and statistical Chinese word and collocation extraction (Fung and Wu 1994; Wu and Fung 1994), followed by an EM word-translation-learning procedure (Wu and Xia 1994). The latter stage gives us the bij probabilities directly. For the two singleton productions, which permit any word in either sentence to be unmatched, a small c-constant can be chosen for the probabilities bit and bq, so that the optimal bracketing resorts to these productions only when it is 391 Computational Linguistics Volume 23, Number 3 otherwise impossible to match the singletons. The parameter a here is of no practical ef"
J97-3002,P95-1033,1,0.194049,"Missing"
J97-3002,A94-1030,1,0.172923,"p potential collocations into individual words. The problem is particularly acute for English and Chinese because word boundaries are not orthographically marked in Chinese text, so not even a default chunking exists upon which word matchings could be postulated. (Sentences (2) and (5) demonstrate why the obvious trick of taking single characters as words is not a workable strategy.) The usual Chinese NLP architecture first preprocesses input text through a word segmentation module (Chiang et al. 1992; Lin, Chiang, and Su 1992, 1993; Chang and Chen 1993; Wu and Tseng 1993; Sproat et al. 1994; Wu and Fung 1994), but, clearly, bilingual parsing will be hampered by any errors arising from segmentation ambiguities that could not be resolved in the isolated monolingual context because even if the Chinese segmentation is acceptable monolingually, it may not agree with the words present in the English sentence. Matters are made still worse by unpredictable omissions in the translation lexicon, even for valid compounds. We therefore extend the algorithm to optimize the Chinese sentence segmentation in conjunction with the bracketing process. Note that the notion of a Chinese ""word"" is a longstanding lingui"
J97-3002,1994.amta-1.26,1,0.160289,"duction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be grammatical, in keeping with the spirit of robust parsing. The formalism's uniform integration of various types of bracketing and alignment constraints is one of its chief strengths. The paper is divided into two main parts. We begin in the first part below by laying out the basic formalism, then show that reduction to a normal form is possible. We then raise several desid"
J97-3002,J93-1004,0,\N,Missing
J97-3002,P94-1010,0,\N,Missing
J97-3002,J93-1006,0,\N,Missing
lo-wu-2010-evaluating,W07-0738,0,\N,Missing
lo-wu-2010-evaluating,W08-0332,0,\N,Missing
lo-wu-2010-evaluating,P02-1040,0,\N,Missing
lo-wu-2010-evaluating,D07-1007,1,\N,Missing
lo-wu-2010-evaluating,P07-1005,0,\N,Missing
lo-wu-2010-evaluating,J05-1004,0,\N,Missing
lo-wu-2010-evaluating,N09-2004,1,\N,Missing
lo-wu-2010-evaluating,2009.eamt-1.30,1,\N,Missing
lo-wu-2010-evaluating,N04-1030,0,\N,Missing
lo-wu-2014-reliability,lo-wu-2010-evaluating,1,\N,Missing
N04-4010,W99-0612,0,0.0357195,"Missing"
N04-4010,W03-1026,0,0.0267296,"Missing"
N04-4010,O03-5001,0,0.0577236,"Missing"
N04-4010,W02-2024,0,0.0232548,"Missing"
N04-4010,W03-0419,0,0.0212412,"Missing"
N04-4010,W03-0433,1,0.873511,"Missing"
N04-4010,C02-1012,0,\N,Missing
N09-2004,P05-1048,1,0.807624,"uracy. On the one hand, there is reason to be optimistic. Over the past decade, we have seen an accumulation of evidence that SMT accuracy can be improved via tree-structured and syntactic models (e.g., Wu 1997; Wu and Chiang 2009), and more recently, work from lexical semantics has also at long last been successfully applied to increasing SMT accuracy, in the form of techniques adapted from word sense disambiguation models (Chan et al. 2007; Giménez and Màrquez 2007b; Carpuat and Wu 2007). On the other hand, both directions saw unexpected disappointments along the way (e.g., Och et al. 2003; Carpuat and Wu 2005). We are therefore forewarned that it is likely to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling to the translation task. In this paper, we present a novel hybrid model that, for the first time to our knowledge, successfully applies semantic parsing technology to the challenge of improving the quality of ChineseEnglish statistical machine translation. The model makes use of a typical representative SMT system based on Moses, plus shallow semantic parsers for both English and Chinese. 2 Hybrid two"
N09-2004,D07-1007,1,0.0695729,"pers, pages 13–16, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tribute to improving SMT accuracy. On the one hand, there is reason to be optimistic. Over the past decade, we have seen an accumulation of evidence that SMT accuracy can be improved via tree-structured and syntactic models (e.g., Wu 1997; Wu and Chiang 2009), and more recently, work from lexical semantics has also at long last been successfully applied to increasing SMT accuracy, in the form of techniques adapted from word sense disambiguation models (Chan et al. 2007; Giménez and Màrquez 2007b; Carpuat and Wu 2007). On the other hand, both directions saw unexpected disappointments along the way (e.g., Och et al. 2003; Carpuat and Wu 2005). We are therefore forewarned that it is likely to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling to the translation task. In this paper, we present a novel hybrid model that, for the first time to our knowledge, successfully applies semantic parsing technology to the challenge of improving the quality of ChineseEnglish statistical machine translation. The model makes use o"
N09-2004,2007.tmi-papers.10,1,0.897646,"Missing"
N09-2004,P01-1030,0,0.0292028,"ng can improve translation accuracy of SMT models. We note that accuracy here was measured via BLEU, and it has been widely observed that the negative impacts of semantic predicate-argument errors on the utility of the translation are underestimated by evaluation metrics based on lexical criteria such as BLEU. We conjecture that more expensive manual evaluation techniques which directly measure translation utility could even more strongly reveal improvement in role confusion errors. The hybrid two-pass approach can be compared with the greedy re-ordering based strategy of the ReWrite decoder (Germann et al. 2001), although our search is breadth-first rather than purely greedy. Whereas ReWrite was based on wordlevel re-ordering, however, our approach is based on constituent phrase re-ordering, and the phrases to be re-ordered are more selectively chosen via the semantic parse labels. Moreover, the objective function being maximized by ReWrite is still the SMT model score; whereas in our case the new objective function is cross-lingual semantic predicate-argument match (plus an implicit search bias toward fewer re-orderings). The hybrid two-pass approach can also be compared with serial combination arch"
N09-2004,P00-1065,0,0.195005,"Missing"
N09-2004,W07-0738,0,0.207301,"of NAACL HLT 2009: Short Papers, pages 13–16, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tribute to improving SMT accuracy. On the one hand, there is reason to be optimistic. Over the past decade, we have seen an accumulation of evidence that SMT accuracy can be improved via tree-structured and syntactic models (e.g., Wu 1997; Wu and Chiang 2009), and more recently, work from lexical semantics has also at long last been successfully applied to increasing SMT accuracy, in the form of techniques adapted from word sense disambiguation models (Chan et al. 2007; Giménez and Màrquez 2007b; Carpuat and Wu 2007). On the other hand, both directions saw unexpected disappointments along the way (e.g., Och et al. 2003; Carpuat and Wu 2005). We are therefore forewarned that it is likely to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling to the translation task. In this paper, we present a novel hybrid model that, for the first time to our knowledge, successfully applies semantic parsing technology to the challenge of improving the quality of ChineseEnglish statistical machine translation"
N09-2004,W08-0332,0,0.117393,"Missing"
N09-2004,W05-1002,0,0.0327693,"Missing"
N09-2004,P02-1040,0,0.102811,"Missing"
N09-2004,N04-1030,0,0.273541,"Missing"
N09-2004,N04-1032,0,0.0625469,"Missing"
N09-2004,J97-3002,1,0.119844,"tic parsers to identify inconsistent semantic frame and role mappings between the input source sentences and their output translations. However, we take note of the difficult experience in making syntactic and semantic models con13 Proceedings of NAACL HLT 2009: Short Papers, pages 13–16, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tribute to improving SMT accuracy. On the one hand, there is reason to be optimistic. Over the past decade, we have seen an accumulation of evidence that SMT accuracy can be improved via tree-structured and syntactic models (e.g., Wu 1997; Wu and Chiang 2009), and more recently, work from lexical semantics has also at long last been successfully applied to increasing SMT accuracy, in the form of techniques adapted from word sense disambiguation models (Chan et al. 2007; Giménez and Màrquez 2007b; Carpuat and Wu 2007). On the other hand, both directions saw unexpected disappointments along the way (e.g., Och et al. 2003; Carpuat and Wu 2005). We are therefore forewarned that it is likely to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role lab"
N09-2004,C08-1115,0,\N,Missing
N09-2004,W07-0719,0,\N,Missing
N09-2004,P07-1005,0,\N,Missing
N09-2004,N04-1021,0,\N,Missing
N10-1050,J93-2003,0,0.072497,"e a word alignment over a parallel corpus. We show that alignment via Stochastic Bracketing LITGs is considerably faster than Stochastic Bracketing ITGs, while still yielding alignments superior to the widelyused heuristic of intersecting bidirectional IBM alignments. Performance is measured as the translation quality of a phrase-based machine translation system built upon the word alignments, and an improvement of 2.85 BLEU points over baseline is noted for French– English. 1 Introduction Machine translation relies heavily on word alignments, which are usually produced by training IBMmodels (Brown et al., 1993) in both directions and combining the resulting alignments via some heuristic. Automatically training an Inversion Transduction Grammar ( ITG) has been suggested as a viable way of producing superior alignments (Saers and Wu, 2009). The main problem of using Bracketing ITGs for alignment is that exhaustive biparsing runs in O(n6 ) time. Several ways to lower the complexity of ITGs has been suggested, but in this paper, a different approach is taken. Instead of using full ITGs, we explore the possibility of subjecting the grammar to a linear constraint, making exhaustive biparsing of a sentence"
N10-1050,P09-1104,0,0.256675,"Missing"
N10-1050,P07-2045,0,0.0192853,"m in a bucket can be analyzed in 8 possible ways, requiring O(1) time. In summary, we have: O(n) × O(n3 ) × O(1) = O(n4 ) The pruning scheme works by limiting the number of items that are processed from each bucket, reducing the cost of processing a bucket from O(n3 ) to O(b), where b is the beam width. This gives time complexity O(n) × O(b) × O(1) = O(bn). 343 We used the guidelines of the shared task of WMT ’081 to train our baseline system as well as our experimental system. This includes induction of word alignments with GIZA ++ (Och and Ney, 2003), induction of a Phrase-based SMT system (Koehn et al., 2007), and tuning with minimum error rate training (Och, 2003), as well as applying some utility scripts provided for the workshop. The translation model is combined with a 5-gram language model (Stolcke, 2002). Our experimental system uses alignments from the Viterbi parses, extracted during EM training of an SBLITG on the training corpus, instead of GIZA ++. Since EM will converge fairly slowly, it was limited to 10 iterations, after which it was halted. We used the French–English part of the WMT ’08 shared task, but limited the training set to sentence pairs where both sentences were of length 2"
N10-1050,J03-1002,0,0.0176392,"aking the total number of items in a bucket O(n3 ). Each item in a bucket can be analyzed in 8 possible ways, requiring O(1) time. In summary, we have: O(n) × O(n3 ) × O(1) = O(n4 ) The pruning scheme works by limiting the number of items that are processed from each bucket, reducing the cost of processing a bucket from O(n3 ) to O(b), where b is the beam width. This gives time complexity O(n) × O(b) × O(1) = O(bn). 343 We used the guidelines of the shared task of WMT ’081 to train our baseline system as well as our experimental system. This includes induction of word alignments with GIZA ++ (Och and Ney, 2003), induction of a Phrase-based SMT system (Koehn et al., 2007), and tuning with minimum error rate training (Och, 2003), as well as applying some utility scripts provided for the workshop. The translation model is combined with a 5-gram language model (Stolcke, 2002). Our experimental system uses alignments from the Viterbi parses, extracted during EM training of an SBLITG on the training corpus, instead of GIZA ++. Since EM will converge fairly slowly, it was limited to 10 iterations, after which it was halted. We used the French–English part of the WMT ’08 shared task, but limited the trainin"
N10-1050,P03-1021,0,0.0182863,"ime. In summary, we have: O(n) × O(n3 ) × O(1) = O(n4 ) The pruning scheme works by limiting the number of items that are processed from each bucket, reducing the cost of processing a bucket from O(n3 ) to O(b), where b is the beam width. This gives time complexity O(n) × O(b) × O(1) = O(bn). 343 We used the guidelines of the shared task of WMT ’081 to train our baseline system as well as our experimental system. This includes induction of word alignments with GIZA ++ (Och and Ney, 2003), induction of a Phrase-based SMT system (Koehn et al., 2007), and tuning with minimum error rate training (Och, 2003), as well as applying some utility scripts provided for the workshop. The translation model is combined with a 5-gram language model (Stolcke, 2002). Our experimental system uses alignments from the Viterbi parses, extracted during EM training of an SBLITG on the training corpus, instead of GIZA ++. Since EM will converge fairly slowly, it was limited to 10 iterations, after which it was halted. We used the French–English part of the WMT ’08 shared task, but limited the training set to sentence pairs where both sentences were of length 20 or less. This was necessary in order to carry out exhau"
N10-1050,P02-1040,0,0.0775543,"Missing"
N10-1050,W09-2304,1,0.850266,"secting bidirectional IBM alignments. Performance is measured as the translation quality of a phrase-based machine translation system built upon the word alignments, and an improvement of 2.85 BLEU points over baseline is noted for French– English. 1 Introduction Machine translation relies heavily on word alignments, which are usually produced by training IBMmodels (Brown et al., 1993) in both directions and combining the resulting alignments via some heuristic. Automatically training an Inversion Transduction Grammar ( ITG) has been suggested as a viable way of producing superior alignments (Saers and Wu, 2009). The main problem of using Bracketing ITGs for alignment is that exhaustive biparsing runs in O(n6 ) time. Several ways to lower the complexity of ITGs has been suggested, but in this paper, a different approach is taken. Instead of using full ITGs, we explore the possibility of subjecting the grammar to a linear constraint, making exhaustive biparsing of a sentence pair in O(n4 ) time possible. This can be further improved by applying pruning. A transduction is the bilingual version of a language. A language (Ll ) can be formally viewed as a set of sentences, sequences of tokens taken from a"
N10-1050,W09-3804,1,0.78707,"Missing"
N10-1050,P95-1033,1,0.573616,"one as transduction, that is: a sentence in one language is rewritten into the other language. In NLP, interest has shifted away from hand-crafted grammars, towards stochastic grammars induced from corpora. To induce a stochastic grammar from a parallel corpus, expectations of all possible parses over a sentence pair are typically needed. S TGs can biparse sentence pairs in polynomial time, but are unable to account for the complexities typically found in natural languages. S DTGs do account for the complexities in natural languages, but are intractable for biparsing. Inversion transductions (Wu, 1995; Wu, 1997) are a special case of transductions that are not monotone, but where permutations are severely limited. By limiting the possible permutations, biparsing becomes tractable. This in turn means that ITGs can be induced from parallel corpora in polynomial time, 341 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 341–344, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics as well as account for most of the reorderings found between natural languages. An Inversion transduction is limited so that it"
N10-1050,J97-3002,1,0.906411,"nsduction, that is: a sentence in one language is rewritten into the other language. In NLP, interest has shifted away from hand-crafted grammars, towards stochastic grammars induced from corpora. To induce a stochastic grammar from a parallel corpus, expectations of all possible parses over a sentence pair are typically needed. S TGs can biparse sentence pairs in polynomial time, but are unable to account for the complexities typically found in natural languages. S DTGs do account for the complexities in natural languages, but are intractable for biparsing. Inversion transductions (Wu, 1995; Wu, 1997) are a special case of transductions that are not monotone, but where permutations are severely limited. By limiting the possible permutations, biparsing becomes tractable. This in turn means that ITGs can be induced from parallel corpora in polynomial time, 341 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 341–344, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics as well as account for most of the reorderings found between natural languages. An Inversion transduction is limited so that it must be exp"
N10-1050,P08-1012,0,0.537157,"Missing"
P00-1060,H92-1026,0,0.0569682,"Missing"
P00-1060,J93-1002,0,0.0189798,"Missing"
P00-1060,P97-1003,0,0.122012,"evaluation modelling for statistical parsing. The basic idea is that we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction. Our experiment quantitatively analyzes several feature types’ power for syntactic structure prediction and draws a series of interesting conclusions. 1 Introduction In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1991] [Magerman, 1992] [Magerman, 1995] [Eisner, 1996]. How to evaluate the different feature types’ effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types’ or feature types combination’s predictive power for syntactic structure. In the following, Section 2 describes the probabilistic evaluation model for sy"
P00-1060,C96-1058,0,0.122575,"at we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction. Our experiment quantitatively analyzes several feature types’ power for syntactic structure prediction and draws a series of interesting conclusions. 1 Introduction In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1991] [Magerman, 1992] [Magerman, 1995] [Eisner, 1996]. How to evaluate the different feature types’ effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types’ or feature types combination’s predictive power for syntactic structure. In the following, Section 2 describes the probabilistic evaluation model for syntactic trees; Section 3 proposes an information-theory-based feat"
P00-1060,H92-1025,0,0.0467581,"Missing"
P00-1060,P95-1037,0,0.239856,"basic idea is that we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction. Our experiment quantitatively analyzes several feature types’ power for syntactic structure prediction and draws a series of interesting conclusions. 1 Introduction In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1991] [Magerman, 1992] [Magerman, 1995] [Eisner, 1996]. How to evaluate the different feature types’ effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types’ or feature types combination’s predictive power for syntactic structure. In the following, Section 2 describes the probabilistic evaluation model for syntactic trees; Section 3 proposes an information-th"
P00-1060,J93-2004,0,0.0267295,"Missing"
P00-1060,W99-0618,1,\N,Missing
P00-1060,P96-1025,0,\N,Missing
P00-1060,P93-1005,0,\N,Missing
P00-1060,1991.iwpt-1.22,0,\N,Missing
P00-1060,E91-1004,0,\N,Missing
P04-1081,S01-1014,0,0.0478551,"Missing"
P04-1081,W02-0813,0,0.0514893,"based model outperforms the SVM-based model. It is hoped that these highly encouraging first results on KPCA for natural language processing tasks will inspire further development of these directions. 1 Introduction Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)). A good foundation for comparative studies has been established by the Senseval data and evaluations; of particular relevance here are the lexical sample tasks from Senseval-1 (Kilgarriff and Rosenzweig, 1999) and Senseval-2 (Kilgarriff, 2001). We therefore chose this problem to introduce an efficient and accurate new word sense disambiguation approach that exploits a nonlinear Kernel PCA technique to make predictions implicitly based on generalizations over feature combinations. The 1 The author would like to thank the Hong Kong Research Grants Council (RGC) for su"
P04-1081,1993.eamt-1.1,0,0.0871062,"Missing"
P04-1081,C02-1054,0,0.0758152,"Missing"
P04-1081,S01-1004,0,0.356016,"uation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)). A good foundation for comparative studies has been established by the Senseval data and evaluations; of particular relevance here are the lexical sample tasks from Senseval-1 (Kilgarriff and Rosenzweig, 1999) and Senseval-2 (Kilgarriff, 2001). We therefore chose this problem to introduce an efficient and accurate new word sense disambiguation approach that exploits a nonlinear Kernel PCA technique to make predictions implicitly based on generalizations over feature combinations. The 1 The author would like to thank the Hong Kong Research Grants Council (RGC) for supporting this research in part through grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. technique is applicable whenever vector representations of a disambiguation task can be generated; thus many properties of our technique can be expected to be highly attractive fro"
P04-1081,W02-1002,0,0.238923,"the SVM-based model. It is hoped that these highly encouraging first results on KPCA for natural language processing tasks will inspire further development of these directions. 1 Introduction Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)). A good foundation for comparative studies has been established by the Senseval data and evaluations; of particular relevance here are the lexical sample tasks from Senseval-1 (Kilgarriff and Rosenzweig, 1999) and Senseval-2 (Kilgarriff, 2001). We therefore chose this problem to introduce an efficient and accurate new word sense disambiguation approach that exploits a nonlinear Kernel PCA technique to make predictions implicitly based on generalizations over feature combinations. The 1 The author would like to thank the Hong Kong Research Grants Council (RGC) for supporting this research in"
P04-1081,P03-1004,0,0.0503102,"to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). A major advantage of KPCA is that, unlike other common analysis techniques, as with other kernel methods it inherently takes combinations of predictive features into account when optimizing dimensionality reduction. For natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations (e.g., Kudo and Matsumoto (2003)). Another advantage of KPCA for the WSD task is that the dimensionality of the input data is generally very Table 1: Two of the Senseval-2 sense classes for the target word “art”, from WordNet 1.7 (Fellbaum 1998). Class 1 2 Sense the creation of beautiful or significant things a superior skill large, a condition where kernel methods excel. Nonlinear principal components (Diamantaras and Kung, 1996) may be defined as follows. Suppose we are given a training set of M pairs (x t , ct ) where the observed vectors xt ∈ Rn in an ndimensional input space X represent the context of the target word be"
P04-1081,W03-0429,0,0.0669449,"Missing"
P04-1081,W96-0208,0,0.0849957,"data. We also contrast against another type of kernel method, the support vector machine (SVM) model, and show that our KPCA-based model outperforms the SVM-based model. It is hoped that these highly encouraging first results on KPCA for natural language processing tasks will inspire further development of these directions. 1 Introduction Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)). A good foundation for comparative studies has been established by the Senseval data and evaluations; of particular relevance here are the lexical sample tasks from Senseval-1 (Kilgarriff and Rosenzweig, 1999) and Senseval-2 (Kilgarriff, 2001). We therefore chose this problem to introduce an efficient and accurate new word sense disambiguation approach that exploits a nonlinear Kernel PCA technique to make predictions implicitly base"
P04-1081,W01-0507,0,0.0663878,"Missing"
P04-1081,S01-1040,0,\N,Missing
P04-1081,S01-1034,0,\N,Missing
P05-1048,P91-1034,0,0.15708,"atically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. 7 8 Related work Most translation disambiguation tasks are defined similarly to the Senseval Multilingual lexical sample tasks. In Senseval-3, the English to Hindi translation disambigation task was defined identically to the English lexical sample task, except that the WSD models are expected to predict Hindi translations instead of WordNet senses. This differs from our approach which consists of producing the translation of complete sentences, and not only of a predefined set of target words. Brown et al. (1991) proposed a WSD algorithm to disambiguate English translations of French target words based on the single most informative context feature. In a pilot study, they found that using this WSD method in their French-English SMT system helped translation quality, manually evaluated using the number of acceptable translations. However, this study is limited to the unrealistic case of words that have exactly two senses in the other language. Most previous work has focused on the distinct problem of exploiting various bilingual resources (e.g., parallel or comparable corpora, or even MT systems) to he"
P05-1048,W04-0822,1,0.870275,"n methodology and datasets from the Senseval-3 Chinese lexical sample task. We showed the accuracy of the SMT model to be significantly lower than that of all the dedicated WSD models considered, even after adding the lexical sample data to the training set for SMT to allow for a fair comparison. These results highlight the relative strength, and the potential hoped-for advantage of dedicated supervised WSD models. 3 The WSD system The WSD system used for the experiments is based on the model that achieved the best performance, by a large margin, on the Senseval-3 Chinese lexical sample task (Carpuat et al., 2004). 3.1 Classification model The model consists of an ensemble of four voting models combined by majority vote. The first voting model is a naive Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1978), since Klein and Manning (2002) found that this model yielded higher accuracy than naive Bayes in a subsequent comparison of WSD performance. (Note, however, that a different subset of either Senseval-1 or Senseva"
P05-1048,W02-1002,0,0.00768203,"ls. 3 The WSD system The WSD system used for the experiments is based on the model that achieved the best performance, by a large margin, on the Senseval-3 Chinese lexical sample task (Carpuat et al., 2004). 3.1 Classification model The model consists of an ensemble of four voting models combined by majority vote. The first voting model is a naive Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1978), since Klein and Manning (2002) found that this model yielded higher accuracy than naive Bayes in a subsequent comparison of WSD performance. (Note, however, that a different subset of either Senseval-1 or Senseval-2 English lexical sample data was used for their comparison.) The third voting model is a boosting model (Freund and Schapire, 1997), since has consistently turned in very competitive scores on related tasks such as named entity classification (Carreras et al., 2002) . Specifically, an AdaBoost.MH model was used (Schapire and Singer, 2000), which is a multiclass generalization of the original boosting algorithm,"
P05-1048,P02-1044,0,0.0471594,"ace problems still to be outdated”). 0  6.6 BLEU score bias The “language model effect” highlights one of the potential weaknesses of the BLEU score. BLEU penalizes for phrasal incoherence, which in the present study means that it can sometimes sacrifice adequacy for fluency. However, the characteristics of BLEU are by no means solely responsible for the problems with WSD that we observed. To doublecheck that n-gram effects were not unduly impacting our study, we also evaluated using BLEU-1, which gave largely similar results as the standard BLEU-4 scores reported above. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a translation disambiguation WSD model, and Diab (2004) who exploited large amounts of automatically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. 7 8 Related work Most translation disambiguation tasks are defined similarly to the Senseval Multilingual lexical sample tasks. In Senseval-3, the English to Hindi translation disambigation task was defined identically to the English lexical sample task, except that the WSD models are expected to predict Hindi translations instead of WordNet senses. T"
P05-1048,P02-1038,0,0.0742986,"r all, WSD is a method of compensating for sparse data. Thus it may be that the present inability of WSD models to help improve accuracy of SMT systems stems not from an inherent weakness of dedicated WSD models, but rather from limitations of present-day SMT architectures. To further test this, our experiments could be tried on other statistical MT models. For example, the WSD model’s predictions could be employed in a Bracketing ITG translation model such as Wu (1996) or Zens et al. (2004), or alternatively they could be incorporated as features for reranking in a maximum-entropy SMT model (Och and Ney, 2002), instead of using them to constrain the sentence translation hypotheses as done here. However, the preceding discussion argues that it is doubtful that this would produce significantly different results, since the inherent problem from the “language model effect” would largely remain, causing sentence translations that include the WSD’s preferred lexical choices to be discounted. For similar reasons, we suspect our findings may also hold even for more sophisticated statistical MT models that rely heavily on n-gram language models. A more grammatically structured statistical MT model that less"
P05-1048,J03-1002,0,0.00663601,"e potential contribution of WSD should be easier to see against this baseline. Note that our focus here is not on the SMT model itself; our aim is to evaluate the impact of WSD on a real Chinese to English statistical machine translation task. 4 Table 1: Example of the translation candidates before and after mapping for the target word “ ” (lu) HowNet Sense ID HowNet glosses 56520 56521 56524 56525, 56526, 56527, 56528 56530, 56531, 56532 56533, 56534 distance sort Lu path, road, route, way line, means, sequence district, region 4.1 Alignment model The alignment model was trained with GIZA++ (Och and Ney, 2003), which implements the most typical IBM and HMM alignment models. Translation quality could be improved using more advanced hybrid phrasal or tree models, but this would interfere with the questions being investigated here. The alignment model used is IBM-4, as required by our decoder. The training scheme consists of IBM-1, HMM, IBM-3 and IBM-4, following (Och and Ney, 2003). The training corpus consists of about 1 million sentences from the United Nations Chinese-English parallel corpus from LDC. This corpus was automatically sentence-aligned, so the training data does not require as much man"
P05-1048,P02-1040,0,0.108677,"of WSD errors. However, we do not have a corpus which contains both sense annotation and multiple reference translations: the MT evaluation corpus is not annotated with the correct senses of Senseval target words, and the Senseval corpus does not include English translations of the sentences. 6 6.1 Results Even state-of-the-art WSD does not help BLEU score Table 2 summarizes the translation quality scores obtained with and without the WSD model. Using our WSD model to constrain the translation candidates given to the decoder hurts translation quality, as measured by the automated BLEU metric (Papineni et al., 2002). Note that we are evaluating on only difficult sentences containing the problematic target words from the lexical sample task, so BLEU scores can be expected to be on the low side. 391 BLEU score 0.1310 0.1253 0.1239 0.1232 WSD still does not help BLEU score with improved translation candidates One could argue that the translation candidates chosen by the WSD models do not help because they are only glosses obtained from the HowNet dictionary. They consist of the root form of words only, while the SMT model can learn many more translations for each target word, including inflected forms and s"
P05-1048,P98-2230,1,0.781111,"Missing"
P05-1048,P04-1081,1,0.714551,"nt subset of either Senseval-1 or Senseval-2 English lexical sample data was used for their comparison.) The third voting model is a boosting model (Freund and Schapire, 1997), since has consistently turned in very competitive scores on related tasks such as named entity classification (Carreras et al., 2002) . Specifically, an AdaBoost.MH model was used (Schapire and Singer, 2000), which is a multiclass generalization of the original boosting algorithm, with boosting on top of decision stump classifiers (i.e., decision trees of depth one). The fourth voting model is a Kernel PCA-based model (Wu et al., 2004). Kernel Principal Component Analysis (KPCA) is a nonlinear kernel method for extracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonlinearly mapped from their original space Rn to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space. (Carpuat et al., 2004) showed that KPCAbased WSD models achieve cl"
P05-1048,W02-2004,0,0.0369807,"Missing"
P05-1048,P96-1021,1,0.666101,"xical choices, then perhaps some alternative model striking a different balance of adequacy and fluency is called for. Ultimately, after all, WSD is a method of compensating for sparse data. Thus it may be that the present inability of WSD models to help improve accuracy of SMT systems stems not from an inherent weakness of dedicated WSD models, but rather from limitations of present-day SMT architectures. To further test this, our experiments could be tried on other statistical MT models. For example, the WSD model’s predictions could be employed in a Bracketing ITG translation model such as Wu (1996) or Zens et al. (2004), or alternatively they could be incorporated as features for reranking in a maximum-entropy SMT model (Och and Ney, 2002), instead of using them to constrain the sentence translation hypotheses as done here. However, the preceding discussion argues that it is doubtful that this would produce significantly different results, since the inherent problem from the “language model effect” would largely remain, causing sentence translations that include the WSD’s preferred lexical choices to be discounted. For similar reasons, we suspect our findings may also hold even for more"
P05-1048,P04-1039,0,0.0269856,"ential weaknesses of the BLEU score. BLEU penalizes for phrasal incoherence, which in the present study means that it can sometimes sacrifice adequacy for fluency. However, the characteristics of BLEU are by no means solely responsible for the problems with WSD that we observed. To doublecheck that n-gram effects were not unduly impacting our study, we also evaluated using BLEU-1, which gave largely similar results as the standard BLEU-4 scores reported above. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a translation disambiguation WSD model, and Diab (2004) who exploited large amounts of automatically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. 7 8 Related work Most translation disambiguation tasks are defined similarly to the Senseval Multilingual lexical sample tasks. In Senseval-3, the English to Hindi translation disambigation task was defined identically to the English lexical sample task, except that the WSD models are expected to predict Hindi translations instead of WordNet senses. This differs from our approach which consists of producing the translation of complete sentences, and not only"
P05-1048,C04-1030,0,0.0525017,"l weaknesses of the BLEU score. BLEU penalizes for phrasal incoherence, which in the present study means that it can sometimes sacrifice adequacy for fluency. However, the characteristics of BLEU are by no means solely responsible for the problems with WSD that we observed. To doublecheck that n-gram effects were not unduly impacting our study, we also evaluated using BLEU-1, which gave largely similar results as the standard BLEU-4 scores reported above. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a translation disambiguation WSD model, and Diab (2004) who exploited large amounts of automatically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. 7 8 Related work Most translation disambiguation tasks are defined similarly to the Senseval Multilingual lexical sample tasks. In Senseval-3, the English to Hindi translation disambigation task was defined identically to the English lexical sample task, except that the WSD models are expected to predict Hindi translations instead of WordNet senses. This differs from our approach which consists of producing the translation of complete sentences, and not only"
P05-1048,J04-1001,0,\N,Missing
P05-1048,P03-1058,0,\N,Missing
P05-1048,C98-2225,1,\N,Missing
P05-1048,N03-1010,0,\N,Missing
P11-1023,W05-0909,0,0.0836133,"Missing"
P11-1023,W07-0738,0,0.210377,"Missing"
P11-1023,W08-0332,0,0.180161,"Missing"
P11-1023,W06-3114,0,0.070746,"Missing"
P11-1023,E06-1031,0,0.0865674,"n judgment as HTER, at an even lower cost—and is still far better correlated than n-gram based evaluation metrics. 2 Related work Lexical similarity based metrics BLEU (Papineni et al., 2002) is the most widely used MT evaluation metric despite the fact that a number of large scale metaevaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where it strongly disagree with human judgment on translation accuracy. Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption—that a “good” translation is one that shares the same lexical choices as the reference translation—is not justified semantically. Lexical similarity does not adequately reflect similarity in meaning. State-of-the-art MT systems are often able to output translations containing roughly the correct words, yet expressing meaning that is not close to that of the input. We argue tha"
P11-1023,W05-0904,0,0.309261,"t checking whether the roles were appropriately attached to the correct predicate. Also, the actor, experiencer, and patient were all conflated into the undistinguished who role, while other crucial elements, like the action, purpose, manner, were ignored. Instead, we argue, evaluating meaning similarity should be done by evaluating the semantic structure as a whole: (a) all core semantic roles should be checked, and (b) not only should we evaluate the presence of semantic role fillers in isolation, but also their relations to the frames’ predicates. Syntax based metrics Unlike Voss and Tate, Liu and Gildea (2005) proposed a structural approach, but it was based on syntactic rather than semantic structure, and focused on checking the correctness of the role structure without checking the correctness of the role fillers. Their subtree metric (STM) and headword chain metric (HWC) address the failure of BLEU to evaluate translation grammaticality; however, the problem remains that a grammatical translation can achieve a high syntax-based score HTER (non-automatic) Despite the fact that Human- even if contains meaning errors arising from confusion of targeted Translation Edit Rate (HTER) as proposed by sem"
P11-1023,N07-1006,0,0.0205359,"updated translations from the same systems”. Instead, we aim for MT evaluation metrics that provide fine-grained scores in a way that also directly reflects interpretable insights on the strengths and weaknesses of MT systems rather than simply replicating human assessments. Semantic roles as features in aggregate metrics Gim´enez and M`arquez (2007, 2008) introduced ULC, an automatic MT evaluation metric that aggregates many types of features, including several shallow semantic similarity features: semantic role overlapping, semantic role matching, and semantic structure overlapping. Unlike Liu and Gildea (2007) who use discriminative training to tune the weight on each feature, ULC uses uniform weights. Although the metric shows an improved correlation with 3 MEANT: SRL for MT evaluation human judgment of translation quality (Callison-Burch et A good translation is one from which human readers al., 2007; Gim´enez and M`arquez, 2007; Callison-Burch may successfully understand at least the basic event strucet al., 2008; Gim´enez and M`arquez, 2008), it is not com- ture—“who did what to whom, when, where and why” monly used in large-scale MT evaluation campaigns, per- (Pradhan et al., 2004)—which repre"
P11-1023,lo-wu-2010-evaluating,1,0.906669,"Missing"
P11-1023,niessen-etal-2000-evaluation,0,0.187548,"lower cost—and is still far better correlated than n-gram based evaluation metrics. 2 Related work Lexical similarity based metrics BLEU (Papineni et al., 2002) is the most widely used MT evaluation metric despite the fact that a number of large scale metaevaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where it strongly disagree with human judgment on translation accuracy. Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption—that a “good” translation is one that shares the same lexical choices as the reference translation—is not justified semantically. Lexical similarity does not adequately reflect similarity in meaning. State-of-the-art MT systems are often able to output translations containing roughly the correct words, yet expressing meaning that is not close to that of the input. We argue that a translation metric that re"
P11-1023,P02-1040,0,0.121519,"Missing"
P11-1023,N04-1030,0,0.199132,"ing. Unlike Liu and Gildea (2007) who use discriminative training to tune the weight on each feature, ULC uses uniform weights. Although the metric shows an improved correlation with 3 MEANT: SRL for MT evaluation human judgment of translation quality (Callison-Burch et A good translation is one from which human readers al., 2007; Gim´enez and M`arquez, 2007; Callison-Burch may successfully understand at least the basic event strucet al., 2008; Gim´enez and M`arquez, 2008), it is not com- ture—“who did what to whom, when, where and why” monly used in large-scale MT evaluation campaigns, per- (Pradhan et al., 2004)—which represents the most essenhaps due to its high time cost and/or the difficulty of in- tial meaning of the source utterances. terpreting its score because of its highly complex combiMEANT measures this as follows. First, semantic role nation of many heterogenous types of features. labeling is performed (either manually or automatically) Specifically, note that the feature based representations on both the reference translation and the machine translaof semantic roles used in these aggregate metrics do not tion. The semantic frame structures thus obtained for the actually capture the struc"
P11-1023,2006.amta-papers.25,0,0.527447,"tructural approach, but it was based on syntactic rather than semantic structure, and focused on checking the correctness of the role structure without checking the correctness of the role fillers. Their subtree metric (STM) and headword chain metric (HWC) address the failure of BLEU to evaluate translation grammaticality; however, the problem remains that a grammatical translation can achieve a high syntax-based score HTER (non-automatic) Despite the fact that Human- even if contains meaning errors arising from confusion of targeted Translation Edit Rate (HTER) as proposed by semantic roles. Snover et al. (2006) shows a high correlation with human STM was the first proposed metric to incorporate synjudgment on translation adequacy, it is not widely used in tactic features in MT evaluation, and STM underlies most day-to-day machine translation evaluation because of its other recently proposed syntactic MT evaluation methigh labor cost. HTER not only requires human experts rics, for example the evaluation metric based on lexicalto understand the meaning expressed in both the refer- functional grammar of Owczarzak et al. (2008). STM is ence translation and the machine translation, but also re- a precisi"
P11-1023,2006.eamt-1.25,0,0.0177916,"eaning as the reference translation. Requiring such heavy manual decision making greatly increases the cost of evaluation, bottlenecking the evaluation cycle. To reduce the cost of evaluation, we aim to reduce any human decisions in the evaluation cycle to be as simple as possible, such that even untrained humans can quickly complete the evaluation. The human decisions should also be defined in a way that can be closely approximated by automatic methods, so that similar objective functions might potentially be used for tuning in MT system development cycles. Task based metrics (non-automatic) Voss and Tate (2006) proposed a task-based approach to MT evaluation that is in some ways similar in spirit to ours, but rather than evaluating how well people understand the meaning as a whole conveyed by a sentence translation, they measured the recall with which humans can extract one of the who, when, or where elements from MT output—and without attaching them to any predicate or frame. A large number of human subjects were instructed to extract only one particular type of wh-item from each sentence. They evaluated only whether the role fillers were correctly identified, without checking whether the roles wer"
P11-1023,E06-1032,0,\N,Missing
P11-1023,W07-0718,0,\N,Missing
P11-1023,W10-1703,0,\N,Missing
P11-1023,W08-0309,0,\N,Missing
P11-1023,W10-3807,1,\N,Missing
P13-2067,2006.iwslt-evaluation.11,0,0.0718925,"c metrics below to justify our choice. Utilizing semantics in SMT In the past few years, there has been a surge of work aimed at incorporating semantics into various stages of the SMT. Wu and Fung (2009) propose a two-pass model that reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorpo"
P13-2067,W05-0909,0,0.275982,"MT system against ULC perhaps due to its expensive running time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features and is based on pure lexical features. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to semantic objectives. MT evaluation metrics As mentioned previously, tuning against n-gram based metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) does not sufficiently drive SMT into making decisions to produce adequate translations that correctly preserve ”who did what to whom, In contrast to TINE, MEANT (Lo et al., 2012), which is the weighted f-score over the matched semantic role labels of the automatically aligned semantic frames and role fillers, outperforms BLEU, NIST, METEOR, WER, CDER and TER. This makes it more suitable for tuning SMT systems to produce much adequate translations. Although TINE (Rios et al., 2011) is an recalloriented automatic evaluation metric which aims to preserve the basic event structure, no work has be"
P13-2067,E06-1032,0,0.770131,"ein automatic semantic parsing might be expected to fare worse. These results strongly indicate that using a semantic frame based objective function for tuning would drive development of MT towards direction of higher utility. Glaring errors caused by semantic role confusion that plague the state-of-the-art MT systems are a consequence of using fast and cheap lexical n-gram based objective functions like BLEU to drive their development. Despite enforcing fluency it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al., 2006; Koehn and Monz, 2006). We argue that instead of BLEU, a metric that focuses on getting the meaning right should be used as an objective function for tuning SMT so as to drive continuing progress towards higher utility. MEANT (Lo et al., 2012), is an automatic semantic MT evaluation metric that measures similarity between the MT output and the reference translation via semantic frames. It correlates better with human adequacy judgment than other automatic MT evaluation metrics. Since a high MEANT score is contingent on correct lexical choices as well as syntactic and semantic structures, we b"
P13-2067,E06-1031,0,0.691061,"of our system with that of two baseline SMT systems tuned against BLEU and TER, the commonly used n-gram and edit distance 375 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375–381, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Related Work when, where and why”. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation accuracy. Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. Relatively little work has been done towards biasing the translation decisions of an SMT system to produce adequate translations that correctly preserve who did what to whom, when, where and why (Pradhan et al., 2004). This is because the development of SMT systems was predominantly driven by tuning against n-gram based evaluation metrics such as BLEU or edit distance based metrics such as TER which do not sufficiently bias SM"
P13-2067,W07-0718,0,0.470422,"Missing"
P13-2067,C10-1081,0,0.0750956,"ncorporating semantics into various stages of the SMT. Wu and Fung (2009) propose a two-pass model that reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality (Callison-Bu"
P13-2067,W08-0309,0,0.411594,"Missing"
P13-2067,niessen-etal-2000-evaluation,0,0.790958,"two baseline SMT systems tuned against BLEU and TER, the commonly used n-gram and edit distance 375 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375–381, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Related Work when, where and why”. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation accuracy. Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. Relatively little work has been done towards biasing the translation decisions of an SMT system to produce adequate translations that correctly preserve who did what to whom, when, where and why (Pradhan et al., 2004). This is because the development of SMT systems was predominantly driven by tuning against n-gram based evaluation metrics such as BLEU or edit distance based metrics such as TER which do not sufficiently bias SMT system’s decisions to pro"
P13-2067,P02-1040,0,0.107244,"àrquez, 2008) but no work has been done towards tuning an MT system against ULC perhaps due to its expensive running time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features and is based on pure lexical features. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to semantic objectives. MT evaluation metrics As mentioned previously, tuning against n-gram based metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) does not sufficiently drive SMT into making decisions to produce adequate translations that correctly preserve ”who did what to whom, In contrast to TINE, MEANT (Lo et al., 2012), which is the weighted f-score over the matched semantic role labels of the automatically aligned semantic frames and role fillers, outperforms BLEU, NIST, METEOR, WER, CDER and TER. This makes it more suitable for tuning SMT systems to produce much adequate translations. Although TINE (Rios et al., 2011) is an recalloriented automatic evaluation metric whic"
P13-2067,W07-0738,0,0.382895,"he model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an MT system against ULC perhaps due to its expensive running time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features and is based on pure lexical features. Although tuning on QUEEN produced slightly more preferable translations"
P13-2067,W08-0332,0,0.421386,"ystem. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an MT system against ULC perhaps due to its expensive running time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features and is based on pure lexical features. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to semantic objectives. MT evaluation metrics As mentioned previously, tuning against n-gram based metrics such as BLEU (Papineni et a"
P13-2067,2006.amta-papers.25,0,0.252688,"against BLEU and TER, the commonly used n-gram and edit distance 375 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375–381, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Related Work when, where and why”. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation accuracy. Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. Relatively little work has been done towards biasing the translation decisions of an SMT system to produce adequate translations that correctly preserve who did what to whom, when, where and why (Pradhan et al., 2004). This is because the development of SMT systems was predominantly driven by tuning against n-gram based evaluation metrics such as BLEU or edit distance based metrics such as TER which do not sufficiently bias SMT system’s decisions to produce adequate translations. Alt"
P13-2067,N09-2004,1,0.891593,"ranslations. We argue that an SMT system tuned against an adequacy-oriented metric that correlates well with human adequacy judgement produces more adequate translations. For this purpose, we choose MEANT, an automatic semantic MT evaluation metric that focuses on getting the meaning right by comparing the semantic structures of the MT output and the reference. We briefly describe some of the alternative semantic metrics below to justify our choice. Utilizing semantics in SMT In the past few years, there has been a surge of work aimed at incorporating semantics into various stages of the SMT. Wu and Fung (2009) propose a two-pass model that reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train"
P13-2067,I11-1004,0,0.0218854,"our choice. Utilizing semantics in SMT In the past few years, there has been a surge of work aimed at incorporating semantics into various stages of the SMT. Wu and Fung (2009) propose a two-pass model that reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semanti"
P13-2067,P12-1095,0,0.0228147,"t reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008)"
P13-2067,W06-3114,0,\N,Missing
P13-2067,W11-2112,0,\N,Missing
P13-2067,W12-3129,1,\N,Missing
P13-2067,W11-2136,0,\N,Missing
P13-2067,N04-1030,0,\N,Missing
P14-2124,2011.mtsummit-papers.52,0,0.078029,"Missing"
P14-2124,W12-3108,0,0.0431379,"an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that"
P14-2124,W06-3114,0,0.0405831,"T assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it pe"
P14-2124,W05-0909,0,0.0845738,"s), pages 765–771, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency s"
P14-2124,E06-1031,0,0.689315,"ces and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. tems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human"
P14-2124,W05-0904,0,0.0689774,"R (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive"
P14-2124,C04-1046,0,0.0699212,"equate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et"
P14-2124,P11-1023,1,0.950242,"echnology Center Department of Computer Science and Engineering Hong Kong University of Science and Technology {jackielo|mbeloucif|masaers|dekai}@cs.ust.hk Abstract than that of the reference translation, and on the other hand, the BITG constraints the word alignment more accurately than the heuristic bag-ofword aggregation used in MEANT. Our results suggest that MT translation adequacy is more accurately evaluated via the cross-lingual semantic frame similarities of the input and the MT output which may obviate the need for expensive human reference translations. The MEANT family of metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012) adopt the principle that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metri"
P14-2124,E06-1032,0,0.0512033,"ing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic ev"
P14-2124,W07-0718,0,0.0464491,"fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the reference via an integrated probabilistic FSM and PDA model. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain several dozens of parameters to tune, and employ expensive linguistic resources like WordNet Figure 1: M"
P14-2124,W12-4206,1,0.863115,"the MT out766 Figure 2: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic shallow semantic parser. The reference and MT output are parsed by an English automatic shallow semantic parser. There are no semantic frames for MT3 since the system decided to drop the predicate. put. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. tems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two diff"
P14-2124,W08-0309,0,0.0729217,"Missing"
P14-2124,2013.mtsummit-papers.12,1,0.927794,"that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of in"
P14-2124,W13-2254,1,0.745244,"that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of in"
P14-2124,W12-3103,0,0.0176841,"event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the reference via an integrated probabilistic FSM and PDA model. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain several dozens of parameters to tune, and employ expensive linguistic resources like WordNet Figure 1: Monolingual MEANT algorithm. or paraphrase tables; the expensive training, tuning, and/or running time makes them hard to incorporate into the MT development cycle. 2.2 The MEANT family of metrics MEANT (Lo et al., 2012), which is the weighted fscore over the matched semantic role labels of the automatically aligned semantic"
P14-2124,W12-3129,1,0.91538,"judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurat"
P14-2124,2013.iwslt-evaluation.5,1,0.868047,"anslations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of input and output sentences. We therefore propose XMEANT, a cross-lingual MT evaluation metric, that modifies MEANT using (1) simple translation probabilities (in our experiments, We introduce XMEANT—a new cross-lingual version of the semantic frame based MT evaluation metric MEANT—which can correl"
P14-2124,W13-2202,0,0.0143653,"one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of input and output sentences. We th"
P14-2124,W12-3122,0,0.0699178,"judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurat"
P14-2124,niessen-etal-2000-evaluation,0,0.245663,"14. 2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005),"
P14-2124,W07-0411,0,0.159197,"Missing"
P14-2124,P02-1040,0,0.103731,"he Association for Computational Linguistics (Short Papers), pages 765–771, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG"
P14-2124,N04-1030,0,0.0990837,"nt more accurately than the heuristic bag-ofword aggregation used in MEANT. Our results suggest that MT translation adequacy is more accurately evaluated via the cross-lingual semantic frame similarities of the input and the MT output which may obviate the need for expensive human reference translations. The MEANT family of metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012) adopt the principle that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is"
P14-2124,quirk-2004-training,0,0.0654216,"two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the pr"
P14-2124,W11-2112,0,0.0746507,"ce. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the referen"
P14-2124,W09-2304,1,0.953515,"ully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the 3.1 Applying MEANT’s f-score within semantic role fillers The first natural approach is to extend MEANT’s f-score based method of aggregating semantic parse accuracy, so as to also apply to aggregat768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G R p ([AA] |A) ≡ ≡ = p (e/f |A) = si,pred = si,j = Table 1: Sentence-level correlation with HAJ (GALE phase 2.5 evaluation data) Metric HMEANT XMEANT (BITG) MEANT (f-score) XMEANT (f-score) MEANT (2013) NIST BLEU/METEOR/TER/PER CDER WER ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p (⟨AA⟩|A) = 0.25 1√ t (e|f ) t (f |e) 2 1 ( ( )) ∗ ln P A⇒ei,pred /fi,pred |G 1− max(|ei,pred |,|fi,pred |) 5 Conclusion 1 1− ( ( )) ∗ ln P A⇒ei,j /fi,j |G We have presented XMEANT, a new cross-lingual variant of MEANT, that correlates even more closely with human translation adequacy judgm"
P14-2124,W09-3804,1,0.872735,"dy demonstrate XMEANT’s potential to drive research progress toward semantic SMT. max(|ei,j |,|fi,j |) where G is a bracketing ITG, whose only nonterminal is A, and where R is a set of transduction rules where e ∈ W 0 ∪ {ϵ} is an output token (or the null token), and f ∈ W 1 ∪ {ϵ} is an input token (or the null token). The rule probability function p is defined using fixed probabilities for the structural rules, and a translation table t trained using IBM model 1 in both directions. To calculate (the inside probability of a pair of seg) ∗ ments, P A ⇒ e/f|G , we use the algorithm described in Saers et al. (2009). si,pred and si,j are the length normalized BITG parsing probabilities of the predicates and role fillers of the arguments of type j between the input and the MT output. 4 Kendall 0.53 0.51 0.48 0.46 0.46 0.29 0.20 0.12 0.10 Results Table 1 shows that for human adequacy judgments at the sentence level, the f-score based XMEANT (1) correlates significantly more closely than other commonly used monolingual automatic MT evaluation metrics, and (2) even correlates nearly as well as monolingual MEANT. This suggests that the semantic structure of the MT output is indeed closer to that of the input"
P14-2124,2013.mtsummit-papers.21,0,0.0112627,"m of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more rec"
P14-2124,2006.amta-papers.25,0,0.117577,"ational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher corr"
P14-2124,2011.eamt-1.12,0,0.024364,"E system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et"
P14-2124,Y12-1062,1,0.912362,"judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurat"
P14-2124,H05-1096,0,0.0325066,"s UMEANT is useful when human judgments on adequacy of the development set are unavailable. tems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small anno"
P14-2124,W12-3107,0,0.0199386,"correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the reference via an integrated probabilistic FSM and PDA model. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain several dozens of parameters to tune, and employ expensive linguistic resources like WordNet Figure 1: Monolingual MEANT algorithm. or paraphrase tables; the expensive training, tuning, and/or running time makes them hard to incorporate into the MT development cycle. 2.2"
P14-2124,J97-3002,1,0.352751,"that MEANT is based upon is fully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the 3.1 Applying MEANT’s f-score within semantic role fillers The first natural approach is to extend MEANT’s f-score based method of aggregating semantic parse accuracy, so as to also apply to aggregat768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G R p ([AA] |A) ≡ ≡ = p (e/f |A) = si,pred = si,j = Table 1: Sentence-level correlation with HAJ (GALE phase 2.5 evaluation data) Metric HMEANT XMEANT (BITG) MEANT (f-score) XMEANT (f-score) MEANT (2013) NIST BLEU/METEOR/TER/PER CDER WER ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p (⟨AA⟩|A) = 0.25 1√ t (e|f ) t (f |e) 2 1 ( ( )) ∗ ln P A⇒ei,pred /fi,pred |G 1− max(|ei,pred |,|fi,pred |) 5 Conclusion 1 1− ( ( )) ∗ ln P A⇒ei,j /fi,j |G We have presented XMEANT, a new cross-lingual variant of MEANT, that correlates even more clos"
P14-2124,P10-1062,0,0.0203401,"rast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of"
P14-2124,P03-1019,0,0.0342442,"T is based upon is fully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the 3.1 Applying MEANT’s f-score within semantic role fillers The first natural approach is to extend MEANT’s f-score based method of aggregating semantic parse accuracy, so as to also apply to aggregat768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G R p ([AA] |A) ≡ ≡ = p (e/f |A) = si,pred = si,j = Table 1: Sentence-level correlation with HAJ (GALE phase 2.5 evaluation data) Metric HMEANT XMEANT (BITG) MEANT (f-score) XMEANT (f-score) MEANT (2013) NIST BLEU/METEOR/TER/PER CDER WER ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p (⟨AA⟩|A) = 0.25 1√ t (e|f ) t (f |e) 2 1 ( ( )) ∗ ln P A⇒ei,pred /fi,pred |G 1− max(|ei,pred |,|fi,pred |) 5 Conclusion 1 1− ( ( )) ∗ ln P A⇒ei,j /fi,j |G We have presented XMEANT, a new cross-lingual variant of MEANT, that correlates even more closely with human trans"
P14-2124,J93-2003,0,\N,Missing
P14-2124,H93-1040,0,\N,Missing
P14-2124,1993.mtsummit-1.24,0,\N,Missing
P14-2124,W07-0738,0,\N,Missing
P14-2124,W08-0332,0,\N,Missing
P14-2124,P11-1124,0,\N,Missing
P14-2124,P13-2067,1,\N,Missing
P14-2124,2012.eamt-1.64,1,\N,Missing
P94-1012,P91-1022,0,0.877514,"Missing"
P94-1012,C94-2178,0,0.459879,"Missing"
P94-1012,P91-1023,0,0.716881,"Missing"
P94-1012,P93-1001,0,\N,Missing
P94-1012,J93-1006,0,\N,Missing
P94-1012,P93-1002,0,\N,Missing
P95-1033,W93-0305,0,0.0508194,"Missing"
P95-1033,O92-1003,0,0.0337672,"Missing"
P95-1033,P93-1001,0,0.11095,"Missing"
P95-1033,A94-1006,0,0.0318491,"Missing"
P95-1033,C94-2178,0,0.0657668,"Missing"
P95-1033,1994.amta-1.11,0,0.0597046,"Missing"
P95-1033,P91-1023,0,0.0136582,"Missing"
P95-1033,1992.tmi-1.9,0,0.12553,"Missing"
P95-1033,O93-1004,0,0.0684624,"Missing"
P95-1033,O92-1001,0,0.0247055,"Missing"
P95-1033,P92-1017,0,0.0180111,"Missing"
P95-1033,P94-1012,1,0.917808,"Missing"
P95-1033,A94-1030,1,0.895084,"Missing"
P95-1033,1994.amta-1.26,1,0.709273,"Missing"
P95-1033,W93-0301,0,0.0898517,"Missing"
P95-1033,J93-1004,0,\N,Missing
P95-1033,J93-2003,0,\N,Missing
P95-1033,J90-2002,0,\N,Missing
P95-1033,P94-1010,0,\N,Missing
P96-1021,J90-2002,0,0.256852,"Missing"
P96-1021,J93-2003,0,0.0248323,"model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process. Assume, as we do throughout this paper, that the input language is Chinese and the task is to translate into English. The underlying generative model, shown in Figure 1, contains a stochastic English sentence generator whose output is &quot;corrupted&quot; by the translation channel to produce Chinese sentences. In the IBM system, the language model employs simple n-grams, while the translation model employs several sets of parameters as discussed below. Estimation of the parameters has been described elsewhere (Brown et al., 1993). Translation is performed in the reverse direction from generation, as usual for recognition under generative models. For each Chinese sentence c that is to be translated, the system must attempt to find the English sentence e* such that: (1) e* = argmaxPr(elc ) e (2) = argmaxPr(cle ) Pr(e) e In the IBM model, the search for the optimal e* is performed using a best-first heuristic &quot;stack search&quot; similar to A* methods. One of the primary obstacles to making the statistical translation approach practical is slow speed of 152 translation, as performed in A* fashion. This price is paid for the ro"
P96-1021,W93-0301,0,0.372456,"mplest ones, &quot;Model 2&quot;; search costs for the more complex models are correspondingly higher. English strings stochastic English generator -- Chinese strings noisy channel i J k direction < i I [ direction of generativemodel ---~-~ of translation Figure 1: Channel translation model. also be used with the new model described below, but the issue is independent of our focus here. In this paper we address the underlying assumptions of core channel model itself which does not directly use linguistic structure. A slightly different model is employed for a word alignment application by Dagan et al. (Dagan, Church, and Gale, 1993). Instead of alignment probabilities, offset probabilities o(k) are employed, where k is essentially the positional distance between the English words aligned to two adjacent Chinese words: (3) k = i - (A(jpreo) + (j - jp~ev)N) where jpr~v is the position of the immediately preceding Chinese word and N is a constant that normalizes for average sentence lengths in different languages. The motivation is that words that are close to each other in the Chinese sentence should tend to be close in the English sentence as well. The size of the parameter set is greatly reduced from the lil x IJl x ITI"
P96-1021,P81-1022,0,0.036576,"er Chinese or English to go unmatched. The SBTG assigns a probability Pr(c, e, q) to all generable trees q and sentence-pairs. In principle it can be used as the translation channel model by normalizing with Pr(e) and integrating out Pr(q) to give Pr(cle ) in Equation (2). In practice, a strong language model makes this unnecessary, so we can instead optimize the simpler Viterbi approximation (4) e* = a r g m a x P r ( c , e, q) Pr(e) e do not need to deal with). As in our word-alignment model, the translation algorithm optimizes Equation (4) via dynamic programming, similar to chart parsing (Earley, 1970) but with a probabilistic objective function as for HMMs (Viterbi, 1967). But unlike the word-alignment model, to accommodate the bigram model we introduce indexes in the recurrence not only on subtrees over the source Chinese string, but also on the delimiting words of the target English substrings. Another feature of the algorithm is that segmentation of the Chinese input sentence is performed in parallel with the translation search. Conventional architectures for Chinese NLP generally attempt to identify word boundaries as a preprocessing stage. 5 Whenever the segmentation preprocessor prem"
P96-1021,P94-1012,1,0.589419,"Missing"
P96-1021,P95-1033,1,0.876455,"per, like Dagan et al.&apos;s model, encourages related words to stay together, and reduces the number of parameters used to describe word-order variation. But more importantly, it makes structural assumptions that eliminate large portions of the space of alignments, based on linguistic motivatations. This greatly reduces the search space and makes possible a polynomial-time optimization algorithm. 153 2 ITG and BTG Overview The new translation model is based on the recently introduced bilingual language modeling approach. Specifically, the model employs a bracketing transduction grammar or B T G (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d). These formalisms were originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. This paper finds they are also useful for the translation system itself. In this section we summarize the main properties of BTGs and ITGs. An ITG consists of context-free productions where terminal symbols come in couples, for example x/y, where z is a Chinese word and y is an English translation of x. 2 Any parse tree thus gen"
P96-1021,1995.tmi-1.28,1,0.713975,"per, like Dagan et al.&apos;s model, encourages related words to stay together, and reduces the number of parameters used to describe word-order variation. But more importantly, it makes structural assumptions that eliminate large portions of the space of alignments, based on linguistic motivatations. This greatly reduces the search space and makes possible a polynomial-time optimization algorithm. 153 2 ITG and BTG Overview The new translation model is based on the recently introduced bilingual language modeling approach. Specifically, the model employs a bracketing transduction grammar or B T G (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d). These formalisms were originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. This paper finds they are also useful for the translation system itself. In this section we summarize the main properties of BTGs and ITGs. An ITG consists of context-free productions where terminal symbols come in couples, for example x/y, where z is a Chinese word and y is an English translation of x. 2 Any parse tree thus gen"
P96-1021,W95-0106,1,0.930798,"per, like Dagan et al.&apos;s model, encourages related words to stay together, and reduces the number of parameters used to describe word-order variation. But more importantly, it makes structural assumptions that eliminate large portions of the space of alignments, based on linguistic motivatations. This greatly reduces the search space and makes possible a polynomial-time optimization algorithm. 153 2 ITG and BTG Overview The new translation model is based on the recently introduced bilingual language modeling approach. Specifically, the model employs a bracketing transduction grammar or B T G (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d). These formalisms were originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. This paper finds they are also useful for the translation system itself. In this section we summarize the main properties of BTGs and ITGs. An ITG consists of context-free productions where terminal symbols come in couples, for example x/y, where z is a Chinese word and y is an English translation of x. 2 Any parse tree thus gen"
P96-1021,A94-1030,1,0.837901,"Missing"
P96-1021,Y95-1025,1,0.537422,"a(ilj, V, T) permits the Chinese word in position j of a length-T sentence to map to any position i of a length-V English sentence. So V T alignments are possible, yielding an exponential space with correspondingly slow search times. Note there are no explicit linguistic grammars in the IBM channel model. Useful methods do exist for incorporating constraints fed in from other preprocessing modules, and some of these modules do employ linguistic grammars. For instance, we previously reported a method for improving search times in channel translation models that exploits bracketing information (Wu and Ng, 1995). If any brackets for the Chinese sentence can be supplied as additional input information, produced for example by a preprocessing stage, a modified version of the A*based algorithm can follow the brackets to guide the search heuristically. This strategy appears to produces moderate improvements in search speed and slightly better translations. Such linguistic-preprocessing techniques could 1Various models have been constructed by the IBM team (Brown et al., 1993). This description corresponds to one of the simplest ones, &quot;Model 2&quot;; search costs for the more complex models are correspondingly"
P98-2230,J90-2002,0,0.0134044,"er rules or semantic ontologies), monolingual syntactic grammars are relatively easy to acquire or construct. We use the grammar in the SITG channel, while retaining the bigram language model. The new model facilitates explicit coding of grammatical knowledge and finer control over channel probabilities. Like Wu's SBTG model, the translation hypothesis space can be exhaustively searched in polynomial time, as shown in Section 5. The experiments discussed in Section 6 show promising results for these directions. 2 Review: Noisy Channel Model The statistical translation model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process. The underlying generative model contains a stochastic Chinese (input) sentence generator whose output is &quot;corrupted&quot; by the translation channel to produce English (output) sentences. Assume, as we do throughout this paper, that the input language is English and the task is to translate into Chinese. In the IBM system, the language model employs simple n-grams, while the translation model employs several sets of parameters as discussed below. Estimation of the parameters has been described elsewhere (Brown et al., 1993). Translation is performed in"
P98-2230,J93-2003,0,0.0186077,"ion model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process. The underlying generative model contains a stochastic Chinese (input) sentence generator whose output is &quot;corrupted&quot; by the translation channel to produce English (output) sentences. Assume, as we do throughout this paper, that the input language is English and the task is to translate into Chinese. In the IBM system, the language model employs simple n-grams, while the translation model employs several sets of parameters as discussed below. Estimation of the parameters has been described elsewhere (Brown et al., 1993). Translation is performed in the reverse direction from generation, as usual for recognition under generative models. For each English sentence e to be translated, the system attempts to find the Chinese sentence c, such that: c* = a r g m a x P r ( c l e ) = a r g m a x P r ( e l e ) Pr(c) (1) g g In the IBM model, the search for the optimal c , is performed using a best-first heuristic &quot;stack search&quot; similar to A* methods. One of the primary obstacles to making the statistical translation approach practical is slow speed of translation, as performed in A* fashion. This price is paid for the"
P98-2230,P81-1022,0,0.103347,"within a certain distance, see Section 5) to the dot position of the item. The the located on the adjacent to the dot position of the item is skipped. Word-skipping provides us the flexibility to parse the source input by skipping possible singleton(s), if when we doing so, the source input can be parsed with the highest likelihood, and grammatical output can be produced. 5 Translation Algorithm The translation search algorithm differs from that of Wu's SBTG model in that it handles arbitrary grammars rather than binary bracketing grammars. As such it is more similar to active chart parsing (Earley, 1970) rather than CYK parsing (Kasami, 1965; Younger, 1967). We take the standard notion of items (Aho and Ullman, 1972), and use the term anticipation to mean an item which still has symbols right of its dot. Items that don't have any symbols right of the dot are called subtree. As with Wu's SBTG model, the algorithm maximizes a probabilistic objective function, Equation (2), using dynamic programming similar to that for H M M recognition (Viterbi, 1967). The presence o f the bigram model in the objective function necessitates indexes in the recurrence not only on subtrees over the source English"
P98-2230,A94-1030,1,0.83698,"Missing"
P98-2230,P94-1012,1,0.574744,"Missing"
P98-2230,P95-1033,1,0.918294,"stic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &quot;stack search&quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions of the space of 1408 word alignments. The SBTG can be regarded as a model of the language-universal hypothesis that closely related arguments tend to stay together (Wu, 1995a; Wu, 1995b). In this paper we introduce a generalization of Wu's method with the objectives of 1. increasing translation speed further, 2. improving meaning-preservation accuracy, 3. improving grammaticality of the output, and 4. seeding a natural transition toward transduction rule models, under the constraint of • employing no additional knowledge resources except a grammar for the target language. To achieve these objectives, we: • replace Wu's SBTG channel with a full stochastic inversion transduction grammar or SITG channel, discussed in Section 3, and • (mis-)use the target language gr"
P98-2230,1995.tmi-1.28,1,0.762267,"stic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &quot;stack search&quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions of the space of 1408 word alignments. The SBTG can be regarded as a model of the language-universal hypothesis that closely related arguments tend to stay together (Wu, 1995a; Wu, 1995b). In this paper we introduce a generalization of Wu's method with the objectives of 1. increasing translation speed further, 2. improving meaning-preservation accuracy, 3. improving grammaticality of the output, and 4. seeding a natural transition toward transduction rule models, under the constraint of • employing no additional knowledge resources except a grammar for the target language. To achieve these objectives, we: • replace Wu's SBTG channel with a full stochastic inversion transduction grammar or SITG channel, discussed in Section 3, and • (mis-)use the target language gr"
P98-2230,W95-0106,1,0.892517,"stic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &quot;stack search&quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions of the space of 1408 word alignments. The SBTG can be regarded as a model of the language-universal hypothesis that closely related arguments tend to stay together (Wu, 1995a; Wu, 1995b). In this paper we introduce a generalization of Wu's method with the objectives of 1. increasing translation speed further, 2. improving meaning-preservation accuracy, 3. improving grammaticality of the output, and 4. seeding a natural transition toward transduction rule models, under the constraint of • employing no additional knowledge resources except a grammar for the target language. To achieve these objectives, we: • replace Wu's SBTG channel with a full stochastic inversion transduction grammar or SITG channel, discussed in Section 3, and • (mis-)use the target language gr"
P98-2230,P96-1021,1,0.435448,"wever, unlike pure statistical translation models, the generated output string is guaranteed to conform to a given target grammar. The model employs only (1) a translation lexicon, (2) a context-free grammar for the target language, and (3) a bigram language model. The fact that no explicit bilingual translation rules are used makes the model easily portable to a variety of source languages. Initial experiments show that it also achieves significant speed gains over our earlier model. 1 Motivation Speed of statistical machine translation methods has long been an issue. A step was taken by Wu (Wu, 1996) who introduced a polynomial-time algorithm for the runtime search for an optimal translation. To achieve this, Wu's method substituted a language-independent stochastic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &quot;stack search&quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions"
P98-2230,J97-3002,1,0.529909,"ition i of a length-V Chinese sentence. So V T alignments are possible, yielding an exponential space with correspondingly slow search times. I Various models have been constructed by the IBM team (Brown et al., 1993). This description corresponds to one of the simplest ones, &quot;Model 2&quot;; search costs for the more complex models are correspondingly higher. 1409 3 A SITG Channel Model The translation channel we propose is based on the recently introduced bilingual language modeling approach. The model employs a stochastic version of an inversion transduction grammar or ITG (Wu, 1995c; Wu, 1995d; Wu, 1997). This formalism was originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. Subsequently, a method was developed to use a special case of the I T G R t h e aforementioned B T G R f o r the translation task itself (Wu, 1996). The next few paragraphs briefly review the main properties of ITGs, before we describe the SITG channel. An ITG consists of context-free productions where terminal symbols come in couples, for example x/y, where x is a English word and y is an Chinese translation of x, with singletons of the form x"
R11-1092,W10-3802,1,0.894696,"Missing"
R11-1092,N10-1050,1,0.873507,"Missing"
R11-1092,J97-3002,1,\N,Missing
R13-1077,N10-1028,0,0.0128659,"time from zero or more input tokens, two opposing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned model is crucial for error analysis, but inspecting a typical state-of-the-art translation system is prohibitively hard."
R13-1077,P09-1088,0,0.276814,"Missing"
R13-1077,P09-1104,0,0.0203367,"actually rely on generating one output token at a time from zero or more input tokens, two opposing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned model is crucial for error analysis, but inspecting a typical state-of-"
R13-1077,J93-2003,0,0.0336153,"Missing"
R13-1077,N10-1015,0,0.0130487,"input tokens, two opposing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned model is crucial for error analysis, but inspecting a typical state-of-the-art translation system is prohibitively hard. Phrasal direct transla"
R13-1077,N03-1017,0,0.0239069,"ernal to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned model is crucial for error analysis, but inspecting a typical state-of-the-art translation system is prohibitively hard. Phrasal direct translation systems rely heavily on the language model to compensate for the mistakes they make, as well as relying on a fine-tuned log-linear combination of several features to choose which lexical units to use. Pinning down exactly wh"
R13-1077,W07-0403,0,0.0208788,"3; Vogel et al., 1996), but since no translation systems in use today actually rely on generating one output token at a time from zero or more input tokens, two opposing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned"
R13-1077,P11-1064,0,0.0260965,"Missing"
R13-1077,N09-1025,0,0.013534,"o inspect the learned model is crucial for error analysis, but inspecting a typical state-of-the-art translation system is prohibitively hard. Phrasal direct translation systems rely heavily on the language model to compensate for the mistakes they make, as well as relying on a fine-tuned log-linear combination of several features to choose which lexical units to use. Pinning down exactly where and why an error occurred in this setup is very hard. The transduction grammar based approach is better in this respect, but the state-of-the-art typically relies on massive amounts, tens of thousands (Chiang et al., 2009), of features. As a community, we still have no clear idea of why these features help translation, only that they do when the whole system pipeline is treated as a black box, but treating the system as a black box prevents effective error analysis. The state-of-the-art systems also relies on long and complicated learning pipelines that form ad-hoc models of how translation happens. These adhoc models differ significantly from the models of how translation happens that are used during actual translation, which violates the basic machine learning assumption that the same model should When parse"
R13-1077,P12-1018,0,0.0252622,"Missing"
R13-1077,P05-1033,0,0.0604327,"testing. In contrast, the only difference between biparsing with ITGs (training) and decoding (testing) is that both sentences are given during biparsing, but only the input sentence during decoding—the model itself does not change, only the way it is used. The space of possible ITG structures is intractably large, and there have been many attempts to introduce external constraints to guide the search. We do completely unsupervised search without introducing such constraints, which limits the scope of error analysis to the search strategy. Popular external constraints include word alignments (Chiang, 2005) and parse trees. Word alignments are typically learned as a many-to-one function from one language into the other language (Brown et al., 1993; Vogel et al., 1996), but since no translation systems in use today actually rely on generating one output token at a time from zero or more input tokens, two opposing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are"
R13-1077,P02-1040,0,0.0998753,"89 Chinese–English translation task from IWSLT07 (Fordyce, 2007) as training and test data. In contains 46,867 sentence pairs of training data, and 489 sentence pairs of test data with 6 reference translations each. To decode with the learned model, we use our in-house ITG decoder with a trigram language model learned on the English part of the training data. The decoder uses CKY-style parsing (Cocke, 1969; Kasami, 1965; Younger, 1967) with cube pruning to integrate the language model (Chiang, 2007). The language model is trained with SRILM (Stolcke, 2002). To evaluate the output we use BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). Table 1: The results of decoding. ITG model BLEU NIST Rules Baseline 17.44 4.3909 47,298 Initial only 15.71 4.1267 251,947 Auxiliary only 16.11 3.9334 60,133 Augmented 19.32 4.4243 301,293 still need to calculate the change in the length of the data, which is: ( ) P (D|Φ′ , Ψ) DL D|Φ′ , Ψ −DL (D|Φ, Ψ) = −lg P (D|Φ, Ψ) For the sake of convenience in efficiently calculating this probability, we make the simplifying assumption that: 8 Results P (D|Φ, Ψ) ≈ P (D|Φ) = P (D|θ) The results (Table 1) show the baseline ITG and the proposed augmented ITG, as well as test sco"
R13-1077,P10-1017,0,0.0150374,"sing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned model is crucial for error analysis, but inspecting a typical state-of-the-art translation system is prohibitively hard. Phrasal direct translation systems rely heavi"
R13-1077,C96-2141,0,0.316229,"only the input sentence during decoding—the model itself does not change, only the way it is used. The space of possible ITG structures is intractably large, and there have been many attempts to introduce external constraints to guide the search. We do completely unsupervised search without introducing such constraints, which limits the scope of error analysis to the search strategy. Popular external constraints include word alignments (Chiang, 2005) and parse trees. Word alignments are typically learned as a many-to-one function from one language into the other language (Brown et al., 1993; Vogel et al., 1996), but since no translation systems in use today actually rely on generating one output token at a time from zero or more input tokens, two opposing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007;"
R13-1077,2011.eamt-1.42,1,0.830351,"Missing"
R13-1077,P08-1012,0,0.310361,"Missing"
R13-1077,W09-3804,1,0.938401,"Missing"
R13-1077,N10-1050,1,0.878032,"typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned model is crucial for error analysis, but inspecting a typical state-of-the-art translation system is prohibitively hard. Phrasal direct translation systems rely heavily on the language m"
R13-1077,C12-1142,1,0.84265,"δsum ← δsum + δ ′ 18: end if 19: end for 20: until δsum ≥ 0 21: return Φ Algorithm 2 Iterative rule segmenting learning driven by minimum conditional description length. 1: Φ, Ψ ▷ The auxiliary and initial ITG 2: repeat 3: δsum ← 0 4: bs ← collect_biaffixes(Φ) 5: bδ ← [] 6: for all b ∈ bs do 7: δ ← eval_cdl(b, Ψ, Φ) 8: if δ &lt; 0 then 9: bδ ← [bδ, ⟨b, δ⟩] 10: end if 11: end for 12: sort_by_delta(bδ) 13: for all ⟨b, δ⟩ ∈ bδ do 14: δ ′ ← eval_cdl(b, Ψ, Φ) 15: if δ ′ &lt; 0 then 16: Φ ← make_segmentations(b, Φ) 17: δsum ← δsum + δ ′ 18: end if 19: end for 20: until δsum ≥ 0 21: return Φ Saers et al. (2012). That is: we start by initializing a token-based bracketing finite-state transduction grammar, or FSTG, parameterized with relative frequencies from the training corpus. We then tune the parameters to the training corpus, and then change the structure of the grammar to include lexical rules that can be formed by chunking adjacent preterminals. The tune–chunk step is repeated twice, before transforming the FSTG into a bracketing linear inversion transduction grammar, or LITG (Saers et al., 2010), whose parameters are also tuned to the training corpus. The LITG is then transformed into a full I"
R13-1077,2007.iwslt-1.1,0,\N,Missing
R13-1077,W09-2304,1,\N,Missing
R13-1077,J97-3002,1,\N,Missing
R13-1077,P06-1121,0,\N,Missing
R13-1077,J07-2003,0,\N,Missing
S16-2006,P07-2045,0,0.00396967,"with 10 iterations of expectation maximization where the 59 Alignment2: ITG based alignment Alignment1: GIZA++ based alignment Alignment3: XMEANT as outside probability based alignment Figure 4: Alignments of bisentences produced by the three discussed alignment systems the experiments comparable. We compare the performance of our proposed semantic frame based alignment to the conventional ITG alignment and to the traditional GIZA++ baseline with grow-diag-final-and to harmonize both alignment directions. We tested the different alignments described above by using the standard Moses toolkit (Koehn et al., 2007), and a 6-gram language model learned with the SRI language model toolkit (Stolcke, 2002) to train our model. 4 of the given bisentence. The English sentence can be divided into three major parts: “the Japanese islands”, “run northeast to southwest” and “in the northwest part of the pacific ocean.”. The conventional ITG based alignment only succeeds to align the first part of the sentence. GIZA++ based system correctly aligns part one and parts of part two. We note from the sentence’s gloss (figure 5) that our proposed alignment outperforms the two other alignments by capturing the relevant in"
S16-2006,2012.eamt-1.64,1,0.905784,"Missing"
S16-2006,W05-0909,0,0.0589966,"etrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a good translation is one in which humans can successfully understand the general meaning of the input sentence as captured by the basic event structure: who did what to whom, for whom, when, where, how and why (Pradhan et al., 2004). Recent work have shown that the semantic frame based metric, MEANT, correlates better with human adequacy judgment than most common evaluation metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Algorithm one in figure 2 shows how a MEANT score is computed (Lo and Wu, 2011, 2012; Lo et al., 2012). S→A A → [BC] A → ⟨BC⟩ A → e/ϵ A → ϵ/f A → e/f 2.2.2 XMEANT: crosslingual MEANT XMEANT (Lo et al., 2014) is the crosslingual version of the semantic evaluation metric MEANT. It has been shown that the crosslingual evaluation metric, XMEANT, correlates even better with human adequacy judgment than MEANT, and also better than most evaluation metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002),"
S16-2006,E06-1031,0,0.0381286,"cs (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a good translation is one in which humans can successfully understand the general meaning of the input sentence as captured by the basic event structure: who did what to whom, for whom, when, where, how and why (Pradhan et al., 2004). Recent work have shown that the semantic frame based metric, MEANT, correlates better with human adequacy judgment than most common evaluation metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Algorithm one in figure 2 shows how a MEANT score is computed (Lo and Wu, 2011, 2012; Lo et al., 2012). S→A A → [BC] A → ⟨BC⟩ A → e/ϵ A → ϵ/f A → e/f 2.2.2 XMEANT: crosslingual MEANT XMEANT (Lo et al., 2014) is the crosslingual version of the semantic evaluation metric MEANT. It has been shown that the crosslingual evaluation metric, XMEANT, correlates even better with human adequacy judgment than MEANT, and also better than most evaluation metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie,"
S16-2006,2014.iwslt-evaluation.4,1,0.857462,"Missing"
S16-2006,P11-1023,1,0.84698,"es. Inversion transductions are a subset of transduction which are synchronously generated and parsed by inversion transduction grammars (ITGs, (Wu, 1997)). An ITG can always be written in a 2normal form and it is represented by a tuple ⟨N, V0 , V1 , R, S⟩ where N is a set of nonterminals, V0 and V1 are the bitokens of L0 and L1 respectively, R is a set of transduction rules and S ∈ N is the start symbol. We can write each transduction rule as follows: 2.2 Semantic frame based evaluation metrics 2.2.1 MEANT’s algorithm Unlike n-gram or edit-distance based metrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a good translation is one in which humans can successfully understand the general meaning of the input sentence as captured by the basic event structure: who did what to whom, for whom, when, where, how and why (Pradhan et al., 2004). Recent work have shown that the semantic frame based metric, MEANT, correlates better with human adequacy judgment than most common evaluation metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006"
S16-2006,P09-1088,0,0.0381703,"Missing"
S16-2006,W12-4206,1,0.892759,"Missing"
S16-2006,J90-2002,0,0.705551,"(e, f ) f ∈f e∈e |f| 2 · precei,pred ,fi,pred · recei,pred ,fi,pred precei,pred ,fi,pred + recei,pred ,fi,pred 2 · precei,j ,fi,j · recei,j ,fi,j precei,j ,fi,j + recei,j ,fi,j where the joint probability p is defined as the har57 2.3 Alignment Word alignment is considered to be a necessary step in training SMT systems, it helps to learn bilingual correlations between the input and the output languages. In this work, we compare the alignment produced by our system to the traditional GIZA++ alignment and the conventional ITG alignment. Most of the conventional alignment algorithms: IBM models (Brown et al., 1990) and hidden Markov models or HMM (Vogel et al., 1996) are flat and directed. In fact, (a) they allow the unstructured movement of words leading to a weak word alignment, (b) consider translations in one direction in isolation, and (c) Figure 2: MEANT vs XMEANT algorithms need two separate alignments to form a single bidirectional alignment. The harmonization of two directed alignments is typically done heuristically. This means that there is no model that considers the final bidirectional alignment that the translation system is trained on to be optimal. Inversion transduction grammars (Wu, 19"
S16-2006,2013.mtsummit-papers.12,1,0.894243,"Missing"
S16-2006,W12-3129,1,0.893813,"Missing"
S16-2006,J93-2003,0,0.044517,"Missing"
S16-2006,P13-2067,1,0.88344,"Missing"
S16-2006,W07-0403,0,0.0256263,"Missing"
S16-2006,2013.iwslt-evaluation.5,1,0.845485,"Missing"
S16-2006,J07-2003,0,0.177819,"Missing"
S16-2006,P11-1042,0,0.0129814,"learned while training using ITG constraints gives much more accurate word alignments than those trained on manually annotated treebanks like in Yamada and Knight (2001) in both Chinese-English and German-English. Haghighi et al. (2009) show that using ITG constraints for supervised word alignment methods not only produce alignments without lower alignment error rates but also produces a better translation quality. Some of the previous work on word alignment used morphological and syntactic features (De Gispert et al., 2006). Log linear models have been proposed to incorporate those features (Chris et al., 2011). The problem with those approaches is that they require language specific knowledge and they always work better on more morphological rich languages. A few studies that try to integrate some semantic knowledge in computing word alignment are proposed by Jeff et al. (2011) and Theerawat and David (2014). However, the former needs to have a prior word alignment learned on lexical items. The latter proposes a semantically oriented word alignment, but requires extracting word similarities from the monolingual data first, before producing alignment using word similarities. 3 Adopting XMEANT scores"
S16-2006,P14-2124,1,0.862533,"me alternations (Addanki et al., 2012). There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve a good result. and Ney, 2000) alignments. 2 Related work The choice of XMEANT, a crosslingual version of MEANT (Lo and Wu, 2011, 2012; Lo et al., 2012), is motivated by the work of Lo et al. (2014) who showed that XMEANT can correlate better with human adequacy judgement than most other metrics under some conditions. Furthermore, previous empirical studies have shown that the crosslingual semantic frame matching measured by XMEANT is fully covered within ITG constraints (Addanki et al., 2012). 2.1 Inversion transduction grammars Inversion transduction grammars (ITGs, Wu (1997)) are a subset of syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972). A transduction is a set of bisentences that define the relation between an input language L0 and an output lan"
S16-2006,P11-1064,0,0.0293755,"Missing"
S16-2006,niessen-etal-2000-evaluation,0,0.173154,"Lo et al., 2012) adopt the principle that a good translation is one in which humans can successfully understand the general meaning of the input sentence as captured by the basic event structure: who did what to whom, for whom, when, where, how and why (Pradhan et al., 2004). Recent work have shown that the semantic frame based metric, MEANT, correlates better with human adequacy judgment than most common evaluation metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Algorithm one in figure 2 shows how a MEANT score is computed (Lo and Wu, 2011, 2012; Lo et al., 2012). S→A A → [BC] A → ⟨BC⟩ A → e/ϵ A → ϵ/f A → e/f 2.2.2 XMEANT: crosslingual MEANT XMEANT (Lo et al., 2014) is the crosslingual version of the semantic evaluation metric MEANT. It has been shown that the crosslingual evaluation metric, XMEANT, correlates even better with human adequacy judgment than MEANT, and also better than most evaluation metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al."
S16-2006,P00-1056,0,0.605537,"Missing"
S16-2006,P05-1059,0,0.0279986,"rsion transduction grammars (Wu, 1997). One problem encountered with such model was that the exhaustive biparsing that runs in O(n6 ). Saers et al. (2009) proposed a more efficient algorithm that runs in O(n3 ). Zens and Ney (2003) showed that ITG constraints allow a higher flexibility in word-ordering for longer sentences than the conventional IBM model. Furthermore, they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional IBM models for both German-English and French-English language pairs. Zhang and Gildea (2005) on the other hand showed that the tree learned while training using ITG constraints gives much more accurate word alignments than those trained on manually annotated treebanks like in Yamada and Knight (2001) in both Chinese-English and German-English. Haghighi et al. (2009) show that using ITG constraints for supervised word alignment methods not only produce alignments without lower alignment error rates but also produces a better translation quality. Some of the previous work on word alignment used morphological and syntactic features (De Gispert et al., 2006). Log linear models have been"
S16-2006,P02-1040,0,0.100842,"MEANT’s algorithm Unlike n-gram or edit-distance based metrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a good translation is one in which humans can successfully understand the general meaning of the input sentence as captured by the basic event structure: who did what to whom, for whom, when, where, how and why (Pradhan et al., 2004). Recent work have shown that the semantic frame based metric, MEANT, correlates better with human adequacy judgment than most common evaluation metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Algorithm one in figure 2 shows how a MEANT score is computed (Lo and Wu, 2011, 2012; Lo et al., 2012). S→A A → [BC] A → ⟨BC⟩ A → e/ϵ A → ϵ/f A → e/f 2.2.2 XMEANT: crosslingual MEANT XMEANT (Lo et al., 2014) is the crosslingual version of the semantic evaluation metric MEANT. It has been shown that the crosslingual evaluation metric, XMEANT, correlates even better with human adequacy judgment than MEANT, and also better than most evaluation metrics"
S16-2006,N04-1030,0,0.082054,"nonterminals, V0 and V1 are the bitokens of L0 and L1 respectively, R is a set of transduction rules and S ∈ N is the start symbol. We can write each transduction rule as follows: 2.2 Semantic frame based evaluation metrics 2.2.1 MEANT’s algorithm Unlike n-gram or edit-distance based metrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a good translation is one in which humans can successfully understand the general meaning of the input sentence as captured by the basic event structure: who did what to whom, for whom, when, where, how and why (Pradhan et al., 2004). Recent work have shown that the semantic frame based metric, MEANT, correlates better with human adequacy judgment than most common evaluation metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Algorithm one in figure 2 shows how a MEANT score is computed (Lo and Wu, 2011, 2012; Lo et al., 2012). S→A A → [BC] A → ⟨BC⟩ A → e/ϵ A → ϵ/f A → e/f 2.2.2 XMEANT: crosslingual MEANT XMEANT (Lo et al., 2014) is the crossli"
S16-2006,W09-2304,1,0.780026,"s typically done heuristically. This means that there is no model that considers the final bidirectional alignment that the translation system is trained on to be optimal. Inversion transduction grammars (Wu, 1997), on the other hand, have proven that learning word alignments using a system that is compositionally-structured, can provide optimal bidirectional alignments. Although this structured optimality comes at a higher cost in terms of time complexity, it allows preexisting structured information to be incorporated into the model. It also allows models to be compared in a meaningful way. Saers and Wu (2009) proposed a better method of producing word alignment by training inversion transduction grammars (Wu, 1997). One problem encountered with such model was that the exhaustive biparsing that runs in O(n6 ). Saers et al. (2009) proposed a more efficient algorithm that runs in O(n3 ). Zens and Ney (2003) showed that ITG constraints allow a higher flexibility in word-ordering for longer sentences than the conventional IBM model. Furthermore, they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional"
S16-2006,W09-3804,1,0.906338,"ther hand, have proven that learning word alignments using a system that is compositionally-structured, can provide optimal bidirectional alignments. Although this structured optimality comes at a higher cost in terms of time complexity, it allows preexisting structured information to be incorporated into the model. It also allows models to be compared in a meaningful way. Saers and Wu (2009) proposed a better method of producing word alignment by training inversion transduction grammars (Wu, 1997). One problem encountered with such model was that the exhaustive biparsing that runs in O(n6 ). Saers et al. (2009) proposed a more efficient algorithm that runs in O(n3 ). Zens and Ney (2003) showed that ITG constraints allow a higher flexibility in word-ordering for longer sentences than the conventional IBM model. Furthermore, they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional IBM models for both German-English and French-English language pairs. Zhang and Gildea (2005) on the other hand showed that the tree learned while training using ITG constraints gives much more accurate word alignments than"
S16-2006,N10-1050,1,0.868714,"Missing"
S16-2006,2006.amta-papers.25,0,0.296813,"ciple that a good translation is one in which humans can successfully understand the general meaning of the input sentence as captured by the basic event structure: who did what to whom, for whom, when, where, how and why (Pradhan et al., 2004). Recent work have shown that the semantic frame based metric, MEANT, correlates better with human adequacy judgment than most common evaluation metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Algorithm one in figure 2 shows how a MEANT score is computed (Lo and Wu, 2011, 2012; Lo et al., 2012). S→A A → [BC] A → ⟨BC⟩ A → e/ϵ A → ϵ/f A → e/f 2.2.2 XMEANT: crosslingual MEANT XMEANT (Lo et al., 2014) is the crosslingual version of the semantic evaluation metric MEANT. It has been shown that the crosslingual evaluation metric, XMEANT, correlates even better with human adequacy judgment than MEANT, and also better than most evaluation metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 20"
S16-2006,D14-1197,0,0.0132334,"t methods not only produce alignments without lower alignment error rates but also produces a better translation quality. Some of the previous work on word alignment used morphological and syntactic features (De Gispert et al., 2006). Log linear models have been proposed to incorporate those features (Chris et al., 2011). The problem with those approaches is that they require language specific knowledge and they always work better on more morphological rich languages. A few studies that try to integrate some semantic knowledge in computing word alignment are proposed by Jeff et al. (2011) and Theerawat and David (2014). However, the former needs to have a prior word alignment learned on lexical items. The latter proposes a semantically oriented word alignment, but requires extracting word similarities from the monolingual data first, before producing alignment using word similarities. 3 Adopting XMEANT scores as EM outside probabilities We implemented a token based BITG system as our ITG baseline, our choice of BITG is motivated by previous work that showed that BITG alignments outperformed alignments from GIZA++ (Saers et al., 2009). Figure 3 shows the BITG induction algorithm that we used in this paper. W"
S16-2006,C96-2141,0,0.357644,"Missing"
S16-2006,W95-0106,1,0.268453,"Missing"
S16-2006,J97-3002,1,0.358477,"and surprisingly achieve a good result. and Ney, 2000) alignments. 2 Related work The choice of XMEANT, a crosslingual version of MEANT (Lo and Wu, 2011, 2012; Lo et al., 2012), is motivated by the work of Lo et al. (2014) who showed that XMEANT can correlate better with human adequacy judgement than most other metrics under some conditions. Furthermore, previous empirical studies have shown that the crosslingual semantic frame matching measured by XMEANT is fully covered within ITG constraints (Addanki et al., 2012). 2.1 Inversion transduction grammars Inversion transduction grammars (ITGs, Wu (1997)) are a subset of syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972). A transduction is a set of bisentences that define the relation between an input language L0 and an output language L1 . Accordingly, a transduction grammar is able to generate, translate or accept a transduction or a set of bisentences. Inversion transductions are a subset of transduction which are synchronously generated and parsed by inversion transduction grammars (ITGs, (Wu, 1997)). An ITG can always be written in a 2normal form and it is represented by a tuple ⟨N, V0 , V1 , R, S⟩ where"
S16-2006,P01-1067,0,0.0240503,"). Zens and Ney (2003) showed that ITG constraints allow a higher flexibility in word-ordering for longer sentences than the conventional IBM model. Furthermore, they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional IBM models for both German-English and French-English language pairs. Zhang and Gildea (2005) on the other hand showed that the tree learned while training using ITG constraints gives much more accurate word alignments than those trained on manually annotated treebanks like in Yamada and Knight (2001) in both Chinese-English and German-English. Haghighi et al. (2009) show that using ITG constraints for supervised word alignment methods not only produce alignments without lower alignment error rates but also produces a better translation quality. Some of the previous work on word alignment used morphological and syntactic features (De Gispert et al., 2006). Log linear models have been proposed to incorporate those features (Chris et al., 2011). The problem with those approaches is that they require language specific knowledge and they always work better on more morphological rich languages."
W02-2035,P00-1027,0,\N,Missing
W03-0433,N01-1025,0,0.0250955,"Schapire and Singer, 2000) and word sense disambiguation (Escudero et al., 2000). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to"
W03-0433,W02-2020,0,0.0354852,"In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. The experiments present"
W03-0433,N01-1006,1,0.827825,"ces. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. 3 3.1 Data Resources Preprocessing the Data The data that was provided by the CoNLL organizers was sentence-delimited and tokenized, and hand-annotated with named entity chunks. The English data was automatically labeled with part-of-speech and chunk tags from the memory-based tagger and chunker (Daelemans et al., 1996), and the German data was labelled with the decision-tree-based TreeTagger (Schmidt, 1994). We replaced the English part-of-speech tags with those generated by a trans"
W03-0433,C00-2102,0,0.0435389,"(Escudero et al., 2000). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be"
W03-0433,W02-2035,1,0.640843,"increasingly focusing more heavily on the instances that the previous classifiers found difficult to classify. For the boosting framework, our system uses AdaBoost.MH (Freund and Schapire, 1997), an n-ary classification variant of the original binary AdaBoost algorithm. It performs well on a number of natural language processing problems, including text categorization (Schapire and Singer, 2000) and word sense disambiguation (Escudero et al., 2000). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill,"
W03-0433,W96-0102,0,\N,Missing
W03-0433,W02-2004,0,\N,Missing
W03-0433,J95-4004,0,\N,Missing
W04-0306,P81-1022,0,0.364218,"uce parser follows in order to construct P . At any given point, T will hold the LR table computed from the MAL parse of all sentences already processed, and A will hold the corresponding parsing sequences for the MAL parses. Entering the loop, we iteratively augment T by adding the states arising from the MAL parse F ∗ of each successive sentence in the training corpus and, in addition, cache the corresponding parsing action sequence A (F ∗ ) into the set A. This is done by first computing a chart for the sentence, in line 4, by parsing Si under the constraining grammar GC using the standard Earley (1970) procedure. We then call find MAL parse in line 5, to compute the parse that requires minimum average lookahead to resolve ambiguity. The items and states produced by the rules in F ∗ are added to the LR table T by calling incremental update LR in line 6, and the parsing action sequence of F ∗ is appended to A in line 7. Note that the indices of the original states in T are not changed and only items are added into them if need be so that A is not changed by adding items and states to T, and there might be new states introduced which are also indexed. By definition, the true MAL grammar does n"
W04-0306,N01-1021,0,0.0195807,"ders the claim of incremental parsing meaningless. Marcus simply postulated that a maximum buffer size of three was sufficient. In contrast, our approach permits greater flexibility and finer gradations, where the average degree of lookahead required can be minimized with the aim of assisting grammar induction. Since Marcus’ work, a significant body of work on incremental parsing has developed in the sentence processing community, but much of this work has actually suggested models with an increased amount of nondeterminism, often with probabilistic weights (e.g., Narayanan & Jurafsky (1998); Hale (2001)). Meanwhile, in the way of formal methods, Tomita (1986) introduced Generalized LR parsing, which offers an interesting hybrid of nondeterministic dynamic programming surrounding LR parsing methods that were originally deterministic. Additionally, methods for determinizing and minimizing finite-state machines are well known (e.g., Mohri (2000), B al & Carton (1968)). However, such methods (a) do not operate at the contextfree level, (b) do not directly minimize lookahead, and (c) do not induce grammars under environmental constraints. Unfortunately, there has still been relatively little work"
W04-0306,P97-1062,0,0.0183887,"eneralized LR parsing, which offers an interesting hybrid of nondeterministic dynamic programming surrounding LR parsing methods that were originally deterministic. Additionally, methods for determinizing and minimizing finite-state machines are well known (e.g., Mohri (2000), B al & Carton (1968)). However, such methods (a) do not operate at the contextfree level, (b) do not directly minimize lookahead, and (c) do not induce grammars under environmental constraints. Unfortunately, there has still been relatively little work on automatic learning of grammars for deterministic parsers to date. Hermjakob & Mooney (1997) describe a semi-automatic procedure for learning a deterministic parser from a treebank, which requires the intervention of a human expert in the loop to determine appropriate derivation order, to resolve parsing conflicts between certain actions such as “merge” and “add-into”, and to identify specific features for disambiguating actions. In our earlier work we described a deterministic parser with a fully automatically learned decision algorithm (Wong and Wu, 1999). But unlike our present work, the decision algorithms in both Hermjakob & Mooney (1997) and Wong & Wu (1999) are procedural; the"
W04-0306,J93-1002,0,\N,Missing
W04-0822,W02-2004,0,0.140229,"Missing"
W04-0822,W02-1002,0,0.117981,"of ensembles utilizing various combinations of four voting models, as follows. Some of these component models were also evaluated on other Senseval-3 tasks: the Basque, Catalan, Italian, and Romanian Lexical Sample tasks (Wicentowski et al., 2004), as well as Semantic Role Labeling (Ngai et al., 2004). The first voting model, a na¨ıve Bayes model, was built as Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model, a maximum entropy model (Jaynes, 1978), was built as Klein and Manning (2002) found that it yielded higher accuracy than na¨ıve Bayes in a subsequent comparison of WSD performance. However, note that a different subset of either Senseval-1 or Senseval-2 English lexical sample data was used. The third voting model, a boosting model (Freund and Schapire, 1997), was built as boosting has consistently turned in very competitive scores on related tasks such as named entity classification (Carreras et al., 2002)(Wu et al., 2002). Specifically, we employed an AdaBoost.MH model (Schapire and Singer, 2000), which is a multi-class generalization of the original boosting algorith"
W04-0822,P03-1004,0,0.0341894,"from their original space R n to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). As with other kernel methods, a major advantage of KPCA over other common analysis techniques is that it can inherently take combinations of predictive features into account when optimizing dimensionality reduction. For WSD and indeed many natural language tasks, significant accuracy gains can often be achieved by generalizing over relevant feature combinations (see, e.g., Kudo and Matsumoto (2003)). A further advantage of KPCA in the context of the WSD problem is that the dimensionality of the input data is generally very large, a condition where kernel methods excel. Nonlinear principal components (Diamantaras and Kung, 1996) are defined as follows. Suppose we are given a training set of M pairs (x t , ct ) where the observed vectors xt ∈ Rn in an n-dimensional input space X represent the context of the target word being disambiguated, and the correct class c t represents the sense of the word, for t = 1, .., M . Suppose Φ is a nonlinear mapping from the input space Rn to the feature"
W04-0822,W04-0845,1,0.757264,"ace Rn to the feature space F . Without loss of generality we assume the M vectors P are centered vectors in the feature space, i.e., M t=1 Φ (xt ) = 0; uncentered vectors can easily be converted to centered vectors (Sch¨olkopf et al., 1998). We wish to Ensemble classification The WSD models presented here consist of ensembles utilizing various combinations of four voting models, as follows. Some of these component models were also evaluated on other Senseval-3 tasks: the Basque, Catalan, Italian, and Romanian Lexical Sample tasks (Wicentowski et al., 2004), as well as Semantic Role Labeling (Ngai et al., 2004). The first voting model, a na¨ıve Bayes model, was built as Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model, a maximum entropy model (Jaynes, 1978), was built as Klein and Manning (2002) found that it yielded higher accuracy than na¨ıve Bayes in a subsequent comparison of WSD performance. However, note that a different subset of either Senseval-1 or Senseval-2 English lexical sample data was used. The third voting model, a boosting model (Freund and Schapire, 1"
W04-0822,P04-1081,1,0.748075,"e new voting model typically degrades the accuracy of the ensemble instead of helping it. In this work, we investigate the potential of one promising new disambiguation model with respect 1 The author would like to thank the Hong Kong Research Grants Council (RGC) for supporting this research in part through research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. to augmenting our existing ensemble combining a maximum entropy model, a boosting model, and a na¨ıve Bayes model—a combination representing some of the best stand-alone WSD models currently known. The new WSD model, proposed by Wu et al. (2004), is a method for disambiguating word senses that exploits a nonlinear Kernel Principal Component Analysis (KPCA) technique. That the KPCA-based model could potentially be a good candidate for a new voting model is suggested by Wu et al.’s empirical results showing that it yielded higher accuracies on Senseval-2 data sets than other models that included maximum entropy, na¨ıve Bayes, and SVM based models. In the following sections, we begin with a description of the experimental setup, which utilizes a number of individual classifiers in a voting ensemble. We then describe the KPCA-based model"
W04-0822,W02-2035,1,\N,Missing
W04-0845,P98-1013,0,0.013804,"in Senseval-3. Results show that these systems, which are based upon common machine learning algorithms, all manage to achieve good performances on the non-restricted Semantic Role Labeling task. 1 Introduction This paper describes the HKPolyU-HKUST systems which participated in the Senseval-3 Semantic Role Labeling task. The systems represent a diverse array of machine learning algorithms, from decision lists to SVMs to Winnow-type networks. Semantic Role Labeling (SRL) is a task that has recently received a lot of attention in the NLP community. The SRL task in Senseval-3 used the Framenet (Baker et al., 1998) corpus: given a sentence instance from the corpus, a system’s job would be to identify the phrase constituents and their corresponding role. The Senseval-3 task was divided into restricted and non-restricted subtasks. In the non-restricted subtask, any and all of the gold standard annotations contained in the FrameNet corpus could be used. Since this includes information on the boundaries of the parse constituents which correspond to some frame element, this effectively maps the SRL task to that of a role-labeling classification task: given a constituent parse, identify the frame element that"
W04-0845,W04-0822,1,0.826871,"Missing"
W04-0845,P97-1003,0,0.0320672,"to the target word. • The position (e.g. before, after) of the constituent, with respect to the target word. In addition to the above features, we also extracted a set of features which required the use of some statistical NLP tools: • Transitivity and voice of the target word — The sentence was first part-of-speech tagged and chunked with the fnTBL transformationbased learning tools (Ngai and Florian, 2001). Simple heuristics were then used to deduce the transitivity voice of the target word. • Head word (and its part-of-speech tag) of the constituent — After POS tagging, a syntactic parser (Collins, 1997) was then used to obtain the parse tree for the sentence. The head word (and the POS tag of the head word) of the syntactic parse constituent whose span corresponded most closely to the candidate constituent was then assumed to be the head word of the candidate constituent. The resulting training data set consisted of 51,366 constituent samples with a total of 151 frame element types. These ranged from “Descriptor” (3520 constituents) to “Baggage” and “Carrier” (1 constituent each). This training data was randomly partitioned into a 80/20 “development training” and “validation” set. 3 Methodol"
W04-0845,J02-3001,0,0.217534,"part through research grants A-PE37 and 4-Z03S. 2 The author would like to thank the Hong Kong Research Grants Council (RGC) for supporting this research in part through research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. 2 Experimental Features This section describes the features that were used for the SRL task. Since the non-restricted SRL task is essentially a classification task, each parse constituent that was known to correspond to a frame element was considered to be a sample. The features that we used for each sample have been previously shown to be helpful for the SRL task (Gildea and Jurafsky, 2002). Some of these features can be obtained directly from the Framenet annotations: • The name of the frame. • The lexical unit of the sentence — i.e. the lexical identity of the target word in the sentence. • The general part-of-speech tag of the target word. • The “phrase type” of the constituent — i.e. the syntactic category (e.g. NP, VP) that the constituent falls into. • The “grammatical function” (e.g. subject, object, modifier, etc) of the constituent, with respect to the target word. • The position (e.g. before, after) of the constituent, with respect to the target word. In addition to th"
W04-0845,W99-0621,0,0.0510482,"Missing"
W04-0845,N01-1006,1,0.808888,"e “phrase type” of the constituent — i.e. the syntactic category (e.g. NP, VP) that the constituent falls into. • The “grammatical function” (e.g. subject, object, modifier, etc) of the constituent, with respect to the target word. • The position (e.g. before, after) of the constituent, with respect to the target word. In addition to the above features, we also extracted a set of features which required the use of some statistical NLP tools: • Transitivity and voice of the target word — The sentence was first part-of-speech tagged and chunked with the fnTBL transformationbased learning tools (Ngai and Florian, 2001). Simple heuristics were then used to deduce the transitivity voice of the target word. • Head word (and its part-of-speech tag) of the constituent — After POS tagging, a syntactic parser (Collins, 1997) was then used to obtain the parse tree for the sentence. The head word (and the POS tag of the head word) of the syntactic parse constituent whose span corresponded most closely to the candidate constituent was then assumed to be the head word of the candidate constituent. The resulting training data set consisted of 51,366 constituent samples with a total of 151 frame element types. These ran"
W04-0845,W04-0862,0,0.0617167,"Missing"
W04-0845,C98-1013,0,\N,Missing
W04-0863,W04-0845,1,0.577242,"Missing"
W04-0863,W04-0862,1,0.8031,"as a joint effort between Swarthmore College, Hong Kong PolyU, and HKUST. Though other models described elsewhere contributed to the final combination model, this paper focuses solely on the joint contributions to the ”Swat-HK” effort. 1 Introduction This paper describes the two joint component models of the Swat-HK systems entered into four of the word sense disambiguation lexical sample tasks in Senseval-3: Basque, Catalan, Italian and Romanian, as well as a combination model for each language. The feature engineering (and construction of three other component models which are described in (Wicentowski et al., 2004)) was performed at Swarthmore College, while the Hong Kong team constructed two component models based on wellknown machine learning algorithms. The combination model, which was constructed at Swarthmore, uses voting to combine all five models. 2 Experimental Features A full description of the experimental features for all four tasks can be found in the report submitted by the Swarthmore College Senseval team (Wicentowski et al., 2004). Briefly, the systems used lexical and syntactic features in the context of the target word: • The “bag of words (and lemmas)” in the context of the ambiguous w"
W05-1205,2003.mtsummit-papers.32,0,0.018956,"qq (`(q)) (`(q)) As mentioned earlier, biparsing for ITGs can be accomplished efficiently in polynomial time, rather than the exponential time required for classical SDTGs. The result in Wu (1997) implies that for the special case of Bracketing ITGs, the time complexity of the algorithm  is Θ T 3 V 3 where T and V are the lengths of the two sentences. This is a factor of V 3 more than monolingual chart parsing, but has turned out to remain quite practical for corpus analysis, where parsing need not be real-time. The ITG scoring model can also be seen as a variant of the approach described by Leusch et al. (2003), which allows us to forego training to estimate true probabilities; instead, rules are simply given unit weights. The ITG scores can be interpreted as a generalization of classical Levenshtein string edit distance, where inverted block transpositions are also allowed. Even without probability estimation, Leusch et al. found excellent correlation with human judgment of similarity between translated paraphrases. 4 methods, making it difficult to make specific claims about distributional neutrality, due to the arbitrary nature of the example selection process. The ITG scoring model produced an u"
W05-1205,W04-3219,0,0.0979958,"n particular the Pascal Recognising Textual Entailment Challenge Corpus (Dagan et al., 2005), provide testbeds that abstract over many tasks, including information retrieval, comparable documents, reading comprehension, question answering, information extraction, machine translation, and paraphrase acquisition. At the same time, the emergence of paraphrasing datasets presents an opportunity for complementary experiments on the task of recognizing symmetric bidirectional entailment rather than asymmetric directional entailment. In particular, for this study we employ the MSR Paraphrase Corpus (Quirk et al., 2004). 25 Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 25–30, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics 2 Inversion Transduction Grammars Formally, ITGs can be defined as the restricted subset of syntax-directed transduction grammars or SDTGs Lewis and Stearns (1968) where all of the rules are either of straight or inverted orientation. Ordinary SDTGs allow any permutation of the symbols on the right-hand side to be specified when translating from the input language to the output language. In contrast, ITGs only all"
W05-1205,I05-1023,1,0.74307,"Missing"
W05-1205,P95-1033,1,0.650082,"e. Designating this category A, a Bracketing ITG has the following form (where, as usual, lexical transductions of the form A → e/f may possibly be singletons of the form A → e/ or A → /f ). → → → → ... A → A A A A [AA] hAAi ,  e1 /f1 ei /fj The simplest class of ITGs, Bracketing ITGs, are particularly interesting in applications like paraphrasing, because they impose ITG constraints in languageindependent fashion, and in the simplest case do not require any language-specific linguistic grammar or training. In Bracketing ITGs, the grammar uses only a single, undifferentiated non-terminal (Wu, 1995). The key modeling property of Bracketing ITGs that is most relevant to paraphrase recognition is that they assign strong preference to candidate paraphrase pairs in which nested constituent subtrees can be recursively aligned with a minimum of constituent boundary violations. Unlike language-specific linguistic approaches, however, the shape of the trees are driven in unsupervised fashion by the data. One way to view this is that the trees are hidden explanatory variables. This not only provides significantly higher robustness than more highly constrained manually constructed grammars, but al"
W05-1205,J97-3002,1,0.908861,"accountable/ ]VV [to/ [the/ [[Financial/ Secretary/ ]NN ]NNN ]NP ]PP iVP ]VP ]SP ./ ]S 5 In a weighted or stochastic ITG (SITG), a weight or a probability is associated with each rewrite rule. Following the standard convention, we use a and b to denote probabilities for syntactic and lexical rules, respectively. 0.4 For example, the probability of the rule NN → [A N] is 0.001 aNN→[A N] = 0.4. The probability of a lexical rule A → x/y is bA (x, y) = 0.001. Let W1 , W2 be the vocabulary sizes of the two languages, and N = {A1 , . . . , AN } be the set of nonterminals with indices 1, . . . , N . Wu (1997) also showed that ITGs can be equivalently be defined in two other ways. First, ITGs can be defined as the restricted subset of SDTGs where all rules are of rank 2. Second, ITGs can also be defined as the restricted subset of SDTGs where all rules are of rank 3. 26 Polynomial-time algorithms are possible for various tasks including translation using ITGs, as well as bilingual parsing or biparsing, where the task is to build the highest-scored parse tree given an input bi-sentence. For present purposes we can employ the special case of Bracketing ITGs, where the grammar employs only one single,"
W05-1205,P01-1067,0,0.0133222,"ly to yield both efficiency and accuracy gains for numerous language acquisition tasks, across a variety of language pairs and tasks. For example, Zens and Ney (2003) show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) find that unsupervised alignment using Bracketing ITGs produces significantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model (Yamada and Knight, 2001). With regard to translation rather than alignment accuracy, Zens et al. (2004) show that decoding under ITG constraints yields significantly lower word error rates and BLEU scores than the IBM constraints. We are conducting a series of investigations motivated by the following observation: the empirically demonstrated suitability of ITG paraphrasing constraints across languages should hold, if anything, even more strongly in the monolingual case. The monolingual case allows in some sense closer testing of various implications of the ITG hypothesis, without irrelevant dimensions of variation a"
W05-1205,P03-1019,0,0.0211166,"g opportu1 The author would like to thank the Hong Kong Research Grants Council (RGC) for supporting this research in part through grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09, and Marine Carpuat and Yihai Shen for invaluable assistance in preparing the datasets and stoplist. nities for meaningful analysis of the ITG Hypothesis in a monolingual setting. The strong inductive bias imposed by the ITG Hypothesis has been repeatedly shown empirically to yield both efficiency and accuracy gains for numerous language acquisition tasks, across a variety of language pairs and tasks. For example, Zens and Ney (2003) show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) find that unsupervised alignment using Bracketing ITGs produces significantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model (Yamada and Knight, 2001). With regard to translation rather than alignment accuracy, Zens et al. (2004) show that decoding under ITG constraints yields significantly"
W05-1205,C04-1030,0,0.0305645,"Missing"
W05-1205,C04-1060,0,0.0109946,"ng the datasets and stoplist. nities for meaningful analysis of the ITG Hypothesis in a monolingual setting. The strong inductive bias imposed by the ITG Hypothesis has been repeatedly shown empirically to yield both efficiency and accuracy gains for numerous language acquisition tasks, across a variety of language pairs and tasks. For example, Zens and Ney (2003) show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) find that unsupervised alignment using Bracketing ITGs produces significantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model (Yamada and Knight, 2001). With regard to translation rather than alignment accuracy, Zens et al. (2004) show that decoding under ITG constraints yields significantly lower word error rates and BLEU scores than the IBM constraints. We are conducting a series of investigations motivated by the following observation: the empirically demonstrated suitability of ITG paraphrasing constraints across languages should hold, if a"
W05-1205,W07-1401,0,\N,Missing
W06-0124,W02-2004,0,0.235444,"Missing"
W06-0124,C04-1058,1,0.837143,"extensive theoretical and empirical studies, where different standard machine learning methods have been used as the weak classifier (e.g., Bauer and Kohavi (1999), Opitz and Maclin (1999), Schapire (2002)). It also performs well on a number of natural language processing problems, including text categorization (e.g., Schapire and Singer (2000), Sebastiani et al. (2000)) and word sense disambiguation (e.g., Escudero et al. (2000)). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al. (2002), Wu et al. (2004), Carreras et al. (2002)). The weak classifiers used in the boosting algorithm come from a wide range of machine learning methods. We have chosen to use a simple classifier called a decision stump in the algorithm. A decision stump is basically a one-level decision tree where the split at the root level is based on a specific attribute/value pair. For example, a possible attribute/value pair could be W2 = ™/. Boosting The main idea behind the boosting algorithm is that a set of many simple and moderately accurate weak classifiers (also called weak hypotheses) can be effectively combined to yie"
W06-0124,W03-1709,0,0.171093,"Missing"
W06-0124,W02-2035,1,\N,Missing
W09-2304,P06-1002,0,0.0269673,"Missing"
W09-2304,2007.mtsummit-papers.14,0,0.0627909,"Missing"
W09-2304,W07-0403,0,0.379146,"Missing"
W09-2304,P05-1033,0,0.216217,"Missing"
W09-2304,2005.mtsummit-papers.11,0,0.0331301,"Missing"
W09-2304,W07-0734,0,0.0644471,"Missing"
W09-2304,P00-1056,0,0.476316,"Missing"
W09-2304,P03-1021,0,0.174322,"Missing"
W09-2304,W06-3117,0,0.030922,"Missing"
W09-2304,P95-1033,1,0.85149,"Missing"
W09-2304,W95-0106,1,0.804478,"Missing"
W09-2304,P96-1021,1,0.761648,"Missing"
W09-2304,J97-3002,1,0.872351,"Missing"
W09-2304,P98-2230,1,0.799746,"Missing"
W09-2304,P05-1059,0,0.213271,"Missing"
W09-2304,W03-0303,0,0.0431593,"Missing"
W09-2304,J93-2003,0,\N,Missing
W09-2304,C96-2141,0,\N,Missing
W09-2304,P02-1040,0,\N,Missing
W09-2304,P07-2045,0,\N,Missing
W09-2304,N03-1017,0,\N,Missing
W09-2304,J03-1002,0,\N,Missing
W09-2304,C98-2225,1,\N,Missing
W09-2304,N06-1033,0,\N,Missing
W09-3804,P07-2045,0,0.0103169,"re made with different pruning parameters. The EM process was halted when a relative improvement in log-likelihood of 10−3 was no longer achieved over the previous iteration. Evaluation We evaluate the parser on a translation task (WMT’08 shared task3 ). In order to evaluate on a translation task, a translation system has to be built. We use the alignments from the Viterbi parses of the training corpus to substitute the alignments of GIZA++. This is the same approach as taken in Saers & Wu (2009). We will evaluate the resulting translations with two automatic 3 Setup We use the Moses Toolkit (Koehn et al., 2007) to train our phrase-based SMT models. The toolkit also includes scripts for applying GIZA++ (Och and Ney, 2003) as a word aligner. We have trained several systems, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100). We used the grow-diag-final-and method to extract phrases from the word alignment, and MERT (Och, 2003) to optimize the resulting model. We trained a 5-gram SRI language model (Stolcke, 2002) using the corpus supplied for this purpose by the shared task organizers. All of the above is consistent with the guid"
W09-3804,2005.mtsummit-papers.11,0,0.0102206,"s, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100). We used the grow-diag-final-and method to extract phrases from the word alignment, and MERT (Och, 2003) to optimize the resulting model. We trained a 5-gram SRI language model (Stolcke, 2002) using the corpus supplied for this purpose by the shared task organizers. All of the above is consistent with the guidelines for building a baseline system for the WMT’08 shared task. The translation tasks we applied the above procedure to are all taken from the Europarl corpus (Koehn, 2005). We selected the tasks German-English, French-English and SpanishEnglish. Furthermore, we restricted the training sentence pairs so that none of the sentences exceeded length 10. This was necessary to be able to carry out exhaustive search. The total amount of training data was roughly 100,000 sentence pairs in each language pair, which is a relatively small corpus, but by no means a toy example. Analysis 5 Empirical results http://www.statmt.org/wmt08/ 31 Metric Baseline (GIZA++) ∞ BLEU NIST time 0.2597 6.6352 0.2663 6.7407 03:20:00 BLEU NIST time 0.2059 5.8668 0.2113 5.9380 03:40:00 BLEU NI"
W09-3804,J03-1002,0,0.0233874,"he item being extended. This fact can be exploited to limit the number of possible siblings explored, but has no impact on time complexity. 3.3 metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 6 In this section we describe the experimental setup as well as the outcomes. Viterbi parsing When doing Viterbi parsing, all derivations but the most probable are discarded. This gives an unambiguous parse, which dictates exactly one alignment between e and f . The alignment of the Viterbi parse can be used to substitute that of other word aligners (Saers and Wu, 2009) such as GIZA++ (Och and Ney, 2003). 4 6.1 Looking at the algorithm, it is clear that there will be a total of T + V = O(n) agendas, each containing items of a certain length. The items in an agenda can start anywhere in the alignment space: O(n2 ) possible starting points, but once the end point in one language is set, the end point in the other follows from that, adding a factor O(n). This means that each agenda contains O(n3 ) active items. Each active item has to go through all possible siblings in the recursive step. Since the start point of the sibling is determined by the item itself (it has to be adjacent), only the O(n"
W09-3804,P03-1021,0,0.00690123,"pus to substitute the alignments of GIZA++. This is the same approach as taken in Saers & Wu (2009). We will evaluate the resulting translations with two automatic 3 Setup We use the Moses Toolkit (Koehn et al., 2007) to train our phrase-based SMT models. The toolkit also includes scripts for applying GIZA++ (Och and Ney, 2003) as a word aligner. We have trained several systems, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100). We used the grow-diag-final-and method to extract phrases from the word alignment, and MERT (Och, 2003) to optimize the resulting model. We trained a 5-gram SRI language model (Stolcke, 2002) using the corpus supplied for this purpose by the shared task organizers. All of the above is consistent with the guidelines for building a baseline system for the WMT’08 shared task. The translation tasks we applied the above procedure to are all taken from the Europarl corpus (Koehn, 2005). We selected the tasks German-English, French-English and SpanishEnglish. Furthermore, we restricted the training sentence pairs so that none of the sentences exceeded length 10. This was necessary to be able to carry"
W09-3804,P02-1040,0,0.106966,"ld a set of extensions E(i) for all active items i. All items in E(i) are then activated by placing them on their corresponding agenda (i ∈ A|i |). E(Xstuv ) = {XStU v |0 ≤ S ≤ s, 0 ≤ U ≤ u, XSsU u ∈ C} ∪ {XsSuU |t ≤ S ≤ T, v ≤ U ≤ V, XtSvU ∈ C} ∪ {XsSU v |t ≤ S ≤ T, 0 ≤ U ≤ u, XtSU u ∈ C} ∪ {XStuU |0 ≤ S ≤ s, v ≤ U ≤ V, XSsvU ∈ C} 30 Since we are processing the agendas in order, any item in the chart will be as long as or shorter than the item being extended. This fact can be exploited to limit the number of possible siblings explored, but has no impact on time complexity. 3.3 metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 6 In this section we describe the experimental setup as well as the outcomes. Viterbi parsing When doing Viterbi parsing, all derivations but the most probable are discarded. This gives an unambiguous parse, which dictates exactly one alignment between e and f . The alignment of the Viterbi parse can be used to substitute that of other word aligners (Saers and Wu, 2009) such as GIZA++ (Och and Ney, 2003). 4 6.1 Looking at the algorithm, it is clear that there will be a total of T + V = O(n) agendas, each containing items of a certain length. The items in an agenda"
W09-3804,W09-2304,1,0.789481,"will be as long as or shorter than the item being extended. This fact can be exploited to limit the number of possible siblings explored, but has no impact on time complexity. 3.3 metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 6 In this section we describe the experimental setup as well as the outcomes. Viterbi parsing When doing Viterbi parsing, all derivations but the most probable are discarded. This gives an unambiguous parse, which dictates exactly one alignment between e and f . The alignment of the Viterbi parse can be used to substitute that of other word aligners (Saers and Wu, 2009) such as GIZA++ (Och and Ney, 2003). 4 6.1 Looking at the algorithm, it is clear that there will be a total of T + V = O(n) agendas, each containing items of a certain length. The items in an agenda can start anywhere in the alignment space: O(n2 ) possible starting points, but once the end point in one language is set, the end point in the other follows from that, adding a factor O(n). This means that each agenda contains O(n3 ) active items. Each active item has to go through all possible siblings in the recursive step. Since the start point of the sibling is determined by the item itself (i"
W09-3804,W95-0106,1,0.822411,"as they must be completely made up of terminals. For notational convenience, the orientation of the rule is written as surrounding the production, like so: X → γ, X → [γ] and X → hγi. The vocabularies of the languages may both include the empty token , allowing for deletions and insertions. The empty biterminal, / is not allowed. 2.1 Stochastic ITGs In a Stochastic ITG (SITG), each rule is also associated with a probability, such that X P r(X → γ) = 1 γ for all X ∈ N . The probability of a deriva∗ tion S ⇒ e/f is defined as the production of the probabilities of all rules used. As shown by Wu (1995), it is possible to fit the parameters of a SITG to a parallel corpus via EM (expectationmaximization) estimation. 2.2 3.1 Bracketing ITGs In the initial step, the set of lexical items L is built. All lexical items i ∈ L are then activated by placing them on their corresponding agenda A|i |.   0 ≤ s ≤ t ≤ T,   L = Xstuv 0 ≤ u ≤ v ≤ V,  X → es..t /fu..v ∈ ∆  An ITG where there is only one nonterminal (other than the start symbol) is called a bracketing ITG (BITG). Since the one nonterminal is devoid of information, it can only be used to group its children together, imposing a bracketing"
W09-3804,J97-3002,1,\N,Missing
W09-3805,P06-1146,0,0.221643,"Missing"
W09-3805,H05-1101,0,0.172609,"s at the level of translation units, it is natural to interpret them conjunctively. Otherwise we would in reality count failures at the level of alignments. (iv) We use (c). The conjunctive interpretation of translation units was also adopted by Fox (2002) and is motivated by the importance of translation units and discontinuous ones in particular to machine translation in general (Simard and colleagues, 2005; Ayan and Dorr, 2006; Macken, 2007; Shieber, 2007). In brief, 2 Related work Aho and Ullman (1972) showed that 4-ary synchronous context-free grammars (SCFGs) could not be binarized, and Satta and Peserico (2005) showed that the hiearchy of SCFGs beyond ternary ones does not collapse; they also showed that the complexity of the universal recognition problem for SCFGs is NP-complete. ITGs on the other hand has a O(|G|n6 ) solvable universal recognition problem, which coincides with the unrestricted alignment problem (Søgaard, 2009). The result extends to decoding in conjunction with a bigram language model (Huang et al., 2005). Wu (1997) introduced ITGs and normal form ITGs. ITGs are a notational variant of the subclass of SCFGs such that all indexed nonterminals in the source side of the RHS occur in"
W09-3805,W02-1039,0,0.0554008,"dy uses the data also used by Søgaard and Kuhn (2009), which to the best of our knowledge uses the largest collection of handaligned parallel corpora used in any of these studies. (ii) Failures are counted at the level of translation units as argued for in the above, but supplemented by parse failure rates for completeness. (iii) Since we count failures at the level of translation units, it is natural to interpret them conjunctively. Otherwise we would in reality count failures at the level of alignments. (iv) We use (c). The conjunctive interpretation of translation units was also adopted by Fox (2002) and is motivated by the importance of translation units and discontinuous ones in particular to machine translation in general (Simard and colleagues, 2005; Ayan and Dorr, 2006; Macken, 2007; Shieber, 2007). In brief, 2 Related work Aho and Ullman (1972) showed that 4-ary synchronous context-free grammars (SCFGs) could not be binarized, and Satta and Peserico (2005) showed that the hiearchy of SCFGs beyond ternary ones does not collapse; they also showed that the complexity of the universal recognition problem for SCFGs is NP-complete. ITGs on the other hand has a O(|G|n6 ) solvable universal"
W09-3805,J07-3002,0,0.0386341,"B nothing/pas] and B → [change/modifie], for example, induces a DTU with a gap in the French side for the pair of substrings hchange nothing, ne modifie pasi. Multigap DTUs with up to three gaps are frequent (Søgaard and Kuhn, 2009) and have shown to be important for translation quality (Simard and colleagues, 2005). While normal form ITGs do not induce multigap DTUs, ITGs induce a particular subclass of multigap DTUs, namely those that are constructed by linear or inverse interpolation. 4 Discussion The usefulness of alignment error rate (AER) (Och and Ney, 2000) has been questioned lately (Fraser and Marcu, 2007); most importantly, AER does not always seem to correlate with translation quality. TUER is likely to correlate better with translation quality, since it by definition correlates with CPER (Ayan and Dorr, 2006). No large-scale experiment has been done yet to estimate the strength of this correlation. Our study also relies on the assumption that simulatenously recognized words are aligned in bilingual parsing. The relationship between parsing and alignment can of course be complicated in ways that will alter the alignment capacity of ITG and its normal form; on some definitions the two formalis"
W09-3805,H05-1095,0,0.565888,"el corpora used in any of these studies. (ii) Failures are counted at the level of translation units as argued for in the above, but supplemented by parse failure rates for completeness. (iii) Since we count failures at the level of translation units, it is natural to interpret them conjunctively. Otherwise we would in reality count failures at the level of alignments. (iv) We use (c). The conjunctive interpretation of translation units was also adopted by Fox (2002) and is motivated by the importance of translation units and discontinuous ones in particular to machine translation in general (Simard and colleagues, 2005; Ayan and Dorr, 2006; Macken, 2007; Shieber, 2007). In brief, 2 Related work Aho and Ullman (1972) showed that 4-ary synchronous context-free grammars (SCFGs) could not be binarized, and Satta and Peserico (2005) showed that the hiearchy of SCFGs beyond ternary ones does not collapse; they also showed that the complexity of the universal recognition problem for SCFGs is NP-complete. ITGs on the other hand has a O(|G|n6 ) solvable universal recognition problem, which coincides with the unrestricted alignment problem (Søgaard, 2009). The result extends to decoding in conjunction with a bigram l"
W09-3805,P04-1064,0,0.100379,"y an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004; Wellington et al., 2006; Søgaard and Kuhn, 2009). As noted by Søgaard (2009), this definition of alignment has the advantageous consequence that candidate alignments can be singled out by mere inspection of the grammar rules. It also has the consequence that alignments are transitive (Goutte et al., 2004), since simultaneity is transitive. While previous work (Søgaard and Kuhn, 2009) has estimated empirical lower bounds for normal form ITGs at the level of translation units (TUER), or cepts (Goutte et al., 2004), defined as maximally connected subgraphs in alignments, nobody has done this for the full class of ITGs. What is important to understand is that while normal form ITGs can induce the same class of translations as the full class of ITGs, they do not induce the same class of alignments. They do not, for exEmpirical lower bounds studies in which the frequency of alignment configurations"
W09-3805,W09-2303,1,0.687645,"or the full class of inversion transduction grammars Dekai Wu Anders Søgaard Human Language Technology Center Center for Language Technology Hong Kong Univ. of Science and Technology University of Copenhagen dekai@cs.ust.hk soegaard@hum.ku.dk Abstract While it is easy to estimate the consequences of restrictions to n-grams of limited size, it is less trivial to estimate the consequences of the structural constraints imposed by syntax-based machine translation formalisms. Consequently, much work has been devoted to this task (Wu, 1997; Zens and Ney, 2003; Wellington et al., 2006; Macken, 2007; Søgaard and Kuhn, 2009). The task of estimating the consequences of the structural constraints imposed by a particular syntax-based formalism consists in finding what is often called “empirical lower bounds” on the coverage of the formalism (Wellington et al., 2006; Søgaard and Kuhn, 2009). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work that enables us to introduce a meaningful"
W09-3805,graca-etal-2008-building,0,0.0692754,"cal item in many cases. 34 3.1 different sides, i.e. D has a gap in the source side that contains at least one token in E, and E has a gap in the target side that contains at least one token in D. Here’s an example of a bonbon configuration from Simard et al. (2005): Data The characteristics of the hand-aligned gold standard parallel corpora used are presented in Figure 1. The Danish-Spanish text is part of the Copenhagen Dependency Treebank (Parole), English-German is from Pado and Lapata (2006) (Europarl), and the six combinations of English, French, Portuguese and Spanish are documented in Graca et al. (2008) (Europarl). 3.2 Pierre ne mange pas Pierre does not eat Multigap DTUs with mixed transfer are, as already mentioned multigap DTUs with crossing alignments from material in two distinct gaps. Alignment configurations 3.3 Results The full class of ITGs induces many alignment configurations that normal form ITGs do not induce, incl. discontinuous translation units (DTUs), i.e. translation units with at least one gap, doublesided DTUs, i.e. DTUs with both a gap in the source side and a gap in the target side, and multigap DTUs with arbitrarily many gaps (as long as the contents in the gap are eit"
W09-3805,W09-2308,1,0.909515,"unds” on the coverage of the formalism (Wellington et al., 2006; Søgaard and Kuhn, 2009). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004; Wellington et al., 2006; Søgaard and Kuhn, 2009). As noted by Søgaard (2009), this definition of alignment has the advantageous consequence that candidate alignments can be singled out by mere inspection of the grammar rules. It also has the consequence that alignments are transitive (Goutte et al., 2004), since simultaneity is transitive. While previous work (Søgaard and Kuhn, 2009) has estimated empirical lower bounds for normal form ITGs at the level of translation units (TUER), or cepts (Goutte et al., 2004), defined as maximally connected subgraphs in alignments, nobody has done this for the full class of ITGs. What is important to understand is that while normal"
W09-3805,W05-1507,0,0.0604146,", 2006; Macken, 2007; Shieber, 2007). In brief, 2 Related work Aho and Ullman (1972) showed that 4-ary synchronous context-free grammars (SCFGs) could not be binarized, and Satta and Peserico (2005) showed that the hiearchy of SCFGs beyond ternary ones does not collapse; they also showed that the complexity of the universal recognition problem for SCFGs is NP-complete. ITGs on the other hand has a O(|G|n6 ) solvable universal recognition problem, which coincides with the unrestricted alignment problem (Søgaard, 2009). The result extends to decoding in conjunction with a bigram language model (Huang et al., 2005). Wu (1997) introduced ITGs and normal form ITGs. ITGs are a notational variant of the subclass of SCFGs such that all indexed nonterminals in the source side of the RHS occur in the same order or exactly in the inverse order in the target side of the RHS. It turns out that this subclass of SCFGs defines the same set of translations that can be defined by binary SCFGs. The different forms of production rules are listed below with the more restricted normal form production rules in the right column, with φ ∈ (N ∪ {e/f |e ∈ T ∗ , f ∈ T ∗ })∗ (N nonterminals and T terminals, as usual). The RHS op"
W09-3805,P06-1123,0,0.827122,"bounds on translation unit error rate for the full class of inversion transduction grammars Dekai Wu Anders Søgaard Human Language Technology Center Center for Language Technology Hong Kong Univ. of Science and Technology University of Copenhagen dekai@cs.ust.hk soegaard@hum.ku.dk Abstract While it is easy to estimate the consequences of restrictions to n-grams of limited size, it is less trivial to estimate the consequences of the structural constraints imposed by syntax-based machine translation formalisms. Consequently, much work has been devoted to this task (Wu, 1997; Zens and Ney, 2003; Wellington et al., 2006; Macken, 2007; Søgaard and Kuhn, 2009). The task of estimating the consequences of the structural constraints imposed by a particular syntax-based formalism consists in finding what is often called “empirical lower bounds” on the coverage of the formalism (Wellington et al., 2006; Søgaard and Kuhn, 2009). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work th"
W09-3805,J97-3002,1,0.658865,"a particular syntax-based formalism consists in finding what is often called “empirical lower bounds” on the coverage of the formalism (Wellington et al., 2006; Søgaard and Kuhn, 2009). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004; Wellington et al., 2006; Søgaard and Kuhn, 2009). As noted by Søgaard (2009), this definition of alignment has the advantageous consequence that candidate alignments can be singled out by mere inspection of the grammar rules. It also has the consequence that alignments are transitive (Goutte et al., 2004), since simultaneity is transitive. While previous work (Søgaard and Kuhn, 2009) has estimated empirical lower bounds for normal form ITGs at the level of translation units (TUER), or cepts (Goutte et al., 2004), defined as maximally connected subgraphs in alignments,"
W09-3805,P03-1019,0,0.0581106,"the target side of the RHS. It turns out that this subclass of SCFGs defines the same set of translations that can be defined by binary SCFGs. The different forms of production rules are listed below with the more restricted normal form production rules in the right column, with φ ∈ (N ∪ {e/f |e ∈ T ∗ , f ∈ T ∗ })∗ (N nonterminals and T terminals, as usual). The RHS operator [ ] preserves source language constituent order in the target language, while h i reverses it.1 A A → → [φ] hφi A A A → → → [BC] hBCi e/f Several studies have adressed the alignment capacity of ITGs and normal form ITGs. Zens and Ney (2003) induce lower bounds on PRFs for normal form ITGs. Wellington et al. (2006) induce lower bounds on PRFs for ITGs. Søgaard and Kuhn (2009) induce lower bounds on TUER for normal form ITGs and more expressive formalisms for syntax-based machine translation. No one has, however, to the best our knowledge induced lower bounds on TUER for ITGs. TUER = 1 − 2|SU ∩ GU | |SU |+ |GU | where GU are the translation units in the gold standard, and SU the translation units produced by the system. This evaluation measure is related to consistent phrase error rate (CPER) introduced in Ayan and Dorr (2006), ex"
W09-3805,C00-2163,0,0.068155,"e combination of the production rules A → [ǫ/ne B nothing/pas] and B → [change/modifie], for example, induces a DTU with a gap in the French side for the pair of substrings hchange nothing, ne modifie pasi. Multigap DTUs with up to three gaps are frequent (Søgaard and Kuhn, 2009) and have shown to be important for translation quality (Simard and colleagues, 2005). While normal form ITGs do not induce multigap DTUs, ITGs induce a particular subclass of multigap DTUs, namely those that are constructed by linear or inverse interpolation. 4 Discussion The usefulness of alignment error rate (AER) (Och and Ney, 2000) has been questioned lately (Fraser and Marcu, 2007); most importantly, AER does not always seem to correlate with translation quality. TUER is likely to correlate better with translation quality, since it by definition correlates with CPER (Ayan and Dorr, 2006). No large-scale experiment has been done yet to estimate the strength of this correlation. Our study also relies on the assumption that simulatenously recognized words are aligned in bilingual parsing. The relationship between parsing and alignment can of course be complicated in ways that will alter the alignment capacity of ITG and i"
W09-3805,C04-1060,0,0.0191821,"ar syntax-based formalism consists in finding what is often called “empirical lower bounds” on the coverage of the formalism (Wellington et al., 2006; Søgaard and Kuhn, 2009). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004; Wellington et al., 2006; Søgaard and Kuhn, 2009). As noted by Søgaard (2009), this definition of alignment has the advantageous consequence that candidate alignments can be singled out by mere inspection of the grammar rules. It also has the consequence that alignments are transitive (Goutte et al., 2004), since simultaneity is transitive. While previous work (Søgaard and Kuhn, 2009) has estimated empirical lower bounds for normal form ITGs at the level of translation units (TUER), or cepts (Goutte et al., 2004), defined as maximally connected subgraphs in alignments, nobody has done this fo"
W09-3805,W07-0412,0,\N,Missing
W09-3805,P06-1002,0,\N,Missing
W10-1724,P02-1040,0,0.0791218,"→eC A→Be A→eC A→Be A→ A→f C A→Bf A→Cf A→f B A→ 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus German–English Europarl German–English news commentary English news commentary German–English news commentary German–English news commentary Type out of domain in-domain in-domain in-domain tuning data in-domain test data Size 1,219,343 sentence pairs 86,941 sentence pairs 48,653,884 sentences 2,051 sentence pairs 2,489 sentence pairs Table 1: Corpora available for the German–English translation task after baseline cleaning. System GIZA++ SBLITG SBLITG (only Europarl) SBLITG (only news) GIZA++ and SBLITG GIZA++ and SBLITG (only Europarl) GIZA++ and SBLI"
W10-1724,W09-2304,1,0.799204,"d one L2 ) synchronized CFG rules: ITG L1 ,L2 CFG L1 φ While training a Stochastic Bracketing ITG (SBITG) or LITG (SBLITG) with EM, expectations of probabilities over the biparse-forest are calculated. These expectations approach the true probabilities, and can be used as approximations. The probabilities over the biparse-forest can be used to select the one-best parse-tree, which in turn forces an alignment over the sentence pair. The alignments given by SBITGs and SBLITGs has been shown to give better translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. CFG L2 A→[BC] A→BC A→BC A→hBCi A→BC A→CB A → e/f A→e A→f Inducing an ITG from a parallel corpus is still slow, as the time complexity is O(Gn6 ). Several ways to get around this has been proposed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual g"
W10-1724,W09-3804,1,0.712078,"zed CFG rules: ITG L1 ,L2 CFG L1 φ While training a Stochastic Bracketing ITG (SBITG) or LITG (SBLITG) with EM, expectations of probabilities over the biparse-forest are calculated. These expectations approach the true probabilities, and can be used as approximations. The probabilities over the biparse-forest can be used to select the one-best parse-tree, which in turn forces an alignment over the sentence pair. The alignments given by SBITGs and SBLITGs has been shown to give better translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. CFG L2 A→[BC] A→BC A→BC A→hBCi A→BC A→CB A → e/f A→e A→f Inducing an ITG from a parallel corpus is still slow, as the time complexity is O(Gn6 ). Several ways to get around this has been proposed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual grammars are linear g"
W10-1724,N10-1050,1,0.31957,"1 ,L2 CFG L1 φ While training a Stochastic Bracketing ITG (SBITG) or LITG (SBLITG) with EM, expectations of probabilities over the biparse-forest are calculated. These expectations approach the true probabilities, and can be used as approximations. The probabilities over the biparse-forest can be used to select the one-best parse-tree, which in turn forces an alignment over the sentence pair. The alignments given by SBITGs and SBLITGs has been shown to give better translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. CFG L2 A→[BC] A→BC A→BC A→hBCi A→BC A→CB A → e/f A→e A→f Inducing an ITG from a parallel corpus is still slow, as the time complexity is O(Gn6 ). Several ways to get around this has been proposed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual grammars are linear grammars (LGs): LITG L"
W10-1724,J93-2003,0,0.0146871,"to get around this has been proposed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual grammars are linear grammars (LGs): LITG L1 ,L2 A → [ e/f C A → [ B e/f A → h e/f C A → h B e/f A → / ] ] i i LG L1 LGL2 A→eC A→Be A→eC A→Be A→ A→f C A→Bf A→Cf A→f B A→ 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus German–English Europarl German–English news commentary English news commentary German–English news commentary German–En"
W10-1724,P09-1104,0,0.0989571,"G), the grammar constant (G) can be eliminated, but O(n6 ) is still prohibitive for large data sets. There has been some work on approximate inference of ITGs. Zhang et al. (2008) present a method for evaluating spans in the sentence pair to determine whether they should be excluded or not. The algorithm has a best case time complexity of O(n3 ). Saers, Nivre & Wu (2009) introduce a beam pruning scheme, which reduces time complexity to O(bn3 ). They also show that severe pruning is possible without significant deterioration in alignment quality (as measured by downstream translation quality). Haghighi et al. (2009) use a simpler aligner as guidance for pruning, which reduces the time complexity by two orders of magnitude. Their work also partially implements the phrasal ITGs for translationdriven segmentation introduced in Wu (1997), although they only allow for one-to-many alignments, rather than many-to-many alignments. A more extreme approach is taken in Saers, Nivre & Wu (2010). Not only is the search severely pruned, but the grammar itself is limited to a lin167 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 167–171, c Uppsala, Sweden, 15-16 July 201"
W10-1724,P07-1065,0,0.0244674,"German–English translation task after baseline cleaning. System GIZA++ SBLITG SBLITG (only Europarl) SBLITG (only news) GIZA++ and SBLITG GIZA++ and SBLITG (only Europarl) GIZA++ and SBLITG (only news) BLEU 17.88 17.61 17.46 15.49 17.66 17.58 17.48 NIST 5.9748 5.8846 5.8491 5.4987 5.9650 5.9819 5.9693 Table 2: Results for the German–English translation task. with both. We also combined all three SBLITG systems with the baseline system to see whether the additional translation paths would help. The system we submitted corresponds to the “GIZA ++ and SBLITG (only news)” system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. This was because we lacked the necessary RAM resources to calculate the full SRILM model before the system submission deadline. We chose to focus on the German–English translation task. The corpora resources available for that task is summarized in Table 1. We used the entire news commentary monolingual data concatenated with the English side of the Europarl bilingual data to train the language model. In retrospect, this was probably a bad choice, as others seem to prefer the use of two language models instead. We contrasted the baseline system with pure S"
W10-1724,C96-2141,0,0.186002,"ed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual grammars are linear grammars (LGs): LITG L1 ,L2 A → [ e/f C A → [ B e/f A → h e/f C A → h B e/f A → / ] ] i i LG L1 LGL2 A→eC A→Be A→eC A→Be A→ A→f C A→Bf A→Cf A→f B A→ 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus German–English Europarl German–English news commentary English news commentary German–English news commentary German–English news commentary Type out of d"
W10-1724,J97-3002,1,0.335182,"air to determine whether they should be excluded or not. The algorithm has a best case time complexity of O(n3 ). Saers, Nivre & Wu (2009) introduce a beam pruning scheme, which reduces time complexity to O(bn3 ). They also show that severe pruning is possible without significant deterioration in alignment quality (as measured by downstream translation quality). Haghighi et al. (2009) use a simpler aligner as guidance for pruning, which reduces the time complexity by two orders of magnitude. Their work also partially implements the phrasal ITGs for translationdriven segmentation introduced in Wu (1997), although they only allow for one-to-many alignments, rather than many-to-many alignments. A more extreme approach is taken in Saers, Nivre & Wu (2010). Not only is the search severely pruned, but the grammar itself is limited to a lin167 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 167–171, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics This means that LITGs are transduction grammars that transduce between linear languages. There is also a nice parallel in search time complexities between CFGs and ITGs on"
W10-1724,P08-1012,0,0.0607247,"TGs nor SDTGs are intuitively useful in translating natural languages, since STGs have no way to model reordering, and SDTGs require exponential time to be induced from examples (parallel corpora). Since Lately, there has been some interest in using Inversion Transduction Grammars (ITGs) for alignment purposes. The main problem with ITGs is the time complexity, O(Gn6 ) doesn’t scale well. By limiting the grammar to a bracketing ITG (BITG), the grammar constant (G) can be eliminated, but O(n6 ) is still prohibitive for large data sets. There has been some work on approximate inference of ITGs. Zhang et al. (2008) present a method for evaluating spans in the sentence pair to determine whether they should be excluded or not. The algorithm has a best case time complexity of O(n3 ). Saers, Nivre & Wu (2009) introduce a beam pruning scheme, which reduces time complexity to O(bn3 ). They also show that severe pruning is possible without significant deterioration in alignment quality (as measured by downstream translation quality). Haghighi et al. (2009) use a simpler aligner as guidance for pruning, which reduces the time complexity by two orders of magnitude. Their work also partially implements the phrasa"
W10-1724,P07-2045,0,\N,Missing
W10-1724,J03-1002,0,\N,Missing
W10-1724,P03-1021,0,\N,Missing
W10-3802,J93-2003,0,0.053371,"parallel corpora an approximate search for parses is needed. The trade-off between speed and end-toend translation quality is investigated and compared to Inversion Transduction Grammars (Wu, 1997) and the standard tool for word alignment, Background Any form of automatic translation that relies on generalizations of observed translations needs to align these translations on a sub-sentential level. The standard way of doing this is by aligning words, which works well for languages that use white space separators between words. The standard method is a combination of the family of IBM-models (Brown et al., 1993) and Hidden Markov Models (Vogel et al., 1996). These methods all arrive at a function (A) from language 1 (F ) to language 2 (E). By running the process in both directions, two functions can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statisti"
W10-3802,P81-1022,0,0.735882,"refer to that pair of terminals as a biterminal, which will be written as e/f . Any SDTG can be rephrased to contain permuted nonterminal productions and biterminal productions only, and we will call this the normal form of SDTGs. Note that it is not possible to produce a two-normal form for SDTGs, as there are some rules that are not binarizable (Wu, 1997; Huang et al., 2009). This is an important point to make, since efficient parsing for CFGs is based on either restricting parsing to only handle binary grammars (Cocke, 1969; Kasami, 1965; Younger, 1967), or rely on onthe-fly binarization (Earley, 1970). When translating with a grammar, parsing only has to be done in F , which is binarizable (since it is a CFG ), and can therefor be computed in polynomial time (O(n3 )). Once there is a parse tree for F , the corresponding tree for E can be easily constructed. When inducing a grammar from examples, however, biparsing (finding an analysis that is consistent across a sentence pair) is needed. The time complexity for biparsing with SDTGs is O(n2n+2 ), which is clearly intractable. Inversion Transduction Grammars or ITGs (Wu, 1997) are transduction grammars that have a two-normal form, thus guara"
W10-3802,P09-1104,0,0.0427671,"directions, two functions can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars ( SDTGs). To differentiate identical nonterminal symbols, indices were used (the b"
W10-3802,J09-4009,0,0.0165327,"terminals are pair up with the empty string (ǫ). A → X x B X a B ; 0, 1, 2, 3 X a → a, ǫ X x → ǫ, x Lexical rules involving the empty string are referred to as singletons. Whenever a preterminal is used to pair up two terminal symbols, we refer to that pair of terminals as a biterminal, which will be written as e/f . Any SDTG can be rephrased to contain permuted nonterminal productions and biterminal productions only, and we will call this the normal form of SDTGs. Note that it is not possible to produce a two-normal form for SDTGs, as there are some rules that are not binarizable (Wu, 1997; Huang et al., 2009). This is an important point to make, since efficient parsing for CFGs is based on either restricting parsing to only handle binary grammars (Cocke, 1969; Kasami, 1965; Younger, 1967), or rely on onthe-fly binarization (Earley, 1970). When translating with a grammar, parsing only has to be done in F , which is binarizable (since it is a CFG ), and can therefor be computed in polynomial time (O(n3 )). Once there is a parse tree for F , the corresponding tree for E can be easily constructed. When inducing a grammar from examples, however, biparsing (finding an analysis that is consistent across"
W10-3802,P07-2045,0,0.00937191,"03:10 35 17:00 1:49 6.7312 6.7101 6.6657 6.6637 6.6464 6.6464 Training times 38:00 1:20:00 3:40 7:33 Table 2: Results for the Spanish–English translation task. out (see table 1). The GIZA++ system was built according to the instructions for creating a baseline system for the Fifth Workshop on Statistical Machine Translation (WMT’10),1 but the above corpora were used instead of those supplied by the workshop. This includes word alignment with GIZA++, a 5-gram language model built with SRILM (Stolcke, 2002) and parameter tuning with MERT (Och, 2003). To carry out the actual translations, Moses (Koehn et al., 2007) was used. The SBITG and SBLTG systems were built in exactly the same way, except that the alignments from GIZA++ were replaced by those from the respective grammars. In addition to trying out exhaustive biparsing 1 http://www.statmt.org/wmt10/ 15 for SBITGs and SBLTGs on three different translation tasks, several different levels of pruning were tried (1, 10, 25, 50, 75 and 100). We also used the grammar induced from SBLTGs with a beam size of 25 to seed SBITGs (see section 5), which were then run for an additional iteration of EM, also with beam size 25. All systems are evaluated with BLEU ("
W10-3802,2005.mtsummit-papers.11,0,0.0193828,"study the impact of pruning on efficiency and translation quality. Initial grammars will be estimated by counting cooccurrences in the training corpus, after which expectation-maximization (EM) will be used to refine the initial estimate. At the last iteration, the one-best parse of each sentence will be considered as the word alignment of that sentence. In order to keep the experiments comparable, relatively small corpora will be used. If larger corpora were used, it would not be possible to get any results for unpruned SBITGs because of the prohibitive time complexity. The Europarl corpus (Koehn, 2005) was used as a starting point, and then all sentence pairs where one of the sentences were longer than 10 tokens were filtered Figure 1: Trade-offs between translation quality (as measured by BLEU) and biparsing time (in seconds plotted on a logarithmic scale) for SBLTGs, SBITGs and the combination. System 1 10 Beam size 50 25 75 100 ∞ 0.2661 0.2625 0.2597 0.2671 0.2633 0.2597 0.2663 0.2628 0.2597 6.7329 6.6714 6.6464 6.7445 6.6863 6.6464 6.6793 6.6765 6.6464 2:00:00 9:44 2:40:00 12:13 3:20:00 11:59 BLEU SBITG SBLTG GIZA++ 0.1234 0.2574 0.2597 0.2608 0.2645 0.2597 0.2655 0.2631 0.2597 0.2653 0"
W10-3802,J03-1002,0,0.0148984,"Missing"
W10-3802,P03-1021,0,0.00600772,"++ SBITG SBLTG 3.9705 6.6023 6.6464 6.6439 6.6800 6.6464 03:10 35 17:00 1:49 6.7312 6.7101 6.6657 6.6637 6.6464 6.6464 Training times 38:00 1:20:00 3:40 7:33 Table 2: Results for the Spanish–English translation task. out (see table 1). The GIZA++ system was built according to the instructions for creating a baseline system for the Fifth Workshop on Statistical Machine Translation (WMT’10),1 but the above corpora were used instead of those supplied by the workshop. This includes word alignment with GIZA++, a 5-gram language model built with SRILM (Stolcke, 2002) and parameter tuning with MERT (Och, 2003). To carry out the actual translations, Moses (Koehn et al., 2007) was used. The SBITG and SBLTG systems were built in exactly the same way, except that the alignments from GIZA++ were replaced by those from the respective grammars. In addition to trying out exhaustive biparsing 1 http://www.statmt.org/wmt10/ 15 for SBITGs and SBLTGs on three different translation tasks, several different levels of pruning were tried (1, 10, 25, 50, 75 and 100). We also used the grammar induced from SBLTGs with a beam size of 25 to seed SBITGs (see section 5), which were then run for an additional iteration of"
W10-3802,P02-1040,0,0.0841085,"was used. The SBITG and SBLTG systems were built in exactly the same way, except that the alignments from GIZA++ were replaced by those from the respective grammars. In addition to trying out exhaustive biparsing 1 http://www.statmt.org/wmt10/ 15 for SBITGs and SBLTGs on three different translation tasks, several different levels of pruning were tried (1, 10, 25, 50, 75 and 100). We also used the grammar induced from SBLTGs with a beam size of 25 to seed SBITGs (see section 5), which were then run for an additional iteration of EM, also with beam size 25. All systems are evaluated with BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 7 Results The results for the three different translation tasks are presented in Tables 2, 3 and 4. It is interesting to note that the trend they portray is quite similar. When the beam is very narrow, GIZA++ is better, but already at beam size 10, both transduction grammars are superior. ConSystem 1 10 Beam size 50 25 75 100 ∞ 0.2668 0.2672 0.2603 0.2655 0.2662 0.2603 0.2663 0.2649 0.2603 6.8068 6.8020 6.6907 6.8088 6.7925 6.6907 6.8151 6.7784 6.6907 2:10:00 9:35 2:45:00 13:56 3:10:00 10:52 BLEU SBITG SBLTG GIZA++ 0.1268 0.2600 0.2603 0.2632 0.2638 0.2603 0.2654"
W10-3802,W09-2304,1,0.787343,"the process in both directions, two functions can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars ( SDTGs). To differentiate identical nonterminal symbols, i"
W10-3802,W09-3804,1,0.888955,"ns can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars ( SDTGs). To differentiate identical nonterminal symbols, indices were used (the bag of nonterminals f"
W10-3802,N10-1050,1,0.576259,"that Linear Transduction Grammars (LTGs) generate the same transductions as Linear Inversion Transduction Grammars, and present a scheme for arriving at LTGs by bilingualizing Linear Grammars. We also present a method for obtaining Inversion Transduction Grammars from Linear (Inversion) Transduction Grammars, which can speed up grammar induction from parallel corpora dramatically. 2 1 Introduction In this paper we introduce Linear Transduction Grammars ( LTGs), which are the bilingual case of Linear Grammars ( LGs). We also show that LTG s are equal to Linear Inversion Transduction Grammars (Saers et al., 2010). To be able to induce transduction grammars directly from parallel corpora an approximate search for parses is needed. The trade-off between speed and end-toend translation quality is investigated and compared to Inversion Transduction Grammars (Wu, 1997) and the standard tool for word alignment, Background Any form of automatic translation that relies on generalizations of observed translations needs to align these translations on a sub-sentential level. The standard way of doing this is by aligning words, which works well for languages that use white space separators between words. The stan"
W10-3802,C96-2141,0,0.410167,"rses is needed. The trade-off between speed and end-toend translation quality is investigated and compared to Inversion Transduction Grammars (Wu, 1997) and the standard tool for word alignment, Background Any form of automatic translation that relies on generalizations of observed translations needs to align these translations on a sub-sentential level. The standard way of doing this is by aligning words, which works well for languages that use white space separators between words. The standard method is a combination of the family of IBM-models (Brown et al., 1993) and Hidden Markov Models (Vogel et al., 1996). These methods all arrive at a function (A) from language 1 (F ) to language 2 (E). By running the process in both directions, two functions can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Bei"
W10-3802,J97-3002,1,0.885768,"rom Linear (Inversion) Transduction Grammars, which can speed up grammar induction from parallel corpora dramatically. 2 1 Introduction In this paper we introduce Linear Transduction Grammars ( LTGs), which are the bilingual case of Linear Grammars ( LGs). We also show that LTG s are equal to Linear Inversion Transduction Grammars (Saers et al., 2010). To be able to induce transduction grammars directly from parallel corpora an approximate search for parses is needed. The trade-off between speed and end-toend translation quality is investigated and compared to Inversion Transduction Grammars (Wu, 1997) and the standard tool for word alignment, Background Any form of automatic translation that relies on generalizations of observed translations needs to align these translations on a sub-sentential level. The standard way of doing this is by aligning words, which works well for languages that use white space separators between words. The standard method is a combination of the family of IBM-models (Brown et al., 1993) and Hidden Markov Models (Vogel et al., 1996). These methods all arrive at a function (A) from language 1 (F ) to language 2 (E). By running the process in both directions, two f"
W10-3802,P08-1012,0,0.0612347,"e 2 (E). By running the process in both directions, two functions can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars ( SDTGs). To differentiate identical no"
W10-3807,E06-1032,0,\N,Missing
W10-3807,brants-2000-inter,0,\N,Missing
W10-3807,W07-0738,0,\N,Missing
W10-3807,P02-1040,0,\N,Missing
W10-3807,D07-1007,1,\N,Missing
W10-3807,P07-1005,0,\N,Missing
W10-3807,W07-0718,0,\N,Missing
W10-3807,W06-3114,0,\N,Missing
W10-3807,P01-1017,0,\N,Missing
W10-3807,J05-1004,0,\N,Missing
W10-3807,W05-0904,0,\N,Missing
W10-3807,W08-0309,0,\N,Missing
W10-3807,N09-2004,1,\N,Missing
W10-3807,2009.eamt-1.30,1,\N,Missing
W10-3807,N04-1030,0,\N,Missing
W11-1002,W05-0909,0,0.315702,"eflects the assumption that a semantic frame that covers more tokens contributes more to the overall sentence translation. We demonstrate empirically that when the degree of each frame’s contribution to its sentence is taken into account, the properly structured role representation is more accurate and intuitive than the flattened role representation for SRL MT evaluation metrics. For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000). These metrics are excellent at ranking overall systems by averaging their scores over entire documents. However, as MT systems improve, the shortcomings of such metrics are becoming more apparent. Though containing roughly the correct words, MT output at the sentence remains often quite incomprehensible, and fails to preserve the meaning of the input. This results from the fact that n-gram based metrics are not as reliable at ranking the adequacy of translations of individual sentences, and are particularl"
W11-1002,W07-0738,0,0.41367,"Missing"
W11-1002,W08-0332,0,0.188443,"Missing"
W11-1002,W06-3114,0,0.139861,"the meaning of the input. This results from the fact that n-gram based metrics are not as reliable at ranking the adequacy of translations of individual sentences, and are particularly 10 Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 10–20, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics poor at reflecting translation quality improvements involving more meaningful word sense or semantic frame decisions—which human judges have no trouble distinguishing. Callison-Burch et al. (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. Newer avenues of research seek substitutes for n-gram based MT evaluation metrics that are better at evaluating translation adequacy, particularly at the sentence level. One line of research emphasizes more the structural correctness of translation. Liu and Gildea (2005) propose STM, a metric based on syntactic structure, that addresses the failure of lexical similarity based metrics to evaluate translation grammaticality. However, the problem remains that a grammatical translation can ach"
W11-1002,E06-1031,0,0.290148,"o preserve the meaning of the input. This results from the fact that n-gram based metrics are not as reliable at ranking the adequacy of translations of individual sentences, and are particularly 10 Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 10–20, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics poor at reflecting translation quality improvements involving more meaningful word sense or semantic frame decisions—which human judges have no trouble distinguishing. Callison-Burch et al. (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. Newer avenues of research seek substitutes for n-gram based MT evaluation metrics that are better at evaluating translation adequacy, particularly at the sentence level. One line of research emphasizes more the structural correctness of translation. Liu and Gildea (2005) propose STM, a metric based on syntactic structure, that addresses the failure of lexical similarity based metrics to evaluate translation grammaticality. However, the problem remains that a gramm"
W11-1002,W05-0904,0,0.0789477,"utational Linguistics poor at reflecting translation quality improvements involving more meaningful word sense or semantic frame decisions—which human judges have no trouble distinguishing. Callison-Burch et al. (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. Newer avenues of research seek substitutes for n-gram based MT evaluation metrics that are better at evaluating translation adequacy, particularly at the sentence level. One line of research emphasizes more the structural correctness of translation. Liu and Gildea (2005) propose STM, a metric based on syntactic structure, that addresses the failure of lexical similarity based metrics to evaluate translation grammaticality. However, the problem remains that a grammatical translation can achieve a high syntaxbased score yet still make significant errors arising from confusion of semantic roles. On the other hand, despite the fact that non-automatic, manually evaluated metrics, such as HTER (Snover et al., 2006), are more adequacy oriented exhibit much higher correlation with human adequacy judgment, their high labor cost prohibits widespread use. There has also"
W11-1002,lo-wu-2010-evaluating,1,0.895941,"erall sentence. The new results also show that, without flattening the structure of semantic frames, weighting the degree of each frame’s contribution gives HMEANT higher correlations than the previously bestperforming flattened model, as well as HTER. 1 Introduction In this paper we provide a more concrete answer to the question: what would be a better representation, structured or flat, of the roles in semantic frames to be used in a semantic machine translation (MT) evaluation metric? We compare recent studies on the MEANT family of semantic role labeling (SRL) based MT evaluation metrics (Lo and Wu, 2010a,b, 2011a,b) by (1) contrasting their variations in semantic role representation and observing disturbing comparative results indicating that segregating the event frames in structured role representation actually damages correlation against human adequacy judgments and (2) showing how SRL based MT evaluation can be improved beyond the current state-of-the-art compared to previous MEANT variants as well as HTER, through the introduction of a simple weighting scheme that reflects the degree of contribution of each semantic frame to the overall sentence. The weighting scheme we propose uses a s"
W11-1002,W10-3807,1,0.840107,"erall sentence. The new results also show that, without flattening the structure of semantic frames, weighting the degree of each frame’s contribution gives HMEANT higher correlations than the previously bestperforming flattened model, as well as HTER. 1 Introduction In this paper we provide a more concrete answer to the question: what would be a better representation, structured or flat, of the roles in semantic frames to be used in a semantic machine translation (MT) evaluation metric? We compare recent studies on the MEANT family of semantic role labeling (SRL) based MT evaluation metrics (Lo and Wu, 2010a,b, 2011a,b) by (1) contrasting their variations in semantic role representation and observing disturbing comparative results indicating that segregating the event frames in structured role representation actually damages correlation against human adequacy judgments and (2) showing how SRL based MT evaluation can be improved beyond the current state-of-the-art compared to previous MEANT variants as well as HTER, through the introduction of a simple weighting scheme that reflects the degree of contribution of each semantic frame to the overall sentence. The weighting scheme we propose uses a s"
W11-1002,P11-1023,1,0.445504,"asic event structure—“who did what to whom, when, where and why” (Pradhan et al., 2004)–which represents the most essential meaning of the source utterances. Adopting this principle, the MEANT family of metrics compare the semantic frames in reference translations against those that can be reconstructed from machine translation output. Preliminary results reported in (Lo and Wu, 2010b) confirm that the blueprint model outperforms BLEU and similar n-gram oriented evalu11 ation metrics in correlation against human adequacy judgments, but does not fare as well as HTER. The more complete study of Lo and Wu (2011a) introduces MEANT and its human variants HMEANT, which implement an extended version of blueprint methodology. Experimental results show that HMEANT correlates against human adequacy judgments as well as the more expensive HTER, even though HMEANT can be evaluated using lowcost untrained monolingual semantic role annotators while still maintaining high inter-annotator agreement (both are far superior to BLEU or other surface oriented evaluation metrics). The study also shows that replacing the human semantic role labelers with an automatic shallow semantic parser yields an approximation that"
W11-1002,niessen-etal-2000-evaluation,0,0.269024,"overall sentence translation. We demonstrate empirically that when the degree of each frame’s contribution to its sentence is taken into account, the properly structured role representation is more accurate and intuitive than the flattened role representation for SRL MT evaluation metrics. For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000). These metrics are excellent at ranking overall systems by averaging their scores over entire documents. However, as MT systems improve, the shortcomings of such metrics are becoming more apparent. Though containing roughly the correct words, MT output at the sentence remains often quite incomprehensible, and fails to preserve the meaning of the input. This results from the fact that n-gram based metrics are not as reliable at ranking the adequacy of translations of individual sentences, and are particularly 10 Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statis"
W11-1002,P09-1034,0,0.0285111,"problem remains that a grammatical translation can achieve a high syntaxbased score yet still make significant errors arising from confusion of semantic roles. On the other hand, despite the fact that non-automatic, manually evaluated metrics, such as HTER (Snover et al., 2006), are more adequacy oriented exhibit much higher correlation with human adequacy judgment, their high labor cost prohibits widespread use. There has also been work on explicitly evaluating MT adequacy by aggregating over a very large set of linguistic features (Gim´enez and M`arquez, 2007, 2008) and textual entailment (Pado et al., 2009). 2 SRL based MT evaluation metrics A blueprint for more direct assessment of meaning preservation across translation was outlined by Lo and Wu (2010a), in which translation utility is manually evaluated with respect to the accuracy of semantic role labels. A good translation is one from which human readers may successfully understand at least the basic event structure—“who did what to whom, when, where and why” (Pradhan et al., 2004)–which represents the most essential meaning of the source utterances. Adopting this principle, the MEANT family of metrics compare the semantic frames in referen"
W11-1002,J05-1004,0,0.0603586,"precision/recall. The sentence precision/recall is the weighted sum of the number of correctly translated roles in all frames normalized by the weighted sum of the total number of 13 3 Experimental setup The evaluation data for our experiments consists of 40 sentences randomly drawn from the DARPA GALE program Phase 2.5 newswire evaluation corpus containing Chinese input sentence, English reference translations, and the machine translation from three different state-of-the-art GALE systems. The Chinese and the English reference translation have both been annotated with gold standard PropBank (Palmer et al., 2005) semantic role labels. The weights wpred , wcore , wadj , wj and wpartial can be estimated by optimizing correlation against human adequacy judgments, using any of the many standard optimization search techniques. In the work of Lo and Figure 3: The flat role representation for the MEANT family of metrics as proposed in Lo and Wu (2011b) . Wu (2011b), the correlations of all individual roles with the human adequacy judgments were found to be non-negative, therefore we found grid search to be quite adequate for estimating the weights. We use linear weighting because we would like to keep the me"
W11-1002,P02-1040,0,0.0813186,"me we propose uses a simple length-based heuristic that reflects the assumption that a semantic frame that covers more tokens contributes more to the overall sentence translation. We demonstrate empirically that when the degree of each frame’s contribution to its sentence is taken into account, the properly structured role representation is more accurate and intuitive than the flattened role representation for SRL MT evaluation metrics. For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000). These metrics are excellent at ranking overall systems by averaging their scores over entire documents. However, as MT systems improve, the shortcomings of such metrics are becoming more apparent. Though containing roughly the correct words, MT output at the sentence remains often quite incomprehensible, and fails to preserve the meaning of the input. This results from the fact that n-gram based metrics are not as reliable at ranking the adequacy"
W11-1002,2006.amta-papers.25,0,0.132086,"at evaluating translation adequacy, particularly at the sentence level. One line of research emphasizes more the structural correctness of translation. Liu and Gildea (2005) propose STM, a metric based on syntactic structure, that addresses the failure of lexical similarity based metrics to evaluate translation grammaticality. However, the problem remains that a grammatical translation can achieve a high syntaxbased score yet still make significant errors arising from confusion of semantic roles. On the other hand, despite the fact that non-automatic, manually evaluated metrics, such as HTER (Snover et al., 2006), are more adequacy oriented exhibit much higher correlation with human adequacy judgment, their high labor cost prohibits widespread use. There has also been work on explicitly evaluating MT adequacy by aggregating over a very large set of linguistic features (Gim´enez and M`arquez, 2007, 2008) and textual entailment (Pado et al., 2009). 2 SRL based MT evaluation metrics A blueprint for more direct assessment of meaning preservation across translation was outlined by Lo and Wu (2010a), in which translation utility is manually evaluated with respect to the accuracy of semantic role labels. A g"
W11-1002,E06-1032,0,\N,Missing
W11-1002,W10-1703,0,\N,Missing
W11-1002,N04-1030,0,\N,Missing
W11-1008,P89-1018,0,0.436958,"anslation, pages 70–78, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics on the one side, and the flexibility of grammar formalism and parsing algorithm development afforded by semiring (bi-) parsing. It is, however, possible to have both, which we will show in Section 5. An integral part of this unification is the concept of contextual probability. Finally, we will offer some conclusions in Section 6. 2 Background A common view on probabilistic parsing—be it bilingual or monolingual—is that it involves the construction of a weighted hypergraph (Billot and Lang, 1989; Manning and Klein, 2001; Huang, 2008). This is an appealing conceptualization, as it separates the construction of the parse forest (the actual hypergraph) from the probabilistic calculations that need to be carried out. The calculations are, in fact, given by the hypergraph itself. To get the probability of the sentence (pair) being parsed, one simply have to query the hypergraph for the value of the goal node. It is furthermore possible to abstract away the calculations themselves, by defining the hypergraph over an arbitrary semiring. When the Boolean semiring is used, the value of the go"
W11-1008,P02-1001,0,0.0341339,"parses were introduced in Goodman (1999). In this paper we will reify the grammar rules by moving them from the meta level to the object level—effectively making them first-class citizens of the parse trees, which are no longer weighted hypergraphs, but mul/add-graphs. This move allows us to calculate rule expectations for expectation maximization (Dempster et al., 1977) as part of the parsing process, which significantly shortens turn-over time for experimenting with different grammar for71 malisms. Another approach which achieve a similar goal is to use a expectation semiring (Eisner, 2001; Eisner, 2002; Li and Eisner, 2009). In this semiring, all values are pairs of probabilities and expectations. The inside-outside algorithm with the expectation semiring requires the usual inside and outside calculations over the probability part of the semiring values, followed by a third traversal over the parse forest to populate the expectation part of the semiring values. The approach taken in this paper also requires the usual inside and outside calculations, but o third traversal of the parse forest. Instead, the proposed approach requires two passes over the rules of the grammar per EM iteration. T"
W11-1008,J99-4004,0,0.48061,"rammar, and false otherwise. When the probabilistic semiring is used, the probability of the sentence (pair) is attained, and with the tropical semiring, the probability of the most likely tree is attained. To further generalize the building of the hypergraph—the parsing algorithm—a deductive system can be used. By defining a hand-full of deductive rules that describe how items can be constructed, the full complexities of a parsing algorithm can be very succinctly summarized. Deductive systems to represent parsers and semirings to calculate the desired values for the parses were introduced in Goodman (1999). In this paper we will reify the grammar rules by moving them from the meta level to the object level—effectively making them first-class citizens of the parse trees, which are no longer weighted hypergraphs, but mul/add-graphs. This move allows us to calculate rule expectations for expectation maximization (Dempster et al., 1977) as part of the parsing process, which significantly shortens turn-over time for experimenting with different grammar for71 malisms. Another approach which achieve a similar goal is to use a expectation semiring (Eisner, 2001; Eisner, 2002; Li and Eisner, 2009). In t"
W11-1008,D09-1005,0,0.0218808,"troduced in Goodman (1999). In this paper we will reify the grammar rules by moving them from the meta level to the object level—effectively making them first-class citizens of the parse trees, which are no longer weighted hypergraphs, but mul/add-graphs. This move allows us to calculate rule expectations for expectation maximization (Dempster et al., 1977) as part of the parsing process, which significantly shortens turn-over time for experimenting with different grammar for71 malisms. Another approach which achieve a similar goal is to use a expectation semiring (Eisner, 2001; Eisner, 2002; Li and Eisner, 2009). In this semiring, all values are pairs of probabilities and expectations. The inside-outside algorithm with the expectation semiring requires the usual inside and outside calculations over the probability part of the semiring values, followed by a third traversal over the parse forest to populate the expectation part of the semiring values. The approach taken in this paper also requires the usual inside and outside calculations, but o third traversal of the parse forest. Instead, the proposed approach requires two passes over the rules of the grammar per EM iteration. The asymptotic time com"
W11-1008,W01-1812,0,0.0458272,"c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics on the one side, and the flexibility of grammar formalism and parsing algorithm development afforded by semiring (bi-) parsing. It is, however, possible to have both, which we will show in Section 5. An integral part of this unification is the concept of contextual probability. Finally, we will offer some conclusions in Section 6. 2 Background A common view on probabilistic parsing—be it bilingual or monolingual—is that it involves the construction of a weighted hypergraph (Billot and Lang, 1989; Manning and Klein, 2001; Huang, 2008). This is an appealing conceptualization, as it separates the construction of the parse forest (the actual hypergraph) from the probabilistic calculations that need to be carried out. The calculations are, in fact, given by the hypergraph itself. To get the probability of the sentence (pair) being parsed, one simply have to query the hypergraph for the value of the goal node. It is furthermore possible to abstract away the calculations themselves, by defining the hypergraph over an arbitrary semiring. When the Boolean semiring is used, the value of the goal node will be true if t"
W11-1008,2011.eamt-1.42,1,0.819373,"ly, the contextual probability is: The contextual probability of something is the sum of the probabilities of all contexts where it was used. This opens up an interesting line of inquiry into what this quantity might represent. In this paper we show that the contextual probabilities of the rules contain precisely the new information needed in order to calculate the expectations needed to reestimate the rule probabilities. This line of inquiry was discovered while working on a preterminalized version of linear inversion transduction grammars (LITGs), so we will use these preterminalized LITGs (Saers and Wu, 2011) as an example throughout this paper. We will start by examining semiring parsing (parsing as deductive systems over semirings, Section 3), followed by a section on how this relates to weighted hypergraphs, a common representation of parse forests (Section 4). This reveals a disparity between weighted hypergraphs and semiring parsing. It seems like we are forced to choose between the inside-outside algorithm for context-free grammars 70 Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 70–78, c ACL HLT 2011, Portland, Oregon, USA, June 2"
W12-3129,W05-0909,0,0.0711719,"achieves significantly higher Kendall correlation with human adequacy judgments than BLEU, NIST, METEOR, PER, CDER, WER, or TER. Furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually. Despite its high performance, fully automated MEANT is still able to preserve HMEANT’s virtues of simplicity, representational transparency, and inexpensiveness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortunately infeasible to incorporate the HMEANT metrics directly i"
W12-3129,E06-1032,0,0.184934,"and MT output. 2. Human judges align the semantic frames between the references and MT output by judging the correctness of the predicates. 3. For each pair of aligned semantic frames, (a) Human judges determine the translation correctness of the semantic role fillers. (b) Human judges align the semantic role fillers between the reference and MT output according to the correctness of the semantic role fillers. Automatic lexical similarity based metrics BLEU (Papineni et al., 2002) remains the most widely used MT evaluation metric despite the fact that a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where it strongly disagrees with human judgments of translation accuracy. Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption that a good translation is one that shares the same lexi"
W12-3129,W07-0718,0,0.414855,"nths SK - 2 products resume in the mainland of China to stop selling nearly two months of SK - 2 products sales So far — — incorrect the ULC representation is based on flat semantic role label features that do not capture the structural relations in semantic frames, i.e., the predicate-argument relations. Also unlike HMEANT, which weights each semantic role type according to its empirically determined relative importance to the adequate preservation of meaning, ULC uses uniform weights. Although the automatic ULC metric shows an improved correlation with human judgment of translation quality (Callison-Burch et al., 2007; Gim´enez and M`arquez, 2007; Callison-Burch et al., 2008; Gim´enez and M`arquez, 2008), it is not commonly used in large-scale MT evaluation campaigns, perhaps due to its high time cost and/or the difficulty of interpreting its score because of its highly complex combination of many heterogeneous types of features. Like system combination approaches, ULC is a vastly more complex aggregate metric compared to widely used metrics like BLEU. We believe it is important for automatic semantic MT evaluation metrics to provide representational transparency via simple, clear, and transparent scoring"
W12-3129,W08-0309,0,0.0304526,"op selling nearly two months of SK - 2 products sales So far — — incorrect the ULC representation is based on flat semantic role label features that do not capture the structural relations in semantic frames, i.e., the predicate-argument relations. Also unlike HMEANT, which weights each semantic role type according to its empirically determined relative importance to the adequate preservation of meaning, ULC uses uniform weights. Although the automatic ULC metric shows an improved correlation with human judgment of translation quality (Callison-Burch et al., 2007; Gim´enez and M`arquez, 2007; Callison-Burch et al., 2008; Gim´enez and M`arquez, 2008), it is not commonly used in large-scale MT evaluation campaigns, perhaps due to its high time cost and/or the difficulty of interpreting its score because of its highly complex combination of many heterogeneous types of features. Like system combination approaches, ULC is a vastly more complex aggregate metric compared to widely used metrics like BLEU. We believe it is important for automatic semantic MT evaluation metrics to provide representational transparency via simple, clear, and transparent scoring schemes that are (a) easily human readable to support erro"
W12-3129,W07-0738,0,0.406358,"Missing"
W12-3129,W08-0332,0,0.268841,"Missing"
W12-3129,W06-3114,0,0.598552,"ems improve, the n-gram based evaluation metrics have begun to show their limits. State-of-the-art MT systems are often able to output translations containing roughly the correct words, while failing to convey important aspects of the meaning of the input sentence. Cases where BLEU strongly disagrees with human judgment of translation quality were 243 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 243–252, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics reported in large scale MT evaluation tasks by CallisonBurch et al. (2006) and Koehn and Monz (2006). Motivated by the goal of addressing the weaknesses of n-gram oriented automatic MT evaluation metrics at evaluating translation adequacy, the HMEANT metric assesses translation utility by matching the basic event structure—“who did what to whom, when, where and why” (Pradhan et al., 2004)—representing the central meaning conveyed by sentences. As mentioned above, however, HMEANT requires humans to manually annotate semantic frames in the reference and machine translations, and then to align the semantic frames—making it difficult to incorporate HMEANT as an objective function in the MT syste"
W12-3129,E06-1031,0,0.86743,"adequacy judgments than BLEU, NIST, METEOR, PER, CDER, WER, or TER. Furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually. Despite its high performance, fully automated MEANT is still able to preserve HMEANT’s virtues of simplicity, representational transparency, and inexpensiveness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortunately infeasible to incorporate the HMEANT metrics directly into SMT training methods, due to the non-automatic proces"
W12-3129,P11-1023,1,0.323724,"veness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortunately infeasible to incorporate the HMEANT metrics directly into SMT training methods, due to the non-automatic processes of (1) semantic parsing and (2) aligning semantic frames. In this paper we introduce an automatic metric in which both the semantic parsing and the alignment of semantic frames are fully automated. Our aim is to show that even with full automation, this new metric still outperforms all the previous automatic metrics mentioned, thus providing a foundation for"
W12-3129,W11-1002,1,0.378166,"veness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortunately infeasible to incorporate the HMEANT metrics directly into SMT training methods, due to the non-automatic processes of (1) semantic parsing and (2) aligning semantic frames. In this paper we introduce an automatic metric in which both the semantic parsing and the alignment of semantic frames are fully automated. Our aim is to show that even with full automation, this new metric still outperforms all the previous automatic metrics mentioned, thus providing a foundation for"
W12-3129,1996.amta-1.13,0,0.209697,"what we did above, which was to sum the lexical similarities of all pairwise combinations of tokens. However, experimental results will show that, surprisingly, to judge the similarity of semantic role fillers, summing the lexical similarities over only word-aligned tokens—instead of all pairwise combinations of tokens— does not help to improve the correlation of the semantic MT evaluation with human adequacy judgment. 7.1 Experimental setup To avoid the danger of aligning a token in one segment to excessive numbers of tokens in the other segment, we adopt a variant of competitive linking by Melamed (1996). Competitive linking is a greedy best-first word alignment algorithm. Table 6: Sentence-level correlation with human adequacy judgments on GALE-A (training set) and GALE-B (testing set) for judging semantic role fillers similarity using pairwise tokens vs. only aligned tokens. Semantic role filler similarity All pairwise tokens Only aligned tokens GALE-A 0.37 0.36 GALE-B 0.19 0.17 The rest of the experimental setup is the same as that used in Section 4. 7.2 Results Table 6 shows that, surprisingly, judging semantic role filler similarity using only the aligned tokens (selected by competitive"
W12-3129,niessen-etal-2000-evaluation,0,0.929744,"U, NIST, METEOR, PER, CDER, WER, or TER. Furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually. Despite its high performance, fully automated MEANT is still able to preserve HMEANT’s virtues of simplicity, representational transparency, and inexpensiveness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortunately infeasible to incorporate the HMEANT metrics directly into SMT training methods, due to the non-automatic processes of (1) semantic parsing"
W12-3129,P02-1040,0,0.123001,"otocol, shows that this fully automated version of HMEANT achieves significantly higher Kendall correlation with human adequacy judgments than BLEU, NIST, METEOR, PER, CDER, WER, or TER. Furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually. Despite its high performance, fully automated MEANT is still able to preserve HMEANT’s virtues of simplicity, representational transparency, and inexpensiveness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortuna"
W12-3129,N04-1030,0,0.931733,"disagrees with human judgment of translation quality were 243 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 243–252, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics reported in large scale MT evaluation tasks by CallisonBurch et al. (2006) and Koehn and Monz (2006). Motivated by the goal of addressing the weaknesses of n-gram oriented automatic MT evaluation metrics at evaluating translation adequacy, the HMEANT metric assesses translation utility by matching the basic event structure—“who did what to whom, when, where and why” (Pradhan et al., 2004)—representing the central meaning conveyed by sentences. As mentioned above, however, HMEANT requires humans to manually annotate semantic frames in the reference and machine translations, and then to align the semantic frames—making it difficult to incorporate HMEANT as an objective function in the MT system training, evaluating, and optimizing cycle. We argue in this paper that both the human semantic parsing and the semantic frame alignment tasks performed within HMEANT can be successfully automated to produce a state-of-the-art automatic metric. Moreover, we show that the spirit of Occam’s"
W12-3129,2006.amta-papers.25,0,0.928821,", or TER. Furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually. Despite its high performance, fully automated MEANT is still able to preserve HMEANT’s virtues of simplicity, representational transparency, and inexpensiveness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortunately infeasible to incorporate the HMEANT metrics directly into SMT training methods, due to the non-automatic processes of (1) semantic parsing and (2) aligning semantic fram"
W12-3129,W10-1703,0,\N,Missing
W12-4204,2004.iwslt-papers.1,0,0.0315707,"esources for interpreting and automating HMEANT. We apply HMEANT to a new language, Czech in particular, by evaluating a set of Englishto-Czech MT systems. HMEANT proves to correlate with manual rankings at the sentence level better than a range of automatic metrics. However, the main contribution of this paper is the identification of several issues of HMEANT annotation and our proposal on how to resolve them. 1 Introduction Manual evaluation of machine translation output is a tricky enterprise. It has been long recognized that different evaluation techniques lead to different outcomes, e.g. Blanchon et al. (2004) mention an evaluation carried out in 1972 where the very same Russian-to-English MT outputs were scored 4.5 out of the maximum 5 points by prospective users of the system but only 1 out of 5 by teachers of English. Throughout the years, many techniques were explored with more or less of a success. The two-scale scoring for adequacy and fluency used in NIST evaluation has been abandoned by some evaluation campaigns, most notably the WMT shared task series, see Koehn and Monz (2006) through Callison-Burch et al. (2012)1 . Since 2008, WMT uses a simple relative ranking of MT outputs as its prima"
W12-4204,W11-2101,1,0.865595,"Missing"
W12-4204,W07-0718,0,0.0354432,"ome evaluation campaigns, most notably the WMT shared task series, see Koehn and Monz (2006) through Callison-Burch et al. (2012)1 . Since 2008, WMT uses a simple relative ranking of MT outputs as its primary manual evaluation technique: the annotator is presented with up to 5 MT outputs for a given input sentence and the task is to rank them from best to worst (ties allowed) on whatever criteria he or she deems appropriate. While this single-scale relative ranking is perhaps faster to annotate and reaches a higher inter- and intra-annotator agreement than the (absolute) fluency and adequacy (Callison-Burch et al., 2007), the technique and its evaluation are still far from satisfactory. Bojar et al. (2011) observe several discrepancies in the interpretation of the rankings, partly due to the high load on human annotators (the comparison of several long sentences at once, among other issues) but partly also due to technicalities of the calculation. Lo and Wu (2011a) present an interesting evaluation technique called MEANT (or HMEANT if carried out by humans), the core of which lies in assessing whether the key elements in the predicateargument structure of the sentence have been preserved. In other words, lay"
W12-4204,W12-3102,0,0.0678334,"Missing"
W12-4204,hajic-etal-2012-announcing,1,0.867193,"Missing"
W12-4204,W06-3114,0,0.0381749,"ky enterprise. It has been long recognized that different evaluation techniques lead to different outcomes, e.g. Blanchon et al. (2004) mention an evaluation carried out in 1972 where the very same Russian-to-English MT outputs were scored 4.5 out of the maximum 5 points by prospective users of the system but only 1 out of 5 by teachers of English. Throughout the years, many techniques were explored with more or less of a success. The two-scale scoring for adequacy and fluency used in NIST evaluation has been abandoned by some evaluation campaigns, most notably the WMT shared task series, see Koehn and Monz (2006) through Callison-Burch et al. (2012)1 . Since 2008, WMT uses a simple relative ranking of MT outputs as its primary manual evaluation technique: the annotator is presented with up to 5 MT outputs for a given input sentence and the task is to rank them from best to worst (ties allowed) on whatever criteria he or she deems appropriate. While this single-scale relative ranking is perhaps faster to annotate and reaches a higher inter- and intra-annotator agreement than the (absolute) fluency and adequacy (Callison-Burch et al., 2007), the technique and its evaluation are still far from satisfacto"
W12-4204,P11-1023,1,0.910768,"t (ties allowed) on whatever criteria he or she deems appropriate. While this single-scale relative ranking is perhaps faster to annotate and reaches a higher inter- and intra-annotator agreement than the (absolute) fluency and adequacy (Callison-Burch et al., 2007), the technique and its evaluation are still far from satisfactory. Bojar et al. (2011) observe several discrepancies in the interpretation of the rankings, partly due to the high load on human annotators (the comparison of several long sentences at once, among other issues) but partly also due to technicalities of the calculation. Lo and Wu (2011a) present an interesting evaluation technique called MEANT (or HMEANT if carried out by humans), the core of which lies in assessing whether the key elements in the predicateargument structure of the sentence have been preserved. In other words, lay annotators are checking, if they recognize who did what to whom, when, where and why from the MT outputs and whether the respective role fillers convey the same meaning as in the reference translation. HMEANT has been shown to correlate reasonably well with manual adequacy and ranking evaluations. It is relatively fast and should lend itself to fu"
W12-4204,W11-1002,1,0.876808,"t (ties allowed) on whatever criteria he or she deems appropriate. While this single-scale relative ranking is perhaps faster to annotate and reaches a higher inter- and intra-annotator agreement than the (absolute) fluency and adequacy (Callison-Burch et al., 2007), the technique and its evaluation are still far from satisfactory. Bojar et al. (2011) observe several discrepancies in the interpretation of the rankings, partly due to the high load on human annotators (the comparison of several long sentences at once, among other issues) but partly also due to technicalities of the calculation. Lo and Wu (2011a) present an interesting evaluation technique called MEANT (or HMEANT if carried out by humans), the core of which lies in assessing whether the key elements in the predicateargument structure of the sentence have been preserved. In other words, lay annotators are checking, if they recognize who did what to whom, when, where and why from the MT outputs and whether the respective role fillers convey the same meaning as in the reference translation. HMEANT has been shown to correlate reasonably well with manual adequacy and ranking evaluations. It is relatively fast and should lend itself to fu"
W12-4204,J05-1004,0,0.0225999,"rgument (how) You may consider the Action predicate to be the central event, while the other roles modify the Action to give a more detailed description of the event. Each semantic frame contains exactly one Action and any number of other roles. Please note that the Action predicate must be exactly ONE single word. There may be multiple semantic frames in one sentence, because a sentence may be constructed to describe multiple events and each semantic frame captures only one event. Functional Generative Description The core ideas of HMEANT follow the case grammar (Fillmore, 1968) or PropBank (Palmer et al., 2005) and can be also directly related to an established linguistic theory which was primarily devel31 oped for Czech, namely the Functional Generative Description (Sgall, 1967; Sgall et al., 1986). The theory defines so-called “tectogrammatical” layer (tlayer). At the t-layer, each sentence is represented as a dependency tree with just content words as separate nodes. All auxiliary words are “hidden” into attributes of the corresponding t-nodes. Moreover, ellipsis is restored to some extent, so e.g. dropped subject pronouns do have a corresponding t-node. An important element of FGD is the valency"
W12-4204,W12-3146,0,0.0428927,"Missing"
W12-4204,sindlerova-bojar-2010-building,1,0.844744,"g the development of the Prague Dependency Treebank (Hajiˇc et al., 2006)2 and the parallel Prague Czech-English Dependency Treebank (Hajiˇc 2 http://ufal.mff.cuni.cz/pdt2.0/ et al., 2012)3 . Note that the latter is a translation of all the 49k sentences of the Penn Treebank WSJ section. Both English and Czech sentences are manually annotated at the tectogrammatical layer, where the English layer is based on the Penn annotation and manually adapted for t-layer. Both languages include their respective valency lexicons and the work on a bilingual valency lexicon is being developed ˇ (Sindlerov´ a and Bojar, 2010). A range of automatic tools to convert plain text up to the t-layer exist for both English and Czech. Most of them are now part of the Treex platform (Popel ˇ and Zabokrtsk´ y, 2010)4 and they were successfully used in automatic annotation of 15 million parallel sentences (Bojar et al., 2012)5 as well as other NLP tasks including English-to-Czech MT. Recently, significant effort was also invested in parsing not quite correct output of MT systems into Czech for the purposes of rule-based grammar correction (Rosa et al., 2012). Establishing the automatic pipeline for MEANT should be relatively"
W12-4204,bojar-etal-2012-joy,1,\N,Missing
W12-4206,W05-0909,0,0.506786,"ed from the supervised training is similar to the relative frequencies of the occurrences of each semantic role in the reference translation. We then introduce a relative frequency weighting scheme to approximate the importance of each semantic role type. With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly reduced. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) because of their support on fast and inexpensive evaluation. These metrics are good at ranking overall systems by averaging their scores over the entire document. As MT systems improve, the focus of MT evaluation changes from generally reflecting the quality of each system to assisting error analysis on each MT output in detail. The failure of such metrics in evaluating translation quality on sentence level are becoming more apparent. Though containing roughly the correct words, the MT output as a whole sen"
W12-4206,E06-1032,0,0.46584,"Missing"
W12-4206,W10-1703,0,0.0875079,"Missing"
W12-4206,W07-0738,0,0.711622,"Missing"
W12-4206,W08-0332,0,0.56885,"Missing"
W12-4206,W06-3114,0,0.15456,"metrics in evaluating translation quality on sentence level are becoming more apparent. Though containing roughly the correct words, the MT output as a whole sentence is still quite incomprehensible and fails to express meaning that is close to the input. Lexical n-gram based evaluation metrics are surface-oriented and do not do so well at ranking translations according to adequacy and are particularly poor at reflecting significant translation quality improvements on more meaningful word sense or semantic frame choices which human judges can indicate clearly. CallisonBurch et al. (2006) and Koehn and Monz (2006) even reported cases where BLEU strongly disagrees with human judgment on translation quality. 49 Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 49–56, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics Liu and Gildea (2005) proposed STM, a structural approach based on syntax to addresses the failure of lexical similarity based metrics in evaluating translation grammaticality. However, a grammatical translation can achieve a high syntax-based score but still contains meaning errors arising from con"
W12-4206,E06-1031,0,0.610611,"quencies of the occurrences of each semantic role in the reference translation. We then introduce a relative frequency weighting scheme to approximate the importance of each semantic role type. With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly reduced. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) because of their support on fast and inexpensive evaluation. These metrics are good at ranking overall systems by averaging their scores over the entire document. As MT systems improve, the focus of MT evaluation changes from generally reflecting the quality of each system to assisting error analysis on each MT output in detail. The failure of such metrics in evaluating translation quality on sentence level are becoming more apparent. Though containing roughly the correct words, the MT output as a whole sentence is still quite incomprehensible and fails to expres"
W12-4206,W05-0904,0,0.0918446,"do not do so well at ranking translations according to adequacy and are particularly poor at reflecting significant translation quality improvements on more meaningful word sense or semantic frame choices which human judges can indicate clearly. CallisonBurch et al. (2006) and Koehn and Monz (2006) even reported cases where BLEU strongly disagrees with human judgment on translation quality. 49 Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 49–56, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics Liu and Gildea (2005) proposed STM, a structural approach based on syntax to addresses the failure of lexical similarity based metrics in evaluating translation grammaticality. However, a grammatical translation can achieve a high syntax-based score but still contains meaning errors arising from confusion of semantic roles. On the other hand, despite the fact that non-automatic, manually evaluations, such as HTER (Snover et al., 2006), are more adequacy oriented and show a high correlation with human adequacy judgment, the high labor cost prohibits their widespread use. There was also work on explicitly evaluating"
W12-4206,N07-1006,0,0.0143897,"ting more linguistic features into MT evaluation metrics raise the discussion on the appropriate approach in weighting and combining them. ULC (Gim´enez and M`arquez, 2007, 2008) uses uniform weights to aggregate linguistic features. This approach does not capture the importance of each feature to the overall translation quality to the MT output. One obvious example of different semantic roles contribute differently to the overall meaning is that readers usually accept translations with errors in adjunct arguments as a valid translation but not those with errors in core arguments. Unlike ULC, Liu and Gildea (2007); Lo and Wu (2011a) approach the weight estimation problem by maximum correlation training which directly optimize the correlation with human adequacy judg50 Figure 1: HMEANT structured role representation with a weighting scheme reflecting the degree of contribution of each semantic role type to the semantic frame. (Lo and Wu, 2011a,b,c). ments. However, the shortcomings of this approach is that it requires a human-ranked training corpus which is expensive, especially for languages with limited resource. We argue in this paper that for semantic MT evaluation, the importance of each semantic r"
W12-4206,P11-1023,1,0.388365,"tric that correlates comparable with human adequacy judgments to previous metrics that require far more expensive human rankings of adequacy over a training corpus. As a result, the cost of semantic MT evaluation is greatly reduced. 1 Introduction In this paper we investigate an unsupervised approach to estimate the degree of contribution of each semantic role type in semantic translation evaluation in low cost without using a human-ranked training corpus but still yields a evaluation metric that correlates comparably with human adequacy judgments to that of recent supervised approaches as in Lo and Wu (2011a, b, c). The new approach is motivated by an analysis showing that the distribution of the weights learned from the supervised training is similar to the relative frequencies of the occurrences of each semantic role in the reference translation. We then introduce a relative frequency weighting scheme to approximate the importance of each semantic role type. With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly reduced. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-"
W12-4206,W11-1002,1,0.173727,"tric that correlates comparable with human adequacy judgments to previous metrics that require far more expensive human rankings of adequacy over a training corpus. As a result, the cost of semantic MT evaluation is greatly reduced. 1 Introduction In this paper we investigate an unsupervised approach to estimate the degree of contribution of each semantic role type in semantic translation evaluation in low cost without using a human-ranked training corpus but still yields a evaluation metric that correlates comparably with human adequacy judgments to that of recent supervised approaches as in Lo and Wu (2011a, b, c). The new approach is motivated by an analysis showing that the distribution of the weights learned from the supervised training is similar to the relative frequencies of the occurrences of each semantic role in the reference translation. We then introduce a relative frequency weighting scheme to approximate the importance of each semantic role type. With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly reduced. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-"
W12-4206,niessen-etal-2000-evaluation,0,0.437044,"each semantic role in the reference translation. We then introduce a relative frequency weighting scheme to approximate the importance of each semantic role type. With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly reduced. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) because of their support on fast and inexpensive evaluation. These metrics are good at ranking overall systems by averaging their scores over the entire document. As MT systems improve, the focus of MT evaluation changes from generally reflecting the quality of each system to assisting error analysis on each MT output in detail. The failure of such metrics in evaluating translation quality on sentence level are becoming more apparent. Though containing roughly the correct words, the MT output as a whole sentence is still quite incomprehensible and fails to express meaning that is close to the"
W12-4206,P09-1034,0,0.0137753,"rics in evaluating translation grammaticality. However, a grammatical translation can achieve a high syntax-based score but still contains meaning errors arising from confusion of semantic roles. On the other hand, despite the fact that non-automatic, manually evaluations, such as HTER (Snover et al., 2006), are more adequacy oriented and show a high correlation with human adequacy judgment, the high labor cost prohibits their widespread use. There was also work on explicitly evaluating MT adequacy with aggregated linguistic features (Gim´enez and M`arquez, 2007, 2008) and textual entailment (Pado et al., 2009). In the work of Lo and Wu (2011a), MEANT and its human variants HMEANT were introduced and empirical experimental results showed that HMEANT, which can be driven by low-cost monolingual semantic roles annotators with high interannotator agreement, correlates as well as HTER and far superior than BLEU and other surfaced oriented evaluation metrics. Along with additional improvements to the MEANT family of metrics, Lo and Wu (2011b) detailed the studies of the impact of each individual semantic role to the metric’s correlation with human adequacy judgments. Lo and Wu (2011c) further discussed t"
W12-4206,P02-1040,0,0.10653,"alysis showing that the distribution of the weights learned from the supervised training is similar to the relative frequencies of the occurrences of each semantic role in the reference translation. We then introduce a relative frequency weighting scheme to approximate the importance of each semantic role type. With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly reduced. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) because of their support on fast and inexpensive evaluation. These metrics are good at ranking overall systems by averaging their scores over the entire document. As MT systems improve, the focus of MT evaluation changes from generally reflecting the quality of each system to assisting error analysis on each MT output in detail. The failure of such metrics in evaluating translation quality on sentence level are becoming more apparent. Though contai"
W12-4206,N04-1030,0,0.468461,"ic MT evaluation metric using the relative frequency weighting scheme to approximate the importance of each semantic role type correlates comparably with human adequacy judgments to previous metrics that use maximum correlation training, which requires expensive human rankings of adequacy over a training corpus. Therefore, the cost of semantic MT evaluation is greatly reduced. 2 Semantic MT evaluation metrics Adopting the principle that a good translation is one from which human readers may successfully understand at least the basic event structure-“who did what to whom, when, where and why” (Pradhan et al., 2004)-which represents the most essential meaning of the source utterances, Lo and Wu (2011a,b,c) proposed HMEANT to evaluate translation utility based on semantic frames reconstructed by human reader of machine translation output. Monolingual (or bilingual) annotators must label the semantic roles in both the reference and machine translations, and then to align the semantic predicates and role fillers in the MT output to the reference translations. These annotations allow HMEANT to then look at the aligned role fillers, and aggregate the translation accuracy for each role. In the spirit of Occam’"
W12-4206,2006.amta-papers.25,0,0.183236,"6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 49–56, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics Liu and Gildea (2005) proposed STM, a structural approach based on syntax to addresses the failure of lexical similarity based metrics in evaluating translation grammaticality. However, a grammatical translation can achieve a high syntax-based score but still contains meaning errors arising from confusion of semantic roles. On the other hand, despite the fact that non-automatic, manually evaluations, such as HTER (Snover et al., 2006), are more adequacy oriented and show a high correlation with human adequacy judgment, the high labor cost prohibits their widespread use. There was also work on explicitly evaluating MT adequacy with aggregated linguistic features (Gim´enez and M`arquez, 2007, 2008) and textual entailment (Pado et al., 2009). In the work of Lo and Wu (2011a), MEANT and its human variants HMEANT were introduced and empirical experimental results showed that HMEANT, which can be driven by low-cost monolingual semantic roles annotators with high interannotator agreement, correlates as well as HTER and far superi"
W13-0806,P09-1088,0,0.727578,"es are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective that we will be using to drive the learning will"
W13-0806,J93-2003,0,0.0286628,"l—this used to be a given in machine learning and statistical prediction, but has been largely ignored in the statistical machine translation (SMT) community, where most current SMT approaches to learning phrase translations that (a) require enormous amounts of run-time memory, and (b) contain a high degree of redundancy. In particular, phrase-based SMT models such as Koehn et al. (2003) and Chiang (2007) often search for candidate translation segments and transduction rules by committing to a word alignment that is completely alien to the grammar, as it is learned with very different models (Brown et al. (1993), Vogel et al. (1996)), whose output is then combined heuristically to form the alignment actually used to extract lexical segment translations (Och 48 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 48–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics and Ney, 2003). The fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates the above points well. Transduction grammars"
W13-0806,W07-0403,0,0.486174,"arch since, usually referred to as tree-to-tree, treeto-string and string-to-tree, depending on where the analyses are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic struct"
W13-0806,P06-1121,0,0.276218,"on Syntax, Semantics and Structure in Statistical Translation, pages 48–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics and Ney, 2003). The fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates the above points well. Transduction grammars can also be induced from treebanks instead of unannotated corpora, which cuts down the vast search space by enforcing additional, external constraints. This approach was pioneered by Galley et al. (2006), and there has been a lot of research since, usually referred to as tree-to-tree, treeto-string and string-to-tree, depending on where the analyses are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through trans"
W13-0806,P09-1104,0,0.163408,"ding on where the analyses are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective that we will be using to"
W13-0806,D07-1103,0,0.0264717,"the grammar, as it is learned with very different models (Brown et al. (1993), Vogel et al. (1996)), whose output is then combined heuristically to form the alignment actually used to extract lexical segment translations (Och 48 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 48–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics and Ney, 2003). The fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates the above points well. Transduction grammars can also be induced from treebanks instead of unannotated corpora, which cuts down the vast search space by enforcing additional, external constraints. This approach was pioneered by Galley et al. (2006), and there has been a lot of research since, usually referred to as tree-to-tree, treeto-string and string-to-tree, depending on where the analyses are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not b"
W13-0806,N03-1017,0,0.051497,"mbination of them is superior in terms of translation accuracy to either of them in isolation. The transduction grammar approach has the advantage that induction, tuning and testing are optimized on the exact same underlying model—this used to be a given in machine learning and statistical prediction, but has been largely ignored in the statistical machine translation (SMT) community, where most current SMT approaches to learning phrase translations that (a) require enormous amounts of run-time memory, and (b) contain a high degree of redundancy. In particular, phrase-based SMT models such as Koehn et al. (2003) and Chiang (2007) often search for candidate translation segments and transduction rules by committing to a word alignment that is completely alien to the grammar, as it is learned with very different models (Brown et al. (1993), Vogel et al. (1996)), whose output is then combined heuristically to form the alignment actually used to extract lexical segment translations (Och 48 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 48–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics and Ney, 2003). The fact that"
W13-0806,P11-1064,0,0.840672,"l constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective that we will be using to drive the learning will provide a way to escape the maximum-likelihood-ofthe-data-given-the-model optimum that w"
W13-0806,P12-1018,0,0.745634,"bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective that we will be using to drive the learning will provide a way to escape the maximum-likelihood-ofthe-data-given-the-model optimum that we start out with. Howe"
W13-0806,J03-1002,0,0.0225479,"Missing"
W13-0806,P03-1021,0,0.0478082,"ctation maximization (Dempster et al., 1977) and parse forests acquired with the above mentioned biparser, again with a beam width of 100. To do the actual decoding, we use our in-house ITG decoder. The decoder uses a CKYstyle parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) and cube pruning (Chiang, 2007) to integrate the language model scores. The decoder builds an efficient hypergraph structure which is then scored using both the induced grammar and the language model. The weights for the language model and the grammar, are tuned towards BLEU (Papineni et al., 2002) using MERT (Och, 2003). We use the ZMERT (Zaidan, 2009) implementation of MERT as it is a robust and flexible implementation of MERT, while being loosely coupled with the decoder. We use SRILM (Stolcke, 2002) for training a trigram language model on the English side of the training data. To evaluate the quality of the resulting translations, we use BLEU, and NIST (Doddington, 2002). 7 Experimental results The results from running the experiments detailed in the previous section can be summarized in four graphs. Figures 1 and 2 show the size of our new, segmenting model during induction, in terms of description leng"
W13-0806,P02-1040,0,0.0867275,"ers to the training data using expectation maximization (Dempster et al., 1977) and parse forests acquired with the above mentioned biparser, again with a beam width of 100. To do the actual decoding, we use our in-house ITG decoder. The decoder uses a CKYstyle parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) and cube pruning (Chiang, 2007) to integrate the language model scores. The decoder builds an efficient hypergraph structure which is then scored using both the induced grammar and the language model. The weights for the language model and the grammar, are tuned towards BLEU (Papineni et al., 2002) using MERT (Och, 2003). We use the ZMERT (Zaidan, 2009) implementation of MERT as it is a robust and flexible implementation of MERT, while being loosely coupled with the decoder. We use SRILM (Stolcke, 2002) for training a trigram language model on the English side of the training data. To evaluate the quality of the resulting translations, we use BLEU, and NIST (Doddington, 2002). 7 Experimental results The results from running the experiments detailed in the previous section can be summarized in four graphs. Figures 1 and 2 show the size of our new, segmenting model during induction, in te"
W13-0806,W09-2304,1,0.894294,"string-to-tree, depending on where the analyses are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective t"
W13-0806,2011.eamt-1.42,1,0.862873,"ess by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective that we will be using to drive the learning will provide a way to escape the maximum-likelihood-ofthe-data-given-th"
W13-0806,W09-3804,1,0.889494,"proach in isolation. We have outlined how this can be done in practice, and we now substantiate that claim empirically. We will initialize a stochastic bracketing inversion transduction grammar (BITG) to rewrite it’s one nonterminal symbol directly into all the sentence pairs of the training data (iteration 0). We will then segment the grammar iteratively a total of seven times (iterations 1–7). For each iteration we will record the change in description length and test the grammar. Each iteration requires us to biparse the training data, which we do with the cubic time algorithm described in Saers et al. (2009), with a beam width of 100. As training data, we use the IWSLT07 Chinese– English data set (Fordyce, 2007), which contains 46,867 sentence pairs of training data, 506 Chinese sentences of development data with 16 English reference translations, and 489 Chinese sentences with 6 English reference translations each as test data; all the sentences are taken from the traveling domain. Since the Chinese is written without whitespace, we use a tool that tries to clump characters together into more “word like” sequences (Wu, 1999). As the bottom-up grammar, we will reuse the grammar learned in Saers e"
W13-0806,C96-2141,0,0.400564,"iven in machine learning and statistical prediction, but has been largely ignored in the statistical machine translation (SMT) community, where most current SMT approaches to learning phrase translations that (a) require enormous amounts of run-time memory, and (b) contain a high degree of redundancy. In particular, phrase-based SMT models such as Koehn et al. (2003) and Chiang (2007) often search for candidate translation segments and transduction rules by committing to a word alignment that is completely alien to the grammar, as it is learned with very different models (Brown et al. (1993), Vogel et al. (1996)), whose output is then combined heuristically to form the alignment actually used to extract lexical segment translations (Och 48 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 48–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics and Ney, 2003). The fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates the above points well. Transduction grammars can also be induced"
W13-0806,N10-1050,1,0.878678,"ining data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective that we will be using to drive the learning will provide a way to esc"
W13-0806,C12-1142,1,0.601597,"of Science and Technology {masaers|vskaddanki|dekai}@cs.ust.hk Abstract We show that combining both bottom-up rule chunking and top-down rule segmentation search strategies in purely unsupervised learning of phrasal inversion transduction grammars yields significantly better translation accuracy than either strategy alone. Previous approaches have relied on incrementally building larger rules by chunking smaller rules bottomup; we introduce a complementary top-down model that incrementally builds shorter rules by segmenting larger rules. Specifically, we combine iteratively chunked rules from Saers et al. (2012) with our new iteratively segmented rules. These integrate seamlessly because both stay strictly within a pure transduction grammar framework inducing under matching models during both training and testing—instead of decoding under a completely different model architecture than what is assumed during the training phases, which violates an elementary principle of machine learning and statistics. To be able to drive induction top-down, we introduce a minimum description length objective that trades off maximum likelihood against model size. We show empirically that combining the more liberal rul"
W13-0806,P08-1012,0,0.870395,"erred to as tree-to-tree, treeto-string and string-to-tree, depending on where the analyses are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The"
W13-0806,W05-0835,0,0.366784,"Missing"
W13-0806,2007.iwslt-1.1,0,\N,Missing
W13-0806,J97-3002,1,\N,Missing
W13-0806,J07-2003,0,\N,Missing
W13-0806,N10-1028,0,\N,Missing
W13-2254,W07-0738,0,0.459432,"ystems are making. 2.2 3 MEANT and UMEANT MEANT (Lo et al., 2012), which is the weighted f-measure over the matched semantic role labels of the automatically aligned semantic frames and role fillers, outperforms BLEU, NIST, METEOR, WER, CDER and TER. Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. PreLinguistic feature based metrics ULC (Giménez and Màrquez, 2007, 2008) is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez 423 Figure 1: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic shallow semantic parser. The reference and MT output are parsed by an English automatic shallow semantic parser. There are no semantic frames for MT3 since there is no predicate. cisely, MEANT is computed as follows: Mi,j Ri,j Si,pred Si,j wpred wj 1. Apply an automatic shallow semantic parser on"
W13-2254,W05-0909,0,0.514438,"lison-Burch et al., 2006; Koehn and Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a tunable and highly-accurate MT evaluation metric that drives MT system development towards higher utility. As described in Lo and Wu (2011a), the paThe linguistically transparent MEANT and UMEANT metrics are tunable, si"
W13-2254,W08-0332,0,0.379246,"ng nothing more than a monolingual corpus and an automatic shallow semantic parser. 1 Introduction We evaluate in the context of WMT 2013 the MEANT (Lo et al., 2012) and UMEANT (Lo and Wu, 2012) semantic machine translation (MT) evaluation metrics—tunable, simple yet highly effective, fully-automatic semantic frame based objective functions that score the degree of similarity 422 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 422–428, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features of ULC and is based on pure lexical similarity. Therefore, QUEEN suffers from the problem of failing to reflect translation adequacy similar to other n-gram based metrics. Similarly, SPEDE (Wang and Manning, 2012) is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan (Castillo a"
W13-2254,W06-3114,0,0.732089,"MT systems against MEANT more robustly improves translation adequacy, compared to tuning against BLEU or TER. In the past decade, the progress of machine translation (MT) research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al., 2006; Koehn and Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al.,"
W13-2254,E06-1032,0,0.21532,"d Wu, 2013) show that tuning MT systems against MEANT more robustly improves translation adequacy, compared to tuning against BLEU or TER. In the past decade, the progress of machine translation (MT) research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al., 2006; Koehn and Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005"
W13-2254,W07-0718,0,0.355592,"Missing"
W13-2254,W08-0309,0,0.257615,"Missing"
W13-2254,E06-1031,0,0.835172,"nd Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a tunable and highly-accurate MT evaluation metric that drives MT system development towards higher utility. As described in Lo and Wu (2011a), the paThe linguistically transparent MEANT and UMEANT metrics are tunable, simple yet highly effective, f"
W13-2254,P11-1023,1,0.921751,"such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a tunable and highly-accurate MT evaluation metric that drives MT system development towards higher utility. As described in Lo and Wu (2011a), the paThe linguistically transparent MEANT and UMEANT metrics are tunable, simple yet highly effective, fully automatic approximation to the human HMEANT MT evaluation metric which measures semantic frame similarity between MT output and reference translations. In this paper, we describe HKUST’s submission to the WMT 2013 metrics evaluation task, MEANT and UMEANT. MEANT is optimized by tuning a small number of weights—one for each semantic role label—so as to maximize correlation with human adequacy judgment on a development set. UMEANT is an unsupervised version where weights for each sem"
W13-2254,W12-3103,0,0.252366,"uez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features of ULC and is based on pure lexical similarity. Therefore, QUEEN suffers from the problem of failing to reflect translation adequacy similar to other n-gram based metrics. Similarly, SPEDE (Wang and Manning, 2012) is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase table. Like ULC, these matrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive in training and tuning due to the large number of parameters to be estimated. Although ROSE (Song and Cohn, 2011) is a weighted linear model of shallow ling"
W13-2254,W12-4206,1,0.893952,"oices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al., 2006; Koehn and Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a t"
W13-2254,2013.mtsummit-papers.12,1,0.836247,"at a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a tunable and highly-accurate MT evaluation metric that drives MT system development towards higher utility. As described in Lo and Wu (2011a), the paThe linguistically transparent MEANT and UMEANT metrics are tunable, simple yet highly effective, fully automatic approximation to the human HMEANT MT evaluation metric which measures semantic frame similarit"
W13-2254,W11-2113,0,0.0503421,"put to match the reference. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase table. Like ULC, these matrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive in training and tuning due to the large number of parameters to be estimated. Although ROSE (Song and Cohn, 2011) is a weighted linear model of shallow linguistic features which is cheaper in run time but it still contains several dozens of weights that need to be tuned which affects the portability of the metric for evaluating translations across domains. Rios et al. (2011) introduced TINE, an automatic recall-oriented evaluation metric which aims to preserve the basic event structure, but no work has been done toward tuning an SMT system against it. TINE performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. rameters in MEANT, i.e. the weight for each semantic ro"
W13-2254,Y12-1062,1,0.767235,"fillers. 424 semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, wpred and wj are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT, wpred and wj are estimated in an unsupervised manner using relative frequency of each semantic role label in the reference translations when the human judgments on adequacy of the development set were unavailable (Lo and Wu, 2012). In this experiment, we use a MEANT / UMEANT implementation along the lines described in Lo et al. (2012) and Tumuluru et al. (2012) but we incorporate a variant of the aggregation function proposed in Mihalcea et al. (2006) for phrasal similarity of role fillers as it normalizes the phrase length better than geometric mean as described in Tumuluru et al. (2012). In case there is no semantic frame in the sentence, we treat the whole sentence as a phrase and calculate the phrasal similarity, like the role fillers in step 3.1, as the MEANT score. 4 We evaluated MEANT and UMEANT on 3 groups of test sets. The first group is the original (without partition) test data for each language pair (translated in English) in WMT12. This"
W13-2254,P13-2067,1,0.797154,"the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a tunable and highly-accurate MT evaluation metric that drives MT system development towards higher utility. As described in Lo and Wu (2011a), the paThe linguistically transparent MEANT and UMEANT metrics are tunable, simple yet highly effective, fully automatic approximation to the human HMEANT MT evaluation metric which measures semant"
W13-2254,W12-3107,0,0.193407,"oices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al., 2006; Koehn and Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a t"
W13-2254,niessen-etal-2000-evaluation,0,0.923805,", or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a tunable and highly-accurate MT evaluation metric that drives MT system development towards higher utility. As described in Lo and Wu (2011a), the paThe linguistically transparent MEANT and UMEANT metrics are tunable, simple yet highly effective, fully automatic approximatio"
W13-2254,P02-1040,0,0.113699,"WU HKUST Human Language Technology Center Department of Computer Science and Engineering Hong Kong University of Science and Technology {jackielo|dekai}@cs.ust.hk Abstract between the MT output and the reference translations via semantic role labels (SRL). Recent studies (Lo et al., 2013; Lo and Wu, 2013) show that tuning MT systems against MEANT more robustly improves translation adequacy, compared to tuning against BLEU or TER. In the past decade, the progress of machine translation (MT) research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al., 2006; Koehn and Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—"
W13-2254,W11-2112,0,0.198213,"une and employ expensive linguistic resources, like WordNet and paraphrase table. Like ULC, these matrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive in training and tuning due to the large number of parameters to be estimated. Although ROSE (Song and Cohn, 2011) is a weighted linear model of shallow linguistic features which is cheaper in run time but it still contains several dozens of weights that need to be tuned which affects the portability of the metric for evaluating translations across domains. Rios et al. (2011) introduced TINE, an automatic recall-oriented evaluation metric which aims to preserve the basic event structure, but no work has been done toward tuning an SMT system against it. TINE performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. rameters in MEANT, i.e. the weight for each semantic role label, could be estimated using simple grid search to optimize the correlation with human adequacy judgments. Later, Lo and Wu (2012) described an unsupervised approach for estimating the parameters of MEANT using relative frequency of each semantic role label"
W13-2254,W12-3102,0,\N,Missing
W13-2254,W12-3129,1,\N,Missing
W13-2254,W12-3104,0,\N,Missing
W13-2254,N04-1030,0,\N,Missing
W13-2810,N10-1028,0,0.296831,"n BLEU, we focus on scientifically understanding the behavior of pure MDL-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing parsimonious, relatively clean phrasal inversion transduction grammars or ITGs (Wu, 1997) that employs a theo"
W13-2810,P09-1088,0,0.389373,"Missing"
W13-2810,D07-1103,0,0.12647,"Missing"
W13-2810,J93-2003,0,0.0792232,"Missing"
W13-2810,N10-1015,0,0.121115,"tifically understanding the behavior of pure MDL-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing parsimonious, relatively clean phrasal inversion transduction grammars or ITGs (Wu, 1997) that employs a theoretically well-founded"
W13-2810,W07-0403,0,0.059124,"s. Instead of embedding our learned ITG in the midst of many other heuristic components for the sake of a short term boost in BLEU, we focus on scientifically understanding the behavior of pure MDL-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised appr"
W13-2810,P11-1064,0,0.177299,"Missing"
W13-2810,P05-1033,0,0.124966,"onious, relatively clean phrasal inversion transduction grammars or ITGs (Wu, 1997) that employs a theoretically well-founded minimum description length (MDL) objective to explicitly drive two opposing, extreme ITGs towards one minimal ITG. This represents a new attack on the problem suffered by most current SMT approaches of learning phrase translations that require enormous amounts of run-time memory, contain a high degree of redundancy, and fails to provide an obvious basis for generalization to abstract translation schemas. In particular, phrasal SMT models such as Koehn et al. (2003) and Chiang (2005) often search for candidate translation segments and transduction rules by committing 67 Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 67–73, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics When the structure of an ITG is induced without supervision, it has so far been assumed that smaller rules get clumped together into larger rules. This is a natural way to search, since maximum likelihood (ML) tends to improve with longer rules, which is typically balanced with Bayesian priors (Zhang et al., 2008). Bayesian priors are also us"
W13-2810,P12-1018,0,0.107404,"Missing"
W13-2810,J07-2003,0,0.208877,"le probability function p′ is identical to p, except that: 46,867 sentence pairs of training data, and 489 Chinese sentences with 6 English reference translations each as test data; all the sentences are taken from the traveling domain. Since the Chinese is written without whitespace, we use a tool that tries to clump characters together into more “word like” sequences (Wu, 1999). After each iteration, we use the long ITG to translate the held out test set with our in-house ITG decoder. The decoder uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) and cube pruning (Chiang, 2007) to integrate the language model scores. The decoder builds an efficient hypergraph structure which is scored using both the induced grammar and a language model. We use SRILM (Stolcke, 2002) for training a trigram language model on the English side of the training corpus. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). We also perform a combination experiment, where the grammar at different stages of the learning process (iterations) are interpolated with each other. This is a straight-forward linear interpolation, where the probabiliti"
W13-2810,J03-1002,0,0.0241249,"Missing"
W13-2810,P02-1040,0,0.0965798,"into more “word like” sequences (Wu, 1999). After each iteration, we use the long ITG to translate the held out test set with our in-house ITG decoder. The decoder uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) and cube pruning (Chiang, 2007) to integrate the language model scores. The decoder builds an efficient hypergraph structure which is scored using both the induced grammar and a language model. We use SRILM (Stolcke, 2002) for training a trigram language model on the English side of the training corpus. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). We also perform a combination experiment, where the grammar at different stages of the learning process (iterations) are interpolated with each other. This is a straight-forward linear interpolation, where the probabilities of the rules are added up and the grammar is renormalized. Although it makes little sense from an MDL point of view to increase the size of the grammar so indiscriminately, it does make sense from an engineering point of view, since more rules typically means better coverage, which in turn typically means better translations of unknown data. p′"
W13-2810,P10-1017,0,0.0597358,"the behavior of pure MDL-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing parsimonious, relatively clean phrasal inversion transduction grammars or ITGs (Wu, 1997) that employs a theoretically well-founded minimum description leng"
W13-2810,P06-1121,0,0.121311,"rs, making it hard to evaluate how accurate the learned models themselves were. In this work we take a radically different approach, and start with the longest rules possible and attempt to segment them into shorter rules iteratively. This makes ML useless, since our initial model maximizes it. Instead, we balance the ML objective with a minimum description length (MDL) objective, which let us escape the initial ML optimum by rewarding model parsimony. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints (Galley et al., 2006). This complicates the learning process by adding external constraints that are bound to match the translation model poorly. It does, however, constitute a way to borrow nonterminal categories that help the translation model. MDL has been used before in monolingual grammar induction (Grünwald, 1996; Stolcke and Omohundro, 1994), as well as to interpret visual scenes (Si et al., 2011). Our work is markedly different in that we (a) induce an ITG rather than a monolingual grammar, and (b) focus on learning the terminal segments rather than the nonterminal categories. Iterative segmentation has al"
W13-2810,C96-2141,0,0.521476,"Missing"
W13-2810,2011.eamt-1.42,1,0.912018,"Missing"
W13-2810,W09-3804,1,0.961398,"y other heuristic components for the sake of a short term boost in BLEU, we focus on scientifically understanding the behavior of pure MDL-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing parsimonious, relatively clean ph"
W13-2810,N10-1050,1,0.86223,"L-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing parsimonious, relatively clean phrasal inversion transduction grammars or ITGs (Wu, 1997) that employs a theoretically well-founded minimum description length (MDL) objective to"
W13-2810,P08-1012,0,0.129418,"ITG in the midst of many other heuristic components for the sake of a short term boost in BLEU, we focus on scientifically understanding the behavior of pure MDL-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing pars"
W13-2810,C12-1142,1,0.930882,"ions, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing parsimonious, relatively clean phrasal inversion transduction grammars or ITGs (Wu, 1997) that employs a theoretically well-founded minimum description length (MDL) objective to explicitly drive two opposi"
W13-2810,W05-0835,0,0.399325,"Missing"
W13-2810,2007.iwslt-1.1,0,\N,Missing
W13-2810,W09-2304,1,\N,Missing
W13-2810,N03-1017,0,\N,Missing
W13-2810,J97-3002,1,\N,Missing
W13-2810,P09-1104,0,\N,Missing
W13-5703,N10-1028,0,0.0803563,"g different iterations of the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into th"
W13-5703,P09-1088,0,0.0748141,"e word alignment). We will show that we can counter this by combining different iterations of the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard con"
W13-5703,N10-1015,0,0.0156261,"the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into the vast engineering effo"
W13-5703,W07-0403,0,0.0205269,"ossible segmental translations (that do not violate the prerequisite word alignment). We will show that we can counter this by combining different iterations of the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for"
W13-5703,J07-2003,0,0.0228713,"w/categories only Segmented ITG only Segmented ITG mixed with chunked ITG Segmented ITG mixed with chunked ITG w/categories Segmented ITG conditioned on chunked ITG Segmented ITG conditioned on chunked ITG w/categories ... with iterations combined ... and improved search parameters 0 Figure 2: Rule count versus BLEU scores for the bootstrapped ITG, the pruned bootstrapped ITG and the segmented ITG conditioned on the pruned bootstrapped ITG. lation systems with our in-house ITG decoder. The decoder uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) and cube pruning (Chiang, 2007) to integrate the language model scores. For language model, we use a trigram language model trained with the SRILM toolkit (Stolcke, 2002) on the English side of the training corpus. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). to have when translating are not explicitly in the grammar. This is potentially a source of translation mistakes, and to investigate this, we create a mixture model from iterations of the segmenting learning process leading up to the learned ITG. All the above outlined ITGs are trained using the IWSLT07 Chines"
W13-5703,W09-2304,1,0.835944,"nter this by combining different iterations of the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it"
W13-5703,2011.eamt-1.42,1,0.822178,"significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into the vast engineering efforts that have gone into perfecting existing decoders, it also prev"
W13-5703,P06-1121,0,0.0173463,"e model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. DL (Ψ, Φ, D) = DL (D|Φ, Ψ) + DL (Φ|Ψ) + DL (Ψ) In minimizing CDL, we fix DL (Ψ) instead of allowing Ψ to vary as we would in full MCDL; to be precise, we seek: argmin DL (Ψ, Φ, D) Φ = argmin DL (D|Φ, Ψ) + DL (Φ|Ψ) + DL (Ψ) Φ = argmin DL (Φ, D|Ψ) Φ Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints (Galley et al., 2006). Although this constitutes a way to borrow nonterminal categories that help the translation model, it complicates the learning process by adding external constraints that are bound to match the translation model poorly. = argmin DL (D|Φ, Ψ) + DL (Φ|Ψ) Φ To measure the CDL of the data, we turn to information theory to count the number of bits needed to encode the data given the two models under an optimal encoding (Shannon, 1948), which gives: DL (D|Φ, Ψ) = −lg P (D|Φ, Ψ) 28 To measure the CDL of the model, we borrow the encoding scheme for description length presented in Saers et al. (2013),"
W13-5703,W09-3804,1,0.850213,"chnique described in Saers et al. (2012) to start with a finite-state transduction grammar (FSTG) and perform chunking before splitting the nonterminal categories and moving the FSTG into ITG form. We will perform one round of chunking, and two rounds of category splitting (resulting in 4 nonterminals and 4 preterminals, which becomes 8 nonterminals in the ITG form). Splitting all categories is guarnteed to at least double the size of the grammar, which makes is impractical to repeat more times. At each stage, we run a few iterations of expectation maximization using the algorithm detailed in Saers et al. (2009) for biparsing. For comparison we also bootstrap a comparable ITG that has not had the categories split. Before using either of the bootstrapped ITGs, we eliminate all rules that do not have a probability above a threshold that we fixed to 10−50 . This eliminates the highly unlikely rules from the ITG. For the second stage, we use the iterative rule segmentation learning algorithm driven by minimum conditional description length that we introduced in Section 5. We will try three different variants on this algorithm: one without an ITG to condition on, one conditioned on the chunked ITG, and on"
W13-5703,N10-1050,1,0.830417,"ammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into the vast engineering efforts that have gone into perfecting existing d"
W13-5703,P09-1104,0,0.0191262,"ill show that we can counter this by combining different iterations of the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in thi"
W13-5703,C12-1142,1,0.85922,"Missing"
W13-5703,P11-1064,0,0.0586452,"LEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into the vast engineering efforts that have gone into perfecting existing decoders, it also prevents you from surpassi"
W13-5703,W13-0806,1,0.776941,"thin the context of relevant related research (Section 2 ), we define the proposed conditional description length approach, which represents the ideal model search (Section 3 ). We then detail the two stages of our proposed learning algorithm, which represents our approximation of the search problem (Sections 4 and 5 ). After the theory we detail the particular experiments we conducted (Section 6 ) and the results from those experiments (Section 7). Finally, we offer some conclusions (Section 8 ). 2 Background Description length has been used before to drive iterative segmenting ITG learning (Saers et al., 2013). We will use their algorithm as our baseline, but the simple mixture model we used then works poorly with our ITG with categories. Instead, we propose a tighter incorporation, where the rule segmenting learning is biased towards rules that are present in the categorized ITG. We refer to this objective as minimizing conditional description length, since technically, the length of the ITG being segmented is conditioned on the categorized ITG. Conditional description length (CDL) is detailed in Section 3. The minimum CDL (MCDL) objective differs from the simple mixture model in that it separates"
W13-5703,P12-1018,0,0.044226,"n the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into the vast engineering efforts that have gone into perfecting existing decoders, it also prevents you from surpassing them in the long run. T"
W13-5703,P02-1040,0,0.0895003,".. and improved search parameters 0 Figure 2: Rule count versus BLEU scores for the bootstrapped ITG, the pruned bootstrapped ITG and the segmented ITG conditioned on the pruned bootstrapped ITG. lation systems with our in-house ITG decoder. The decoder uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) and cube pruning (Chiang, 2007) to integrate the language model scores. For language model, we use a trigram language model trained with the SRILM toolkit (Stolcke, 2002) on the English side of the training corpus. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). to have when translating are not explicitly in the grammar. This is potentially a source of translation mistakes, and to investigate this, we create a mixture model from iterations of the segmenting learning process leading up to the learned ITG. All the above outlined ITGs are trained using the IWSLT07 Chinese–English data set (Fordyce, 2007), which contains 46,867 sentence pairs of training data, and 489 Chinese sentences with 6 English reference translations each as test data; all the sentences are taken from the traveling domain. Since the Chinese is written w"
W13-5703,P10-1017,0,0.0185273,"process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into the vast engineering efforts that have gone into"
W13-5703,P08-1012,0,0.0881242,"lations (that do not violate the prerequisite word alignment). We will show that we can counter this by combining different iterations of the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly"
W14-4003,W12-3103,0,0.116296,"porates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to match the machine translation to the reference translation via an integrated probabilistic FSM and probabilistic PDA model. The semantic textual similarity metric Sagan (Castillo and Estrella, 2012) is based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain many parameters that need to be tuned, and employ expensive linguistic resources such as WordNet or paraphrase tables. The expensive training, tuning and/or running time renders these metrics difficult to use in the SMT training cycle. 3 replace humans with automatic SRL and alignment algorithms. MEANT typically outperforms BLEU, NIST, METEOR, WER, CDER and TER in correlation with human adequacy judgment, and is relatively easy to port to other languages, requiri"
W14-4003,W14-3348,0,0.0416962,"ditdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Ri"
W14-4003,W07-0738,0,0.459718,"adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to match the machine translation to the reference translation via an integrated probabilistic FSM and probabilistic PDA model. The semantic textual similarity"
W14-4003,2012.eamt-1.64,1,0.919302,"ITGs are guaranteed to have a two-normal form similar to context-free grammars, and( can) be biparsed(in polynomial time and space (O n6 ) 4 time and O n space). It (is also ) possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This makes ITGs an appealing alternative for evalWe introduce an inversion transduction grammar based restructuring of the MEANT automatic semantic frame based MT evaluation metric, which, by leveraging ITG language biases, is able to further improve upon MEANT’s already-high correlation with human adequacy judgments. The new metric, called IMEANT, uses bracketing ITGs to biparse the reference and machine translations, but subject to obeying the semantic frames in both. Resulting improvements support the presumption that ITGs, which constrain the allowable permutations between compositional"
W14-4003,W08-0332,0,0.114202,"high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to match the machine translation to the reference translation via an integrated probabilistic FSM and probabilistic PDA model. The semantic textual similarity metric Sagan (Castillo and Estrella, 2012) is based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain many parameters that need to be tuned, and employ expensive linguistic"
W14-4003,W06-3114,0,0.111485,"T systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG"
W14-4003,W05-0909,0,0.276417,"who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and"
W14-4003,E06-1032,0,0.101147,"e, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency"
W14-4003,2003.mtsummit-papers.32,0,0.0492008,"sible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related Work 2.1 ITGs and MT evaluation Relatively little investigation into the potential benefits of ITGs is found in previous MT evaluation work. One exception is invWER, proposed by Leusch et al. (2003) and Leusch and Ney (2008). The invWER metric interprets weighted BITGs as a generalization of the Levenshtein edit distance, in which entire segments (blocks) can be inverted, as long as this is done strictly compositionally so as not to violate legal ITG biparse tree structures. The input and output languages are considered to be those of the reference and machine translations, and thus are over the same vocabulary (say,English). At the sentence level, correlation of invWER with human adequacy judgments was found to be among the best. Our current approach differs in several key respects from"
W14-4003,W07-0718,0,0.496708,"Missing"
W14-4003,E06-1031,0,0.492343,")—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER"
W14-4003,W05-0904,0,0.146685,"acy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to mat"
W14-4003,W13-2202,0,0.0494942,"reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-e"
W14-4003,P11-1023,1,0.944104,"on grammars, is able to exploit bracketing ITGs (also known as BITGs or BTGs) which are ITGs containing only a single non-differentiated non terminal category (Wu, 1995a), so as to produce even higher correlation with human adequacy judgments than any automatic MEANT variants, or other common automatic metrics. We argue that the constraints provided by BITGs over the semantic frames and arguments of the reference and MT output sentences are essential for accurate evaluation of the phrasal similarity of the semantic role fillers. In common with the various MEANT semantic MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), our proposed IMEANT metric measures the degree to which the basic semantic event structure is preserved by translation—the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEAN"
W14-4003,1996.amta-1.13,0,0.120255,"n be seen in Figure 3, which shows the result on the same example sentence as in Figure 1. Disregarding the semantic parsing errors arising from the current limitations of automatic SRL tools, the ITG tends to provide clean, sparse alignments for role fillers like the ARG1 of the resumed PRED, preferring to leave tokens like complete and range unaligned instead of aligning them anyway as MEANT’s maximal alignment algorithm tends to do. Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed that the competitive linking approach (Melamed, 1996) which also generally produces sparser alignments does not work as well in MEANT, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure. For contrast, Figure 4 shows a case where IMEANT appropriately accepts dense alignments. Table 1: Sentence-level correlation with human adequacy judgements on different partitions of GALE P2.5 data. IMEANT always yields top correlations, and is more consistent than either MEANT or its recent cross-lingual XMEANT quality estimation variant. For reference, the human HMEANT upper bound is 0.53 for"
W14-4003,W12-4206,1,0.915736,"Missing"
W14-4003,niessen-etal-2000-evaluation,0,0.583405,"ranslation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and"
W14-4003,2013.mtsummit-papers.12,1,0.803851,"mprove the accuracy of MT evaluation metrics, despite long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints (and the observation that most current state-of-the-art SMT systems employ ITG decoders). We show that ITGs can be used to redesign the MEANT semantic frame based MT evaluation metric (Lo et al., 22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related Work 2.1 ITGs and"
W14-4003,W13-2254,1,0.727687,"mprove the accuracy of MT evaluation metrics, despite long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints (and the observation that most current state-of-the-art SMT systems employ ITG decoders). We show that ITGs can be used to redesign the MEANT semantic frame based MT evaluation metric (Lo et al., 22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related Work 2.1 ITGs and"
W14-4003,P02-1040,0,0.0984325,"ic semantic event structure is preserved by translation—the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papin"
W14-4003,P13-2067,1,0.947369,"rs (Wu, 1997) to improve the accuracy of MT evaluation metrics, despite long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints (and the observation that most current state-of-the-art SMT systems employ ITG decoders). We show that ITGs can be used to redesign the MEANT semantic frame based MT evaluation metric (Lo et al., 22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related"
W14-4003,2013.iwslt-evaluation.5,1,0.888254,"rs (Wu, 1997) to improve the accuracy of MT evaluation metrics, despite long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints (and the observation that most current state-of-the-art SMT systems employ ITG decoders). We show that ITGs can be used to redesign the MEANT semantic frame based MT evaluation metric (Lo et al., 22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related"
W14-4003,W11-2112,0,0.235694,"Missing"
W14-4003,W09-3804,1,0.903027,"rminal is A, and R is a set of transduction rules with e ∈ W 0 ∪{ϵ} denoting a token in the MT output (or the null token) and f ∈ W 1 ∪ {ϵ} denoting a token in the reference translation (or the null token). The rule probability (or more accurately, rule weight) function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is defined using MEANT’s context vector model based lexical similarity measure. To calculate the inside probability (or more(accurately, inside score) of a ) ∗ pair of segments, P A ⇒ e/f|G , we use the algorithm described in Saers et al. (2009). Given this, si,pred and si,j now represent the length normalized BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and machine translations. 4 Experiments In this section we discuss experiments indicating that IMEANT further improves upon MEANT’s 26 ARG0 PRED ARG1 [MT2] The level of reduction is conducive to raising the inspection and supervision work efficiency . [REF] The reduction in hierarchy helps raise the efficiency of inspection and supervisory work . ARG0 ARG0 pred The level of reduction is conducive to raising the inspection and s"
W14-4003,2006.amta-papers.25,0,0.129607,"essfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do no"
W14-4003,Y12-1062,1,0.928956,"ly two months of SK - II line of products . Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using automatic English SRL. There are no semantic frames for MT3 since there is no predicate in the MT output. ∑ prece,f e∈e = ∑ rece,f = si,pred = si,j = max s(e, f ) |e| f ∈f max e∈e precei,pred ,fi,pred + recei,pred ,fi,pred si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we will assess IMEANT against the latest version of MEANT (Lo et al., 2014) which, as shown, uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers,since this has been sho"
W14-4003,W12-3107,0,0.556805,"na for nearly two months of SK - II line of products . Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using automatic English SRL. There are no semantic frames for MT3 since there is no predicate in the MT output. ∑ prece,f e∈e = ∑ rece,f = si,pred = si,j = max s(e, f ) |e| f ∈f max e∈e precei,pred ,fi,pred + recei,pred ,fi,pred si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we will assess IMEANT against the latest version of MEANT (Lo et al., 2014) which, as shown, uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fill"
W14-4003,P95-1033,1,0.718211,"eorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and( can) be biparsed(in polynomial time and space (O n6 ) 4 time and O n space). It (is also ) possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This makes ITGs an appealing alternative for evalWe introduce an inversion transduction grammar based restructuring of the MEANT automatic semantic frame based MT evaluation metric, which, by leveraging ITG language biases, is able to further improve upon MEANT’s already-high correlation with human adequacy judgments. The new metric, called IMEANT, uses bracketing ITGs to biparse the reference"
W14-4003,W95-0106,1,0.710667,"eorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and( can) be biparsed(in polynomial time and space (O n6 ) 4 time and O n space). It (is also ) possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This makes ITGs an appealing alternative for evalWe introduce an inversion transduction grammar based restructuring of the MEANT automatic semantic frame based MT evaluation metric, which, by leveraging ITG language biases, is able to further improve upon MEANT’s already-high correlation with human adequacy judgments. The new metric, called IMEANT, uses bracketing ITGs to biparse the reference"
W14-4003,J97-3002,1,0.672318,"Missing"
W14-4003,P03-1019,0,0.242835,"ANT still aims to evaluate MT output in terms of the degree to which the translation has preserved the essential “who did what to whom,for whom, when, where, how and why” of the foreign input sentence. Unlike MEANT, however, IMEANT aligns and scores under ITG assumptions. MEANT uses a maximum alignment algorithm to align the tokens in the role fillers between the reference and machine translations, and then scores by aggregating the lexical similarities into a phrasal similarity using an f-measure. In contrast, IMEANT aligns and scores by utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). To be precise in this regard, we can see IMEANT as differing from the foregoing description of MEANT in the definition of si,pred and si,j , as follows. The IMEANT metric Although MEANT was previously shown to produce higher correlation with human adequacy judgments compared to other automatic metrics, our error analyses suggest that it still suffers from a common weakness among metrics employing lexical similarity, namely that word/token alignments between the reference and machine translations are severely under constrained. No bijectivity or perm"
W14-4003,W07-0411,0,\N,Missing
W14-4003,W12-3129,1,\N,Missing
W14-4010,P09-1088,0,0.0224651,"be able to induce the desired ITG from the above corpus. The paper is structured so that we start by giving a definition of the grammar formalism we use: ITGs (Section 2). We then describe the notion of description length that we use (Section 3), and how ternary segmentation differs from and complements binary segmentation (Section 4). We then present our induction algorithm (Section 5) and give an example of a run through (Section 6). Finally we offer some concluding remarks (Section 7). There is also the option of implicitly defining all possible grammars, and sample from that distribution. Blunsom et al. (2009) do exactly that; they induce with collapsed Gibbs sampling which keeps one derivation for each training sentence that is altered and then resampled. The operations to change the derivations are split, join, delete and insert. The split-operator corresponds to binary segmentation, the join-operator corresponds to chunking; the delete-operator removes an internal node, resulting in its parent having three children, and the insert-operator allows a parent with three children to be normalized to have only two. The existence of ternary nodes in the derivation means that the learned grammar contain"
W14-4010,J07-2003,0,0.0256093,"h in Top-down Induction of Segmental ITGs Markus Saers Dekai Wu HKUST Human Language Technology Center Department of Computer Science and Engineering Hong Kong University of Science and Technology {masaers|dekai}@cs.ust.hk Abstract of a sequence of segments that can be individually translated and reordered to form the translation. This is counter-intuitive, as the who-did-what-towhoms of a sentence tends to be translated and reordered as units, rather than have their components mixed together. Transduction grammars (Aho and Ullman, 1972; Wu, 1997), also called hierarchical translation models (Chiang, 2007) or synchronous grammars, address this through a mechanism similar to context-free grammars. Inducing a segmental transduction grammar is hard, so the standard practice is to use a similar method as the surfacebased models use to learn the chunks, which is problematic, since that method mostly relies on memorizing the relationships that the mechanics of a compositional model is designed to generalize. A compositional translation model would be able to translate lexical chunks, as well as generalize different kinds of compositions; a segmental transduction grammar captures this by having segmen"
W14-4010,W99-0604,0,0.354448,"Missing"
W14-4010,I13-1165,1,0.734083,"of three sentence pairs with identical structure: Another way to arrive at a segmental ITG is to start with the degenerate chunking case: each sentence pair as a lexical item, and segment the existing lexical rules into shorter rules. Since the start point is the degenerate case when optimizing for data likelihood, this approach requires a different objective function to optimize against. Saers et al. (2013c) proposes to use description length of the model and the data given the model, which is subsequently expressed in a Bayesian form with the addition of a prior over the rule probabilities (Saers and Wu, 2013). The way they generate hypotheses is restricted to segmenting an existing lexical item into two parts, which is problematic, because embedded lexical items are potentially overlooked. he has a red book / han har en röd bok she has a biology book / hon har en biologibok it has begun / det har börjat The main difference is that Swedish concatenates rather than juxtaposes compounds such as biologibok instead of biology book. A bilingual person looking at this corpus would produce bilingual parse trees like those in Figure 1. Inducing this relatively simple segmental ITG from the data is, however"
W14-4010,W09-3804,1,0.898619,"Missing"
W14-4010,P08-1012,0,0.0283478,"chunk. Existing surface-based models (Och et al., 1999) have high recall in capturing the chunks, but tend to over-generate, which leads to big models and low precision. Surface-based models have no concept of hierarchical composition, instead they make the assumption that a sentence consists One natural way would be to start with a tokenbased grammar and chunk adjacent tokens to form segments. The main problem with chunking is that the data becomes more and more likely as the segments get larger, with the degenerate end point of all sentence pairs being memorized lexical items. Zhang et al. (2008) combat this tendency by introducing a sparsity prior over the rule probabilities, and variational Bayes to maximize the posterior probability of the data subject to this symmetric Dirichlet prior. To hypothesize possible chunks, they examine the Viterbi biparse of the existing model. Saers et al. (2012) use the entire parse forest to generate the hypotheses. They also bootstrap the ITG from linear and finite-state transduction grammars (LTGs, Saers (2011), and FSTGs), 86 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 86–93, c Octobe"
W14-4010,C12-1142,1,0.856627,"l way would be to start with a tokenbased grammar and chunk adjacent tokens to form segments. The main problem with chunking is that the data becomes more and more likely as the segments get larger, with the degenerate end point of all sentence pairs being memorized lexical items. Zhang et al. (2008) combat this tendency by introducing a sparsity prior over the rule probabilities, and variational Bayes to maximize the posterior probability of the data subject to this symmetric Dirichlet prior. To hypothesize possible chunks, they examine the Viterbi biparse of the existing model. Saers et al. (2012) use the entire parse forest to generate the hypotheses. They also bootstrap the ITG from linear and finite-state transduction grammars (LTGs, Saers (2011), and FSTGs), 86 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 86–93, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics rather than initialize the lexical probabilities from IBM models. identical, they already illustrate the common problem of rare embedded correspondences. Imagine a really simple corpus of three sentence pairs with identical structure"
W14-4010,W13-0806,1,0.907575,"nverted order respectively. With straight order, both the L0 and the L1 productions are generated left-to-right, but with inverted order, the L1 production is generated right-to-left. The brackets are frequently left out when there is only one element on the righthand side, which means that S → [A] is shortened to S → A. Like CFGs, ITGs also have a 2-normal form, analogous to the Chomsky normal form for CFGs, where the rules are further restricted to only the following four forms: 3 Description length We follow the definition of description length from Saers et al. (2013b,c,d,a); Saers and Wu (2013), that is: the size of the model is determined by counting the number of symbols needed to encode the rules, and the size of the data given the model is determined by biparsing the data with the model. Formally, given a grammar Φ its description length DL (Φ) is the sum of the length of the symbols needed to serialize the rule set. For convenience later on, the symbols are assumed to be uniformly distributed with a length of −lg N1 bits each (where N is the number of different symbols). The description length of the data D given the model is defined as DL (D|Φ) = −lgP (D|Φ). S → A, A → [BC] ,"
W14-4010,W13-2810,1,0.89492,"nverted order respectively. With straight order, both the L0 and the L1 productions are generated left-to-right, but with inverted order, the L1 production is generated right-to-left. The brackets are frequently left out when there is only one element on the righthand side, which means that S → [A] is shortened to S → A. Like CFGs, ITGs also have a 2-normal form, analogous to the Chomsky normal form for CFGs, where the rules are further restricted to only the following four forms: 3 Description length We follow the definition of description length from Saers et al. (2013b,c,d,a); Saers and Wu (2013), that is: the size of the model is determined by counting the number of symbols needed to encode the rules, and the size of the data given the model is determined by biparsing the data with the model. Formally, given a grammar Φ its description length DL (Φ) is the sum of the length of the symbols needed to serialize the rule set. For convenience later on, the symbols are assumed to be uniformly distributed with a length of −lg N1 bits each (where N is the number of different symbols). The description length of the data D given the model is defined as DL (D|Φ) = −lgP (D|Φ). S → A, A → [BC] ,"
W14-4010,J97-3002,1,0.67298,"Missing"
W14-4010,R13-1077,1,\N,Missing
W14-4013,P14-1129,0,0.0291439,"Missing"
W14-4013,W09-3804,1,0.937726,"ibuted vector representations have long been used for n-gram language modeling; these continuous-valued models exploit the generalization capabilities of neural networks, although there is no hidden contextual or hierarchical structure as in RAAM. Schwenk (2010) applies one such language model within an SMT system. In the simple recurrent neural networks (RNNs or SRNs) of Elman (1990), hidden layer representations are fed back to the input to dynamically represent an aggregate of the immediate contextual history. More recently, the probabilistic NNLMs of Bengio et al. (2003) and Bengio et al. (2009) follow in this vein. To represent hierarchical tree structure using vector representations, one simple family of approaches employs convolutional networks, as in Lee et al. (2009) for example. Collobert and Weston (2008) use a convolution neural network layer quite effectively to learn vector representations for words which are then used in a host of NLP tasks such as POS tagging, chunking, and semantic role labeling. RAAM approaches, and related recursive autoencoder approaches, can be more flexible than convolutional networks. Like SRNs, they can be extended in numerous ways. The URAAM (Uni"
W14-4013,P14-1066,0,0.020023,"sambiguation) approach for context-dependent translation probabilities of Carpuat and Wu (2007). Unlike TRAAM, the model does not contain structural constraints, and permutation of phrases must still be done in conventional PBSMT “shake’n’bake” style by relying mostly on a language model (in their case, a NNLM). The majority of work on learning bilingual distributed vector representations has not made use of recursive approaches or hidden contextual or compositional structure, as in the bilingual word embedding learning of Klementiev et al. (2012) or the bilingual phrase embedding learning of Gao et al. (2014). Schwenk (2012) uses a non-recursive neural network to predict phrase translation probabilities in conventional phrase-based SMT. Attempts have been made to generalize the distributed vector representations of monolingual ngram language models, avoiding any hidden contextual or hierarchical structure. Working within the framework of n-gram translation models, Son et al. (2012) generalize left-to-right monolingual n-gram models to bilingual n-grams, and study bilingual variants of class-based n-grams. However, their model does not allow tackling the challenge of modeling cross-lingual constitu"
W14-4013,W10-3815,0,0.0225199,"ristics while making predictions on reordering them. Similarly, for specific cross-lingual tasks such as word alignment, sense disambiguation, or machine translation, classifiers can simultaneously be trained in conjunction with evolving the vector representations to optimize task-specific accuracy (Chrisman, 1991). 2.1 Monolingual related work Distributed vector representations have long been used for n-gram language modeling; these continuous-valued models exploit the generalization capabilities of neural networks, although there is no hidden contextual or hierarchical structure as in RAAM. Schwenk (2010) applies one such language model within an SMT system. In the simple recurrent neural networks (RNNs or SRNs) of Elman (1990), hidden layer representations are fed back to the input to dynamically represent an aggregate of the immediate contextual history. More recently, the probabilistic NNLMs of Bengio et al. (2003) and Bengio et al. (2009) follow in this vein. To represent hierarchical tree structure using vector representations, one simple family of approaches employs convolutional networks, as in Lee et al. (2009) for example. Collobert and Weston (2008) use a convolution neural network l"
W14-4013,P14-1006,0,0.0186121,"nd demonstrate that the resulting embeddings outperform the baselines in word semantic similarity. They also add a single semantic similarity feature induced with bilingual embeddings to a phrase-based SMT log-linear model, and report improvements in BLEU. Compared to TRAAM, however, they only learn noncompositional features, with distributed vectors only representing biterminals (as opposed to biconstituents or bilingual subtrees), and so other A few applications of monolingual RAAM-style recursive autoencoders to bilingual tasks have also appeared. For cross-lingual document classification, Hermann and Blunsom (2014) use two separate monolingual fixed vector composition networks, one for each language. One provides the training signal for the other, and training is only on the embeddings. Li et al. (2013) described a use of monolingual recursive autoencoders within maximum entropy ITGs. They replace their earlier model for predicting reordering based on the first and the last tokens in a constituent, by instead using the context vector generated using the recursive autoencoder. Only input language context is used, unlike TRAAM which can use the input and output language contexts equally. Autoencoders have"
W14-4013,C12-2104,0,0.025553,"a neural network analogy to the PSD (phrase sense disambiguation) approach for context-dependent translation probabilities of Carpuat and Wu (2007). Unlike TRAAM, the model does not contain structural constraints, and permutation of phrases must still be done in conventional PBSMT “shake’n’bake” style by relying mostly on a language model (in their case, a NNLM). The majority of work on learning bilingual distributed vector representations has not made use of recursive approaches or hidden contextual or compositional structure, as in the bilingual word embedding learning of Klementiev et al. (2012) or the bilingual phrase embedding learning of Gao et al. (2014). Schwenk (2012) uses a non-recursive neural network to predict phrase translation probabilities in conventional phrase-based SMT. Attempts have been made to generalize the distributed vector representations of monolingual ngram language models, avoiding any hidden contextual or hierarchical structure. Working within the framework of n-gram translation models, Son et al. (2012) generalize left-to-right monolingual n-gram models to bilingual n-grams, and study bilingual variants of class-based n-grams. However, their model does not"
W14-4013,D11-1014,0,0.26836,"in general can be formed for transduction grammars of any rank. Moreover, with distributed vector representations, the notion of nonterminal categories in TRAAM is that of soft membership, unlike in symbolically represented transduction grammars. We start with bracketed training data that contains no bilingual category labels (like training data for Bracketing ITGs or BITGs). Training results in self-organizing clusters that have been automatically induced, representing soft nonterminal categories (unlike BITGs, which do not have differentiated nonterminal categories). 113 feature structures. Socher et al. (2011) used monolingual recursive autoencoders for sentiment prediction, with or without parse tree information; this was perhaps the first use of a RAAM style approach on a large scale NLP task, albeit monolingual. Scheible and Schütze (2013) automatically simplified the monolingual tree structures generated by recursive autoencoders, validated the simplified structures via manual evaluation, and showed that sentiment classification accuracy is not affected. 2.2 mechanisms for combining biterminal scores still need to be used to handle hierarchical structure, as opposed to seamlessly being integrat"
W14-4013,D13-1176,0,0.032655,"ted. 2.2 mechanisms for combining biterminal scores still need to be used to handle hierarchical structure, as opposed to seamlessly being integrated into the distributed vector representation model. Devlin et al. (2014) obtain translation accuracy improvements by extending the probabilistic NNLMs of Bengio et al. (2003), which are used for the output language, by adding input language context features. Unlike TRAAM, neither of these approaches symmetrically models the recursive structure of both the input and output language sides. Bilingual related work For convolutional network approaches, Kalchbrenner and Blunsom (2013) use a recurrent probabilistic model to generate a representation of the source sentence and then generate the target sentence from this representation. This use of input language context to bias translation choices is in some sense a neural network analogy to the PSD (phrase sense disambiguation) approach for context-dependent translation probabilities of Carpuat and Wu (2007). Unlike TRAAM, the model does not contain structural constraints, and permutation of phrases must still be done in conventional PBSMT “shake’n’bake” style by relying mostly on a language model (in their case, a NNLM). T"
W14-4013,C12-1089,0,0.0309618,"s is in some sense a neural network analogy to the PSD (phrase sense disambiguation) approach for context-dependent translation probabilities of Carpuat and Wu (2007). Unlike TRAAM, the model does not contain structural constraints, and permutation of phrases must still be done in conventional PBSMT “shake’n’bake” style by relying mostly on a language model (in their case, a NNLM). The majority of work on learning bilingual distributed vector representations has not made use of recursive approaches or hidden contextual or compositional structure, as in the bilingual word embedding learning of Klementiev et al. (2012) or the bilingual phrase embedding learning of Gao et al. (2014). Schwenk (2012) uses a non-recursive neural network to predict phrase translation probabilities in conventional phrase-based SMT. Attempts have been made to generalize the distributed vector representations of monolingual ngram language models, avoiding any hidden contextual or hierarchical structure. Working within the framework of n-gram translation models, Son et al. (2012) generalize left-to-right monolingual n-gram models to bilingual n-grams, and study bilingual variants of class-based n-grams. However, their model does not"
W14-4013,N12-1005,0,0.0328449,"esentations has not made use of recursive approaches or hidden contextual or compositional structure, as in the bilingual word embedding learning of Klementiev et al. (2012) or the bilingual phrase embedding learning of Gao et al. (2014). Schwenk (2012) uses a non-recursive neural network to predict phrase translation probabilities in conventional phrase-based SMT. Attempts have been made to generalize the distributed vector representations of monolingual ngram language models, avoiding any hidden contextual or hierarchical structure. Working within the framework of n-gram translation models, Son et al. (2012) generalize left-to-right monolingual n-gram models to bilingual n-grams, and study bilingual variants of class-based n-grams. However, their model does not allow tackling the challenge of modeling cross-lingual constituent order, as TRAAM does; instead it relies on the assumption that some other preprocessor has already managed to accurately re-order the words of the input sentence into exactly the order of words in the output sentence. Similarly, generalizations of monolingual SRNs to the bilingual case have been studied. Zou et al. (2013) generalize the monolingual recurrent NNLM model of B"
W14-4013,D13-1054,0,0.0202732,"SMT log-linear model, and report improvements in BLEU. Compared to TRAAM, however, they only learn noncompositional features, with distributed vectors only representing biterminals (as opposed to biconstituents or bilingual subtrees), and so other A few applications of monolingual RAAM-style recursive autoencoders to bilingual tasks have also appeared. For cross-lingual document classification, Hermann and Blunsom (2014) use two separate monolingual fixed vector composition networks, one for each language. One provides the training signal for the other, and training is only on the embeddings. Li et al. (2013) described a use of monolingual recursive autoencoders within maximum entropy ITGs. They replace their earlier model for predicting reordering based on the first and the last tokens in a constituent, by instead using the context vector generated using the recursive autoencoder. Only input language context is used, unlike TRAAM which can use the input and output language contexts equally. Autoencoders have also been applied to SMT in a very different way by Zhao et al. (2014) but without recursion and not for learning distributed vector representations of words; rather, they used nonrecursive a"
W14-4013,J97-3002,1,0.611129,"Missing"
W14-4013,D13-1141,0,0.0272077,"g within the framework of n-gram translation models, Son et al. (2012) generalize left-to-right monolingual n-gram models to bilingual n-grams, and study bilingual variants of class-based n-grams. However, their model does not allow tackling the challenge of modeling cross-lingual constituent order, as TRAAM does; instead it relies on the assumption that some other preprocessor has already managed to accurately re-order the words of the input sentence into exactly the order of words in the output sentence. Similarly, generalizations of monolingual SRNs to the bilingual case have been studied. Zou et al. (2013) generalize the monolingual recurrent NNLM model of Bengio et al. (2009) to learn bilingual word embeddings using conventional SMT word alignments, and demonstrate that the resulting embeddings outperform the baselines in word semantic similarity. They also add a single semantic similarity feature induced with bilingual embeddings to a phrase-based SMT log-linear model, and report improvements in BLEU. Compared to TRAAM, however, they only learn noncompositional features, with distributed vectors only representing biterminals (as opposed to biconstituents or bilingual subtrees), and so other A"
W14-4013,2007.mtsummit-papers.11,1,\N,Missing
W14-4719,2012.eamt-1.64,1,0.930826,"Gs are guaranteed to have a two-normal form similar ( ) ( ) to context-free grammars, and can be biparsed in polynomial( time ) and space (O n6 time and O n4 space). It is also possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for evaluating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent"
W14-4719,W05-0909,0,0.0608269,"valuation metrics—a high-impact real-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring"
W14-4719,E06-1032,0,0.0515052,"oo many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a"
W14-4719,W08-0309,0,0.0215813,"e j between the reference and machine translations. 4 Experiments In this section we discuss experiments comparing the four alternative lexical access preference and constraint strategies. 4.1 Experimental setup We compared using the DARPA GALE P2.5 Chinese-English translation test set, as used in Lo and Wu (2011a). The corpus includes the Chinese input sentences, each accompanied by an English reference translation and three participating state-of-the-art MT systems’ output. We computed sentence-level correlations following the benchmark assessment procedure used by WMT and NIST MetricsMaTr (Callison-Burch et al., 2008, 2010, 2011, 2012; Macháček and Bojar, 2013), which use Kendall’s τ correlation coefficient, to evaluate the correlation of evaluation metrics against human judgment on ranking the translation adequacy of the three systems’ output. A higher value for Kendall’s τ indicates more similarity to the human adequacy rankings by the evaluation metrics. The range of possible values of Kendall’s τ correlation coefficient is [-1, 1], where 1 means the 148 Table 1: Sentence-level correlation with human adequacy judgements on different partitions of GALE P2.5 data. For reference, the human HMEANT upper bo"
W14-4719,W10-1703,0,0.0746439,"Missing"
W14-4719,W12-3102,0,0.0524454,"Missing"
W14-4719,W06-3114,0,0.0383454,"plicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety"
W14-4719,E06-1031,0,0.030895,"eal-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the bas"
W14-4719,P11-1023,1,0.865963,"e main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST"
W14-4719,W12-4206,1,0.780835,"MT and REF, respectively. These weights estimate the degree of contribution of each frame to the overall meaning of the sentence. wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT output. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. 3 Comparison of multiword expression association approaches To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider four alternative approaches to defining the lexical similarities si,pred and si,j , all of which employ a standard context vector model of the individual words/tokens in the multiword expressio"
W14-4719,2013.mtsummit-papers.12,1,0.794283,"surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee"
W14-4719,W13-2254,1,0.767435,"surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee"
W14-4719,W12-3129,1,0.780658,"Missing"
W14-4719,P13-2067,1,0.835635,"Missing"
W14-4719,2013.iwslt-evaluation.5,1,0.781022,"Missing"
W14-4719,W13-2202,0,0.0555413,"and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as edit-distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: url{http://creativecommons.org/licenses/by/4.0/ 144 Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 144–153, Dublin, Ireland, August 23, 2014. Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using auto"
W14-4719,1996.amta-1.13,0,0.0738935,", 2009). Secondly, the permutation and bijectivity constraints enforced by the ITG provide better leverage to reject token alignments when they are not appropriate, compared with the maximal alignment approach which tends to be rather promiscuous. The ITG tends whenever appropriate to accept clean, sparse alignments for role fillers, prefering to leave tokens unaligned instead of aligning them anyway as the other strategies tend to do. Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed that the competitive linking approach (Melamed, 1996) does not work as well as the strategies considered in this paper, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure. 5 Conclusion We have compared four alternative lexical access strategies for aggregation, preferences, and constraints in scoring multiword expression associations that are far too numerous to be explicitly enumerated in lexicons, within the context of semantic frame based machine translation evaluation: bag-of-words, 149 Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized"
W14-4719,niessen-etal-2000-evaluation,0,0.0616973,"sed performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is prese"
W14-4719,P02-1040,0,0.0917038,"rd expression associations within automatic semantic MT evaluation metrics—a high-impact real-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation"
W14-4719,N04-1030,0,0.320691,"the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as edit-distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furth"
W14-4719,W09-2304,1,0.930201,"d expressions of semantic role fillers to be matched even when they should not be. In contrast, using a bracketing inversion transduction grammar can potentially better constrain permissible token alignment patterns between aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed permutations between semantic role fillers across the reference and machine translations for a sample sentence from the evaluation data. In this approach, both alignment and scoring are performed utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define si,pred and si,j as follows. ( ( ))  ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |)  si,pred = si,j = where G R ≡ ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p ([AA] |A) p (e/f |A) = = p (⟨AA⟩|A) = 1 s(e, f ) Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with e ∈ W 0 ∪ {ϵ} denoting a token in the MT output (or the null token) and f ∈ W 1 ∪ {ϵ} denoting a token in the reference translation (or the null token)"
W14-4719,W09-3804,1,0.924813,"general syntax-directed transductions (Aho and Ullman, 1972) is exponential. By constraining a syntax-directed transduction grammar to allow only monotonic straight and inverted reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar ( ) ( ) to context-free grammars, and can be biparsed in polynomial( time ) and space (O n6 time and O n4 space). It is also possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for e"
W14-4719,2006.amta-papers.25,0,0.030085,"adequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who d"
W14-4719,Y12-1062,1,0.864908,"bel in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. 3 Comparison of multiword expression association approaches To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider four alternative approaches to defining the lexical similarities si,pred and si,j , all of which employ a standard context vector model of the individual words/tokens in the multiword expression arguments between the reference and machine translations, as descibed by Lo et al. (2012) and Tumuluru et al. (2012). 3.1 Bag of words (geometric mean) The original MEANT approaches employed standard a bag-of-words strategy for lexical association. This baseline approach applies no alignment constraints on multiword expressions: ∑ si,pred si,j = = e e ∑ ∑ e∈ei,pred f ∈fi,pred lg(s(e,f )) |ei,pred |·|fi,pred | e∈ei,j ∑ f ∈fi,j lg(s(e,f )) |ei,j |·|fi,j | 146 3.2 Maximum alignment (precision-recall average) In the first maximum alignment based approach we will consider, the definitions of si,pred and si,j are inspired by Mihalcea et al. (2006) who normalize phrasal similarities according to the phrase length."
W14-4719,P95-1033,1,0.486517,"derings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar ( ) ( ) to context-free grammars, and can be biparsed in polynomial( time ) and space (O n6 time and O n4 space). It is also possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for evaluating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic f"
W14-4719,W95-0106,1,0.54073,"derings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar ( ) ( ) to context-free grammars, and can be biparsed in polynomial( time ) and space (O n6 time and O n4 space). It is also possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for evaluating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic f"
W14-4719,J97-3002,1,0.760126,"f precision and recall with a proper f-score. Although this is less consistent with the previous literature, such as Mihalcea et al. (2006), it seems more consistent with the overall f-score based approach of MEANT, and thus we include it in our comparison as a variant of the maximum alignment strategy. 3.4 si,pred = si,j = 2 · precei,pred ,fi,pred · recei,pred ,fi,pred precei,pred ,fi,pred + recei,pred ,fi,pred 2 · precei,j ,fi,j · recei,j ,fi,j precei,j ,fi,j + recei,j ,fi,j Inversion transduction grammar based There has been to date relatively little use of inversion transduction grammars (Wu, 1997) to improve the accuracy of MT evaluation metrics—despite (1) long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints, and (2) the observation that most current state-of-the-art SMT systems employ ITG decoders. Especially when considering semantic MT metrics, ITGs would seem to be a natural strategy for multiword expression association for several cognitively motivated reasons, having to do with language universal properties of cross-linguistic semantic frame structure. To begin with, it is quite natural to think of s"
W14-4719,P03-1019,0,0.0475249,"s can cause multiword expressions of semantic role fillers to be matched even when they should not be. In contrast, using a bracketing inversion transduction grammar can potentially better constrain permissible token alignment patterns between aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed permutations between semantic role fillers across the reference and machine translations for a sample sentence from the evaluation data. In this approach, both alignment and scoring are performed utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define si,pred and si,j as follows. ( ( ))  ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |)  si,pred = si,j = where G R ≡ ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p ([AA] |A) p (e/f |A) = = p (⟨AA⟩|A) = 1 s(e, f ) Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with e ∈ W 0 ∪ {ϵ} denoting a token in the MT output (or the null token) and f ∈ W 1 ∪ {ϵ} denoting a token in the reference translation"
W14-4719,W11-2103,0,\N,Missing
W15-3056,P12-1092,0,0.0486048,"els on a wide range of lexical semantic tasks. It is also common knowledge that raw co-occurrence counts do not work very well and performance can be improved when transformed by reweighing the counts for context informativeness and dimensionality reduction. In contrast to conventional word vector models, prediction based word vector models estimate the vectors directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus (Bengio et al., 2006; Collobert and Weston, 2008; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013; Turian et al., 2010). In this paper, we show that MEANT’s correlation with human adequacy judgments can be further improved by incorporating the word embeddings trained by the predict models. Subsequently, tuning MT system against the improved version of MEANT produce more adequate translations than tuning against BLEU. We show that, consistent with MEANTtuned systems that translate into Chinese, MEANT-tuned MT systems that translate into English also outperforms BLEUtuned systems across commonly used MT evaluation metrics, even in BLEU. The result is achieved by signif"
W15-3056,E06-1031,0,0.260585,"ce in both n-gram based metrics and edit distance based metrics, without overfitting to either type of metrics. We argue with the significant improvement in sentence-level correlation with human preferences in evaluating translations in English, the performance of MT system tuned against the newly improved MEANT would also improved. For WMT2015 tuning task, we tuned the basic Czech-English baseline system against the newly improved MEANT using the official development 5 Related Work Most of the common used MT evaluation metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) rely heavily on the exact match of the surface form of the tokens in the reference and the MT output. Thus, they do not only fail to capture the 438 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and backoff algorithm. system BLEU-tuned MEANT-tuned (official submitted buggy system) MEANT-tuned (α=1) MEANT-tuned (α=0.5) BLEU 19.38 18.20 18.96 19.74 NIST 6.48 6.27 6.44 6.62 WER 67.63 70.09 68.41 66.31"
W15-3056,W05-0904,0,0.0263281,"ed metrics, even in BLEU. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) evaluates lexical similarities beyond surface-form by incorporating a large collection of linguistic resources, like synonym table from hand-crafted WordNet and paraphrase table learned from large parallel corpus. Another trend of improving MT evaluation metrics is incorporating the evaluation of meaning structure of the translations. Owczarzak et al. (2007a,b) improved the correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity in Liu and Gildea (2005), but did not improve the correlation with human adequacy judgments when comparing to METEOR. Similarly, TINE, an automatic recall-oriented basic meaning event structured based evaluation metric (Rios et al., 2011) correlated with human adequacy judgment comparable to that of BLEU but not as high as that of METEOR. ULC (Giménez and Màrquez, 2007, 2008) incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no"
W15-3056,P11-1023,1,0.844558,"gn the arguments between the reference and MT output according to the lexical similarity of role fillers. = i wi0 ∑ wpred si,pred + j wj si,j ∑ 0 | wpred + j wj |qi,j ∑ i recall = wi0 ∑ 1 wpred si,pred +∑j wj si,j ∑ 1 | i wi wpred + j wj |qi,j ∑ wi1 2 · precision · recall precision + recall (1) (2) i MEANT 435 = (3) where wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT output. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). The value of these weights are determined in supervised manner using a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a) for MEANT and in unsupervised manner using relative frequency of each semantic role label in the references for UMEANT (Lo and Wu, 2012). Thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,j are the phrasal similarities of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et a"
W15-3056,W05-0909,0,0.01641,"BLEU-tuned MEANT-tuned (official submitted buggy system) MEANT-tuned (α=1) MEANT-tuned (α=0.5) BLEU 17.06 15.89 16.75 17.15 meaning similarities of lexicons that do not share the same surface form, but also ignore the meaning structures of the translations. NIST 5.99 5.80 5.95 6.08 WER 69.67 71.82 70.19 68.53 PER 52.86 53.93 53.05 52.03 CDER 59.85 61.43 60.29 59.07 TER 65.71 67.59 66.25 64.65 MEANT 40.10 39.34 40.12 40.23 structural and lexical semantics accurately and thus, MT system tuned against the improved MEANT beats BLEU-tuned system across commonly used metrics, even in BLEU. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) evaluates lexical similarities beyond surface-form by incorporating a large collection of linguistic resources, like synonym table from hand-crafted WordNet and paraphrase table learned from large parallel corpus. Another trend of improving MT evaluation metrics is incorporating the evaluation of meaning structure of the translations. Owczarzak et al. (2007a,b) improved the correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity in Liu and Gildea (2005), but did not improve the correlation"
W15-3056,P14-1023,0,0.0481982,"Missing"
W15-3056,W12-4206,1,0.882808,"T 435 = (3) where wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT output. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). The value of these weights are determined in supervised manner using a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a) for MEANT and in unsupervised manner using relative frequency of each semantic role label in the references for UMEANT (Lo and Wu, 2012). Thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,j are the phrasal similarities of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical similarities, s(e, f ), are computed using a discrete context vector model and how the phrasal similarities are computed by aggregating the lexical similarities via various heuristics. In the latest version of MEANT (Lo et al., 2014), as shown in above, it uses f-score t"
W15-3056,2013.mtsummit-papers.12,1,0.862524,"MT evaluation metrics, even in BLEU. The result is achieved by significantly improving MEANT’s sentence-level ranking correlation with human preferences through incorporating a more accurate distributional semantic model for lexical similarity and a novel backoff algorithm for evaluating MT output which automatic semantic parser fails to parse. The surprising result of MEANT-tuned systems having a higher BLEU score than BLEU-tuned systems suggests that MEANT is a more accurate objective function guiding the development of MT systems towards producing more adequate translation. 1 Introduction Lo and Wu (2013) showed that MEANT-tuned system for translating into Chinese outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU. However, such phenomena are not observed in MEANT-tuned system for translating into English. In this paper, for the first time, we present MT systems for translating into English, which is tuned to a improved version of MEANT, also outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU. The improvements in MEANT include incorporating more accurate distributional semantic model for lexical similarity and a novel back"
W15-3056,W07-0718,0,0.0416452,"ts by using LFG to extend the approach of evaluating syntactic dependency structure similarity in Liu and Gildea (2005), but did not improve the correlation with human adequacy judgments when comparing to METEOR. Similarly, TINE, an automatic recall-oriented basic meaning event structured based evaluation metric (Rios et al., 2011) correlated with human adequacy judgment comparable to that of BLEU but not as high as that of METEOR. ULC (Giménez and Màrquez, 2007, 2008) incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an MT system using a pure form of ULC perhaps due to its expensive run time. 6 Conclusion In this paper we presented the first results of using word embeddings to improve the correlation with human adequacy judgments of MEANT, the stateof-the-art semantic MT evaluation metric. We also showed that using a smaller and easy-to-obtain monolingual corpus (e.g., Gigaword, Wikipedia) for training the word embeddings does not significantly affect the accuracy of MEANT. We showe"
W15-3056,W12-3129,1,0.835401,"Missing"
W15-3056,W08-0309,0,0.09684,"Missing"
W15-3056,P13-2067,1,0.831076,"Missing"
W15-3056,2013.iwslt-evaluation.5,1,0.809594,"Missing"
W15-3056,P14-2124,1,0.923906,"using automatic English SRL. There are no semantic frames for MT3 since there is no predicate in the MT output. tic frames and role fillers in the reference and machine translations. MEANT typically outperforms BLEU, NIST, METEOR, WER, CDER and TER in correlation with human adequacy judgment, and is relatively easy to port to other languages, requiring only an automatic semantic parser and a monolingual corpus of the output language, which is used to train the discrete context vector model for computing the lexical similarity between the semantic role fillers of the reference and translation. Lo et al. (2014) describe a cross-lingual quality estimation variant, XMEANT, capable of evaluating translation quality without the need for expensive human reference translations, by utilizing semantic parses of the original foreign input sentence instead of a reference translation. MEANT is generally computed as follows: 4. Compute the weighted f-score over the matching role labels of these aligned predicates and role fillers according to the following definitions: 1. Apply an automatic shallow semantic parser to both the reference and machine translations. (Figure 1 shows examples of automatic shallow sema"
W15-3056,W14-3348,0,0.0363668,"fficial submitted buggy system) MEANT-tuned (α=1) MEANT-tuned (α=0.5) BLEU 17.06 15.89 16.75 17.15 meaning similarities of lexicons that do not share the same surface form, but also ignore the meaning structures of the translations. NIST 5.99 5.80 5.95 6.08 WER 69.67 71.82 70.19 68.53 PER 52.86 53.93 53.05 52.03 CDER 59.85 61.43 60.29 59.07 TER 65.71 67.59 66.25 64.65 MEANT 40.10 39.34 40.12 40.23 structural and lexical semantics accurately and thus, MT system tuned against the improved MEANT beats BLEU-tuned system across commonly used metrics, even in BLEU. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) evaluates lexical similarities beyond surface-form by incorporating a large collection of linguistic resources, like synonym table from hand-crafted WordNet and paraphrase table learned from large parallel corpus. Another trend of improving MT evaluation metrics is incorporating the evaluation of meaning structure of the translations. Owczarzak et al. (2007a,b) improved the correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity in Liu and Gildea (2005), but did not improve the correlation with human adequacy judgmen"
W15-3056,W14-3336,0,0.0808341,"different from that for MT optimization. = (4) 0 wi0 + wnf ∑ 1 wpred si,pred +∑j wj si,j ∑ 1 | i wi wpred + j wj |qi,j ∑ 0 + wnf ssent 1 + wnf ssent 1 wi1 + wnf precision · recall α · precision + (1 − α) · recall (5) i MEANT = (6) Note that we have also introduced the weight α for the precision and recall. Later, we show that 437 Table 1 shows the document-level Pearson’s score correlation and table 2 shows the sentence-level Kendall’s rank correlation with human preferences of the improved version of MEANT with the previous version of MEANT (Lo et al., 2014) on WMT2014 metrics task test set (Macháček and Bojar, 2014). For the sake of stable performance across all the tested language pairs, the weights of the semantic role labels are estimated in unsupervised manner. First and the most importantly, the documentlevel score correlation with human preferences of all versions of MEANT consistently outperforms all the submitted metrics in Macháček and Bojar (2014). While the variations on document-level correlation with human preferences of different versions of MEANT are not significant, we focus on discussing about the sentence-level results. On sentence-level ranking, MEANT with Gigaword word embeddings corr"
W15-3056,niessen-etal-2000-evaluation,0,0.0776404,"rics and edit distance based metrics, without overfitting to either type of metrics. We argue with the significant improvement in sentence-level correlation with human preferences in evaluating translations in English, the performance of MT system tuned against the newly improved MEANT would also improved. For WMT2015 tuning task, we tuned the basic Czech-English baseline system against the newly improved MEANT using the official development 5 Related Work Most of the common used MT evaluation metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) rely heavily on the exact match of the surface form of the tokens in the reference and the MT output. Thus, they do not only fail to capture the 438 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and backoff algorithm. system BLEU-tuned MEANT-tuned (official submitted buggy system) MEANT-tuned (α=1) MEANT-tuned (α=0.5) BLEU 19.38 18.20 18.96 19.74 NIST 6.48 6.27 6.44 6.62 WER 67.63 70.09 68.41 66.31 PER 50.48 51.84 50.77 49.22"
W15-3056,W07-0738,0,0.0261617,"etrics is incorporating the evaluation of meaning structure of the translations. Owczarzak et al. (2007a,b) improved the correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity in Liu and Gildea (2005), but did not improve the correlation with human adequacy judgments when comparing to METEOR. Similarly, TINE, an automatic recall-oriented basic meaning event structured based evaluation metric (Rios et al., 2011) correlated with human adequacy judgment comparable to that of BLEU but not as high as that of METEOR. ULC (Giménez and Màrquez, 2007, 2008) incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an MT system using a pure form of ULC perhaps due to its expensive run time. 6 Conclusion In this paper we presented the first results of using word embeddings to improve the correlation with human adequacy judgments of MEANT, the stateof-the-art semantic MT evaluation metric. We also showed that using a smaller"
W15-3056,W07-0411,0,0.0565554,"Missing"
W15-3056,P02-1040,0,0.10148,"t tuning against MEANT only achieves balanced performance in both n-gram based metrics and edit distance based metrics, without overfitting to either type of metrics. We argue with the significant improvement in sentence-level correlation with human preferences in evaluating translations in English, the performance of MT system tuned against the newly improved MEANT would also improved. For WMT2015 tuning task, we tuned the basic Czech-English baseline system against the newly improved MEANT using the official development 5 Related Work Most of the common used MT evaluation metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) rely heavily on the exact match of the surface form of the tokens in the reference and the MT output. Thus, they do not only fail to capture the 438 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and backoff algorithm. system BLEU-tuned MEANT-tuned (official submitted buggy system) MEANT-tuned (α=1) MEANT-tuned (α=0.5) BLEU 19.38 18.20 18.96 19.74"
W15-3056,W11-2112,0,0.0129728,"from hand-crafted WordNet and paraphrase table learned from large parallel corpus. Another trend of improving MT evaluation metrics is incorporating the evaluation of meaning structure of the translations. Owczarzak et al. (2007a,b) improved the correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity in Liu and Gildea (2005), but did not improve the correlation with human adequacy judgments when comparing to METEOR. Similarly, TINE, an automatic recall-oriented basic meaning event structured based evaluation metric (Rios et al., 2011) correlated with human adequacy judgment comparable to that of BLEU but not as high as that of METEOR. ULC (Giménez and Màrquez, 2007, 2008) incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an MT system using a pure form of ULC perhaps due to its expensive run time. 6 Conclusion In this paper we presented the first results of using word embeddings to improve the corre"
W15-3056,2006.amta-papers.25,0,0.0809249,"trics, without overfitting to either type of metrics. We argue with the significant improvement in sentence-level correlation with human preferences in evaluating translations in English, the performance of MT system tuned against the newly improved MEANT would also improved. For WMT2015 tuning task, we tuned the basic Czech-English baseline system against the newly improved MEANT using the official development 5 Related Work Most of the common used MT evaluation metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) rely heavily on the exact match of the surface form of the tokens in the reference and the MT output. Thus, they do not only fail to capture the 438 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and backoff algorithm. system BLEU-tuned MEANT-tuned (official submitted buggy system) MEANT-tuned (α=1) MEANT-tuned (α=0.5) BLEU 19.38 18.20 18.96 19.74 NIST 6.48 6.27 6.44 6.62 WER 67.63 70.09 68.41 66.31 PER 50.48 51.84 50.77 49.22 CDER 58.17 59.93 58.74 57.20 T"
W15-3056,Y12-1062,1,0.830125,"and Wu (2011b). The value of these weights are determined in supervised manner using a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a) for MEANT and in unsupervised manner using relative frequency of each semantic role label in the references for UMEANT (Lo and Wu, 2012). Thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,j are the phrasal similarities of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical similarities, s(e, f ), are computed using a discrete context vector model and how the phrasal similarities are computed by aggregating the lexical similarities via various heuristics. In the latest version of MEANT (Lo et al., 2014), as shown in above, it uses f-score to aggregate individual token similarities into the phrasal similarities of semantic role fillers. Another MEANT’s variant, IMEANT (Wu et al., 2014), which uses ITG to constrain the token alignments between the semantic role fillers of the reference and the machine translations and is shown outperformi"
W15-3056,P10-1040,0,0.010477,"sks. It is also common knowledge that raw co-occurrence counts do not work very well and performance can be improved when transformed by reweighing the counts for context informativeness and dimensionality reduction. In contrast to conventional word vector models, prediction based word vector models estimate the vectors directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus (Bengio et al., 2006; Collobert and Weston, 2008; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013; Turian et al., 2010). In this paper, we show that MEANT’s correlation with human adequacy judgments can be further improved by incorporating the word embeddings trained by the predict models. Subsequently, tuning MT system against the improved version of MEANT produce more adequate translations than tuning against BLEU. We show that, consistent with MEANTtuned systems that translate into Chinese, MEANT-tuned MT systems that translate into English also outperforms BLEUtuned systems across commonly used MT evaluation metrics, even in BLEU. The result is achieved by significantly improving MEANT’s sentence-level ran"
W15-3056,W14-4003,1,0.719675,"imilarities of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical similarities, s(e, f ), are computed using a discrete context vector model and how the phrasal similarities are computed by aggregating the lexical similarities via various heuristics. In the latest version of MEANT (Lo et al., 2014), as shown in above, it uses f-score to aggregate individual token similarities into the phrasal similarities of semantic role fillers. Another MEANT’s variant, IMEANT (Wu et al., 2014), which uses ITG to constrain the token alignments between the semantic role fillers of the reference and the machine translations and is shown outperforming MEANT (Lo et al., 2014). 3 vector model is the total number of token types in the training corpus. The vector sparsity issue makes the lexical similarity highly sensitive of exact token matching and thus hurts the accuracy of MEANT. We aim at tackling the sparse vector issue by replacing the discrete context vector model with the continuous word embeddings in order to further improve the accuracy of MEANT. We first train the word embeddin"
W16-1207,J93-2003,0,0.119052,"forms: S → A, A → [BC] , A → ⟨BC⟩, A → e/f, A → e/ϵ, A → ϵ/f where S ∈ N is the start symbol, A, B, C ∈ N are nonterminals, e ∈ W0 is an L0 token, f ∈ W1 is an L1 token, and ϵ is the empty token. A bracketing ITG, or BITG, has only one nonterminal symbol (other than the dedicated start symbol), which means that the nonterminals carry no information at all other than the fact that their yields are discrete unit. 2 Related work The use of structure as input to the training of a statistical model in machine translation was pioneered by Yamada and Knight (2001), where they extend the IBM model 1 (Brown et al., 1993) to incorporate syntactic features derived from a parse tree on the output language (the input to the noisy channel, but the output of the decoder). The generative story of the model is that an English parse tree has the children of its nodes reordered, gets the option to insert a foreign token to the left or right of any node, and finally have all the English leaf nodes translated. Reading the leaf nodes of the tree in order yields the generated foreign sentence. Being a generative model, it is straight forward to train using EM, which they do. Manual evaluation shows that the word alignments"
W16-1207,D12-1079,0,0.0167389,"sa and Marcu (2010) formulate a discriminative k-best alignment model with hundreds of features that can be used to choose the best alignment that matches a given tree. The method as presented require some analysis on both input and output language for the feature model, as well as a set of hand aligned sentences for training; neither of which are readily available for low resource languages. Similarly, DeNero and Klein (2007) describe a model to tailor word alignments to existing syntactic trees; in our model, we instead consider all possible trees allowed by a CFG. Burkett et al. (2010) and Burkett and Klein (2012) take the opposite approach and alter the trees to fit the alignments, as presented, their approach requires parallel treebanks to train on, which cannot be expected to be available when translating to or from a low resource language. It is also possible to learn translation rules focusing on the syntax of the input language rather than output language. Huang et al. (2006) turn the tables, and learn translation rules with the input language parsed. Their approach forces the decoder to commit to a single input tree and then build possible output strings rather than building possible output tree"
W16-1207,N10-1015,0,0.0231144,"ments and parse trees. Riesa and Marcu (2010) formulate a discriminative k-best alignment model with hundreds of features that can be used to choose the best alignment that matches a given tree. The method as presented require some analysis on both input and output language for the feature model, as well as a set of hand aligned sentences for training; neither of which are readily available for low resource languages. Similarly, DeNero and Klein (2007) describe a model to tailor word alignments to existing syntactic trees; in our model, we instead consider all possible trees allowed by a CFG. Burkett et al. (2010) and Burkett and Klein (2012) take the opposite approach and alter the trees to fit the alignments, as presented, their approach requires parallel treebanks to train on, which cannot be expected to be available when translating to or from a low resource language. It is also possible to learn translation rules focusing on the syntax of the input language rather than output language. Huang et al. (2006) turn the tables, and learn translation rules with the input language parsed. Their approach forces the decoder to commit to a single input tree and then build possible output strings rather than"
W16-1207,P07-1003,0,0.0320137,"s the sentences, within a generative model that can be optimized globally across the entire training data. It is possible to alleviate the mismatch between given alignments and parse trees. Riesa and Marcu (2010) formulate a discriminative k-best alignment model with hundreds of features that can be used to choose the best alignment that matches a given tree. The method as presented require some analysis on both input and output language for the feature model, as well as a set of hand aligned sentences for training; neither of which are readily available for low resource languages. Similarly, DeNero and Klein (2007) describe a model to tailor word alignments to existing syntactic trees; in our model, we instead consider all possible trees allowed by a CFG. Burkett et al. (2010) and Burkett and Klein (2012) take the opposite approach and alter the trees to fit the alignments, as presented, their approach requires parallel treebanks to train on, which cannot be expected to be available when translating to or from a low resource language. It is also possible to learn translation rules focusing on the syntax of the input language rather than output language. Huang et al. (2006) turn the tables, and learn tra"
W16-1207,N04-1035,0,0.0950343,"t of the evaluated 50 sentence pairs perfectly aligned, whereas IBM model 5 got none. There are two key differences between Yamada and Knight (2001) and out model: (a) Their model describes how a foreign sentence is generated from an English parse tree, our model describes how sentence pairs are jointly generated. And (b) their model requires committing to a single English parse tree, our model jointly parses and aligns the sentences, effectively integrating out all parse trees that our grammar allows for the English sentence. Perhaps the most prolific translation model that involves trees is Galley et al. (2004), which learns very complex rules such as (ne VB pas) → (VP (AUX does) (RB not) x2 ) where x2 is a variable binding to the second element in the left-hand 57 side. The method takes a parallel sentence pair where one of the sentences has been parsed, and a word alignment, and produces, for each observed word aligned sentence pair, the minimal set of rules to explain it. This method allows complicated rules to be extracted, and different feature scores to be calculated for the extracted rules. This does, however, come at the cost of not being able to optimize the model in any meaningful way. Ins"
W16-1207,2006.amta-papers.8,0,0.0439611,"urce languages. Similarly, DeNero and Klein (2007) describe a model to tailor word alignments to existing syntactic trees; in our model, we instead consider all possible trees allowed by a CFG. Burkett et al. (2010) and Burkett and Klein (2012) take the opposite approach and alter the trees to fit the alignments, as presented, their approach requires parallel treebanks to train on, which cannot be expected to be available when translating to or from a low resource language. It is also possible to learn translation rules focusing on the syntax of the input language rather than output language. Huang et al. (2006) turn the tables, and learn translation rules with the input language parsed. Their approach forces the decoder to commit to a single input tree and then build possible output strings rather than building possible output trees. The model has also been generalized so that rules can be extracted from input forests rather than single trees (Mi and Huang, 2008), and the decoder has been extended to accept forests as input (Mi et al., 2008). The rule extraction process still requires a word alignment to be provided. Since our model is based on a grammar rather than parse trees, it essentially integ"
W16-1207,J93-2004,0,0.0531507,"). We implement skipping as implicit, low probability rules on the following forms: A → [A ϵ/f ] , A → [ϵ/f A] , A → [A e/ϵ] , A → [e/ϵ A] This allows the parser to maintain the category, but consume a foreign or English token adjacent to a known constituent. The low probability makes the parser avoid skipping if possible. 4 Experimental Setup To test the induction algorithm, we empirically compare the results of our proposed system to a bracketing ITG induced from the same parallel corpus, but without any prior knowledge of English. To extract a CFG over English, we use the Penn treebank 60 (Marcus et al., 1993), with relative frequencies of the productions as the rule probabilities. As translation dictionary, we use the Chinese–English Translation Lexicon(Huang and Graff, 2002). When transforming the CFG into an ITG (Section 3.1) we divide the probability mass of the CFG-rules uniformly among the ITG rules they spawn. As the small parallel data set we use the IWSLT07 Chinese–English data set (Fordyce, 2007), which contains 46,867 sentence pairs. Chinese sentence are typically written without spaces, so we use a tool (Wu, 1999) to segment it into more “word like” units. We allow the wildcard to match"
W16-1207,D08-1022,0,0.0235409,"el treebanks to train on, which cannot be expected to be available when translating to or from a low resource language. It is also possible to learn translation rules focusing on the syntax of the input language rather than output language. Huang et al. (2006) turn the tables, and learn translation rules with the input language parsed. Their approach forces the decoder to commit to a single input tree and then build possible output strings rather than building possible output trees. The model has also been generalized so that rules can be extracted from input forests rather than single trees (Mi and Huang, 2008), and the decoder has been extended to accept forests as input (Mi et al., 2008). The rule extraction process still requires a word alignment to be provided. Since our model is based on a grammar rather than parse trees, it essentially integrates out all trees as well as the alignments needed to perform the rule extraction. There is a difference in that we have an output language grammar, but we still implicitly build input language forests. Our model is related to the grammatical channel model of Wu and Wong (1998) in that the way we set up our initial grammar is similar, but where they then"
W16-1207,P08-1023,0,0.0317867,"to or from a low resource language. It is also possible to learn translation rules focusing on the syntax of the input language rather than output language. Huang et al. (2006) turn the tables, and learn translation rules with the input language parsed. Their approach forces the decoder to commit to a single input tree and then build possible output strings rather than building possible output trees. The model has also been generalized so that rules can be extracted from input forests rather than single trees (Mi and Huang, 2008), and the decoder has been extended to accept forests as input (Mi et al., 2008). The rule extraction process still requires a word alignment to be provided. Since our model is based on a grammar rather than parse trees, it essentially integrates out all trees as well as the alignments needed to perform the rule extraction. There is a difference in that we have an output language grammar, but we still implicitly build input language forests. Our model is related to the grammatical channel model of Wu and Wong (1998) in that the way we set up our initial grammar is similar, but where they then proceed to translate directly with it, induction has just started with our model"
W16-1207,P10-1017,0,0.023479,"scores to be calculated for the extracted rules. This does, however, come at the cost of not being able to optimize the model in any meaningful way. Instead, one has to resort to tuning the feature weights used by the decoder. Any mistakes made by the parser or the automatic word aligner are incorporated into the model without any recourse. In contrast, our model jointly parses the output language and aligns the sentences, within a generative model that can be optimized globally across the entire training data. It is possible to alleviate the mismatch between given alignments and parse trees. Riesa and Marcu (2010) formulate a discriminative k-best alignment model with hundreds of features that can be used to choose the best alignment that matches a given tree. The method as presented require some analysis on both input and output language for the feature model, as well as a set of hand aligned sentences for training; neither of which are readily available for low resource languages. Similarly, DeNero and Klein (2007) describe a model to tailor word alignments to existing syntactic trees; in our model, we instead consider all possible trees allowed by a CFG. Burkett et al. (2010) and Burkett and Klein ("
W16-1207,W09-3805,1,0.778881,"it relates two formal languages to each other. Inversion transductions are generated by Inversion Transduction Grammars, or ITGs (Wu, 1997), which share several traits with CFGs in that the transduction rules have single non-terminals on the left-hand side, and in that there is always a 2-normal form equivalence for every ITG. The latter is quite rare for transduction grammars, and limits the structural differences that can be generated between the languages. These limits in structural differences have been empirically shown to include most of the differences found between natural languages (Søgaard and Wu, 2009), and make efficient processing possible. Formally, an ITG is a tuple ⟨N , W0 , W1 , R, S⟩, where N is a finite nonempty set of nonterminals, W0 is a finite set of terminals in the output language L0 , W1 is a finite set of terminals in the input language L1 , R is a finite nonempty set of inversion transduction rules and S ∈ N is a designated start symbol. An inversion transduction rule is restricted to take one of the following forms: S → [A] , A → [φ+ ] , A → ⟨φ+ ⟩ where S ∈ N is the start symbol, A ∈ N is a nonterminal, and φ+ is a nonempty sequence of nonterminals and biterminals. A biter"
W16-1207,P98-2230,1,0.605963,"une 17, 2016. 2016 Association for Computational Linguistics the input language, and large amounts of resources in the output language (English). The job of a translation system is to produce fluent output that adequately represents the meaning of the input, so a translation system should be biased towards the output language. We do this by basing our ITG model on an English treebank, which allows us to (a) extract a binarized context-free grammar, CFG, and (b) estimate initial probabilities for the structural rules. The stochastic CFG can then be mirrored to form a grammatical channel model (Wu and Wong, 1998). Conventional statistical machine translation, or SMT, systems such as phrase-based SMT rely on large amounts of parallel data to collect statistics over how large chunks translate between two languages. These models are highly specific, and may have two different rules for example, for a long and complicated noun phrase with the determiner and for the very same noun phrase without it. Needless to say this kind of modeling is too wasteful to be of much use when there is very small amounts of parallel data available. Tree-based models, models that allow for chunks containing general categories"
W16-1207,P01-1067,0,0.249215,"where the rules are further restricted to only the following forms: S → A, A → [BC] , A → ⟨BC⟩, A → e/f, A → e/ϵ, A → ϵ/f where S ∈ N is the start symbol, A, B, C ∈ N are nonterminals, e ∈ W0 is an L0 token, f ∈ W1 is an L1 token, and ϵ is the empty token. A bracketing ITG, or BITG, has only one nonterminal symbol (other than the dedicated start symbol), which means that the nonterminals carry no information at all other than the fact that their yields are discrete unit. 2 Related work The use of structure as input to the training of a statistical model in machine translation was pioneered by Yamada and Knight (2001), where they extend the IBM model 1 (Brown et al., 1993) to incorporate syntactic features derived from a parse tree on the output language (the input to the noisy channel, but the output of the decoder). The generative story of the model is that an English parse tree has the children of its nodes reordered, gets the option to insert a foreign token to the left or right of any node, and finally have all the English leaf nodes translated. Reading the leaf nodes of the tree in order yields the generated foreign sentence. Being a generative model, it is straight forward to train using EM, which t"
W16-1207,P08-1012,0,0.0788354,"Missing"
W16-4507,2012.eamt-1.64,1,0.908136,"Missing"
W16-4507,W05-0909,0,0.470778,"penalty on the Chinese-English translation set. cased/uncased BLEU METEOR TER WER PER CDER 16.29/16.63 36.9/38.9 69.09/68.69 71.34/71.03 60.78/60.22 67.89/67.44 15.93/16.34 36.4/38.6 69.14/68.77 71.80/71.42 60.99/60.43 68.29/67.87 15.77/15.99 37.0/38.9 69.30/68.90 71.85/71.48 60.46/59.90 68.18/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating m"
W16-4507,2014.iwslt-evaluation.4,1,0.80785,"60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic information while training SMT systems can help to learn more semantically correct bilingual correlations for low resource languages. 54 Weight 0 0.01 0.1 0.5 0.6 0.9 3 3.1 Table 3: Tuning the error penalty on the Hausa-English translation set. cased/uncased BLEU METEOR TER WER PER CDER 16.60/17.14 44.8/47.8 70.63/69.69 73."
W16-4507,2015.mtsummit-papers.26,1,0.11633,"59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic information while training SMT systems can help to learn more semantically correct bilingual correlations for low resource languages. 54 Weight 0 0.01 0.1 0.5 0.6 0.9 3 3.1 Table 3: Tuning the error penalty on the Hausa-English translation set. cased/uncased BLEU METEOR TER WER PER CDER 16.60/17.14 44.8/47.8 70.63/69.69 73.16/72.46 58.24/56.77 69.59/68.71 16.83/17.37 43.9/46.7 71.06/70.08 73.62/72."
W16-4507,P09-1088,0,0.0245513,"ted rules on the other hand, generate transduction in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a bet"
W16-4507,J90-2002,0,0.776613,"hey emphasize issues of efficient generalization as opposed to mere memorization from big data collections. We report results and examples showing that this way for inducing ITGs gives better translation quality compared to the conventional ITG (Saers and Wu, 2009) and GIZA++ (Och and Ney, 2000) alignments. 2 2.1 Related work Alignment Word alignment is considered to be an important step in training machine translation systems, since it helps to learn the correlations between the input and the output languages. Unfortunately, conventional alignments are generally based on training IBM models (Brown et al., 1990), which are known to produce weak word alignment since they allow unstructured movement of words. Then use heuristics to combine alignments of both directions to produce the final alignment. A hidden Markov model (HMM) based alignment was proposed (Vogel et al., 1996), but similarly to IBM models, the objective function uses surface based alignment rather than a more structure based alignment. No constraints are used while training, allowing any random word-to-word permutations. Such an alignment generally hurts the translation accuracy. The traditional GIZA++ (Och and Ney, 2000) toolkit imple"
W16-4507,J93-2003,0,0.0456634,"orm A → [BC] and inverted rules use inverted brackets and take the form A → ⟨BC⟩. Straight transduction rules generate transductions with the same order in L0 and L1 , inverted rules on the other hand, generate transduction in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a g"
W16-4507,N12-1047,0,0.0504114,"Missing"
W16-4507,W07-0403,0,0.0218618,"er in L0 and L1 , inverted rules on the other hand, generate transduction in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). Th"
W16-4507,J07-2003,0,0.0868354,"th the same order in L0 and L1 , inverted rules on the other hand, generate transduction in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 201"
W16-4507,P11-1042,0,0.0212706,"they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional IBM models for both German-English and French-English. Zhang and Gildea (2005) presented a version of ITG where rule probabilities are lexicalized throughout the synchronous parse tree for efficient training which helped to align sentences up to 15 words. Some of the previous work on word alignment used morphological and syntactic features (De Gispert et al., 2006). Some loglinear models have been proposed to incorporate those features (Dyer et al., 2011). The problem with those approaches is that they require language specific knowledge and that they work better on more morphologically rich languages. Few studies that approximately integrate semantic knowledge in computing word alignment are proposed by Ma et al. (2011) and Songyot and Chiang (2014). However, the former needs to have a prior word alignment learned on lexical words. The authors in the latter model proposed a semantic oriented word alignment. However, the problem is, they need to extract word similarity from the monolingual data for both languages, which is problematic in low r"
W16-4507,J02-3001,0,0.0993818,"the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding the meaning of a given sentence than the conventional syntax-based parsing. Recent approaches in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semantic evaluation metrics that correlate more closely with human adequacy judgements than the commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Unlike n-gram or edit-distance based metrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a g"
W16-4507,P09-1104,0,0.0207323,"hand, generate transduction in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding"
W16-4507,P07-2045,0,0.00400548,"ts used for our experimental setup. We tried to vary the data size and the language family for tuning the error weight and testing our proposed model to show that our approach is not language dependent and can easily be generalized across languages. We adopted the DARPA LORELEI program approach by using a relatively small Chinese corpus, a medium Hausa corpus and a slightly larger Uzbek corpus, we show that our approach is able to learn from small to medium datasets and does not rely on heavy memorization. We tested the different alignments described above by using the standard MOSES toolkit (Koehn et al., 2007), and a 4-gram language model learned with the SRI language model toolkit (Stolcke, 2002) trained on the training data of each language respectively. To tune the loglinear mixture weights, we use k-best MIRA (Cherry and Foster, 2012), a version of margin-based classification algorithm or MIRA (Chiang, 2012). 56 4 Results We compared the performance of the semantic frame based BITG alignments against both the conventional token based BITG alignments and the traditional GIZA++ alignments. We evaluated our MT output using the surface based evaluation metrics BLEU (Papineni et al., 2002), METEOR ("
W16-4507,E06-1031,0,0.0949572,"ranslation set. cased/uncased BLEU METEOR TER WER PER CDER 16.29/16.63 36.9/38.9 69.09/68.69 71.34/71.03 60.78/60.22 67.89/67.44 15.93/16.34 36.4/38.6 69.14/68.77 71.80/71.42 60.99/60.43 68.29/67.87 15.77/15.99 37.0/38.9 69.30/68.90 71.85/71.48 60.46/59.90 68.18/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic informat"
W16-4507,P11-1023,1,0.929401,"2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding the meaning of a given sentence than the conventional syntax-based parsing. Recent approaches in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semantic evaluation met"
W16-4507,W12-4206,1,0.897757,"Missing"
W16-4507,2013.mtsummit-papers.12,1,0.921253,". Recent approaches in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semantic evaluation metrics that correlate more closely with human adequacy judgements than the commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Unlike n-gram or edit-distance based metrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a good translation is one in which humans can successfully understand the general meaning of the input sentence as captured by the basic event structure defined in (Pradhan et al., 2004). Recent works have shown that the semantic frame based metric, MEANT, correlates better with human adequacy judgment than common evaluation metrics (Lo and Wu, 2011, 2012; 53 Figure 1: Token based BITG induction algorithm. Weight 0 0.01 0.1 0."
W16-4507,W13-2254,1,0.915683,". Recent approaches in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semantic evaluation metrics that correlate more closely with human adequacy judgements than the commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Unlike n-gram or edit-distance based metrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a good translation is one in which humans can successfully understand the general meaning of the input sentence as captured by the basic event structure defined in (Pradhan et al., 2004). Recent works have shown that the semantic frame based metric, MEANT, correlates better with human adequacy judgment than common evaluation metrics (Lo and Wu, 2011, 2012; 53 Figure 1: Token based BITG induction algorithm. Weight 0 0.01 0.1 0."
W16-4507,W12-3129,1,0.891634,"Missing"
W16-4507,P13-2067,1,0.832594,"8/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic information while training SMT systems can help to learn more semantically correct bilingual correlations for low resource languages. 54 Weight 0 0.01 0.1 0.5 0.6 0.9 3 3.1 Table 3: Tuning the error penalty on the Hausa-English translation set. cased/uncased BLEU"
W16-4507,2013.iwslt-evaluation.5,1,0.846151,"8/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic information while training SMT systems can help to learn more semantically correct bilingual correlations for low resource languages. 54 Weight 0 0.01 0.1 0.5 0.6 0.9 3 3.1 Table 3: Tuning the error penalty on the Hausa-English translation set. cased/uncased BLEU"
W16-4507,2011.mtsummit-papers.41,0,0.0371905,"le probabilities are lexicalized throughout the synchronous parse tree for efficient training which helped to align sentences up to 15 words. Some of the previous work on word alignment used morphological and syntactic features (De Gispert et al., 2006). Some loglinear models have been proposed to incorporate those features (Dyer et al., 2011). The problem with those approaches is that they require language specific knowledge and that they work better on more morphologically rich languages. Few studies that approximately integrate semantic knowledge in computing word alignment are proposed by Ma et al. (2011) and Songyot and Chiang (2014). However, the former needs to have a prior word alignment learned on lexical words. The authors in the latter model proposed a semantic oriented word alignment. However, the problem is, they need to extract word similarity from the monolingual data for both languages, which is problematic in low resource conditions, then produce alignments using word similarities. 2.2 Inversion transduction grammars Inversion transduction grammars, or ITGs, (Wu, 1997) are by definition a subset of syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972"
W16-4507,W13-2202,0,0.0131989,"s in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semantic evaluation metrics that correlate more closely with human adequacy judgements than the commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Unlike n-gram or edit-distance based metrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a good translation is one in which humans can successfully understand the general meaning of the input sentence as captured by the basic event structure defined in (Pradhan et al., 2004). Recent works have shown that the semantic frame based metric, MEANT, correlates better with human adequacy judgment than common evaluation metrics (Lo and Wu, 2011, 2012; 53 Figure 1: Token based BITG induction algorithm. Weight 0 0.01 0.1 0.5 0.6 0.9 Table 2: Tuning th"
W16-4507,P11-1064,0,0.0187818,"der. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding the meaning of a given sentence than the c"
W16-4507,niessen-etal-2000-evaluation,0,0.255367,"ed BLEU METEOR TER WER PER CDER 16.29/16.63 36.9/38.9 69.09/68.69 71.34/71.03 60.78/60.22 67.89/67.44 15.93/16.34 36.4/38.6 69.14/68.77 71.80/71.42 60.99/60.43 68.29/67.87 15.77/15.99 37.0/38.9 69.30/68.90 71.85/71.48 60.46/59.90 68.18/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic information while training SMT syst"
W16-4507,P00-1056,0,0.3775,"ge amount of parallel corpora. We believe that low resource conditions are 51 Proceedings of the Sixth Workshop on Hybrid Approaches to Translation, pages 51–60, Osaka, Japan, December 11, 2016. more interesting than high resource conditions because they are both scientifically and socioeconomically more interesting as they emphasize issues of efficient generalization as opposed to mere memorization from big data collections. We report results and examples showing that this way for inducing ITGs gives better translation quality compared to the conventional ITG (Saers and Wu, 2009) and GIZA++ (Och and Ney, 2000) alignments. 2 2.1 Related work Alignment Word alignment is considered to be an important step in training machine translation systems, since it helps to learn the correlations between the input and the output languages. Unfortunately, conventional alignments are generally based on training IBM models (Brown et al., 1990), which are known to produce weak word alignment since they allow unstructured movement of words. Then use heuristics to combine alignments of both directions to produce the final alignment. A hidden Markov model (HMM) based alignment was proposed (Vogel et al., 1996), but sim"
W16-4507,J05-1004,0,0.00819179,"(SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding the meaning of a given sentence than the conventional syntax-based parsing. Recent approaches in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semantic evaluation metrics that correlate more closely with human adequacy judgements than the commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Unlike n-gram or edit-distance based metrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a good translation is one in which humans can"
W16-4507,P02-1040,0,0.0969543,". Weight 0 0.01 0.1 0.5 0.6 0.9 Table 2: Tuning the error penalty on the Chinese-English translation set. cased/uncased BLEU METEOR TER WER PER CDER 16.29/16.63 36.9/38.9 69.09/68.69 71.34/71.03 60.78/60.22 67.89/67.44 15.93/16.34 36.4/38.6 69.14/68.77 71.80/71.42 60.99/60.43 68.29/67.87 15.77/15.99 37.0/38.9 69.30/68.90 71.85/71.48 60.46/59.90 68.18/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the"
W16-4507,N04-1030,0,0.0713533,"5); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding the meaning of a given sentence than the conventional syntax-based parsing. Recent approaches in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semant"
W16-4507,W09-2304,1,0.807729,"ons without memorizing from a huge amount of parallel corpora. We believe that low resource conditions are 51 Proceedings of the Sixth Workshop on Hybrid Approaches to Translation, pages 51–60, Osaka, Japan, December 11, 2016. more interesting than high resource conditions because they are both scientifically and socioeconomically more interesting as they emphasize issues of efficient generalization as opposed to mere memorization from big data collections. We report results and examples showing that this way for inducing ITGs gives better translation quality compared to the conventional ITG (Saers and Wu, 2009) and GIZA++ (Och and Ney, 2000) alignments. 2 2.1 Related work Alignment Word alignment is considered to be an important step in training machine translation systems, since it helps to learn the correlations between the input and the output languages. Unfortunately, conventional alignments are generally based on training IBM models (Brown et al., 1990), which are known to produce weak word alignment since they allow unstructured movement of words. Then use heuristics to combine alignments of both directions to produce the final alignment. A hidden Markov model (HMM) based alignment was propose"
W16-4507,W09-3804,1,0.916681,"e based alignment rather than a more structure based alignment. No constraints are used while training, allowing any random word-to-word permutations. Such an alignment generally hurts the translation accuracy. The traditional GIZA++ (Och and Ney, 2000) toolkit implements both IBM and HMM models described above. Saers and Wu (2009) proposed a better method of producing word alignment by training inversion transduction grammars (Wu, 1997). One problem encountered with such a model was the exhaustive biparsing that runs in O(n6 ). A more efficient version that runs in O(n3 ) was proposed later (Saers et al., 2009). Zens and Ney (2003) show that ITG constraints allow a higher flexibility in word ordering for longer sentences than the conventional IBM model. Furthermore, they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional IBM models for both German-English and French-English. Zhang and Gildea (2005) presented a version of ITG where rule probabilities are lexicalized throughout the synchronous parse tree for efficient training which helped to align sentences up to 15 words. Some of the previous work"
W16-4507,N10-1050,1,0.827079,"ion in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding the meaning of a giv"
W16-4507,2006.amta-papers.25,0,0.298353,"16.29/16.63 36.9/38.9 69.09/68.69 71.34/71.03 60.78/60.22 67.89/67.44 15.93/16.34 36.4/38.6 69.14/68.77 71.80/71.42 60.99/60.43 68.29/67.87 15.77/15.99 37.0/38.9 69.30/68.90 71.85/71.48 60.46/59.90 68.18/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic information while training SMT systems can help to learn more sema"
W16-4507,D14-1197,0,0.0136424,"lexicalized throughout the synchronous parse tree for efficient training which helped to align sentences up to 15 words. Some of the previous work on word alignment used morphological and syntactic features (De Gispert et al., 2006). Some loglinear models have been proposed to incorporate those features (Dyer et al., 2011). The problem with those approaches is that they require language specific knowledge and that they work better on more morphologically rich languages. Few studies that approximately integrate semantic knowledge in computing word alignment are proposed by Ma et al. (2011) and Songyot and Chiang (2014). However, the former needs to have a prior word alignment learned on lexical words. The authors in the latter model proposed a semantic oriented word alignment. However, the problem is, they need to extract word similarity from the monolingual data for both languages, which is problematic in low resource conditions, then produce alignments using word similarities. 2.2 Inversion transduction grammars Inversion transduction grammars, or ITGs, (Wu, 1997) are by definition a subset of syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972). A transduction is a set of"
W16-4507,C96-2141,0,0.514981,"d GIZA++ (Och and Ney, 2000) alignments. 2 2.1 Related work Alignment Word alignment is considered to be an important step in training machine translation systems, since it helps to learn the correlations between the input and the output languages. Unfortunately, conventional alignments are generally based on training IBM models (Brown et al., 1990), which are known to produce weak word alignment since they allow unstructured movement of words. Then use heuristics to combine alignments of both directions to produce the final alignment. A hidden Markov model (HMM) based alignment was proposed (Vogel et al., 1996), but similarly to IBM models, the objective function uses surface based alignment rather than a more structure based alignment. No constraints are used while training, allowing any random word-to-word permutations. Such an alignment generally hurts the translation accuracy. The traditional GIZA++ (Och and Ney, 2000) toolkit implements both IBM and HMM models described above. Saers and Wu (2009) proposed a better method of producing word alignment by training inversion transduction grammars (Wu, 1997). One problem encountered with such a model was the exhaustive biparsing that runs in O(n6 )."
W16-4507,W95-0106,1,0.607333,"tion rules generate transductions with the same order in L0 and L1 , inverted rules on the other hand, generate transduction in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pr"
W16-4507,J97-3002,1,0.754102,"duce the final alignment. A hidden Markov model (HMM) based alignment was proposed (Vogel et al., 1996), but similarly to IBM models, the objective function uses surface based alignment rather than a more structure based alignment. No constraints are used while training, allowing any random word-to-word permutations. Such an alignment generally hurts the translation accuracy. The traditional GIZA++ (Och and Ney, 2000) toolkit implements both IBM and HMM models described above. Saers and Wu (2009) proposed a better method of producing word alignment by training inversion transduction grammars (Wu, 1997). One problem encountered with such a model was the exhaustive biparsing that runs in O(n6 ). A more efficient version that runs in O(n3 ) was proposed later (Saers et al., 2009). Zens and Ney (2003) show that ITG constraints allow a higher flexibility in word ordering for longer sentences than the conventional IBM model. Furthermore, they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional IBM models for both German-English and French-English. Zhang and Gildea (2005) presented a version of IT"
W16-4507,P03-1019,0,0.0446594,"her than a more structure based alignment. No constraints are used while training, allowing any random word-to-word permutations. Such an alignment generally hurts the translation accuracy. The traditional GIZA++ (Och and Ney, 2000) toolkit implements both IBM and HMM models described above. Saers and Wu (2009) proposed a better method of producing word alignment by training inversion transduction grammars (Wu, 1997). One problem encountered with such a model was the exhaustive biparsing that runs in O(n6 ). A more efficient version that runs in O(n3 ) was proposed later (Saers et al., 2009). Zens and Ney (2003) show that ITG constraints allow a higher flexibility in word ordering for longer sentences than the conventional IBM model. Furthermore, they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional IBM models for both German-English and French-English. Zhang and Gildea (2005) presented a version of ITG where rule probabilities are lexicalized throughout the synchronous parse tree for efficient training which helped to align sentences up to 15 words. Some of the previous work on word alignment use"
W16-4507,P05-1059,0,0.114782,"aining inversion transduction grammars (Wu, 1997). One problem encountered with such a model was the exhaustive biparsing that runs in O(n6 ). A more efficient version that runs in O(n3 ) was proposed later (Saers et al., 2009). Zens and Ney (2003) show that ITG constraints allow a higher flexibility in word ordering for longer sentences than the conventional IBM model. Furthermore, they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional IBM models for both German-English and French-English. Zhang and Gildea (2005) presented a version of ITG where rule probabilities are lexicalized throughout the synchronous parse tree for efficient training which helped to align sentences up to 15 words. Some of the previous work on word alignment used morphological and syntactic features (De Gispert et al., 2006). Some loglinear models have been proposed to incorporate those features (Dyer et al., 2011). The problem with those approaches is that they require language specific knowledge and that they work better on more morphologically rich languages. Few studies that approximately integrate semantic knowledge in compu"
W95-0106,J90-2002,0,0.495452,"Missing"
W95-0106,J93-2003,0,0.0161382,"Missing"
W95-0106,P93-1001,0,0.0587908,"Missing"
W95-0106,C94-1014,0,0.0245197,"Missing"
W95-0106,W93-0301,0,0.0708593,"Missing"
W95-0106,C94-2178,0,0.0237871,"Missing"
W95-0106,1994.amta-1.11,0,0.0205811,"Missing"
W95-0106,P91-1023,0,0.0569683,"Missing"
W95-0106,1992.tmi-1.9,0,0.0767366,"Missing"
W95-0106,C92-2101,0,0.0861799,"Missing"
W95-0106,P93-1004,0,0.0307124,"Missing"
W95-0106,P92-1017,0,0.0377568,"Missing"
W95-0106,C90-3101,0,0.0197924,"Missing"
W95-0106,P94-1012,1,0.902558,"Missing"
W95-0106,1995.tmi-1.28,1,0.727672,"Missing"
W95-0106,A94-1030,1,0.786754,"Missing"
W95-0106,1994.amta-1.26,1,0.832392,"Missing"
W96-0202,P81-1022,0,0.0628437,"Missing"
W96-0202,P95-1033,1,0.862008,"Missing"
W96-0202,W95-0106,1,0.869887,"Missing"
W97-0406,1995.tmi-1.13,0,0.0583723,"Missing"
W97-0406,1995.tmi-1.15,0,0.127456,"ght forward implementation of a mixed language model-based speech recognizer performs less well than the concatenation of pure language recognizers. Our experimental results also show that a common feature set, parameter set, and common algorithm lead to different performance output for Cantonese and English speech recognition modules. 1 Introduction In the past few decades, automatic speech recognition (ASR) and machine translation (MT) have both undergone rapid technical progress. Spoken language translation has emerged as a new field combining the advances in ASR and MT(Levin et al., 1995; Mayfield et al., 1995; Lavie et al., 1995; Vilar et al., 1996). Robustness is a critical issue which must be addressed for this technology to be useful in real applications. There are several robustness issues arising from the multilingual characteristics of many spoken language translation systems which have not studied by the speech recognition community since the latter tends to focus on monolingual recognition systems. One problem in a multilingual system is accent variability. It is frequently assumed that the speakers using a system are native speakers belonging to the same accent group. However, this is not"
W99-0618,P97-1003,0,0.104519,"Missing"
W99-0618,P96-1025,0,\N,Missing
W99-0630,P98-2230,1,0.802156,"n merging is obtained from 7 = 0.8 and A = 0.00001 in a gridsearch. way is that the need for an anti-lexicon is eliminated. We are also currently investigating the mapping power of such variant methods. In general, we have observed different behaviors depending on factors such as the granularity of the tagsets, the linguistic theories behind the tagsets, and the coverage of the lexicons. Finally, in addition to lexicon merging, POS mapping table is also useful in other applications. Wu and Wong apply them in their SITG channel model to give better performance in their translation application (Wu and Wong, 1998). There is a serious problem of low recall on our anti-lexicon model. This is because our model prunes out many possible POS mapping rules which results in very conservative lexeme selection during the lexicon merging process. Moreover, our model cannot discover which POS tags in original lexicon have no corresponding tag in the additional lexicon. Our model took POS mapping rules as a natural starting point since this repre5.3 D i s c u s s i o n As mentioned earlier, the mapping rule learning algorithm we used permits m - t o - n mappings so long as the mapping rules created for every tag in"
W99-0630,C98-2225,1,\N,Missing
wu-etal-2004-raising,W03-0433,1,\N,Missing
wu-etal-2004-raising,W02-2004,0,\N,Missing
wu-etal-2004-raising,W02-2010,0,\N,Missing
wu-etal-2004-raising,W02-2035,1,\N,Missing
wu-etal-2004-raising,N01-1006,1,\N,Missing
wu-etal-2004-raising,J95-4004,0,\N,Missing
wu-etal-2004-raising,P98-2188,0,\N,Missing
wu-etal-2004-raising,C98-2183,0,\N,Missing
Y12-1062,W05-0909,0,0.865368,"t of information for training the word vectors. The combined results suggest a new formulation of MEANT with significantly improved robustness across data sets. 1 Introduction We present larger-scale evidence overturning previous results, showing that the Jaccard coefficient among the alternative lexical similarity measure based on word vectors most increases the robustness of MEANT, even more than that of the Min/Max metric with mutual information metric, as used by Lo et al. (2012) in their formulation of MEANT that outperformed BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER 574 (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). MEANT, the fully-automatic, state-of-the-art semantic MT evaluation metric as introduced by Lo et al. (2012) uses the Min/Max metric with mutual information on word vectors as the similarity measure to score phrasal similarity of the semantic role fillers which is the matching criterion to align semantic frames. In achieving the same, word vectors are trained on a window size of 5 and use arithmetic mean to aggregate token similarity scores into segment similarity scores. We"
Y12-1062,P93-1022,0,0.214809,"rd in the lexicon is represented by a word vector, where each entry corresponds to the frequency of cooccurence with every other word in the lexicon. The definition of the cooccurence relation decides the nature of the context we capture and have been used in a wide variety of tasks, such as in word sense disambiguation by Gale et al. (1992) by defining the relation as the cooccurence within a distance of 50 words. Grammatical and syntactic relations were also identified, by defining the relation as the cooccurence in a relatively shorter window of 5 words, as in the work of Smadja (1993) and Dagan et al. (1993) . The word vector models can be readily trained on any large mono-lingual corpora 575 and hence their utility is not constrained to resource rich languages. In this work, we make a choice of defining the cooccurence relation as the joint cooccurence of the word within a short window of text, by the principle of occam’s razor. A window size of n symmetrically encomon both passes word tokens at a distance of upto (n−1) 2 directions and hence captures not only semantic context, but but also a mixture of grammatical and topical cooccurences. We make a choice of not using any techniques such as st"
Y12-1062,E06-1031,0,0.876312,"ts suggest a new formulation of MEANT with significantly improved robustness across data sets. 1 Introduction We present larger-scale evidence overturning previous results, showing that the Jaccard coefficient among the alternative lexical similarity measure based on word vectors most increases the robustness of MEANT, even more than that of the Min/Max metric with mutual information metric, as used by Lo et al. (2012) in their formulation of MEANT that outperformed BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER 574 (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). MEANT, the fully-automatic, state-of-the-art semantic MT evaluation metric as introduced by Lo et al. (2012) uses the Min/Max metric with mutual information on word vectors as the similarity measure to score phrasal similarity of the semantic role fillers which is the matching criterion to align semantic frames. In achieving the same, word vectors are trained on a window size of 5 and use arithmetic mean to aggregate token similarity scores into segment similarity scores. We explore the potential of alternate similarity metrics on wor"
Y12-1062,W11-1002,1,0.913882,"cs - cosine similarity, Min/Max with mutual information, Jensen Shannon divergence, Jaccard coefficient and the Dice’s coefficient on the word vector models as described above as criterion for aligning semantic frames in MEANT. We train the word vector models on the uncased Gigaword corpus. We do not use techniqes such as stemming, lemmatisation or stop-word pruning. We train the word vectors on the Gigaword corpus with window sizes ranging from 3 to 13. For our benchmark comparison, the evaluation data for our experiments is the same two sets of sentences, GALE-A and GALE-B that were used in Lo and Wu (2011), where in GALE-A is used for estimating the weight parameters of the metric by optimizing the correlation with human adequacy judgment, and then the learned weights are applied to testing on GALE-B. For the automatic semantic role labeling, we used the publicly available off-the-shelf shallow semantic parser, ASSERT (Pradhan et al., 2004). Semantic frame alignment is done by applying maximum bipartite matching algorithm with the lexical similrity of predicates as edge weights. The correlation with human adequacy judgments on sentence-level system ranking is assessed by the standard NIST Metri"
Y12-1062,W12-3129,1,0.844765,"ing style word alignments. Furthermore, we show empirically that a context window size of 5 captures the optimal amount of information for training the word vectors. The combined results suggest a new formulation of MEANT with significantly improved robustness across data sets. 1 Introduction We present larger-scale evidence overturning previous results, showing that the Jaccard coefficient among the alternative lexical similarity measure based on word vectors most increases the robustness of MEANT, even more than that of the Min/Max metric with mutual information metric, as used by Lo et al. (2012) in their formulation of MEANT that outperformed BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER 574 (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). MEANT, the fully-automatic, state-of-the-art semantic MT evaluation metric as introduced by Lo et al. (2012) uses the Min/Max metric with mutual information on word vectors as the similarity measure to score phrasal similarity of the semantic role fillers which is the matching criterion to align semantic frames. In achieving the same, word"
Y12-1062,1996.amta-1.13,0,0.291369,"is → − the number of word tokens in v . S(ui , vj )is the token − similarity score of the ith token in → u and the j th token → − in v obtained using any of the above mentioned token similarity metrics. 4.3 Σ max (c( x, wxi ), c(y, wy i )) 1 ΣΣS(ui , vj ) t×s i j Modified Competitive Linking In this method we attempt to align the tokens in the phrases using the similarity score of the token pair as a heuristic. As in the previous methods, we avoid the danger of aligning a token in one segment to excessive numbers of tokens in the other segment, by adopting a variant of competitive linking by Melamed (1996). The competitive linking algorithm adopts a greedy best first strategy in making strictly one to one word alignments. Since we frequently encounter phrases for alignment with unequal lengths, this one to one constraint severely restricts alignments and so we modify the competitive linking strategy by allowing one to many alignments. The number of such one to many alignments must be equal to the difference in the segment lengths. Once these alignments have been made, we compute the similarity of the two phrases as the arithmetic mean of the similarity scores of the aligned tokens. 5 Jaccard co"
Y12-1062,niessen-etal-2000-evaluation,0,0.870845,"n of MEANT with significantly improved robustness across data sets. 1 Introduction We present larger-scale evidence overturning previous results, showing that the Jaccard coefficient among the alternative lexical similarity measure based on word vectors most increases the robustness of MEANT, even more than that of the Min/Max metric with mutual information metric, as used by Lo et al. (2012) in their formulation of MEANT that outperformed BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER 574 (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). MEANT, the fully-automatic, state-of-the-art semantic MT evaluation metric as introduced by Lo et al. (2012) uses the Min/Max metric with mutual information on word vectors as the similarity measure to score phrasal similarity of the semantic role fillers which is the matching criterion to align semantic frames. In achieving the same, word vectors are trained on a window size of 5 and use arithmetic mean to aggregate token similarity scores into segment similarity scores. We explore the potential of alternate similarity metrics on word vectors such as the Jense"
Y12-1062,P02-1040,0,0.0940357,"hat a context window size of 5 captures the optimal amount of information for training the word vectors. The combined results suggest a new formulation of MEANT with significantly improved robustness across data sets. 1 Introduction We present larger-scale evidence overturning previous results, showing that the Jaccard coefficient among the alternative lexical similarity measure based on word vectors most increases the robustness of MEANT, even more than that of the Min/Max metric with mutual information metric, as used by Lo et al. (2012) in their formulation of MEANT that outperformed BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER 574 (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). MEANT, the fully-automatic, state-of-the-art semantic MT evaluation metric as introduced by Lo et al. (2012) uses the Min/Max metric with mutual information on word vectors as the similarity measure to score phrasal similarity of the semantic role fillers which is the matching criterion to align semantic frames. In achieving the same, word vectors are trained on a window size of 5 and use arithmetic mean to aggregat"
Y12-1062,N04-1030,0,0.379075,"mmatisation or stop-word pruning. We train the word vectors on the Gigaword corpus with window sizes ranging from 3 to 13. For our benchmark comparison, the evaluation data for our experiments is the same two sets of sentences, GALE-A and GALE-B that were used in Lo and Wu (2011), where in GALE-A is used for estimating the weight parameters of the metric by optimizing the correlation with human adequacy judgment, and then the learned weights are applied to testing on GALE-B. For the automatic semantic role labeling, we used the publicly available off-the-shelf shallow semantic parser, ASSERT (Pradhan et al., 2004). Semantic frame alignment is done by applying maximum bipartite matching algorithm with the lexical similrity of predicates as edge weights. The correlation with human adequacy judgments on sentence-level system ranking is assessed by the standard NIST MetricsMaTr procedure (Callison-Burch et al., 2010) using Kendall correlation coefficients. We first run a grid search on the GALE-A data set for each of these metrics on all window sizes to obtain weights for the role labels. We then use these weights to evaluate the GALE-C data set. The Kendall correlation score is obtained using MEANT as des"
Y12-1062,2006.amta-papers.25,0,0.485642,"mproved robustness across data sets. 1 Introduction We present larger-scale evidence overturning previous results, showing that the Jaccard coefficient among the alternative lexical similarity measure based on word vectors most increases the robustness of MEANT, even more than that of the Min/Max metric with mutual information metric, as used by Lo et al. (2012) in their formulation of MEANT that outperformed BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER 574 (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). MEANT, the fully-automatic, state-of-the-art semantic MT evaluation metric as introduced by Lo et al. (2012) uses the Min/Max metric with mutual information on word vectors as the similarity measure to score phrasal similarity of the semantic role fillers which is the matching criterion to align semantic frames. In achieving the same, word vectors are trained on a window size of 5 and use arithmetic mean to aggregate token similarity scores into segment similarity scores. We explore the potential of alternate similarity metrics on word vectors such as the Jensen Shannon divergence, the Dice’"
Y12-1062,J93-1007,0,\N,Missing
Y12-1062,W10-1703,0,\N,Missing
Y95-1025,J90-2002,0,0.249572,"Missing"
Y95-1025,J93-2003,0,0.00710001,"Missing"
Y95-1025,P94-1012,1,0.886598,"formation on the source language sentence. It is one of the advantages of the probabilistic formulation that competing translations may be compared and ranked by a principled measure, but at the same time, optimizing likelihoods over the translation space dictates heavy search costs. To make statistical architectures practical, heuristics to reduce search computation must be incorporated. An experiment applying our method to a prototype Chinese-English translation system demonstrates substantial improvement. 1 Introduction The work we discuss here is embedded within the SILC project at HKUST (Wu 1994; Fung Wu 1994; Wu & Fung 1994; Wu & Xia 1995; Wu 1995a; Wu 1995b; Wu 1995c) which focuses on problems of machine translation learning. We are developing machine learning techniques to bear upon the shortage of adequate knowledge resources for natural language analysis, particularly for Chinese where there is relatively little previous computational linguistics research from which to draw. It is one of our objectives to investigate the suitability for Chinese of the statistical translation model originally proposed by IBM (Brown et al. 1990; Brown et al. 1993) for Indo-European languages. Henc"
Y95-1025,P95-1033,1,0.842389,"f the advantages of the probabilistic formulation that competing translations may be compared and ranked by a principled measure, but at the same time, optimizing likelihoods over the translation space dictates heavy search costs. To make statistical architectures practical, heuristics to reduce search computation must be incorporated. An experiment applying our method to a prototype Chinese-English translation system demonstrates substantial improvement. 1 Introduction The work we discuss here is embedded within the SILC project at HKUST (Wu 1994; Fung Wu 1994; Wu & Fung 1994; Wu & Xia 1995; Wu 1995a; Wu 1995b; Wu 1995c) which focuses on problems of machine translation learning. We are developing machine learning techniques to bear upon the shortage of adequate knowledge resources for natural language analysis, particularly for Chinese where there is relatively little previous computational linguistics research from which to draw. It is one of our objectives to investigate the suitability for Chinese of the statistical translation model originally proposed by IBM (Brown et al. 1990; Brown et al. 1993) for Indo-European languages. Henceforth we will therefore use ""Chinese"" to refer to the"
Y95-1025,W95-0106,1,0.897078,"Missing"
Y95-1025,A94-1030,1,0.904748,"Missing"
Y95-1025,O90-1007,0,\N,Missing
