2020.emnlp-main.472,cotterell-callison-burch-2014-multi,0,0.0310173,"9.00 92.00 — — 92.50 ASTD 72.00 66.00 73.00 — — 78.50 SemEv 63.00 60.00 69.00 — — 70.50 ASC — — — — 76.67 90.86 OFF — — — 90.51 — 91.47 Table 7: Evaluation of MARBERT on external tasks. 10 Related Work Dialectal Arabic Data and Models. Much of the early work on Arabic varieties focused on collecting data for main varieties such as Egyptian and Levantine (Diab et al., 2010; Elfardy and Diab, 2012; Al-Sabbagh and Girju, 2012; Sadat et al., 2014; Zaidan and Callison-Burch, 2011). Many works developed models for detecting 2-3 dialects (Elfardy and Diab, 2013; Zaidan and CallisonBurch, 2011, 2014; Cotterell and Callison-Burch, 2014). Larger datasets, mainly based on Twitter, were recently introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018; Bouamor et al., 2019a). Our dataset is orders of magnitude larger than other datasets, F1P N was defined by SemEval-2017 as the macro F1 over the positive and negative classes only while neglecting the neutral class. 20 more balanced, and more diverse. It is also, by far, the most fine-grained. Geolocation, Variation, and MTL. Research on geolocation is also relevant, whether based on text (Roller et al., 2012; Graham et al., 2014; Han et al.,"
2020.emnlp-main.472,2020.acl-main.79,0,0.0330337,"ic research has shown how language varies across geographical regions, even for areas as small as different parts of the same city (Labov, 1964; Trudgill, 1974). These pioneering studies often used field work data from a handful of individuals and focused on small sets of carefully chosen features, often phonological. Inspired by this early work, researchers have used geographically tagged social media data from hundreds of thousands of users to predict user location (Paul and Dredze, 2011; Amitay et al., 2004; Han et al., 2014; Rahimi et al., 2017; Huang and Carley, 2019b; Tian et al., 2020; Zhong et al., 2020) or to develop language identification tools (Lui and Baldwin, 2012; Zubiaga et al., 2016; Jurgens et al., 2017a; Dunn and Adams, 2020). Whether it is possible at all to predict the micro-varieties 2 of the same general 1 Our labeled data and models will be available at: https: //github.com/UBC-NLP/microdialects. 2 We use micro-variety and micro-dialect interchangeably. language is a question that remains, to the best of our knowledge, unanswered. In this work, our goal is to investigate this specific question by introducing the novel task of Micro-Dialect Identification (MDI). Given a single"
2020.emnlp-main.472,W14-5307,0,0.0393652,"Missing"
2020.emnlp-main.472,W19-4637,1,0.856044,"Missing"
2020.eval4nlp-1.3,P17-4012,0,0.0110133,"s4 ; Cakechat5 , which is a reimplementation of the hierarchical encoderdecoder model (HRED) (Serban et al., 2016); and the Neural Conversation Model’s (NCM) released responses from Vinyals and Le (2015). Cakechat was trained on Twitter data, and NCM and OpenNMT benchmark were trained on movie subtitle data from OpenSubtitles (Tiedemann, 2012). We also evaluated two state-of-the-art Transformer base models: DialoGPT6 medium (Zhang et al., 2019) and Blender (2.7B)7 (Roller et al., 2020). Two human baselines created by Sedoc et al. (2019) were used. All other models were trained with OpenNMTpy (Klein et al., 2017) Seq2Seq implementation with its default parameters: two layers of LSTMs with 512 hidden neurons for the bidirectional encoder and the unidirectional decoder. We trained several models and chose the best using non-exhaustive human evaluation.8 OpenNMT Seq2SeqAttn is trained using OpenSubtitles (Tiedemann, 2012) and Seq2SeqAttn OpenSubtitles Questions is trained using pairs where the first utterance ends in a Experimental Details While human evaluation remains the gold standard for dialog research, the design of human evaluation experiments is far from standard. We restrict our analysis to desi"
2020.eval4nlp-1.3,2012.iwslt-papers.5,0,0.0339271,"for Computational Linguistics pairwise block comparison experiments (Bradley and Terry, 1952). Dras (2015) describes further extensions and application of the BT model to machine translation. Extended BT models can correct for dependent categorical object covariates (correlated examples) as well as subject covariates (annotator ratings) (Cattelan, 2012). As Dras (2015) points out, the BT model and IRT are similar in formulation, but IRT additionally estimates the difficulty of each item using a latent variable Bayesian model. Fixed effect BT models (Borenstein et al., 2010) or bootstrapping (Koehn, 2012) could be used to compare chatbots, but IRT’s ability to assess prompts is more attractive for this task where every annotation has a non-trivial cost. to assess chatbot model performance based on human evaluations of chatbot responses to prompts, while simultaneously assessing how informative each prompt is. IRT is a latent variable Bayesian model, with relative chatbot model quality (or student ability) being latent variables that probabilistically produce observable responses (one chatbot response to a prompt being judged as better than another, or a student answering a question correctly o"
2020.eval4nlp-1.3,W19-8609,0,0.0174977,"prompts. After each evaluation, the accuracy of all comparisons will increase. IRT can also be used to adapt evaluation sets as chatbot models improve in performance, reducing annotation costs. While our main exposition addresses single turn prompts for chatbot evaluation, our IRT model comparison method generalizes to many natural language generation tasks, including machine translation and text simplification. It also generalizes to multi-turn prompts, point-wise evaluation, pairwise conversational evaluation (e.g. AcuteEval (Li et al., 2019a)), and interactive evaluations such as those of Kulikov et al. (2019). Figure 2: Standard error of discriminative accuracy as a function of the number of prompts. We compare selecting random subset (Random) to selecting prompts (Prompt Weighted), and both prompt difficulty and model performance (Prompt and Model Weighted). Our work generalizes beyond the evaluation set from Vinyals and Le (2015). While other evaluation sets, such as random subsets of Twitter or OpenSubtitles may have fewer covariate prompts, there are many examples where further conversational context is required causing the prompts to have low discriminative power. For example, the prompt from"
2020.eval4nlp-1.3,P17-1103,0,0.07256,"t (e.g. Baheti et al. (2018)). These methods do not assess or incorporate the effectiveness of prompts (conversational chunks used for evaluation). Given that human evaluation is necessary, it is desirable to discriminate the performance of two different systems with minimal cost. Introduction One of the main problems in conversation dialog modeling is evaluation. Unlike in machine translation and task-driven dialog, automated metrics for non-task driven open-domain generative conversational models (chatbots) seem not to correlate well with human judgments (Liu et al., 2016; Tao et al., 2017; Lowe et al., 2017). While the creation of new automatic metrics is an extremely active area of research (Liu et al., 2016; Tao et al., 2017; Lowe et al., 2017; Novikova et al., 2017; Galley et al., 2015; Sugiyama et al., 2019), human annotations are currently the gold standard for assessing model improvements. Prior work mainly uses straightforward approaches, such as a two-sided ttest or binomial tests (e.g., (Serban et al., 2016; Li et al., 2015; Asghar et al., 2017; Ghazvininejad In this paper, we present the use of Item Response Theory (IRT) (Lord et al., 1968) to compare chatbot models using a head-to-head"
2020.eval4nlp-1.3,D16-1062,0,0.133961,"tive each prompt is. IRT is a latent variable Bayesian model, with relative chatbot model quality (or student ability) being latent variables that probabilistically produce observable responses (one chatbot response to a prompt being judged as better than another, or a student answering a question correctly or wrong). IRT is widely used in psychometric studies (Embretson and Reise, 2013), and for paired comparison in psychological studies (MaydeuOlivares and Brown, 2010). However, it is almost entirely ignored in natural language processing (NLP), with the exception of Hopkins and May (2013); Lalor et al. (2016); Otani et al. (2016); Lalor et al. (2019); Dras (2015). Recent work has criticized the statistical methodology used in NLP and called for use of better statistical methods (Dror et al., 2018). Here, we present IRT as a powerful method for statistical assessment of model improvements. IRT not only assesses the relative quality between two systems, but also assesses the usefulness of a prompt in comparing systems. We show that IRT can filter and choose a subset of prompts from the evaluation set efficiently, i.e. with little loss in statistical power (Figure 2), and that IRT finds different pro"
2020.eval4nlp-1.3,D17-2014,0,0.0295165,"ps://parl.ai/ 8 We experimented with whether or not to use pre-trained word embeddings, the impact of optimizer stochasticity, and various types of data preprocessing. 5 2 If the number of annotators is variable, then we scale uij to a fixed range which here we set to [−3, 3]. 3 We calculate the correlation of judgments uij between all prompts over all annotators and evaluations. 25 question mark and the second does not. Finally, Seq2SeqAttn Twitter was trained on Twitter micro-blogging data as originally done by Ritter et al. (2010).9 All of the data was extracted and tokenized using ParlAI (Miller et al., 2017).10 ever, “bad” workers will create bias in the estimate of mean difference (a.k.a. ability) of models to be closer to 0 (see the Appendix for further details). 5.2 We used IRT to compare multiple neural models for their relative strength. Furthermore, we also included human baselines in our model comparison. Finally, we assessed the discriminative quality of the hand-crafted prompts from Vinyals and Le (2015). 6 Selection of Evaluation Set Our evaluation set is the list of 200 questions released by Vinyals and Le (2015) in their seminal work on neural conversational models using a standard Se"
2020.eval4nlp-1.3,D19-1434,0,0.174811,"ble Bayesian model, with relative chatbot model quality (or student ability) being latent variables that probabilistically produce observable responses (one chatbot response to a prompt being judged as better than another, or a student answering a question correctly or wrong). IRT is widely used in psychometric studies (Embretson and Reise, 2013), and for paired comparison in psychological studies (MaydeuOlivares and Brown, 2010). However, it is almost entirely ignored in natural language processing (NLP), with the exception of Hopkins and May (2013); Lalor et al. (2016); Otani et al. (2016); Lalor et al. (2019); Dras (2015). Recent work has criticized the statistical methodology used in NLP and called for use of better statistical methods (Dror et al., 2018). Here, we present IRT as a powerful method for statistical assessment of model improvements. IRT not only assesses the relative quality between two systems, but also assesses the usefulness of a prompt in comparing systems. We show that IRT can filter and choose a subset of prompts from the evaluation set efficiently, i.e. with little loss in statistical power (Figure 2), and that IRT finds different prompts to be useful for assessing high quali"
2020.eval4nlp-1.3,D17-1238,0,0.0625756,"Missing"
2020.eval4nlp-1.3,N18-2012,0,0.139419,"Missing"
2020.eval4nlp-1.3,D16-1049,0,0.219335,"IRT is a latent variable Bayesian model, with relative chatbot model quality (or student ability) being latent variables that probabilistically produce observable responses (one chatbot response to a prompt being judged as better than another, or a student answering a question correctly or wrong). IRT is widely used in psychometric studies (Embretson and Reise, 2013), and for paired comparison in psychological studies (MaydeuOlivares and Brown, 2010). However, it is almost entirely ignored in natural language processing (NLP), with the exception of Hopkins and May (2013); Lalor et al. (2016); Otani et al. (2016); Lalor et al. (2019); Dras (2015). Recent work has criticized the statistical methodology used in NLP and called for use of better statistical methods (Dror et al., 2018). Here, we present IRT as a powerful method for statistical assessment of model improvements. IRT not only assesses the relative quality between two systems, but also assesses the usefulness of a prompt in comparing systems. We show that IRT can filter and choose a subset of prompts from the evaluation set efficiently, i.e. with little loss in statistical power (Figure 2), and that IRT finds different prompts to be useful for"
2020.eval4nlp-1.3,N10-1020,0,0.0364194,"/lukalabs/cakechat from Replika.ai. 6 https://github.com/microsoft/DialoGPT 7 https://parl.ai/ 8 We experimented with whether or not to use pre-trained word embeddings, the impact of optimizer stochasticity, and various types of data preprocessing. 5 2 If the number of annotators is variable, then we scale uij to a fixed range which here we set to [−3, 3]. 3 We calculate the correlation of judgments uij between all prompts over all annotators and evaluations. 25 question mark and the second does not. Finally, Seq2SeqAttn Twitter was trained on Twitter micro-blogging data as originally done by Ritter et al. (2010).9 All of the data was extracted and tokenized using ParlAI (Miller et al., 2017).10 ever, “bad” workers will create bias in the estimate of mean difference (a.k.a. ability) of models to be closer to 0 (see the Appendix for further details). 5.2 We used IRT to compare multiple neural models for their relative strength. Furthermore, we also included human baselines in our model comparison. Finally, we assessed the discriminative quality of the hand-crafted prompts from Vinyals and Le (2015). 6 Selection of Evaluation Set Our evaluation set is the list of 200 questions released by Vinyals and Le"
2020.eval4nlp-1.3,tiedemann-2012-parallel,0,0.00944839,"Turk crowd workers). 5.1 System Descriptions We conducted a series of experiments to establish high-quality baselines for several popular training sets to show the efficacy of our proposed method. We compared our baselines against the OpenNMT benchmark for dialog systems4 ; Cakechat5 , which is a reimplementation of the hierarchical encoderdecoder model (HRED) (Serban et al., 2016); and the Neural Conversation Model’s (NCM) released responses from Vinyals and Le (2015). Cakechat was trained on Twitter data, and NCM and OpenNMT benchmark were trained on movie subtitle data from OpenSubtitles (Tiedemann, 2012). We also evaluated two state-of-the-art Transformer base models: DialoGPT6 medium (Zhang et al., 2019) and Blender (2.7B)7 (Roller et al., 2020). Two human baselines created by Sedoc et al. (2019) were used. All other models were trained with OpenNMTpy (Klein et al., 2017) Seq2Seq implementation with its default parameters: two layers of LSTMs with 512 hidden neurons for the bidirectional encoder and the unidirectional decoder. We trained several models and chose the best using non-exhaustive human evaluation.8 OpenNMT Seq2SeqAttn is trained using OpenSubtitles (Tiedemann, 2012) and Seq2SeqAt"
2020.eval4nlp-1.3,W14-3301,0,0.320903,"Missing"
2020.eval4nlp-1.3,N19-4011,1,0.90337,"rics means that every change in model architecture requires new evaluations. Our goal is efficient and cost-effective model assessment. Ideally, chatbots would be interactively evaluated, but due to the high cost, next utterance simulation is used as a surrogate. Although next utterance generation is a more artificial task, Logacheva et al. (2018) observed a Pearson correlation of 0.6 between conversation-level and utterance-level ratings. Human judgments are often inconsistent for non-task driven chatbots, since there is no clear objective, which leads to low inter-annotator agreement (IAA) (Sedoc et al., 2019; Yuwono et al., 2019). However, Amidei et al. (2019) point out that even with low IAA we can still find statistical significance. There are further tensions between local coherence assessments using standard evaluation sets and human interactive evaluation. These issues are exacerbated for non task-driven dialog systems, as there is rarely a single “correct” response, leading to more local minima. Thus, there is a need to obtain the maximum possible 23 obtains a score above c (the “rated scale assignment”) for question j (Andrich, 1978). Pijc (θi ), the probability that student score (or aggr"
2020.eval4nlp-1.3,P15-1152,0,0.0276255,"Missing"
2020.eval4nlp-1.3,N15-1020,0,0.0511891,"Missing"
2020.findings-emnlp.137,W93-0231,0,0.427929,"ding existing survey datasets and questionnaires to new subject populations and to new theoretical constructs, greatly improving the generalizability of psychological research and opening up many practical applications for personality research. 2 2.1 Related Work Personality questionnaires One of the most widely known and researched psychological personality models is the Five Factor or “Big Five” personality model. This comprehensive model categorizes human personality traits into five bipolar categories: Openness to Experience, Conscientiousness, Extraversion, Agreeableness and Neuroticism (Goldberg, 1993). These categories are meant to describe a person’s characteristic behaviors throughout different contexts of their daily life. The NEO-PI-R is one of the most established and widely accepted BIG 5 questionnaires (Costa and McCrae, 1989; Costa Jr and McCrae, 2008). As a proxy to the NEO-PI-R, this study uses the 100 question set from the publicly available International Personality Item Pool (IPIP), which is a large collection of questions for use in psychometric testing (Goldberg et al., 2006). This set of questions has been widely used in previous research such as Kulkarni et al. (2018); Par"
2020.findings-emnlp.137,N19-1423,0,0.0408201,"Missing"
2020.findings-emnlp.137,N18-1202,0,0.0606951,"Missing"
2020.lrec-1.206,C18-1245,1,0.848329,"positivevs.-negative resources such as SentiWordNet and VADER (Baccianella et al., 2010; Hutto and Gilbert, 2014). In contrast, resources from psychologists tend to focus on valence and arousal (or other representations of affective states (Ekman, 1992)). In particular, this includes the Affective Norms for English Words (ANEW; (Bradley and Lang, 1999)) which have been adopted to many languages (Redondo et al., 2007; Montefinese et al., 2014), and their extension by Warriner et al. (2013). Such lexica have recently became popular in NLP (Wang et al., 2016; Sedoc et al., 2017b; Mohammad, 2018; Buechel and Hahn, 2018a). Lexica also exist for many other constructs, including concrete/abstractness, familiarity, imageability and humor (Brysbaert et al., 2014; Yee, 2017; Engelthaler and Hills, 2018). Yet, noticeably, an empathy lexicon is missing. Psychologists use such lexica either for content analysis, most noticeably using the Linguistic Inquiry and Word Count (LIWC) lexica (Tausczik and Pennebaker, 2010), or as controlled stimuli for experiments, e.g., on language processing and memory (Hofmann et al., 2009; Monnier and Syssau, 2008). Applications of lexica in NLP have been discussed in Section 1.. Where"
2020.lrec-1.206,N18-1173,1,0.8499,"positivevs.-negative resources such as SentiWordNet and VADER (Baccianella et al., 2010; Hutto and Gilbert, 2014). In contrast, resources from psychologists tend to focus on valence and arousal (or other representations of affective states (Ekman, 1992)). In particular, this includes the Affective Norms for English Words (ANEW; (Bradley and Lang, 1999)) which have been adopted to many languages (Redondo et al., 2007; Montefinese et al., 2014), and their extension by Warriner et al. (2013). Such lexica have recently became popular in NLP (Wang et al., 2016; Sedoc et al., 2017b; Mohammad, 2018; Buechel and Hahn, 2018a). Lexica also exist for many other constructs, including concrete/abstractness, familiarity, imageability and humor (Brysbaert et al., 2014; Yee, 2017; Engelthaler and Hills, 2018). Yet, noticeably, an empathy lexicon is missing. Psychologists use such lexica either for content analysis, most noticeably using the Linguistic Inquiry and Word Count (LIWC) lexica (Tausczik and Pennebaker, 2010), or as controlled stimuli for experiments, e.g., on language processing and memory (Hofmann et al., 2009; Monnier and Syssau, 2008). Applications of lexica in NLP have been discussed in Section 1.. Where"
2020.lrec-1.206,D18-1507,1,0.638373,"Nachmany, was responsible for conducting experiments (Section 4.) and clustering analysis (Section 5.2.). Experimental design and supervision of the implementation of the algorithms were done jointly by both first authors. †Work partially conducted as a visiting researcher at the University of Pennsylvania. rate predictions (Schwartz et al., 2013; Eichstaedt et al., 2015; Pennebaker, 2011; Liu et al., 2016). While lexica for many kinds of emotion already exist (see Section 2.1.), there is no such resource for empathy despite its growing popularity in the NLP community (Khanpour et al., 2017; Buechel et al., 2018). Hand-curated lexica for empathy are difficult to create in part because there is no clear set of words that can accurately distinguish empathy from self-focused distress. The gold standard for discerning these is an emotion rating scale by Batson et al. (1987). This scale is a collection of emotion words (e.g., compassionate, tender, warm) that could serve as a rudimentary lexicon, but it contains many words that are rarely used (e.g., “perturbed”), and many words that can take on meanings that are far from empathy (e.g, “warm”, “tender”). These word-based scales have shown good reliability"
2020.lrec-1.206,N16-3018,0,0.10665,"of sympathy and concern for unfortunate others and Personal Distress measures “self-oriented” feelings of personal anxiety and unease in tense interpersonal settings. For conciseness, we will use the terms “empathy” and “distress” to refer to this pair of constructs throughout the paper. 2.3. Empathy and Distress in AI Most previous work in language-centered AI for empathy has been conducted with a focus on speech and especially spoken dialogue. Conversational agents, psychological interventions, and call center applications have been addressed particularly often (McQuiggan and Lester, 2007; Fung et al., 2016; P´erez-Rosas et al., 2017; Alam et al., 2017). In contrast, studies addressing empathy in written language are surprisingly rare. Abdul-Mageed et al. (2017), in contrast, focus on trait empathy, a temporally more stable personal attribute. In particular, they studied the detection of “pathogenic empathy”, marked by selffocused distress, a potentially detrimental form of empathy associated with health risks, in social media language using a wide array of features, including n-grams and demographic information. Khanpour et al. (2017) present a corpus of messages from online health communities"
2020.lrec-1.206,D16-1057,0,0.0231075,"controlled stimuli for experiments, e.g., on language processing and memory (Hofmann et al., 2009; Monnier and Syssau, 2008). Applications of lexica in NLP have been discussed in Section 1.. Whereas most lexica are created manually, there is an extensive body of work on learning such ratings automatically (see Kulkarni et al. (2019) for a survey). Early work focused on deriving scores through linguistic patterns or statistical association with a small set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). More recent approaches almost always rely on word embeddings (Hamilton et al., 2016; Li et al., 2017; Buechel and Hahn, 2018b). This line of work is predominantly based on word-level supervision. In contrast, we learn word ratings from document-level ratings. 2.2. Empathy and Distress in Psychology Empathic emotions, “reactions of one individual to the observed experiences of another” (Davis, 1983), often in response to their need or suffering, is complex and controversial, with luminary scientists both arguing for the benefits of empathy (De Waal, 2009) and “against empathy” (Bloom, 2016). Empathy has been linked to a multitude of positive outcomes, from volunteering (Batso"
2020.lrec-1.206,P97-1023,0,0.296453,"t analysis, most noticeably using the Linguistic Inquiry and Word Count (LIWC) lexica (Tausczik and Pennebaker, 2010), or as controlled stimuli for experiments, e.g., on language processing and memory (Hofmann et al., 2009; Monnier and Syssau, 2008). Applications of lexica in NLP have been discussed in Section 1.. Whereas most lexica are created manually, there is an extensive body of work on learning such ratings automatically (see Kulkarni et al. (2019) for a survey). Early work focused on deriving scores through linguistic patterns or statistical association with a small set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). More recent approaches almost always rely on word embeddings (Hamilton et al., 2016; Li et al., 2017; Buechel and Hahn, 2018b). This line of work is predominantly based on word-level supervision. In contrast, we learn word ratings from document-level ratings. 2.2. Empathy and Distress in Psychology Empathic emotions, “reactions of one individual to the observed experiences of another” (Davis, 1983), often in response to their need or suffering, is complex and controversial, with luminary scientists both arguing for the benefits of empathy (De Waal, 2009) and “again"
2020.lrec-1.206,P15-1162,0,0.0754914,"Missing"
2020.lrec-1.206,I17-2042,0,0.368429,"Sedoc, along with Yoni Nachmany, was responsible for conducting experiments (Section 4.) and clustering analysis (Section 5.2.). Experimental design and supervision of the implementation of the algorithms were done jointly by both first authors. †Work partially conducted as a visiting researcher at the University of Pennsylvania. rate predictions (Schwartz et al., 2013; Eichstaedt et al., 2015; Pennebaker, 2011; Liu et al., 2016). While lexica for many kinds of emotion already exist (see Section 2.1.), there is no such resource for empathy despite its growing popularity in the NLP community (Khanpour et al., 2017; Buechel et al., 2018). Hand-curated lexica for empathy are difficult to create in part because there is no clear set of words that can accurately distinguish empathy from self-focused distress. The gold standard for discerning these is an emotion rating scale by Batson et al. (1987). This scale is a collection of emotion words (e.g., compassionate, tender, warm) that could serve as a rudimentary lexicon, but it contains many words that are rarely used (e.g., “perturbed”), and many words that can take on meanings that are far from empathy (e.g, “warm”, “tender”). These word-based scales have"
2020.lrec-1.206,C18-1187,0,0.0176278,"ce in sentiment and emotion analysis. In light of this development, lexica, lists of words and associated weights for a particular affective variable, which used to be a key component for feature extraction (Mohammad and Bravo-Marquez, 2017a), may seem obsolete. However, this is far from the truth. Lexica can be used as features to improve performance for sentence-level emotion prediction even in advanced neural architectures (Mohammad and Bravo-Marquez, 2017b; De Bruyne et al., 2019). Word ratings are also often used to refine pre-trained embedding models for specific tasks (Yu et al., 2017; Khosla et al., 2018). But much more importantly, word ratings are relatively cheap to acquire and have been found to be robust across domains and even languages, regarding their translational equivalents (Leveau et al., 2012; Warriner et al., 2013). This gives lexica a pivotal role for processing under-resourced languages. Perhaps most importantly, using lexica allows for interpretable models since the resulting document-level predictions can be easily broken down to the words within it. This gives lexica an important role for building justifiable AI and addressing related ethical challenges (Clos et al., 2017)."
2020.lrec-1.206,L18-1008,0,0.0355161,"otion according to the psychological Valence-Arousal-Dominance scheme. EmoBank contains ten thousand sentences with multiple genres and has annotations from both writer and reader emotion. Word-level supervision to test against comes from the well-known affective norms (psychological valence, arousal, and dominance) dataset collected by Warriner et al. (2013) containing 13,915 English word types. We fit all four models on EmoBank and evaluate against the word ratings by Warriner et al. (2013) using 10-fold cross-validation. For word embeddings we used off-theshelf Fasttext subword embeddings (Mikolov et al., 2018).6 The embeddings are trained with subword information on Common Crawl (600B tokens). Performance will be mea3 Another seemingly obvious evaluation strategy would be to predict document-level ratings from derived word-level lexica using the empathic reactions dataset in a cross-validation setup. However, we found that this approach has two major drawbacks. First, rather then validating the resulting word ratings directly, this strategy constitutes a down-stream (extrinsic) evaluation, similar to what we propose in Section 4.2.. Second, we found empirically that this approach lacks statistical"
2020.lrec-1.206,S17-1007,0,0.0211398,"best, and use the MLFFN to create the first-ever empathy lexicon. We then use Signed Spectral Clustering to gain insights into the resulting words. The empathy and distress lexica are publicly available at: http://www.wwbp.org/lexica.html. Keywords: lexicon creation, empathy, distress 1. Introduction Deep learning, applied to ever larger datasets, has led to large improvements in performance in sentiment and emotion analysis. In light of this development, lexica, lists of words and associated weights for a particular affective variable, which used to be a key component for feature extraction (Mohammad and Bravo-Marquez, 2017a), may seem obsolete. However, this is far from the truth. Lexica can be used as features to improve performance for sentence-level emotion prediction even in advanced neural architectures (Mohammad and Bravo-Marquez, 2017b; De Bruyne et al., 2019). Word ratings are also often used to refine pre-trained embedding models for specific tasks (Yu et al., 2017; Khosla et al., 2018). But much more importantly, word ratings are relatively cheap to acquire and have been found to be robust across domains and even languages, regarding their translational equivalents (Leveau et al., 2012; Warriner et al"
2020.lrec-1.206,W17-5205,0,0.0329528,"best, and use the MLFFN to create the first-ever empathy lexicon. We then use Signed Spectral Clustering to gain insights into the resulting words. The empathy and distress lexica are publicly available at: http://www.wwbp.org/lexica.html. Keywords: lexicon creation, empathy, distress 1. Introduction Deep learning, applied to ever larger datasets, has led to large improvements in performance in sentiment and emotion analysis. In light of this development, lexica, lists of words and associated weights for a particular affective variable, which used to be a key component for feature extraction (Mohammad and Bravo-Marquez, 2017a), may seem obsolete. However, this is far from the truth. Lexica can be used as features to improve performance for sentence-level emotion prediction even in advanced neural architectures (Mohammad and Bravo-Marquez, 2017b; De Bruyne et al., 2019). Word ratings are also often used to refine pre-trained embedding models for specific tasks (Yu et al., 2017; Khosla et al., 2018). But much more importantly, word ratings are relatively cheap to acquire and have been found to be robust across domains and even languages, regarding their translational equivalents (Leveau et al., 2012; Warriner et al"
2020.lrec-1.206,S12-1033,0,0.104019,"ctives and adjective phrases (absolutely fantastic vs. just awful) based on a corpus of online product reviews. Since the individual reviews come with a one-to-five star rating, the evaluative meaning of an adjective or phrase was computed as the average rating of all reviews it occurs in (Mean Star Rating, see Section 3.). This approach was later adopted by Ruppenhofer et al. (2014) who found that it works quite well for classifying quality and intelligence adjectives into intensity classes (excellent vs. mediocre and brilliant vs. dim, respectively). Another related approach was proposed by Mohammad (2012), who used hashtags in Twitter posts as distant supervision labels of emotion categories, e.g., #sadness. Word ratings were then computed based on pointwise mutual information between word types and emotion 1665 Step 1: Training Step 2: Inference document ratings for empathy or distress word ratings for empathy or distress yˆ yˆ 1 2 ... 128 1 2 ... Methods This section formalizes the learning problem we address, describes the three baseline methods we compare against and the Mixed-Level Feed Forward Network, and concludes with a brief discussion. Signed Spectral Clustering, which we use for qu"
2020.lrec-1.206,P18-1017,0,0.0189884,"stly focused on positivevs.-negative resources such as SentiWordNet and VADER (Baccianella et al., 2010; Hutto and Gilbert, 2014). In contrast, resources from psychologists tend to focus on valence and arousal (or other representations of affective states (Ekman, 1992)). In particular, this includes the Affective Norms for English Words (ANEW; (Bradley and Lang, 1999)) which have been adopted to many languages (Redondo et al., 2007; Montefinese et al., 2014), and their extension by Warriner et al. (2013). Such lexica have recently became popular in NLP (Wang et al., 2016; Sedoc et al., 2017b; Mohammad, 2018; Buechel and Hahn, 2018a). Lexica also exist for many other constructs, including concrete/abstractness, familiarity, imageability and humor (Brysbaert et al., 2014; Yee, 2017; Engelthaler and Hills, 2018). Yet, noticeably, an empathy lexicon is missing. Psychologists use such lexica either for content analysis, most noticeably using the Linguistic Inquiry and Word Count (LIWC) lexica (Tausczik and Pennebaker, 2010), or as controlled stimuli for experiments, e.g., on language processing and memory (Hofmann et al., 2009; Monnier and Syssau, 2008). Applications of lexica in NLP have been discus"
2020.lrec-1.206,P17-1131,0,0.242909,"Missing"
2020.lrec-1.206,W16-0404,1,0.88931,"Missing"
2020.lrec-1.206,E14-4023,0,0.0256872,"esulting lexicon was used to estimate user happiness over the course of an average 24hour day as well as a seven-day week. Rill et al. (2012) independently came up with a very similar approach for identifying the evaluative meaning of adjectives and adjective phrases (absolutely fantastic vs. just awful) based on a corpus of online product reviews. Since the individual reviews come with a one-to-five star rating, the evaluative meaning of an adjective or phrase was computed as the average rating of all reviews it occurs in (Mean Star Rating, see Section 3.). This approach was later adopted by Ruppenhofer et al. (2014) who found that it works quite well for classifying quality and intelligence adjectives into intensity classes (excellent vs. mediocre and brilliant vs. dim, respectively). Another related approach was proposed by Mohammad (2012), who used hashtags in Twitter posts as distant supervision labels of emotion categories, e.g., #sadness. Word ratings were then computed based on pointwise mutual information between word types and emotion 1665 Step 1: Training Step 2: Inference document ratings for empathy or distress word ratings for empathy or distress yˆ yˆ 1 2 ... 128 1 2 ... Methods This section"
2020.lrec-1.206,D14-1121,1,0.805267,"Missing"
2020.lrec-1.206,D17-2010,1,0.825764,"ver, note that per our problem definition, word and document labels populate the same label space. Hence, we can predict yiw by feeding vec(wi ) into the FFN without any further adjustments. Since the FFN can predict both word and document labels, we call this model Mixed-Level Feed Forward Network (MLFFN). 1 Hyperparameters and Implementation. The implementation of Mean Star Rating and Mean Binary Rating is straightforward and requires no further details. For Regression Weights, we used the same setup as Sap et al. (2014), as implemented in the Differential Language Analysis Toolkit (DLATK, (Schwartz et al., 2017)). For MLFFN, we built on the implementation2 and hyperparameter choices Buechel et al. (2018) used for the Empathic Reactions dataset. Thus, MLFFN has two hidden layers (256 and 128 units, respectively) with ReLU activation. The model was trained using the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 10−3 and a batch size of 32. We trained for a maximum of 200 epochs, and applied early stopping if the performance on the validation set did not improve for 20 consecutive epochs. We applied dropout with probabilities of .2 and .5 on input and dense layers, respectively. Moreover"
2020.lrec-1.206,P17-1087,1,0.922773,"ly work in NLP was mostly focused on positivevs.-negative resources such as SentiWordNet and VADER (Baccianella et al., 2010; Hutto and Gilbert, 2014). In contrast, resources from psychologists tend to focus on valence and arousal (or other representations of affective states (Ekman, 1992)). In particular, this includes the Affective Norms for English Words (ANEW; (Bradley and Lang, 1999)) which have been adopted to many languages (Redondo et al., 2007; Montefinese et al., 2014), and their extension by Warriner et al. (2013). Such lexica have recently became popular in NLP (Wang et al., 2016; Sedoc et al., 2017b; Mohammad, 2018; Buechel and Hahn, 2018a). Lexica also exist for many other constructs, including concrete/abstractness, familiarity, imageability and humor (Brysbaert et al., 2014; Yee, 2017; Engelthaler and Hills, 2018). Yet, noticeably, an empathy lexicon is missing. Psychologists use such lexica either for content analysis, most noticeably using the Linguistic Inquiry and Word Count (LIWC) lexica (Tausczik and Pennebaker, 2010), or as controlled stimuli for experiments, e.g., on language processing and memory (Hofmann et al., 2009; Monnier and Syssau, 2008). Applications of lexica in NLP"
2020.lrec-1.206,E17-2090,1,0.822566,"Missing"
2020.lrec-1.206,D17-1052,0,0.0194933,"ge quantities of raw text. In another line of work, (Sap et al., 2014) address the task of modeling user age and gender in social media. They showed that by training a linear model with Bag-of-Words (BoW) unigram features, the resulting feature weights can effectively be interpreted as word-level ratings. In a later study Preot¸iuc-Pietro et al. (2016) employed the same method to create a valence and arousal lexicon based on annotated Facebook posts. This is the second baseline method we used in our evaluation; Technical details are given in Section 3. (Regression Weights). In a recent study, Wang and Xia (2017) present a three step approach to infer word polarity. Based on a Twitter corpus with hashtag-derived polarity labels, they (1) apply the method of Mohammad (2012) to generate a first set of word labels (see above). Those ratings are used (2) to train sentiment-aware word embeddings. The embeddings are then used (3) as input to a classifier which is trained on a set of seed words to predict the final word ratings. In essence, this is a semi-supervised approach because the last step requires word-level gold data and does not address the problem at hand. Problem Statement. We address the problem"
2020.lrec-1.206,P16-2037,0,0.0296367,"d emo1664 tion. Early work in NLP was mostly focused on positivevs.-negative resources such as SentiWordNet and VADER (Baccianella et al., 2010; Hutto and Gilbert, 2014). In contrast, resources from psychologists tend to focus on valence and arousal (or other representations of affective states (Ekman, 1992)). In particular, this includes the Affective Norms for English Words (ANEW; (Bradley and Lang, 1999)) which have been adopted to many languages (Redondo et al., 2007; Montefinese et al., 2014), and their extension by Warriner et al. (2013). Such lexica have recently became popular in NLP (Wang et al., 2016; Sedoc et al., 2017b; Mohammad, 2018; Buechel and Hahn, 2018a). Lexica also exist for many other constructs, including concrete/abstractness, familiarity, imageability and humor (Brysbaert et al., 2014; Yee, 2017; Engelthaler and Hills, 2018). Yet, noticeably, an empathy lexicon is missing. Psychologists use such lexica either for content analysis, most noticeably using the Linguistic Inquiry and Word Count (LIWC) lexica (Tausczik and Pennebaker, 2010), or as controlled stimuli for experiments, e.g., on language processing and memory (Hofmann et al., 2009; Monnier and Syssau, 2008). Applicati"
2020.lrec-1.206,D17-1056,0,0.0601722,"Missing"
2020.peoples-1.13,P17-1067,1,0.7801,"regression, random forests)? 2. How many observations do you believe are necessary for deep learning approaches to provide a clear benefit over traditional discriminative learning? Thank you for completing the survey! Do you have any additional comments regarding this questionnaire? Figure 1: Survey on expected data requirements of deep learning. For the social media domain, this lack of gold data can be partly countered by (pre-)training with distant supervision using signals such as emojis or hashtags as a surrogate for manual annotation (Mohammad and Kiritchenko, 2015; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Yet, this procedure is less viable for target domains other than social media, as well as for predicting other subjective phenomena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et al., 2018). Very recently, con"
2020.peoples-1.13,I17-2024,0,0.0413058,"Missing"
2020.peoples-1.13,Q17-1010,0,0.0159771,"below 100 observations, the vast majority of participants (20) states that 1,000 or more instances are necessary for that. Yet, no one responded with a number between 100 and 1,000. While we do not validate the claim of this minority, the remainder of the paper provides strong evidence that the majority of the participants largely overestimated the data requirements of deep learning. 3 Data For the following study, we selected four small (< 3000 instances) and typologically diverse datasets described below. Pre-trained, publicly available word2vec (Mikolov et al., 2013) and FastText vectors (Bojanowski et al., 2017) of matching language and target domain were used as model input. Table 1 summarizes the employed data. Illustrative examples of the particular styles and annotation formats of those corpora are provided in Table 2. SE07: The test set of SemEval 2007 Task 14 (Strapparava and Mihalcea, 2007) comprises 1000 English news headlines that are annotated according to six Basic Emotions, joy, anger, sadness, fear, disgust, 131 Corpus SE07 WASSA ANPST MAS Text Val Aro Dom Joy Ang Sad Fea Dis Sur Inter Milan set Serie A win record - - - 50 2 0 0 0 9 TBS to pay $2M fine for ad campaign bomb scare - - - 11"
2020.peoples-1.13,C18-1179,0,0.015303,"; Pennington et al., 2014) or contextualized word sequences (Devlin et al., 2019; Yang et al., 2019; Joshi et al., 2020). With this profile, DL seems ill-suited for many prediction tasks in sentiment and subjectivity analysis (Balahur et al., 2014). For the widely studied problem of polarity prediction (distinguishing only between positive and negative emotion), training data is relatively abundant especially for the social media domain (Rosenthal et al., 2017). However, in recent years, there has been a growing interest in more nuanced and informative annotation formats for affective states (Bostan and Klinger, 2018; De Bruyne et al., 2019). Such annotation schemes often follow distinct psychological theories such as the dimensional approach to emotion representation (Bradley and Lang, 1994) or basic emotions (Ekman, 1992). Yet, annotating for more complex representations of affective states seems to be significantly harder in terms of both time consumption and inter-annotator agreement (IAA) (Strapparava and Mihalcea, 2007). Adding even more complexity, computational work following this trend often uses numerical scores as target variables making, emotion analysis a regression, rather than a classificat"
2020.peoples-1.13,C18-1245,1,0.905525,"Adding even more complexity, computational work following this trend often uses numerical scores as target variables making, emotion analysis a regression, rather than a classification problem (Buechel and Hahn, 2016; Mohammad et al., 2018). What makes this situation even worse is that, first, we currently have a situation where there is no community-wide consensus on how emotion should be represented. That is, different ways of annotating emotion (see, e.g., Table 2) compete with each other, leading to decreased inter-operability of language resources and provoking additional data sparsity (Buechel and Hahn, 2018b). And, second, especially large-scale annotated corpora are almost exclusively available for English, leaving most of the world’s languages with little or no gold data at all. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. * Work partially conducted at the University of Pennsylvania. License details: http:// 129 Proceedings of the Third Workshop on Computational Modeling of PEople’s Opinions, PersonaLity, and Emotions in Social media, pages 129–139 Barcelona, Spain (Online), December 13, 2020. Data Requirements for"
2020.peoples-1.13,L18-1028,1,0.930957,"Adding even more complexity, computational work following this trend often uses numerical scores as target variables making, emotion analysis a regression, rather than a classification problem (Buechel and Hahn, 2016; Mohammad et al., 2018). What makes this situation even worse is that, first, we currently have a situation where there is no community-wide consensus on how emotion should be represented. That is, different ways of annotating emotion (see, e.g., Table 2) compete with each other, leading to decreased inter-operability of language resources and provoking additional data sparsity (Buechel and Hahn, 2018b). And, second, especially large-scale annotated corpora are almost exclusively available for English, leaving most of the world’s languages with little or no gold data at all. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. * Work partially conducted at the University of Pennsylvania. License details: http:// 129 Proceedings of the Third Workshop on Computational Modeling of PEople’s Opinions, PersonaLity, and Emotions in Social media, pages 129–139 Barcelona, Spain (Online), December 13, 2020. Data Requirements for"
2020.peoples-1.13,N18-1173,1,0.921064,"Adding even more complexity, computational work following this trend often uses numerical scores as target variables making, emotion analysis a regression, rather than a classification problem (Buechel and Hahn, 2016; Mohammad et al., 2018). What makes this situation even worse is that, first, we currently have a situation where there is no community-wide consensus on how emotion should be represented. That is, different ways of annotating emotion (see, e.g., Table 2) compete with each other, leading to decreased inter-operability of language resources and provoking additional data sparsity (Buechel and Hahn, 2018b). And, second, especially large-scale annotated corpora are almost exclusively available for English, leaving most of the world’s languages with little or no gold data at all. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. * Work partially conducted at the University of Pennsylvania. License details: http:// 129 Proceedings of the Third Workshop on Computational Modeling of PEople’s Opinions, PersonaLity, and Emotions in Social media, pages 129–139 Barcelona, Spain (Online), December 13, 2020. Data Requirements for"
2020.peoples-1.13,S07-1094,0,0.121246,"Missing"
2020.peoples-1.13,W14-4012,0,0.0557733,"Missing"
2020.peoples-1.13,N19-1423,0,0.124642,"hat high-quality, pre-trained word embeddings are a main factor for achieving those results. 1 Introduction Deep Learning (DL) has radically changed the rules of the game in NLP by boosting performance figures in almost all application areas. Yet in contrast to more conventional techniques, such as n-gram based linear models, neural methodologies seem to rely on vast amounts of training data, as is obvious in areas such as machine translation (Vaswani et al., 2017) or representation learning for individual words (Mikolov et al., 2013; Pennington et al., 2014) or contextualized word sequences (Devlin et al., 2019; Yang et al., 2019; Joshi et al., 2020). With this profile, DL seems ill-suited for many prediction tasks in sentiment and subjectivity analysis (Balahur et al., 2014). For the widely studied problem of polarity prediction (distinguishing only between positive and negative emotion), training data is relatively abundant especially for the social media domain (Rosenthal et al., 2017). However, in recent years, there has been a growing interest in more nuanced and informative annotation formats for affective states (Bostan and Klinger, 2018; De Bruyne et al., 2019). Such annotation schemes often"
2020.peoples-1.13,D17-1169,0,0.0296226,"VM, penalized linear regression, random forests)? 2. How many observations do you believe are necessary for deep learning approaches to provide a clear benefit over traditional discriminative learning? Thank you for completing the survey! Do you have any additional comments regarding this questionnaire? Figure 1: Survey on expected data requirements of deep learning. For the social media domain, this lack of gold data can be partly countered by (pre-)training with distant supervision using signals such as emojis or hashtags as a surrogate for manual annotation (Mohammad and Kiritchenko, 2015; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Yet, this procedure is less viable for target domains other than social media, as well as for predicting other subjective phenomena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et"
2020.peoples-1.13,W15-4322,0,0.0480146,"Missing"
2020.peoples-1.13,L18-1550,0,0.150562,"l., 2017; Abdul-Mageed and Ungar, 2017). Yet, this procedure is less viable for target domains other than social media, as well as for predicting other subjective phenomena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et al., 2018). Very recently, contextualized word representations generated by pre-trained language models have established themselves as a powerful alternative (Peters et al., 2018; Devlin et al., 2019). In summary, deep learning supposedly depends on vast amounts of annotated data—and this seems particularly troublesome for the field of emotion analysis because such phenomena are intrinsically hard to annotate. However, we suspect that this gold data dependency may, for emotion analysis at least, be less severe than anticipated because large, pre-trained embedding models already seem to encode wordlevel"
2020.peoples-1.13,2020.tacl-1.5,0,0.0127331,"dings are a main factor for achieving those results. 1 Introduction Deep Learning (DL) has radically changed the rules of the game in NLP by boosting performance figures in almost all application areas. Yet in contrast to more conventional techniques, such as n-gram based linear models, neural methodologies seem to rely on vast amounts of training data, as is obvious in areas such as machine translation (Vaswani et al., 2017) or representation learning for individual words (Mikolov et al., 2013; Pennington et al., 2014) or contextualized word sequences (Devlin et al., 2019; Yang et al., 2019; Joshi et al., 2020). With this profile, DL seems ill-suited for many prediction tasks in sentiment and subjectivity analysis (Balahur et al., 2014). For the widely studied problem of polarity prediction (distinguishing only between positive and negative emotion), training data is relatively abundant especially for the social media domain (Rosenthal et al., 2017). However, in recent years, there has been a growing interest in more nuanced and informative annotation formats for affective states (Bostan and Klinger, 2018; De Bruyne et al., 2019). Such annotation schemes often follow distinct psychological theories"
2020.peoples-1.13,P14-1062,0,0.0144161,"models which both rely on Ridge regression, an `2 -regularized version of linear regression. The first one, Ridgengram , is based on n-gram features where we use n ∈ {1, 2, 3}. The second one, RidgeBV uses bag-of-vectors features, i.e., the pointwise mean of the embeddings of the words in a text. Regarding the deep learning approaches, we compare Feed-Forward Networks (FFN), Gated Recurrent Unit Networks (GRU), Long Short-Term Memory Networks (LSTM), Convolutional Neural Networks (CNN), as well as a combination of the latter two (CNN-LSTM) (Cho et al., 2014; Hochreiter and Schmidhuber, 1997; Kalchbrenner et al., 2014). Since holding out a dev set from the already limited training data does not seem feasible for some of the datasets (see Table 1), we decided to instead use constant hyperparameter settings across all corpora. We also keep most hyperparameters constant between models. Hence, hyperparameter choices followed well-established recommendations described in the next paragraph. The input to the DL models is based on pre-trained word vectors. ReLu activation was used everywhere except in recurrent layers. Dropout is used for regularization with a probability of .2 for embedding layers and .5 for dens"
2020.peoples-1.13,I17-2042,0,0.0239748,"e survey! Do you have any additional comments regarding this questionnaire? Figure 1: Survey on expected data requirements of deep learning. For the social media domain, this lack of gold data can be partly countered by (pre-)training with distant supervision using signals such as emojis or hashtags as a surrogate for manual annotation (Mohammad and Kiritchenko, 2015; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Yet, this procedure is less viable for target domains other than social media, as well as for predicting other subjective phenomena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et al., 2018). Very recently, contextualized word representations generated by pre-trained language models have established themselves as a powerful alternative (Peters et al., 2018; Devlin et al., 2019). In summary, deep learning sup"
2020.peoples-1.13,E17-1071,0,0.0174329,"comments regarding this questionnaire? Figure 1: Survey on expected data requirements of deep learning. For the social media domain, this lack of gold data can be partly countered by (pre-)training with distant supervision using signals such as emojis or hashtags as a surrogate for manual annotation (Mohammad and Kiritchenko, 2015; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Yet, this procedure is less viable for target domains other than social media, as well as for predicting other subjective phenomena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et al., 2018). Very recently, contextualized word representations generated by pre-trained language models have established themselves as a powerful alternative (Peters et al., 2018; Devlin et al., 2019). In summary, deep learning supposedly depends on vast amounts"
2020.peoples-1.13,S17-1007,0,0.0147001,"performance by Beck (2017); p < .001). Our GRU also outperforms IAA, as already did B ECK. This may sound improbable at first glance. However, Strapparava and Mihalcea (2007) employ a rather weak notion of human performance which is—broadly speaking—based on the reliability of a single human rater.2 Interestingly, the GRU shows particularly large improvements over human performance for categories where the IAA is low (anger, disgust, and surprise). WASSA 2017 Shared Task Data Table 7 displays the official results of the four best systems (out of 21 submissions) of the WASSA 2017 shared task (Mohammad and Bravo-Marquez, 2017b) as well as the performance our GRU achieved. For this experiment, we deviated from the above 10×10-CV set-up but instead used the official train-dev-test split for comparability. As for all experiments in this paper, hyperparameters were kept constant and were not adjusted to this dataset. Consequently, train and dev sets were combined for training. Training and testing were repeated ten times with different random seeds but otherwise identical configuration following the recommendation by Reimers and Gurevych (2018). Table 7 shows our average performance over those ten runs. As can be seen"
2020.peoples-1.13,W17-5205,0,0.0172932,"performance by Beck (2017); p < .001). Our GRU also outperforms IAA, as already did B ECK. This may sound improbable at first glance. However, Strapparava and Mihalcea (2007) employ a rather weak notion of human performance which is—broadly speaking—based on the reliability of a single human rater.2 Interestingly, the GRU shows particularly large improvements over human performance for categories where the IAA is low (anger, disgust, and surprise). WASSA 2017 Shared Task Data Table 7 displays the official results of the four best systems (out of 21 submissions) of the WASSA 2017 shared task (Mohammad and Bravo-Marquez, 2017b) as well as the performance our GRU achieved. For this experiment, we deviated from the above 10×10-CV set-up but instead used the official train-dev-test split for comparability. As for all experiments in this paper, hyperparameters were kept constant and were not adjusted to this dataset. Consequently, train and dev sets were combined for training. Training and testing were repeated ten times with different random seeds but otherwise identical configuration following the recommendation by Reimers and Gurevych (2018). Table 7 shows our average performance over those ten runs. As can be seen"
2020.peoples-1.13,S18-1001,0,0.0205739,"ation schemes often follow distinct psychological theories such as the dimensional approach to emotion representation (Bradley and Lang, 1994) or basic emotions (Ekman, 1992). Yet, annotating for more complex representations of affective states seems to be significantly harder in terms of both time consumption and inter-annotator agreement (IAA) (Strapparava and Mihalcea, 2007). Adding even more complexity, computational work following this trend often uses numerical scores as target variables making, emotion analysis a regression, rather than a classification problem (Buechel and Hahn, 2016; Mohammad et al., 2018). What makes this situation even worse is that, first, we currently have a situation where there is no community-wide consensus on how emotion should be represented. That is, different ways of annotating emotion (see, e.g., Table 2) compete with each other, leading to decreased inter-operability of language resources and provoking additional data sparsity (Buechel and Hahn, 2018b). And, second, especially large-scale annotated corpora are almost exclusively available for English, leaving most of the world’s languages with little or no gold data at all. This work is licensed under a Creative Co"
2020.peoples-1.13,D14-1162,0,0.0833341,"regression on only 100 data points. Our analysis suggests that high-quality, pre-trained word embeddings are a main factor for achieving those results. 1 Introduction Deep Learning (DL) has radically changed the rules of the game in NLP by boosting performance figures in almost all application areas. Yet in contrast to more conventional techniques, such as n-gram based linear models, neural methodologies seem to rely on vast amounts of training data, as is obvious in areas such as machine translation (Vaswani et al., 2017) or representation learning for individual words (Mikolov et al., 2013; Pennington et al., 2014) or contextualized word sequences (Devlin et al., 2019; Yang et al., 2019; Joshi et al., 2020). With this profile, DL seems ill-suited for many prediction tasks in sentiment and subjectivity analysis (Balahur et al., 2014). For the widely studied problem of polarity prediction (distinguishing only between positive and negative emotion), training data is relatively abundant especially for the social media domain (Rosenthal et al., 2017). However, in recent years, there has been a growing interest in more nuanced and informative annotation formats for affective states (Bostan and Klinger, 2018;"
2020.peoples-1.13,N18-1202,0,0.0369629,"mena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et al., 2018). Very recently, contextualized word representations generated by pre-trained language models have established themselves as a powerful alternative (Peters et al., 2018; Devlin et al., 2019). In summary, deep learning supposedly depends on vast amounts of annotated data—and this seems particularly troublesome for the field of emotion analysis because such phenomena are intrinsically hard to annotate. However, we suspect that this gold data dependency may, for emotion analysis at least, be less severe than anticipated because large, pre-trained embedding models already seem to encode wordlevel emotion quite well (Du and Zhang, 2016; Li et al., 2017; Buechel and Hahn, 2018c), possibly allowing to fit sentence-level DL architectures on rather small datasets. If"
2020.peoples-1.13,S17-2088,0,0.0140358,"training data, as is obvious in areas such as machine translation (Vaswani et al., 2017) or representation learning for individual words (Mikolov et al., 2013; Pennington et al., 2014) or contextualized word sequences (Devlin et al., 2019; Yang et al., 2019; Joshi et al., 2020). With this profile, DL seems ill-suited for many prediction tasks in sentiment and subjectivity analysis (Balahur et al., 2014). For the widely studied problem of polarity prediction (distinguishing only between positive and negative emotion), training data is relatively abundant especially for the social media domain (Rosenthal et al., 2017). However, in recent years, there has been a growing interest in more nuanced and informative annotation formats for affective states (Bostan and Klinger, 2018; De Bruyne et al., 2019). Such annotation schemes often follow distinct psychological theories such as the dimensional approach to emotion representation (Bradley and Lang, 1994) or basic emotions (Ekman, 1992). Yet, annotating for more complex representations of affective states seems to be significantly harder in terms of both time consumption and inter-annotator agreement (IAA) (Strapparava and Mihalcea, 2007). Adding even more compl"
2020.peoples-1.13,N07-2036,0,0.0363685,"ny additional comments regarding this questionnaire? Figure 1: Survey on expected data requirements of deep learning. For the social media domain, this lack of gold data can be partly countered by (pre-)training with distant supervision using signals such as emojis or hashtags as a surrogate for manual annotation (Mohammad and Kiritchenko, 2015; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Yet, this procedure is less viable for target domains other than social media, as well as for predicting other subjective phenomena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et al., 2018). Very recently, contextualized word representations generated by pre-trained language models have established themselves as a powerful alternative (Peters et al., 2018; Devlin et al., 2019). In summary, deep learning supposedly depen"
2020.peoples-1.13,S07-1013,0,0.431474,"ly for the social media domain (Rosenthal et al., 2017). However, in recent years, there has been a growing interest in more nuanced and informative annotation formats for affective states (Bostan and Klinger, 2018; De Bruyne et al., 2019). Such annotation schemes often follow distinct psychological theories such as the dimensional approach to emotion representation (Bradley and Lang, 1994) or basic emotions (Ekman, 1992). Yet, annotating for more complex representations of affective states seems to be significantly harder in terms of both time consumption and inter-annotator agreement (IAA) (Strapparava and Mihalcea, 2007). Adding even more complexity, computational work following this trend often uses numerical scores as target variables making, emotion analysis a regression, rather than a classification problem (Buechel and Hahn, 2016; Mohammad et al., 2018). What makes this situation even worse is that, first, we currently have a situation where there is no community-wide consensus on how emotion should be represented. That is, different ways of annotating emotion (see, e.g., Table 2) compete with each other, leading to decreased inter-operability of language resources and provoking additional data sparsity"
2021.naacl-main.177,E17-2068,0,0.0643433,"a post are fed to bidirectional LSTM, followed by a softmax layer for output (Yang et al., 2016c). • biLSTM-Attention: For each sentence, convolutional and max-over-time pooling layers are applied on the embeddings of its words. The resultant sentence representations are put through bi-LSTM with the attention mechanism (Yang et al., 2016c). • NeuralMT: Embeddings are fed into a bidirectional-GRU followed by a decoder with the attention mechanism (Bahdanau et al., 2015). • FastText: Word representations are averaged into a sentence representation, which is, in turn, fed to a linear classifier (Joulin et al., 2017). A softmax function is used to compute the probability distribution over the predefined classes, and a cross-entropy loss is used for tuning. Hierarchical softmax is used to speed up the training process. • Transformer: The architecture implemented was based on recent previous work (Vaswani et al., 2017). • OpenAI GPT: The Generative Pretrained Transformer implementation (Radford, 2018) with the original hyperparameter settings. Triplets In the case of CNN and BiLSTM based models, we used the referral hyper-parameters from the original implementation for all models4 . For Neural MT, FastText,"
2021.naacl-main.177,D14-1181,0,0.016991,"31 8,506 Table 2: Dataset statistics. • BERT and RoBERTa: The pre-trained BERT model (Devlin et al., 2018) and the Robustly optimized BERT model (RoBERTa) (Liu et al., 2019), where BERT is retrained with more data and an improved methodology. Models were fine-tuned using the simple transformers library. • XLNET: Finally, we evaluate the performance of XLNet (Yang et al., 2019), which combines bidirectional learning with the state-of-the-art autoregressive model such as TransformerXL. Deep learning models We also experimented with a variety of deep learning baselines: • CNN: The CNN framework (Kim, 2014) involves applying convolutional filters followed by max-over-time pooling to the word vectors for a post. • RCNN: The RCNN framework (Lai et al., 2015), recurrent convolutional layers followed by max-pooling. A fully connected layer then follows it with a softmax for output. • biLSTM: The word embeddings for all words in a post are fed to bidirectional LSTM, followed by a softmax layer for output (Yang et al., 2016c). • biLSTM-Attention: For each sentence, convolutional and max-over-time pooling layers are applied on the embeddings of its words. The resultant sentence representations are put"
2021.naacl-main.177,2021.ccl-1.108,0,0.0336492,"Missing"
2021.naacl-main.177,P16-2032,0,0.0480145,"Missing"
2021.naacl-main.177,W16-2820,0,0.0501086,"Missing"
2021.naacl-main.177,L16-1206,0,0.413733,"ity norms, is more likely to persuade the contributor to perform edits but is less likely to lead to a positive emotion. We developed baseline classifiers trained on pretrained RoBERTa features that can predict editorial change with an F1 score of .54, as compared to an F1 score of .66 for predicting emotional change. A diagnostic analysis of persisting errors is also provided. We conclude with possible applications and recommendations for future work. The dataset is publicly available for the research community at https://github.com/kj2013/WikiTalkEdit/. 1 Introduction change. Previous work (Yang et al., 2016a,b, 2017) has explored the role of editors and the types of edits made on Wikipedia, but have not related them to the ongoing conversation on the Wikipedia Talk pages. We introduce the WikiTalkEdit dataset, a novel dataset for research in online collaboration. The dataset is a subset of the Wikipedia Talk Corpus available as of May 20181 . It contains 12,882 dialogue triples with labels about editors’ subsequent editorial (editing) behavior, and 19,632 triplets with labels corresponding to editors’ emotion as manifested in their replies. Table 1 has examples from the dataset.2 This new datase"
2021.naacl-main.177,P15-1159,0,0.16695,"trategies that evoke favorable dialogic responses from those evoking behavioral compliance. Dialogue is a language game of influence, action, and reaction that progresses in a turn-taking manner. Persuasion occurs through dialogue when a listener favorably evaluates the authority, claims, 2 Related Work and evidentiality through the cues and arguments made by the speaker (Krippendorff, 1993; Schulte, Conversational quality is largely the focus of a body of work modeling the formal (Pavlick and 1980; Durik et al., 2008). Discussions on Wikipedia Talk pages can be use- Tetreault, 2016), polite (Niculae et al., 2015) and ful for determining strategies that lead to an im1 https://figshare.com/articles/Wikipedia_Talk_Corpus/4264973 2 provement of the article discussed, and for examCode to replicate the data collection is available at ining if they also lead to an amicable dialogic ex- https://github.com/kj2013/WikiTalkEdit/. 2191 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2191–2200 June 6–11, 2021. ©2021 Association for Computational Linguistics toxic (Zhang et al., 2018) features of comments on Wikipe"
2021.naacl-main.177,D17-1213,0,0.0192587,"editorial behavior. Negative responses should not be the only yardstick to measure the successful outcome of a conversation. Editorial changes occur when Editors use interpersonal language in talking about evidentiality and notability. However, these strategies are also associated with a negative emotional change. Despite the apparent negative feedback, referencing norms and sources is a successful strategy to prompt behavioral compliance. In related work, social influence 2198 dialogic exchanges. Finally, we could encode the latent space with information about the type of editorial feedback (Yang et al., 2017), which would be helpful in predicting how the OP responds. 10 Conclusion and Future Applications The WikiTalkEdit dataset offers insights that have important implications for understanding online disagreements and better supporting the Wikipedia community (Klein et al., 2019). We recommend the use of the WikiTalkEdit dataset to model the dynamics of consensus among multiple contributors. Scholars can also use the WikiTalkEdit dataset to address issues of quality, retention, and loyalty in online communities. For instance, the insights could shed light on how new OPs can be retained as sustain"
2021.naacl-main.177,Q16-1005,0,0.0643568,"Missing"
2021.naacl-main.177,P15-1012,0,0.072485,"Missing"
2021.naacl-main.177,N16-1174,0,0.0777448,"Missing"
2021.naacl-main.177,P18-1125,0,0.355202,"ior modeling tasks. In general, the dataset is important for understanding linguistic coordination, online cooperation, style matching, and teamwork in online contexts. More specifically, it offers linguistic insights about the norms on Wikipedia, such as (i) the feedback which is associated with a positive emotion vs a positive editing action, (ii) identifying and characterizing successful editorial coordination (Lerner and Lomi, 2019), (iii) generating constructive suggestions based on a given Wikipedia edit, and (iv) identifying and resolving disagreements on Wikipedia before they go awry (Zhang et al., 2018). In this study, we examine the first research problem. That is, we demonstrate how the dataset is helpful to compare and contrast the linguistic strategies that evoke favorable dialogic responses from those evoking behavioral compliance. Dialogue is a language game of influence, action, and reaction that progresses in a turn-taking manner. Persuasion occurs through dialogue when a listener favorably evaluates the authority, claims, 2 Related Work and evidentiality through the cues and arguments made by the speaker (Krippendorff, 1993; Schulte, Conversational quality is largely the focus of a"
C12-1148,S10-1013,0,0.0410594,"Missing"
C12-1148,P10-1089,0,0.0514466,"the substitutes’ probability distribution itself was the entire feature set, rather than used to supplement an existing feature set, and the resulting accuracies were lower than those we find with selectors. Our approach utilizes web-scale N-grams, a source of unlabeled data which has previously been used for many other supervised lexico-semantic tasks including delimiting named entities, preposition selection, spelling correction, search query processing, adjective ordering, verb POS disambiguation, and noun compound bracketing (Downey et al., 2007; Bergsma et al., 2009; Huang et al., 2010; Bergsma et al., 2010). All of these systems utilized n-grams to find frequency information for specific n-grams. In contrast, we use the n-grams as a source for acquiring sets of lexical data (selectors), where we search with context and ask for the missing piece rather than search for a complete n-grams. We believe this is the first work to use web-scale N-grams as a source for selectors; motivation for using this source is discussed in the next section. 3 Acquiring Selectors A selector is a word which appears in the same local context as a given instance of a focus word. For example, in the sentence below, with"
C12-1148,J92-4003,0,0.212205,"es (Yarowsky, 1995; Mihalcea and Moldovan, 1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1998; Pantel and Lin, 2002)). Distributional clusters are made up of words that appear in similar contexts to each other, whereas selectors are words which show up in the specific context of a single instance. In other words, selectors are instance-specific while distributional clusters are created based on observing many instances of context. This key difference should become more clear when we present our method of acquiring selectors. The traditional use of selectors is in knowledge-based word sense disambiguation systems, not utilizing training d"
C12-1148,D07-1108,0,0.0285815,"ta and to examine if selectors help for another lexico-semantic task: named-entity classification. Details about the OntoNotes test sets are included when discussing those results (sections 4.3.4 and 4.3.5). 4.2 Baseline Features As a consistent baseline throughout our experiments, we use the same features as Zhong et al. (2008)’s state-of-the-art system, first explored by Lee and Ng (2002). These features give the best published results that we are aware of over the Wall Street Journal portion of OntoNotes, plus they are the common denominator in many high-performance supervised WSD systems (Cai et al., 2007; Chan et al., 2007; Zhong et al., 2008). • collocations (coll). Tokens relative to the target, denoted ci, j , starting at i; ending at j. 1-grams: c−1,−1 , c+1,+1 , c−2,−2 , c+2,+2 , 2-grams: c−2,−1 , c+1,+2 , 3-grams: c−3,−1 , c+1,+3 , c−1,+1 , 4-grams: c−2,+1 , c−1,+2 • parts-of-speech (pos). The part-of-speech for the following words relative to the target word: p−3 , p−2 , p−1 , p0 , p+1 , p+2 , p+3 (0 is the target word). • surrounding words (surr). The bag-of-words from the current, previous, and next sentence. 4.3 Results 4.3.1 SemEval-2007 Table 2 shows the results with and without s"
C12-1148,S07-1054,0,0.153222,"t onto the ship this morning. beer crates luggage beer supplies wine wine Gatoraid cough_syrup medicine ... medicine ... She purchased a couple bottles of port from the store. Figure 1: Word instances which do not share context can share selectors. 2 Related Work The idea of improving a supervised classifier by utilizing unlabeled data has been investigated at different levels. For example, other approaches to disambiguation have used bootstrapped samples (Yarowsky, 1995; Mihalcea and Moldovan, 1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1998; Pantel and Lin, 2002)). Distributional clusters are made up of words that appear in similar c"
C12-1148,P08-2008,0,0.0224664,"selectors function as an abstraction of word instance context rather than as a list of semantically similar words. Our current goal is to get the most out of supervised training data by leveraging unannotated data via selectors (no use of a knowledge-base or similarity metrics). Consequently, 2425 our system achieves state-of-the-art results in line with top supervised systems while our earlier knowledge-based approaches produce results in line with systems not utilizing training data. A couple previous works have integrated unannotated data as features into supervised disambiguation systems. Dligach and Palmer (2008) used dynamic dependency neighbors, a feature encoding verbs with the same object, according to a dependency parsed corpus, as a given target verb in a verb WSD task. Besides our method not being limited to verbs, selectors are much more specific than dependency neighbors; They are found by matching a larger context and from a much larger, web-scale, dataset. Cárcamo et al. (2008) adapt the predominant sense method of McCarthy et al. (2004) to find the best sense choice for a word instance rather than it’s most common sense over a corpus. Yuret (2007) leveraged web-scale data to acquire probab"
C12-1148,N09-1037,0,0.0149837,"078(test) For the W SJ we stick with standard training and test sets, while we divide X h and S r corpora similarly. Out of the 1,000 randomly selected sentences across these corpora there are 2,106 total named entity instances: 1,847 training examples and 259 test examples. We find this to be a representative sample of the W SJ, X h, and S r portions of OntoNotes 5 . We choose our features by looking at the most common types of features used during the CoNLL-2003 Shared Task in Named Entity Recognition(Tjong Kim Sang and De Meulder, 2003), and more recent developments(Ratinov and Roth, 2009; Finkel and Manning, 2009). To the best of our knowledge state-of-the-art features have not been established for labeling all classes of Named Entities in OntoNotes, though Finkel and Manning use the three most common classes and group the others into a misc category. • character n-grams. Character sequences of length 1 to 6. • case information. Case of the first, second, and last letter, as well as an indicator for punctuation. • lexical information. The target word, its base form, as well as the same collocations used in WSD: c−1,−1 , c+1,+1 , c−2,−2 , c+2,+2 , c−2,−1 , c+1,+2 , c−3,−1 , c+1,+3 , c−1,+1 , c−2,+1 , co"
C12-1148,W02-1006,0,0.0405918,"adjectives, fine-grained senses, and difference in corpus gives us a more robust evaluation of selectors. Lastly, we experiment with random samples over portions of the full Ontonotes 4.0 in order to test on out-of-domain data and to examine if selectors help for another lexico-semantic task: named-entity classification. Details about the OntoNotes test sets are included when discussing those results (sections 4.3.4 and 4.3.5). 4.2 Baseline Features As a consistent baseline throughout our experiments, we use the same features as Zhong et al. (2008)’s state-of-the-art system, first explored by Lee and Ng (2002). These features give the best published results that we are aware of over the Wall Street Journal portion of OntoNotes, plus they are the common denominator in many high-performance supervised WSD systems (Cai et al., 2007; Chan et al., 2007; Zhong et al., 2008). • collocations (coll). Tokens relative to the target, denoted ci, j , starting at i; ending at j. 1-grams: c−1,−1 , c+1,+1 , c−2,−2 , c+2,+2 , 2-grams: c−2,−1 , c+1,+2 , 3-grams: c−3,−1 , c+1,+3 , c−1,+1 , 4-grams: c−2,+1 , c−1,+2 • parts-of-speech (pos). The part-of-speech for the following words relative to the target word: p−3 , p"
C12-1148,P97-1009,0,0.619247,"3), it is difficult to connect any two instances based on local context; the parts-of-speech even differ substantially. Models for disambiguation can benefit from the addition of a feature that does not rely directly on the local context. We present a new class of features which encodes an abstraction of a word’s context, rather than encoding contents of the local context itself. We refer to this new feature class as selectors, borrowing the term from an approach to knowledge-based (unsupervised) word sense disambiguation which uses the idea of searching for words that share the same context (Lin, 1997; Schwartz and Gomez, 2008). More precisely, selectors are words that show up in the same local context as a given instance of another word. For example, selectors for ‘port’ in sentence 1 might be ‘bottles’, ‘crates’, ‘passengers’, ‘wine’, ‘luggage’, etc. Considering that the other sentences may share some of the same selectors such as ‘bottles’ or ‘wine’, one can see how this abstraction of context to selectors can be beneficial. Figure 1 demonstrates mapping the context from one instance to selectors, which match the selectors of another instance. In this sense, it is the contexts (or word"
C12-1148,P98-2127,0,0.160335,"1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1998; Pantel and Lin, 2002)). Distributional clusters are made up of words that appear in similar contexts to each other, whereas selectors are words which show up in the specific context of a single instance. In other words, selectors are instance-specific while distributional clusters are created based on observing many instances of context. This key difference should become more clear when we present our method of acquiring selectors. The traditional use of selectors is in knowledge-based word sense disambiguation systems, not utilizing training data. In Lin (1997), dependency re"
C12-1148,lin-etal-2010-new,0,0.0254235,"loaded the port onto the ship last night. More formally, for a given word instance, w i , selectors are found based on the particular context of w i . What defines the context may vary from syntactic or dependency relations (i.e., other nouns which are objects of the verb ‘loaded’) to simple sequences of tokens (e.g.,finding words that fill in the blank in “The workers loaded the ___ onto the ship last night.”). 3.1 Approach We find selectors by searching for sequences of tokens in the Google N-grams version 2, which contains 4.1 billion n-grams that were automatically part-of-speech tagged (Lin et al., 2010). The primary reason we chose web-scale N-grams as a source is because it has become difficult to get selectors via search engines.1 Still, using web-scale n-grams for context searches has advantages: there is a decent likelihood of finding selectors for a given instance, the search 1 The Web search engines which support wildcard queries no longer run public APIs or allow scripted access. 2426 Workers loaded the port onto the ship last night. workers loaded 〈det〉? (〈noun〉+) onto loaded 〈det〉? (〈noun〉+) onto (〈noun〉+) onto 〈d et〉 ship last My objective was to fight as a mother for what I hold d"
C12-1148,P04-1036,0,0.042276,"e with systems not utilizing training data. A couple previous works have integrated unannotated data as features into supervised disambiguation systems. Dligach and Palmer (2008) used dynamic dependency neighbors, a feature encoding verbs with the same object, according to a dependency parsed corpus, as a given target verb in a verb WSD task. Besides our method not being limited to verbs, selectors are much more specific than dependency neighbors; They are found by matching a larger context and from a much larger, web-scale, dataset. Cárcamo et al. (2008) adapt the predominant sense method of McCarthy et al. (2004) to find the best sense choice for a word instance rather than it’s most common sense over a corpus. Yuret (2007) leveraged web-scale data to acquire probability distributions of substitutes being within the same context as target instances. Unlike selectors which are open-ended, substitutes were chosen from an a priori word list derived from thesauri, and contextual part-of-speech was not considered. Additionally, the substitutes’ probability distribution itself was the entire feature set, rather than used to supplement an existing feature set, and the resulting accuracies were lower than tho"
C12-1148,W04-2405,0,0.0294378,"ing actual instances from our experimental corpus (Section 5). 2424 The workers loaded the port onto the ship this morning. beer crates luggage beer supplies wine wine Gatoraid cough_syrup medicine ... medicine ... She purchased a couple bottles of port from the store. Figure 1: Word instances which do not share context can share selectors. 2 Related Work The idea of improving a supervised classifier by utilizing unlabeled data has been investigated at different levels. For example, other approaches to disambiguation have used bootstrapped samples (Yarowsky, 1995; Mihalcea and Moldovan, 1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1"
C12-1148,N07-1025,0,0.0296414,"on 5). 2424 The workers loaded the port onto the ship this morning. beer crates luggage beer supplies wine wine Gatoraid cough_syrup medicine ... medicine ... She purchased a couple bottles of port from the store. Figure 1: Word instances which do not share context can share selectors. 2 Related Work The idea of improving a supervised classifier by utilizing unlabeled data has been investigated at different levels. For example, other approaches to disambiguation have used bootstrapped samples (Yarowsky, 1995; Mihalcea and Moldovan, 1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1998; Pantel and Lin, 2002)). Distributional clusters are"
C12-1148,W04-0807,0,0.27123,"data. Because selectors leverage unlabeled data, their inclusion in a supervised system constitutes semi-supervised learning. The paper proceeds with a discussion of related work in semi-supervised WSD and the use of web-scale data in language processing (Section 2). Then, we present our approach to acquiring selectors as features from n-grams, and show how we translate selectors into features (Section 3). The effectiveness of selectors is evaluated within supervised word sense disambiguation classifiers over the SemEval-2007 Task 17 (Pradhan et al., 2007), Senseval 3 English Lexical Sample (Mihalcea et al., 2004), and OntoNotes 4 (Weischedel et al., 2011) (Section 4). We also test selectors as features for the classification step of named-entity recognition over a representative sample of OntoNotes. Lastly, we discuss the robustness of selectors as features by inspecting actual instances from our experimental corpus (Section 5). 2424 The workers loaded the port onto the ship this morning. beer crates luggage beer supplies wine wine Gatoraid cough_syrup medicine ... medicine ... She purchased a couple bottles of port from the store. Figure 1: Word instances which do not share context can share selector"
C12-1148,S07-1006,0,0.0322964,"tion of selectors and we record a simple accuracy of |al l_inst ances |∗ 100 of the testing data.3 In particular, we use support vector classifiers implemented with Scikit-learn (Pedregosa et al., 2011) with a radial basis kernel and other parameters set via 5-fold crossvalidation over the training set. As a standard point of comparison, most frequent sense (M F S) accuracy is also reported, indicating the testing accuracy if the system always predicted the most common sense according to the training data. As often noted, state-of-the-art supervised systems often perform just above the M F S (Navigli et al., 2007; Pradhan et al., 2007). 2 An implementation of this method is included in supplementary data. accur ac y = pr ecision = r ecal l under the standard (SemEval) definition of precision and recall for W SD, and because we attempt all instances of our samples. 3 2428 4.1 Data Sets We test selectors over three sense-annotated corpora. For our primary corpus, we use the SemEval-2007 Task 17: Lexical Sample (Pradhan et al., 2007) (results in sections 4.3.1 and 4.3.3). This corpus is an early selection from the Wall Street Journal portion of OntoNotes (Weischedel et al., 2011), and contains coarse-gra"
C12-1148,P93-1024,0,0.531211,"Mihalcea and Moldovan, 1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1998; Pantel and Lin, 2002)). Distributional clusters are made up of words that appear in similar contexts to each other, whereas selectors are words which show up in the specific context of a single instance. In other words, selectors are instance-specific while distributional clusters are created based on observing many instances of context. This key difference should become more clear when we present our method of acquiring selectors. The traditional use of selectors is in knowledge-based word sense disambiguation systems, not utilizing training data. In Lin (1997), de"
C12-1148,S07-1016,0,0.236361,"tch against an orders-of-magnitude-larger unlabeled set of data. Because selectors leverage unlabeled data, their inclusion in a supervised system constitutes semi-supervised learning. The paper proceeds with a discussion of related work in semi-supervised WSD and the use of web-scale data in language processing (Section 2). Then, we present our approach to acquiring selectors as features from n-grams, and show how we translate selectors into features (Section 3). The effectiveness of selectors is evaluated within supervised word sense disambiguation classifiers over the SemEval-2007 Task 17 (Pradhan et al., 2007), Senseval 3 English Lexical Sample (Mihalcea et al., 2004), and OntoNotes 4 (Weischedel et al., 2011) (Section 4). We also test selectors as features for the classification step of named-entity recognition over a representative sample of OntoNotes. Lastly, we discuss the robustness of selectors as features by inspecting actual instances from our experimental corpus (Section 5). 2424 The workers loaded the port onto the ship this morning. beer crates luggage beer supplies wine wine Gatoraid cough_syrup medicine ... medicine ... She purchased a couple bottles of port from the store. Figure 1: W"
C12-1148,W09-1119,0,0.0514657,"rain); sections 1060 - 1078(test) For the W SJ we stick with standard training and test sets, while we divide X h and S r corpora similarly. Out of the 1,000 randomly selected sentences across these corpora there are 2,106 total named entity instances: 1,847 training examples and 259 test examples. We find this to be a representative sample of the W SJ, X h, and S r portions of OntoNotes 5 . We choose our features by looking at the most common types of features used during the CoNLL-2003 Shared Task in Named Entity Recognition(Tjong Kim Sang and De Meulder, 2003), and more recent developments(Ratinov and Roth, 2009; Finkel and Manning, 2009). To the best of our knowledge state-of-the-art features have not been established for labeling all classes of Named Entities in OntoNotes, though Finkel and Manning use the three most common classes and group the others into a misc category. • character n-grams. Character sequences of length 1 to 6. • case information. Case of the first, second, and last letter, as well as an indicator for punctuation. • lexical information. The target word, its base form, as well as the same collocations used in WSD: c−1,−1 , c+1,+1 , c−2,−2 , c+2,+2 , c−2,−1 , c+1,+2 , c−3,−1 , c+"
C12-1148,W97-0209,0,0.166468,". This key difference should become more clear when we present our method of acquiring selectors. The traditional use of selectors is in knowledge-based word sense disambiguation systems, not utilizing training data. In Lin (1997), dependency relationships over a small corpus were used to find noun selectors. We previously extended this to the Web, treating context as surrounding text and introduced the ideas of acquiring selectors for additional parts-of-speech as well as for words in context in addition to the target word (Schwartz and Gomez, 2008, 2009). Similar to selectional preferences (Resnik, 1997), selectors essentially indicate the types of concepts expected in a given syntactic or grammatical position. In these knowledge-based approaches, disambiguation is performed by computing the semantic distance between selectors and senses of the target word. These approaches rely on both a knowledge source such as WordNet (Miller et al., 1993) and a semantic distance metric. In contrast, in the current approach we do not need such a knowledge source or similarity judgments, and since our approach is data-driven, selectors function as an abstraction of word instance context rather than as a lis"
C12-1148,J98-1004,0,0.297808,"lcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1998; Pantel and Lin, 2002)). Distributional clusters are made up of words that appear in similar contexts to each other, whereas selectors are words which show up in the specific context of a single instance. In other words, selectors are instance-specific while distributional clusters are created based on observing many instances of context. This key difference should become more clear when we present our method of acquiring selectors. The traditional use of selectors is in knowledge-based word sense disambiguation systems, not utilizing training data. In Lin (1997), dependency relationships ove"
C12-1148,W08-2114,1,0.92865,"ifficult to connect any two instances based on local context; the parts-of-speech even differ substantially. Models for disambiguation can benefit from the addition of a feature that does not rely directly on the local context. We present a new class of features which encodes an abstraction of a word’s context, rather than encoding contents of the local context itself. We refer to this new feature class as selectors, borrowing the term from an approach to knowledge-based (unsupervised) word sense disambiguation which uses the idea of searching for words that share the same context (Lin, 1997; Schwartz and Gomez, 2008). More precisely, selectors are words that show up in the same local context as a given instance of another word. For example, selectors for ‘port’ in sentence 1 might be ‘bottles’, ‘crates’, ‘passengers’, ‘wine’, ‘luggage’, etc. Considering that the other sentences may share some of the same selectors such as ‘bottles’ or ‘wine’, one can see how this abstraction of context to selectors can be beneficial. Figure 1 demonstrates mapping the context from one instance to selectors, which match the selectors of another instance. In this sense, it is the contexts (or word instances) that have select"
C12-1148,W09-2405,1,0.894124,"Missing"
C12-1148,W03-0419,0,0.187378,"Missing"
C12-1148,P95-1026,0,0.398462,"obustness of selectors as features by inspecting actual instances from our experimental corpus (Section 5). 2424 The workers loaded the port onto the ship this morning. beer crates luggage beer supplies wine wine Gatoraid cough_syrup medicine ... medicine ... She purchased a couple bottles of port from the store. Figure 1: Word instances which do not share context can share selectors. 2 Related Work The idea of improving a supervised classifier by utilizing unlabeled data has been investigated at different levels. For example, other approaches to disambiguation have used bootstrapped samples (Yarowsky, 1995; Mihalcea and Moldovan, 1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992"
C12-1148,S07-1044,0,0.0255207,"vised disambiguation systems. Dligach and Palmer (2008) used dynamic dependency neighbors, a feature encoding verbs with the same object, according to a dependency parsed corpus, as a given target verb in a verb WSD task. Besides our method not being limited to verbs, selectors are much more specific than dependency neighbors; They are found by matching a larger context and from a much larger, web-scale, dataset. Cárcamo et al. (2008) adapt the predominant sense method of McCarthy et al. (2004) to find the best sense choice for a word instance rather than it’s most common sense over a corpus. Yuret (2007) leveraged web-scale data to acquire probability distributions of substitutes being within the same context as target instances. Unlike selectors which are open-ended, substitutes were chosen from an a priori word list derived from thesauri, and contextual part-of-speech was not considered. Additionally, the substitutes’ probability distribution itself was the entire feature set, rather than used to supplement an existing feature set, and the resulting accuracies were lower than those we find with selectors. Our approach utilizes web-scale N-grams, a source of unlabeled data which has previous"
C12-1148,D08-1105,0,0.342076,"cluding selectors for the classification step of named-entity recognition over a representative sample of OntoNotes. These significant improvements come free of any human annotation cost, only requiring unlabeled Web-Scale corpora. KEYWORDS: word sense disambiguation, lexical semantics, semi-supervised learning. Proceedings of COLING 2012: Technical Papers, pages 2423–2440, COLING 2012, Mumbai, December 2012. 2423 1 Introduction Supervised word sense disambiguation (WSD) systems often rely directly on the local contexts in which target words appear. For example, the state-of-the-art system of Zhong et al. (2008) uses features based on collocations centered on the target word. Models relying on such features do well with copious amounts of training data, but they are prone to errors when the local context of a test instance differs from local context observed during training. Consider the sentences below. 1. The workers loaded the port onto the ship this morning. 2. She purchased a couple of bottles of port from the store. 3. The couple enjoyed their richly-flavored port. Though referring to the same sense of ‘port’, “a sweet dark-red dessert wine” (Miller et al., 1993), it is difficult to connect any"
C12-1148,S07-1053,0,\N,Missing
C12-1148,C98-2122,0,\N,Missing
C12-2053,W10-0731,0,0.299246,"what is driving its difficulty? This study examines human WSD performance and tries to identify drivers of accuracy. We hope that our findings can be incorporated into future WSD systems. To examine human WSD performance, we tap pools of anonymous untrained human labor; this is known as “crowdsourcing.” A thriving pool of crowdsourced labor is Amazon’s Mechanical Turk (MTurk), an Internet-based microtask marketplace where the workers (called “Turkers”) do simple, one-off tasks (called “human intelligence tasks” or “HITs”), for small payments. See Snow et al. (2008); Callison-Burch (2010); and Akkaya et al. (2010) for MTurk’s use in NLP, and Chandler and Kapelner (2010) and Mason and Suri (2011) for further reading on MTurk as a research platform. We performed the first extensive look at coarse-grained WSD on MTurk. We studied a large and variegated set of words: 1,000 contextual examples of 89 distinct words annotated by 10 unique Turkers each. In the closest related literature, Snow et al. (2008) found high Turker annotation accuracy but only annotated a single word, while Passonneau et al. (2011) focused on only a few words and annotated fine-grained senses. The extensive size of our study lends its"
C12-2053,brown-etal-2010-number,0,0.0161488,"uality results (Snow et al., 2008; Akkaya et al., 2010), and that workers do not improve with experience (Akkaya et al., 2010). Third, we present a system of crowdsourcing WSD boasting a throughput of about 5,000 disambiguations per day at $0.011 per annotation. 2 Methods and data collection We selected a subset of the OntoNotes data (Hovy et al., 2006), the SemEval-2007 coarse-grained English Lexical Sample WSD task training data (Pradhan et al., 2007). The coarse-grained senses in OntoNotes address a concern that nuanced differences in sense inventories drives disagreement among annotators (Brown et al., 2010). We picked 1,000 contextual examples at random from the full set of 22,281.1 Our sample is detailed in table 1. It consisted of 590 nouns and 410 verb examples that had between 2-15 senses each (nouns: 5.7 ± 3.0 senses, verbs: 4.7 ± 3.3 senses). For each snippet, ten annotations were completed by ten unique Turkers. 1 We later disqualified 9 of the 1,000 because they had words with only one sense. 540 target word affect-v allow-v announce-v approve-v area-n ask-v attempt-v authority-n avoid-v base-n begin-v believe-v bill-n build-v buy-v capital-n care-v carrier-n chance-n claim-v come-v comp"
C12-2053,W10-0701,0,0.0129617,". Why is WSD difficult and what is driving its difficulty? This study examines human WSD performance and tries to identify drivers of accuracy. We hope that our findings can be incorporated into future WSD systems. To examine human WSD performance, we tap pools of anonymous untrained human labor; this is known as “crowdsourcing.” A thriving pool of crowdsourced labor is Amazon’s Mechanical Turk (MTurk), an Internet-based microtask marketplace where the workers (called “Turkers”) do simple, one-off tasks (called “human intelligence tasks” or “HITs”), for small payments. See Snow et al. (2008); Callison-Burch (2010); and Akkaya et al. (2010) for MTurk’s use in NLP, and Chandler and Kapelner (2010) and Mason and Suri (2011) for further reading on MTurk as a research platform. We performed the first extensive look at coarse-grained WSD on MTurk. We studied a large and variegated set of words: 1,000 contextual examples of 89 distinct words annotated by 10 unique Turkers each. In the closest related literature, Snow et al. (2008) found high Turker annotation accuracy but only annotated a single word, while Passonneau et al. (2011) focused on only a few words and annotated fine-grained senses. The extensive s"
C12-2053,W97-0206,0,0.0424948,"the task was found readily on the homepage which drove the rapid completion. 3 As Kapelner and Chandler (2010) found, this accomplishes three things: (1) Turkers who plan on cheating will be more likely to leave our task, (2) Turkers will spend more time on the task and, most importantly, (3) Turkers will more carefully read and concentrate on the meaning of the text. 541 Figure 1: An example of the WSD task that appears inside an MTurk HIT. This was displayed piecewise as each word in the example (""snippet"") and senses faded-in slowly. the senses in descending frequency order as observed by Fellbaum et al. (1997). We also limited participation to US Turkers to encourage fluency in English. Upon completion, the Turker was given an option to do another of our WSD tasks (this is the MTurk default). A Turker was not limited in the number of annotations they could do.4 The entire study took 51 hours and cost $110. The code, raw data, and analysis scripts are available under GPL2 at github.com/kapelner/wordturk_pilot. 3 Results and data analysis We were interested in investigating (1) which features in the target word, the context, and sense definition text affect Turker accuracy, (2) which characteristics"
C12-2053,N06-2015,0,0.0415799,"rating that Turkers are respectably accurate (Snow et al., 2008), they’re approximately equal in ability (Parent, 2010; Passonneau et al., 2011), spam is virtually non-existent (Akkaya et al., 2010), responses from multiple Turkers can be pooled to achieve high quality results (Snow et al., 2008; Akkaya et al., 2010), and that workers do not improve with experience (Akkaya et al., 2010). Third, we present a system of crowdsourcing WSD boasting a throughput of about 5,000 disambiguations per day at $0.011 per annotation. 2 Methods and data collection We selected a subset of the OntoNotes data (Hovy et al., 2006), the SemEval-2007 coarse-grained English Lexical Sample WSD task training data (Pradhan et al., 2007). The coarse-grained senses in OntoNotes address a concern that nuanced differences in sense inventories drives disagreement among annotators (Brown et al., 2010). We picked 1,000 contextual examples at random from the full set of 22,281.1 Our sample is detailed in table 1. It consisted of 590 nouns and 410 verb examples that had between 2-15 senses each (nouns: 5.7 ± 3.0 senses, verbs: 4.7 ± 3.3 senses). For each snippet, ten annotations were completed by ten unique Turkers. 1 We later disqua"
C12-2053,J98-1001,0,0.0248244,"; and (c) spending more time is associated with a decrease in accuracy. We also observe that all participants are about equal in ability, practice (without feedback) does not seem to lead to improvement, and that having many participants label the same example provides a partial substitute for more expensive annotation. KEYWORDS: Word sense disambiguation, crowdsourcing. Proceedings of COLING 2012: Posters, pages 539–548, COLING 2012, Mumbai, December 2012. 539 1 Introduction Word sense disambiguation (WSD) is the process of identifying the meaning, or “sense,” of a word in a written context (Ide and Véronis, 1998). In his comprehensive survey, Navigli (2009) considers WSD an AI-complete problem — a task which is at least as hard as the most difficult problems in artificial intelligence. Why is WSD difficult and what is driving its difficulty? This study examines human WSD performance and tries to identify drivers of accuracy. We hope that our findings can be incorporated into future WSD systems. To examine human WSD performance, we tap pools of anonymous untrained human labor; this is known as “crowdsourcing.” A thriving pool of crowdsourced labor is Amazon’s Mechanical Turk (MTurk), an Internet-based"
C12-2053,W10-0703,0,0.0190085,"factors affecting annotator accuracy. Our contribution is three-fold. First, we use regression to identify a variety of factors that drive accuracy such as (a) the number of rephrasings within a sense definition is associated with higher accuracy; (b) as word frequency increases, accuracy decreases even if the number of senses is kept constant; and (c) time-spent on an annotation is associated with lower accuracy. Second, we echo previous findings, mostly from non-WSD experiments, demonstrating that Turkers are respectably accurate (Snow et al., 2008), they’re approximately equal in ability (Parent, 2010; Passonneau et al., 2011), spam is virtually non-existent (Akkaya et al., 2010), responses from multiple Turkers can be pooled to achieve high quality results (Snow et al., 2008; Akkaya et al., 2010), and that workers do not improve with experience (Akkaya et al., 2010). Third, we present a system of crowdsourcing WSD boasting a throughput of about 5,000 disambiguations per day at $0.011 per annotation. 2 Methods and data collection We selected a subset of the OntoNotes data (Hovy et al., 2006), the SemEval-2007 coarse-grained English Lexical Sample WSD task training data (Pradhan et al., 200"
C12-2053,S07-1016,0,0.0260694,"ility (Parent, 2010; Passonneau et al., 2011), spam is virtually non-existent (Akkaya et al., 2010), responses from multiple Turkers can be pooled to achieve high quality results (Snow et al., 2008; Akkaya et al., 2010), and that workers do not improve with experience (Akkaya et al., 2010). Third, we present a system of crowdsourcing WSD boasting a throughput of about 5,000 disambiguations per day at $0.011 per annotation. 2 Methods and data collection We selected a subset of the OntoNotes data (Hovy et al., 2006), the SemEval-2007 coarse-grained English Lexical Sample WSD task training data (Pradhan et al., 2007). The coarse-grained senses in OntoNotes address a concern that nuanced differences in sense inventories drives disagreement among annotators (Brown et al., 2010). We picked 1,000 contextual examples at random from the full set of 22,281.1 Our sample is detailed in table 1. It consisted of 590 nouns and 410 verb examples that had between 2-15 senses each (nouns: 5.7 ± 3.0 senses, verbs: 4.7 ± 3.3 senses). For each snippet, ten annotations were completed by ten unique Turkers. 1 We later disqualified 9 of the 1,000 because they had words with only one sense. 540 target word affect-v allow-v ann"
C12-2053,D08-1027,0,0.359862,"Missing"
C18-1005,E14-1060,0,0.110604,"expensive, especially for low-resource languages. Narasimhan et al. (2015) adopt a log-linear model with semantic similarity measures obtained from word embedding to identify morphologically related word pairs (Schone and Jurafsky, 2001) and achieve impressive segmentation results on the Morpho-Challenge data. Such semantically based model, however, requires a large corpus to train reliable word embeddings, which renders the method unsuitable for low-resource languages. The idea of paradigms has been explored in previous studies (Parkes et al., 1998; Goldsmith, 2001; Dreyer and Eisner, 2011; Ahlberg et al., 2014). Parkes et al. (1998) propose a model that learns neat inflectional paradigms only for English verbs from a corpus. Goldsmith (2001; 2006) uses heuristic rules with the MDL principle to greedily search morphological patterns (signatures). But the performance of rule-based search methods is crucially determined by the heuristic rules, and transformation rules are difficult to incorporate. Dreyer and Eisner (2011) proposed a log-linear model to identify paradigms. However, their method requires a number of seed paradigms for training. In morphologically rich languages such as Turkish, where one"
C18-1005,W06-3209,0,0.667607,"ften follows a verbal stem with y changed to i, as in carries) are recognized (Narasimhan et al., 2015), making use of transformations akin to rewrite rules (Goldwater and Johnson, 2004; Lignos et al., 2010). In all these approaches, the dependency between morphemes is generally local, and the overall distribution of the underlying paradigms implied by the segmentation result is not explored. In this paper, we propose to exploit the notion of the paradigm, a global property of morphological systems, for the task of unsupervised morphological segmentation (Parkes et al., 1998; Goldsmith, 2001; Chan, 2006). The idea of using paradigm to describe the morphological structure of a language can be traced back to a long time ago, and has been widely adopted in modern linguistic studies, starting from Ferdinand de Saussure. A paradigm refers to a set of morphological categories such as suffixes that can be can be applied to a homogeneous class of words. For instance, the paradigm (NULL, -er, -est, -ly) is defined over English adjectives (e.g., high, higher, highest, highly), the paradigm (NULL, -ing, -ed, -s, -er) is defined over English verbs (e.g, walk, walking, walked, walks, walker), etc. In esse"
C18-1005,W02-0603,0,0.282998,"ogical learning that provides an initial segmentation including the identification of potential suffixes. Section 6 lays out the details of constructing morphological paradigms and a pruning process that eliminates spurious morphological relations. Section 7 reports the results of our experiments on Morpho-Challenge data including English, Turkish, and Finnish in comparison with previous models. Section 8 concludes with a discussion of future research. 2 Related Work The Morpho-Challenge, held from 2005 to 2010, led to many successful morphology learning models. The Morfessor baseline system (Creutz and Lagus, 2002; Virpioja et al., 2013) provides a framework that maximizes the likelihood of the observation under the MDL principle. Creutz and Lagus (2005; 2007) extend the model with the maximum a posteriori (MAP) on both observed data and the model. Semi-supervised models have shown to be effective on morphological segmentation (Kohonen et al., 2010; Spiegler et al., 2010). In this paper, we focus on unsupervised learning of language morphologies, based on the consideration that constructing annotating data is expensive, especially for low-resource languages. Narasimhan et al. (2015) adopt a log-linear"
C18-1005,N07-1020,0,0.0397607,"ph ← {} 5: for all w in D do 6: segs ← GEN S EG(w) 7: seg ← arg max(r,s,t)∈segs P (r, s, t) 8: morph ← morph + (w, seg) 9: pdgs ← PARADIGMS(morph) 10: pdgs reliable, pdgs unreliable ← RELIABLE(pdgs) 11: {P (r, s, t)} ← ESTIMATE(pdgs reliable) 12: pdgs pruned ← PRUNE(pdgs unreliable) 13: return SEGMENTATION(pdgs pruned + pdgs reliable) 4 Generating Candidate Segmentations 4.1 Selecting Candidate Suffixes To obtain a working set of suffixes, we first adopt a simple method from previous studies: given a word pair (w1 , w2 ), if w2 = w1 + s, then s is a candidate suffix (Keshava and Pitler, 2006; Dasgupta and Ng, 2007). By comparing all possible word pairs, we can generate a set of candidate suffixes with their counted frequencies. The more frequent a candidate is, the more likely it is to be a real suffix. In our system, we only keep candidate suffixes that are at most six character long and appear at least three times in the word list. If applied naively, this method produces many short, spurious suffixes that are frequently occurring substrings in words, e.g. (for, fore), (are, area), (not, note), (she, shed) etc. The problem can be overcome by imposing a minimum length on words that are subject to candi"
C18-1005,D11-1057,0,0.338872,"ucting annotating data is expensive, especially for low-resource languages. Narasimhan et al. (2015) adopt a log-linear model with semantic similarity measures obtained from word embedding to identify morphologically related word pairs (Schone and Jurafsky, 2001) and achieve impressive segmentation results on the Morpho-Challenge data. Such semantically based model, however, requires a large corpus to train reliable word embeddings, which renders the method unsuitable for low-resource languages. The idea of paradigms has been explored in previous studies (Parkes et al., 1998; Goldsmith, 2001; Dreyer and Eisner, 2011; Ahlberg et al., 2014). Parkes et al. (1998) propose a model that learns neat inflectional paradigms only for English verbs from a corpus. Goldsmith (2001; 2006) uses heuristic rules with the MDL principle to greedily search morphological patterns (signatures). But the performance of rule-based search methods is crucially determined by the heuristic rules, and transformation rules are difficult to incorporate. Dreyer and Eisner (2011) proposed a log-linear model to identify paradigms. However, their method requires a number of seed paradigms for training. In morphologically rich languages suc"
C18-1005,J01-2001,0,0.906607,"lish suffix -es often follows a verbal stem with y changed to i, as in carries) are recognized (Narasimhan et al., 2015), making use of transformations akin to rewrite rules (Goldwater and Johnson, 2004; Lignos et al., 2010). In all these approaches, the dependency between morphemes is generally local, and the overall distribution of the underlying paradigms implied by the segmentation result is not explored. In this paper, we propose to exploit the notion of the paradigm, a global property of morphological systems, for the task of unsupervised morphological segmentation (Parkes et al., 1998; Goldsmith, 2001; Chan, 2006). The idea of using paradigm to describe the morphological structure of a language can be traced back to a long time ago, and has been widely adopted in modern linguistic studies, starting from Ferdinand de Saussure. A paradigm refers to a set of morphological categories such as suffixes that can be can be applied to a homogeneous class of words. For instance, the paradigm (NULL, -er, -est, -ly) is defined over English adjectives (e.g., high, higher, highest, highly), the paradigm (NULL, -ing, -ed, -s, -er) is defined over English verbs (e.g, walk, walking, walked, walks, walker),"
C18-1005,W04-0105,0,0.639707,"or many NLP applications such as language generation, information retrieval etc. (Sproat, 1992). Morphology analyzing is non-trivial especially for morphologically rich languages such as Turkish where the word formation process is extremely productive and can create in principle tens of billions of word forms. The identification of morphological relations between words provides a basis for uncovering their syntactic and semantic relations, which in turn can be exploited by downstream NLP applications. Most unsupervised models of morphological segmentation (Virpioja et al., 2013; Goldwater and Johnson, 2004; Creutz and Lagus, 2005; Creutz and Lagus, 2007; Lignos, 2010; Poon et al., 2009; Snyder and Barzilay, 2008) treat words as concatenation of morphemes. In some models, the dependencies between morphemes (e.g., the English suffix -es often follows a verbal stem with y changed to i, as in carries) are recognized (Narasimhan et al., 2015), making use of transformations akin to rewrite rules (Goldwater and Johnson, 2004; Lignos et al., 2010). In all these approaches, the dependency between morphemes is generally local, and the overall distribution of the underlying paradigms implied by the segmen"
C18-1005,W10-2210,0,0.0304621,"urkish, and Finnish in comparison with previous models. Section 8 concludes with a discussion of future research. 2 Related Work The Morpho-Challenge, held from 2005 to 2010, led to many successful morphology learning models. The Morfessor baseline system (Creutz and Lagus, 2002; Virpioja et al., 2013) provides a framework that maximizes the likelihood of the observation under the MDL principle. Creutz and Lagus (2005; 2007) extend the model with the maximum a posteriori (MAP) on both observed data and the model. Semi-supervised models have shown to be effective on morphological segmentation (Kohonen et al., 2010; Spiegler et al., 2010). In this paper, we focus on unsupervised learning of language morphologies, based on the consideration that constructing annotating data is expensive, especially for low-resource languages. Narasimhan et al. (2015) adopt a log-linear model with semantic similarity measures obtained from word embedding to identify morphologically related word pairs (Schone and Jurafsky, 2001) and achieve impressive segmentation results on the Morpho-Challenge data. Such semantically based model, however, requires a large corpus to train reliable word embeddings, which renders the method"
C18-1005,Q15-1012,0,0.307988,"ion of morphological relations between words provides a basis for uncovering their syntactic and semantic relations, which in turn can be exploited by downstream NLP applications. Most unsupervised models of morphological segmentation (Virpioja et al., 2013; Goldwater and Johnson, 2004; Creutz and Lagus, 2005; Creutz and Lagus, 2007; Lignos, 2010; Poon et al., 2009; Snyder and Barzilay, 2008) treat words as concatenation of morphemes. In some models, the dependencies between morphemes (e.g., the English suffix -es often follows a verbal stem with y changed to i, as in carries) are recognized (Narasimhan et al., 2015), making use of transformations akin to rewrite rules (Goldwater and Johnson, 2004; Lignos et al., 2010). In all these approaches, the dependency between morphemes is generally local, and the overall distribution of the underlying paradigms implied by the segmentation result is not explored. In this paper, we propose to exploit the notion of the paradigm, a global property of morphological systems, for the task of unsupervised morphological segmentation (Parkes et al., 1998; Goldsmith, 2001; Chan, 2006). The idea of using paradigm to describe the morphological structure of a language can be tr"
C18-1005,W98-1113,1,0.808596,"phemes (e.g., the English suffix -es often follows a verbal stem with y changed to i, as in carries) are recognized (Narasimhan et al., 2015), making use of transformations akin to rewrite rules (Goldwater and Johnson, 2004; Lignos et al., 2010). In all these approaches, the dependency between morphemes is generally local, and the overall distribution of the underlying paradigms implied by the segmentation result is not explored. In this paper, we propose to exploit the notion of the paradigm, a global property of morphological systems, for the task of unsupervised morphological segmentation (Parkes et al., 1998; Goldsmith, 2001; Chan, 2006). The idea of using paradigm to describe the morphological structure of a language can be traced back to a long time ago, and has been widely adopted in modern linguistic studies, starting from Ferdinand de Saussure. A paradigm refers to a set of morphological categories such as suffixes that can be can be applied to a homogeneous class of words. For instance, the paradigm (NULL, -er, -est, -ly) is defined over English adjectives (e.g., high, higher, highest, highly), the paradigm (NULL, -ing, -ed, -s, -er) is defined over English verbs (e.g, walk, walking, walked"
C18-1005,N09-1024,0,0.0808029,"tc. (Sproat, 1992). Morphology analyzing is non-trivial especially for morphologically rich languages such as Turkish where the word formation process is extremely productive and can create in principle tens of billions of word forms. The identification of morphological relations between words provides a basis for uncovering their syntactic and semantic relations, which in turn can be exploited by downstream NLP applications. Most unsupervised models of morphological segmentation (Virpioja et al., 2013; Goldwater and Johnson, 2004; Creutz and Lagus, 2005; Creutz and Lagus, 2007; Lignos, 2010; Poon et al., 2009; Snyder and Barzilay, 2008) treat words as concatenation of morphemes. In some models, the dependencies between morphemes (e.g., the English suffix -es often follows a verbal stem with y changed to i, as in carries) are recognized (Narasimhan et al., 2015), making use of transformations akin to rewrite rules (Goldwater and Johnson, 2004; Lignos et al., 2010). In all these approaches, the dependency between morphemes is generally local, and the overall distribution of the underlying paradigms implied by the segmentation result is not explored. In this paper, we propose to exploit the notion of"
C18-1005,N01-1024,0,0.416977,"e. Creutz and Lagus (2005; 2007) extend the model with the maximum a posteriori (MAP) on both observed data and the model. Semi-supervised models have shown to be effective on morphological segmentation (Kohonen et al., 2010; Spiegler et al., 2010). In this paper, we focus on unsupervised learning of language morphologies, based on the consideration that constructing annotating data is expensive, especially for low-resource languages. Narasimhan et al. (2015) adopt a log-linear model with semantic similarity measures obtained from word embedding to identify morphologically related word pairs (Schone and Jurafsky, 2001) and achieve impressive segmentation results on the Morpho-Challenge data. Such semantically based model, however, requires a large corpus to train reliable word embeddings, which renders the method unsuitable for low-resource languages. The idea of paradigms has been explored in previous studies (Parkes et al., 1998; Goldsmith, 2001; Dreyer and Eisner, 2011; Ahlberg et al., 2014). Parkes et al. (1998) propose a model that learns neat inflectional paradigms only for English verbs from a corpus. Goldsmith (2001; 2006) uses heuristic rules with the MDL principle to greedily search morphological"
C18-1005,P08-1084,0,0.2976,"Morphology analyzing is non-trivial especially for morphologically rich languages such as Turkish where the word formation process is extremely productive and can create in principle tens of billions of word forms. The identification of morphological relations between words provides a basis for uncovering their syntactic and semantic relations, which in turn can be exploited by downstream NLP applications. Most unsupervised models of morphological segmentation (Virpioja et al., 2013; Goldwater and Johnson, 2004; Creutz and Lagus, 2005; Creutz and Lagus, 2007; Lignos, 2010; Poon et al., 2009; Snyder and Barzilay, 2008) treat words as concatenation of morphemes. In some models, the dependencies between morphemes (e.g., the English suffix -es often follows a verbal stem with y changed to i, as in carries) are recognized (Narasimhan et al., 2015), making use of transformations akin to rewrite rules (Goldwater and Johnson, 2004; Lignos et al., 2010). In all these approaches, the dependency between morphemes is generally local, and the overall distribution of the underlying paradigms implied by the segmentation result is not explored. In this paper, we propose to exploit the notion of the paradigm, a global prop"
C18-1130,D16-1120,0,0.048004,"Missing"
C18-1130,D11-1120,0,0.271446,"show that emotions are mainly correlated with the Latino and AA groups, with only a single other significant correlation for the Asian and White groups. African American users on Twitter express overall more emotions than other groups, including both positive sentiment as well as a range of negative emotions, especially anger. On the other hand, Latino users express overall less sentiment, both positive and negative, and fewer emotions. 1542 10 Conclusion We presented a detailed study of user-level race/ethnicity prediction along the lines of previous work on predicting user traits from text (Burger et al., 2011; Rao et al., 2010; Pennacchiotti and Popescu, 2011; Schwartz et al., 2013; Sap et al., 2014; Volkova et al., 2014; Preot¸iuc-Pietro et al., 2015b; Flekova et al., 2016b; Preot¸iuc-Pietro et al., 2016; Preot¸iuc-Pietro et al., 2017). In contrast with previous research on race/ethnicity, we used labels obtained by directly surveying Twitter users, rather than distantly supervised geo-located data or perceived race labels, which lead to multiple biases and lower accuracies on real data. We built models that obtain state-of-the-art out-of-sample accuracy on predicting the four prominent racial/et"
C18-1130,P07-1033,0,0.260304,"Missing"
C18-1130,R13-1026,0,0.0532334,"model, which in addition to emotions also includes positive and negative sentiment (Volkova and Bachrach, 2016). We also used the word lexicon derived using crowd-sourcing from (Mohammad and Turney, 2010; Mohammad and Turney, 2013) and found it had slightly worse predictive results. Using these models, we assign sentiment and emotion probabilities to each message and then average across all users’ posts to obtain user level emotion expression scores. Part-of-Speech Tags We analyze part-of-speech tag usage across groups by POS tagging all tweets using the Twitter model of the Stanford Tagger (Derczynski et al., 2013), which showed best tagging results for AAVE (Jørgensen et al., 2015) and also uses the finer grained Penn Treebank tagset. Each user is thus represented as a distribution over POS tags. Linear Ensemble Finally, we build a logistic regression model having as features the real-valued predictions of the models trained on all previous feature sets. 6 Baselines We introduce the following competitive baselines: Demographics User demographics collected from the users using our survey (age, gender, education and income level) are used as features. A classifier using only these demographics as feature"
C18-1130,D10-1124,0,0.152654,"Missing"
C18-1130,P11-1137,0,0.0310565,"nd Hispanic/Latinos football (OpenDorse, 2013) – and are consequently more likely to post about these sports. In addition, previous studies used either perceived race labels (Mohammady and Culotta, 2015; Volkova and Bachrach, 2016; Culotta et al., 2016) which are subject to human stereotypes (Flekova et al., 2016a) or mapped geo-located tweets to census statistics (Mohammady and Culotta, 2014; Blodgett et al., 2016) which lead to other biases (Jørgensen et al., 2015) because: 1) census statistics are outdated; 2) the Twitter population is not a representative sample of the general population (Eisenstein et al., 2011) with African Americans over-represented on Twitter (Duggan, 2015); 3) users who geo-locate tweets are not a representative sample of the Twitter population (Eisenstein, 2016); 4) geo-located tweets might be posted from a different location than the user’s home. In this study, we introduce a new data set where user-level race and ethnicity labels are collected through an online survey of Twitter users. We build models for accurately classifying Twitter users into four of the largest racial and ethnic groups in the U.S. as defined by the Census Bureau: Non-Hispanic Whites, This work is licensed"
C18-1130,P16-1080,1,0.910573,"Missing"
C18-1130,P16-2051,1,0.924535,"Missing"
C18-1130,W15-4302,0,0.123658,". One of the most important demographic differences – especially across the US – is that of race and ethnicity. For example, in Twitter-based political polling applications it is important to adjust for the fact that ethnic minorities are overall less likely to support either party, but prefer the Democratic Party over the Republican Party (Hajnal and Lee, 2011). In this paper, we present the first extensive study on identifying user-level race and ethnicity. So far, linguistic differences have mostly been studied in the context of dialects, usually African-American Vernacular English – AAVE (Jørgensen et al., 2015), using message-level data which offered insight into syntactic (Stewart, 2014) or lexical markers (Blodgett et al., 2016). However, not all users from a racial or ethnic group use these markers or, more generally, an associated dialect and usage is different across sociodemographic traits – use of the AAVE is correlated with lower income and education (Rickford, 1999). In addition, there are differences in language use across racial/ethnic groups not caused by dialects e.g., in the US, African Americans prefer basketball, Whites ice-hockey and Hispanic/Latinos football (OpenDorse, 2013) – and"
C18-1130,W16-5614,0,0.114835,"Missing"
C18-1130,E14-1043,1,0.811566,"Missing"
C18-1130,N13-1090,0,0.0114529,"Missing"
C18-1130,W10-0204,0,0.0657472,"odel of discrete emotions is the Ekman model (Ekman, 1992; Strapparava and Mihalcea, 2008; Strapparava et al., 2004) which posits the existence of six basic 5 http://www.preotiuc.ro https://twitter.com/privacy/ 7 https://support.twitter.com/articles/20170368#/ 6 1536 emotions: anger, disgust, fear, joy, sadness and surprise. We automatically quantify these emotions from our Twitter data set using a publicly available model, which in addition to emotions also includes positive and negative sentiment (Volkova and Bachrach, 2016). We also used the word lexicon derived using crowd-sourcing from (Mohammad and Turney, 2010; Mohammad and Turney, 2013) and found it had slightly worse predictive results. Using these models, we assign sentiment and emotion probabilities to each message and then average across all users’ posts to obtain user level emotion expression scores. Part-of-Speech Tags We analyze part-of-speech tag usage across groups by POS tagging all tweets using the Twitter model of the Stanford Tagger (Derczynski et al., 2013), which showed best tagging results for AAVE (Jørgensen et al., 2015) and also uses the finer grained Penn Treebank tagset. Each user is thus represented as a distribution over POS"
C18-1130,W14-2702,0,0.599999,"elated with lower income and education (Rickford, 1999). In addition, there are differences in language use across racial/ethnic groups not caused by dialects e.g., in the US, African Americans prefer basketball, Whites ice-hockey and Hispanic/Latinos football (OpenDorse, 2013) – and are consequently more likely to post about these sports. In addition, previous studies used either perceived race labels (Mohammady and Culotta, 2015; Volkova and Bachrach, 2016; Culotta et al., 2016) which are subject to human stereotypes (Flekova et al., 2016a) or mapped geo-located tweets to census statistics (Mohammady and Culotta, 2014; Blodgett et al., 2016) which lead to other biases (Jørgensen et al., 2015) because: 1) census statistics are outdated; 2) the Twitter population is not a representative sample of the general population (Eisenstein et al., 2011) with African Americans over-represented on Twitter (Duggan, 2015); 3) users who geo-locate tweets are not a representative sample of the Twitter population (Eisenstein, 2016); 4) geo-located tweets might be posted from a different location than the user’s home. In this study, we introduce a new data set where user-level race and ethnicity labels are collected through"
C18-1130,N15-1019,0,0.0201032,"However, not all users from a racial or ethnic group use these markers or, more generally, an associated dialect and usage is different across sociodemographic traits – use of the AAVE is correlated with lower income and education (Rickford, 1999). In addition, there are differences in language use across racial/ethnic groups not caused by dialects e.g., in the US, African Americans prefer basketball, Whites ice-hockey and Hispanic/Latinos football (OpenDorse, 2013) – and are consequently more likely to post about these sports. In addition, previous studies used either perceived race labels (Mohammady and Culotta, 2015; Volkova and Bachrach, 2016; Culotta et al., 2016) which are subject to human stereotypes (Flekova et al., 2016a) or mapped geo-located tweets to census statistics (Mohammady and Culotta, 2014; Blodgett et al., 2016) which lead to other biases (Jørgensen et al., 2015) because: 1) census statistics are outdated; 2) the Twitter population is not a representative sample of the general population (Eisenstein et al., 2011) with African Americans over-represented on Twitter (Duggan, 2015); 3) users who geo-locate tweets are not a representative sample of the Twitter population (Eisenstein, 2016); 4"
C18-1130,D14-1162,0,0.0773161,"Missing"
C18-1130,P15-1169,1,0.9345,"Missing"
C18-1130,P17-1068,1,0.771783,"Missing"
C18-1130,D14-1121,1,0.910766,"Missing"
C18-1130,E14-3004,0,0.0130914,"of race and ethnicity. For example, in Twitter-based political polling applications it is important to adjust for the fact that ethnic minorities are overall less likely to support either party, but prefer the Democratic Party over the Republican Party (Hajnal and Lee, 2011). In this paper, we present the first extensive study on identifying user-level race and ethnicity. So far, linguistic differences have mostly been studied in the context of dialects, usually African-American Vernacular English – AAVE (Jørgensen et al., 2015), using message-level data which offered insight into syntactic (Stewart, 2014) or lexical markers (Blodgett et al., 2016). However, not all users from a racial or ethnic group use these markers or, more generally, an associated dialect and usage is different across sociodemographic traits – use of the AAVE is correlated with lower income and education (Rickford, 1999). In addition, there are differences in language use across racial/ethnic groups not caused by dialects e.g., in the US, African Americans prefer basketball, Whites ice-hockey and Hispanic/Latinos football (OpenDorse, 2013) – and are consequently more likely to post about these sports. In addition, previous"
C18-1130,strapparava-valitutti-2004-wordnet,0,0.258361,"Missing"
C18-1130,P16-1148,0,0.0957697,"a racial or ethnic group use these markers or, more generally, an associated dialect and usage is different across sociodemographic traits – use of the AAVE is correlated with lower income and education (Rickford, 1999). In addition, there are differences in language use across racial/ethnic groups not caused by dialects e.g., in the US, African Americans prefer basketball, Whites ice-hockey and Hispanic/Latinos football (OpenDorse, 2013) – and are consequently more likely to post about these sports. In addition, previous studies used either perceived race labels (Mohammady and Culotta, 2015; Volkova and Bachrach, 2016; Culotta et al., 2016) which are subject to human stereotypes (Flekova et al., 2016a) or mapped geo-located tweets to census statistics (Mohammady and Culotta, 2014; Blodgett et al., 2016) which lead to other biases (Jørgensen et al., 2015) because: 1) census statistics are outdated; 2) the Twitter population is not a representative sample of the general population (Eisenstein et al., 2011) with African Americans over-represented on Twitter (Duggan, 2015); 3) users who geo-locate tweets are not a representative sample of the Twitter population (Eisenstein, 2016); 4) geo-located tweets might b"
C18-1130,P14-1018,0,0.062799,"Missing"
D10-1071,P06-1084,0,0.048861,"Missing"
D10-1071,2008.eamt-1.3,0,0.0560375,"Missing"
D10-1071,N04-4038,0,0.100647,"Missing"
D10-1071,W09-0805,0,0.0501513,"Missing"
D10-1071,R09-1018,0,0.0352063,"Missing"
D10-1071,P08-1085,0,0.0364335,"Missing"
D10-1071,P05-1071,0,0.0769356,"Missing"
D10-1071,A00-2013,0,0.01337,"lly, we also consider a context-less approach, i.e. using only “unigram” features for labels as well as LEs. We call this feature set Zuni , and the corresponding SVM model Muni . The results of these various models, along with those of Mf ull are summarized in Table 5. Model Mbaseline Mind Muni Mcheat Mf ull E1 13.6 18.7 11.6 8.2 9.4 E2 9.1 6.0 6.4 2.8 3.8 Table 5: Percent error rates of alternative approaches. Note: The results reported are 10 fold cross validation test accuracies and no parameters have been tuned on them. We used same train-test splits for all the datasets. 5 Related Work (Hajic, 2000) show that for highly inflectional languages, the use of a morphological analyzer 1 A new version of MADA was released very close to the submission deadline for this conference. improves accuracy of disambiguation. (Diab et al., 2004) perform tokenization, POS tagging and base phrase chunking using an SVM based learner. (Ahmed and N¨urnberger, 2008) perform word-sense disambiguation using a Naive Bayesian model and rely on parallel corpora and matching schemes instead of a morphological analyzer. (Kulick, 2010) perform simultaneous tokenization and part-of-speech tagging for Arabic by separati"
D10-1071,C08-2011,0,0.0587293,"Missing"
D10-1071,P10-2063,0,0.0213625,"Missing"
D10-1071,mohamed-kubler-2010-arabic,0,0.024886,"Missing"
D10-1071,P08-2030,0,0.0268738,"Missing"
D10-1071,H05-1060,0,0.0542564,"Missing"
D10-1071,P09-1055,0,0.0233945,"Missing"
D10-1071,W02-1001,0,\N,Missing
D12-1019,P12-1024,1,0.832772,"and trigram counts sensitive to the underlying dependency structure of the given sentence. Recently, Luque et al. (2012) have also proposed a spectral method for dependency parsing, however they deal with horizontal markovization and use hidden states to model sequential dependencies within a word’s sequence of children. In contrast with that, in this paper, we propose a spectral learning algorithm where latent states are not restricted to HMM-like distributions of modifier sequences for a particular head, but instead allow information to be propagated through the entire tree. More recently, Cohen et al. (2012) have proposed a spectral method for learning PCFGs. Its worth noting that recent work by Parikh et al. (2011) also extends Hsu et al. (2008) to latent variable dependency trees like us but under the restrictive conditions that model parameters are trained for a specified, albeit arbitrary, tree topology.2 In other words, all training sentences and test sentences must have identical tree topologies. By doing this they allow for node-specific model parameters, but must retrain the model entirely when a different tree topology is encountered. Our model on the other hand allows the flexibility an"
D12-1019,P99-1059,0,0.349064,"Missing"
D12-1019,P08-1068,1,0.290812,"s a workhorse of statistical pattern recognition with applications ranging from speech to vision to language. Adding latent variables to these models gives us additional modeling power and have shown success in applications like POS tagging (Merialdo, 1994), speech recognition (Rabiner, 1989) and object recognition (Quattoni et al., 2004). However, this comes at the cost that the resulting parameter estimation problem becomes non-convex and techniques like EM (Dempster et al., 1977) which are used to estimate the parameters can only lead to locally optimal solutions. Recent work by Hsu et al. (2008) has shown that globally consistent estimates of the parameters of HMMs can be found by using spectral methods, particularly by singular value decomposition (SVD) of appropriately defined linear systems. They avoid the NP Hard problem of the global optimization problem of the HMM parameters (Terwijn, 2002), by putting restrictions on the smallest singular value of the HMM parameters. The main intuition behind the model is that, although the observed data (i.e. words) seem to live in a very high dimensional space, but in reality they live in a very low dimensional space (size k ∼ 30 − 50) and a"
D12-1019,E12-1042,0,0.291709,"t Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 205–213, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics ing which has one hidden node for each word in the sentence, like the one shown in Figure 1 and work out the details for the parameter estimation of the corresponding spectral learning model. At a very high level, the parameter estimation of our model involves collecting unigram, bigram and trigram counts sensitive to the underlying dependency structure of the given sentence. Recently, Luque et al. (2012) have also proposed a spectral method for dependency parsing, however they deal with horizontal markovization and use hidden states to model sequential dependencies within a word’s sequence of children. In contrast with that, in this paper, we propose a spectral learning algorithm where latent states are not restricted to HMM-like distributions of modifier sequences for a particular head, but instead allow information to be propagated through the entire tree. More recently, Cohen et al. (2012) have proposed a spectral method for learning PCFGs. Its worth noting that recent work by Parikh et al"
D12-1019,J94-2001,0,0.170465,"Missing"
D12-1019,P08-2054,0,0.0194386,"is that, although the observed data (i.e. words) seem to live in a very high dimensional space, but in reality they live in a very low dimensional space (size k ∼ 30 − 50) and an appropriate eigen decomposition of the observed data will reveal the underlying low dimensional dynamics and thereby revealing the parameters of the model. Besides ducking the NP hard problem, the spectral methods are very fast and scalable to train compared to EM methods. In this paper we generalize the approach of Hsu et al. (2008) to learn dependency tree structures with latent variables.1 Petrov et al. (2006) and Musillo and Merlo (2008) have shown that learning PCFGs and dependency grammars respectively with latent variables can produce parsers with very good generalization performance. However, both these approaches rely on EM for parameter estimation and can benefit from using spectral methods. We propose a simple yet powerful latent variable generative model for use with dependency pars1 Actually, instead of using the model by Hsu et al. (2008) we work with a related model proposed by Foster et al. (2012) which addresses some of the shortcomings of the earlier model which we detail below. 205 Proceedings of the 2012 Joint"
D12-1019,P06-1055,0,0.0305203,"tuition behind the model is that, although the observed data (i.e. words) seem to live in a very high dimensional space, but in reality they live in a very low dimensional space (size k ∼ 30 − 50) and an appropriate eigen decomposition of the observed data will reveal the underlying low dimensional dynamics and thereby revealing the parameters of the model. Besides ducking the NP hard problem, the spectral methods are very fast and scalable to train compared to EM methods. In this paper we generalize the approach of Hsu et al. (2008) to learn dependency tree structures with latent variables.1 Petrov et al. (2006) and Musillo and Merlo (2008) have shown that learning PCFGs and dependency grammars respectively with latent variables can produce parsers with very good generalization performance. However, both these approaches rely on EM for parameter estimation and can benefit from using spectral methods. We propose a simple yet powerful latent variable generative model for use with dependency pars1 Actually, instead of using the model by Hsu et al. (2008) we work with a related model proposed by Foster et al. (2012) which addresses some of the shortcomings of the earlier model which we detail below. 205"
D12-1019,W96-0213,0,0.037909,"ents both first and second order parsers and is trained using MIRA (Crammer et al., 2006) and used the standard baseline features as described in McDonald (2006). We tested our methods on the English Penn Treebank (Marcus et al., 1993). We use the standard splits of Penn Treebank; i.e., we used sections 2-21 for training, section 22 for development and section 23 for testing. We used the PennConverter7 tool to convert Penn Treebank from constituent to dependency format. Following (McDonald, 2006; Koo 7 http://nlp.cs.lth.se/software/treebank_ converter/ et al., 2008), we used the POS tagger by Ratnaparkhi (1996) trained on the full training data to provide POS tags for development and test sets and used 10way jackknifing to generate tags for the training set. As is common practice we stripped our sentences of all the punctuation. We evaluated our approach on sentences of all lengths. 4.2 Details of spectral learning For the spectral learning phase, we need to just collect word counts from the training data as described above, so there are no tunable parameters as such. However, we need to have access to an attribute dictionary U which contains a k dimensional representation for each word in the corpu"
D12-1019,J93-2004,0,\N,Missing
D12-1019,J05-1003,1,\N,Missing
D12-1019,P02-1062,1,\N,Missing
D14-1121,D11-1120,0,0.392361,"et al., 2011; Tausczik and Pennebaker, 2010). 2 Related Work Online behavior is representative of many aspects of a user’s demographics (Pennacchiotti and Popescu, 2011; Rao et al., 2010). Many studies have used linguistic cues (such as ngrams) to determine if someone belongs to a certain age group, be it on Twitter or another social media platform (Al Zamal et al., 2012; Argamon et al., 2009; Nguyen et al., 2013; Rangel and Rosso, 2013). Gender prediction has been studied across blogs (Burger and Henderson, 2006; Goswami et al., 2009), Yahoo! search queries (Jones et al., 2007), and Twitter (Burger et al., 2011; Nguyen et al., 2013; Liu and Ruths, 2013; Rao et al., 2010). Because Twitter does not make gender or age available, such work infers gender and age by leveraging profile information, such as gender-discriminating names or crawling for links to publicly available data (e.g. Burger et al., 2 The LIWC lexicon, derived manually based on psychological theory, (Pennebaker et al., 2001) had 1136 citations in 2013 alone. 1146 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1146–1151, c October 25-29, 2014, Doha, Qatar. 2014 Association for Comput"
D14-1121,D13-1114,0,0.137891,"Missing"
D14-1121,S13-2053,0,0.0426009,"Missing"
D14-1121,J11-2001,0,0.0618245,"Missing"
D14-1121,D12-1005,0,0.0276481,"Missing"
D14-1121,D13-1187,0,0.201167,"Missing"
D14-1121,S13-2052,0,0.02001,"Missing"
D16-1217,abdul-mageed-diab-2014-sana,1,0.828748,"MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic, also finding that automatic translation of English texts yields competitive results (Abdul-Mageed and Diab, 2014; Mohammad et al., 2016). However, translated texts tend to lose sentiment information such that the translated data is more neutral than the source language (Salameh et al., 2015). It is less obvious how well expressions of emotion or subjective well-being translate between languages and cultures; the words for liking a phone or TV may be more similar across cultures than the ones for finding life and relationships satisfying, or work meaningful and engaging. 2.2 Well-being In contrast to classic sentiment analysis, well-being is not restricted to positive and negative emotion. In 2011, the p"
D16-1217,W12-3709,0,0.0172632,"g camp is developing methods to estimate personality and emotion, asking “how does she feel?” rather than “how much does she like the product?” (Mohammad and Kiritchenko, 2015; Park et al., 2014). In social media, the well-being of individuals as well as communities has been studied, on various platforms such as Facebook and Twitter (Bollen et al., 2011; Schwartz et al., 2013; Eichstaedt et al., 2015; Schwartz et al., 2016). 2.1 Translating sentiment Past work has, on the whole, regarded state-of-theart automatic translation for sentiment analysis optimistically. In assessing statistical MT, (Balahur and Turchi, 2012) found that modern SMT systems can produce reliable training data for languages other than English. Comparative evaluations between English and Romanian (Mihalcea et al., 2007) and English and both Spanish and Romanian (Banea et al., 2008) based on the English MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-spec"
D16-1217,D08-1014,0,0.0225301,"s well as communities has been studied, on various platforms such as Facebook and Twitter (Bollen et al., 2011; Schwartz et al., 2013; Eichstaedt et al., 2015; Schwartz et al., 2016). 2.1 Translating sentiment Past work has, on the whole, regarded state-of-theart automatic translation for sentiment analysis optimistically. In assessing statistical MT, (Balahur and Turchi, 2012) found that modern SMT systems can produce reliable training data for languages other than English. Comparative evaluations between English and Romanian (Mihalcea et al., 2007) and English and both Spanish and Romanian (Banea et al., 2008) based on the English MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic, also finding that automatic translation of English texts yields"
D16-1217,P12-3005,0,0.0677526,"Missing"
D16-1217,P07-1123,0,0.096535,"rk et al., 2014). In social media, the well-being of individuals as well as communities has been studied, on various platforms such as Facebook and Twitter (Bollen et al., 2011; Schwartz et al., 2013; Eichstaedt et al., 2015; Schwartz et al., 2016). 2.1 Translating sentiment Past work has, on the whole, regarded state-of-theart automatic translation for sentiment analysis optimistically. In assessing statistical MT, (Balahur and Turchi, 2012) found that modern SMT systems can produce reliable training data for languages other than English. Comparative evaluations between English and Romanian (Mihalcea et al., 2007) and English and both Spanish and Romanian (Banea et al., 2008) based on the English MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic,"
D16-1217,N15-1078,0,0.0358916,"t languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic, also finding that automatic translation of English texts yields competitive results (Abdul-Mageed and Diab, 2014; Mohammad et al., 2016). However, translated texts tend to lose sentiment information such that the translated data is more neutral than the source language (Salameh et al., 2015). It is less obvious how well expressions of emotion or subjective well-being translate between languages and cultures; the words for liking a phone or TV may be more similar across cultures than the ones for finding life and relationships satisfying, or work meaningful and engaging. 2.2 Well-being In contrast to classic sentiment analysis, well-being is not restricted to positive and negative emotion. In 2011, the psychologist Martin Selig2043 man proposed PERMA (Seligman, 2011), a fivedimensional model of well-being where ‘P’ stands for positive emotion, ‘E’ is engagement, ‘R’ is positive re"
D16-1217,D14-1121,1,0.829336,"irani, 1996) models to predict the average annotation score for each of the crowdsourced PERMA labels. Separate models, each consisting of regression weights for each term in the lexicon, were built for each of the ten (five positive and five negative) PERMA components in both English and Spanish1 . Each model was validated using 10-fold cross validation, with Pearson correlations averaged over the 10 positive/negative PERMA components. Re1 Available at www.wwbp.org. 2044 sults are presented in Table 1. The models were then transformed into a predictive lexicon using the methods described in (Sap et al., 2014), where the weights in the lexicon were derived from the above Lasso regression model. Model Spanish English r 0.36 0.36 Table 1: Performance as measured by Pearson r correlation averaged over the 10 positive/negative PERMA components using 10-fold cross validation. 3.3 Translating the models We used Google Translate to translate both the original English and Spanish Tweets and the words in the models. We also created versions of the translated models in which we manually corrected apparent translation errors for 25 terms with the largest regression coefficients for each of the 10 PERMA compon"
D16-1217,D08-1058,0,0.0554644,"egarded state-of-theart automatic translation for sentiment analysis optimistically. In assessing statistical MT, (Balahur and Turchi, 2012) found that modern SMT systems can produce reliable training data for languages other than English. Comparative evaluations between English and Romanian (Mihalcea et al., 2007) and English and both Spanish and Romanian (Banea et al., 2008) based on the English MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic, also finding that automatic translation of English texts yields competitive results (Abdul-Mageed and Diab, 2014; Mohammad et al., 2016). However, translated texts tend to lose sentiment information such that the translated data is more neutral than the source language (Salameh et al., 2015)."
D16-1219,W14-3214,1,0.845748,"5.4 million status updates. All users completed a 100-item personality questionnaire (an International Personality Item Pool (IPIP) proxy to the NEO-PI-R (Goldberg, 1999). User-level degree of depression (DDep) was estimated as the average response to seven depression facet items (nested within the larger Neuroti2 To quantify the pervasiveness of pronoun studies in social science, we consider the citation count, via Google Scholar (July, 2016), to works mentioning “pronoun” by one of the top researchers, James W. Pennebaker, which number over 10,000. 2055 cism item pool of the questionnaire) (Schwartz et al., 2014). Dependency Features. We used dependency annotations in order to determine the syntactic function of personal pronouns i.e. subject (S) and direct object (DO). We obtained dependency parses of our corpus using Stanford Parser (Socher et al., 2013) that provides universal dependencies in (relation, head, dependent) triples. In the next step, we extracted the words in in the nominal subject (“nsubj&quot;) and direct object (“dobj&quot;) positions including nsubj 1st-person singular pronoun “I&quot;, and dobj 1st-person singular pronoun “me&quot;. We also extracted the corresponding verbs for each of the nominal su"
D16-1219,P13-1045,0,0.0225755,"o seven depression facet items (nested within the larger Neuroti2 To quantify the pervasiveness of pronoun studies in social science, we consider the citation count, via Google Scholar (July, 2016), to works mentioning “pronoun” by one of the top researchers, James W. Pennebaker, which number over 10,000. 2055 cism item pool of the questionnaire) (Schwartz et al., 2014). Dependency Features. We used dependency annotations in order to determine the syntactic function of personal pronouns i.e. subject (S) and direct object (DO). We obtained dependency parses of our corpus using Stanford Parser (Socher et al., 2013) that provides universal dependencies in (relation, head, dependent) triples. In the next step, we extracted the words in in the nominal subject (“nsubj&quot;) and direct object (“dobj&quot;) positions including nsubj 1st-person singular pronoun “I&quot;, and dobj 1st-person singular pronoun “me&quot;. We also extracted the corresponding verbs for each of the nominal subjects, and direct object words. Verb categorization. In order to integrate the verbal semantic categories in the syntactic analysis of pronouns, we utilize two verb categorization methods (a) linguistically-driven Levin’s Verb Classes, and (b) emp"
D17-1248,P12-3005,0,0.0193612,"d in (Flekova et al., 2016). The users are chosen to have an age in the 15–34 year old interval in the year 2015 and we only use tweets posted in 2015 in our analysis. We selected exactly 10 users of each age in this interval, as these are the most frequent ages present in our data set, most language variation happens in this interval and these are the age range which raters can most accurately predict (Nguyen et al., 2014). We use the Twitter API to download up to 3200 tweets from these users. We pre-process tweets by filtering those not written in English as detected by an automated method (Lui and Baldwin, 2012), removing duplicate tweets (i.e., having the same first 6 tokens) and removing re-tweets as these are not authored by the user. All potentially sensitive or revealing information contained in tweets such as URL’s, usernames, @-mentions, phone numbers were removed and replaced with placeholders before shown to annotators. Other than publicly available tweets, no other metadata or information was presented with the task, so raters were not able to map the tweets to actual user identities. The raters were also unaware of the conditions (Random, Opposite, Same, Youngest or Oldest) they were assig"
D17-1248,D15-1130,0,0.0604396,"Missing"
D17-1248,D11-1120,0,0.0469407,"ser traits can also go beyond basic demographics to more salient ones such as social status (Preot¸iuc-Pietro et al., 2015a,b), political ideology (Preot¸iuc-Pietro et al., 2017a) or psychological traits such as personality (Schwartz et al., 2013; Guntuku et al., 2015a,b, 2016, 2017), narcissism (Preotiuc-Pietro et al., 2016), trust or empathy (Abdul-Mageed et al., 2017). 2 Data Set We study two user traits through two Twitter data sets containing users with known gender and age information. First, for gender, we use a subset of 200 users (100 males, 100 females) of the data set collected by (Burger et al., 2011) and released by (Volkova et al., 2013) which mapped users to their gender by linking their Twitter account to their publicly self-declared gender on related blogs. The age data set consists of 200 users that self-reported their age in a survey and disclosed their public Twitter data that are part of a larger set used in (Flekova et al., 2016). The users are chosen to have an age in the 15–34 year old interval in the year 2015 and we only use tweets posted in 2015 in our analysis. We selected exactly 10 users of each age in this interval, as these are the most frequent ages present in our data"
D17-1248,C14-1184,0,0.0309599,"Missing"
D17-1248,P16-1080,1,0.878376,"Missing"
D17-1248,P17-1068,1,0.846055,"Missing"
D17-1248,W17-2903,1,0.776183,"Missing"
D17-1248,P15-1169,1,0.901456,"Missing"
D17-1248,E17-1101,0,0.0220252,"when predicting males. For age perception, we show consistent results in altering perception as both younger or older, albeit for relatively smaller age differences. Applications of our proposed line of research include conversational agents or automated email generation. Personalization was motivated in the context of machine translation (Mirkin et al., 2335 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2335–2341 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2015) and recently attempted for gender (Rabinovich et al., 2017), even though the authors do not use humans to evaluate perception of gender. Automatic text personalization to user traits can also go beyond basic demographics to more salient ones such as social status (Preot¸iuc-Pietro et al., 2015a,b), political ideology (Preot¸iuc-Pietro et al., 2017a) or psychological traits such as personality (Schwartz et al., 2013; Guntuku et al., 2015a,b, 2016, 2017), narcissism (Preotiuc-Pietro et al., 2016), trust or empathy (Abdul-Mageed et al., 2017). 2 Data Set We study two user traits through two Twitter data sets containing users with known gender and age inf"
D17-1248,W16-5603,0,0.0457329,"Missing"
D17-1248,D14-1121,1,0.894562,"Missing"
D17-1248,D13-1187,0,0.0622444,"Missing"
D17-1250,P07-1053,0,0.0401273,"ength of comment to be the dominant predictor, with other features providing minimal benefit. However, a few studies (including our own) have found this baseline can be overcome. For example, Racherla and Friske (Racherla and Friske, 2012) investigated perceived usefulness of consumer reviews on Yelp and found that reputation and expertise were more important than total number of words on perceived usefulness. All of these prior works focus on assessing subjective aspects of comments (usefulness, quality, and helpfulness); Perhaps the study coming closest in spirit to our own was Ghose et al. Ghose et al. (2007) who quantified quality of reviews by the economic change they produced. However, they still were not dealing with a randomized experiment and so conclusions were correlational and the objective was better sales of the product rather than benefit to the reader (i.e. leading to a better decision). 6 Conclusion Our results suggest three key findings. First, what one writes in a comment is more important than simply how much one writes; this is true across both subjective and objective outcomes, though length had virtually no predictive ability for improving forecaster accuracy. Second, we found"
D17-1250,D07-1035,0,0.0594546,"al rater biases; for example, forecasters are subjectively biased in favor of recommendations mentioning business deals and material things, even though such recommendations do not indeed prove any more useful objectively. 1 Introduction Finding good recommendations is an integral part of a modern information-seeking life – from purchasing products based on reviews to finding answers to baffling questions. Following the tradition of sentiment analysis, many have proposed methods to automatically assess the quality of recommendations or comments based on subjective ratings of their usefulness (Liu et al., 2007; Siersdorfer et al., 2010; Becker et al., 2012; Momeni et al., 2013) or of persuasiveness (Wei et al., 2016). However, information thought to be useful does not always prove so, and subjective ratings may be driven by biases. Reviews which convince you to watch a movie or buy a product do not guarantee that you will enjoy the product, and the most convincing or highest rated answers to questions on sites like Stack Overflow or Yahoo Answers are not always the most accurate. We explore recommendations in a unique dataset, an online forecasting competition, which offers a rare glimpse into both"
D17-1250,J11-2003,0,0.0337835,"best to include such noisy data when training, it does not provide a very accurate assessment. Therefore, we use dedicated training and test sets, where the test set is a random sample of 500 comments with more than 3 updates and thus a more reliable mean change in forecaster accuracy. As a baseline, we use the square-root of the number of words in the comment. This may seem like weak measure of quality, but the history of automatic quality assessment is saturated with findings that length is the best predictor of quality. This holds true for both answers to questions (Agichtein et al., 2008; Surdeanu et al., 2011; Beygelzimer et al., 2015); as well as e-commerce reviews (Cao et al., 2011; Racherla and Friske, 2012). Of course, length is not as shallow as it may seem at first; given no strong incentive for authors to leave long comments, length is likely a proxy for thoroughness of the comment. Still, because we would like to understand the content distinguishing various metrics of quality, we view length as a baseline to move beyond. Table 2 compares the accuracy of models built on content (ngrams, parts-of-speech, and concepts) to the baseline of length. In all cases, our models, based on content, pe"
D17-1250,P16-2032,0,0.0878527,"iness deals and material things, even though such recommendations do not indeed prove any more useful objectively. 1 Introduction Finding good recommendations is an integral part of a modern information-seeking life – from purchasing products based on reviews to finding answers to baffling questions. Following the tradition of sentiment analysis, many have proposed methods to automatically assess the quality of recommendations or comments based on subjective ratings of their usefulness (Liu et al., 2007; Siersdorfer et al., 2010; Becker et al., 2012; Momeni et al., 2013) or of persuasiveness (Wei et al., 2016). However, information thought to be useful does not always prove so, and subjective ratings may be driven by biases. Reviews which convince you to watch a movie or buy a product do not guarantee that you will enjoy the product, and the most convincing or highest rated answers to questions on sites like Stack Overflow or Yahoo Answers are not always the most accurate. We explore recommendations in a unique dataset, an online forecasting competition, which offers a rare glimpse into both subjective and objective quality. In this competition, the users (forecasters) had a measurable goal — to fo"
D17-1250,D17-2010,1,0.819689,"ugh typically not over thousands of potential independent variables as we do here. Therefore, we also correct for multiple hypotheses by using the Benjamini-Hochberg procedure (Benjamini and Hochberg, 1995) over our significance tests. Differential language analysis allows us to observe and test the unique relationship between each feature and each metric, holding length constant. In addition, we use the difference between standardized metric scores to find the features that distinguish high quality comments in one metric versus another. All methods were implemented within the package, dlatk (Schwartz et al., 2017). 4.2 Quality Comment Features Figure 2 shows the n-grams most highly correlated with each of our quality metrics. Size indicates correlation strength while color represents overall frequency. Across both subjective ratings and objective update rates, we see discussion of news plays an important role (e.g. “news”, “article”, and “www.reuters.com”). We do not see the same from comments resulting in positive changes of forecaster accuracy (benefit), which seemed to be distinguished by negation (e.g. “no”, “unlikely”). For influence, we see other features indicating probabilistic reasoning (e.g."
D17-2010,D14-1082,0,0.0303662,"regression models in order to take in features as well as extra-linguistic information and output a continuous value predictions. These include variants on penalized linear regression: Ridge, 57 threshold which of the units of analyses are available (e.g. only include users with at least 1000 words or counties with 50,000 words). Integration of Popular Packages. DLATK sits on top of many popular open source packages used for data analysis and machine learning (scikitlearn (Pedregosa et al., 2011) and statsmodels (Seabold and Perktold, 2010)) as well as NLP specific packages (Stanford parser (Chen and Manning, 2014), TweetNLP (Gimpel et al., 2011) and NLTK (Loper and Bird, 2002)). LDA topics can be created with the Mallet (McCallum, 2002) interface. After creation these topics can then be used downstream in any standard DLATK analysis pipeline. The pip and conda package management systems control python library dependencies. (a) Age (pos) (b) Age (neg) (c) Educator (d) Technology Worker Figure 2: 1- to 3-grams correlated with age and occupation class. or language categories, such as ‘positive emotion’ or references to work and occupational terms). According to citations, the most popular tool is Linguist"
D17-2010,W16-0404,1,0.403608,"Missing"
D17-2010,W15-1205,1,0.858201,"Missing"
D17-2010,D14-1121,1,0.298185,".42 Agreeableness R = 0.35 Neuroticism R = 0.35 Temporal orientation (message-level) 3-way classif Acc = 0.72 Intensity & affect (message-level) Intensity R = 0.85 Affect R = 0.65 Mental health (user-level) PTSD AUC = 0.86 Depression AUC = 0.87 Degree of dprssn R = 0.39 Physical health (US county-level) Heart disease morR = 0.42 tality graphic or socioeconomic baselines. 6 Evaluations DLATK has been used as a data analysis platform in over 30 peer-reviewed publications, with venues ranging from general-interest (PLoS ONE: Schwartz et al., 2013b) to computer science methods proceedings (EMNLP: Sap et al., 2014) to psychology journals (JPSP: Park et al., 2015). The most straightforward use for DLATK is to provide insight on linguistic features associated with a given outcome, the differential language analyses presented in Schwartz et al. (2013b). Other works to primarily use DLATK for correlation-type analyses examine outcomes like age (Kern et al., 2014b), gendered language and stereotypes (Park et al., 2016; Carpenter et al., 2016b), and efficacy of app-based well-being interventions (Carpenter et al., 2016a). Another area one can evaluate the utility of DLATK is in building predictive models. Tab"
D17-2010,W14-3214,1,0.769622,"xamine outcomes like age (Kern et al., 2014b), gendered language and stereotypes (Park et al., 2016; Carpenter et al., 2016b), and efficacy of app-based well-being interventions (Carpenter et al., 2016a). Another area one can evaluate the utility of DLATK is in building predictive models. Table 1 summarizes some predictive models reported in peer-reviewed publications. DLATK works to create models at multiple scales, i.e., for predicting aspects of single messages (e.g., tweet-wise temporal orientation; Schwartz et al., 2015), or predicting user-level attributes (e.g., severity of depression; Schwartz et al., 2014), or predicting community-level health outcomes (e.g., heart disease mortality; Eichstaedt et al., 2015). 7 Source Sap et al. (2014) Park et al. (2015) Schwartz et al. (2015) Preot¸iuc-Pietro et al. (2016) Preot¸iuc-Pietro et al. (2015) Schwartz et al. (2014) Eichstaedt et al. (2015) Table 1: Survey of predictive model scores trained using DLATK in peer-reviewed publications. Scores reported are: R: Pearson correlation; Acc: accuracy; AUC: area under the ROC curve. Thomas Apicella, Masoud Rouhizadeh, Daniel Rieman, Selah Lynch and Daniel Preot¸iuc-Pietro. References Alan Agresti and Barbara Fi"
D17-2010,S13-1042,1,0.8385,"gression where, by assuming a dichotomous outcome, statistical significance tests are usually more accurate (Menard, 2002). Where controls are not needed, there are many other options, often less computationally complex, such as TF-IDF, Informative Dirichlet Prior,3 or classification accuracy metrics like Differential Language Analyses The prototypical use of DLATK is to perform differential language analysis – the identification of linguistic features which either (a) independently explain the most variance for continuous outcomes or (b) are individually most predictive of discrete outcomes (Schwartz et al., 2013b). Unlike predictive techniques where one seeks to produce outcome(s) given language (discussed next), here, the 2 Even in basic prediction methods, like linear regression, the relationship between each linguistic feature and the outcome is complex – dependent on the covariance structure between all the variables. DLA works in a univariate, perfeature fashion or with a limited set of control variables (e.g. age and gender when discriminating personality). 3 Bayesian approach to log-odds (Monroe et al., 2008). 56 Lasso, Elastic-Net, as well as non-linear techniques such as Extremely Random For"
D17-2010,N15-1044,1,0.849586,"Schwartz et al. (2013b). Other works to primarily use DLATK for correlation-type analyses examine outcomes like age (Kern et al., 2014b), gendered language and stereotypes (Park et al., 2016; Carpenter et al., 2016b), and efficacy of app-based well-being interventions (Carpenter et al., 2016a). Another area one can evaluate the utility of DLATK is in building predictive models. Table 1 summarizes some predictive models reported in peer-reviewed publications. DLATK works to create models at multiple scales, i.e., for predicting aspects of single messages (e.g., tweet-wise temporal orientation; Schwartz et al., 2015), or predicting user-level attributes (e.g., severity of depression; Schwartz et al., 2014), or predicting community-level health outcomes (e.g., heart disease mortality; Eichstaedt et al., 2015). 7 Source Sap et al. (2014) Park et al. (2015) Schwartz et al. (2015) Preot¸iuc-Pietro et al. (2016) Preot¸iuc-Pietro et al. (2015) Schwartz et al. (2014) Eichstaedt et al. (2015) Table 1: Survey of predictive model scores trained using DLATK in peer-reviewed publications. Scores reported are: R: Pearson correlation; Acc: accuracy; AUC: area under the ROC curve. Thomas Apicella, Masoud Rouhizadeh, Dan"
D17-2010,D16-1217,1,0.641174,"2 regularization, as well ensemble and gradient boosting techniques such as Extremely Randomized Trees. As with regression, techniques have been setup so as to leverage extra-linguistic information effectively either as additional predictors or controls to try to “-out-predict”. 5 Notable Functionality Linguistic information Because DLATK was designed to exploit the full power of social media, a special emoticon-aware tokenizer is used while also leveraging Python’s unicode capabilities. Though not specifically designed to be language independent, DLATK has been used in one non-English study (Smith et al., 2016). Predictive Methods As with traditional NLP, many social-scientific research objectives can be framed as prediction tasks, in which a model is fit to language features to predict an outcome. DLATK implements many available regression and classification tools, supplemented with feature selection functions for refining the feature space. A wide range of feature selection techniques have been empirically refined for accurate use in regression problems. Extra-linguistic information. Most functionality in DLATK is designed with extra-linguistic, also referred to as “outcomes”, in mind. Such inform"
D17-2010,P11-2008,0,0.0182113,"Missing"
D17-2010,W02-0109,0,0.223537,"inguistic information and output a continuous value predictions. These include variants on penalized linear regression: Ridge, 57 threshold which of the units of analyses are available (e.g. only include users with at least 1000 words or counties with 50,000 words). Integration of Popular Packages. DLATK sits on top of many popular open source packages used for data analysis and machine learning (scikitlearn (Pedregosa et al., 2011) and statsmodels (Seabold and Perktold, 2010)) as well as NLP specific packages (Stanford parser (Chen and Manning, 2014), TweetNLP (Gimpel et al., 2011) and NLTK (Loper and Bird, 2002)). LDA topics can be created with the Mallet (McCallum, 2002) interface. After creation these topics can then be used downstream in any standard DLATK analysis pipeline. The pip and conda package management systems control python library dependencies. (a) Age (pos) (b) Age (neg) (c) Educator (d) Technology Worker Figure 2: 1- to 3-grams correlated with age and occupation class. or language categories, such as ‘positive emotion’ or references to work and occupational terms). According to citations, the most popular tool is Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2015), foll"
D17-2010,W14-3207,0,\N,Missing
D18-1145,P16-1231,0,0.0160791,"quency distribution of the Dominance score in the Affective Norms in English Words (ANEW) (Bradley and Lang, 1999). Word2Vec Topics: We use word clusters built on top of Word2Vec, by Preot¸iuc-Pietro, Lampos and Aletras (2015). They built 200 open-ended word clusters by applying spectral clustering to the word-to-word similarity matrix from the neural embeddings Mikolov et. al (2013). Pronouns: We clustered all pronouns (except for possessives) into 1st-, 2nd-, and 3rd-person regardless of their syntactic role. 4.2 Syntax-based features We acquire dependency parses of our corpus by SyntaxNet (Andor et al., 2016) with Parsey McParseface model3 that produces universal dependencies in relation, head, dependent triples in CONLL format. We obtain subject-verb tuples (SVs) and subject-verb-object triples (SVOs) from the dependency trees. In our in-house evaluation on a random set of 100 Tweets, SyntaxNet with the Parsey McParseface model outperforms the Stanford Parser (Socher et al., 2013) on extracting SVs and SVOs from social media (P=.75, R= 68, TN Rate =.90 for the former; P=.51, R= .55, TN Rate =.80 for the latter). SyntaxNet is also a better tool for our purpose compared to the Tweebo 2 https://dlat"
D18-1145,D16-1066,0,0.163723,"is easy to identify, it is more challenging to label it is internally or externally controlled, with lexical features outperforming syntactic features at the task. Our findings could have important implications for studying selfexpression in social media. 1 Introduction Language is a form of action, and written occurrences constitute a performance of power and control (Fairclough, 2001). Research in natural language processing has long focused on distilling the constituents of writing that conveys authority (Danescu-Niculescu-Mizil et al., 2012), dominance (Bradley and Lang, 1999), dogmatism (Fast and Horvitz, 2016), expertise (Levy and Potts, 2015) and politeness (Danescu-Niculescu-Mizil et al., 2012). These studies have shown how authors’ use of certain lexical and syntactic patterns achieve specific rhetorical effects. In this study, we contribute to the growing literature with an analysis of how individuals express control, and compare the insights and predictive power obtained from both, lexical and syntactic features. We operationalize the key aspects of locus of control described by existing psychology theories ∗ Masoud Rouhizadeh & Kokil Jaidka co-lead this work. (Rotter, 1966) to identify an ext"
D18-1145,P18-2032,1,0.816179,"on). Internal control is often associated with causing a given event or doing something that is clearly a choice. The external control language, on the other hand, is characterized by lack of intention or awareness, or by concrete mention of being controlled by others. It is usually lined to describing an out-of-control event, or something that is not a choice Organizations and governments are attempting to better understand issues of locus of control and self-efficacy, which are closely related with physical health and job performance (Marmot et al., 1991; Harter et al., 2003). The study by (Jaidka et al., 2018b) offered to explore the relationship between locus of control (measured through surveys) and the Big 5 personality taxonomy (John and Srivastava, 1999), and touched briefly upon the content-related linguistic signals. However, the present study is more interested in comparing the lexical (aka open-vocabulary or bag-of-words) and syntactic signals of control that are embedded in language. If deployed on a large scale with the informed consent of the authors, it may allow unobtrusive, cost-effective estimates of well-being1 . Signals within the language should provide insight into the cognitiv"
D18-1145,D14-1108,0,0.035318,"Missing"
D18-1145,N13-1039,0,0.0193918,"Missing"
D18-1145,P15-1169,0,0.0768304,"Missing"
D18-1145,I17-1077,1,0.843805,"“miss” or “feel” are correlated with lack of control of the surroundings. A caveat of using language-based models is that their association with traits is correlational, not causal. Furthermore, the differences in platform affordances (Jaidka et al., 2018c) and diachronic drifts in language use over time (Jaidka et al., 2018a) imply that language models to identify control may need domain adaptation before they are applied on other corpora, language from other time periods and other social media platforms, as well as posthoc domain adaptation before they can scale to measure community traits (Rieman et al., 2017). Existing psychological theories are mainly based on self-expressed and self-perceived locus of control in questionnaires, but they may be susceptible to self-report biases. Instead, we demonstrate that some of these constructs can reliably be extracted from language samples, such as social media posts, which are unsolicited selfexpressions of control. Internal locus of control has been argued to be important for mental and physical health, and to characterize well-run (“empowered”) work teams; We hope that the model presented here can be used–with appropriate consent and privacy–for unobtrus"
D18-1145,D16-1219,1,0.860524,"5, R= 68, TN Rate =.90 for the former; P=.51, R= .55, TN Rate =.80 for the latter). SyntaxNet is also a better tool for our purpose compared to the Tweebo 2 https://dlatk.wwbp.org https://github.com/tensorflow/models/ tree/master/research/syntaxnet 3 Parser (Kong et al., 2014), that only provides dependency graphs and not the relations. Verb predicate features: We identify the verb classes using (a) linguistically-driven Levin’s Classes (Lev) (Levin, 1993), and (b) an in-house manual verb clustering on the most frequent 130 verb into 40 semantic classes (M). Inspired by the previous research (Rouhizadeh et al., 2016), we extract five sets of dependency-driven verb predicate features. (1) Pronouns-SVO: occurrence of 1st, 2nd, and 3rd person pronouns in the subject/object positions, (2) VerbCat-1-2-3PP occurrence of 1st, 2nd, and 3rd person pronouns in the subject/object positions of each verb category (from the Levin’s or our own verb classes), (3) VerbCat-1PP: occurrence of 1st or non-1st pronouns (i.e. self-reference ratio) in the subject/object positions of each verb category, (4) VerbCat-SVO: all words in the subject/object positions of each verb category, and (5) VerbCatall categories of all the verbs"
D18-1145,D17-2010,1,0.848028,"Missing"
D18-1145,P13-1045,0,0.015675,"(2013). Pronouns: We clustered all pronouns (except for possessives) into 1st-, 2nd-, and 3rd-person regardless of their syntactic role. 4.2 Syntax-based features We acquire dependency parses of our corpus by SyntaxNet (Andor et al., 2016) with Parsey McParseface model3 that produces universal dependencies in relation, head, dependent triples in CONLL format. We obtain subject-verb tuples (SVs) and subject-verb-object triples (SVOs) from the dependency trees. In our in-house evaluation on a random set of 100 Tweets, SyntaxNet with the Parsey McParseface model outperforms the Stanford Parser (Socher et al., 2013) on extracting SVs and SVOs from social media (P=.75, R= 68, TN Rate =.90 for the former; P=.51, R= .55, TN Rate =.80 for the latter). SyntaxNet is also a better tool for our purpose compared to the Tweebo 2 https://dlatk.wwbp.org https://github.com/tensorflow/models/ tree/master/research/syntaxnet 3 Parser (Kong et al., 2014), that only provides dependency graphs and not the relations. Verb predicate features: We identify the verb classes using (a) linguistically-driven Levin’s Classes (Lev) (Levin, 1993), and (b) an in-house manual verb clustering on the most frequent 130 verb into 40 semant"
D18-1148,W15-1201,0,0.195395,"Missing"
D18-1148,P13-1098,1,0.882773,"Missing"
D18-1148,P12-3005,0,0.0944372,"Missing"
D18-1148,W16-4320,0,0.304397,"Missing"
D18-1148,D17-2010,1,0.855028,"Missing"
D18-1507,N16-3018,0,0.119754,"16; Abdul-Mageed and Ungar, 2017; Mohammad and Bravo-Marquez, 2017a; Buechel and Hahn, 2017, 2018a,b). Yet such approaches, often rooted in psychological theory, also turned out to be more challenging in respect to annotation and modeling (Strapparava and Mihalcea, 2007). Surprisingly, one of the most valuable affective phenomena for improving human-machine interaction has received surprisingly little attention: Empathy. Prior work focused mostly on spoken dialogue, commonly addressing conversational agents, psychological interventions, or call center applications (McQuiggan and Lester, 2007; Fung et al., 2016; P´erez-Rosas et al., 2017; Alam et al., 2017). In contrast, to the best of our knowledge, only three contributions (Xiao et al., 2012; Gibson et al., 2015; Khanpour et al., 2017) previously addressed text-based empathy prediction1 (see Section 4 for details). Yet, all of them are limited in three ways: (a) neither of their corpora are available leaving the NLP community without shared data, (b) empathy ratings were provided by others than the one actually experiencing it which qualifies only as a weak form of ground truth, and (c) their notion of empathy is quite basic, falling short of curr"
D18-1507,I17-2042,0,0.109307,"ut to be more challenging in respect to annotation and modeling (Strapparava and Mihalcea, 2007). Surprisingly, one of the most valuable affective phenomena for improving human-machine interaction has received surprisingly little attention: Empathy. Prior work focused mostly on spoken dialogue, commonly addressing conversational agents, psychological interventions, or call center applications (McQuiggan and Lester, 2007; Fung et al., 2016; P´erez-Rosas et al., 2017; Alam et al., 2017). In contrast, to the best of our knowledge, only three contributions (Xiao et al., 2012; Gibson et al., 2015; Khanpour et al., 2017) previously addressed text-based empathy prediction1 (see Section 4 for details). Yet, all of them are limited in three ways: (a) neither of their corpora are available leaving the NLP community without shared data, (b) empathy ratings were provided by others than the one actually experiencing it which qualifies only as a weak form of ground truth, and (c) their notion of empathy is quite basic, falling short of current and past theory. 1 Psychological studies commonly distinguish between state and trait empathy. While the former construct describes the amount of empathy a person experiences a"
D18-1507,L18-1008,0,0.0436406,"elopment experiments. Then, the main experiment was conducted using 10-fold crossvalidation (CV), providing each model with identical train-test splits to increase reliability. The dev set was excluded for the CV experiment. Model performance is measured in terms of Pearson correlation r between predicted values and the human gold ratings. Thus, we phrase the prediction of empathy and distress as regression problems. The input to our models is based on word embeddings, namely the publicly available FastText embeddings which were trained on Common Crawl (≈600B tokens) (Bojanowski et al., 2017; Mikolov et al., 2018). Ridge. Our first approach is Ridge regression, an `2 -regularized version of linear regression. The centroid of the word embeddings of the words in a message is used as features (embedding centroid). The regularization coefficient α is automatically chosen from {1, .5, .1, ..., .0001} during training. FFN. Our second approach is a Feed-Forward Net with two hidden layers (256 and 128 units, respectively) with ReLU activation. Again, the embedding centroid is used as features. CNN. The last approach is a Convolutional Neural Net.5 We use a single convolutional layer with filter sizes 1 to 3, e"
D18-1507,W17-5205,0,0.159051,"e actually experiencing it which qualifies only as a weak form of ground truth, and (c) their notion of empathy is quite basic, falling short of current and past theory. 1 Psychological studies commonly distinguish between state and trait empathy. While the former construct describes the amount of empathy a person experiences as a direct result of encountering a given stimulus, the latter refers to how empathetic one is on average and across situations. This studies exclusively addresses state empathy. For a contribution addressing trait empathy from an NLP perspective, see AbdulMageed et al. (2017). 4758 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4758–4765 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics In this contribution we present the first publicly available gold standard for text-based empathy prediction. It is constructed using a novel annotation methodology which reliably captures empathy assessments via multi-item scales. The corpus as well as our work as a whole is also unique in being—to the best of our knowledge—the first computational approach differentiating multiple types"
D18-1507,S17-1007,0,0.340919,"survey described in Section 2, and provided psychological background knowledge. Sven Buechel was responsible for corpus creation, data analysis, and modeling. The technical set-up of the crowdsourcing task and the survey was done jointly by both first authors. †Work conducted while being at the University of Pennsylvania. or evaluation, usually in social media postings or product reviews (Rosenthal et al., 2017; Socher et al., 2013). Only very recently, researchers started exploring more sophisticated models of human emotion on a larger scale (Wang et al., 2016; Abdul-Mageed and Ungar, 2017; Mohammad and Bravo-Marquez, 2017a; Buechel and Hahn, 2017, 2018a,b). Yet such approaches, often rooted in psychological theory, also turned out to be more challenging in respect to annotation and modeling (Strapparava and Mihalcea, 2007). Surprisingly, one of the most valuable affective phenomena for improving human-machine interaction has received surprisingly little attention: Empathy. Prior work focused mostly on spoken dialogue, commonly addressing conversational agents, psychological interventions, or call center applications (McQuiggan and Lester, 2007; Fung et al., 2016; P´erez-Rosas et al., 2017; Alam et al., 2017)."
D18-1507,P17-1131,0,0.0835719,"Missing"
D18-1507,S17-2088,0,0.1424,"e actually experiencing it which qualifies only as a weak form of ground truth, and (c) their notion of empathy is quite basic, falling short of current and past theory. 1 Psychological studies commonly distinguish between state and trait empathy. While the former construct describes the amount of empathy a person experiences as a direct result of encountering a given stimulus, the latter refers to how empathetic one is on average and across situations. This studies exclusively addresses state empathy. For a contribution addressing trait empathy from an NLP perspective, see AbdulMageed et al. (2017). 4758 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4758–4765 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics In this contribution we present the first publicly available gold standard for text-based empathy prediction. It is constructed using a novel annotation methodology which reliably captures empathy assessments via multi-item scales. The corpus as well as our work as a whole is also unique in being—to the best of our knowledge—the first computational approach differentiating multiple types"
D18-1507,D17-2010,1,0.865835,"Missing"
D18-1507,D13-1170,0,0.00350658,"cally only distinguishing between positive and negative feeling * These authors contributed equally to this work. Anneke Buffone designed and supervised the crowdsourcing task and the survey described in Section 2, and provided psychological background knowledge. Sven Buechel was responsible for corpus creation, data analysis, and modeling. The technical set-up of the crowdsourcing task and the survey was done jointly by both first authors. †Work conducted while being at the University of Pennsylvania. or evaluation, usually in social media postings or product reviews (Rosenthal et al., 2017; Socher et al., 2013). Only very recently, researchers started exploring more sophisticated models of human emotion on a larger scale (Wang et al., 2016; Abdul-Mageed and Ungar, 2017; Mohammad and Bravo-Marquez, 2017a; Buechel and Hahn, 2017, 2018a,b). Yet such approaches, often rooted in psychological theory, also turned out to be more challenging in respect to annotation and modeling (Strapparava and Mihalcea, 2007). Surprisingly, one of the most valuable affective phenomena for improving human-machine interaction has received surprisingly little attention: Empathy. Prior work focused mostly on spoken dialogue,"
D18-1507,P16-2037,0,0.0338604,"ned and supervised the crowdsourcing task and the survey described in Section 2, and provided psychological background knowledge. Sven Buechel was responsible for corpus creation, data analysis, and modeling. The technical set-up of the crowdsourcing task and the survey was done jointly by both first authors. †Work conducted while being at the University of Pennsylvania. or evaluation, usually in social media postings or product reviews (Rosenthal et al., 2017; Socher et al., 2013). Only very recently, researchers started exploring more sophisticated models of human emotion on a larger scale (Wang et al., 2016; Abdul-Mageed and Ungar, 2017; Mohammad and Bravo-Marquez, 2017a; Buechel and Hahn, 2017, 2018a,b). Yet such approaches, often rooted in psychological theory, also turned out to be more challenging in respect to annotation and modeling (Strapparava and Mihalcea, 2007). Surprisingly, one of the most valuable affective phenomena for improving human-machine interaction has received surprisingly little attention: Empathy. Prior work focused mostly on spoken dialogue, commonly addressing conversational agents, psychological interventions, or call center applications (McQuiggan and Lester, 2007; Fu"
E17-2090,D16-1057,0,0.0505736,"rity has shown to slightly improve results (Recchia and Louwerse, 2015). A different approach to rating prediction is based on graph methods inspired by label propagation (Wang et al., 2016). In a related task of adjective intensity prediction, Sharma et al. (2015) also use distributional methods, but their work is restricted to discrete categories and relative ranking within each semantic property. Another related task to affective norm prediction is building sentiment and polarity lexicons (Turney, 2002; Turney and Littman, 2003; Velikovich et al., 2010; Yih et al., 2012; Tang et al., 2014; Hamilton et al., 2016). However, polarity is assigned to words in order to determine if a text is subjective and its sentiment, which is slightly different to word-level affective norms e.g., ‘sunshine’ is an objective word (neural polarity), but has a positive affective rating. Our approach builds upon recent work in learning word representations and enriches these by integrating a set of existing ratings. Including this information allows our method to differentiate between words that are semantically similar, but on opposite sides of the rating scale. Results show that our automatic word prediction approach obta"
E17-2090,N16-1018,0,0.0547579,"Missing"
E17-2090,L16-1413,0,0.0648969,"Missing"
E17-2090,D14-1162,0,0.0824644,"improvement over the mean baseline and K-NN are obtained on valence. This is particularly intuitive, as opposite valence words are usually antonyms and are more useful to split apart compared to low/high arousal words, which might also not be as distributionally similar to each other. In all cases, the signed clustering step improves rating prediction significantly over vanilla spectral clustering (NCut), highlighting the utility of signed clustering. Out of the baseline methods, none consistently outperforms the others. In addition, we also used English 300 dimensional GloVe word embeddings (Pennington et al., 2014) instead of word2vec, which led to similar results using SNCut where for valence RMSE= 0.82, ρ = 0.76 and arousal RMSE= 0.73 and ρ = 0.56. As an upper bound comparison, Warriner et al. (2013) reported that the human inter-annotator agreements are 0.85 to 0.97, and 0.56 to 0.76 for valence and arousal respectively across various languages. Results We compare the proposed method with other baselines and approaches which assign to the unrated word: 1. the mean of the available ratings (Mean); 2. the average of its k nearest rated neighbors in the semantic space – the method introduced in (Bestgen"
E17-2090,P15-2096,0,0.0521073,"Missing"
E17-2090,P15-2004,0,0.029274,"Missing"
E17-2090,W16-0400,0,0.266603,"Missing"
E17-2090,P14-1146,0,0.250454,"Orthographic similarity has shown to slightly improve results (Recchia and Louwerse, 2015). A different approach to rating prediction is based on graph methods inspired by label propagation (Wang et al., 2016). In a related task of adjective intensity prediction, Sharma et al. (2015) also use distributional methods, but their work is restricted to discrete categories and relative ranking within each semantic property. Another related task to affective norm prediction is building sentiment and polarity lexicons (Turney, 2002; Turney and Littman, 2003; Velikovich et al., 2010; Yih et al., 2012; Tang et al., 2014; Hamilton et al., 2016). However, polarity is assigned to words in order to determine if a text is subjective and its sentiment, which is slightly different to word-level affective norms e.g., ‘sunshine’ is an objective word (neural polarity), but has a positive affective rating. Our approach builds upon recent work in learning word representations and enriches these by integrating a set of existing ratings. Including this information allows our method to differentiate between words that are semantically similar, but on opposite sides of the rating scale. Results show that our automatic word"
E17-2090,L16-1652,0,0.0198316,"tional hypothesis – a word is characterised by the company it keeps – to represent words as low dimensional numeric vectors using large text corpora (Harris, 1954; Firth, 1957). We use the word2vec algorithm (Mikolov et al., 2013), without loss of generality, to generate word vectors as it is arguably the most popular model out of the variety of existing word representations. The word2vec embeddings for English and Spanish have 300 dimensions and are trained on the Gigaword corpora (Parker et al., 2011; Mendonca et al., 2011). For Dutch, we use the word2vec embeddings with 320 dimensions from Tulkens et al. (2016). All words in the embeddings have minimal tokenization, with no additional stemming or lowercasing. Our vocabulary consists of the words that have ratings on either scale. Data Our gold standard data is represented by affective norms of words. The ratings are obtained by asking human coders to indicate the emotional reaction evoked by specific words on 9-point scales: valence (1–negative to 9–positive) and arousal (from 1–calm to 9–excited). Originally, word ratings were computed using trained raters in a laboratory setup. The Affective Norms for English Words (Bradley and Lang, 1999) – ANEW"
E17-2090,P02-1053,0,0.0246671,"nclude this as it was not present in all data sets. the accuracy of these methods. Orthographic similarity has shown to slightly improve results (Recchia and Louwerse, 2015). A different approach to rating prediction is based on graph methods inspired by label propagation (Wang et al., 2016). In a related task of adjective intensity prediction, Sharma et al. (2015) also use distributional methods, but their work is restricted to discrete categories and relative ranking within each semantic property. Another related task to affective norm prediction is building sentiment and polarity lexicons (Turney, 2002; Turney and Littman, 2003; Velikovich et al., 2010; Yih et al., 2012; Tang et al., 2014; Hamilton et al., 2016). However, polarity is assigned to words in order to determine if a text is subjective and its sentiment, which is slightly different to word-level affective norms e.g., ‘sunshine’ is an objective word (neural polarity), but has a positive affective rating. Our approach builds upon recent work in learning word representations and enriches these by integrating a set of existing ratings. Including this information allows our method to differentiate between words that are semantically s"
E17-2090,N10-1119,0,0.0112948,"data sets. the accuracy of these methods. Orthographic similarity has shown to slightly improve results (Recchia and Louwerse, 2015). A different approach to rating prediction is based on graph methods inspired by label propagation (Wang et al., 2016). In a related task of adjective intensity prediction, Sharma et al. (2015) also use distributional methods, but their work is restricted to discrete categories and relative ranking within each semantic property. Another related task to affective norm prediction is building sentiment and polarity lexicons (Turney, 2002; Turney and Littman, 2003; Velikovich et al., 2010; Yih et al., 2012; Tang et al., 2014; Hamilton et al., 2016). However, polarity is assigned to words in order to determine if a text is subjective and its sentiment, which is slightly different to word-level affective norms e.g., ‘sunshine’ is an objective word (neural polarity), but has a positive affective rating. Our approach builds upon recent work in learning word representations and enriches these by integrating a set of existing ratings. Including this information allows our method to differentiate between words that are semantically similar, but on opposite sides of the rating scale."
E17-2090,K15-1026,0,0.0421246,"Missing"
E17-2090,D15-1300,0,0.0227713,"s (Stadthagen-Gonzalez et al., 2016). In our experiments, we use valence and arousal ratings for these three languages. Although some affective norms contain a third dimension of dominance (from feeling dominated to feeling dominant), we choose not to include this as it was not present in all data sets. the accuracy of these methods. Orthographic similarity has shown to slightly improve results (Recchia and Louwerse, 2015). A different approach to rating prediction is based on graph methods inspired by label propagation (Wang et al., 2016). In a related task of adjective intensity prediction, Sharma et al. (2015) also use distributional methods, but their work is restricted to discrete categories and relative ranking within each semantic property. Another related task to affective norm prediction is building sentiment and polarity lexicons (Turney, 2002; Turney and Littman, 2003; Velikovich et al., 2010; Yih et al., 2012; Tang et al., 2014; Hamilton et al., 2016). However, polarity is assigned to words in order to determine if a text is subjective and its sentiment, which is slightly different to word-level affective norms e.g., ‘sunshine’ is an objective word (neural polarity), but has a positive aff"
E17-2090,D12-1111,0,0.0605911,"Missing"
E17-2090,N15-1184,0,\N,Missing
E17-2090,N13-1090,0,\N,Missing
E17-2090,D13-1167,0,\N,Missing
E17-2090,W16-0404,1,\N,Missing
I17-1077,E14-1043,0,0.0639917,"Missing"
I17-1077,D11-1120,0,0.109168,"Missing"
I17-1077,P07-1033,0,0.160665,"Missing"
I17-1077,P15-1169,0,0.113881,"Missing"
I17-1077,P11-2071,0,0.0366988,"Missing"
I17-1077,W10-2608,0,0.0373361,"Missing"
L16-1591,N15-1171,0,0.0823246,"ile conservatives hate the anti-authoritarian, public disrespect of legitimate authority figures. We perform a large scale analysis of news from partisan news sources across a wide variety of issues. To date, political orientation prediction has been focused mostly on predicting the leaning of individual users (Rao et al., 2010; Pennacchiotti and Popescu, 2011; Al Zamal et al., 2012; Cohen and Ruths, 2013; Volkova et al., 2014; Volkova et al., 2015; Sylwester and Purver, 2015), mostly on social media. While recently, computational approaches have been used to study framing (Tsur et al., 2015; Baumer et al., 2015; Card et al., 2015), little empirical work has been performed to study deeper psychological factors that cause this linguistic divergence (Yano et al., 2010) and analyses have mostly focused on data from political actors (Nguyen et al., 2015). Social psychology research suggests that differences in the frames used follows the different values that the groups adhere to, and that these values span different issues. Moral Foundations Theory (Haidt and Joseph, 2004; Haidt and Graham, 2007; Graham et al., 2009) was developed to model and explain these differences. Under this theory, there are a sm"
L16-1591,P15-2072,0,0.326,"e the anti-authoritarian, public disrespect of legitimate authority figures. We perform a large scale analysis of news from partisan news sources across a wide variety of issues. To date, political orientation prediction has been focused mostly on predicting the leaning of individual users (Rao et al., 2010; Pennacchiotti and Popescu, 2011; Al Zamal et al., 2012; Cohen and Ruths, 2013; Volkova et al., 2014; Volkova et al., 2015; Sylwester and Purver, 2015), mostly on social media. While recently, computational approaches have been used to study framing (Tsur et al., 2015; Baumer et al., 2015; Card et al., 2015), little empirical work has been performed to study deeper psychological factors that cause this linguistic divergence (Yano et al., 2010) and analyses have mostly focused on data from political actors (Nguyen et al., 2015). Social psychology research suggests that differences in the frames used follows the different values that the groups adhere to, and that these values span different issues. Moral Foundations Theory (Haidt and Joseph, 2004; Haidt and Graham, 2007; Graham et al., 2009) was developed to model and explain these differences. Under this theory, there are a small number of basic"
L16-1591,P15-1139,0,0.0581408,"has been focused mostly on predicting the leaning of individual users (Rao et al., 2010; Pennacchiotti and Popescu, 2011; Al Zamal et al., 2012; Cohen and Ruths, 2013; Volkova et al., 2014; Volkova et al., 2015; Sylwester and Purver, 2015), mostly on social media. While recently, computational approaches have been used to study framing (Tsur et al., 2015; Baumer et al., 2015; Card et al., 2015), little empirical work has been performed to study deeper psychological factors that cause this linguistic divergence (Yano et al., 2010) and analyses have mostly focused on data from political actors (Nguyen et al., 2015). Social psychology research suggests that differences in the frames used follows the different values that the groups adhere to, and that these values span different issues. Moral Foundations Theory (Haidt and Joseph, 2004; Haidt and Graham, 2007; Graham et al., 2009) was developed to model and explain these differences. Under this theory, there are a small number of basic moral values that people intuitively support, emerging out of both cultural and evolutionary factors, and individuals differ in the relative level to which they endorse these values. The five moral foundations are: care/har"
L16-1591,P15-1157,0,0.104171,"ed social group, while conservatives hate the anti-authoritarian, public disrespect of legitimate authority figures. We perform a large scale analysis of news from partisan news sources across a wide variety of issues. To date, political orientation prediction has been focused mostly on predicting the leaning of individual users (Rao et al., 2010; Pennacchiotti and Popescu, 2011; Al Zamal et al., 2012; Cohen and Ruths, 2013; Volkova et al., 2014; Volkova et al., 2015; Sylwester and Purver, 2015), mostly on social media. While recently, computational approaches have been used to study framing (Tsur et al., 2015; Baumer et al., 2015; Card et al., 2015), little empirical work has been performed to study deeper psychological factors that cause this linguistic divergence (Yano et al., 2010) and analyses have mostly focused on data from political actors (Nguyen et al., 2015). Social psychology research suggests that differences in the frames used follows the different values that the groups adhere to, and that these values span different issues. Moral Foundations Theory (Haidt and Joseph, 2004; Haidt and Graham, 2007; Graham et al., 2009) was developed to model and explain these differences. Under this t"
L16-1591,P14-1018,0,0.125971,"Missing"
L16-1591,W10-0723,0,0.0237866,"sources across a wide variety of issues. To date, political orientation prediction has been focused mostly on predicting the leaning of individual users (Rao et al., 2010; Pennacchiotti and Popescu, 2011; Al Zamal et al., 2012; Cohen and Ruths, 2013; Volkova et al., 2014; Volkova et al., 2015; Sylwester and Purver, 2015), mostly on social media. While recently, computational approaches have been used to study framing (Tsur et al., 2015; Baumer et al., 2015; Card et al., 2015), little empirical work has been performed to study deeper psychological factors that cause this linguistic divergence (Yano et al., 2010) and analyses have mostly focused on data from political actors (Nguyen et al., 2015). Social psychology research suggests that differences in the frames used follows the different values that the groups adhere to, and that these values span different issues. Moral Foundations Theory (Haidt and Joseph, 2004; Haidt and Graham, 2007; Graham et al., 2009) was developed to model and explain these differences. Under this theory, there are a small number of basic moral values that people intuitively support, emerging out of both cultural and evolutionary factors, and individuals differ in the relati"
N06-1016,W02-0817,0,0.0354071,"ka, 1998). In other words, if c and c &apos; are the two most likely categories for example xn , the margin is measured as follows: (2) M n = Pr(c |x n ) − Pr(c&apos; |x n ) In this case Algorithm 1 would rank examples by increasing values of margin, with the smallest value at the top of the ranking. Using either method of uncertainty sampling, the computational cost of picking an example from T candidates is: O(TD) where D is the number of model parameters. 2.3 Related Work To our best knowledge, there have been very few attempts to apply active learning to WSD in the literature (Fujii and Inui, 1999; Chklovski and Mihalcea, 2002; Dang, 2004). Fujii and Inui (1999) developed an example sampling method for their example-based WSD system in the active learning of verb senses in a pool-based setting. Unlike the uncertainty sampling methods (such as the two methods we used), their method did not select examples for which the system had the minimal certainty. Rather, it selected the examples such that after training using those examples the system would be most certain about its predictions on the rest of the unlabeled examples in the next iteration. This sample selection criterion was enforced by calculating a training ut"
N06-1016,P04-1075,0,0.391246,"ive learning Data analysis on the learning process, where there is an abundant supply of unlabeled based on both instance and feature levels, data, but where the labeling process is expensive. suggests that a careful treatment of feature In NLP problems such as text classification (Lewis extraction is important for the active and Gale, 1994; McCallum and Nigam, 1998), learning to be useful for WSD. The statistical parsing (Tang et al., 2002), information overfitting phenomena that occurred extraction (Thompson et al., 1999), and named during the active learning process are entity recognition (Shen et al., 2004), pool-based identified as classic overfitting in machine active learning has produced promising results. learning based on the data analysis. This paper presents our experiments in applying two active learning methods, a min-margin based 1 Introduction method and a Shannon-entropy based one, to the Corpus-based methods for word sense task of the disambiguation of English verb senses. disambiguation (WSD) have gained popularity in The contribution of our work is not only in recent years. As evidenced by the SENSEVAL demonstrating that these methods work well for the exercises (http://www.sense"
N06-1016,P02-1016,0,0.226056,"Missing"
N06-1016,J98-4002,0,\N,Missing
N06-1016,N06-2015,1,\N,Missing
N06-1016,I05-1081,1,\N,Missing
N13-1015,H91-1060,0,0.284786,"e a coarse-to-fine algorithm for parsing with either the EM or spectral derived grammar: a PCFG without latent states is used to calculate marginals, and dynamic programming items are removed if their marginal probability is lower than some threshold (0.00005 in our experiments). For simplicity the parser takes part-of-speech tagged sentences as input. We use automatically tagged data from Turbo Tagger (Martins et al., 2010). The tagger is used to tag both the development data and the test data. The tagger was retrained on sections 2–21. We use the F1 measure according to the Parseval metric (Black et al., 1991). For the spectral algorithm, we tuned the smoothing parameters using section 0 of the treebank. 4.1 Comparison to EM: Accuracy We compare models trained using EM and the spectral algorithm using values for m in {8, 16, 24, 32}.5 For EM, we found that it was important to use development data to choose the number of iterations of training. We train the models for 100 iterations, then test accuracy of the model on section 22 (development data) at different iteration numbers. Table 1 shows that a peak level of accuracy is reached for all values of m, other than m = 8, at iteration 20–30, with som"
N13-1015,W08-2102,1,0.366224,"Missing"
N13-1015,P05-1022,0,0.193929,"Missing"
N13-1015,P12-1024,1,0.782914,"iments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a single pass over the data where parameter values are cal"
N13-1015,J03-4003,1,0.155015,"Missing"
N13-1015,D12-1019,1,0.792489,"Missing"
N13-1015,P96-1024,0,0.481709,", 2, . . . n}. 149 The parsing problem is to take a sentence as input, and produce a skeletal tree as output. A standard method for parsing with L-PCFGs is as follows. First, for a given input sentence x1 . . . xn , for any triple (a, i, j) such that a ∈ N and 1 ≤ i ≤ j ≤ n, the marginal µ(a, i, j) is defined as X p(t) (1) µ(a, i, j) = t:(a,i,j)∈t where the sum is over all skeletal trees t for x1 . . . xn that include non-terminal a spanning words xi . . . xj . A variant of the inside-outside algorithm can be used to calculate marginals. Once marginals have been computed, Goodman’s algorithm (Goodman, 1996) is used to find P arg maxt (a,i,j)∈t µ(a, i, j).3 2.2 The Spectral Learning Algorithm We now give a sketch of the spectral learning algorithm. The training data for the algorithm is a set of skeletal trees. The output from the algorithm is a set of parameter estimates for t, q and π (more precisely, the estimates are estimates of linearly transformed parameters; see Cohen et al. (2012) and section 2.3.1 for more details). The algorithm takes two inputs in addition to the set of skeletal trees. The first is an integer m, specifying the number of latent state values in the model. Typically m is"
N13-1015,P12-1046,0,0.0332165,"Missing"
N13-1015,P03-1054,0,0.0369327,"D∗ N. • The two-level and three-level rule fragments above the foot node. In the above example these features would be VP V S NP D∗ NP N VP V NP D∗ N • The label of the foot node, together with the label of its parent. In the above example this is (D, NP). • The label of the foot node, together with the label of its parent and grandparent. In the above example this is (D, NP, VP). • The part of speech of the first head word along the path from the foot of the outside tree to the root of the tree which is different from the head node of 4 We use the English head rules from the Stanford parser (Klein and Manning, 2003). 152 the foot node. In the above example this is N. • The width of the span to the left of the foot node, paired with the label of the foot node. • The width of the span to the right of the foot node, paired with the label of the foot node. Scaling of features. The features defined above are almost all binary valued features. We scale the features in the following way. For each feature φi (t), define count(i) to be the number of times the feature is equal to 1, and M to be the number of training examples. The feature is then redefined to be s M φi (t) × count(i) + κ where κ is a smoothing ter"
N13-1015,E12-1042,0,0.573193,"Missing"
N13-1015,J93-2004,0,0.0490646,"ds to the lowest probability parse being output under the model). We suspect that this is because in some cases a dominant parameter has had its sign flipped due to sampling error; more theoretical and empirical work is required in fully understanding this issue. 4 Experiments In this section we describe parsing experiments using the L-PCFG estimation method. We give comparisons to the EM algorithm, considering both speed of training, and accuracy of the resulting model; we also give experiments investigating the various choices described in the previous section. We use the Penn WSJ treebank (Marcus et al., 1993) for our experiments. Sections 2–21 were used as training data, and sections 0 and 22 were used as development data. Section 23 is used as the final test set. We binarize the trees in training data using the same method as that described in Petrov et al. (2006). For example, the non-binary rule VP → V NP PP SBAR would be converted to the structure [VP [@VP [@VP V NP] PP] SBAR] where @VP is a new symbol in the grammar. Unary rules are removed by collapsing non-terminal chains: for example the unary rule S → VP would be replaced by a single non-terminal S|VP. For the EM algorithm we use the init"
N13-1015,D10-1004,0,0.0243085,"example the unary rule S → VP would be replaced by a single non-terminal S|VP. For the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). For efficiency, we use a coarse-to-fine algorithm for parsing with either the EM or spectral derived grammar: a PCFG without latent states is used to calculate marginals, and dynamic programming items are removed if their marginal probability is lower than some threshold (0.00005 in our experiments). For simplicity the parser takes part-of-speech tagged sentences as input. We use automatically tagged data from Turbo Tagger (Martins et al., 2010). The tagger is used to tag both the development data and the test data. The tagger was retrained on sections 2–21. We use the F1 measure according to the Parseval metric (Black et al., 1991). For the spectral algorithm, we tuned the smoothing parameters using section 0 of the treebank. 4.1 Comparison to EM: Accuracy We compare models trained using EM and the spectral algorithm using values for m in {8, 16, 24, 32}.5 For EM, we found that it was important to use development data to choose the number of iterations of training. We train the models for 100 iterations, then test accuracy of the mo"
N13-1015,P05-1010,0,0.673932,"uarantees of sample complexity). This paper describes experiments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a"
N13-1015,N07-1051,0,0.036876,"Missing"
N13-1015,P06-1055,0,0.331212,"lexity). This paper describes experiments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a single pass over the d"
N13-4005,E12-1042,0,\N,Missing
N13-4005,J92-4003,0,\N,Missing
N13-4005,P06-1055,0,\N,Missing
N13-4005,P05-1010,0,\N,Missing
N13-4005,W99-0613,1,\N,Missing
N13-4005,D12-1019,1,\N,Missing
N13-4005,N13-1015,1,\N,Missing
N13-4005,W97-0309,0,\N,Missing
N13-4005,P96-1024,0,\N,Missing
N15-1044,N13-1121,0,0.0605509,"Missing"
N15-1044,D11-1120,0,0.0643487,"erform traditional computational linguistics tasks, while others focus particularly on predicting user attributes. User network information has been used for tweet summarization or filtering (Panigrahy et al., 2012; Chang et al., 2013; Feng and Wang, 2013). Others utilize psychological knowledge about people, such as exploiting the human tendency to report more positive extreme feelings than negative in order to improve on sentiment analysis (Guerra et al., 2014). Toward attribute prediction, a large proportion of works have focused on demographics (Argamon et al., 2009; Goswami et al., 2009; Burger et al., 2011; Al Zamal et al., 2012; Bergsma et al., 2013; Sap et al., 2014). and personality prediction (Mairesse et al., 2007; Iacobelli et al., 2011; Schwartz et al., 2013; Park et al., 2015). Human temporal orientation, as we study it here, differs from previous studies of user attribute prediction in that temporal orientation calls for consideration of additional language features (some more sophisticated, such as time expressions), and exploration of classification techniques (e.g. that can capture non-linear relationships or interactions). We also add multidisciplinary applications, showing not jus"
N15-1044,chang-manning-2012-sutime,0,0.380774,"f writing (e.g. the time expression ‘yesterday’ in a document written on January 15, 2014 is resolved as January 14, 2014). Many methods have been used, ranging from hand-crafting rules to machine learning models. Unlike other areas of natural language processing where stochastic techniques dominate, rule-based systems have been quite competitive in time expressions recognition, especially in less domain dependent settings or for relaxed matching tasks (UzZaman et al., 2013). A number of useful toolkits have been produced for temporal text analysis (Verhagen et al., 2005; Ling and Weld, 2010; Chang and Manning, 2012). In this work, we use Stanford University’s rulebased temporal tagger, SUTime, which geve accuracy in line with the state-of-the-art systems at identifying time expressions at TempEval (Chang and Manning, 2012).2 SUTime, built on top of Stanford’s part-of-speech and named entity taggers, la2 Our goals differ slightly from the TempEval accuracy criteria. For example, when SUTime fails to distinguish “one and a half weeks” from “one week”, it does not affect our performance. However, other errors, such as confusing the verb ‘march’ with the month March will harm our accuracy. 411 bels times, du"
N15-1044,D14-1121,1,0.810387,"ocus particularly on predicting user attributes. User network information has been used for tweet summarization or filtering (Panigrahy et al., 2012; Chang et al., 2013; Feng and Wang, 2013). Others utilize psychological knowledge about people, such as exploiting the human tendency to report more positive extreme feelings than negative in order to improve on sentiment analysis (Guerra et al., 2014). Toward attribute prediction, a large proportion of works have focused on demographics (Argamon et al., 2009; Goswami et al., 2009; Burger et al., 2011; Al Zamal et al., 2012; Bergsma et al., 2013; Sap et al., 2014). and personality prediction (Mairesse et al., 2007; Iacobelli et al., 2011; Schwartz et al., 2013; Park et al., 2015). Human temporal orientation, as we study it here, differs from previous studies of user attribute prediction in that temporal orientation calls for consideration of additional language features (some more sophisticated, such as time expressions), and exploration of classification techniques (e.g. that can capture non-linear relationships or interactions). We also add multidisciplinary applications, showing not just how accurately our models predict, but also studying how tempo"
N15-1044,N03-1033,0,0.00526355,"2012), discussed previously. Specific features recorded include the temporal difference itself (e.g. -2.5 for “two and half days ago”), its base 2 log (log(1 + value)), its absolute value, total number of time expressions, and binary variables indicating if any past, present, or future expressions appear in the text. We also include binary features for each of the named-entity time tags for the time expression provided by SUTime (e.g. “future ref”, “present ref”, “next immediate”). POS tags: The relative frequency of each part-ofspeech tag. Tagging is done via Stanford’s part-ofspeech tagger (Toutanova et al., 2003). Stanford’s tagger does not have explicit social media tags, but we are most interested in capturing tense which it does well.4 Also, it is already being used as part of SUTime. Each part-of-speech tag is encoded as the frequency of tag usage (f req(tag, msg)) divided by the total number of tokens in the message (tokensmsg ): p(tag|msg) = f req(tag, msg) |tokensmsg | lexica: The relative frequency of categories, based on the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker et al., 2007). We use the 2007 version of LIWC which includes 64 categories of psychologicallyrelevant lan"
N15-1044,S13-2001,0,0.147471,"Missing"
N15-1044,P05-3021,0,0.0339366,"pressed time and date relative to the time of writing (e.g. the time expression ‘yesterday’ in a document written on January 15, 2014 is resolved as January 14, 2014). Many methods have been used, ranging from hand-crafting rules to machine learning models. Unlike other areas of natural language processing where stochastic techniques dominate, rule-based systems have been quite competitive in time expressions recognition, especially in less domain dependent settings or for relaxed matching tasks (UzZaman et al., 2013). A number of useful toolkits have been produced for temporal text analysis (Verhagen et al., 2005; Ling and Weld, 2010; Chang and Manning, 2012). In this work, we use Stanford University’s rulebased temporal tagger, SUTime, which geve accuracy in line with the state-of-the-art systems at identifying time expressions at TempEval (Chang and Manning, 2012).2 SUTime, built on top of Stanford’s part-of-speech and named entity taggers, la2 Our goals differ slightly from the TempEval accuracy criteria. For example, when SUTime fails to distinguish “one and a half weeks” from “one week”, it does not affect our performance. However, other errors, such as confusing the verb ‘march’ with the month M"
N15-1044,S07-1014,0,0.0447369,"on seem plausible (e.g. one might suspect it is smart to think about the future, or wonder if one’s reflection on the past is related to their popularity as measure by number of friends). Related work. Studying temporal language is by no means new to the field of computational linguistics (or NLP). Most recently, time annotation has gained greater interest with a successive sequence of three SemEval tasks (TempEval-1, -2 and -3). The SemEval competitions have provided data sets that facilitate the comparison of different methods for evaluating time expressions, events, and temporal relations (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Such research on temporal text analysis generally focuses on determining when events start and end or how they relate temporally to each other; specific goals include information extraction of time-dependent facts from news media (Ling and Weld, 2010; Talukdar et al., 2012), or extracting personal histories in social media (Wen et al., 2013). In contrast, our goal is to find the temporal orientation of people. Of the numerous TempEval tasks, we build upon those which identify time expressions and resolve their expressed time and date relative to"
N15-1044,P13-2145,0,0.037317,"Missing"
N15-1044,S10-1010,0,\N,Missing
N19-1331,S15-2045,0,0.0394602,"Missing"
N19-1331,S14-2010,0,0.0405235,"Missing"
N19-1331,S16-1081,0,0.0261124,"Missing"
N19-1331,S12-1051,0,0.0364149,"← C([qs ]s∈Di , α) C i ← C temp ∨ C i−1 end for s ∈ G do P 1 a qs ← |s| p(w)+a vw w∈s 11 12 fsCA ← qs − C M qs end Output: {fsCA }s∈G A simple modification of Algorithm 2 yields a “zero-shot” sentence encoder that requires only pre-trained word embeddings and no training corpus: we can simply skip those corpus-dependent steps (line 2-8) and use C 0 in place of C M in line 11 in Algorithm 2 to embed sentences. This method will be referred to as “zero-shot CA.” 4 Experiment We evaluated our approach for continual sentence representation learning using semantic textual similarity (STS) datasets (Agirre et al., 2012, 2013, 2014, 2015, 2016). The evaluation criterion for such datasets is the Pearson correlation coefficient (PCC) between the predicted sentence similarities and the ground-truth sentence similarities. We split these datasets into five corpora by their genre: news, captions, wordnet, forums, tweets (for details see appendix). Throughout this section, we use publicly available 300- 3276 PCC 72.0 News Captions 1 2 3 4 5 first n training corpora used 75.0 Wordnet 87.0 1 2 3 4 5 76.0 Forums 66.0 82.7 81.2 67.5 63.0 87.0 60.1 1 2 3 4 5 51.0 corpus-specialized SIF train-from-scratch SIF Tweets 79.0"
N19-1331,Q17-1010,0,0.0486674,"f this experiment mimics (Zenke et al., 2017, section 5.1). av. train-from-scratch SIF zero-shot CA av. CA News Captions WordNet Forums Tweets 66.5 65.6 69.7 79.7 79.8 83.8 80.3 82.5 83.2 55.5 61.5 62.5 74.2 75.2 76.2 Table 1: Time-course averaged PCC of train-from-scratch SIF and conceptor-aided (CA) methods, together with the result of zero-shot CA. Best results are in boldface and the second best results are underscored. dimensional GloVe vectors (trained on the 840 billion token Common Crawl) (Pennington et al., 2014). Additional experiments with Word2Vec (Mikolov et al., 2013), Fasttext (Bojanowski et al., 2017), Paragram-SL-999 (Wieting et al., 2015) are in the appendix. We use a standard continual learning experiment setup (cf. (Zenke et al., 2017, section 5.1)) as follows. We sequentially present the five training datasets in the order2 of news, captions, wordnet, forums, and tweets, to train sentence encoders. Whenever a new training corpus is presented, we train a SIF encoder from scratch3 (by combining all available training corpora which have been already presented) and then test it on each corpus. At the same time, we incrementally adapt a CA encoder4 using the newly presented corpus and test"
N19-1331,D18-2029,0,0.027809,"e retaining its competence on previously encountered corpora. 1 1. Zero-shot learning. The initialized sentence encoder (no training corpus used) can effectively produce sentence embeddings. 2. Resistant to catastrophic forgetting. When the sentence encoder is adapted on a new training corpus, it retains strong performances on old ones. Introduction Distributed representations of sentences are essential for a wide variety of natural language processing (NLP) tasks. Although recently proposed sentence encoders have achieved remarkable results (e.g., (Yin and Sch¨utze, 2015; Arora et al., 2017; Cer et al., 2018; Pagliardini et al., 2018)), most, if not all, of them are trained on a priori fixed corpora. However, in open-domain NLP systems such as conversational agents, oftentimes we are facing a dynamic environment, where training data are accumulated sequentially over time and the distributions of training data vary with respect to external input (Lee, 2017; Mathur and Singh, 2018). To effectively use sentence encoders in such systems, we propose to consider the following continual sentence representation learning task: Given a sequence of corpora, we aim to train sentence encoders such that they c"
N19-1331,L18-1269,0,0.0178322,"ually fall short at solving this task as they leverage on “common discourse” statistics estimated based on a priori fixed corpora. We proposed two sentence encoders (CA encoder and zero-shot CA encoder) and demonstrate their the effectiveness at the continual sentence representation learning task using STS datasets. As the first paper considering continual sentence representation learning task, this work has been limited in a few ways – it remains for future work to address these limitations. First, it is worthwhile to incorporate more benchmarks such as GLUE (Wang et al., 2019) and SentEval (Conneau and Kiela, 2018) into the continual sentence representation task. Second, this work only considers the case of linear sentence encoder, but future research can attempt to devise (potentially more powerful) non-linear sentence encoders to address the same task. Thirdly, the proposed CA encoder operates at a corpus level, which might be a limitation if boundaries of training corpora are ill-defined. As a future direction, we expect to lift this assumption, for example, by updating the common direction statistics at a sentence level using Autoconceptors (Jaeger, 2014, section 3.14). Finally, the continual learni"
N19-1331,P18-1002,0,0.0216372,"vw w∈s 3 4 5 6 end Let u be the first singular vector of [qs ]s∈D . for sentence sP∈ G do a 1 qs ← |s| p(w)+a vw 3.1 8 Continual learning for linear sentence encoders Matrix conceptors In this section, we briefly introduce matrix conceptors, drawing heavily on (Jaeger, 2017; He and Jaeger, 2018; Liu et al., 2019). Consider a set of vectors {x1 , · · · , xn }, xi ∈ RN for all i ∈ {1, · · · , n}. A conceptor matrix is a regularized identity map that minimizes w∈s 7 3 fsSIF ← qs − uu&gt; qs . end Output: {fsSIF }s∈G Building upon SIF, recent studies have proposed further improved sentence encoders (Khodak et al., 2018; Pagliardini et al., 2018; Yang et al., 2018). These algorithms roughly share the core procedures of SIF, albeit using more refined n 1X kxi − Cxi k22 + α−2 kCk2F . n (1) i=1 where k · kF is the Frobenius norm and α−2 is a scalar parameter called aperture. It can be shown 3275 that C has a closed form solution: C= 1 1 XX &gt; ( XX &gt; + α−2 I)−1 , n n (2) where X = [xi ]i∈{1,··· ,n} is a data collection matrix whose columns are vectors from {x1 , · · · , xn }. In intuitive terms, C is a soft projection matrix on the linear subspace where the typical components of xi samples lie. For convenience in"
N19-1331,N18-1049,0,0.0966988,"mpetence on previously encountered corpora. 1 1. Zero-shot learning. The initialized sentence encoder (no training corpus used) can effectively produce sentence embeddings. 2. Resistant to catastrophic forgetting. When the sentence encoder is adapted on a new training corpus, it retains strong performances on old ones. Introduction Distributed representations of sentences are essential for a wide variety of natural language processing (NLP) tasks. Although recently proposed sentence encoders have achieved remarkable results (e.g., (Yin and Sch¨utze, 2015; Arora et al., 2017; Cer et al., 2018; Pagliardini et al., 2018)), most, if not all, of them are trained on a priori fixed corpora. However, in open-domain NLP systems such as conversational agents, oftentimes we are facing a dynamic environment, where training data are accumulated sequentially over time and the distributions of training data vary with respect to external input (Lee, 2017; Mathur and Singh, 2018). To effectively use sentence encoders in such systems, we propose to consider the following continual sentence representation learning task: Given a sequence of corpora, we aim to train sentence encoders such that they can continually learn featur"
N19-1331,D14-1162,0,0.0981448,"Missing"
N19-1331,Q15-1025,0,0.029198,"7, section 5.1). av. train-from-scratch SIF zero-shot CA av. CA News Captions WordNet Forums Tweets 66.5 65.6 69.7 79.7 79.8 83.8 80.3 82.5 83.2 55.5 61.5 62.5 74.2 75.2 76.2 Table 1: Time-course averaged PCC of train-from-scratch SIF and conceptor-aided (CA) methods, together with the result of zero-shot CA. Best results are in boldface and the second best results are underscored. dimensional GloVe vectors (trained on the 840 billion token Common Crawl) (Pennington et al., 2014). Additional experiments with Word2Vec (Mikolov et al., 2013), Fasttext (Bojanowski et al., 2017), Paragram-SL-999 (Wieting et al., 2015) are in the appendix. We use a standard continual learning experiment setup (cf. (Zenke et al., 2017, section 5.1)) as follows. We sequentially present the five training datasets in the order2 of news, captions, wordnet, forums, and tweets, to train sentence encoders. Whenever a new training corpus is presented, we train a SIF encoder from scratch3 (by combining all available training corpora which have been already presented) and then test it on each corpus. At the same time, we incrementally adapt a CA encoder4 using the newly presented corpus and test it on each corpus. The lines of each pa"
N19-1331,N15-1091,0,0.0615459,"Missing"
N19-1331,S13-1004,0,\N,Missing
N19-4011,P17-4012,0,0.0255253,"s test statistical significance. IRT is the basis for almost all psychometric studies (Embretson and Reise, 2013). We follow the work of Otani et al. (2016), who used head-to-head pairwise testing to compare machine translation systems. However, we further this work by also examining the discriminative power of prompts. For instance “my name is david . what is my name ?” from the NCM evaluation dataset has been shown to have low discriminative power, Selection of Baselines We seek to establish reasonable public baselines for Seq2Seq-based chatbots. All models trained by us use the OpenNMT-py (Klein et al., 2017) Seq2Seq implementation with its default parameters: two layers of LSTMs with 512 hidden neurons for the bidirectional encoder and the unidirectional decoder. We trained models on three standard datasets: OpenSubtitles, SubTle, and Twitter, and plan to introduce baselines trained on other datasets. The number of baseline methods will continue to grow. We plan to add an information retrieval baseline, the hierarchical encoder-decoder model (Serban et al., 2017), and several other baselines from ParlAI. Conclusion and Future Work ChatEval is a framework for systematic evaluation of chatbots. The"
N19-4011,W18-5028,0,0.0449826,"ion of chatbots in a standardized way, and (2) a web portal for accessing model code, trained parameters, and evaluation results, which grows with participation. In addition, ChatEval includes newly created and curated evaluation datasets with both human annotated and automated baselines. RankME5 (Novikova et al., 2018) is an evaluation system for natural language generation. While RankME could be adapted for chatbot evaluation, this would require significant modification of the source code. Furthermore, RankME is only a crowdsourcing framework, which is more narrow than ChatEval. DialCrowd6 (Lee et al., 2018) is a tool for the easy creation of human evaluation tasks for conversational agents. Finally, Kaggle7 is another important venue for competitions, which allows for multiple test beds. However, none of these tools and websites offer a unified solution to public baselines, evaluation sets, and an integrated A/B model testing framework. In many ways, the goal of ChatEval is similar to Appraise: an Open-Source Toolkit for Manual Evaluation of MT Output (Federmann, 2012). Just as Appraise is integrated with WMT, ChatEval should also be used in shared tasks in dialog competitions. Related Work The"
N19-4011,D18-1431,0,0.0316226,"amazon.com/ alexaprize 2 http://convai.io/ 3 http://workshop.colips.org/wochat/ 4 https://parl.ai 5 https://github.com/jeknov/RankME https://dialrc.org/dialcrowd.html 7 https://www.kaggle.com 6 61 Overfitting One important feature of ChatEval is the ease of adding new evaluation datasets. In order to assure that researchers are not overfitting to any evaluation set, the ChatEval team will take top performing models and also apply them to other datasets. New evaluation datasets can be added upon request from the ChatEval team. We plan to add both the prompts as well as the model responses from Baheti et al. (2018) as well as Li et al. (2019). Finally, we have added the ability to interact with baseline models using FlowAI (Wubben, 2018).10 the ChatEval website. The profile consists of the URLs and description provided by the researcher, the responses of the model to each prompt in the evaluation set, and a visualization of the results of human and automatic evaluation. Response Comparison To facilitate the qualitative comparison of models, we offer a response comparison interface where users can see all the prompts in a particular evaluation set and the responses generated by each model. Evaluation Dat"
N19-4011,D17-2014,0,0.0308827,"king for absolute assessments of quality yields less discriminative results than soliciting direct comparisons of quality. In the dataset introduced for the ConvAI2 competition, nearly all the proposed algorithms were evaluated to be within one standard deviation of each other (Zhang et al., 2018). Therefore, for our human evaluation task, we ask humans to directly compare the responses of two models given the previous utterances in the conversation. Both Facebook and Amazon have developed evaluation systems that allow humans to converse with (and then rate) a chatbot (Venkatesh et al., 2018; Miller et al., 2017). Facebook’s ParlAI 4 is the most comparable system for a unified framework for sharing, training, and evaluating chatbots; however, ChatEval is different in that it entirely focuses on the evaluation and warehousing of models. Our infrastructure takes as input text files containing model responses and does not require any code base integration. The ChatEval web interface consists of four primary pages. Aside from the overview page, there is a model submission form, a page for viewing the profile of any submitted model, and a page for comparing the responses of multiple models. Model Submissio"
N19-4011,N18-2012,0,0.107522,"Missing"
N19-4011,P13-1166,0,0.0333177,"l's profile on proﬁle on ChatEval SETC Evaluation Toolkit Automatic Evaluation Introduction Figure 1: Flow of information in ChatEval. A researcher submits information about her model, including its responses to prompts in a standard evaluation set. Automatic evaluation as well as human evaluation are conducted, then the results are posted publicly on the ChatEval website. Reproducibility and model assessment for opendomain dialog systems is challenging, as many small variations in the training setup or evaluation technique can result in significant differences in perceived model performance (Fokkens et al., 2013). While reproducibility is problematic for NLP in general, this is especially true for dialog systems due to the lack of automatic metrics. In addition, as the field has grown, it has become increasingly fragmented in human evaluation methodologies. Papers often focus on novel methods, but insufficient attention is paid to ensuring that datasets and evaluation remain consistent and reproducible. For example, while human evaluation of chatbot quality is extremely common, few papers publish the set of prompts used for this evaluation, and almost no papers release their learned model parameters."
N19-4011,P18-1205,0,0.196042,"Appraise is integrated with WMT, ChatEval should also be used in shared tasks in dialog competitions. Related Work The ChatEval Web Interface Prize,1 ConvAI2 Competitions such as the Alexa and WOCHAT,3 rank submitted chatbots by having humans converse with them and then rate the quality of the conversation. However, asking for absolute assessments of quality yields less discriminative results than soliciting direct comparisons of quality. In the dataset introduced for the ConvAI2 competition, nearly all the proposed algorithms were evaluated to be within one standard deviation of each other (Zhang et al., 2018). Therefore, for our human evaluation task, we ask humans to directly compare the responses of two models given the previous utterances in the conversation. Both Facebook and Amazon have developed evaluation systems that allow humans to converse with (and then rate) a chatbot (Venkatesh et al., 2018; Miller et al., 2017). Facebook’s ParlAI 4 is the most comparable system for a unified framework for sharing, training, and evaluating chatbots; however, ChatEval is different in that it entirely focuses on the evaluation and warehousing of models. Our infrastructure takes as input text files conta"
P09-2065,W02-1004,0,0.210178,"nse Disambiguation by incorporating a feature relevance prior for each word indicating which features are more likely to be selected. We use transfer of knowledge from similar words to learn this prior over the features, which permits us to learn higher accuracy models, particularly for the rarer word senses. Results on the O NTO N OTES verb data show significant improvement over the baseline feature selection algorithm and results that are comparable to or better than other state-of-the-art methods. 1 Introduction The task of WSD has been mostly studied in a supervised learning setting e.g. (Florian and Yarowsky, 2002) and feature selection has always been an important component of high accuracy word sense disambiguation, as one often has thousands of features but only hundreds of observations of the words (Florian and Yarowsky, 2002). The main problem that arises with supervised WSD techniques, including ones that do feature selection, is the paucity of labeled data. For example, the training set of S ENSEVAL -2 English lexical sample task has only 10 labeled examples per sense (Florian and Yarowsky, 2002), which makes it difficult to build high accuracy models using only supervised learning techniques. It"
P09-2065,N06-2015,0,0.115373,"Missing"
P09-2065,W06-2911,0,\N,Missing
P12-1024,P96-1024,0,0.812379,"rm for calculation of p(r1 . . . rN ). 1. For a given p(r1 . . . rN ). s-tree r1 . . . rN , calculate 2. For a given input sentence x = x1 . . . xN , calculate the marginal probabilities X µ(a, i, j) = p(τ ) τ ∈T (x):(a,i,j)∈τ for each non-terminal a ∈ N , for each (i, j) such that 1 ≤ i ≤ j ≤ N . Here T (x) denotes the set of all possible s-trees for the sentence x, and we write (a, i, j) ∈ τ if nonterminal a spans words xi . . . xj in the parse tree τ . The marginal probabilities have a number of uses. Perhaps most importantly, for a given sentence x = x1 . . . xN , the parsing algorithm of Goodman (1996) can be used to find X arg max µ(a, i, j) τ ∈T (x) (a,i,j)∈τ This is the parsing algorithm used by Petrov et al. (2006), for example. In addition, we can calculate the probability for anPinput sentence, p(x) = P τ ∈T (x) p(τ ), as p(x) = a∈I µ(a, 1, N ). Variants of the inside-outside algorithm can be used for problems 1 and 2. This section introduces a novel form of these algorithms, using tensors. This is the first step in deriving the spectral estimation method. The algorithms are shown in figures 2 and 3. Each algorithm takes the following inputs: 5 Tensor Form of the Inside-Outside Algori"
P12-1024,E12-1042,0,0.119777,"The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation 4 L-PCFGs: Basic Definitions This section gives a definition of the L-PCFG formalism used in this paper. An L-PCFG is a 5-tuple (N , I, P, m, n) where: • N is the set of non-terminal symbols in the grammar. I ⊂ N is a finite set of in-terminals. P ⊂ N is a finite set of pre-terminals. We assume that N = I ∪ P, and I ∩ P = ∅. Hence we have partitioned the set of non-terminals into two subsets. • [m] is the set of possi"
P12-1024,P05-1010,0,0.93,"ar value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/σ, where σ is the minimum singular value of an underlying decomposition. These methods are not susceptible to problems with local maxima, and give consistent parameter estimates. In this paper we derive a spectral algorithm for learning of latent-variable PCFGs (L-PCFGs) (Petrov et al., 2006; Matsuzaki et al., 2005). Our method involves a significant extension of the techniques from Hsu et al. (2009). L-PCFGs have been shown to be a very effective model for natural language parsing. Under a separation (singular value) condition, our algorithm provides consistent parameter estimates; this is in contrast with previous work, which has used the EM algorithm for parameter estimation, with the usual problems of local optima. The parameter estimation algorithm (see figure 4) is simple and efficient. The first step is to take an SVD of the training examples, followed by a projection of the training examples down"
P12-1024,P92-1017,0,0.508816,"tion for Computational Linguistics, pages 223–231, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics veloped in this paper are quite general, and should be relevant to the development of spectral methods for estimation in other models in NLP, for example alignment models for translation, synchronous PCFGs, and so on. The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation 4 L-PCFGs: Basic Definitions This section gives a definition of the L-"
P12-1024,P06-1055,0,0.878323,"in particular singular value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/σ, where σ is the minimum singular value of an underlying decomposition. These methods are not susceptible to problems with local maxima, and give consistent parameter estimates. In this paper we derive a spectral algorithm for learning of latent-variable PCFGs (L-PCFGs) (Petrov et al., 2006; Matsuzaki et al., 2005). Our method involves a significant extension of the techniques from Hsu et al. (2009). L-PCFGs have been shown to be a very effective model for natural language parsing. Under a separation (singular value) condition, our algorithm provides consistent parameter estimates; this is in contrast with previous work, which has used the EM algorithm for parameter estimation, with the usual problems of local optima. The parameter estimation algorithm (see figure 4) is simple and efficient. The first step is to take an SVD of the training examples, followed by a projection of t"
P12-1024,J98-4004,0,\N,Missing
P12-1024,J93-2004,0,\N,Missing
P12-1024,P97-1003,1,\N,Missing
P12-1024,P03-1054,0,\N,Missing
P16-1080,P11-1078,0,0.0161905,"Missing"
P16-1080,D11-1120,0,0.900837,"t al., 1996; John and Robins, 1994; Kobrynowicz and Branscombe, 1997). Such differences are likely reflected through one’s writing. With the Internet a substantial part of daily life, users leave enough footprints which allow algorithms to learn a range of individual traits, some with even higher accuracy than the users’ own family (Youyou et al., 2015). With an increase in readily available user generated content, prediction of user attributes has become more popular than ever. Researchers built learning models to infer different user traits from text, such as age (Rao et al., 2010), gender (Burger et al., 2011; Flekova and Gurevych, 2013), location (Eisenstein et al., 2010), political orientation (Volkova et al., 2014), income (Preot¸iucPietro et al., 2015c), socio-economic status (Lampos et al., 2016), popularity (Lampos et al., 2014), personality (Schwartz et al., 2013) or mental illnesses (De Choudhury et al., 2013; Coppersmith et al., 2014; Preot¸iuc-Pietro et al., 2015a). Prediction models are trained on large data sets with labels extracted either from user selfreports (Preot¸iuc-Pietro et al., 2015b) or perceived from annotations (Volkova et al., 2015; Volkova and Bachrach, 2015). The former"
P16-1080,W14-3207,0,0.00854118,"Youyou et al., 2015). With an increase in readily available user generated content, prediction of user attributes has become more popular than ever. Researchers built learning models to infer different user traits from text, such as age (Rao et al., 2010), gender (Burger et al., 2011; Flekova and Gurevych, 2013), location (Eisenstein et al., 2010), political orientation (Volkova et al., 2014), income (Preot¸iucPietro et al., 2015c), socio-economic status (Lampos et al., 2016), popularity (Lampos et al., 2014), personality (Schwartz et al., 2013) or mental illnesses (De Choudhury et al., 2013; Coppersmith et al., 2014; Preot¸iuc-Pietro et al., 2015a). Prediction models are trained on large data sets with labels extracted either from user selfreports (Preot¸iuc-Pietro et al., 2015b) or perceived from annotations (Volkova et al., 2015; Volkova and Bachrach, 2015). The former is useful in obtaining accurate prediction models for unknown users while the latter is more suitable in applications that interact with humans. Previous studies showed the implications of perceived individual traits to the believability and likability of autonomous agents (Bates, 1994; Loyall and Bates, 1997; Baylor and Kim, 2004). This"
P16-1080,D10-1124,0,0.188056,"Missing"
P16-1080,P16-2051,1,0.447562,"Missing"
P16-1080,P15-2079,0,0.0605738,"Missing"
P16-1080,K15-1011,0,0.062871,"8) showed that female authors are more likely to include pronouns, verbs, references to home, family, friends and to various emotions. Male authors use longer words, more articles, prepositions and numbers. Topical differences include males writing more about current concerns (e.g., money, leisure or sports). More recent author profiling experiments (Rangel et al., 2014; Rangel et al., 2015) revealed that gender can be well predicted from a large spectrum of textual features, ranging from paraphrase choice (Preot¸iuc-Pietro et al., 2016), emotions (Volkova and Bachrach, 2016), part-of-speech (Johannsen et al., 2015) and abbreviation usage to social network metadata, web traffic (Culotta et al., 2015), apps installed (Seneviratne et al., 2015) or Facebook likes (Kosinski et al., 2013). Bamman et al. (2014) also examine individuals whose language does not match their automatically predicted gender. Most of these experiments were based on self-reported gender in social media profiles. The relationship between age and language has also been extensively studied by both psychologists and computational linguists. Schler et al. (2006) automatically classified blogposts into three age groups based on self-reporte"
P16-1080,E14-1043,1,0.861901,"Missing"
P16-1080,C14-1184,0,0.184873,"Missing"
P16-1080,P12-3005,0,0.0418144,"from users with known self-reported age and gender. To study gender, we use the users from Burger et al. (2011), which are mapped to their self-identified gender as mentioned in other user public profiles linked to their Twitter account. This data set consists of 67,337 users, from which we subsample 2,607 users for human assessment. The age data set consists of 826 users that selfreported their year of birth and Twitter handle as part of an online survey. We use the Twitter API to download up to 3200 tweets from these users. These are filtered for English language using an automatic method (Lui and Baldwin, 2012) and duplicate tweets are eliminated (i.e., having the same first 6 tokens) as these are usually generated automatically by apps. Tweet URLs and @-mentions are anonymized as they may contain sensitive information or cues external to language use. For human assessment, we randomly select 100 tweets posted in the same 6 month time interval from the users where gender is known. For the users of known age we randomly select 100 tweets posted during the year 2015. 4 Experimental Setup We use Amazon Mechanical Turk to create crowdsourcing tasks for predicting age and gender from tweets. Each HIT con"
P16-1080,W15-1203,1,0.252763,"Missing"
P16-1080,P15-1169,1,0.444621,"Missing"
P16-1080,P13-1162,0,0.0490211,"Missing"
P16-1080,P11-1077,0,0.0281109,"s (Kosinski et al., 2013). Bamman et al. (2014) also examine individuals whose language does not match their automatically predicted gender. Most of these experiments were based on self-reported gender in social media profiles. The relationship between age and language has also been extensively studied by both psychologists and computational linguists. Schler et al. (2006) automatically classified blogposts into three age groups based on self-reported age using features from the Linguistic Inquiry and Word Count Framework (Pennebaker et al., 2001), online slang and part-of-speech information. Rosenthal and McKeown (2011) analyzed how both stylistic and lexical cues relate to gender on blogs. On Twitter, Nguyen et al. (2013) analyzed the relationship between language use and age, modelled as a continuous variable. They found similar language usage trends for both genders, with increasing word and tweet length with age, and an increasing tendency to write more grammatically correct, standardized 844 text. Flekova et al. (2016) identified age specific differences in writing style and analyzed their impact beyond income. Recently, Nguyen et al. (2014) showed that age prediction is more difficult as age increases,"
P16-1080,D14-1121,1,0.56599,"Missing"
P16-1080,P16-1148,0,0.0183593,"males and females discuss. Newman et al. (2008) showed that female authors are more likely to include pronouns, verbs, references to home, family, friends and to various emotions. Male authors use longer words, more articles, prepositions and numbers. Topical differences include males writing more about current concerns (e.g., money, leisure or sports). More recent author profiling experiments (Rangel et al., 2014; Rangel et al., 2015) revealed that gender can be well predicted from a large spectrum of textual features, ranging from paraphrase choice (Preot¸iuc-Pietro et al., 2016), emotions (Volkova and Bachrach, 2016), part-of-speech (Johannsen et al., 2015) and abbreviation usage to social network metadata, web traffic (Culotta et al., 2015), apps installed (Seneviratne et al., 2015) or Facebook likes (Kosinski et al., 2013). Bamman et al. (2014) also examine individuals whose language does not match their automatically predicted gender. Most of these experiments were based on self-reported gender in social media profiles. The relationship between age and language has also been extensively studied by both psychologists and computational linguists. Schler et al. (2006) automatically classified blogposts in"
P16-1080,P14-1018,0,0.107398,"Missing"
P16-2051,D13-1181,0,0.013339,"ve shown that the age of the authors should be taken into account when building and using part-of-speech taggers. Likewise, socioeconomic factors have been found to influence language use (Labov, 2006). Understanding these biases and their underlying factors in detail is important to develop NLP tools without sociodemographic bias. Writing style measures have initially been created to be applied at the document level, where they are often used to assess the quality of a document (Louis and Nenkova, 2013) or a summarization (Louis and Nenkova, 2014) , or even to predict the success of a novel (Ashok et al., 2013). In contrast to these document-level studies, we adopt a user-centric approach to measuring stylistic differences. We examine writing style of users on Twitter in relation to their age and income. Both attributes should be closely related to writing style: users of older age write on average more standard-conform (up to a certain point), and higher income is an indicator of education and conscientiousness (Judge et al., 1999), which determines writing style. Indeed, many features that aim to measure the complexity of the language use have been developed in order to study human cognitive abili"
P16-2051,D11-1120,0,0.335268,"nally, we use our validated features to study daily variations in writing style of users from distinct income groups. Temporal stylistic patterns not only provide novel psychological insight into user behavior, but are useful for future research and applications in social media. 1 Introduction The widespread use of social media enables researchers to examine human behavior at a scale hardly imaginable before. Research in text profiling has recently shown that a diverse set of user traits is predictable from language use. Examples range from demographics such as age (Rao et al., 2010), gender (Burger et al., 2011; Bamman et al., 2014), popularity (Lampos et al., 2014), occupation (Preot¸iuc-Pietro et al., 2015a) and location (Eisenstein et al., 2010) to psychological traits such as personality (Schwartz et al., 2013) or mental illness (De Choudhury et al., 2013) and their interplay (Preotiuc-Pietro et al., 2015). To a large extent, the prominent differences captured by text are topical: adolescents post more about school, females about relationships (Sap et al., 2014) and sport fans about their local team (Cheng et al., ∗ Project carried out during a research stay at the University of Pennsylvania 201"
P16-2051,D10-1124,0,0.178234,"Missing"
P16-2051,P11-1137,0,0.019517,"usage trends for both genders, with increasing word and tweet length with age, and an increasing tendency to write more grammatically correct, standardized text. Such findings encourage further research in the area of measuring readability, which not only facilitates adjusting the text to the reader (Danescu-Niculescu-Mizil et al., 2011), but can also play an important role in identifying authorial style (Pitler and Nenkova, 2008). Davenport and DeLine (2014) report negative correlation between tweet readability (i.e., simplicity) and the percentage of people with college degree in the area. Eisenstein et al. (2011) employ language use as a socio-demographic predictor. In this paper we analyze two data sets of millions of tweets produced by thousands of users annotated with their age and income. We define a set of features ranging from readability and style to syntactic features. We use both linear and non-linear machine learning regression methods to predict and analyze user income and age. We show that writing style measures give large correlations with both age and income, and that writing style is predictive of income even beyond age. Finally, Twitter data allows the unique possibility to study the v"
P16-2051,E09-1027,0,0.00870168,"most prominent readability measures per user: the Automatic Readability Index (Senter and Smith, 1967), the FleschKincaid Grade Level (Kincaid et al., 1975), the Coleman-Liau Index (Coleman and Liau, 1975), the Flesch Reading Ease (Flesch, 1948), the LIX Index (Anderson, 1983), the SMOG grade (McLaughlin, 1969) and the Gunning-Fog Index (Gunning, 1969). The majority of those are computed using the average word and sentence lengths and number of syllables per sentence, combined with weights. Syntax Researchers argue about longer sentences not necessarily being more complex in terms of syntax (Feng et al., 2009; Pitler and Nenkova, 2008). However, advanced sentence parsing on Twitter remains a challenging task. We thus limit ourselves in this study to the part-of-speech (POS) 314 (a) ARI Readability Index. (b) Pronouns. (c) Interjections. (d) Named Entities. Figure 1: Temporal patterns for groups of lowest (blue) and highest (orange) income users in our data set. X-axis shows the course of 24 hours in normalized time of day. Y-axis shows a normalized difference of the hourly means from the overall mean feature value. Width of a line shows the standard error. information. In previous work on writing"
P16-2051,P05-1045,0,0.00300953,"ean feature value. Width of a line shows the standard error. information. In previous work on writing style (Pennebaker et al., 2003; Argamon et al., 2009; Rangel et al., 2014), a text with more nouns and articles as opposed to pronouns and adverbs is considered more formal. We thus measure the ratio of each POS using the universal tagset (Petrov et al., 2012). Style We implemented a contextuality measure, based on the work of Heylighen and Dewaele (2002), which assesses explicitness of the text based on the POS used and serves as a proxy for formality. Using Stanford Named Entity Recognizer (Finkel et al., 2005), we measure the proportion of named entities (3-classed) to words, as their presence potentially decreases readability (Beinborn et al., 2012), and netspeak aspects such as the proportion of elongations (wooow) and words with numbers (good n8). We quantify the number of hedges (Hyland, 2005) and abstract words1 used, and the ratio of standalone numbers stated per user as these are indicators of specificity (Pennebaker et al., 2003; Pitler and Nenkova, 2008). We also capture the ratio of hapax legomena, and of superlatives and plurals using Stanford POS Tagger 1 www.englishbanana.com (Toutanov"
P16-2051,P11-2008,0,0.100748,"Missing"
P16-2051,P15-2079,0,0.0518844,"ent differences captured by text are topical: adolescents post more about school, females about relationships (Sap et al., 2014) and sport fans about their local team (Cheng et al., ∗ Project carried out during a research stay at the University of Pennsylvania 2010). Writing style and readability offer a different insight into who the authors are. This can help applications such as cross-lingual adaptations without direct translation, for text simplification closely matching the reader’s age, level of education and income or tailored to the specific moment the document is presented. Recently, Hovy and Søgaard (2015) have shown that the age of the authors should be taken into account when building and using part-of-speech taggers. Likewise, socioeconomic factors have been found to influence language use (Labov, 2006). Understanding these biases and their underlying factors in detail is important to develop NLP tools without sociodemographic bias. Writing style measures have initially been created to be applied at the document level, where they are often used to assess the quality of a document (Louis and Nenkova, 2013) or a summarization (Louis and Nenkova, 2014) , or even to predict the success of a nove"
P16-2051,K15-1011,0,0.0427707,"et al., 2011). The relationship between age and language has been extensively studied by psychologists, and more recently by computational linguists in various corpora, including social media. Pennebaker et al. (2003) connect language use with style and personality, while Schler et al. (2006) automatically 313 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 313–319, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics classified blogs text into three classes based on self-reported age using part-of-speech features. Johannsen et al. (2015) uncover some consistent age patterns in part-of-speech usage across languages, while Rosenthal and McKeown (2011) studies the use of Internet specific phenomena such as slang, acronyms and capitalisation patterns. Preot¸iucPietro et al. (2016) study differences in paraphrase choice between older and younger Twitter users as a measure of style. Nguyen et al. (2013) analyzed the relationship between language use and age, modelled as a continuous variable. They found similar language usage trends for both genders, with increasing word and tweet length with age, and an increasing tendency to writ"
P16-2051,E14-1043,1,0.685031,"Missing"
P16-2051,Q13-1028,0,0.0172855,"ation and income or tailored to the specific moment the document is presented. Recently, Hovy and Søgaard (2015) have shown that the age of the authors should be taken into account when building and using part-of-speech taggers. Likewise, socioeconomic factors have been found to influence language use (Labov, 2006). Understanding these biases and their underlying factors in detail is important to develop NLP tools without sociodemographic bias. Writing style measures have initially been created to be applied at the document level, where they are often used to assess the quality of a document (Louis and Nenkova, 2013) or a summarization (Louis and Nenkova, 2014) , or even to predict the success of a novel (Ashok et al., 2013). In contrast to these document-level studies, we adopt a user-centric approach to measuring stylistic differences. We examine writing style of users on Twitter in relation to their age and income. Both attributes should be closely related to writing style: users of older age write on average more standard-conform (up to a certain point), and higher income is an indicator of education and conscientiousness (Judge et al., 1999), which determines writing style. Indeed, many features that"
P16-2051,E14-1067,0,0.0129426,"moment the document is presented. Recently, Hovy and Søgaard (2015) have shown that the age of the authors should be taken into account when building and using part-of-speech taggers. Likewise, socioeconomic factors have been found to influence language use (Labov, 2006). Understanding these biases and their underlying factors in detail is important to develop NLP tools without sociodemographic bias. Writing style measures have initially been created to be applied at the document level, where they are often used to assess the quality of a document (Louis and Nenkova, 2013) or a summarization (Louis and Nenkova, 2014) , or even to predict the success of a novel (Ashok et al., 2013). In contrast to these document-level studies, we adopt a user-centric approach to measuring stylistic differences. We examine writing style of users on Twitter in relation to their age and income. Both attributes should be closely related to writing style: users of older age write on average more standard-conform (up to a certain point), and higher income is an indicator of education and conscientiousness (Judge et al., 1999), which determines writing style. Indeed, many features that aim to measure the complexity of the languag"
P16-2051,P12-3005,0,0.035735,"ter data allows the unique possibility to study the variation in writing with time. We explore the effects of time of day in user behavior dependent in part on the socio-demographic group. 2 Data We study two large data sets of tweets. Each data set consists of users and their historical record of tweet content, profile information and trait level features extracted with high precision from their profile information. All data was tokenized using the Trendminer pipeline (Preot¸iuc-Pietro et al., 2012), @-mentions and URL’s collapsed, automatically filtered for English using the langid.py tool (Lui and Baldwin, 2012) and part-of-speech tagged using the ArkTweet POS tagger (Gimpel et al., 2011). Income (D1 ) First, we use a large data set consisting of 5,191 Twitter users mapped to their income through their occupational class. This data set, introduced in (Preot¸iuc-Pietro et al., 2015a; Preot¸iuc-Pietro et al., 2015b), relies on a standardised job classification taxonomy (the UK Standard Occupational Classification) to extract job-related keywords, search user profile fields for users having those jobs and map them to their mean UK income, independently of user location. The final data set consists of 10"
P16-2051,petrov-etal-2012-universal,0,0.016532,"ed Entities. Figure 1: Temporal patterns for groups of lowest (blue) and highest (orange) income users in our data set. X-axis shows the course of 24 hours in normalized time of day. Y-axis shows a normalized difference of the hourly means from the overall mean feature value. Width of a line shows the standard error. information. In previous work on writing style (Pennebaker et al., 2003; Argamon et al., 2009; Rangel et al., 2014), a text with more nouns and articles as opposed to pronouns and adverbs is considered more formal. We thus measure the ratio of each POS using the universal tagset (Petrov et al., 2012). Style We implemented a contextuality measure, based on the work of Heylighen and Dewaele (2002), which assesses explicitness of the text based on the POS used and serves as a proxy for formality. Using Stanford Named Entity Recognizer (Finkel et al., 2005), we measure the proportion of named entities (3-classed) to words, as their presence potentially decreases readability (Beinborn et al., 2012), and netspeak aspects such as the proportion of elongations (wooow) and words with numbers (good n8). We quantify the number of hedges (Hyland, 2005) and abstract words1 used, and the ratio of stand"
P16-2051,D08-1020,0,0.457321,"r and younger Twitter users as a measure of style. Nguyen et al. (2013) analyzed the relationship between language use and age, modelled as a continuous variable. They found similar language usage trends for both genders, with increasing word and tweet length with age, and an increasing tendency to write more grammatically correct, standardized text. Such findings encourage further research in the area of measuring readability, which not only facilitates adjusting the text to the reader (Danescu-Niculescu-Mizil et al., 2011), but can also play an important role in identifying authorial style (Pitler and Nenkova, 2008). Davenport and DeLine (2014) report negative correlation between tweet readability (i.e., simplicity) and the percentage of people with college degree in the area. Eisenstein et al. (2011) employ language use as a socio-demographic predictor. In this paper we analyze two data sets of millions of tweets produced by thousands of users annotated with their age and income. We define a set of features ranging from readability and style to syntactic features. We use both linear and non-linear machine learning regression methods to predict and analyze user income and age. We show that writing style"
P16-2051,P15-1169,1,0.283778,"Missing"
P16-2051,W15-1203,1,0.681062,"The widespread use of social media enables researchers to examine human behavior at a scale hardly imaginable before. Research in text profiling has recently shown that a diverse set of user traits is predictable from language use. Examples range from demographics such as age (Rao et al., 2010), gender (Burger et al., 2011; Bamman et al., 2014), popularity (Lampos et al., 2014), occupation (Preot¸iuc-Pietro et al., 2015a) and location (Eisenstein et al., 2010) to psychological traits such as personality (Schwartz et al., 2013) or mental illness (De Choudhury et al., 2013) and their interplay (Preotiuc-Pietro et al., 2015). To a large extent, the prominent differences captured by text are topical: adolescents post more about school, females about relationships (Sap et al., 2014) and sport fans about their local team (Cheng et al., ∗ Project carried out during a research stay at the University of Pennsylvania 2010). Writing style and readability offer a different insight into who the authors are. This can help applications such as cross-lingual adaptations without direct translation, for text simplification closely matching the reader’s age, level of education and income or tailored to the specific moment the do"
P16-2051,P11-1077,0,0.06605,"ore recently by computational linguists in various corpora, including social media. Pennebaker et al. (2003) connect language use with style and personality, while Schler et al. (2006) automatically 313 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 313–319, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics classified blogs text into three classes based on self-reported age using part-of-speech features. Johannsen et al. (2015) uncover some consistent age patterns in part-of-speech usage across languages, while Rosenthal and McKeown (2011) studies the use of Internet specific phenomena such as slang, acronyms and capitalisation patterns. Preot¸iucPietro et al. (2016) study differences in paraphrase choice between older and younger Twitter users as a measure of style. Nguyen et al. (2013) analyzed the relationship between language use and age, modelled as a continuous variable. They found similar language usage trends for both genders, with increasing word and tweet length with age, and an increasing tendency to write more grammatically correct, standardized text. Such findings encourage further research in the area of measuring"
P16-2051,D14-1121,1,0.783001,"diverse set of user traits is predictable from language use. Examples range from demographics such as age (Rao et al., 2010), gender (Burger et al., 2011; Bamman et al., 2014), popularity (Lampos et al., 2014), occupation (Preot¸iuc-Pietro et al., 2015a) and location (Eisenstein et al., 2010) to psychological traits such as personality (Schwartz et al., 2013) or mental illness (De Choudhury et al., 2013) and their interplay (Preotiuc-Pietro et al., 2015). To a large extent, the prominent differences captured by text are topical: adolescents post more about school, females about relationships (Sap et al., 2014) and sport fans about their local team (Cheng et al., ∗ Project carried out during a research stay at the University of Pennsylvania 2010). Writing style and readability offer a different insight into who the authors are. This can help applications such as cross-lingual adaptations without direct translation, for text simplification closely matching the reader’s age, level of education and income or tailored to the specific moment the document is presented. Recently, Hovy and Søgaard (2015) have shown that the age of the authors should be taken into account when building and using part-of-spee"
P16-2051,N03-1033,0,0.0580512,"., 2005), we measure the proportion of named entities (3-classed) to words, as their presence potentially decreases readability (Beinborn et al., 2012), and netspeak aspects such as the proportion of elongations (wooow) and words with numbers (good n8). We quantify the number of hedges (Hyland, 2005) and abstract words1 used, and the ratio of standalone numbers stated per user as these are indicators of specificity (Pennebaker et al., 2003; Pitler and Nenkova, 2008). We also capture the ratio of hapax legomena, and of superlatives and plurals using Stanford POS Tagger 1 www.englishbanana.com (Toutanova et al., 2003) using the Twitter model. 4 Temporal Patterns in Style Social media data offers the opportunity to interpret the features in a richer context, including time or space. In our income data set, a timestamp is available for each message. Golder and Macy (2011) showed user-level diurnal and seasonal patterns of mood across the world using Twitter data, suggesting that individuals awaken in a good mood that deteriorates as the day progresses. In this work we explore user-level daily temporal trends in style for the 1500 highest- and 1500 lowest-income users (mean income ≥ £35,000 vs mean income ≤ £"
P17-1067,P11-2102,0,0.0152851,"Missing"
P17-1067,P14-1062,0,0.078461,"Missing"
P17-1067,D14-1181,0,0.00781316,"Missing"
P17-1067,P13-2087,0,0.0140046,"Missing"
P17-1067,P10-3008,0,0.201778,"Missing"
P17-1067,D15-1278,0,0.0238346,"Missing"
P17-1067,pak-paroubek-2010-twitter,0,0.0336727,"Missing"
P17-1067,D15-1280,0,0.0223252,"Missing"
P17-1067,P12-3005,0,0.0118018,"acters of > 2 to 2] and user mentions (as detected by a string starting with an “@” sign). We then performed a manual inspection of a random sample of 1,000 tweets from the data and found no evidence of any remaining tweet duplicates. Next, even though the emotion hashtags themselves are exclusively in English, we observe the data do have tweets in languages other than English. This is due to code-switching, but also to the fact that our data dates back to 2009 and Twitter did not allow use of hashtags for several non-English languages until 2012. To filter out non-English, we use the langid (Lui and Baldwin, 2012) (https://github.com/ saffsd/langid.py) library to assign language tags to the tweets. Since the common wisdom in the literature (e.g., (Mohammad, 2012; Wang et al., 2012)) is to restrict data to hashtags occurring in final position of a tweet, we investigate correlations between a tweet’s relevance and emotion hashtag location in Section 4 and test models exclusively on data with hashtags occurring in final position. We also only use tweets conamong the various emotion types. The eight sectors are meant to capture that there are eight primary emotion dimensions arranged as four pairs of oppos"
P17-1067,P11-1015,0,0.13061,"Missing"
P17-1067,E12-1049,0,0.0299375,"than (Mohammad, 2012) and (Volkova and Bachrach, 2016) and the range of emotions we target is much more fine grained than (Mohammad, 2012; Wang et al., 2012; Volkova and Bachrach, 2016) since we model 24 emotion types, rather than focus on ≤ 7 basic emotions. Our emotion modeling relies on distant supervision (Read, 2005; Mintz et al., 2009), the approach of using cues in data (e.g., hashtags or emoticons) as a proxy for “ground truth” labels as we explained above. Distant supervision has been investigated by a number of researchers for emotion detection (Tanaka et al., 2005; Mohammad, 2012; Purver and Battersby, 2012; Wang et al., 2012; Pak and Paroubek, 2010; Yang et al., 2007) and for other semantic tasks such as sentiment analysis (Read, 2005; Go et al., 2009) and sarcasm detection (Gonz´alez-Ib´anez et al., 2011). In these works, authors successfully use emoticons and/or hashtags as marks to label data after performing varying degrees of data quality assurance. We take a similar approach, using a larger collection of tweets, richer emotion definitions, and stronger filtering for tweet quality. The remainder of the paper is organized as follows: We first overview related literature in Section 2, descri"
P17-1067,N13-1090,0,0.0234662,"Missing"
P17-1067,P09-1113,0,0.225266,"a, however, is costly and so it is desirable to develop labeled emotion data without annotators. While the proliferation of social media has made it possible for us to acquire large datasets with implicit labels in the form of hashtags (Mohammad and Kiritchenko, 2015), such labels are noisy and reliable. In this work, we seek to enable deep learning by creating a large dataset of fine-grained emotions using Twitter data. More specifically, we harness cues in Twitter data in the form of emotion hashtags as a way to build a labeled emotion dataset that we then exploit using distant supervision (Mintz et al., 2009) (the use of hashtags as a surrogate for annotator-generated emotion labels) to build emotion models grounded in psychology. We construct such a dataset and exploit it using powerful deep learning methods to build accurate, high coverage models for emotion prediction. Overall, we make the following contributions: 1) Grounded in psychological theory of emotions, we build a large-scale, high quality dataset of tweets labeled with emotions. Key to this are methods to ensure data quality, 2) we validate the data collection method using human annotations, 3) we develop powerful deep learning models"
P17-1067,P05-2008,0,0.018251,"Missing"
P17-1067,D13-1170,0,0.0237764,"Missing"
P17-1067,S12-1033,0,0.0655914,"uch, developing emotion detection models is important; they have a wide array of applications, ranging from building nuanced virtual assistants that cater for the emotions of their users to detecting the emotions of social media users in order to understand their mental and/or physical health. 1 https://en.oxforddictionaries.com/ definition/emotion. 718 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 718–728 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1067 example, (Mohammad, 2012) shows that by using a simple domain adaptation method to train a classifier on their data they are able to improve both precision and recall on the SemEval-2007 (Strapparava and Mihalcea, 2007) dataset. As the author points out, this is another premise that the selflabeled hashtags acquired from Twitter are consistent, to some degree, with the emotion labels given by the trained human judges who labeled the SemEval-2007 data. As pointed out earlier, (Wang et al., 2012) randomly sample a set of 400 tweets from their data and human-label as relevant/irrelevant, as a way to verify the distant su"
P17-1067,S07-1013,0,0.201123,"Missing"
P17-1067,P15-1150,0,0.0612687,"Missing"
P17-1067,D15-1167,0,0.0234391,"Missing"
P17-1067,C14-1018,0,0.0178222,"Missing"
P17-1067,P14-1146,0,0.114972,"Missing"
P17-1067,P16-1148,0,0.0350034,"Missing"
P17-1067,N16-2011,0,0.160285,"Missing"
P17-1067,L16-1183,0,0.0483304,"Missing"
P17-1068,D11-1120,0,0.0191502,"uc-Pietro Positive Psychology Center University of Pennsylvania danielpr@sas.upenn.edu Ye Liu∗ School of Computing National University of Singapore liuye@comp.nus.edu.sg Daniel J. Hopkins Political Science Department University of Pennsylvania danhop@sas.upenn.edu Lyle Ungar Computing & Information Science University of Pennsylvania ungar@cis.upenn.edu Abstract User trait prediction from text is based on the assumption that language use reflects a user’s demographics, psychological states or preferences. Applications include prediction of age (Rao et al., 2010; Flekova et al., 2016b), gender (Burger et al., 2011; Sap et al., 2014), personality (Schwartz et al., 2013; Preot¸iuc-Pietro et al., 2016), socioeconomic status (Preot¸iuc-Pietro et al., 2015a,b; Liu et al., 2016c), popularity (Lampos et al., 2014) or location (Cheng et al., 2010). Research on predicting political orientation has focused on methodological improvements (Pennacchiotti and Popescu, 2011) and used data sets with publicly stated dichotomous political orientation labels due to their easy accessibility (Sylwester and Purver, 2015). However, these data sets are not representative samples of the entire population (Cohen and Ruths, 2013"
P17-1068,P07-1033,0,0.0855173,"Missing"
P17-1068,E14-1043,1,0.81408,"Missing"
P17-1068,P16-1080,1,0.855564,"Missing"
P17-1068,P16-2051,1,0.751888,"of Twitter Users Daniel Preot¸iuc-Pietro Positive Psychology Center University of Pennsylvania danielpr@sas.upenn.edu Ye Liu∗ School of Computing National University of Singapore liuye@comp.nus.edu.sg Daniel J. Hopkins Political Science Department University of Pennsylvania danhop@sas.upenn.edu Lyle Ungar Computing & Information Science University of Pennsylvania ungar@cis.upenn.edu Abstract User trait prediction from text is based on the assumption that language use reflects a user’s demographics, psychological states or preferences. Applications include prediction of age (Rao et al., 2010; Flekova et al., 2016b), gender (Burger et al., 2011; Sap et al., 2014), personality (Schwartz et al., 2013; Preot¸iuc-Pietro et al., 2016), socioeconomic status (Preot¸iuc-Pietro et al., 2015a,b; Liu et al., 2016c), popularity (Lampos et al., 2014) or location (Cheng et al., 2010). Research on predicting political orientation has focused on methodological improvements (Pennacchiotti and Popescu, 2011) and used data sets with publicly stated dichotomous political orientation labels due to their easy accessibility (Sylwester and Purver, 2015). However, these data sets are not representative samples of the entire po"
P17-1068,P14-1105,0,0.170219,"2013), audio (Alam and Riccardi, 2014), text (Preot¸iuc-Pietro et al., 2015a), profile images (Liu et al., 2016a), social data (Van Der Heide et al., 2012; Hall et al., 2014), social networks (Perozzi and Skiena, 2015; Rout et al., 2013), payment data (Wang et al., 2016) and endorsements (Kosinski et al., 2013). Political orientation prediction has been studied in two related, albeit crucially different scenarios, as also identified in (Zafar et al., 2016). First, researchers aimed to identify and quantify orientation of words (Monroe et al., 2008), hashtags (Weber et al., 2013) or documents (Iyyer et al., 2014), 1 3 Data Set The main data set used in this study consists of 3,938 users recruited through the Qualtrics platform (D1 ). Each participant was compensated Data is available at http://www.preotiuc.ro 730 Political Orientation 1000 @JoeBiden, @CoryBooker, @JohnKerry) or US conservative politics (@marcorubio, @tedcruz, @RandPaul, @RealBenCarson). Liberals in our set (Nl = 7417) had to follow on Twitter all of the liberal political figures and none of the conservative figures. Likewise, conservative users (Nc = 6234) had to follow all of the conservative figures and no liberal figures. We downlo"
P17-1068,N13-1090,0,0.0061874,"Missing"
P17-1068,W16-5608,0,0.0543691,"Missing"
P17-1068,W10-0204,0,0.127542,"Missing"
P17-1068,D14-1121,1,0.787582,"ychology Center University of Pennsylvania danielpr@sas.upenn.edu Ye Liu∗ School of Computing National University of Singapore liuye@comp.nus.edu.sg Daniel J. Hopkins Political Science Department University of Pennsylvania danhop@sas.upenn.edu Lyle Ungar Computing & Information Science University of Pennsylvania ungar@cis.upenn.edu Abstract User trait prediction from text is based on the assumption that language use reflects a user’s demographics, psychological states or preferences. Applications include prediction of age (Rao et al., 2010; Flekova et al., 2016b), gender (Burger et al., 2011; Sap et al., 2014), personality (Schwartz et al., 2013; Preot¸iuc-Pietro et al., 2016), socioeconomic status (Preot¸iuc-Pietro et al., 2015a,b; Liu et al., 2016c), popularity (Lampos et al., 2014) or location (Cheng et al., 2010). Research on predicting political orientation has focused on methodological improvements (Pennacchiotti and Popescu, 2011) and used data sets with publicly stated dichotomous political orientation labels due to their easy accessibility (Sylwester and Purver, 2015). However, these data sets are not representative samples of the entire population (Cohen and Ruths, 2013) and do not accura"
P17-1068,D14-1162,0,0.0879074,"Missing"
P17-1068,strapparava-valitutti-2004-wordnet,0,0.020873,"Missing"
P17-1068,P15-1169,1,0.848954,"Missing"
P17-1068,P14-1018,0,0.125913,"Missing"
P17-1068,W10-0723,0,0.00988584,"Missing"
P17-1087,D13-1167,0,0.027165,"r, they explicitly made verb clusters using Dirichlet Process Mixture Models and must-link / cannot-link clustering. Furthermore, they note that cannot-link clustering does not improve performance whereas our signed clustering antonyms are key. Most recent models either attempt to make richer contexts, in order to find semantic similarity, or overlay thesaurus information in a supervised or semi-supervised manner. One line of active research is post processing the word vector embedding by transforming the space using a single or multi-relational objective (Yih et al., 2012; Tang et al., 2014; Chang et al., 2013; Tang et al., 2014; Zhang et al., 2014; Faruqui et al., 2015; Mrkˇsi´c et al., 2016). Alternatively, there are methods to modify the objective function for generating the word embeddings (Ono et al., 2015; Pham et al., 2015; Schwartz et al., 2015). Our approach differs from the aforementioned methods in that we created word clusters using the antonym relationships as negative links. Unlike 2 2.1 Signed Graph Cluster Estimation Signed Normalized Cut Weighted graphs for which the weight matrix is a symmetric matrix in which negative and positive entries are allowed are called signed graphs. 2 T"
P17-1087,J15-4004,0,0.117716,"Missing"
P17-1087,P12-1092,0,0.102217,"oget, and the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) as a gold standard, but manual inspection as well as empirical results showed that none of the automatically generated datasets were a sufficient gold standard. Possibly the symmetric pattern of (Schwartz et al., 2015) would have been sufficient; we did not have time to validate this. Word Embeddings We used four different word embedding methods for evaluation: Skip-gram vectors (word2vec) (Mikolov et al., 2013), Global vectors (GloVe) (Pennington et al., 2014), Eigenwords (Dhillon et al., 2015), and Global Context (GloCon) (Huang et al., 2012); however, we only report the results for word2vec, which is the most popular word embedding (see the supplemental material for other embeddings). We used word2vec 300 dimensional embeddings which were trained on several billion words of English: the Gigaword and the English discussion forum data gathered as part of BOLT. Tokenization was performed using CMU’s Twokenize.4 6.2 6.4 We also evaluated our clusters by using them as features for predicting sentiment, using sentiment treebank 5 (Socher et al., 2013) with coarsegrained labels on phrases and sentences from movie review excerpts. This d"
P17-1087,D14-1181,0,0.00347396,"n to extend the hard signed clustering presented here to probabilistic soft clustering. K were optimized minimizing error using grid search. We compared our model against existing models: Naive Bayes with bag of words (NB) (Socher et al., 2013), sentence word embedding averages (VecAvg), retrofitted sentence word embeddings (RVecAvg) (Faruqui et al., 2015) that incorporate thesaurus information, simple recurrent neural networks (RNN), and two baselines of normalized cuts and signed normalized cuts using only thesaurus information. While the state-of-the art Convolutional Neural Network (CNN) (Kim, 2014) is at 0.881, our model performs quite well with much less information and complexity. Table 5 shows that signed clustering outperforms the baselines of Naive Bayes, normalized cuts, and signed cuts using just thesaurus information. Furthermore, we outperform comparable models, including retrofitting, which has thesaurus information, and the recurrent neural network, which has access to domain specific context information. Signed clustering using only thesaurus information (SC(Thes)) performed significantly worse than all other methods. This was largely due to low coverage; rare words such as"
P17-1087,N15-1184,0,0.0490728,"Missing"
P17-1087,P98-2127,0,0.719244,"Missing"
P17-1087,N13-1092,0,0.0886034,"Missing"
P17-1087,D16-1235,0,0.147329,"Missing"
P17-1087,I13-1056,0,0.0172817,"d Clusters Using Signed Spectral Clustering Jo˜ao Sedoc, Jean Gallier, Lyle Ungar Computer & Information Science University of Pennsylvania joao, jean, ungar@cis.upenn.edu Abstract contexts (Harris, 1954), thus assigning small cosine or euclidean distances between the vector representations of “great” and “awful”. While vector space models (Turney et al., 2010) such as word2vec (Mikolov et al., 2013), Global vectors (GloVe) (Pennington et al., 2014), or Eigenwords (Dhillon et al., 2015) capture relatedness, they do not adequately encode synonymy and semantic similarity (Mohammad et al., 2013; Scheible et al., 2013). Our goal is to create clusters of synonyms or semantically equivalent words and linguistically motivated unified constructs. Signed graphs, which are graphs with negative edge weights, were first introduced by Cartwright and Harary (1956). However, signed graph clustering for multiclass normalized cuts (K-clusters) has been largely unexplored until recently. We present a novel theory and method that extends multiclass normalized cuts (K-cluster) of Yu and Shi (2003) to signed graphs (Gallier, 2016)1 and the work of Kunegis et al. (2010) to K-clustering. This extension allows the incorporatio"
P17-1087,J13-3004,0,0.0576665,"Missing"
P17-1087,N16-1018,0,0.0607067,"Missing"
P17-1087,K15-1026,0,0.205827,"Most recent models either attempt to make richer contexts, in order to find semantic similarity, or overlay thesaurus information in a supervised or semi-supervised manner. One line of active research is post processing the word vector embedding by transforming the space using a single or multi-relational objective (Yih et al., 2012; Tang et al., 2014; Chang et al., 2013; Tang et al., 2014; Zhang et al., 2014; Faruqui et al., 2015; Mrkˇsi´c et al., 2016). Alternatively, there are methods to modify the objective function for generating the word embeddings (Ono et al., 2015; Pham et al., 2015; Schwartz et al., 2015). Our approach differs from the aforementioned methods in that we created word clusters using the antonym relationships as negative links. Unlike 2 2.1 Signed Graph Cluster Estimation Signed Normalized Cut Weighted graphs for which the weight matrix is a symmetric matrix in which negative and positive entries are allowed are called signed graphs. 2 The full exposition by Gallier (2016) is available on arXiv. 940 Such graphs (with weights (−1, 0, +1)) were introduced as early as 1953 by (Harary, 1953), to model social relations involving disliking, indifference, and liking. The problem of clust"
P17-1087,N15-1100,0,0.012369,"r signed clustering antonyms are key. Most recent models either attempt to make richer contexts, in order to find semantic similarity, or overlay thesaurus information in a supervised or semi-supervised manner. One line of active research is post processing the word vector embedding by transforming the space using a single or multi-relational objective (Yih et al., 2012; Tang et al., 2014; Chang et al., 2013; Tang et al., 2014; Zhang et al., 2014; Faruqui et al., 2015; Mrkˇsi´c et al., 2016). Alternatively, there are methods to modify the objective function for generating the word embeddings (Ono et al., 2015; Pham et al., 2015; Schwartz et al., 2015). Our approach differs from the aforementioned methods in that we created word clusters using the antonym relationships as negative links. Unlike 2 2.1 Signed Graph Cluster Estimation Signed Normalized Cut Weighted graphs for which the weight matrix is a symmetric matrix in which negative and positive entries are allowed are called signed graphs. 2 The full exposition by Gallier (2016) is available on arXiv. 940 Such graphs (with weights (−1, 0, +1)) were introduced as early as 1953 by (Harary, 1953), to model social relations involving disliking, ind"
P17-1087,D13-1170,0,0.264893,"nington et al., 2014), Eigenwords (Dhillon et al., 2015), and Global Context (GloCon) (Huang et al., 2012); however, we only report the results for word2vec, which is the most popular word embedding (see the supplemental material for other embeddings). We used word2vec 300 dimensional embeddings which were trained on several billion words of English: the Gigaword and the English discussion forum data gathered as part of BOLT. Tokenization was performed using CMU’s Twokenize.4 6.2 6.4 We also evaluated our clusters by using them as features for predicting sentiment, using sentiment treebank 5 (Socher et al., 2013) with coarsegrained labels on phrases and sentences from movie review excerpts. This dataset is widely used for the evaluation of sentiment analysis. We used the standard partition of the treebank into training (6920), development (872), and test (1821) sets. Thesauri Several thesauri were used in order to test the robustness including Roget’s Thesaurus (Roget, 1852), the Microsoft Word English (MS Word) thesaurus from Samsonovic et al. (2010) and WordNet 3.0 (Miller, 1995). We chose a subset of 5108 words for the training dataset, which had high overlap between various sources. Changes to the"
P17-1087,D14-1162,0,0.0896904,"Missing"
P17-1087,P14-1146,0,0.0414835,"al. (2009). However, they explicitly made verb clusters using Dirichlet Process Mixture Models and must-link / cannot-link clustering. Furthermore, they note that cannot-link clustering does not improve performance whereas our signed clustering antonyms are key. Most recent models either attempt to make richer contexts, in order to find semantic similarity, or overlay thesaurus information in a supervised or semi-supervised manner. One line of active research is post processing the word vector embedding by transforming the space using a single or multi-relational objective (Yih et al., 2012; Tang et al., 2014; Chang et al., 2013; Tang et al., 2014; Zhang et al., 2014; Faruqui et al., 2015; Mrkˇsi´c et al., 2016). Alternatively, there are methods to modify the objective function for generating the word embeddings (Ono et al., 2015; Pham et al., 2015; Schwartz et al., 2015). Our approach differs from the aforementioned methods in that we created word clusters using the antonym relationships as negative links. Unlike 2 2.1 Signed Graph Cluster Estimation Signed Normalized Cut Weighted graphs for which the weight matrix is a symmetric matrix in which negative and positive entries are allowed are calle"
P17-1087,P15-2004,0,0.125308,"g antonyms are key. Most recent models either attempt to make richer contexts, in order to find semantic similarity, or overlay thesaurus information in a supervised or semi-supervised manner. One line of active research is post processing the word vector embedding by transforming the space using a single or multi-relational objective (Yih et al., 2012; Tang et al., 2014; Chang et al., 2013; Tang et al., 2014; Zhang et al., 2014; Faruqui et al., 2015; Mrkˇsi´c et al., 2016). Alternatively, there are methods to modify the objective function for generating the word embeddings (Ono et al., 2015; Pham et al., 2015; Schwartz et al., 2015). Our approach differs from the aforementioned methods in that we created word clusters using the antonym relationships as negative links. Unlike 2 2.1 Signed Graph Cluster Estimation Signed Normalized Cut Weighted graphs for which the weight matrix is a symmetric matrix in which negative and positive entries are allowed are called signed graphs. 2 The full exposition by Gallier (2016) is available on arXiv. 940 Such graphs (with weights (−1, 0, +1)) were introduced as early as 1953 by (Harary, 1953), to model social relations involving disliking, indifference, and liki"
P17-1087,E09-1077,0,0.0619706,"Missing"
P17-1087,D12-1111,0,0.205213,"Missing"
P17-1087,D14-1161,0,0.0168933,"sing Dirichlet Process Mixture Models and must-link / cannot-link clustering. Furthermore, they note that cannot-link clustering does not improve performance whereas our signed clustering antonyms are key. Most recent models either attempt to make richer contexts, in order to find semantic similarity, or overlay thesaurus information in a supervised or semi-supervised manner. One line of active research is post processing the word vector embedding by transforming the space using a single or multi-relational objective (Yih et al., 2012; Tang et al., 2014; Chang et al., 2013; Tang et al., 2014; Zhang et al., 2014; Faruqui et al., 2015; Mrkˇsi´c et al., 2016). Alternatively, there are methods to modify the objective function for generating the word embeddings (Ono et al., 2015; Pham et al., 2015; Schwartz et al., 2015). Our approach differs from the aforementioned methods in that we created word clusters using the antonym relationships as negative links. Unlike 2 2.1 Signed Graph Cluster Estimation Signed Normalized Cut Weighted graphs for which the weight matrix is a symmetric matrix in which negative and positive entries are allowed are called signed graphs. 2 The full exposition by Gallier (2016) is"
P17-1087,C98-2122,0,\N,Missing
P17-2013,P16-1080,1,0.850033,"multivariate Bernoulli with those based on multinomial distributions for document classification. Jansche Introduction NLP for studying people has grown rapidly as more than one-third of the human population use social media actively.1 While traditional NLP tasks (e.g. POS tagging, parsing, sentiment analysis) mostly work at the word, sentence, or document level, the increased focus on social scientific applications has shifted attention to new levels of analysis (e.g. user-level and communitylevel) (Koppel et al., 2009; Sarawgi et al., 2011; Schwartz et al., 2013a; Coppersmith et al., 2014; Flekova et al., 2016). Figure 1 shows the distribution of two unigrams, ‘the’ and ‘love’ at three levels of analysis. While both words have zero counts in most messages, ‘the’ starts to look Normal across 2 While the distribution of word frequencies (i.e. a Zipfian distribution) is often discussed in NLP, it is important to note that we are focused on the distribution of single features (e.g. words) over documents, users, or communities. 3 While other sources of corpora can also be aggregated to the user- or community-level (e.g. newswire, books), we believe the question of distributions is particularly important"
P17-2013,W14-3207,0,0.0234073,"babilistic models based on multivariate Bernoulli with those based on multinomial distributions for document classification. Jansche Introduction NLP for studying people has grown rapidly as more than one-third of the human population use social media actively.1 While traditional NLP tasks (e.g. POS tagging, parsing, sentiment analysis) mostly work at the word, sentence, or document level, the increased focus on social scientific applications has shifted attention to new levels of analysis (e.g. user-level and communitylevel) (Koppel et al., 2009; Sarawgi et al., 2011; Schwartz et al., 2013a; Coppersmith et al., 2014; Flekova et al., 2016). Figure 1 shows the distribution of two unigrams, ‘the’ and ‘love’ at three levels of analysis. While both words have zero counts in most messages, ‘the’ starts to look Normal across 2 While the distribution of word frequencies (i.e. a Zipfian distribution) is often discussed in NLP, it is important to note that we are focused on the distribution of single features (e.g. words) over documents, users, or communities. 3 While other sources of corpora can also be aggregated to the user- or community-level (e.g. newswire, books), we believe the question of distributions is"
P17-2013,P03-1037,0,0.0794531,"). The bar on the left of each plot represents the percentage of observations that are zero for each feature where the shading represents the percent of features reaching the given threshold. As the bar gets darker it means more features out of 500 are zero in that percentage of individuals. The right portion of each plot is based on standardized relative frequencies of the variables (mean centered and divided by the standard deviation). component is governed by a Bernoulli distribution that generates excess zeros, while the second component generates counts, some of which also could be zero (Jansche, 2003). is semi-transparent such that an aggregate trend across multiple features will emerge darkest. As we move along a row ranging specific features (unigrams) to generic features (lexicon), the empirical distribution gradually changes from resembling a “power law” (or binomial distribution with low number of trials and probability of success) to something more “Normal”. Similar shifts are also observed as we move across levels of modeling. We investigate whether the best-fitting distributions vary across the three levels of analysis and three types of lexical features. We consider the following"
P17-2013,W11-0310,0,0.0244414,"llum et al. (1998) compares the results of probabilistic models based on multivariate Bernoulli with those based on multinomial distributions for document classification. Jansche Introduction NLP for studying people has grown rapidly as more than one-third of the human population use social media actively.1 While traditional NLP tasks (e.g. POS tagging, parsing, sentiment analysis) mostly work at the word, sentence, or document level, the increased focus on social scientific applications has shifted attention to new levels of analysis (e.g. user-level and communitylevel) (Koppel et al., 2009; Sarawgi et al., 2011; Schwartz et al., 2013a; Coppersmith et al., 2014; Flekova et al., 2016). Figure 1 shows the distribution of two unigrams, ‘the’ and ‘love’ at three levels of analysis. While both words have zero counts in most messages, ‘the’ starts to look Normal across 2 While the distribution of word frequencies (i.e. a Zipfian distribution) is often discussed in NLP, it is important to note that we are focused on the distribution of single features (e.g. words) over documents, users, or communities. 3 While other sources of corpora can also be aggregated to the user- or community-level (e.g. newswire, bo"
P17-2013,D13-1187,0,0.0569268,"Missing"
P17-2103,W12-1614,0,0.068548,"Missing"
P17-2103,W15-4612,0,0.352173,"Missing"
P17-2103,P09-1077,0,0.151697,"Missing"
P17-2103,prasad-etal-2008-penn,0,0.11777,"Missing"
P17-2103,P11-2008,0,0.0884882,"Missing"
P17-2103,P14-1002,0,0.0591999,"Missing"
P17-2103,D09-1036,0,\N,Missing
P18-2032,P17-1068,1,0.896438,"Missing"
P18-2032,Q16-1003,0,0.0447774,"or hundreds of years, we address a research gap by exploring finer temporal granularity and using a more accessible language corpus. Twitter’s1 discourse is rather different from traditional English writing. So far, word embeddings trained on Twitter (Kulkarni et al., 2015; Mikolov et al., 2013) have considered it a static corpus, and have not used it to study short term changes in word connotations. It contributes with the following observations: Introduction Natural languages are dynamic–they are constantly evolving and adapting to the needs of their users and the environment of their use (Frermann and Lapata, 2016). The arrival of large-scale collections of historic texts and online libraries and Google Books have greatly facilitated computational investigations of language change over the span of decades. Diachronic differences measure semantic drift specifically for languages over time. For instance, the meaning of the word ‘follow’ has changed from a reference, then to surveillance, and finally to the act of subscribing to a social media user’s feed. In a quantitative analysis, diachronic differences may explain why predictive models go ‘stale’. For instance, a sentiment model trained on Victorian-er"
P18-2032,I17-1077,1,0.860422,"ge e.g. social media. We illustrate that it is possible to measure diachronic semantic drifts within social media and within the span of a few years. Furthermore, we are arguing that yearrelated change affects different cohorts differently. 3.2 Language insights about diachronic differences We use language for the following insights into diachronic drift: • Important changes: For each temporal cohort, we identify the language features which have the most drift in terms of recalibrated coefficients, in regression models trained in 2011 vs. 2015. In doing so, we follow the approach described by Rieman et al. (2017) to compare the standardized coefficients of the age and gender models trained on the language samples from 2011 and 2015. • High drift concepts: We use word embeddings to identify the semantic differences in the connotations around common concepts, for two sets of users who are a generation apart. Following the framework proposed by Garg et al. (2017), we calculate semantic drift as the changes in the relative normalized distance for the context words describing a set of target concepts. Target concepts comprise a group of words representing a single idea (Garg et al., 2017), for instance, po"
P18-2032,D16-1229,0,0.428397,", e.g. 20-year-olds in 2011 vs. 20-year-olds in 2015. 1 https://twitter.com/ 195 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 195–200 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related work 3.1 We test the predictive performance of language models trained on a year’s worth of social media posts from a subset of users who have provided their age and gender information. • We train language models on the age- and gender-labeled primary dataset and evaluate their diachronic validity (Hamilton et al., 2016b), i.e. their predictive performance on subsequently collected language samples. • We identify age groups which drift faster than others by reporting predictive performance on users, stratified by year of birth. A number of studies have built language models to predict users’ age and gender (Sap et al., 2014), personality (Schwartz et al., 2013) and other traits (Jaidka et al., 2018a) with high accuracy from a sample of their social media posts. We offer the explanation that these language models may have ‘degraded’ due to the diachronic changes in language over the past few years, as compare"
P18-2032,D14-1121,1,0.913685,"t the predictive performance of language models trained on a year’s worth of social media posts from a subset of users who have provided their age and gender information. • We train language models on the age- and gender-labeled primary dataset and evaluate their diachronic validity (Hamilton et al., 2016b), i.e. their predictive performance on subsequently collected language samples. • We identify age groups which drift faster than others by reporting predictive performance on users, stratified by year of birth. A number of studies have built language models to predict users’ age and gender (Sap et al., 2014), personality (Schwartz et al., 2013) and other traits (Jaidka et al., 2018a) with high accuracy from a sample of their social media posts. We offer the explanation that these language models may have ‘degraded’ due to the diachronic changes in language over the past few years, as compared to the predictions on their posts in 2011, which is closer to the time period for which their model was actually trained (see Figure 1). The work by Frerman and Lapata (2016) quantified meaning change in terms of emerging meanings over many time periods, on a corpus collating documents spanning the years 170"
P18-2032,P16-1141,0,0.335723,", e.g. 20-year-olds in 2011 vs. 20-year-olds in 2015. 1 https://twitter.com/ 195 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 195–200 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related work 3.1 We test the predictive performance of language models trained on a year’s worth of social media posts from a subset of users who have provided their age and gender information. • We train language models on the age- and gender-labeled primary dataset and evaluate their diachronic validity (Hamilton et al., 2016b), i.e. their predictive performance on subsequently collected language samples. • We identify age groups which drift faster than others by reporting predictive performance on users, stratified by year of birth. A number of studies have built language models to predict users’ age and gender (Sap et al., 2014), personality (Schwartz et al., 2013) and other traits (Jaidka et al., 2018a) with high accuracy from a sample of their social media posts. We offer the explanation that these language models may have ‘degraded’ due to the diachronic changes in language over the past few years, as compare"
P18-2032,D17-2010,1,0.891806,"Missing"
P18-2032,P17-2071,0,0.291071,"the predictions on their posts in 2011, which is closer to the time period for which their model was actually trained (see Figure 1). The work by Frerman and Lapata (2016) quantified meaning change in terms of emerging meanings over many time periods, on a corpus collating documents spanning the years 1700-2010. Studies measuring semantic drift using word embedding models trained on Twitter corpora, such as Twitter GloVe and Word2Vec (Mikolov et al., 2013; Kulkarni et al., 2015), have considered microblog posts a static resource, reflective of modern language usage at a single point in time. Szymanski (2017) highlights the need to explore it in contemporary language e.g. social media. We illustrate that it is possible to measure diachronic semantic drifts within social media and within the span of a few years. Furthermore, we are arguing that yearrelated change affects different cohorts differently. 3.2 Language insights about diachronic differences We use language for the following insights into diachronic drift: • Important changes: For each temporal cohort, we identify the language features which have the most drift in terms of recalibrated coefficients, in regression models trained in 2011 vs"
P18-2032,S13-2053,0,0.0225143,"Missing"
S12-1101,P05-1047,0,0.0199863,"abeled data.) Each dimension of these representations captures latent information about a combination of syntactic and semantic word properties. In the original paper, the word embeddings are context-specific. For this task, we only use context-oblivious embeddings i.e. one embedding per word type for this task, based 680 on their model. Word similarity can then be calculated as cosine similarity between the eigenword representation vectors for any two words. To move from word-level similarity to sentencelevel a few more steps are necessary. We adapted the method of matrix similarity given by Stevenson and Greenwood (2005). One calculates similarity between all pairs of words, and each sentence is represented as a binary vector (with elements equal to 1 if a word is present and 0 otherwise). The similarity between these sentences vectors ~a and ~b is given by: ~aW~b s(~a, ~b) = |~a||~b| (1) where W is a semantic similarity matrix containing information about the similarity of word pairs. Each element in matrix W represents the similarity of words according to some lexical or spectral similarity measure. 2.3 Selector Similarity Another novel method to account for the similarity between words is via comparison of"
S12-1101,O97-1002,0,0.0565332,"d previously via regression (Pedregosa et al., 2011). We included the following sets of features: Other Similarity Metrics Knowledge-Based. We use WordNet to calculate semantic distances between all open-class words in the sentence pairs. There are three classifications of similarity metrics over WordNet: path-based, information- content based, and gloss-based (Pederson et al., 2004). We chose to incorporate those measures performing best in the Schwartz & Gomez (2011) application-oriented evaluation: (a) the pathbased measure of Schwartz & Gomez (2008); (b) the information-content measure of Jiang & Conrath (1997) utilizing the difference in information content between concepts and their point of intersection; (c) the gloss-based measure of Patwardhan & Pedersen (2006). By including metrics utilizing different sources of information, we suspect they will each have something novel to contribute. Because WordNet provides similarity between concepts (word senses), we take the maximum similarity between all senses of each word to be the similarity between the two words. Such similarity can then be computed between multiple pairs of words to populate the semantic similarity matrix W in formula (1) and gener"
S12-1101,P97-1009,0,0.241406,"Missing"
S12-1101,N04-3012,0,0.0503307,"Missing"
S12-1101,W08-2114,1,0.881149,"similarity between all pairs of words, and each sentence is represented as a binary vector (with elements equal to 1 if a word is present and 0 otherwise). The similarity between these sentences vectors ~a and ~b is given by: ~aW~b s(~a, ~b) = |~a||~b| (1) where W is a semantic similarity matrix containing information about the similarity of word pairs. Each element in matrix W represents the similarity of words according to some lexical or spectral similarity measure. 2.3 Selector Similarity Another novel method to account for the similarity between words is via comparison of Web selectors (Schwartz and Gomez, 2008). Selectors are words that take the place of an instance of a target word within its local context. For example, in “he addressed the strikers at the rally”, selectors for ‘strikers’ might be ‘crowd’, ‘audience’, ‘workers’, or ‘students’ words which can realize the same constituent position as the target word. Since selectors are determined based on the context, a set of selectors is an abstraction for the context of a word instance. Thus, comparing selector sets produces a measure of word instance similarity. A key difference between selectors and the eigenwords used in this paper are that se"
S12-1101,P10-1040,0,\N,Missing
S13-1042,baccianella-etal-2010-sentiwordnet,0,0.0344651,"Missing"
S13-1042,P97-1023,0,0.216743,"Missing"
S13-1042,W06-1642,0,0.0696434,"Missing"
S13-1042,C04-1200,0,0.301213,"Missing"
S13-1042,lin-etal-2010-new,0,0.0408064,"Missing"
S13-1042,P04-1036,0,0.121164,"Missing"
S13-1042,H93-1061,0,0.118574,"nstead we leverage part-of-speech and word sense data to help us determine which words are lexically ambiguous. 297 Figure 1: The relationship between text expressing positive emotion (POSEMO) and text containing LIWC terms for POSEMO. Our approach of eliminating ambiguous words increases the precision at the expense of recall, a reasonable trade-off in social media where we are working with millions or even billions of word instances. Additionally, it is minimally-supervised, in that we do not require training data on human-state; instead we use existing hand-labeled corpora, such as SemCor (Miller et al., 1993), for word sense information. Not requiring training data also means our refinement is flexible; it can be applied to multiple domains and lexica, it makes few assumptions that might introduce problems of over-fitting, and it is parsimonious in that it merely improves an established approach. This paper makes two primary contributions: (1) an analysis of the types of errors common for the word count approach (Section 3), and (2) a general method for refining psychosocial lexica based on the ambiguity of words (Section 4). Before describing these contributions, we discuss related work, making t"
S13-1042,W02-1011,0,0.0176418,"Missing"
S13-1042,J09-3003,0,0.0396365,"Missing"
S13-1042,H93-1052,0,0.317146,"Missing"
W04-3111,kingsbury-palmer-2002-treebank,1,\N,Missing
W04-3111,P03-1002,0,\N,Missing
W04-3111,N03-1028,0,\N,Missing
W04-3111,P96-1008,0,\N,Missing
W04-3111,tateisi-tsujii-2004-part,0,\N,Missing
W14-3214,S13-2053,0,0.114763,"ession as assessed by surveys. Red line is a LOESS smoothed trend (+/- 1 SE) over the average of scores from users who completed the survey on that day. rate). Figure 1a shows the distribution of surveyassessed DDep (standardized). The items can be seen in Table 1. Figure 2 shows the daily averages of surveyassessed DDep, collapsed across years. A LOESS smoother over the daily averages illustrates a seasonal trend, with depression rising over the winter months and dropping during the summer. lexica: 64 LIWC categories (Pennebaker et al., 2007) as well as the sentiment lexicon from NRC Canada (Mohammad et al., 2013).2 Usage of a lexicon (lex) was calculated similar to the LDA topics, where w is the weight of the word in the lexicon in the case of sentiment and always 1 in the case of LIWC which has no weights: Regression modeling. In order to get a continuous value output from our model, we explored regression techniques over our training data. Since this first work exploring regression was concerned primarily with language content, our features for predicting depression were based entirely on language use (other social media activity and friend networks may be considered in future work). These features"
W15-1203,W14-3207,0,0.256668,"ne 5, 2015. 2015 Association for Computational Linguistics mental illness. We show that a model which uses only the text-predicted user level ‘Big Five’ personality dimensions plus age and gender perform with high accuracy, comparable to methods that use standard dictionaries of psychology as features. Users who self-report a diagnosis appear more neurotic and more introverted when compared to average users. 2 Data We use a dataset of Twitter users reported to suffer from a mental illness, specifically depression and post traumatic stress disorder (PTSD). This dataset was first introduced in (Coppersmith et al., 2014a). The self-reports are collected by searching a large Twitter archive for disclosures using a regular expression (e.g. ‘I have been diagnosed with depression’). Candidate users were filtered manually and then all their most recent tweets have been continuously crawled using the Twitter Search API. The selfdisclosure messages were excluded from the dataset and from the estimation of user inferred demographics and personality scores. The control users were selected at random from Twitter. In total there are 370 users diagnosed only with PTSD, 483 only with depression and 1104 control users. On"
W15-1203,E14-1043,1,0.570596,"Missing"
W15-1203,W11-1515,0,0.19154,"Missing"
W15-1203,W15-1205,1,0.892742,"Missing"
W15-1203,D14-1121,1,0.12773,"Missing"
W15-1203,N15-1044,1,0.787795,"uild a large dataset of users and their textual information. 3 Features We use the Twitter posts of a user to infer several user traits which we expect to be relevant to mental illnesses based on standard clinical criteria (American Psychiatric Association, 2013). Recently, automatic user profiling methods have used on usergenerated text and complementary features in order to predict different user traits such as: age (Nguyen et al., 2011), gender (Sap et al., 2014), location (Cheng et al., 2010), impact (Lampos et al., 2014), political preference (Volkova et al., 2014), temporal orientation (Schwartz et al., 2015) or personality (Schwartz et al., 2013). 22 3.1 Age, Gender and Personality We use the methods developed in (Schwartz et al., 2013) to assign each user scores for age, gender and personality from the popular five factor model of personality – ‘Big Five’ – (McCrae and John, 1992), which consists of five dimensions: extraversion, agreeableness, conscientiousness, neuroticism and openness to experience. The model was trained on a large sample of around 70,000 Facebook users who have taken Big Five personality tests and shared their posts using a model using 1-3 grams and topics as features (Park"
W15-1203,P14-1018,0,0.123068,"Missing"
W15-1205,J92-4003,0,0.248875,"Missing"
W15-1205,W14-3207,0,0.123307,"Missing"
W15-1205,W15-1204,0,0.109158,"Missing"
W15-1205,E14-1043,1,0.854115,"Missing"
W15-1205,N13-1090,0,0.00236123,"PMI). This information-theoretic measure indicates which words co-occur in the same context (Bouma, 2009) where the context is the entire tweet. To obtain hard clusters of words we use spectral clustering (Shi and Malik, 2000; Ng et al., 2002). This methods was shown to deal well with highdimensional and non-convex data (von Luxburg, 2007). In our experiments we used 1000 clusters from 54,592 tokens. Word2Vec Word Clusters (W2V) Neural methods have recently been gaining popularity in order to obtain low-rank word embeddings and obtained state-of-the-art results for a number of semantic tasks (Mikolov et al., 2013b). These methods, like many recent word embeddings, also allow to capture local context order rather than just ‘bag-of-words’ relatedness, which leads to also capture syntactic information. We use the skip-gram model with negative sampling (Mikolov et al., 2013a) to learn word embeddings from a corpus of 400 million tweets also used in (Lampos et al., 2014). We use a hidden layer size of 50 with the Gensim implementation.2 We then apply spectral clustering on these embeddings to obtain hard clusters of words. We create 2000 clusters from 46,245 tokens. GloVe Word Clusters (GloVe) A different"
W15-1205,N13-1039,0,0.0269951,"Missing"
W15-1205,D14-1162,0,0.115824,"Missing"
W15-1205,W15-1203,1,0.90032,"Missing"
W15-1205,D14-1121,1,0.531179,"Missing"
W16-0404,N10-1122,0,0.00952595,"al media. This can be used in applications such as mental illness detection or in automated large-scale psychological studies. 1 Introduction Sentiment analysis is a very active research area that aims to identify, extract and analyze subjective information from text (Pang and Lee, 2008). This generally includes identifying if a piece of text is subjective or objective, what sentiment it expresses (positive or negative; often referred to as valence), 9 what emotion it conveys (Strapparava and Mihalcea, 2007) and towards which entity or aspect of the text i.e., aspect based sentiment analysis (Brody and Elhadad, 2010). Downstream applications are mostly interested in automatically inferring public opinion about products or actions. Besides expressing attitudes towards other objects, texts can also express the emotions of the ones writing them, most common recently with the rise of Social Media usage (Rosenthal et al., 2015). This study focuses on presenting a gold standard data set as well as a model trained on this data in order to drive research in learning about the affective norms of people posting subjective messages. This is of great interest to applications in social science which study text at a la"
W16-0404,P15-1073,0,0.0187284,"s with age for both genders, especially at the start and end of our age intervals (13–16 and 30–35), confirming the aging positivity bias (Mather and Carstensen, 2005). Valence is higher for females across almost the entire age range. Posts written by females are also significantly higher in arousal for all age groups. Age does not play a significant effect in post arousal, although there is a slight increase with age especially for females. Overall, these figures again illustrate the importance of age and gender as factors to be considered in these types of application (Volkova et al., 2013; Hovy, 2015). 3 Predicting Valence and Arousal To study the linguistic differences of both dimensions, we build a bag-of-words prediction model of valence and arousal from our corpus.2 We train two linear regression models with `2 regularisation on the posts and test their predictive power in a 10fold cross-validation setup. Results for predicting the two scores are presented in Table 4. We compare to a number of different existing general purpose lexicons. First, we use the ANEW (Bradley and Lang, 1999) weighted dictionary to compute a valence and arousal score as the weighted sum of individual word vale"
W16-0404,S13-2053,0,0.0134793,"s 700 500 300 300 100 100 1 2 3 4 5 6 Valence 7 8 9 1 (a) Valence. 2 3 4 5 6 Arousal 7 8 9 (b) Arousal. Figure 1: Histrograms of average rating scores. of words obtained by extending ANEW with human ratings for ∼14000 words (Warriner et al., 2013). We also benchmark with standard methods for estimating valence from sentiment analysis. First, we use the MPQA lexicon (Wilson et al., 2005), which contains 7629 words rated for positive or negative sentiment, to obtain a score based on the difference between positive and negative words in the post. Second, we use the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013), which obtained the best performance on the Semeval Twitter Sentiment Analysis tasks.3 Our method achieves very high correlations with the target score. Arousal is easier to predict, reaching r = 0.85 correlation between predicted and rater score. ANEW obtains significant correlations with both of our ratings, however these are significantly lower than our model. The extended list of affective norms obtains, perhaps surprisingly, lower correlation for valence, but stronger correlation with arousal than ANEW. For valence, both sentiment analysis lexicons provide better performance 3 https://ww"
W16-0404,S15-2078,0,0.0307292,"identifying if a piece of text is subjective or objective, what sentiment it expresses (positive or negative; often referred to as valence), 9 what emotion it conveys (Strapparava and Mihalcea, 2007) and towards which entity or aspect of the text i.e., aspect based sentiment analysis (Brody and Elhadad, 2010). Downstream applications are mostly interested in automatically inferring public opinion about products or actions. Besides expressing attitudes towards other objects, texts can also express the emotions of the ones writing them, most common recently with the rise of Social Media usage (Rosenthal et al., 2015). This study focuses on presenting a gold standard data set as well as a model trained on this data in order to drive research in learning about the affective norms of people posting subjective messages. This is of great interest to applications in social science which study text at a large scale and with orders of magnitude more users than traditional studies. Emotion classification is a widely debated topic in psychology (Gendron and Barrett, 2009). Two main theories about emotions exist: the first posits a discrete and finite set of emotions, while the second suggests that emotions are a co"
W16-0404,S07-1013,0,0.384501,"th arousal annotations. Our data set offers a building block to a deeper study of personal affect as expressed in social media. This can be used in applications such as mental illness detection or in automated large-scale psychological studies. 1 Introduction Sentiment analysis is a very active research area that aims to identify, extract and analyze subjective information from text (Pang and Lee, 2008). This generally includes identifying if a piece of text is subjective or objective, what sentiment it expresses (positive or negative; often referred to as valence), 9 what emotion it conveys (Strapparava and Mihalcea, 2007) and towards which entity or aspect of the text i.e., aspect based sentiment analysis (Brody and Elhadad, 2010). Downstream applications are mostly interested in automatically inferring public opinion about products or actions. Besides expressing attitudes towards other objects, texts can also express the emotions of the ones writing them, most common recently with the rise of Social Media usage (Rosenthal et al., 2015). This study focuses on presenting a gold standard data set as well as a model trained on this data in order to drive research in learning about the affective norms of people po"
W16-0404,strapparava-valitutti-2004-wordnet,0,0.0311959,"ons in social science which study text at a large scale and with orders of magnitude more users than traditional studies. Emotion classification is a widely debated topic in psychology (Gendron and Barrett, 2009). Two main theories about emotions exist: the first posits a discrete and finite set of emotions, while the second suggests that emotions are a combination of different scales. Research in Natural Language Processing (NLP) has been focused mostly on Ekman’s model of emotion (Ekman, 1992) which posits the existence of six basic emotions: anger, disgust, fear, joy, sadness and surprise (Strapparava and Valitutti, 2004; Strapparava and Mihalcea, 2008; Calvo and D’Mello, 2010). In this study, we focus on the most popular dimensional model of emotion: the circumplex model introduced in (Russell, 1980). This model suggests that all affective Proceedings of NAACL-HLT 2016, pages 9–15, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics states are represented in a two-dimensional space with two independent neurophysiological systems: valence (or sentiment) and arousal. Any affective experience is a linear combination of these two independent systems, which is then interprete"
W16-0404,D13-1187,0,0.0129217,"data. Valence increases with age for both genders, especially at the start and end of our age intervals (13–16 and 30–35), confirming the aging positivity bias (Mather and Carstensen, 2005). Valence is higher for females across almost the entire age range. Posts written by females are also significantly higher in arousal for all age groups. Age does not play a significant effect in post arousal, although there is a slight increase with age especially for females. Overall, these figures again illustrate the importance of age and gender as factors to be considered in these types of application (Volkova et al., 2013; Hovy, 2015). 3 Predicting Valence and Arousal To study the linguistic differences of both dimensions, we build a bag-of-words prediction model of valence and arousal from our corpus.2 We train two linear regression models with `2 regularisation on the posts and test their predictive power in a 10fold cross-validation setup. Results for predicting the two scores are presented in Table 4. We compare to a number of different existing general purpose lexicons. First, we use the ANEW (Bradley and Lang, 1999) weighted dictionary to compute a valence and arousal score as the weighted sum of individ"
W16-0404,H05-1044,0,0.0554045,"ia, June 12-17, 2016. 2016 Association for Computational Linguistics states are represented in a two-dimensional space with two independent neurophysiological systems: valence (or sentiment) and arousal. Any affective experience is a linear combination of these two independent systems, which is then interpreted as representing a particular emotion. For example, fear is a state involving the combination of negative valence and high arousal (Posner et al., 2005). Previous research in NLP focused mostly on valence or sentiment, either binary or having a strength component coupled with sentiment (Wilson et al., 2005; Thelwall et al., 2010; Thelwall et al., 2012). In this paper we build a new data set consisting of 2895 anonymized Facebook posts labeled with both valence and arousal by two annotators with psychology training. The ratings are made on two independent nine point scales, reaching a high agreement correlations of .768 for valence and .827 for arousal. Data set statistics suggest that while the dimensions of valence and arousal are associated, they present distinct information, especially in posts with a clear positive or negative valence. Further, we train a bag-of-words linear regression mode"
W17-2903,N13-1092,0,0.0964836,"Missing"
W17-2903,W14-3207,0,0.0402342,"Missing"
W17-2903,P15-1073,0,0.0208051,"ed by personality, with the Five Factor Model or the ‘Big Five’ being the most widely used model for representing personality. This posits the existence of five traits in which people vary: openness to experience, conscientiousness, extraversion, agreeableness and neuroticism (McCrae and John, 1992). Methods for user trait prediction can uncover sociological insight into user behaviour or implicit biases and also improve a range of applications in recommender systems, targeted marketing or in natural language processing where they can lead to improvements in tasks such as text classification (Hovy, 2015) or sentiment analysis (Volkova et al., 2013). While these methods achieve good predictive performance, they pose significant challenges to the anonymization of identity online. Most differences in language use across traits are topical. For example, users high in extraversion post more about social activities (‘party’, ‘cant wait’, ‘weekend’), while introverts prefer to post more about computer related activities (‘Internet’, ‘computer’, ‘anime’). Users high in neuroticism post about their negative feelings (‘depressed’, ‘sick of’, ‘lonely’), while users low in neuroticism post more about rel"
W17-2903,E14-1043,1,0.871057,"Missing"
W17-2903,D10-1124,0,0.017771,"Missing"
W17-2903,P16-1080,1,0.898893,"Missing"
W17-2903,P16-2051,1,0.843517,"hypotheses. This work is relevant to future applications that aim to personalize text generation to specific personality types. 1 Introduction The task of user trait prediction from text has increased in popularity and importance with the availability of user generated content which encodes various information about the author of the text. Using machine learning techniques and large data sets, past research managed to predict with varying degrees of accuracy a series of both demographic traits such as age (Rao et al., 2010; Sap et al., 2014), gender (Burger et al., 2011; Rangel et al., 2015; Flekova et al., 2016a), location (Eisenstein et al., 2010), political affiliation (Volkova et al., 2014; Preot¸iuc-Pietro et al., 2017), popularity (Lampos et al., 2014), occupation (Preot¸iuc-Pietro et al., 2015b; Liu et al., 2016), income (Preot¸iuc-Pietro et al., 2015c; Flekova et al., 2016b) and psychological traits such as personality dimensions (Schwartz et al., 2013; Preot¸iuc-Pietro et al., 2016a) or mental 17 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 17–26, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics P"
W17-2903,P17-1068,1,0.880867,"Missing"
W17-2903,D15-1130,0,0.0420274,"in language use across traits are topical. For example, users high in extraversion post more about social activities (‘party’, ‘cant wait’, ‘weekend’), while introverts prefer to post more about computer related activities (‘Internet’, ‘computer’, ‘anime’). Users high in neuroticism post about their negative feelings (‘depressed’, ‘sick of’, ‘lonely’), while users low in neuroticism post more about religion (‘blessings’, ‘praise’) or sports (‘basketball’, ‘soccer’, ‘success’) (Park et al., 2015). However, stylistic rather than topical differences are needed in some applications. For example, (Mirkin et al., 2015) propose that the output text of machine translation systems should reproduce the traits of the author of the source text. In this case, topical information is fixed, and the trait information can be transmitted only using stylistic cues. Following the work of (Preot¸iuc-Pietro et al., 2016b) who studied demographic traits, we Personality plays a decisive role in how people behave in different scenarios, including online social media. Researchers have used such data to study how personality can be predicted from language use. In this paper, we study phrase choice as a particular stylistic ling"
W17-2903,P15-1146,0,0.0216891,"Missing"
W17-2903,P15-2070,0,0.0238452,"Missing"
W17-2903,D14-1121,1,0.846487,"ice in user profiling and use phrase choice to study psycholinguistic hypotheses. This work is relevant to future applications that aim to personalize text generation to specific personality types. 1 Introduction The task of user trait prediction from text has increased in popularity and importance with the availability of user generated content which encodes various information about the author of the text. Using machine learning techniques and large data sets, past research managed to predict with varying degrees of accuracy a series of both demographic traits such as age (Rao et al., 2010; Sap et al., 2014), gender (Burger et al., 2011; Rangel et al., 2015; Flekova et al., 2016a), location (Eisenstein et al., 2010), political affiliation (Volkova et al., 2014; Preot¸iuc-Pietro et al., 2017), popularity (Lampos et al., 2014), occupation (Preot¸iuc-Pietro et al., 2015b; Liu et al., 2016), income (Preot¸iuc-Pietro et al., 2015c; Flekova et al., 2016b) and psychological traits such as personality dimensions (Schwartz et al., 2013; Preot¸iuc-Pietro et al., 2016a) or mental 17 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 17–26, c Vancouver,"
W17-2903,N15-1023,0,0.056667,"Missing"
W17-2903,W11-0310,0,0.0296859,"raits in our data set. A very revealing aspect of paraphrase choice for each trait is the order of preference within a para20 6 Linguistic Hypotheses We use a list of ∼14,000 words rated in all three affective norms introduced in (Warriner et al., 2013). For words rated in both perceived happiness and valence, the correlation is very high (r = .918). We investigate a number of psycholinguistic hypotheses about language choice and style by using our paraphrase based method. We argue that word choice within a paraphrase pair excludes the topical influence that confounds studies using all words (Sarawgi et al., 2011) 6.1 Concreteness Concreteness evaluates the degree to which the concept denoted by a word refers to a perceptible entity (Brysbaert et al., 2014). Although the paraphrase pairs refer to the same entity, some words are perceived as more concrete (or conversely more abstract) than others. The dual-coding theory posits that humans process and represent verbal and non-verbal information in separate, related systems. According to this, both concrete and abstract words are represented in the verbal system, but only concrete words are represented in the non-verbal system. Thus, concrete words are mo"
W17-2903,W15-1203,1,0.871786,"Missing"
W17-2903,P14-1018,0,0.0423862,"Missing"
W17-2903,D13-1187,0,0.0442232,"ctor Model or the ‘Big Five’ being the most widely used model for representing personality. This posits the existence of five traits in which people vary: openness to experience, conscientiousness, extraversion, agreeableness and neuroticism (McCrae and John, 1992). Methods for user trait prediction can uncover sociological insight into user behaviour or implicit biases and also improve a range of applications in recommender systems, targeted marketing or in natural language processing where they can lead to improvements in tasks such as text classification (Hovy, 2015) or sentiment analysis (Volkova et al., 2013). While these methods achieve good predictive performance, they pose significant challenges to the anonymization of identity online. Most differences in language use across traits are topical. For example, users high in extraversion post more about social activities (‘party’, ‘cant wait’, ‘weekend’), while introverts prefer to post more about computer related activities (‘Internet’, ‘computer’, ‘anime’). Users high in neuroticism post about their negative feelings (‘depressed’, ‘sick of’, ‘lonely’), while users low in neuroticism post more about religion (‘blessings’, ‘praise’) or sports (‘bas"
W18-0610,N13-1090,0,0.0120432,"Missing"
W18-0610,P18-2032,1,0.831545,"ssibly because the difference in both the modality on and the time at which these features are built when compared to the essays being analyzed. NRCEmot was primarily developed for identifying emotion-related words on Twitter. The huge difference in the language of Twitter and essays written by the children in this sample would have led to poor generalisation of NRCEmot. The Personality model was also built on another social media platform – Facebook; considering the time period in which the model was built and that in which the essays were written, drift in language (Biber and Finegan, 1989; Jaidka et al., 2018; Wijaya and Yeniterzi, 2011) apart from modality differences would have led to poor generalization of the feature space. At the time of submission, we did not evaluate the performance of unigram features, and subTable 4: Performance (measured by Disattenuated Pearson Correlation, rdisatt and Mean Absolute Error, MAE) of different features at predicting current mental health aspects (Task A). 3.2 Results and Discussion Methods Task A and B We stratified individuals into fivefolds. In this five-fold cross validation setting, we tried linear regression with ridge regularization. We used the impl"
W18-0610,E14-1043,0,0.0608708,"Missing"
W18-0610,D17-1119,0,0.0187515,"t age 11 (Kellam et al., 1977). Bad mental health is associated with both the ‘i’ and ‘informal’ categories at age 11 with pronoun usage and with pronoun usage at older ages. While ‘leisure‘ is protective at all ages, no categories are associated with mental illness at every age. This is consistent with the linguistic manifestation of several mental health conditions (e.g. depression (Schwartz et al., 2014; ?)). For future work, since the Socio Demographic performed best, we could apply methods such as User-Factor Adaptation which focus on the author of the content in addition to the content (Lynn et al., 2017; Zhu et al., 2018). It would also be interesting to investigate if word clusters trained on historical sources (for e.g. Google books) might yield reliable feature representations when studying mental health aspects at different ages to emulate the linguistic associations of elderly, for whom data from other platforms such as social media is be scarce. 5 References Conclusions Yoav Benjamini and Yosef Hochberg. 1995. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the royal statistical society. Series B (Methodological), pages 289–300. T"
W18-0610,D17-1248,1,0.882725,"Missing"
W18-0610,W14-3214,1,0.769535,"Missing"
W18-0610,D17-2010,1,0.872904,"Missing"
W18-1104,R09-1010,0,0.0321819,"the utility of our phrase based data acquisition approach, and the advantage of our models. 6.5 Negative Results with MT In absence of labeled data, MT can be been used for converting labeled data from a source language (often English) into one or more target languages for classification. Although, to the best of our knowledge, there are currently no attempts to exploit MT for emotion detection, there have been successful efforts on the (conceptually relevant) task of sentiment analysis. Examples of sentiment systems employing MT include Hiroshi et al. (2004) (Japanese), Wan (2008) (Chinese), Brooke et al. (2009); Smith et al. (2016) (Spanish), Mihalcea et al. (2007) (Romanian), and Mohammad et al. (2016) (Arabic). Clearly, MT has its limitations. Hence, whether MT will be as useful for emotion as it proved to be for sentiment is in our view an interesting question. As a first attempt to explore answers, we experiment with the MT-DIST data described in Section 3 under two settings: (a) We train exclusively on MT-DIST and test on LAMA-DINA, and (b) We merge MTDIST with the training split of LAMA-DINA to form a single training set that we refer to as MTD2. Again, we use the same settings as described in"
W18-1104,L18-1577,1,0.808669,"d is most similar to Abdul-Mageed et al. (2016), who also use phrase seeds to acquire tweets for Ekman’s 6 basic emotions, but we extend the work to 8 emotions, expand the list of seed expressions used, improve on the manual annotation study, and empirically validate the method on the practical emotion modeling task both on our data and on an external dataset. Our work also has affinity to works on Arabic text classification (Abdul-Mageed et al., 2011; Refaee and Rieser, 2014; Abdul-Mageed et al., 2014; Nabil et al., 2015; Salameh et al., 2015; Abdul-Mageed, 2017, 2018; Alshehri et al., 2018; Abdul-Mageed et al., 2018), but we focus on emotion. emotion that is over double the size of their data (i.e., 7, 268 vs. 2, 984 tweets), (3) we introduce a hybrid supervision method and apply it to develop promising emotion detection models using a powerful deep gated recurrent neural network (GRU), and (4) we explore the utility of MT in the context of emotion detection, hoping our data-driven findings will lead to work enhancing our understanding of emotion. The remainder of the paper is organized as follows: Section 2 is a review of related work. Section 3 is an overview of the different datasets acquired and used"
W18-1104,P11-2103,1,0.876709,"Missing"
W18-1104,P17-1067,1,0.948241,"l suffers from the bottleneck of labeled data. This is true for the Arabic language. With the exception of Abdul-Mageed et al. (2016) who develop data for Ekman’s (Ekman, 1992) 6 basic emotions {anger, disgust, fear, joy, sadness, surprise} and another dataset released very recently as part of SemEval 2018 (Mohammad and Kiritchenko, 2018) that focuses on the 4 emotions 25 Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 25–35 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics Abdul-Mageed and Ungar (2017). For example, Mohammad (2012) collects a corpus of 50, 000 tweets using seed words corresponding to the 6 Ekman emotions and exploits it for building emotion models. More recently, Mohammad and Bravo-Marquez (2017) label a dataset of 7, 097 tweets with emotion intensity tags for the four emotions {anger, fear, joy, sadness} using a method they refer to as best-worst annotation (Kiritchenko and Mohammad, 2016). They describe the method as producing reliable labels. In a similar vein, Wang et al. (2012) collect a large emotion corpus (N= 5 million) for 5 of Ekman’s 6 basic emotions (skipping di"
W18-1104,C04-1071,0,0.071016,"gains on the SE18 external dataset further demonstrate the utility of our phrase based data acquisition approach, and the advantage of our models. 6.5 Negative Results with MT In absence of labeled data, MT can be been used for converting labeled data from a source language (often English) into one or more target languages for classification. Although, to the best of our knowledge, there are currently no attempts to exploit MT for emotion detection, there have been successful efforts on the (conceptually relevant) task of sentiment analysis. Examples of sentiment systems employing MT include Hiroshi et al. (2004) (Japanese), Wan (2008) (Chinese), Brooke et al. (2009); Smith et al. (2016) (Spanish), Mihalcea et al. (2007) (Romanian), and Mohammad et al. (2016) (Arabic). Clearly, MT has its limitations. Hence, whether MT will be as useful for emotion as it proved to be for sentiment is in our view an interesting question. As a first attempt to explore answers, we experiment with the MT-DIST data described in Section 3 under two settings: (a) We train exclusively on MT-DIST and test on LAMA-DINA, and (b) We merge MTDIST with the training split of LAMA-DINA to form a single training set that we refer to a"
W18-1104,D15-1299,0,0.073461,"different in that we use seed expressions, rather than hashtags. Our data collection method is most similar to Abdul-Mageed et al. (2016), who also use phrase seeds to acquire tweets for Ekman’s 6 basic emotions, but we extend the work to 8 emotions, expand the list of seed expressions used, improve on the manual annotation study, and empirically validate the method on the practical emotion modeling task both on our data and on an external dataset. Our work also has affinity to works on Arabic text classification (Abdul-Mageed et al., 2011; Refaee and Rieser, 2014; Abdul-Mageed et al., 2014; Nabil et al., 2015; Salameh et al., 2015; Abdul-Mageed, 2017, 2018; Alshehri et al., 2018; Abdul-Mageed et al., 2018), but we focus on emotion. emotion that is over double the size of their data (i.e., 7, 268 vs. 2, 984 tweets), (3) we introduce a hybrid supervision method and apply it to develop promising emotion detection models using a powerful deep gated recurrent neural network (GRU), and (4) we explore the utility of MT in the context of emotion detection, hoping our data-driven findings will lead to work enhancing our understanding of emotion. The remainder of the paper is organized as follows: Section 2"
W18-1104,N16-1095,0,0.0275146,"n Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 25–35 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics Abdul-Mageed and Ungar (2017). For example, Mohammad (2012) collects a corpus of 50, 000 tweets using seed words corresponding to the 6 Ekman emotions and exploits it for building emotion models. More recently, Mohammad and Bravo-Marquez (2017) label a dataset of 7, 097 tweets with emotion intensity tags for the four emotions {anger, fear, joy, sadness} using a method they refer to as best-worst annotation (Kiritchenko and Mohammad, 2016). They describe the method as producing reliable labels. In a similar vein, Wang et al. (2012) collect a large emotion corpus (N= 5 million) for 5 of Ekman’s 6 basic emotions (skipping disgust), but adding love and thankfulness using a seed set of 131 hashtags representing these emotions. The authors then randomly sample 400 tweets and label them manually with a tag from the set relevant, irrelevant. Abdul-Mageed and Ungar (2017) also collect a large dataset of English tweets using 665 hashtags representing 24 different types of emotions. The authors also perform a manual annotation study show"
W18-1104,P10-3008,0,0.0282429,"these emotions. The authors then randomly sample 400 tweets and label them manually with a tag from the set relevant, irrelevant. Abdul-Mageed and Ungar (2017) also collect a large dataset of English tweets using 665 hashtags representing 24 different types of emotions. The authors also perform a manual annotation study showing the utility of using hashtags as labels. Other work includes Yan and Turtle (2016) who use crowdsourcing and lab-controlled conditions to label a dataset of 15, 553 tweets that they then exploit to build baseline models. Related to our work is also scholarship on mood (Nguyen, 2010; De Choudhury et al., 2012) which also depend on collecting data using seed words. Our work also falls under distant supervision, but is different in that we use seed expressions, rather than hashtags. Our data collection method is most similar to Abdul-Mageed et al. (2016), who also use phrase seeds to acquire tweets for Ekman’s 6 basic emotions, but we extend the work to 8 emotions, expand the list of seed expressions used, improve on the manual annotation study, and empirically validate the method on the practical emotion modeling task both on our data and on an external dataset. Our work"
W18-1104,P04-1035,0,0.0180555,"Missing"
W18-1104,P07-1123,0,0.0627695,"oach, and the advantage of our models. 6.5 Negative Results with MT In absence of labeled data, MT can be been used for converting labeled data from a source language (often English) into one or more target languages for classification. Although, to the best of our knowledge, there are currently no attempts to exploit MT for emotion detection, there have been successful efforts on the (conceptually relevant) task of sentiment analysis. Examples of sentiment systems employing MT include Hiroshi et al. (2004) (Japanese), Wan (2008) (Chinese), Brooke et al. (2009); Smith et al. (2016) (Spanish), Mihalcea et al. (2007) (Romanian), and Mohammad et al. (2016) (Arabic). Clearly, MT has its limitations. Hence, whether MT will be as useful for emotion as it proved to be for sentiment is in our view an interesting question. As a first attempt to explore answers, we experiment with the MT-DIST data described in Section 3 under two settings: (a) We train exclusively on MT-DIST and test on LAMA-DINA, and (b) We merge MTDIST with the training split of LAMA-DINA to form a single training set that we refer to as MTD2. Again, we use the same settings as described in 5 with both the online classifiers and GRUs, and 7 Con"
W18-1104,P09-1113,0,0.0819144,"e annotators received no training, but were given samples of annotated sentences to illustrate Ekman’s 6 types of emotions. Annotators also labeled the data for mixed-emotion and no-emotion. In addition, annotators were required to assign emotion intensity tags from the set {low, medium, high} to all emotion-carrying sentences (thus excluding sentences tagged with no-emotion). Our work differs from these in that we focus on Arabic and the Twitter domain. A number of works use emotion hashtags (e.g., #happy, #sad) as a way of automatically labeling data for emotion (i.e., distant supervision) (Mintz et al., 2009). These include Mohammad (2012); Mohammad and Kiritchenko (2015); Wang et al. (2012); Volkova and Bachrach (2016); 3 Data Building LAMA: We collect a dataset of Arabic tweets from the Twitter public stream ex26 ploiting the Twitter API 1 using a seed set of emotion-carrying expressions following AbdulMageed et al. (2016). More specifically, we use a list of seeds for each of the Plutchik 8 primary emotions from the set: {anger, anticipation, disgust, fear, joy, sadness, surprise, trust}. As such, we add anticipation and trust to the 6 categories Abdul-Mageed et al. (2016) work with. In this ap"
W18-1104,refaee-rieser-2014-arabic,0,0.018628,"ur work also falls under distant supervision, but is different in that we use seed expressions, rather than hashtags. Our data collection method is most similar to Abdul-Mageed et al. (2016), who also use phrase seeds to acquire tweets for Ekman’s 6 basic emotions, but we extend the work to 8 emotions, expand the list of seed expressions used, improve on the manual annotation study, and empirically validate the method on the practical emotion modeling task both on our data and on an external dataset. Our work also has affinity to works on Arabic text classification (Abdul-Mageed et al., 2011; Refaee and Rieser, 2014; Abdul-Mageed et al., 2014; Nabil et al., 2015; Salameh et al., 2015; Abdul-Mageed, 2017, 2018; Alshehri et al., 2018; Abdul-Mageed et al., 2018), but we focus on emotion. emotion that is over double the size of their data (i.e., 7, 268 vs. 2, 984 tweets), (3) we introduce a hybrid supervision method and apply it to develop promising emotion detection models using a powerful deep gated recurrent neural network (GRU), and (4) we explore the utility of MT in the context of emotion detection, hoping our data-driven findings will lead to work enhancing our understanding of emotion. The remainder"
W18-1104,S17-2088,0,0.0592273,"Missing"
W18-1104,S18-1001,0,0.40644,"ee of emotion arousal when an emotion exists) as a single task (rather then two stages). We believe a single stage set up can cause annotator cognitive overload and empirically show how a more simplified, two-stage annotation process yields higher annotator inter-rater reliability.We then proceed to show the utility of exploiting data acquired with our method to develop emotion detection models, including supervised, distant supervised, and hybridly-supervised (i.e., a mixture of supervised and distant supervised). We also validate our method of data acquisition on an external dataset (i.e., (Mohammad and Kiritchenko, 2018)), further proving its usefulness in capturing emotion signal. Finally, training on machine translation (MT) data, we acquire initial results that may be suggesting emotion does not translate (i.e., it may not be possible to successfully build emotion detection systems using MT). Overall, we offer the following contributions: (1) We extend a first-person seed phrase approach introduced by (Abdul-Mageed et al., 2016) for emotion data collection from 6 to 8 emotion categories, and improve on the annotation procedure, acquiring higher agreement between the judges, (2) we introduce a new dataset f"
W18-1104,N15-1078,0,0.0619384,"e use seed expressions, rather than hashtags. Our data collection method is most similar to Abdul-Mageed et al. (2016), who also use phrase seeds to acquire tweets for Ekman’s 6 basic emotions, but we extend the work to 8 emotions, expand the list of seed expressions used, improve on the manual annotation study, and empirically validate the method on the practical emotion modeling task both on our data and on an external dataset. Our work also has affinity to works on Arabic text classification (Abdul-Mageed et al., 2011; Refaee and Rieser, 2014; Abdul-Mageed et al., 2014; Nabil et al., 2015; Salameh et al., 2015; Abdul-Mageed, 2017, 2018; Alshehri et al., 2018; Abdul-Mageed et al., 2018), but we focus on emotion. emotion that is over double the size of their data (i.e., 7, 268 vs. 2, 984 tweets), (3) we introduce a hybrid supervision method and apply it to develop promising emotion detection models using a powerful deep gated recurrent neural network (GRU), and (4) we explore the utility of MT in the context of emotion detection, hoping our data-driven findings will lead to work enhancing our understanding of emotion. The remainder of the paper is organized as follows: Section 2 is a review of relate"
W18-1104,S12-1033,0,0.340576,"a. This is true for the Arabic language. With the exception of Abdul-Mageed et al. (2016) who develop data for Ekman’s (Ekman, 1992) 6 basic emotions {anger, disgust, fear, joy, sadness, surprise} and another dataset released very recently as part of SemEval 2018 (Mohammad and Kiritchenko, 2018) that focuses on the 4 emotions 25 Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 25–35 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics Abdul-Mageed and Ungar (2017). For example, Mohammad (2012) collects a corpus of 50, 000 tweets using seed words corresponding to the 6 Ekman emotions and exploits it for building emotion models. More recently, Mohammad and Bravo-Marquez (2017) label a dataset of 7, 097 tweets with emotion intensity tags for the four emotions {anger, fear, joy, sadness} using a method they refer to as best-worst annotation (Kiritchenko and Mohammad, 2016). They describe the method as producing reliable labels. In a similar vein, Wang et al. (2012) collect a large emotion corpus (N= 5 million) for 5 of Ekman’s 6 basic emotions (skipping disgust), but adding love and th"
W18-1104,D16-1217,1,0.763424,"ase based data acquisition approach, and the advantage of our models. 6.5 Negative Results with MT In absence of labeled data, MT can be been used for converting labeled data from a source language (often English) into one or more target languages for classification. Although, to the best of our knowledge, there are currently no attempts to exploit MT for emotion detection, there have been successful efforts on the (conceptually relevant) task of sentiment analysis. Examples of sentiment systems employing MT include Hiroshi et al. (2004) (Japanese), Wan (2008) (Chinese), Brooke et al. (2009); Smith et al. (2016) (Spanish), Mihalcea et al. (2007) (Romanian), and Mohammad et al. (2016) (Arabic). Clearly, MT has its limitations. Hence, whether MT will be as useful for emotion as it proved to be for sentiment is in our view an interesting question. As a first attempt to explore answers, we experiment with the MT-DIST data described in Section 3 under two settings: (a) We train exclusively on MT-DIST and test on LAMA-DINA, and (b) We merge MTDIST with the training split of LAMA-DINA to form a single training set that we refer to as MTD2. Again, we use the same settings as described in 5 with both the onli"
W18-1104,S17-1007,0,0.0384645,"fear, joy, sadness, surprise} and another dataset released very recently as part of SemEval 2018 (Mohammad and Kiritchenko, 2018) that focuses on the 4 emotions 25 Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 25–35 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics Abdul-Mageed and Ungar (2017). For example, Mohammad (2012) collects a corpus of 50, 000 tweets using seed words corresponding to the 6 Ekman emotions and exploits it for building emotion models. More recently, Mohammad and Bravo-Marquez (2017) label a dataset of 7, 097 tweets with emotion intensity tags for the four emotions {anger, fear, joy, sadness} using a method they refer to as best-worst annotation (Kiritchenko and Mohammad, 2016). They describe the method as producing reliable labels. In a similar vein, Wang et al. (2012) collect a large emotion corpus (N= 5 million) for 5 of Ekman’s 6 basic emotions (skipping disgust), but adding love and thankfulness using a seed set of 131 hashtags representing these emotions. The authors then randomly sample 400 tweets and label them manually with a tag from the set relevant, irrelevant"
W18-1104,S07-1013,0,0.173257,"as follows: Section 2 is a review of related work. Section 3 is an overview of the different datasets acquired and used in our work. Section 4 is a description of both the first-person seed phrase approach to data acquisition and the annotation study we performed. Section 5 is about our methods, and 6 is where we introduce our models and describe negative experiments with MT. We conclude in Section 7. 2 Related Work There is a small, but growing, body of NLP literature on emotion. A number of papers have focused on creating datasets for emotion detection. The SemEval 2007 Affective Text task (Strapparava and Mihalcea, 2007) focused on emotion annotation and classification where a dataset of 1, 250 news headlines was human labeled with the 6 basic emotions of Ekman (Ekman, 1972) and provided to participants. Similarly, Aman and Szpakowicz (2007) describe an emotion annotation and classification task on blog post data of 4, 090 sentences. The data were collected with identified emotion seeds words. Aman and Szpakowicz (2007) point out that the annotators received no training, but were given samples of annotated sentences to illustrate Ekman’s 6 types of emotions. Annotators also labeled the data for mixed-emotion"
W18-1104,P16-1148,0,0.164395,"types of emotions. Annotators also labeled the data for mixed-emotion and no-emotion. In addition, annotators were required to assign emotion intensity tags from the set {low, medium, high} to all emotion-carrying sentences (thus excluding sentences tagged with no-emotion). Our work differs from these in that we focus on Arabic and the Twitter domain. A number of works use emotion hashtags (e.g., #happy, #sad) as a way of automatically labeling data for emotion (i.e., distant supervision) (Mintz et al., 2009). These include Mohammad (2012); Mohammad and Kiritchenko (2015); Wang et al. (2012); Volkova and Bachrach (2016); 3 Data Building LAMA: We collect a dataset of Arabic tweets from the Twitter public stream ex26 ploiting the Twitter API 1 using a seed set of emotion-carrying expressions following AbdulMageed et al. (2016). More specifically, we use a list of seeds for each of the Plutchik 8 primary emotions from the set: {anger, anticipation, disgust, fear, joy, sadness, surprise, trust}. As such, we add anticipation and trust to the 6 categories Abdul-Mageed et al. (2016) work with. In this approach, we collect all tweets where a seed phrase appears in the tweet body text. Note this approach is only cond"
W18-1104,D08-1058,0,0.0421448,"t further demonstrate the utility of our phrase based data acquisition approach, and the advantage of our models. 6.5 Negative Results with MT In absence of labeled data, MT can be been used for converting labeled data from a source language (often English) into one or more target languages for classification. Although, to the best of our knowledge, there are currently no attempts to exploit MT for emotion detection, there have been successful efforts on the (conceptually relevant) task of sentiment analysis. Examples of sentiment systems employing MT include Hiroshi et al. (2004) (Japanese), Wan (2008) (Chinese), Brooke et al. (2009); Smith et al. (2016) (Spanish), Mihalcea et al. (2007) (Romanian), and Mohammad et al. (2016) (Arabic). Clearly, MT has its limitations. Hence, whether MT will be as useful for emotion as it proved to be for sentiment is in our view an interesting question. As a first attempt to explore answers, we experiment with the MT-DIST data described in Section 3 under two settings: (a) We train exclusively on MT-DIST and test on LAMA-DINA, and (b) We merge MTDIST with the training split of LAMA-DINA to form a single training set that we refer to as MTD2. Again, we use t"
W18-1104,J04-3002,0,0.195303,"Missing"
W18-1104,N16-2011,0,0.101397,"ilar vein, Wang et al. (2012) collect a large emotion corpus (N= 5 million) for 5 of Ekman’s 6 basic emotions (skipping disgust), but adding love and thankfulness using a seed set of 131 hashtags representing these emotions. The authors then randomly sample 400 tweets and label them manually with a tag from the set relevant, irrelevant. Abdul-Mageed and Ungar (2017) also collect a large dataset of English tweets using 665 hashtags representing 24 different types of emotions. The authors also perform a manual annotation study showing the utility of using hashtags as labels. Other work includes Yan and Turtle (2016) who use crowdsourcing and lab-controlled conditions to label a dataset of 15, 553 tweets that they then exploit to build baseline models. Related to our work is also scholarship on mood (Nguyen, 2010; De Choudhury et al., 2012) which also depend on collecting data using seed words. Our work also falls under distant supervision, but is different in that we use seed expressions, rather than hashtags. Our data collection method is most similar to Abdul-Mageed et al. (2016), who also use phrase seeds to acquire tweets for Ekman’s 6 basic emotions, but we extend the work to 8 emotions, expand the"
W18-1104,Q17-1021,0,0.0494919,"Missing"
W18-6709,D16-1230,0,0.0666099,"Missing"
W18-6709,D17-2014,0,0.0159722,"king for absolute assessments of quality yields less discriminative results than soliciting direct comparisons of quality. In the dataset introduced for the ConvAI2 competition, nearly all the proposed algorithms were evaluated to be within one standard deviation of each other (Zhang et al., 2018). Therefore, for our human evaluation task, we ask humans to directly compare the responses of two models given the previous utterances in the conversation. Both Facebook and Amazon have developed evaluation systems that allow humans to converse with (and then rate) a chatbot (Venkatesh et al., 2018; Miller et al., 2017). Facebook’s ParlAI 4 is the most comparable system for a unified framework for sharing, training, and evaluating chatbots; however, ChatEval is different in that it entirely focuses on the evaluation and warehousing of models. Our infrastructure relies only on output text files, and does not require any code base integration . Response Comparison To facilitate qualitative comparison of models, we offer a response comparison interface where users can see all the prompts in a particular evaluation set, and the responses generated by each model. Evaluation Toolkit The ChatEval evaluation toolkit"
W19-3806,D16-1235,0,0.02315,"Missing"
W19-3806,J15-4004,0,0.125159,"Missing"
W19-3806,P15-1073,0,0.0318573,"en show WEAT in matrix notation. Related Work NLP has begun tackling the problems that inhibit the achievement of fair and ethical AI (Hovy and Spruit, 2016; Friedler et al., 2016), in part by developing techniques for mitigating demographic biases in models. In brief, a demographic bias is a difference in model output based on gender (either of the data author or of the content itself) or selected demographic dimension (“protected class”) such as race. Demographic biases manifest in many ways, ranging from disparities in tagging and classification accuracy depending on author age and gender (Hovy, 2015; Dixon et al., 2018), to over-amplification of demographic differences in language generation (Yatskar et al., 2016; Zhao et al., 2017), to diverging implicit associations between words or concepts within embeddings or language models (Bolukbasi et al., 2016; Rudinger et al., 2018). Here, we are concerned with the societal bias towards protected classes that manifests in prejudice and stereotypes (Bhatia, 2017). Greenwald and Banaji (1995); implicit attitudes such that “introspectively unidentified (or inaccurately identified) traces of past experience that mediate favorable or unfavorable fe"
W19-3806,P16-2096,0,0.0312528,"oved performance on coreference resolution. Our work is complementary, as debiasing conceptors can be used in place of hard-debiasing. Bolukbasi et al. (2016) also examine a soft debiasing method, but find that it does not perform well. In contrast, our debiasing conceptor does a successful soft damping of the relevant principal components. To understand why, we first introduce the conceptor method for capturing the “bias subspaces”, next formalize bias, and then show WEAT in matrix notation. Related Work NLP has begun tackling the problems that inhibit the achievement of fair and ethical AI (Hovy and Spruit, 2016; Friedler et al., 2016), in part by developing techniques for mitigating demographic biases in models. In brief, a demographic bias is a difference in model output based on gender (either of the data author or of the content itself) or selected demographic dimension (“protected class”) such as race. Demographic biases manifest in many ways, ranging from disparities in tagging and classification accuracy depending on author age and gender (Hovy, 2015; Dixon et al., 2018), to over-amplification of demographic differences in language generation (Yatskar et al., 2016; Zhao et al., 2017), to diver"
W19-3806,N19-1064,0,0.202112,"w that conceptor debiasing diminishes racial and gender bias of word representations as measured using the Word Embedding Association Test (WEAT) of Caliskan et al. (2017). 1 (a) The original space Introduction Word embeddings capture distributional similarities and thus inherit demographic stereotypes (Bolukbasi et al., 2016). Such embedding biases tend to track statistical regularities such as the percentage of people with a given occupation (Nikhil Garg and Zou, 2018) but sometimes deviate from them (Bhatia, 2017). Recent work has shown that gender bias exists in contextualized embeddings (Wang et al., 2019; May et al., 2019). Here, we provide a quantitative analysis of bias in traditional and contextual word embeddings and introduce a method of mitigating bias (i.e., debiasing) using the debiasing conceptor, a clean mathematical representation of subspaces that can be operated on and composed by logic-based manipulations (Jaeger, 2014). Specifically, conceptor negation is a soft damping of the principal components of the target subspace (e.g., the subset of words being debiased) (Liu et al., 2019b) (See Figure 1.) Key to our method is how it treats wordassociation lists (sometimes called target"
W19-3806,S18-2005,0,0.0658232,"mbeddings. A conceptor matrix, C, is a regularized identity map (in our case, from the original word embeddings to their biased versions) that minimizes kZ − CZk2F +α−2 kCk2F . (1) where α−2 is a scalar parameter.2 To describe matrix conceptors, we draw heavily on (Jaeger, 2014; He and Jaeger, 2018; Liu et al., 2019b,a). C has a closed form solution: 1 1 ZZ &gt; ( ZZ &gt; + α−2 I)−1 . (2) k k Intuitively, C is a soft projection matrix on the linear subspace where the word embeddings Z have C= 1 2 Previous work has shown that debiasing methods can have different effects on different word embeddings (Kiritchenko and Mohammad, 2018). Note that the conceptor and WEAT literature disagree on notation and we follow WEAT. In conceptor notation, the matrix Z would be denoted as X. 41 the highest variance. Once C has been learned, it can be ‘negated’ by subtracting it from the identity matrix and then applied to any word embeddings to shrink the bias directions. Conceptors can represent laws of Boolean logic, such as NOT ¬, AND ∧ and OR ∨. For two conceptors C and B, we define the following operations: ¬C := I −C, C ∧ B :=(C −1 +B This naturally gives rise to a large set of concepts and scoring functions. 3.1 The Word Embedding"
W19-3806,D17-1323,0,0.0322482,"al AI (Hovy and Spruit, 2016; Friedler et al., 2016), in part by developing techniques for mitigating demographic biases in models. In brief, a demographic bias is a difference in model output based on gender (either of the data author or of the content itself) or selected demographic dimension (“protected class”) such as race. Demographic biases manifest in many ways, ranging from disparities in tagging and classification accuracy depending on author age and gender (Hovy, 2015; Dixon et al., 2018), to over-amplification of demographic differences in language generation (Yatskar et al., 2016; Zhao et al., 2017), to diverging implicit associations between words or concepts within embeddings or language models (Bolukbasi et al., 2016; Rudinger et al., 2018). Here, we are concerned with the societal bias towards protected classes that manifests in prejudice and stereotypes (Bhatia, 2017). Greenwald and Banaji (1995); implicit attitudes such that “introspectively unidentified (or inaccurately identified) traces of past experience that mediate favorable or unfavorable feeling, thought, or action toward social objects.” Bias is often quantified in people using the Implicit Association Test (IAT) (Greenwal"
W19-3806,N19-1331,1,0.908137,"m them (Bhatia, 2017). Recent work has shown that gender bias exists in contextualized embeddings (Wang et al., 2019; May et al., 2019). Here, we provide a quantitative analysis of bias in traditional and contextual word embeddings and introduce a method of mitigating bias (i.e., debiasing) using the debiasing conceptor, a clean mathematical representation of subspaces that can be operated on and composed by logic-based manipulations (Jaeger, 2014). Specifically, conceptor negation is a soft damping of the principal components of the target subspace (e.g., the subset of words being debiased) (Liu et al., 2019b) (See Figure 1.) Key to our method is how it treats wordassociation lists (sometimes called target lists), which define the bias subspace. These lists include pre-chosen words associated with a target (b) After applying the debiasing conceptor Figure 1: BERT word representations of the union of the set of contextualized word representations of relatives, executive, wedding, salary projected on to the first two principal components of the WEAT gender first names, which capture the primary component of gender. Note how the debiasing conceptor collapses relatives and wedding, and executive and"
W19-3806,W13-3512,0,0.0506172,"th visualization and end-task evaluation. It should also be noted that our results are in line with those from May et al. (2019). 6 Retaining Semantic Similarity In order to understand if the debiasing conceptor was harming the semantic content of the word embeddings, we examined conceptor debiased embedding for semantic similarity tasks. As done in Liu et al. (2018) we used the seven standard word similarity test set and report Pearson’s correlation. The word similarity sets are: the RG65 (Rubenstein and Goodenough, 1965), the WordSim-353 (WS) (Finkelstein et al., 2002), the rare-words (RW) (Luong et al., 2013), the MEN dataset (Bruni et al., 2014), the MTurk (Radinsky et al., 2011), the SimLex-999 (SimLex) (Hill et al., 2015), and the SimVerb-3500 (Gerz et al., 2016). Table 8 shows that conceptors help in preserving and at times increasing the semantic information in the embeddings. It should be noted that these tasks can not be applied to contextualized embeddings such as ELMo and BERT. So, we do not report these results. RG65 WS RW MEN MTurk SimLex SimVerb GloVe Orig. CN 76.03 70.92 73.79 75.17 51.01 55.25 80.13 80.10 69.16 71.17 40.76 45.85 28.42 34.51 word2vec Orig. CN 74.94 78.58 69.34 69.34 5"
W19-3806,N18-2002,0,\N,Missing
W19-3808,D14-1162,0,0.0833136,"Missing"
W19-3808,P15-1073,0,0.030283,"hat the linguistic markers are unambiguous NLP has begun tackling the problems that are limiting the achievement of fair and ethical AI (Hovy and Spruit, 2016; Friedler et al., 2016), including techniques for mitigating demographic biases in models. In brief, a demographic bias is taken to mean a difference in model output based on gender (either of the data author or within the content itself) or selected demographic dimension (“protected class”) such as race. Demographic biases manifest in many ways, from disparities in tagging and classification accuracy depending on author age and gender (Hovy, 2015; Dixon et al., 2018), to over-amplification of demographic differences in language generation (Yatskar et al., 2016; Zhao et al., 2017), to diverging implicit associations between words or concepts within embeddings or language models (Bolukbasi et al., 2016; Rudinger 1 Some methods also require a list of unbiased words as well, but we will not address those since conceptor debiasing does not require them. 2 https://github.com/jsedoc/ ConceptorDebias/tree/master/lists 3 https://github.com/uclanlp/corefBias, https://github.com/uclanlp/gn_glove 4 https://www.cs.cmu.edu/Groups/AI/ areas/nlp/corp"
W19-3808,N18-1202,0,0.0355859,"the PCA space is just a single vector pointing in the difference between those two vectors.) Context-sensitive embedding such as ELMo and BERT give an embedding for every token (based on its context), giving large numbers of embedding for each word (such as ”man”), so that principal components can be calculated even for word lists of size two as shown in Figure 1. Contextualized word representations are replacing word vectors in many natural language processing (NLP) tasks such as sentiment analysis, coreference resolution, question answering, textual entailment, and named entity recognition (Peters et al., 2018; Devlin et al., 2018). However, ELMo and BERT have bias similar (Wang et al., 2019; May et al., 2019; Kurita et al., 2019) to the well documented bias in traditional word embedding methods (Bolukbasi et al., 2016; Bhatia, 2017; Caliskan et al., 2017; Nikhil Garg and Zou, 2018; Kiritchenko and Mohammad, 2018; Rudinger et al., 2018; Zhang et al., 2018), and this could cause bias in NLP pipelines used for high stakes downstream tasks such as resume selection or bail setting algorithms (Hansen et al., 2015; Bolukbasi et al., 2016; Ayres, 2002). Traditional word embeddings, such as word2vec (Mikol"
W19-3808,P16-2096,0,0.0485282,"d Work Word Lists and Principal Components Recall most debiasing methods rely on principal components of the matrix of embeddings of the target words. Hard debiasing methods remove the first or first several principal components (Bolukbasi et al., 2016; Mu and Viswanath, 2018). Conceptors, as explained below, do soft debiasing in the same principal component space. Paired nouns and pronouns should provide better support for debiasing than names if we assume that the linguistic markers are unambiguous NLP has begun tackling the problems that are limiting the achievement of fair and ethical AI (Hovy and Spruit, 2016; Friedler et al., 2016), including techniques for mitigating demographic biases in models. In brief, a demographic bias is taken to mean a difference in model output based on gender (either of the data author or within the content itself) or selected demographic dimension (“protected class”) such as race. Demographic biases manifest in many ways, from disparities in tagging and classification accuracy depending on author age and gender (Hovy, 2015; Dixon et al., 2018), to over-amplification of demographic differences in language generation (Yatskar et al., 2016; Zhao et al., 2017), to divergi"
W19-3808,N19-1064,0,0.312774,"ctors.) Context-sensitive embedding such as ELMo and BERT give an embedding for every token (based on its context), giving large numbers of embedding for each word (such as ”man”), so that principal components can be calculated even for word lists of size two as shown in Figure 1. Contextualized word representations are replacing word vectors in many natural language processing (NLP) tasks such as sentiment analysis, coreference resolution, question answering, textual entailment, and named entity recognition (Peters et al., 2018; Devlin et al., 2018). However, ELMo and BERT have bias similar (Wang et al., 2019; May et al., 2019; Kurita et al., 2019) to the well documented bias in traditional word embedding methods (Bolukbasi et al., 2016; Bhatia, 2017; Caliskan et al., 2017; Nikhil Garg and Zou, 2018; Kiritchenko and Mohammad, 2018; Rudinger et al., 2018; Zhang et al., 2018), and this could cause bias in NLP pipelines used for high stakes downstream tasks such as resume selection or bail setting algorithms (Hansen et al., 2015; Bolukbasi et al., 2016; Ayres, 2002). Traditional word embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and Fasttext (Bojanowski et al.,"
W19-3808,S18-2005,0,0.0180433,"be calculated even for word lists of size two as shown in Figure 1. Contextualized word representations are replacing word vectors in many natural language processing (NLP) tasks such as sentiment analysis, coreference resolution, question answering, textual entailment, and named entity recognition (Peters et al., 2018; Devlin et al., 2018). However, ELMo and BERT have bias similar (Wang et al., 2019; May et al., 2019; Kurita et al., 2019) to the well documented bias in traditional word embedding methods (Bolukbasi et al., 2016; Bhatia, 2017; Caliskan et al., 2017; Nikhil Garg and Zou, 2018; Kiritchenko and Mohammad, 2018; Rudinger et al., 2018; Zhang et al., 2018), and this could cause bias in NLP pipelines used for high stakes downstream tasks such as resume selection or bail setting algorithms (Hansen et al., 2015; Bolukbasi et al., 2016; Ayres, 2002). Traditional word embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and Fasttext (Bojanowski et al., 2017) require large sets of target words, since debiasing is generally done in the space of the PCA of the word embeddings. (If one only uses a two words, like Use of contextualized word embedding allows better debiasing by a"
W19-3808,D17-1323,0,0.0763425,"ical AI (Hovy and Spruit, 2016; Friedler et al., 2016), including techniques for mitigating demographic biases in models. In brief, a demographic bias is taken to mean a difference in model output based on gender (either of the data author or within the content itself) or selected demographic dimension (“protected class”) such as race. Demographic biases manifest in many ways, from disparities in tagging and classification accuracy depending on author age and gender (Hovy, 2015; Dixon et al., 2018), to over-amplification of demographic differences in language generation (Yatskar et al., 2016; Zhao et al., 2017), to diverging implicit associations between words or concepts within embeddings or language models (Bolukbasi et al., 2016; Rudinger 1 Some methods also require a list of unbiased words as well, but we will not address those since conceptor debiasing does not require them. 2 https://github.com/jsedoc/ ConceptorDebias/tree/master/lists 3 https://github.com/uclanlp/corefBias, https://github.com/uclanlp/gn_glove 4 https://www.cs.cmu.edu/Groups/AI/ areas/nlp/corpora/names/ 56 words, say Mary / John to the pair man / woman, as shown in Figure 2. The first principal component is now capturing prono"
W19-3808,N19-1331,1,0.818309,"vector per word token, as we use here for contextsensitive embeddings; for best results, Z should be mean-centered.) A conceptor matrix, C, is a regularized identity map (in our case, from the original word embeddings to their biased versions) that minimizes (b) ELMo PC of John / Mary Figure 4: ELMo word representations of man / woman projected onto the first and second principal components defined by the pair (a) male / female and (b) John / Mary. kZ − CZk2F +α−2 kCk2F . where α−2 is a scalar parameter. As described in the orignal work on matrix conceptors (Jaeger, 2014; He and Jaeger, 2018; Liu et al., 2019b,a) C has a closed form solution: 1 1 C = ZZ &gt; ( ZZ &gt; + α−2 I)−1 . (2) k k Intuitively, C is a soft projection matrix on the linear subspace that gives the largest shrinkage where the word embeddings Z have the highest variance. Once C has been learned, it can be ‘negated’ by subtracting it from the identity matrix and then applied to any word embeddings to shrink their bias directions. Conceptors can represent laws of Boolean logic, such as NOT ¬, AND ∧, and OR ∨. For two conceptors C and B, we define the following operations: alized representations. Figure 4 shows that the ELMO vectors of m"
W19-3808,N19-1063,0,0.166959,"sitive embedding such as ELMo and BERT give an embedding for every token (based on its context), giving large numbers of embedding for each word (such as ”man”), so that principal components can be calculated even for word lists of size two as shown in Figure 1. Contextualized word representations are replacing word vectors in many natural language processing (NLP) tasks such as sentiment analysis, coreference resolution, question answering, textual entailment, and named entity recognition (Peters et al., 2018; Devlin et al., 2018). However, ELMo and BERT have bias similar (Wang et al., 2019; May et al., 2019; Kurita et al., 2019) to the well documented bias in traditional word embedding methods (Bolukbasi et al., 2016; Bhatia, 2017; Caliskan et al., 2017; Nikhil Garg and Zou, 2018; Kiritchenko and Mohammad, 2018; Rudinger et al., 2018; Zhang et al., 2018), and this could cause bias in NLP pipelines used for high stakes downstream tasks such as resume selection or bail setting algorithms (Hansen et al., 2015; Bolukbasi et al., 2016; Ayres, 2002). Traditional word embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and Fasttext (Bojanowski et al., 2017) require lar"
