2007.mtsummit-papers.5,2005.mtsummit-osmtw.4,0,0.0172384,"compare the output quality of a direct MT process with that of a pivot MT process. MT between closely related languages has been very successful, achieving near-publishable quality (which needs very little or no post-editing) for a number of historically and structurally-related languages, such as Czech and Slovak (Hajic et al., 2000b), Catalan and Spanish (Alonso, 2005), Ukrainian and Russian (Gryaznukhina, 2004). Such engines explore similarities between the related languages (Dvorak et al., 2006) and typically rely on shallow processing techniques and knowledge-light linguistic resources (Armentano-Oller et al., 2005). High quality makes such MT systems useful in the pivot-based MT framework, which we take here to mean that the text is translated in several stages via one or more intermediate natural languages, or pivots. Overall translation quality crucially depends on the quality of the weakest link in the pipeline, which is usually the stage between more distant languages. From an engineering perspective, therefore, it is beneficial to use the best available MT system for that stage, even if there is no access to its source code. The only existing reference to an approach involving pivot-based translati"
2007.mtsummit-papers.5,C04-1016,1,0.899644,"terview and an article by a British diplomat). Table 1 presents the characteristics of the corpus. Language Texts Paras Sentences Ukrainian 35 1449 4675 Russian 35 1449 4528 English 35 1449 3513 Table 1: Parameters of MT evaluation corpus Words 64575 65181 68445 The size of our corpus is almost twice that of the DARPA 94 MT evaluation corpus of 36k words (White et al., 1994), which has been widely used for such tasks and has been shown to be sufficient for automated MT evaluation methods (e.g., BLEU) to ensure high correlation with human evaluation scores for translation adequacy and fluency (Babych et al., 2004). The corpus was aligned on the paragraph level and MT-translated into English using commercial MT systems available for Ukrainian, Russian and English. Table 2 gives the characterisitics of the MT systems used for the experiment. MT Pragma Plaj-Ruta Version / Dev 2.0 (2002) Trident Soft ProMT XP 5.0 (2003) ProLingLtd. 3.0 (2002) ProMT Systran 5.0 (2004) Systran S.A. Source L Ukrainian Russian Ukrainian Target L English Russian English Russian Russian English German French Russian German French English Table 2: MT systems The quality of MT was measured using the standard BLEU metric (Papineni"
2007.mtsummit-papers.5,P04-1079,1,0.835977,"erisitics of the MT systems used for the experiment. MT Pragma Plaj-Ruta Version / Dev 2.0 (2002) Trident Soft ProMT XP 5.0 (2003) ProLingLtd. 3.0 (2002) ProMT Systran 5.0 (2004) Systran S.A. Source L Ukrainian Russian Ukrainian Target L English Russian English Russian Russian English German French Russian German French English Table 2: MT systems The quality of MT was measured using the standard BLEU metric (Papineni et al., 2002), as well as the less commonly used WNM (Weighted N-gram Model), which on large corpus has been shown to produce a better correlation with human adequacy judgments (Babych and Hartley, 2004). BLEU and WNM are in some sense complementary, measuring different quality parameters: WNM assigns salience scores (similar to tf.idf) to Ngrams, which rewards matches of those content words that are most important for the general text structure. So its correlation with adequacy can be expected to be higher. BLEU, however, is a better predictor for fluency, since it does not disregard matching sequences of function words. BLEU was computed with one reference and N-gram size 5 (BLEUr1n5). The automated scores were computed for direct translation from Russian and Ukrainian into English, then fo"
2007.mtsummit-papers.5,2005.mtsummit-posters.13,1,0.882248,"both automated scores the best direct translation quality for English–Russian direction is achieved by ProMT (which is not surprising for a mainstream translation direction developed by a wellresourced Russian team working for many years). Thirdly, BLEU scores for closely related translation (ua&gt;ru) are much higher than for distant translation (ua&gt;en and ru&gt;en). Even though BLEU scores for translation into different languages (English vs. Russian) are not directly comparable – the difference in scores does not necessarily correspond to a difference in human judgment about translation quality (Babych et al., 2005) – there is still no doubt that for MT between closely related languages the number of N-gram matches between MT output and human reference is much higher, especially for longer N-grams. Interestingly, the distribution of BLEU scores for Ngrams of different length is different for MT between closely related languages and MT for distant languages. Chart 3 illustrates these distributions for N-grams N=1 to N=5. The most surprising fact is not the even greater Ngram precision for closely related translation, but the different rates of decline in precision for longer N-grams: the decline is close"
2007.mtsummit-papers.5,feldman-etal-2006-cross,0,0.0385578,"Missing"
2007.mtsummit-papers.5,A00-1002,0,0.315821,"anecdotal experience, but to our knowledge there has been no published evaluation of the actual drop in quality. The method proposed in this paper is novel in two respects. First, our pivot is closely related to the source language. Second, we use a parallel corpus to evaluate and compare the output quality of a direct MT process with that of a pivot MT process. MT between closely related languages has been very successful, achieving near-publishable quality (which needs very little or no post-editing) for a number of historically and structurally-related languages, such as Czech and Slovak (Hajic et al., 2000b), Catalan and Spanish (Alonso, 2005), Ukrainian and Russian (Gryaznukhina, 2004). Such engines explore similarities between the related languages (Dvorak et al., 2006) and typically rely on shallow processing techniques and knowledge-light linguistic resources (Armentano-Oller et al., 2005). High quality makes such MT systems useful in the pivot-based MT framework, which we take here to mean that the text is translated in several stages via one or more intermediate natural languages, or pivots. Overall translation quality crucially depends on the quality of the weakest link in the pipeline,"
2007.mtsummit-papers.5,2005.mtsummit-papers.11,0,0.0357083,"for better-resourced languages. This bottleneck can be opened by using Statistical Machine Translation (Och and Ney, 2003), (Marcu and Wong, 2002), which can be trained on parallel corpora for any language pair. However, development of a good quality SMT system requires the use of large collections of parallel texts aligned at the sentence level, amounting to at least several million words. At the same time, parallel corpora of this size tend to be very rare, especially for under-resourced languages. Even for well-resourced languages such resources also tend to be specialised, e.g. Europarl (Koehn, 2005), which covers the language of debates in the European Parliament, so their performance degrades significantly when the system is applied to a slightly different domain, e.g. news (Babych et al., 2007). In this paper we investigate the performance of translation from an under-resourced language into English via a closely-related, or cognate, pivot language with well-developed translation resources. Typically any language can be used as the pivot if it covers the bridge for a language pair that is not available in a given MT system. For instance, if no system translating from French to Japanese"
2007.mtsummit-papers.5,J03-1002,0,0.00373206,"eater than for others. There are commercial systems for translation into English from well-resourced languages, such as French or Russian, that can achieve acceptable quality for many practical applications of machine translation. At the same time there are many more languages for which good quality translation resources are not available. For some of those languages MT systems have occasionally been developed, but their lexical and syntactic coverage is very far from what has been achieved for better-resourced languages. This bottleneck can be opened by using Statistical Machine Translation (Och and Ney, 2003), (Marcu and Wong, 2002), which can be trained on parallel corpora for any language pair. However, development of a good quality SMT system requires the use of large collections of parallel texts aligned at the sentence level, amounting to at least several million words. At the same time, parallel corpora of this size tend to be very rare, especially for under-resourced languages. Even for well-resourced languages such resources also tend to be specialised, e.g. Europarl (Koehn, 2005), which covers the language of debates in the European Parliament, so their performance degrades significantly"
2007.mtsummit-papers.5,P02-1040,0,0.0820725,"l., 2004). The corpus was aligned on the paragraph level and MT-translated into English using commercial MT systems available for Ukrainian, Russian and English. Table 2 gives the characterisitics of the MT systems used for the experiment. MT Pragma Plaj-Ruta Version / Dev 2.0 (2002) Trident Soft ProMT XP 5.0 (2003) ProLingLtd. 3.0 (2002) ProMT Systran 5.0 (2004) Systran S.A. Source L Ukrainian Russian Ukrainian Target L English Russian English Russian Russian English German French Russian German French English Table 2: MT systems The quality of MT was measured using the standard BLEU metric (Papineni et al., 2002), as well as the less commonly used WNM (Weighted N-gram Model), which on large corpus has been shown to produce a better correlation with human adequacy judgments (Babych and Hartley, 2004). BLEU and WNM are in some sense complementary, measuring different quality parameters: WNM assigns salience scores (similar to tf.idf) to Ngrams, which rewards matches of those content words that are most important for the general text structure. So its correlation with adequacy can be expected to be higher. BLEU, however, is a better predictor for fluency, since it does not disregard matching sequences of"
2007.mtsummit-papers.5,P99-1067,0,0.0607218,"stem for translation into the related pivot. Existing research with Czech and Slovak (Hajic et al., 2000a) shows that simple transfer systems operating at the word level can produce reasonable results for closely related languages. The induction of transfer rules for closely related languages can be achieved using comparable corpora: bootstrapping from a small initial bilingual lexicon or the set of orthographic cognates, the system can identify words of the two languages that occur in contexts with a large number of words that are known mutual translations from the seed lexicon. As shown in (Rapp, 1999) this automatic procedure can produce a reliable bilingual lexicon without resorting to parallel corpora. This procedure relies on the availability of morphological resources and sufficiently large comparable corpora (of the size of 20-100 million words). The feasibility of semi-automatic acquisition of such corpora has already been demonstrated (Sharoff, 2006). Experiments with creating taggers and lemmatisers (Feldman et al., 2006) also show that it is possible to bootstrap a sufficiently accurate tagger on the basis of existing resources for cognate languages. This opens the possibility to"
2007.mtsummit-papers.5,1994.amta-1.25,0,0.213082,"Missing"
2007.tc-1.3,P98-1117,0,0.0129615,"semantic field categories. Often a lexical item is mapped to multiple semantic categories, reflecting its potential multiple senses. In such cases, the tags are arranged by the order of likelihood of meanings, with the most prominent first. 3 Objective evaluation In the objective evaluation we tested the performance of our system on a selection of indirect translation problems, extracted from a parallel corpus consisting mostly of articles from English and Russian newspapers (118,497 words in the R-E direction, 589,055 words in the E-R direction). It was aligned at the sentence level by JAPA (Langlais et al., 1998), and further at the word level by GIZA++ (Och and Ney, 2003). 3.1 Comparative performance The intuition behind the objective evaluation experiment is that the capacity of ASSIST to find indirect translation equivalents in comparable corpora can be compared with the results of automatic alignment of parallel texts used in translation models in SMT: one of the major 6/10 advantages of the SMT paradigm is its ability to reuse indirect equivalents found in parallel corpora (equivalents that may never come up in hand-crafted dictionaries). Thus, automatically generated GIZA++ dictionaries with wor"
2007.tc-1.3,J03-1002,0,0.00770091,"tch, but also because of the paucity of suitable aligned (parallel) corpora. The approach adopted here includes the use of comparable corpora in source and target languages, i.e. corpora of texts dealing with similar subject matter and intended for similar readerships. These are relatively easy to create, by ‘harvesting’ them from the internet, for example, and there are no alignment costs. The greatest challenge is to generate a list of solutions that translators will find usable and to rank them such that the best are at the top. While ASSIST is unlike statistical machine translation (SMT – Och and Ney, 2003), where lexical selection is effected by a translation model based on aligned, parallel corpora, the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from 2/10 now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terms (Grefenstette, 2002) which exhibit a one-to-one correspondence irrespective of the context. ASSIST addresses difficulties with expressions from the general lexicon, whose translation is context-dependent. 2 Methodology The software acts as a decision support system for nologies for extra"
2007.tc-1.3,2001.mtsummit-papers.68,0,0.0213128,"actory condition, bad state of repair, badly in need of repair, and so on. The objective evaluation shows that the system has been able to find the suggestion used by a particular translator for the problem studied. It does not tell us whether the system has found some other translations suitable for the context. Such legitimate translation variation implies that the performance of a system should be studied on the basis of multiple reference translations. For the purposes of evaluating a fully automatic MT tool, the typical practice of using just two reference translations may be sufficient (Papineni, et al, 2001). However, in the context of a translator’s amanuensis which deals with expressions difficult for human translators, it is reasonable to work with a larger range of acceptable target expressions. With this in mind we evaluated the performance of the tool with a panel of 12 professional translators, members of ITI and the Chartered Institute of Linguists. Test materials were provided in which problematic expressions were highlighted and the translators were asked to find suitable suggestions produced by the tool for these expressions and rank their usability on a scale from 1 to 5 (‘not accepta"
2007.tc-1.3,P99-1067,0,0.0508505,"create, by ‘harvesting’ them from the internet, for example, and there are no alignment costs. The greatest challenge is to generate a list of solutions that translators will find usable and to rank them such that the best are at the top. While ASSIST is unlike statistical machine translation (SMT – Och and Ney, 2003), where lexical selection is effected by a translation model based on aligned, parallel corpora, the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from 2/10 now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terms (Grefenstette, 2002) which exhibit a one-to-one correspondence irrespective of the context. ASSIST addresses difficulties with expressions from the general lexicon, whose translation is context-dependent. 2 Methodology The software acts as a decision support system for nologies for extracting indirect translation equivalents following subsections we give the user perspective on ogy underlying each of its sub-tasks. Explanations of vided by (Babych et al., 2007). 2.1 translators. It integrates different techfrom large comparable corpora. In the the system and describe the m"
2007.tc-1.3,rapp-2004-freely,0,0.0655345,"Missing"
2007.tc-1.3,P06-2095,1,0.807661,"the following: Children attend schools that are in poor repair and lacking basic essentials Thus ASSIST supports translators in making decisions about indirect translation equivalents in a number of ways: it suggests possible structural and lexical transformations for contextual descriptors; it verifies which translation variants co-occur in the TL corpus; and it illustrates the use of the transformed TL lexical descriptors in actual contexts. 2.2 Generating and ranking translation equivalents The method for generating translation equivalents is a generalisation of one used in previous work (Sharoff et al., 2006) on extracting equivalents for continuous multiword expressions (MWEs). Essentially, the method expands the search queries for each word and its dictionary translations with entries from thesauri automatically computed from the corpora. It then checks which combinations are possible in the TL corpus or corpora. These potential translation equivalents are now ranked by their distributional similarity to the original query and presented to the user. In this way, the range of retrievable equivalents has been extended from a relatively limited range of two-word constructions which mirror POS categ"
2009.eamt-1.6,2007.mtsummit-papers.5,1,0.845111,"Missing"
2012.tc-1.14,W06-2810,0,0.070523,"Missing"
2012.tc-1.14,baroni-bernardini-2004-bootcat,0,0.298214,"ot large, but the resources become even more sparse when we attempt to extract a multilingual corpus in this way, e.g., there are just 13 pages in the renewable energy domain shared between English, Chinese and Russian. At the same time, the relatively small Wiki corpus from Table 1 provides the possibility of testing our methods for detection of comparable texts in a realistic setup without relying on parallel corpora, which do not exhibit topical and lexical variation possible with real comparable texts within the same domain. The second strategy is a multilingual generalisation of BootCat (Baroni and Bernardini, 2004), in which good translation equivalents are used to find and retrieve webpages returned in response to queries to a search engine. The assumption is that parallel queries consisting of several terms yield similar pages, e.g. 2 wind farm geothermal power hydroelectricity photovoltaics 风力发电厂 地热能 水力发电 太阳能光伏 ... âåòðîýëåêòðîñòàíöèÿ ãåîòåðìàëüíàÿ ýíåðãèÿ ãèäðîýíåðãåòèêà ôîòîýëåêòðè÷åñòâî A set of 42 parallel terms of this kind resulted in a trilingual corpus, also listed in Table 1 (Bing was used as the search engine in this experiment). The corpus is substantially bigger than the selection of arti"
2012.tc-1.14,C10-1073,0,0.0181279,"9.94 3 âåêòîð (vector) rotor 6944 27.29 4 ñêîðîñòü (speed) spin 29185 24.16 2 ìãíîâåííûé (instantaneous) generate 69293 18.47 4 ñèëà (force) rotate 23012 17.67 2 âðàùåíèå (spin) speed 137005 17.25 2 ïëîõîé (difficult) propeller 9107 17.25 2 êîýôôèöèåíò (rate) self-starting 52 pitch 69433 airflow 2059 force 405295 tower 85230 load 32271 conventional 32674 vertical 33781 axis 21728 CorpusF 1889 1699 13 14719 22180 99 3092 5503 35960 1280 111062 6518 7239 7253 2012) or by a more complicated measure, which also takes into account the salience of terms and the possibility of multiple translations (Li and Gaussier, 2010). The drawback of the dictionary-based approaches is that they are reasonably successful only when they have a good dictionary to map the features. Even the coverage of texts by words in this dictionary is not a good approximation, since proper translations of keywords specific to the domain can be quite different from their use in a general-purpose dictionary, e.g., blade, pitch or spin from Table 2. After all, the purpose of our corpus collection procedure is to create a good glossary when it did not exist before. With this in mind, we developed another approach, which is based on the idea o"
2012.tc-1.14,P07-1084,0,0.0316871,"y alignment The technologies for measuring the similarity of texts across languages offer particular benefits in term management. Comparable corpora are also used in Statistical Machine Translation, but their use tends to be limited to detection of parallel or nearly parallel sentences (Adafre and de Rijke, 2006; Munteanu and Marcu, 2006). On the other hand, automatic terminology extraction needs relatively large collections of texts to obtain statistically significant results. However, for alignment purposes the texts also need to be on the same topic, even at the expense of the corpus size (Morin et al., 2007). Fortunately, the process of alignment of two monolingual lists coming from two languages of a comparable corpus can ignore the exact parallelism of their syntactic constructions, and can benefit from a considerable amount of on-topic weakly comparable texts. Below I describe our work in progress on using filtered comparable corpora to align their term lists. The alignment procedure was tested on Wikipedia with alignment information coming from the interwiki links (Rapp et al., 2012). However, it can also use a tentative mapping between the most similar documents identified using the anchor-"
2012.tc-1.14,P06-1011,0,0.0612727,"iki-Ru texts words 165 130797 51 47927 Crawled-En texts words Crawled-Ru texts words Crawled-Zh texts words 5762 5126 3287 7505765 7766462 12431752 measuring their degree of comparability, (3) extracting term lists, (4) aligning the terms between different languages and (5) using the resulting bilingual glossaries in CAT and MT applications. The use of comparable texts for training MT systems usually requires noisy parallel or at least strongly comparable texts, e.g. finding nearly parallel sentences in Wikipedia articles (Adafre and de Rijke, 2006) or in the BBC News in English and Romanian (Munteanu and Marcu, 2006). However, for the task of term extraction we can use a large amount of weakly comparable texts collected by crawling the web. In the paper, we will discuss the ways to collect comparable corpora (Section 2), ways to find similar texts and to filter out less similar ones (Section 3) and ways to use this procedure for term extraction (Section 4). 2 Sources of weakly comparable corpora For collecting comparable corpora, three strategies are possible: 1. targeted crawling of specific resources, which are known to be comparable; 2. collection of responses from search engines using parallel terms;"
2012.tc-1.14,W11-1212,0,0.0140307,"used in TTC by means of Babouk, a specially developed topical crawler for terminology mining (de Groc, 2011). These approaches can be very successful in creating large comparable corpora, but they do not answer the question of how similar the corpora collected in this way are. This the question investigated in our recent work presented here. 3 3.1 Measuring the similarity across languages Feature selection Usually the pages are compared using their textual content as the feature. Some researchers compare texts using the most frequent words (Kilgarriff, 2001), some prefer using hapax legomena (Patry and Langlais, 2011). In other studies we experimented with flexigrams (Forsyth and Sharoff, 2011), i.e., combinations of words with the possibility of having gaps between them. The similarity of texts by their genres can be captured by using their part-of-speech signatures (Sharoff, 2010) or by a mixed feature set, which combines the most frequent words with POS tags for less frequent words, (Baroni and Bernardini, 2006; Sharoff, 2007). However, because of the need to determine the similarity between terminologically rich texts in this study we restricted the feature vector for each text to the keywords extracte"
2012.tc-1.14,rapp-etal-2012-identifying,1,0.903384,"c resources, which are known to be comparable; 2. collection of responses from search engines using parallel terms; 3. focused crawling which starts from a small number of seeds. A variant of the first strategy is the use of Wikipedia in which the articles are linked via the interwiki links. For collecting a topically-diverse comparable corpus, Wikipedia is a good resource providing about 600,000 aligned document pairs for English-German, 350,000 for English-Russian or 140,000 for a lessresourced pair like English-Ukrainian, all figures are for the Wikipedia dumps downloaded in November 2011 (Rapp et al., 2012). However, Wikipedia does not provide suitable resources for specific domains, see data in Table 1 for a comparable English-Russian corpus in the domain of nuclear and renewable energy. It was created by selecting specific categories provided by the Wikipedia users, e.g., ‘Bioenergy’, ‘Tidal power’ or ‘Wind power’, and extracting the interlinked articles classified by the Wikipedia editors under these categories in either of the two languages. This corpus is already not large, but the resources become even more sparse when we attempt to extract a multilingual corpus in this way, e.g., there ar"
2012.tc-1.14,W00-0901,0,0.619723,"ch combines the most frequent words with POS tags for less frequent words, (Baroni and Bernardini, 2006; Sharoff, 2007). However, because of the need to determine the similarity between terminologically rich texts in this study we restricted the feature vector for each text to the keywords extracted using the log-likelihood (LL) score. It is calculated by taking into account the relative ratio of the term usage in a document and in the rest of the corpus, as well as the absolute frequency of its occurrence, at the same time, to estimare its statistical significance as a keyword for this text (Rayson and Garside, 2000). Examples of keywords are given in Table 2, a longer English article (with the length of 1464 words) gives a longer keyword list in comparison to the Russian one (218 words only). 3.2 Anchor-based method One way of comparing corpora across languages is by translating the features obtained from their documents and by measuring the difference in the frequencies of the translations of the keywords for each document. This can be done by computing the cosine similarity score between the feature vectors (Su and Babych, 3 LL 326.11 224.29 208.15 135.47 95.85 79.8 68.83 66.15 61.91 54.37 50.57 47.59"
2012.tc-1.14,su-babych-2012-development,0,0.0201043,"Missing"
2020.bucc-1.2,2020.bucc-1.11,0,0.0293513,"Missing"
2020.bucc-1.2,W15-3411,1,0.812659,"r of 2 billion, Russian about 3 billion running words (Sharoff et al., 2017). The compressed sizes of the Wikipedia corpora are: English: 3.6 GB, Spanish: 0.9 GB, Chinese: 0.4 GB. They are in a one-line per document format. The first tabseparated field in each line contains metadata, the second field contains the text. Paragraph boundaries are marked with HTML tags. As cleaning up the original Wikipedia dump files is not trivial, occasionally there can be some noise in the form of not fully cleaned HTML and Javascript fragments. Details of the cleanup and preparation procedure can be found in Sharoff et al. (2015). Expressions of interest to participate in the shared task Release of shared task training sets Release of shared task test sets Submission of shared task results 3.2 Table 3: Time schedule. 2 ukWaC deWaC esWiki deWiki frWaC deWaC en deWaC ukWaC Table 4: Language pairs supported and corpora (WaCky or Wikipedia) to be used in the closed track. Table 2: Checklist for participants (abbreviated). Any time Corpora Table 4 lists the corpora to be used for the language pairs supported in the closed track. Due to their free availability for several languages and their size, for the shared task we use"
2020.bucc-1.2,Q17-1010,0,0.151641,"Missing"
2020.bucc-1.2,J17-2001,0,0.0752755,"ir competition between systems. This was accomplished by providing corpora and bilingual datasets for a number of language pairs involving Chinese, English, French, German, Russian and Spanish, and by comparing the results using a common evaluation framework. For the shared task we provided corpora as well as training and test data. However, as we anticipated that these corpora and datasets may not suit all needs, we divided the shared task into two tracks: Quite a few research groups have been working on this problem using a wide variety of approaches. There are comprehensive studies such as Irvine & Callison-Burch (2017) and also overview papers at least in part discussing the topic like Jakubina & Langlais (2016), Rapp et al. (2016), Sharoff et al. (2013). 1 Target (French) bébé poupon bain lit plumard commodité médecin docteur aigle montagne nerveux travail  https://comparable.limsi.fr/bucc2020/bucc2020-task.html 6 In the closed track, participants were required to only use the data provided by the organizers. In this way equal conditions were ensured and, as the outcome of 3. this track, the systems could be compared and ranked according to the quality of their results. In the open track, participants wer"
2020.bucc-1.2,2020.bucc-1.9,0,0.0902575,"Missing"
2020.bucc-1.2,W05-0809,0,0.0808028,"llel data. It has been suggested that it may be possible to extract multilingual lexical knowledge from comparable rather than from parallel corpora (see e.g. Sharoff et al., 2013). From a theoretical perspective, this suggestion may lead to advances in understanding human second language acquisition. From a practical perspective, as comparable corpora are available in much larger quantities than parallel corpora, this approach might help in relieving the data acquisition bottleneck which tends to be especially severe when dealing with language pairs involving low resource languages (see e.g. Martin et al., 2005). Table 1: Sample word translations from English to French. In the shared task a similar tab-separated format was used. However, as up to now there was no standard way to measure the performance of the systems, the published results are not comparable and the pros and cons of the various approaches are not clear. A well-established practical task to approach this topic is bilingual lexicon extraction from comparable corpora, which is in the focus of this shared task. Typically, its aim is to extract word translations such as exemplified in Table 1 from comparable corpora, where a given source"
2020.bucc-1.2,W03-0301,0,0.111131,"r ranking the participating systems. We also summarize the approaches used and present the results of the evaluation. In conclusion, the outcome of the competition is the results of a number of systems which provide surprisingly good solutions to an ambitious problem. Keywords: bilingual dictionary, lexicon induction, comparable corpora 1. Introduction Source (English) baby baby bath bed bed convenience doctor doctor eagle mountain nervous work In the framework of machine translation, the extraction of bilingual dictionaries from parallel corpora has been conducted very successfully (see e.g. Mihalcea & Pedersen, 2003). But on the other hand, human second language acquisition appears not to be based on parallel data. This means that there must be a way of acquiring and relating lexical knowledge across two or more languages without the use of parallel data. It has been suggested that it may be possible to extract multilingual lexical knowledge from comparable rather than from parallel corpora (see e.g. Sharoff et al., 2013). From a theoretical perspective, this suggestion may lead to advances in understanding human second language acquisition. From a practical perspective, as comparable corpora are availabl"
2020.lrec-1.229,W18-6401,0,0.0676527,"Missing"
2020.lrec-1.229,P19-4007,0,0.01969,"w that the neural model with attention can outperform conventional feature-based methods as well as a baseline neural model. To our knowledge, we are the first to apply neural networks to reference-free fine-grained HTQE. Our code and dataset of expert-annotated translations with finegrained scores for the English-Chinese direction is available under a permissive licence.7 In the future, we plan expanding this study in two directions. While initial experiments with BERT (Devlin et al., 2018) did not show improvements in the model, we will try truly cross-lingual language models such as XML-R (Conneau et al., 2019), since cross-lingual language models are likely to be more effective in comparison to the current model which uses independent embeddings for each language, while the training set itself is too small to infer links between languages from bilingual data. Next, we will experiment with the integration of other features into attention, such as alignment information from large parallel corpora, to introduce quality vectors similarly to (Kim and Lee, 2016). Even though the neural architecture outperforms feature-based methods, we can try integrating features which manifest translators’ decision-mak"
2020.lrec-1.229,D15-1124,0,0.0288844,"Missing"
2020.lrec-1.229,P15-1078,0,0.0224169,"n between parsed trees, argument roles, phrase alignments, etc. In contrast, Zhou and Bollegala (2019) took an unsupervised approach to approximate and grade human translations into different categories using the bidirectional Word Mover’s Distance (Kusner et al., 2015). There has been recent work using neural models to compare a target translation with reference(s) in MT evaluation. For example, Gupta et al. (2015) use Tree Long Short Term Memory (Tree-LSTM) based networks for reference-based MT evaluation. They propose a method that is competitive to the current complex feature engineering. Guzmán et al. (2015) implemented neural models aiming to select the better translation from a pair of hypotheses, given the reference translation. Neural models for MT Quality Evaluation have been also recently tested either as Neural Language models on a mixture of n-grams (Paetzold and Specia, 2016) or a reference-free MTQE prediction model built on quality vectors obtained from parallel corpora (Kim and Lee, 2016). Often sentence-level MTQE learn to predict translation quality in a indirect manner by ranking translations from best to worst, while learning the direct assessment which matches human evaluators is"
2020.lrec-1.229,W16-2384,0,0.0296928,"pta et al. (2015) use Tree Long Short Term Memory (Tree-LSTM) based networks for reference-based MT evaluation. They propose a method that is competitive to the current complex feature engineering. Guzmán et al. (2015) implemented neural models aiming to select the better translation from a pair of hypotheses, given the reference translation. Neural models for MT Quality Evaluation have been also recently tested either as Neural Language models on a mixture of n-grams (Paetzold and Specia, 2016) or a reference-free MTQE prediction model built on quality vectors obtained from parallel corpora (Kim and Lee, 2016). Often sentence-level MTQE learn to predict translation quality in a indirect manner by ranking translations from best to worst, while learning the direct assessment which matches human evaluators is a challenging task, requiring extensive feature engineering and suffering from data sparsity, particularly for sentence-level predictions. Compared with discrete models with manual quality features, neural network models take low-dimensional dense embeddings as the input, which can be trained from a large-scale dataset, thereby overcoming the issue of sparsity, and capture complex non-local synta"
2020.lrec-1.229,W06-3114,0,0.0809144,"oprediction of fine-grained scores for measuring different aspects of translation quality, such as terminological accuracy or idiomatic writing. Empirical results on a large human annotated dataset show that the neural model outperforms feature-based methods significantly. The dataset and the tools are available. Keywords: human translation quality estimation, sentence-level, attention mechanism, neural networks 1. Introduction Translation quality can be assessed in many different ways (House, 2015), for example, in the context of MT it is typically assessed in terms of adequacy and fluency (Koehn and Monz, 2006). While human evaluation does provide a good estimate of translation quality, it is time consuming, expensive, subjective and not directly applicable to new translations. Automatic translation evaluation can be fast, cheap and consistent. A typical method is to compare the similarity between MT output and references, e.g. BLEU (Papineni et al., 2002). On the other hand, more recent reference-free approaches to MT Quality Estimation (MTQE), see (Bojar et al., 2018; Barrault et al., 2019), use machine learning to predict MT quality from linguistic features from the source sentences and MT output"
2020.lrec-1.229,2012.amta-wptp.2,0,0.0605284,"Missing"
2020.lrec-1.229,D15-1166,0,0.0631176,"UT TS IW TM UT TS IW TM UT TS IW TM UT TS IW TM The Model &gt;=0.01 Figure 2: Attention for a Sentence Pair multitask-learning for different translation quality aspects, such as Usefulness or Terminology. 4.4. Case Studies 4.4.1. Attention Visualization Given the importance of the attention mechanism in our implementation to model HTQE, we visualize a translation pair extracted from the training process, as shown in Figure 2. The attention mechanism in our approach, as manifested by the plotted weights, does not seek monotonic or predictive alignment as it happens in Neural Machine Translation (Luong et al., 2015). The weights for words in the English source sentence and the Chinese target sentence are not necessarily ‘aligned’ unlike in traditional NMT attention models. This relaxation is advantageous to the task, given that first we have much less data in our quality estimation training set in comparison to NMT parallel corpora. More importantly, even though aligned segments are indicative of translation quality, they do not contribute equally to the final quality of a translation segment. For example, given a batch of sentences, with all the essential components such as nouns, verbs, adjectives, adv"
2020.lrec-1.229,W16-2388,0,0.0227623,"e has been recent work using neural models to compare a target translation with reference(s) in MT evaluation. For example, Gupta et al. (2015) use Tree Long Short Term Memory (Tree-LSTM) based networks for reference-based MT evaluation. They propose a method that is competitive to the current complex feature engineering. Guzmán et al. (2015) implemented neural models aiming to select the better translation from a pair of hypotheses, given the reference translation. Neural models for MT Quality Evaluation have been also recently tested either as Neural Language models on a mixture of n-grams (Paetzold and Specia, 2016) or a reference-free MTQE prediction model built on quality vectors obtained from parallel corpora (Kim and Lee, 2016). Often sentence-level MTQE learn to predict translation quality in a indirect manner by ranking translations from best to worst, while learning the direct assessment which matches human evaluators is a challenging task, requiring extensive feature engineering and suffering from data sparsity, particularly for sentence-level predictions. Compared with discrete models with manual quality features, neural network models take low-dimensional dense embeddings as the input, which ca"
2020.lrec-1.229,P02-1040,0,0.106977,"estimation, sentence-level, attention mechanism, neural networks 1. Introduction Translation quality can be assessed in many different ways (House, 2015), for example, in the context of MT it is typically assessed in terms of adequacy and fluency (Koehn and Monz, 2006). While human evaluation does provide a good estimate of translation quality, it is time consuming, expensive, subjective and not directly applicable to new translations. Automatic translation evaluation can be fast, cheap and consistent. A typical method is to compare the similarity between MT output and references, e.g. BLEU (Papineni et al., 2002). On the other hand, more recent reference-free approaches to MT Quality Estimation (MTQE), see (Bojar et al., 2018; Barrault et al., 2019), use machine learning to predict MT quality from linguistic features from the source sentences and MT outputs. The popularity of MTQE is largely driven by the research in MT development and the necessity of evaluating mass output by various types of MT systems. At the same time, automatic human translation estimation (HTQE) has received much less attention, as this is a much more challenging task. However, there is a surging need of automating the evaluati"
2020.lrec-1.229,2010.jec-1.5,0,0.071947,"e-based methods have been used for translation quality estimation, particularly for MT. A number of attempts have been made to use machine learned classifiers and regressors for sentence level MT quality in 1 https://www.atanet.org/certification/aboutpractice_test.php https://www.iti.org.uk/membership/professional-assessment 3 http://www.catti.net.cn/ 2 1858 S1 S2 S3 Scores S4 MSE Loss + Pooling Pooling attentions BiLSTM CNN CNN Word embeddings the series of quality estimation shared tasks, predicting indirect quality indexes, such as post-editing effort (Specia, 2011), post-editing distance (Specia and Farzindar, 2010), post-editing time (Koponen et al., 2012). Automatic quality estimation of human translations is a newly emerging topic. Yuan et al. (2016) developed a feature set to predict adequacy and fluency of human translations at the document level, which includes comparison between parsed trees, argument roles, phrase alignments, etc. In contrast, Zhou and Bollegala (2019) took an unsupervised approach to approximate and grade human translations into different categories using the bidirectional Word Mover’s Distance (Kusner et al., 2015). There has been recent work using neural models to compare a ta"
2020.lrec-1.229,2011.eamt-1.12,0,0.0344166,"y. 2. Related Work Conventional feature-based methods have been used for translation quality estimation, particularly for MT. A number of attempts have been made to use machine learned classifiers and regressors for sentence level MT quality in 1 https://www.atanet.org/certification/aboutpractice_test.php https://www.iti.org.uk/membership/professional-assessment 3 http://www.catti.net.cn/ 2 1858 S1 S2 S3 Scores S4 MSE Loss + Pooling Pooling attentions BiLSTM CNN CNN Word embeddings the series of quality estimation shared tasks, predicting indirect quality indexes, such as post-editing effort (Specia, 2011), post-editing distance (Specia and Farzindar, 2010), post-editing time (Koponen et al., 2012). Automatic quality estimation of human translations is a newly emerging topic. Yuan et al. (2016) developed a feature set to predict adequacy and fluency of human translations at the document level, which includes comparison between parsed trees, argument roles, phrase alignments, etc. In contrast, Zhou and Bollegala (2019) took an unsupervised approach to approximate and grade human translations into different categories using the bidirectional Word Mover’s Distance (Kusner et al., 2015). There has"
2020.lrec-1.229,vilar-etal-2006-error,0,0.115775,"Missing"
2020.lrec-1.229,L16-1581,1,0.942705,"rtification Exam1 , ITI professional assessment2 , CATTI3 require assessment of many submissions. Using automated evaluation can help in reducing the cost of organizing the examination and mitigate the subjectivity of human evaluation in case an automatic evaluation systems can yield reliable judgement of the quality of input translations. The reference-free MTQE approaches, nevertheless, do not necessarily work well on the task of predicting quality of human translations, since human translators tend to differ from MT in the kinds of errors they make. There has been some recent work on HTQE (Yuan et al., 2016) using rich syntactic and semantic features, which are however languageand resource-dependent. To address these shortcomings, we take a different direction and investigate a neural network model for fine-grained HTQE. In particular we propose a customized attention mechanism in order to capture both local and global bilingual quality information. Experiments show that the proposed method outperforms two featurebased methods with 0.22+ higher correlation with human judgement, maintaining stable performance across four aspects of translation quality. 2. Related Work Conventional feature-based me"
2020.lrec-1.229,C16-1329,0,0.0287922,"ge-scale dataset, thereby overcoming the issue of sparsity, and capture complex non-local syntactic and semantic information that discrete indicator features can hardly encode. There has been some research on different ways for integration of LSTMs and CNNs, since the two methods for building the neural networks are somewhat complementary. Roussinov et al. (2020) studied the use of LSTMs (or pretrained transformers) with convolution filters to predict the lexical relations for pairs of concepts, for example, Tom Cruise is an actor or a rat has a tail. Most similar to our work is the study by (Zhou et al., 2016), which also used a stacked architecture with LSTM followed by twodimensional pooling to obtain a fixed-length representation for text classification tasks. Here we contribute by having a novel stacked siamese architecture applied to a different task, namely HTQE. Therefore, our contribution is two-fold: we work on a more challenging task (Guzmán et al., 2017) than learning the relative ranking of translations or estimating the similarity between candidate translations and references to simulate the scores produced by professional evaluators; we propose a stacked neural networks for fine-grain"
2020.lrec-1.229,P13-4014,0,\N,Missing
2020.lrec-1.229,D14-1162,0,\N,Missing
2020.lrec-1.229,W19-5301,0,\N,Missing
2020.lrec-1.298,C00-1027,0,0.200492,"ns more links to discussion forums, opinion columns, political blogs and other argumentative texts. At the same time, ukWac contains more advertising and shopping pages coming from a random snapshot of crawling. Since OWT consists of pages upvoted by several users it is less likely to contain pages aimed at promotion. 6. Related studies Kenneth Church investigated the impact of frequency bursts on the probability of words by splitting a text into two parts (’history’ and ’test’). He demonstrates a much greater probability of seeing a word in the test part once it occurred in the history part (Church, 2000). In the end, if the probability of seeing a topical word in a text once is p(k = 1), then the probability of seeing it twice is p(k = 2) ≈ p/2 rather than p2 as expected in the binomial distribution. Cf. also a more in-depth discussion by Harald Baayen in Chapter 5 of (Baayen, 2001). In his Spanish frequency dictionary Juilland introduced a measure of dispersion of word frequencies, which is essentially based on the standard error of the mean normalised by the mean (Juilland, 1964): σ D =1− √ µ T −1 This proposal was followed by several other measures aimed at identification and mitigation of"
2020.lrec-1.298,L16-1042,0,0.0304865,"s does not help in detecting the expected frequency value. A common practice in frequency dictionaries is to multiply the raw counts by a dispersion measure (by Juilland’s D in the frequency dictionaries), but this applies a uniform correction measure to the overall count, while the frequency bursts are specific to individual texts. Active research in comparing corpus composition using keywords and most frequent words has started since (Kilgarriff, 2001), followed by (Kilgarriff, 2012). It has been shown that the use of topic modelling helps in finding the differences between the Web corpora (Fothergill et al., 2016; Sharoff, 2013). Since the arrival of machine learning methods in the 1990s, genre classification and related approaches to classification of texts with respect to their stylistic features developed from (Karlgren and Cutting, 1994) to (Pritsos and Stamatatos, 2018), see a recent overview in (Argamon, 2019). However, genre classification methods have not been yet applied to very large corpora from the Web. 7. Conclusions and further work The study explored the significant differences in the lexicon and in the composition of OpenWebText, ukWac and Wikipedia, three large Web corpora, which are"
2020.lrec-1.298,P13-2121,0,0.0167503,"quency) measure. By this count wonderful becomes more common than stdclass. However, the application of range-based frequency lists is also limited by the fact that they do not distinguish evidence coming from short and long texts, so that their values can vary radically between otherwise reasonably similar corpora. Language modelling pays attention to smoothing, i.e., estimating the frequency of ’unseen’ n-grams, while the frequency of observed n-grams is measured as it is without using information from the document frequencies, only the sentence frequencies are sometimes taken into account (Heafield et al., 2013). Therefore, LM does not distinguish between the probabilities of new stdclass vs wonderful moment, which have similar raw frequencies (in OWT) and very different burstiness properties. Another problem which concerns all of these measures is that we do not have an estimation of what reliable counts are likely to be: we can detect the lack of a well-behaving distribution across a number of documents, but this does not help in detecting the expected frequency value. A common practice in frequency dictionaries is to multiply the raw counts by a dispersion measure (by Juilland’s D in the frequency"
2020.lrec-1.298,E17-2068,0,0.0107005,"ing. OpenWebText is a public replication of the corpus used to train GPT-2 (Radford et al., 2019), which is based on extraction of Web pages from all URLs upvoted 3 or more times on the Reddit website. OpenWebText (OWT) is also one of the components used for pre-training Roberta (Liu et al., 2019) using a publicly available pipeline.2 In contrast to extraction of popular URLs, ukWac is a corpus produced by crawling the .uk Internet domain (Baroni et al., 2009). Wikipedia is also often used for pre-training, in particular in such commonly used models as BERT (Devlin et al., 2018) and fastText (Joulin et al., 2017). The size of the lexicon for each corpus in Table 1 is presented for all orthographic tokens (excluding punctuation and tokens only consisting of numbers), as well as the lexicon of tokens occurring 10 or more times (L10). Another important difference between these corpora is that there are many users authoring any given text in Wikipedia, while a single author is typically responsible for writing each text in other Web corpora. 3. 3.1. Estimation of frequency distributions the number of occurrences of a word in a text i ni P the size of a text i in tokens C= ci count, the total number of occ"
2020.lrec-1.298,C94-2174,0,0.172089,"orm correction measure to the overall count, while the frequency bursts are specific to individual texts. Active research in comparing corpus composition using keywords and most frequent words has started since (Kilgarriff, 2001), followed by (Kilgarriff, 2012). It has been shown that the use of topic modelling helps in finding the differences between the Web corpora (Fothergill et al., 2016; Sharoff, 2013). Since the arrival of machine learning methods in the 1990s, genre classification and related approaches to classification of texts with respect to their stylistic features developed from (Karlgren and Cutting, 1994) to (Pritsos and Stamatatos, 2018), see a recent overview in (Argamon, 2019). However, genre classification methods have not been yet applied to very large corpora from the Web. 7. Conclusions and further work The study explored the significant differences in the lexicon and in the composition of OpenWebText, ukWac and Wikipedia, three large Web corpora, which are commonly used for pre-training language models. The size of a corpus (all of them measure in billions of words) is not the only consideration for its effective use in pre-training. Its lexicon and its composition in terms of topics a"
2020.lrec-1.298,pomikalek-etal-2012-building,0,0.0848488,"Missing"
2020.lrec-1.298,W00-0901,0,0.436116,"re derived from statistical considerations on the influence functions (Huber, 2011). They can be tuned to reflect the nature of the frequency distributions in corpora and the desired effect, i.e. the smaller they are the higher is the penalty on frequency bursts. Summing up the ri values gives an estimate of the robust frequency R for a word in a corpus, which can be used for establishing the core lexicon in order to determine the BPEs less affected by the frequency bursts. 3.3. Core lexicon estimation results The effect of Winsorisation can be measured by using the log-likelihood (LL) score (Rayson and Garside, 2000), i.e., by comparing the original frequency counts against the robust frequency counts from the same corpus as follows: C C +R R + C ln ; where E = E E 2 Words affected by the frequency bursts in ukWac ordered by their LL score are: insurance, shall, sudoku, search, fire, waste, library, hotel, tax, wedding, credit, language, loan, cancer, mortgage, surfing, replies, hms, mulder, nigritude The topical word nigritude in ukWac is a remainder of a Search Engine Optimisation contest run in 2004, in which the aim was to win by having a contestant’s page at the top of Google searches for a non-sensi"
2020.lrec-1.298,P16-1162,0,0.00924014,". . . 1 determined2175 , favor, license, prepared, wonderful, combined, stdclass, percentage, tree, entry, feed, vast The frequencies of highly topical words, like game, Trump, government, are close to those of function words; similarly stdclass is unlikely to belong to the core lexicon. This suggests that there are unknown biases in OpenWebText. Many Deep Learning approaches need to limit their lexicon by applying frequency thresholds, since neural predictions need to choose from a relatively small number of options, such as 20-30,000 words. Even methods operating with subwords, such as BPE (Sennrich et al., 2016), produce a substantial proportion of full-word entries. For example, out of 22,702 BPE codes of the uncased BERT model 20,079 items are full words, which are taken from the frequency list of the respective training corpus. Another important problem with corpora derived from the Web concerns the lack of reliable information about their composition. Usually Web corpora consist of millions of Web pages retrieved as a result of making queries to the search engines (Sharoff, 2006), crawling from a seed list (Baroni et al., 2009) or taking URLs which the users shared through social media, such as r"
2020.lrec-1.298,J11-2004,0,\N,Missing
2020.lrec-1.715,W11-2501,0,0.0994873,"Missing"
2020.lrec-1.715,C92-2082,0,0.557014,"an our neural path-based model, the combination of our two approaches demonstrates additional gains, since the two approaches use somewhat different data sources. 5) We illustrate that even our best transformer model still has certain limitations which are not always revealed by the standard datasets. We make our code and data publicly available. The next section overviews the prior related work. It is followed by the description of the models, followed by our empirical results. 2. Prior Work While earlier path-based approaches used small sets of manually crafted templates to detect patterns (Hearst, 1992; Snow et al., 2004), later works successfully involved trainable templates (Nakashole et al., 2012; Riedel et al., 2013). Successful models using trainable distributional representation of words (their embedding vectors) (Mikolov et al., 2013; Pennington et al., 2014) were developed and for 5838 some time surpassed the path-based methods in performance (Santus et al., 2016; Necsulescu et al., 2015). Levy et al. (2015) noted that supervised distributional methods tend to perform lexical memorization: instead of learning a relation between the two terms, they learn an independent property of a"
2020.lrec-1.715,N15-1098,0,0.161538,"y the description of the models, followed by our empirical results. 2. Prior Work While earlier path-based approaches used small sets of manually crafted templates to detect patterns (Hearst, 1992; Snow et al., 2004), later works successfully involved trainable templates (Nakashole et al., 2012; Riedel et al., 2013). Successful models using trainable distributional representation of words (their embedding vectors) (Mikolov et al., 2013; Pennington et al., 2014) were developed and for 5838 some time surpassed the path-based methods in performance (Santus et al., 2016; Necsulescu et al., 2015). Levy et al. (2015) noted that supervised distributional methods tend to perform lexical memorization: instead of learning a relation between the two terms, they learn an independent property of a single term in the pair. For example, if the training set contains pairs such as (dog, animal), (cat, animal), and (cow, animal), the algorithm learns to classify any new (x, animal) pair as true, regardless of x. Shwartz et al. (2016) successfully combined both distributional and path-based approaches into a single model that uses a recurrent neural network (RNN) and exceeded the best results at the time for the hyper"
2020.lrec-1.715,D12-1104,0,0.0197673,"Missing"
2020.lrec-1.715,S15-1021,0,0.143779,"ted work. It is followed by the description of the models, followed by our empirical results. 2. Prior Work While earlier path-based approaches used small sets of manually crafted templates to detect patterns (Hearst, 1992; Snow et al., 2004), later works successfully involved trainable templates (Nakashole et al., 2012; Riedel et al., 2013). Successful models using trainable distributional representation of words (their embedding vectors) (Mikolov et al., 2013; Pennington et al., 2014) were developed and for 5838 some time surpassed the path-based methods in performance (Santus et al., 2016; Necsulescu et al., 2015). Levy et al. (2015) noted that supervised distributional methods tend to perform lexical memorization: instead of learning a relation between the two terms, they learn an independent property of a single term in the pair. For example, if the training set contains pairs such as (dog, animal), (cat, animal), and (cow, animal), the algorithm learns to classify any new (x, animal) pair as true, regardless of x. Shwartz et al. (2016) successfully combined both distributional and path-based approaches into a single model that uses a recurrent neural network (RNN) and exceeded the best results at th"
2020.lrec-1.715,D14-1162,0,0.0905266,"Missing"
2020.lrec-1.715,N13-1008,0,0.0274534,"o approaches use somewhat different data sources. 5) We illustrate that even our best transformer model still has certain limitations which are not always revealed by the standard datasets. We make our code and data publicly available. The next section overviews the prior related work. It is followed by the description of the models, followed by our empirical results. 2. Prior Work While earlier path-based approaches used small sets of manually crafted templates to detect patterns (Hearst, 1992; Snow et al., 2004), later works successfully involved trainable templates (Nakashole et al., 2012; Riedel et al., 2013). Successful models using trainable distributional representation of words (their embedding vectors) (Mikolov et al., 2013; Pennington et al., 2014) were developed and for 5838 some time surpassed the path-based methods in performance (Santus et al., 2016; Necsulescu et al., 2015). Levy et al. (2015) noted that supervised distributional methods tend to perform lexical memorization: instead of learning a relation between the two terms, they learn an independent property of a single term in the pair. For example, if the training set contains pairs such as (dog, animal), (cat, animal), and (cow,"
2020.lrec-1.715,P18-2057,0,0.0119266,"or for the pair (x,y) is defined as the average context vector for its paths: P → − p vp (x, y) → − v xy = (2) #P aths(x, y) where #P aths(x, y) is the number of word paths connecting the pair (x,y). This vector, in turn, is used to make a classification decision, with an optional hidden layer. There have been several related studies following Shwartz et al. (2016): Shwartz et al. (2017) did extensive comparison of supervised vs. unsupervised approaches to detecting “is-a” relation. Washio and Kato (2018) looked at how additional word paths can be predicted even if they are not in the corpus. Roller et al. (2018) also looked at “is-a” relation and confirmed the importance of modeling word paths in addition to purely distributional methods. Still, the models from Shwartz et al. (2016) and Shwartz and Dagan (2016) remain unsurpassed within the class of wordpath models. We are using them as one of our baselines, along with the same datasets and same corpus data for a direct comparison of our models. Among distributional approaches, Wang et al. (2019) suggested using hyperspherical relation embeddings and improved over the results of Shwartz and Dagan (2016) on 3 out of 4 datasets. We use the model from W"
2020.lrec-1.715,L16-1722,0,0.0789538,"rviews the prior related work. It is followed by the description of the models, followed by our empirical results. 2. Prior Work While earlier path-based approaches used small sets of manually crafted templates to detect patterns (Hearst, 1992; Snow et al., 2004), later works successfully involved trainable templates (Nakashole et al., 2012; Riedel et al., 2013). Successful models using trainable distributional representation of words (their embedding vectors) (Mikolov et al., 2013; Pennington et al., 2014) were developed and for 5838 some time surpassed the path-based methods in performance (Santus et al., 2016; Necsulescu et al., 2015). Levy et al. (2015) noted that supervised distributional methods tend to perform lexical memorization: instead of learning a relation between the two terms, they learn an independent property of a single term in the pair. For example, if the training set contains pairs such as (dog, animal), (cat, animal), and (cow, animal), the algorithm learns to classify any new (x, animal) pair as true, regardless of x. Shwartz et al. (2016) successfully combined both distributional and path-based approaches into a single model that uses a recurrent neural network (RNN) and excee"
2020.lrec-1.715,W16-5304,0,0.143952,"d of learning a relation between the two terms, they learn an independent property of a single term in the pair. For example, if the training set contains pairs such as (dog, animal), (cat, animal), and (cow, animal), the algorithm learns to classify any new (x, animal) pair as true, regardless of x. Shwartz et al. (2016) successfully combined both distributional and path-based approaches into a single model that uses a recurrent neural network (RNN) and exceeded the best results at the time for the hypernymy detection (“is-a” relation). Their model was later extended to multiple relations in Shwartz and Dagan (2016) and became the stateof-the-art after winning a competition-format workshop on recognizing semantic relations. We include a mathematical description of their approach here since 1) We are using it as one of our baselines and 2) in the immediately following section, we elaborate how we overcome its shortcomings. Their approach (HypeNet, later called Lexnet) proceeds as following. The data consists of the targeted pairs of words along with their relationship label and all the word paths connecting the target pairs in the corpus. Each path consists of edges (words). Each edge is represented by a"
2020.lrec-1.715,P16-1226,0,0.270585,"a, 2015) and human language acquisition (Bybee and Beckner, 2015). While manually curated dictionaries exist, they are often out-of-date, not covering specialized domains, designed to be used by people, and exist for only a few well resourced languages (English, German, etc.). Therefore, here we are interested in methods for automated discovery (knowledge acquisition, taxonomy mining, etc.) . The automated approaches to detecting semantic relations between concepts (words or phrases) can be divided into two major groups: 1) path-based and 2) distributional methods. Path-based approaches (e.g. Shwartz et al. (2016)) essentially look for certain patterns in the joint occurrences of words (phrases, concepts, etc.) in the corpus. Thus, every word pair of interest (x,y) is represented by the set of word paths that connect x and y in a raw text corpus (e.g. Wikipedia). Distributional approaches (e.g. Wang et al. (2019)) are based on modeling the occurrences of each word, x or y, separately, not necessary in the proximity to each other. Our goal here is to improve, compare and combine those two classes of approaches. Attention-based transformers (e.g. Vaswani et al. (2017)) have been recently shown more effec"
2020.lrec-1.715,E17-1007,0,0.0121748,"2 , ve3 , ...} = {vet (x, y)} for t = 1, ..., lp , where lp is the path length. This sequence − is mapped by an RNN into a context vector → vp (x, y) defined for each path: −−→ → − vp (x, y) = RNN({vet (x, y)}) (1) The context vector for the pair (x,y) is defined as the average context vector for its paths: P → − p vp (x, y) → − v xy = (2) #P aths(x, y) where #P aths(x, y) is the number of word paths connecting the pair (x,y). This vector, in turn, is used to make a classification decision, with an optional hidden layer. There have been several related studies following Shwartz et al. (2016): Shwartz et al. (2017) did extensive comparison of supervised vs. unsupervised approaches to detecting “is-a” relation. Washio and Kato (2018) looked at how additional word paths can be predicted even if they are not in the corpus. Roller et al. (2018) also looked at “is-a” relation and confirmed the importance of modeling word paths in addition to purely distributional methods. Still, the models from Shwartz et al. (2016) and Shwartz and Dagan (2016) remain unsurpassed within the class of wordpath models. We are using them as one of our baselines, along with the same datasets and same corpus data for a direct comp"
2020.lrec-1.715,P19-1169,0,0.0478122,"ted in methods for automated discovery (knowledge acquisition, taxonomy mining, etc.) . The automated approaches to detecting semantic relations between concepts (words or phrases) can be divided into two major groups: 1) path-based and 2) distributional methods. Path-based approaches (e.g. Shwartz et al. (2016)) essentially look for certain patterns in the joint occurrences of words (phrases, concepts, etc.) in the corpus. Thus, every word pair of interest (x,y) is represented by the set of word paths that connect x and y in a raw text corpus (e.g. Wikipedia). Distributional approaches (e.g. Wang et al. (2019)) are based on modeling the occurrences of each word, x or y, separately, not necessary in the proximity to each other. Our goal here is to improve, compare and combine those two classes of approaches. Attention-based transformers (e.g. Vaswani et al. (2017)) have been recently shown more effective than convolutional and recurrent neural models for several natural text applications, leading to new state-of-the-art results on several benchmarks including GLUE, MultiNLI, and SQuAD (Devlin et al., 2018; Lample and Conneau, 2019). At the same time, we are not aware of any applications of attention"
2020.lrec-1.715,N18-1102,0,0.0158221,"o a context vector → vp (x, y) defined for each path: −−→ → − vp (x, y) = RNN({vet (x, y)}) (1) The context vector for the pair (x,y) is defined as the average context vector for its paths: P → − p vp (x, y) → − v xy = (2) #P aths(x, y) where #P aths(x, y) is the number of word paths connecting the pair (x,y). This vector, in turn, is used to make a classification decision, with an optional hidden layer. There have been several related studies following Shwartz et al. (2016): Shwartz et al. (2017) did extensive comparison of supervised vs. unsupervised approaches to detecting “is-a” relation. Washio and Kato (2018) looked at how additional word paths can be predicted even if they are not in the corpus. Roller et al. (2018) also looked at “is-a” relation and confirmed the importance of modeling word paths in addition to purely distributional methods. Still, the models from Shwartz et al. (2016) and Shwartz and Dagan (2016) remain unsurpassed within the class of wordpath models. We are using them as one of our baselines, along with the same datasets and same corpus data for a direct comparison of our models. Among distributional approaches, Wang et al. (2019) suggested using hyperspherical relation embedd"
2020.lrec-1.715,2020.lrec-1.229,1,0.811297,"ls from Shwartz et al. (2016) and Shwartz and Dagan (2016) remain unsurpassed within the class of wordpath models. We are using them as one of our baselines, along with the same datasets and same corpus data for a direct comparison of our models. Among distributional approaches, Wang et al. (2019) suggested using hyperspherical relation embeddings and improved over the results of Shwartz and Dagan (2016) on 3 out of 4 datasets. We use the model from Wang et al. (2019) as another baseline. A cognate approach concerns a stacked architecture combining CNN and LSTM, which for example was used in (Yuan and Sharoff, 2020) for classification of translation errors. 3. 3.1. Combined Models for Semantic Relations Path-Based 3.1.1. Informal Description Since our proposed path-based model does not use a recurrent network, it is simpler to describe and faster to train. It also resolves several shortcomings of the current state-ofthe-art model by Shwartz et al. ((Shwartz et al., 2016)) as we explain below. Figure 1 presents an informal intuitive illustration. We jointly train our semantic classification along with an unsupervised language modeling (LM) task which captures the probability distribution over sequences of"
2021.wanlp-1.11,2020.osact-1.2,0,0.0506288,"Missing"
2021.wanlp-1.11,P19-4007,0,0.0207848,"Missing"
2021.wanlp-1.11,W14-4912,1,0.779442,"Missing"
2021.wanlp-1.11,L16-1038,0,0.0383982,"Missing"
2021.wanlp-1.11,L18-1550,0,0.0123915,"matiser led to our new wide-coverage Arabic frequency list, which can be used to predict difficulty as Entropy of the probability distribution of each label in a sentence. The current list shows some consistency with the English profile list in terms of the percentage of words allocated to each CEFR level. 3.2 Sentence embeddings In addition to the 34 traditional features we can represent sentences as embedding vectors using different neural models as following: fastText 2 tool. Using the Arabic ar.300.bin file in which each word in WE is represented by the 1D vector mapped of 300 attributes (Grave et al., 2018). We had to normalize the sentence vectors to have the same length with respect to dimensions. For this, we calculated tf-idf weights of each word in the corpus to use them as weights: s = w1 w2 . . . .wn Embed.[s] = 1X tf idf [wi ] ∗ Embed.[wi ] (1) n i Universal sentence encoder (Yang et al., 2019) This model requires modeling the meaning of word sequences rather than just individual words. Also it was generated mainly to be used on the sentence level which after sentence tokenization, it encodes sentence to a 512-dimensional vector. We used here the large version3 . fastText A straightforwa"
2021.wanlp-1.11,pasha-etal-2014-madamira,0,0.110326,"Missing"
2021.wanlp-1.11,2020.tacl-1.54,0,0.0380466,"Missing"
2021.wanlp-1.11,W18-3703,0,0.0346728,"Missing"
2021.wanlp-1.11,2020.semeval-1.271,0,0.0840895,"Missing"
2021.wanlp-1.11,D16-1192,0,0.0487058,"Missing"
2021.wanlp-1.11,P05-1065,0,0.135411,"ied with label 0, in a total of 5960 sentence. The two models trained on this similarity task (AraBert and Arabic-Bert) achieve the F-1 measure of 0.98, leading to the ability to detect sentences which need simplification according to the Dataset Two standard. 5 Related Work on Arabic The last two decades have seen enormous efforts (especially for the English language) to develop readability measurement ranging from the traditional readability formulae to ML algorithms. English language researchers have introduced more than 200 readability formulae (DuBay, 2004) as well as hundreds of models (Schwarm and Ostendorf, 2005). In contrast, less research has addressed Arabic language issues and their challenges for robust readability formulae. Some attempts to formulate statistical formulae 111 for the Arabic language reflected traditional English formulae such as the Flesch–Kincaid Grade. The simplest formulae included the average word length, the average sentence and other surface features. According to Cavalli-Sforza et al. (2018) these simple formulae are Dawood formula (1977), Al-Heeti formula (1984), and the formula presented by Daud et al. (2013) based on a corpus. The more sophisticated formulae represent t"
2021.wanlp-1.11,C16-2048,0,0.0223623,"n shorter ones, which are connected by conjunctions The POS-features Syntactic features Features (22-27) from Table 3 provide some information about the sentences structures and number of phrases as well as phases types. These features are derived from a dependency grammar analysis. Because dependency grammar is based on word-word relations, it assumes that the structure of a sentence consists of lexical items that are attached to each other by binary asymmetrical relations, which is known as dependency relations. These relations will be more representative for this task. We used CamelParser (Shahrour et al., 2016) a system for Arabic syntactic dependency analysis together with contextually disambiguated morphological features which rely on the MADAMIRA morphological analysis for more robust results. 3.1.3 CEFR-level lexical features Features (28-34) from Table 3 are used to assign each word in the sentence with an appropriate CEFR level. For this, we created a new Arabic word list consisting of 8834 unique lemmas labelled with CEFR levels. This list was a combination of three frequency lists, 1) Buckwalter and Parkinson 5000 frequency word list based on 107 1 2 3 4 5 6 7 8 9 10 11 22 23 24 28 29 30 31"
2021.wanlp-1.11,W14-3509,0,0.030029,"ic-BERT5 ). Both models contain both Modern Standard Arabic (MSA) and Dialectal Arabic (DA). The pretraining data used for the AraBERT model consist of 70 million sentences (Antoun et al., 2020). Arabic-BERT trained on both filtered Arabic Common Crawl and a recent dump of Arabic Wikipedia contain approximately 8.2 Billion words(Safaya et al., 2020). 4 Experiments CEFR language proficiency levels can be presented as labels or as a continuous scale. The former is solved as a classification task with macro-averaged F-1 as the main measure for accuracy. The latter is solved as a regression task (Vajjala and Loo, 2014). At first we decided to work with the three main levels (A,B,and C) because it was quite difficult to determine the boundary between the inner sub levels as in the boundary between B1 and B2.Yet, the other binary classification is either Simple (A+B) or Complex (C). Here there is a problem for evaluation, since the gold standard labels are represented as integers 1, 2, 3 (for the A, B and C levels respectively), which leads to a large number of ties. Out of the standard correlation measures, Kendall’s tau-b is designed to handle ties, so in addition to Pearson’s ρ this is our measure for regr"
asheghi-etal-2014-designing,W04-0811,0,\N,Missing
asheghi-etal-2014-designing,sharoff-etal-2010-web,1,\N,Missing
asheghi-etal-2014-designing,D08-1027,0,\N,Missing
asheghi-etal-2014-designing,J06-3004,0,\N,Missing
asheghi-etal-2014-designing,P08-1080,0,\N,Missing
asheghi-etal-2014-designing,J06-1005,0,\N,Missing
asheghi-etal-2014-designing,J08-4004,0,\N,Missing
asheghi-etal-2014-designing,D09-1030,0,\N,Missing
asheghi-etal-2014-designing,P97-1005,0,\N,Missing
asheghi-etal-2014-designing,P09-1076,0,\N,Missing
asheghi-etal-2014-designing,J00-4003,0,\N,Missing
asheghi-etal-2014-designing,rehm-etal-2008-towards,0,\N,Missing
asheghi-etal-2014-designing,P05-1045,0,\N,Missing
asheghi-etal-2014-designing,J09-4005,0,\N,Missing
babych-etal-2008-generalising,rapp-2004-freely,0,\N,Missing
babych-etal-2008-generalising,W06-2405,0,\N,Missing
babych-etal-2008-generalising,P07-1018,1,\N,Missing
babych-etal-2008-generalising,P06-1085,0,\N,Missing
babych-etal-2008-generalising,P06-1011,0,\N,Missing
babych-etal-2008-generalising,P07-1084,0,\N,Missing
babych-etal-2008-generalising,2005.mtsummit-papers.11,0,\N,Missing
babych-etal-2008-generalising,sharoff-2006-uniform,1,\N,Missing
baroni-etal-2008-cleaneval,W06-1421,1,\N,Missing
baroni-etal-2008-cleaneval,J03-3001,1,\N,Missing
bateman-etal-2000-resources,P96-1026,0,\N,Missing
bateman-etal-2000-resources,J94-4004,0,\N,Missing
C00-1069,A97-1017,0,0.0313464,"Missing"
ciobanu-etal-2006-using,tufis-2000-using,0,\N,Missing
E06-2014,bennison-bowker-2000-designing,0,0.0158976,"or a sentence, phrase or a query expression in the source language the tool detects the semantic type of the situation in question and gives examples of similar contexts from the target language corpus. 1 Introduction It is widely acknowledged that human translators can benefit from a wide range of applications in computational linguistics, including Machine Translation (Carl and Way, 2003), Translation Memory (Planas and Furuse, 2000), etc. There have been recent research on tools detecting translation equivalents for technical vocabulary in a restricted domain, e.g. (Dagan and Church, 1997; Bennison and Bowker, 2000). The methodology in this case is based on extraction of terminology (both single and multiword units) and alignment of extracted terms using linguistic and/or statistical techniques (Déjean et al., 2002). In this project we concentrate on words from the general lexicon instead of terminology. The rationale for this focus is related to the fact that translation of terms is (should be) stable, while general words can vary significantly in their translation. It is important to populate the terminological database with terms that are missed in dictionaries or specific to a problem domain. However"
E06-2014,C02-1166,0,0.0130442,"duction It is widely acknowledged that human translators can benefit from a wide range of applications in computational linguistics, including Machine Translation (Carl and Way, 2003), Translation Memory (Planas and Furuse, 2000), etc. There have been recent research on tools detecting translation equivalents for technical vocabulary in a restricted domain, e.g. (Dagan and Church, 1997; Bennison and Bowker, 2000). The methodology in this case is based on extraction of terminology (both single and multiword units) and alignment of extracted terms using linguistic and/or statistical techniques (Déjean et al., 2002). In this project we concentrate on words from the general lexicon instead of terminology. The rationale for this focus is related to the fact that translation of terms is (should be) stable, while general words can vary significantly in their translation. It is important to populate the terminological database with terms that are missed in dictionaries or specific to a problem domain. However, once the translation of a term in a domain has been identified, stored in a dictionary and learned by the translator, the process of translation can go on without consulting a dictionary or a corpus. In"
E06-2014,C00-2090,0,0.0118943,"contextual examples of translation equivalents for words from the general lexicon using comparable corpora and semantic annotation that is uniform for the source and target languages. For a sentence, phrase or a query expression in the source language the tool detects the semantic type of the situation in question and gives examples of similar contexts from the target language corpus. 1 Introduction It is widely acknowledged that human translators can benefit from a wide range of applications in computational linguistics, including Machine Translation (Carl and Way, 2003), Translation Memory (Planas and Furuse, 2000), etc. There have been recent research on tools detecting translation equivalents for technical vocabulary in a restricted domain, e.g. (Dagan and Church, 1997; Bennison and Bowker, 2000). The methodology in this case is based on extraction of terminology (both single and multiword units) and alignment of extracted terms using linguistic and/or statistical techniques (Déjean et al., 2002). In this project we concentrate on words from the general lexicon instead of terminology. The rationale for this focus is related to the fact that translation of terms is (should be) stable, while general wor"
E06-2014,rapp-2004-freely,0,0.0194653,"are common for concepts in both languages. The search space is further restricted by applying knowledge-based and statistical filters (such as part-of-speech and semantic class filters, IDF filter, etc), by testing the co-occurrence of members of different similarity classes or by manually selecting the presented variants. These procedures are elementary building blocks that are used in designing different search strategies efficient for different types of translation equivalents 4 Simclasses consist of words sharing collocates and are computed using Singular Value Decomposition, as used by (Rapp, 2004), e.g. Paris and Strasbourg are produced for Brussels, or bus, tram and driver for passenger. and contexts. The core functionality of the system is intended to be self-explanatory and to have a shallow learning curve: in many cases default search parameters work well, so it is sufficient to input a word or an expression in the source language in order to get back a useful list of translation equivalents, which can be manually checked by a translator to identify the most suitable solution for a given context. For example, the word combination frustrated passenger is not found in the major Engli"
J15-1009,baroni-etal-2008-cleaneval,1,0.863599,"Missing"
kurella-etal-2008-corpus,ciobanu-etal-2006-using,1,\N,Missing
kurella-etal-2008-corpus,baroni-bernardini-2004-bootcat,0,\N,Missing
L16-1581,W12-3108,0,0.0590944,"Missing"
L16-1581,babych-hartley-2008-sensitivity,1,0.803416,"great variation of otherwise more or less acceptable translation options, even for the same translator, while MT errors are often system-bound, with more or less foreseeable changes. Secondly, MT output is often below what is acceptable, for example, for post-editing tasks, while human translations usually have reasonable quality, at least acceptable for post-editing. Therefore, TQA distinguishes between translations of usable quality that would in any case require only a minimum amount of post-editing, while MT evaluation aims at distinguishing low-quality from reasonable quality MT output (Babych & Hartley, 2008). As the authors propose, automated MT evaluation metrics that compute proximity of MT output to its human gold-standard reference measure structural matches and heavily rely on the lexical level, so they tend to be insensitive to higher-level errors that are more typical for better translations. Human translations typically contain errors beyond the lexical level, to which proximity-based MT evaluation metrics are less sensitive. In spite of such gaps, automatic MT estimation can still lend some insight into automatic human translation estimation. As in automatic MT evaluation, automatic 3663"
L16-1581,eisele-chen-2010-multiun,0,0.0285594,"ic word properties, and such induced representations can be used as features in a supervised classifier(typically discriminative). In recent years, there has been an increased interest in using semantic embeddings as high-quality semantic features that embody bilingual translation equivalence across languages. The methods and rationale to train bilingual word embeddings have been well explained in Hermann & Blunsom (2014). Following the same approach, in order to train the English-Chinese word embedding model for this study, we make use of a combination of the English-Chinese part of MultiUN (Eisele & Chen, 2010) and UM corpora (Tian et al., 2014) that is roughly about 312 million tokens for English and 289 million for Chinese of ≈11million lines, with misaligned sentences eliminated from both corpora, and BiCVM 1 code to train the bilingual embedding 1 models (100 dimensions in order to save computing power and time) first and then extract the word embeddings for the source texts and all the translations from the English embedding model and Chinese embedding model respectively. For each word in the source text and its translation, we extracted a 100 dimension vector and then sum the total vectors of"
L16-1581,W12-3110,0,0.0186538,"her 11 high frequent part-of-speeches (POS) shared between the STs and TTs are selected as POS features in our experiment. We referred to Universal POS-Tagset to match pos tags in both English source text and Chinese translations in order to achieve better comparability (Petrov, Das & McDonald, 2011). As linguistically motivated features, part-of-speech related features are used as baseline features in the WMT Quality Estimation shared-task 2012 (Callison-Burch et al., 2012). For instance, number, percentage and ratio of content words and function words are extracted as linguistic features in Felice & Specia (2012). In the similar vein, POS tags were counted as shallow grammatical matches on both the source and the target (Avramidis, 2012). We have good reason that it might be contributing to the meaning transference from the source texts to the target texts. As a consequence, the researcher included frequencies of these 11 POS-tags (excluding foreign words as it is sparse) in both STs and TTs as a feature group. In both the source and the target texts. Dependencies have found their way into translation quality prediction (Fox, 2002; Owczarzak et al., 2007; Padó et al., 2009; Shah et al., 2013). In our"
L16-1581,W02-1039,0,0.0113431,"and function words are extracted as linguistic features in Felice & Specia (2012). In the similar vein, POS tags were counted as shallow grammatical matches on both the source and the target (Avramidis, 2012). We have good reason that it might be contributing to the meaning transference from the source texts to the target texts. As a consequence, the researcher included frequencies of these 11 POS-tags (excluding foreign words as it is sparse) in both STs and TTs as a feature group. In both the source and the target texts. Dependencies have found their way into translation quality prediction (Fox, 2002; Owczarzak et al., 2007; Padó et al., 2009; Shah et al., 2013). In our experiment, we extracted 28 types of target translation dependency structures, and used them in the prediction model. See Table 1 for the detailed list of monolingual features # F1-22 F23-78 F79-80 F81-82 F83-84 F85-86 F87-88 F89-90 F91-100 F101-108 2. Translation Quality Indicators Translation is subject to a continuous interaction of inner linguistic-textual factors, e.g., language norms and their constrains, and extra-linguistic factors, such as intertextuality, the translation brief, working conditions, translator’s co"
L16-1581,N13-1045,0,0.0184329,"r, Intellimetric and PaperRater, to just name a few, but it is not the case with human translation assessment, particularly student translations. A different set of technologies have been applied for automated translation quality evaluation, where a lot of attention has been paid to automatic evaluation of Machine Translation (MT), in the form of methods based on parallel corpora, for example, BLEU (Papineni et al., 2002), or MT Quality Estimation (QE), which estimates the suitability of MT output without a reference translation, QuEst (Shah et al., 2013) selects high quality MT translations (Ma & McKeown, 2013) or detects machine translation errors (Xiong, Zhang, & Li, 2010). However, automatic Translation Quality Assessment (TQA) for human translations is a more complicated problem compared to the automated evaluation of MT. One of the reasons is that machine translations, generally inferior to human translation, usually contain a much smaller range of translation errors in comparison with human translations. Human translation errors only partially overlap with errors made by MT and display greater variability, which makes human translations less predictable, see section 4.2 our pilot experiment fo"
L16-1581,P14-5010,0,0.00313667,"of this algorithm can be found in (Zhu & Ghahramani, 2002). In this study, we will use label spreading implemented on scikit-learn to see if we can improve the prediction accuracy with the majority of unlabelled data in our data set. Details will be reported in the following section. 4. Dataset and Results Chinese EFL Learners (Wen & Wang, 2008). The translations are produced by upper-intermediate level English and Non-English Majors, and thus can be viewed as trainee translators’ work. The data has been processed, sentence aligned and annotated with Stanford-Corenlp for English and Chinese (Manning et al., 2014). We extracted 165 features from both Six English source texts spanning from general texts to mildly scientific domain, with ad-hoc python scripts, plus the 200 word embedding features. Each translation text measures approximately 300-400 Chinese characters. Among the 2119 training samples, we manually scored 277 pieces of them in terms of their adequacy and fluency on a scale of 60 points (mean=38.23, interquartile range=7, range=18) for content adequacy and 40 points (mean= 27.84, interquartile range=8, range=22) for language fluency. Two Chinese native annotators, both are University Englis"
L16-1581,W07-0411,0,0.0379538,"Missing"
L16-1581,P02-1040,0,0.103953,"scoring have been sufficiently reliable to be commercialized and deployed for large international language test (Dodigovic, 2005: 104). Some well-known systems include Project Essay Grader, E2rater, Intellimetric and PaperRater, to just name a few, but it is not the case with human translation assessment, particularly student translations. A different set of technologies have been applied for automated translation quality evaluation, where a lot of attention has been paid to automatic evaluation of Machine Translation (MT), in the form of methods based on parallel corpora, for example, BLEU (Papineni et al., 2002), or MT Quality Estimation (QE), which estimates the suitability of MT output without a reference translation, QuEst (Shah et al., 2013) selects high quality MT translations (Ma & McKeown, 2013) or detects machine translation errors (Xiong, Zhang, & Li, 2010). However, automatic Translation Quality Assessment (TQA) for human translations is a more complicated problem compared to the automated evaluation of MT. One of the reasons is that machine translations, generally inferior to human translation, usually contain a much smaller range of translation errors in comparison with human translations"
L16-1581,petrov-etal-2012-universal,0,0.076107,"Missing"
L16-1581,W15-5710,1,0.768876,"ortance to it. Through many iterations, the top n predictors yielding the best performance are then selected (Kirkpatrick, 1984; Guyon et al., 2002). The experiment was performed using the Caret 2 package in R. 3.5 Label Propagation Label propagation finds communities in the real, complex networks. This algorithm, in comparison to others, has advantage in its running time, amount of priori information required, with the exception that it produces an aggregate of multiple solution instead. This approach resembles the k-NN nearest neighbours where closer data points tend to have similar labels (Rios & Sharoff, 2015). More detailed explanation of this algorithm can be found in (Zhu & Ghahramani, 2002). In this study, we will use label spreading implemented on scikit-learn to see if we can improve the prediction accuracy with the majority of unlabelled data in our data set. Details will be reported in the following section. 4. Dataset and Results Chinese EFL Learners (Wen & Wang, 2008). The translations are produced by upper-intermediate level English and Non-English Majors, and thus can be viewed as trainee translators’ work. The data has been processed, sentence aligned and annotated with Stanford-Corenl"
L16-1581,tian-etal-2014-um,0,0.0490161,"representations can be used as features in a supervised classifier(typically discriminative). In recent years, there has been an increased interest in using semantic embeddings as high-quality semantic features that embody bilingual translation equivalence across languages. The methods and rationale to train bilingual word embeddings have been well explained in Hermann & Blunsom (2014). Following the same approach, in order to train the English-Chinese word embedding model for this study, we make use of a combination of the English-Chinese part of MultiUN (Eisele & Chen, 2010) and UM corpora (Tian et al., 2014) that is roughly about 312 million tokens for English and 289 million for Chinese of ≈11million lines, with misaligned sentences eliminated from both corpora, and BiCVM 1 code to train the bilingual embedding 1 models (100 dimensions in order to save computing power and time) first and then extract the word embeddings for the source texts and all the translations from the English embedding model and Chinese embedding model respectively. For each word in the source text and its translation, we extracted a 100 dimension vector and then sum the total vectors of all words in the source text and it"
L16-1581,P10-1062,0,0.0166113,"is not the case with human translation assessment, particularly student translations. A different set of technologies have been applied for automated translation quality evaluation, where a lot of attention has been paid to automatic evaluation of Machine Translation (MT), in the form of methods based on parallel corpora, for example, BLEU (Papineni et al., 2002), or MT Quality Estimation (QE), which estimates the suitability of MT output without a reference translation, QuEst (Shah et al., 2013) selects high quality MT translations (Ma & McKeown, 2013) or detects machine translation errors (Xiong, Zhang, & Li, 2010). However, automatic Translation Quality Assessment (TQA) for human translations is a more complicated problem compared to the automated evaluation of MT. One of the reasons is that machine translations, generally inferior to human translation, usually contain a much smaller range of translation errors in comparison with human translations. Human translation errors only partially overlap with errors made by MT and display greater variability, which makes human translations less predictable, see section 4.2 our pilot experiment for reference. As a result, this non-uniformity of the creative hu"
L16-1581,P14-1006,0,\N,Missing
L18-1135,D16-1250,0,0.41524,"ingual dictionary to convert monolingual word embeddings into a shared space. That study was followed by other studies aimed at 844 Table 1: Polish Z z˙ ycia marionetek Wska´znik jako´sci z˙ ycia Alignments from Wikipedia for titles and words Russian English Из жизни марионеток From the Life of the Marionettes Индекс качества жизни Quality-of-life index Character alignment for words: m a r i o n e t e k м а р и о н е т о к improving the process of TM production, e.g., via Canonical Correspondence Analysis (Faruqui and Dyer, 2014), Global Correction (Dinu et al., 2014) or TM orthogonalisation (Artetxe et al., 2016). A traditionally accepted model for this task is based on constructing a linear transformation matrix W by minimising the following objective: min W X ||Wei − fi ||2 (2) where ei and fi are the respective embedding vectors in the two languages, which are supposed to be translations of each other according to the training set. The differences between the approaches are primarily in the method for building W, e.g., by stochastic gradient descent (Mikolov et al., 2013), CCA (Faruqui and Dyer, 2014), multivariate regression (Dinu et al., 2014) or matrices from the SVD transform (Artetxe et al., 2"
L18-1135,E17-2067,0,0.0463135,"Missing"
L18-1135,2007.mtsummit-papers.5,1,0.660779,"ll “morphological structure” (actually the Levenshtein Distance) for keeping only the cognate words in the output. However, further work on bilingual lexicon induction did not include the use of cognates, especially in the context of related languages. The importance of utilising links between related languages can be illustrated by the use of Machine Translation via a pivot language. A simple dictionary transfer from Ukrainian into Russian followed by MT for the better resourced Russian-English pair easily beats MT translating from Ukrainian directly into English using far smaller resources (Babych et al., 2007). Overall, many lesser resourced languages can benefit from Language Adaptation by applying the models developed for the better resourced ones. The present study advances the state of the art by combining existing techniques of building cross-lingual embedding from comparable corpora with the Weighted Levenshtein distance, when the weights are themselves obtained from the seed dictionaries, see Section 3. In addition to intrinsic evaluation of the parameters of bilingual lexicon induction, the quality of cross-lingual embeddings can be measured extrinsically through accuracy of their use in do"
L18-1135,P14-1023,0,0.0390271,"edding vector is: v(w) = w2v(w) + 1 X xn |N | (1) n∈N where w2v is the standard word embedding of w (using the skip-gram model), while N is the set of ngrams derived from this word, xn are their respective embeddings. Studies in extraction of bilingual lexicons from comparable corpora can be traced back to at least (Fung, 1995; Rapp, 1995), who described words via a vector of their collocates, translated some words using a seed dictionary and compared the vectors across the languages. Word embeddings offer a better way of building word vectors in comparison to the vectors of collocate counts (Baroni et al., 2014). Word embeddings across languages have been studied since (Klementiev et al., 2012). A seminal study, which transformed the field, was (Mikolov et al., 2013), which used a translation matrix (TM) trained on a seed bilingual dictionary to convert monolingual word embeddings into a shared space. That study was followed by other studies aimed at 844 Table 1: Polish Z z˙ ycia marionetek Wska´znik jako´sci z˙ ycia Alignments from Wikipedia for titles and words Russian English Из жизни марионеток From the Life of the Marionettes Индекс качества жизни Quality-of-life index Character alignment for wo"
L18-1135,P17-1080,0,0.0570699,"Missing"
L18-1135,N13-1073,0,0.0375414,"ering step. Similarly, filtering of cross-lingual embedding spaces via z˙ y c i a ж и зн и In a low resource setting, the seed dictionaries for related languages can be obtained from the titles of interlinked Wikipedia articles in two languages (iWiki links),1 see examples of aligned titles in Table 1. This helps in modelling scenarios when few parallel texts are available, e.g., for the Polish-Russian pair (Polish is included in Europarl, Russian is in the UN corpus, but very few reliable resources are available for the Polish-Russian pair). The titles have been word-aligned using FastAlign (Dyer et al., 2013). The resulting word-level dictionaries have been filtered against the respective frequency lists, since the Wikipedia titles are dominated by relatively infrequent proper names. In addition to providing the training lexicon, a seed dictionary can also be used to provide a character-level model for matching the cognates, see the part of Table 1 for examples of character alignment. The pairs of words from the training dictionary have been aligned on the character level (again using FastAlign in this study) to produce the probabilities of regular correspondences between the characters in the two"
L18-1135,eisele-chen-2010-multiun,0,0.0327466,"anguages. The tools and the aligned word embedding spaces for the Romance and Slavonic language families have been released. Keywords: Word embeddings, Related languages, Cognate words, Comparable corpora 1. Introduction Parallel corpora play an important role in many multilingual NLP applications, such as Machine Translation, Cross-Lingual Text Classification or Information Retrieval. However, the topics and genres of parallel corpora are limited even for better resourced languages, e.g., resources are scarcer outside of the official documents of Europarl and the United Nations (Koehn, 2005; Eisele and Chen, 2010). Also, even if each individual language has reasonably good parallel resources, such as Polish and Russian aligned with English, it is difficult to find a large parallel corpus, which contains this specific, e.g., Polish-Russian, language pair. Monolingual corpora can be substantially bigger and more varied in comparison to parallel ones. Comparable corpora of different levels of comparability (Sharoff et al., 2013) can be used for induction of bilingual lexicons from small seed dictionaries. The present paper follows an influential study (Mikolov et al., 2013), which presented a method for b"
L18-1135,E14-1049,0,0.198925,", was (Mikolov et al., 2013), which used a translation matrix (TM) trained on a seed bilingual dictionary to convert monolingual word embeddings into a shared space. That study was followed by other studies aimed at 844 Table 1: Polish Z z˙ ycia marionetek Wska´znik jako´sci z˙ ycia Alignments from Wikipedia for titles and words Russian English Из жизни марионеток From the Life of the Marionettes Индекс качества жизни Quality-of-life index Character alignment for words: m a r i o n e t e k м а р и о н е т о к improving the process of TM production, e.g., via Canonical Correspondence Analysis (Faruqui and Dyer, 2014), Global Correction (Dinu et al., 2014) or TM orthogonalisation (Artetxe et al., 2016). A traditionally accepted model for this task is based on constructing a linear transformation matrix W by minimising the following objective: min W X ||Wei − fi ||2 (2) where ei and fi are the respective embedding vectors in the two languages, which are supposed to be translations of each other according to the training set. The differences between the approaches are primarily in the method for building W, e.g., by stochastic gradient descent (Mikolov et al., 2013), CCA (Faruqui and Dyer, 2014), multivariat"
L18-1135,W95-0114,0,0.0686247,"rom predictions of word neighbours using neural networks (Bengio et al., 2003; Mikolov, 2012). Recently, the Facebook group developed FastText, an updated method for producing monolingual embeddings by using information from character ngrams (Mikolov et al., 2017), i.e., a word embedding vector is: v(w) = w2v(w) + 1 X xn |N | (1) n∈N where w2v is the standard word embedding of w (using the skip-gram model), while N is the set of ngrams derived from this word, xn are their respective embeddings. Studies in extraction of bilingual lexicons from comparable corpora can be traced back to at least (Fung, 1995; Rapp, 1995), who described words via a vector of their collocates, translated some words using a seed dictionary and compared the vectors across the languages. Word embeddings offer a better way of building word vectors in comparison to the vectors of collocate counts (Baroni et al., 2014). Word embeddings across languages have been studied since (Klementiev et al., 2012). A seminal study, which transformed the field, was (Mikolov et al., 2013), which used a translation matrix (TM) trained on a seed bilingual dictionary to convert monolingual word embeddings into a shared space. That study w"
L18-1135,C12-1089,0,0.0312543,"word embedding of w (using the skip-gram model), while N is the set of ngrams derived from this word, xn are their respective embeddings. Studies in extraction of bilingual lexicons from comparable corpora can be traced back to at least (Fung, 1995; Rapp, 1995), who described words via a vector of their collocates, translated some words using a seed dictionary and compared the vectors across the languages. Word embeddings offer a better way of building word vectors in comparison to the vectors of collocate counts (Baroni et al., 2014). Word embeddings across languages have been studied since (Klementiev et al., 2012). A seminal study, which transformed the field, was (Mikolov et al., 2013), which used a translation matrix (TM) trained on a seed bilingual dictionary to convert monolingual word embeddings into a shared space. That study was followed by other studies aimed at 844 Table 1: Polish Z z˙ ycia marionetek Wska´znik jako´sci z˙ ycia Alignments from Wikipedia for titles and words Russian English Из жизни марионеток From the Life of the Marionettes Индекс качества жизни Quality-of-life index Character alignment for words: m a r i o n e t e k м а р и о н е т о к improving the process of TM production,"
L18-1135,2005.mtsummit-papers.11,0,0.00993544,"ss Slavonic languages. The tools and the aligned word embedding spaces for the Romance and Slavonic language families have been released. Keywords: Word embeddings, Related languages, Cognate words, Comparable corpora 1. Introduction Parallel corpora play an important role in many multilingual NLP applications, such as Machine Translation, Cross-Lingual Text Classification or Information Retrieval. However, the topics and genres of parallel corpora are limited even for better resourced languages, e.g., resources are scarcer outside of the official documents of Europarl and the United Nations (Koehn, 2005; Eisele and Chen, 2010). Also, even if each individual language has reasonably good parallel resources, such as Polish and Russian aligned with English, it is difficult to find a large parallel corpus, which contains this specific, e.g., Polish-Russian, language pair. Monolingual corpora can be substantially bigger and more varied in comparison to parallel ones. Comparable corpora of different levels of comparability (Sharoff et al., 2013) can be used for induction of bilingual lexicons from small seed dictionaries. The present paper follows an influential study (Mikolov et al., 2013), which"
L18-1135,N16-1030,0,0.0107657,"cross-lingual shared space has been tested through the Named Entity Recognition (NER) task, which consists in detection and labelling of all occurrences of person names, organisations or locations. This is a convenient downstream task for which there are existing methods and test sets. Recently, various neural network approaches produced very convincing results for NER (Collobert et al., 2011). A particular implementation used in the extrinsic evaluation experiment reported below is based on a sequence tagging method, which combines bidirectional LSTM with CRF for making the final prediction (Lample et al., 2016). Each word is represented by its embedding vector from the shared embedding space, in addition to other universal features, such as character-level embeddings or the presence of capitalisation. The tagger was trained on an existing NERannotated corpus from (Krek et al., 2012) (in Slovenian) with addition of small samples in Croatian, Czech, Polish, Russian and Ukrainian in order to provide at least some information for the character-level embeddings. The samples were derived from the titles of Wikipedia articles in the respective languages for categories matching such patterns as ‘Births’ (fo"
L18-1135,W17-1414,0,0.0130941,"es for categories matching such patterns as ‘Births’ (for person names), ‘Organisations’ and ‘Countries’ or ‘Villages’ (for the lack of a more generic category of locations in Wikipedia). 4.2. BSNLP NER shared task The NER shared task at BSNLP’17 contained two separate test sets with no training sets for individual languages. One test set was based on the European Commission reports, another one on news wires concerning Donald Trump. The baseline system (Piskorski et al., 2017) was based on large gazetteers developed by the JRC, while the only other submission covering all Slavonic languages (Mayfield et al., 2017) was based on projection of labels via word-aligned parallel corpora, see Table 4. The shared embedding space is surprisingly efficient. The Slovenian space was used for training, so it provides the upper baseline for adaptation. Czech, Croatian and Polish are sufficiently similar typologically, so the accuracy on those languages is slightly below what has been achieved for Slovenian. Russian and Ukrainian are East Slavonic languages, further away typologically from the rest, which is probably the main reason for the markedly lower accuracy of adaptation of the Slovenian training set. Across a"
L18-1135,W17-1412,0,0.0136617,"some information for the character-level embeddings. The samples were derived from the titles of Wikipedia articles in the respective languages for categories matching such patterns as ‘Births’ (for person names), ‘Organisations’ and ‘Countries’ or ‘Villages’ (for the lack of a more generic category of locations in Wikipedia). 4.2. BSNLP NER shared task The NER shared task at BSNLP’17 contained two separate test sets with no training sets for individual languages. One test set was based on the European Commission reports, another one on news wires concerning Donald Trump. The baseline system (Piskorski et al., 2017) was based on large gazetteers developed by the JRC, while the only other submission covering all Slavonic languages (Mayfield et al., 2017) was based on projection of labels via word-aligned parallel corpora, see Table 4. The shared embedding space is surprisingly efficient. The Slovenian space was used for training, so it provides the upper baseline for adaptation. Czech, Croatian and Polish are sufficiently similar typologically, so the accuracy on those languages is slightly below what has been achieved for Slovenian. Russian and Ukrainian are East Slavonic languages, further away typologi"
L18-1135,P95-1050,0,0.19826,"ons of word neighbours using neural networks (Bengio et al., 2003; Mikolov, 2012). Recently, the Facebook group developed FastText, an updated method for producing monolingual embeddings by using information from character ngrams (Mikolov et al., 2017), i.e., a word embedding vector is: v(w) = w2v(w) + 1 X xn |N | (1) n∈N where w2v is the standard word embedding of w (using the skip-gram model), while N is the set of ngrams derived from this word, xn are their respective embeddings. Studies in extraction of bilingual lexicons from comparable corpora can be traced back to at least (Fung, 1995; Rapp, 1995), who described words via a vector of their collocates, translated some words using a seed dictionary and compared the vectors across the languages. Word embeddings offer a better way of building word vectors in comparison to the vectors of collocate counts (Baroni et al., 2014). Word embeddings across languages have been studied since (Klementiev et al., 2012). A seminal study, which transformed the field, was (Mikolov et al., 2013), which used a translation matrix (TM) trained on a seed bilingual dictionary to convert monolingual word embeddings into a shared space. That study was followed b"
L18-1135,W15-3410,1,0.848358,"ing vectors in the two languages, which are supposed to be translations of each other according to the training set. The differences between the approaches are primarily in the method for building W, e.g., by stochastic gradient descent (Mikolov et al., 2013), CCA (Faruqui and Dyer, 2014), multivariate regression (Dinu et al., 2014) or matrices from the SVD transform (Artetxe et al., 2016). The latter model ensures that W is an orthogonal matrix built using a closed form solution: W = V × UT LD for the purposes of Statistical Machine Translation between related languages has been explored in (Rios and Sharoff, 2015). A manually developed set of rules for a Finite State Transducer (FST) was used for identification of cognates and borrowings in (Tsvetkov and Dyer, 2016). However, post hoc filtering improves precision at the expense of reduced coverage. The method suggested below operates at the ranking stage, while it also uses the Weighted Levenshtein Distance (WLD), a simpler alternative to FSTs. 3. 3.1. Dictionary induction using cognates Cross-lingual mapping The method for cross-lingual mapping across related languages in this study consists of three steps: 1. automated collection of seed bilingual di"
L18-1312,W16-2380,0,0.0145351,"r, it seems that our BMWU alignments have less effect on fluency. This contradicts our intuition but can be explained by the fact that alignment places more emphasis on correspondences, which are often oriented at semantic equivalence. In terms of the fluency scores, our BMWUs are relatively short (up to 4 words in this study), so longer units to capture the discourse markers, cohesion devices, etc. 5. Related Work Recent years have seen attempts using word alignment information for translation quality estimation (QE), for either machine translation or human translation (Ueffing et al., 2003; Abdelsalam et al., 2016; Specia et al., 2015; Camargo de Souza et al., 2013; Bach et al., 2011; Popovi´c et al., 2011; Popovic, 2012; Yuan et al., 2016) . As Abdelsalam et al. (2016) noted, the majority of these research focus on exploiting alignment related information for wordlevel QE. Among the few studies Abdelsalam et al. (2016), Camargo de Souza et al. (2013) and Yuan et al. (2016) actually try to tackle QE at the sentence-level or above, some features are too complex and not friendly interpretable to humans. For instance, Bach et al. (2011) use the source and target alignment context and even combine alignmen"
L18-1312,P11-1022,0,0.0306461,"dicts our intuition but can be explained by the fact that alignment places more emphasis on correspondences, which are often oriented at semantic equivalence. In terms of the fluency scores, our BMWUs are relatively short (up to 4 words in this study), so longer units to capture the discourse markers, cohesion devices, etc. 5. Related Work Recent years have seen attempts using word alignment information for translation quality estimation (QE), for either machine translation or human translation (Ueffing et al., 2003; Abdelsalam et al., 2016; Specia et al., 2015; Camargo de Souza et al., 2013; Bach et al., 2011; Popovi´c et al., 2011; Popovic, 2012; Yuan et al., 2016) . As Abdelsalam et al. (2016) noted, the majority of these research focus on exploiting alignment related information for wordlevel QE. Among the few studies Abdelsalam et al. (2016), Camargo de Souza et al. (2013) and Yuan et al. (2016) actually try to tackle QE at the sentence-level or above, some features are too complex and not friendly interpretable to humans. For instance, Bach et al. (2011) use the source and target alignment context and even combine alignment context with PoS tags, and Camargo de Souza et al. (2013) 1985 implem"
L18-1312,W02-1801,0,0.119358,", 2013), etc.) to obtain an aligned list of MWUs. Our study is mainly focused on investigating the contribution of BMWUs of varying lengths to the translation quality of trainee translations. For this task, we need an authentic database of BMWUs from a sizeable bilingual corpus of professional translations to have enough statistics for MWU identification and pruning. The extracted BMWUs will be used to measure the degree of adequacy and fluency of trainee translations. If professional translators view language expressions that can transfer meaning unambiguously as the basic translation units (Baobao et al., 2002), we believe, such a database of BMWUs can be a useful resource for human translation quality estimation. Investigation of BMWUs of different lengths can be viewed as part of the feature engineering for human quality estimation task. It is often observed that human translators translate group of words as a whole and words are rarely treated as the working translation units individually. Variation in translations from a large corpus and distribution of frequencies, as illustrated in Table 11 will lead to varying probabilities of the aligned BMWUs. Though the list of candidate translations is no"
L18-1312,W13-2243,0,0.0476003,"Missing"
L18-1312,P08-2007,0,0.0331479,"gnment, saving us from the trouble of a difficult task of monolingual MWU identification (Sag et al., 2002). The alignment process which is aimed at producing phrase tables for statistical machine translation (SMT) (Koehn et al., 2003) can be based on flat models or on hierarchical models. In traditionally used flat IBM family models, the phrase tables are generated in two steps, first generating word alignments and then extracting a scored table of phrase pairs. However, this often yields a large proportion of unwanted word alignments, as there are only minimal phrases memorized by the model(DeNero and Klein, 2008), so it has to be combined with heuristic phrase extraction to exhaustively combine adjacent phrases permitted by the word alignment (Och et al., 1999). In contrast, Bayesian-based phrase alignment as proposed in (Neubig et al., 2011) is a model for joint phrase alignment and extraction using non-parametric Bayesian methods and inversion transduction grammars (ITGs). A hierarchical ITG model relies on the Pitman-Yor process (Pitman and Yor, 1997) to directly use probabilities of the model as 1981 Table 1: Professional Translations for the Same Source Text Phrase Source Translation Frequency 总而"
L18-1312,N13-1073,0,0.0333914,"model relies on the Pitman-Yor process (Pitman and Yor, 1997) to directly use probabilities of the model as 1981 Table 1: Professional Translations for the Same Source Text Phrase Source Translation Frequency 总而言之 in sum in summary all in all in short in conclusion in general overall −− (omitted) 4 4 3 5 2 4 4 2 2.2. a replacement for the phrase table generated by heuristic techniques, e.g. intersection, grow-diag in Giza ++ (Och and Ney, 2003). Because of its compactness and competitive accuracy, we choose this method over other standard heuristic alignment tools ( e.g. Giza ++, fast align (Dyer et al., 2013), etc.) to obtain an aligned list of MWUs. Our study is mainly focused on investigating the contribution of BMWUs of varying lengths to the translation quality of trainee translations. For this task, we need an authentic database of BMWUs from a sizeable bilingual corpus of professional translations to have enough statistics for MWU identification and pruning. The extracted BMWUs will be used to measure the degree of adequacy and fluency of trainee translations. If professional translators view language expressions that can transfer meaning unambiguously as the basic translation units (Baobao"
L18-1312,W17-3204,0,0.0199992,"se probabilistic scores. It is interesting to investigate prediction of translation quality using parallel corpora from the same domain to measure the contribution of the proposed BMWU alignment ratios. Second, our experiment reported here accepts any phrase alignment from the professionally translated corpus as matching the trainee translations without taking their neighbouring contexts into consideration. We can try including a model for the context by using Recurrent Neural Networks methods from Neural Machine Translation when the neighbouring words contribute to the translation decisions (Koehn and Knowles, 2017). Another extension for this study concerns increasing the amount of reliable BMWUs by extracting them from the comparable corpora (Sharoff et al., 2013), since the amount of data from monolingual corpora is much greater than what comes from parallel corpora, especially for specific domains. There has been extensive research on alignment of the monolingual embedding spaces for individual words, see an overview in (Conneau et al., 2017), but so far not much on BMWUs. 7. Acknowledgements This study is partially funded by the Jiangsu Provincial Social Science Fund (No.: 17YYB013) and the teaching"
L18-1312,N03-1017,0,0.0491345,"s are used in this study, the method we use to extract BMWUs and our exploration of the relationship between BMWUs of different lengths and translation quality in terms of fluency and adequacy using the mixed-effect modelling. 2.1. Extraction of BMWUs As we work with the parallel corpora we can combine the task of identification of monolingual MWUs with the process of bilingual alignment, saving us from the trouble of a difficult task of monolingual MWU identification (Sag et al., 2002). The alignment process which is aimed at producing phrase tables for statistical machine translation (SMT) (Koehn et al., 2003) can be based on flat models or on hierarchical models. In traditionally used flat IBM family models, the phrase tables are generated in two steps, first generating word alignments and then extracting a scored table of phrase pairs. However, this often yields a large proportion of unwanted word alignments, as there are only minimal phrases memorized by the model(DeNero and Klein, 2008), so it has to be combined with heuristic phrase extraction to exhaustively combine adjacent phrases permitted by the word alignment (Och et al., 1999). In contrast, Bayesian-based phrase alignment as proposed in"
L18-1312,P07-2045,0,0.0105175,"61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100] range (Table 3). Parallel Corpus and the Trainee Data For this study we use the English Chinese parallel UM corpus of mixed domains (Tian et al., 2014). It is a multidomain and balanced parallel corpus covering several topics and text genres, including education, law, microblogs, news, science, spoken, subtitles and theses. The English part is tokenised with the scripts included within the Statistical Machine Translation system moses (Koehn et al., 2007). The Chinese part is segmented with Jieba Chinese word segmentation module.4 As for trainee translations, we have 277 student translations in six different domains scored by two raters in terms of their adequacy and fluency on a scale of 60 points (mean=38.23, interquartile range=7, range=18) for content adequacy and 40 points (mean= 27.84, interquartile range=8, range=22) for language fluency, so that the total Two Chinese native annotators, both are PhD students in Translation Studies, following the scoring scheme of ATA Certification Programme Rubric for Grading (Version 2011),5 measure th"
L18-1312,P11-1064,0,0.0175871,"an be based on flat models or on hierarchical models. In traditionally used flat IBM family models, the phrase tables are generated in two steps, first generating word alignments and then extracting a scored table of phrase pairs. However, this often yields a large proportion of unwanted word alignments, as there are only minimal phrases memorized by the model(DeNero and Klein, 2008), so it has to be combined with heuristic phrase extraction to exhaustively combine adjacent phrases permitted by the word alignment (Och et al., 1999). In contrast, Bayesian-based phrase alignment as proposed in (Neubig et al., 2011) is a model for joint phrase alignment and extraction using non-parametric Bayesian methods and inversion transduction grammars (ITGs). A hierarchical ITG model relies on the Pitman-Yor process (Pitman and Yor, 1997) to directly use probabilities of the model as 1981 Table 1: Professional Translations for the Same Source Text Phrase Source Translation Frequency 总而言之 in sum in summary all in all in short in conclusion in general overall −− (omitted) 4 4 3 5 2 4 4 2 2.2. a replacement for the phrase table generated by heuristic techniques, e.g. intersection, grow-diag in Giza ++ (Och and Ney, 20"
L18-1312,J03-1002,0,0.028412,"g et al., 2011) is a model for joint phrase alignment and extraction using non-parametric Bayesian methods and inversion transduction grammars (ITGs). A hierarchical ITG model relies on the Pitman-Yor process (Pitman and Yor, 1997) to directly use probabilities of the model as 1981 Table 1: Professional Translations for the Same Source Text Phrase Source Translation Frequency 总而言之 in sum in summary all in all in short in conclusion in general overall −− (omitted) 4 4 3 5 2 4 4 2 2.2. a replacement for the phrase table generated by heuristic techniques, e.g. intersection, grow-diag in Giza ++ (Och and Ney, 2003). Because of its compactness and competitive accuracy, we choose this method over other standard heuristic alignment tools ( e.g. Giza ++, fast align (Dyer et al., 2013), etc.) to obtain an aligned list of MWUs. Our study is mainly focused on investigating the contribution of BMWUs of varying lengths to the translation quality of trainee translations. For this task, we need an authentic database of BMWUs from a sizeable bilingual corpus of professional translations to have enough statistics for MWU identification and pruning. The extracted BMWUs will be used to measure the degree of adequacy a"
L18-1312,W99-0604,0,0.257852,"g phrase tables for statistical machine translation (SMT) (Koehn et al., 2003) can be based on flat models or on hierarchical models. In traditionally used flat IBM family models, the phrase tables are generated in two steps, first generating word alignments and then extracting a scored table of phrase pairs. However, this often yields a large proportion of unwanted word alignments, as there are only minimal phrases memorized by the model(DeNero and Klein, 2008), so it has to be combined with heuristic phrase extraction to exhaustively combine adjacent phrases permitted by the word alignment (Och et al., 1999). In contrast, Bayesian-based phrase alignment as proposed in (Neubig et al., 2011) is a model for joint phrase alignment and extraction using non-parametric Bayesian methods and inversion transduction grammars (ITGs). A hierarchical ITG model relies on the Pitman-Yor process (Pitman and Yor, 1997) to directly use probabilities of the model as 1981 Table 1: Professional Translations for the Same Source Text Phrase Source Translation Frequency 总而言之 in sum in summary all in all in short in conclusion in general overall −− (omitted) 4 4 3 5 2 4 4 2 2.2. a replacement for the phrase table generate"
L18-1312,W11-2109,0,0.0778963,"Missing"
L18-1312,W12-3116,0,0.021934,"y the fact that alignment places more emphasis on correspondences, which are often oriented at semantic equivalence. In terms of the fluency scores, our BMWUs are relatively short (up to 4 words in this study), so longer units to capture the discourse markers, cohesion devices, etc. 5. Related Work Recent years have seen attempts using word alignment information for translation quality estimation (QE), for either machine translation or human translation (Ueffing et al., 2003; Abdelsalam et al., 2016; Specia et al., 2015; Camargo de Souza et al., 2013; Bach et al., 2011; Popovi´c et al., 2011; Popovic, 2012; Yuan et al., 2016) . As Abdelsalam et al. (2016) noted, the majority of these research focus on exploiting alignment related information for wordlevel QE. Among the few studies Abdelsalam et al. (2016), Camargo de Souza et al. (2013) and Yuan et al. (2016) actually try to tackle QE at the sentence-level or above, some features are too complex and not friendly interpretable to humans. For instance, Bach et al. (2011) use the source and target alignment context and even combine alignment context with PoS tags, and Camargo de Souza et al. (2013) 1985 implement features, such as proportion of al"
L18-1312,P15-4020,0,0.0184896,"alignments have less effect on fluency. This contradicts our intuition but can be explained by the fact that alignment places more emphasis on correspondences, which are often oriented at semantic equivalence. In terms of the fluency scores, our BMWUs are relatively short (up to 4 words in this study), so longer units to capture the discourse markers, cohesion devices, etc. 5. Related Work Recent years have seen attempts using word alignment information for translation quality estimation (QE), for either machine translation or human translation (Ueffing et al., 2003; Abdelsalam et al., 2016; Specia et al., 2015; Camargo de Souza et al., 2013; Bach et al., 2011; Popovi´c et al., 2011; Popovic, 2012; Yuan et al., 2016) . As Abdelsalam et al. (2016) noted, the majority of these research focus on exploiting alignment related information for wordlevel QE. Among the few studies Abdelsalam et al. (2016), Camargo de Souza et al. (2013) and Yuan et al. (2016) actually try to tackle QE at the sentence-level or above, some features are too complex and not friendly interpretable to humans. For instance, Bach et al. (2011) use the source and target alignment context and even combine alignment context with PoS ta"
L18-1312,tian-etal-2014-um,0,0.0203659,"5 13-15 10-12 7-9 4-6 1-3 score of a student translation can be in the [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100] range (Table 3). Parallel Corpus and the Trainee Data For this study we use the English Chinese parallel UM corpus of mixed domains (Tian et al., 2014). It is a multidomain and balanced parallel corpus covering several topics and text genres, including education, law, microblogs, news, science, spoken, subtitles and theses. The English part is tokenised with the scripts included within the Statistical Machine Translation system moses (Koehn et al., 2007). The Chinese part is segmented with Jieba Chinese word segmentation module.4 As for trainee translations, we have 277 student translations in six different domains scored by two raters in terms of their adequacy and fluency on a scale of 60 points (mean=38.23, interquartile range=7, range=18"
L18-1312,2003.mtsummit-papers.52,0,0.103876,"cessing effort. However, it seems that our BMWU alignments have less effect on fluency. This contradicts our intuition but can be explained by the fact that alignment places more emphasis on correspondences, which are often oriented at semantic equivalence. In terms of the fluency scores, our BMWUs are relatively short (up to 4 words in this study), so longer units to capture the discourse markers, cohesion devices, etc. 5. Related Work Recent years have seen attempts using word alignment information for translation quality estimation (QE), for either machine translation or human translation (Ueffing et al., 2003; Abdelsalam et al., 2016; Specia et al., 2015; Camargo de Souza et al., 2013; Bach et al., 2011; Popovi´c et al., 2011; Popovic, 2012; Yuan et al., 2016) . As Abdelsalam et al. (2016) noted, the majority of these research focus on exploiting alignment related information for wordlevel QE. Among the few studies Abdelsalam et al. (2016), Camargo de Souza et al. (2013) and Yuan et al. (2016) actually try to tackle QE at the sentence-level or above, some features are too complex and not friendly interpretable to humans. For instance, Bach et al. (2011) use the source and target alignment context"
L18-1312,L16-1581,1,0.851818,"alignment places more emphasis on correspondences, which are often oriented at semantic equivalence. In terms of the fluency scores, our BMWUs are relatively short (up to 4 words in this study), so longer units to capture the discourse markers, cohesion devices, etc. 5. Related Work Recent years have seen attempts using word alignment information for translation quality estimation (QE), for either machine translation or human translation (Ueffing et al., 2003; Abdelsalam et al., 2016; Specia et al., 2015; Camargo de Souza et al., 2013; Bach et al., 2011; Popovi´c et al., 2011; Popovic, 2012; Yuan et al., 2016) . As Abdelsalam et al. (2016) noted, the majority of these research focus on exploiting alignment related information for wordlevel QE. Among the few studies Abdelsalam et al. (2016), Camargo de Souza et al. (2013) and Yuan et al. (2016) actually try to tackle QE at the sentence-level or above, some features are too complex and not friendly interpretable to humans. For instance, Bach et al. (2011) use the source and target alignment context and even combine alignment context with PoS tags, and Camargo de Souza et al. (2013) 1985 implement features, such as proportion of alignments connecting"
L18-1596,2010.eamt-1.44,0,0.245165,"Missing"
L18-1596,W04-1217,0,0.0270337,"e. number of tokens) of TT. This normalized term count can serve as a quality indicator in quality estimation tasks (i.e. supervised classification or regression to predict quality scores or class labels) as illustrated in the correlation analysis afterwards. 3.1. Term Classification N-gram technique is commonly used as a languageindependent approach, particularly for under-resourced language. Therefore, the term candidate classification is framed as a N-gram classification task rather than the conventional sequence labelling methods that are commonly seen in previous work (Zhou and Su, 2004; Finkel et al., 2004). From a pragmatic point of view, our features are computed by JATE 2.0 (Zhang et al., 2016). Most representative and language-independent statistic ATR techniques are available in the package. These features (See Table 1) . We briefly describe the features below: TTF, namely Term Total Frequency, is the total frequency of a candidate in the target corpus. This algorithm takes into account frequency information for retrieving words or phrases that are both highly indicative of document content and highly distinctive within a text collection. ATTF takes the average of TTF by dividing it by the"
L18-1596,I11-2003,0,0.0330642,"acy of methods in this approach is that they rely on the morphosynactic analyser of the term extractor that does not recognize all candidate terms and those chunk-based methods, having extended the alignment model with automatically extracted language pair specific rules. As a consequence, this method blurs the distinction between terms and non-terms. • Comparable-corpus Based Bilingual corpora in specialized domains are actually scarce and it is expensive to build high quality parallel texts of specialized domains. A practical solution to this limitation is to make use of comparable corpora (Rocheteau and Daille, 2011; Xu et al., 2015; Hakami and Bollegala, 2017) that are available in large quantities. However, term extraction along this line is often limited to noun phrases (< 5 words) from monolingual comparable corpora. Thus, the recall of such an approach is not satisfactory under some circumstances. For other studies in this approach, ambiguity of term translations and identification of synonymous terms need to be further addressed. • Web-data Based Web data mining is another means to collect terminology pairs (Erdmann et al., 2009; Gaizauskas et al., 2015). Despite the favourable findings from the ev"
L18-1596,W14-4811,1,0.824587,"matically identifying terms in human translations. However, drawbacks in handling low frequency terms and term variations shall be dealt in the future. Keywords: Bilingual terminology, translation quality, supervised learning, correlation analysis 1. Introduction Terminology helps translators organize their domain knowledge, and provides them means (usually terms in various lexical units) to express subject knowledge adequately. Translation scholars and practitioners maintain that terminology correctness is associated with the quality of translation (and interpretation) (Hartley et al., 2004; Xu and Sharoff, 2014; Kim et al., 2015; Brunette, 2000; Karoubi, 2016). The acknowledgement of the contribution of terminology to translation quality is also echoed by the translation industry and users (Secar˘a, 2005; Lommel et al., 2014; Warburton, 2013). Accurately reproducing the content of the original and using appropriate terminology has become the official assessment criteria of some famous in-use translation-errorbased evaluation schemes. For instance, the MeLLANGE project (Secar˘a, 2005) defines more than six terminology errors1 , and the Multidimensional Quality Metrics lists terminology as one of the"
L18-1596,L16-1581,1,0.868196,"Missing"
L18-1596,L16-1359,0,0.0275974,"ality estimation tasks (i.e. supervised classification or regression to predict quality scores or class labels) as illustrated in the correlation analysis afterwards. 3.1. Term Classification N-gram technique is commonly used as a languageindependent approach, particularly for under-resourced language. Therefore, the term candidate classification is framed as a N-gram classification task rather than the conventional sequence labelling methods that are commonly seen in previous work (Zhou and Su, 2004; Finkel et al., 2004). From a pragmatic point of view, our features are computed by JATE 2.0 (Zhang et al., 2016). Most representative and language-independent statistic ATR techniques are available in the package. These features (See Table 1) . We briefly describe the features below: TTF, namely Term Total Frequency, is the total frequency of a candidate in the target corpus. This algorithm takes into account frequency information for retrieving words or phrases that are both highly indicative of document content and highly distinctive within a text collection. ATTF takes the average of TTF by dividing it by the number of documents in which the candidate term occurs. 3775 Feature TTF ATTF TTF-IDF RIDF C"
L18-1596,W04-1219,0,0.0577761,"and the length (i.e. number of tokens) of TT. This normalized term count can serve as a quality indicator in quality estimation tasks (i.e. supervised classification or regression to predict quality scores or class labels) as illustrated in the correlation analysis afterwards. 3.1. Term Classification N-gram technique is commonly used as a languageindependent approach, particularly for under-resourced language. Therefore, the term candidate classification is framed as a N-gram classification task rather than the conventional sequence labelling methods that are commonly seen in previous work (Zhou and Su, 2004; Finkel et al., 2004). From a pragmatic point of view, our features are computed by JATE 2.0 (Zhang et al., 2016). Most representative and language-independent statistic ATR techniques are available in the package. These features (See Table 1) . We briefly describe the features below: TTF, namely Term Total Frequency, is the total frequency of a candidate in the target corpus. This algorithm takes into account frequency information for retrieving words or phrases that are both highly indicative of document content and highly distinctive within a text collection. ATTF takes the average of TTF"
L18-1605,S16-1081,0,0.0234795,"ome past shared tasks addressed related objectives. Cross-language plagiarism detection in PAN (Potthast et al., 2012) aims to spot text that has been translated into a target language and reused in (inserted into) text in that target language. It is therefore quite close to our task. However, plagiarism detection can take advantage of differences in style between the original target text and the translated text, and of intrinsic properties of ‘translationese’. This is not the case in our task, where all sentences are expected to be original. Cross-language text similarity as in SemEval 2016 (Agirre et al., 2016) assesses the level of semantic similarity of pairs of sentences on a given scale. It is also close to our task. Nevertheless, it has been proposed with already paired sentences instead of large monolingual corpora, thus removing the sentence spotting stage. Bilingual document alignment in a large Web collection has been proposed in WMT 2016 (Buck and Koehn, 2016). However, on the one hand it addressed documents instead of sentences; and on the other hand, it included meta-information in the form of document URLs, a property that we want to avoid. This highlights the need for a publicly availa"
L18-1605,W17-2508,0,0.0208688,"by human review of samples of resulting sequences of two sentences. • A configuration for indexing and search in the Solr search engine, typically based on a tokenizer, stop words, and possibly more language components. These datasets were used in the BUCC 2017 and 2018 Shared Tasks (Zweigenbaum et al., 2017; Zweigenbaum et al., 2018). Participants were taskeed with detecting in a bilingual pair of corpora the inserted parallel sentences. Three of the four language pairs were addressed by the participants in 2017: French, German, and Chinese, with a maximum F-score of 0.84 on German-English (Azpeitia et al., 2017) (see Table 3). All four language pairs were addressed in 2018, with improved F-scores topping at 0.86 for German-English again. • Constraints on the range of sentence lengths. We report here how we included Chinese data in the present corpus. The Chinese writing system does not separate words with spaces6 . This raises issues for tokenization that have consequences on our dataset construction pipeline. Various methods have been proposed to tokenize Chinese, including Conditional Random Fields classifiers in the Stanford Chinese Word Segmenter (Tseng et al., 2005) and in the Chinese Mecab7 . I"
L18-1605,W16-2347,0,0.026976,"n the original target text and the translated text, and of intrinsic properties of ‘translationese’. This is not the case in our task, where all sentences are expected to be original. Cross-language text similarity as in SemEval 2016 (Agirre et al., 2016) assesses the level of semantic similarity of pairs of sentences on a given scale. It is also close to our task. Nevertheless, it has been proposed with already paired sentences instead of large monolingual corpora, thus removing the sentence spotting stage. Bilingual document alignment in a large Web collection has been proposed in WMT 2016 (Buck and Koehn, 2016). However, on the one hand it addressed documents instead of sentences; and on the other hand, it included meta-information in the form of document URLs, a property that we want to avoid. This highlights the need for a publicly available dataset that would make it possible to compare methods that extract parallel sentences from comparable corpora. This paper describes the principles according to which we designed such a corpus, their implementation, the resulting corpus and a first use of that corpus in a shared task. This corpus was built in the context of the BUCC 2017 Shared Task described"
L18-1605,W03-1722,0,0.0180004,"eport here how we included Chinese data in the present corpus. The Chinese writing system does not separate words with spaces6 . This raises issues for tokenization that have consequences on our dataset construction pipeline. Various methods have been proposed to tokenize Chinese, including Conditional Random Fields classifiers in the Stanford Chinese Word Segmenter (Tseng et al., 2005) and in the Chinese Mecab7 . Independently of these methods, several guidelines have been proposed for human annotation of Chinese tokens, including the Chinese Penn Treebank and the Peking University standard (Duan et al., 2003). This results in tokens with shorter or larger spans depending on the guideline, for instance 有线 (cable) 电视 (television) according to Peking University vs. 有线电视 (cable television) according to Chinese Penn Treebank. Chinese tokenizers display the same variety in their choices of token span length; some, such as Stanford or jieba,8 leave it to the user to choose which strategy to apply (full=short, default=large, search=multiple solutions). We attempted to avoid these considerations by working directly with characters. This was initially motivated by the technical choice of Solr (v6.4.0), whos"
L18-1605,N04-1034,0,0.166203,"Missing"
L18-1605,W15-3411,1,0.839088,"ora with known parallel sentence pairs. We therefore needed to prevent as much as possible naturally occurring parallel sentence pairs from remaining in our monolingual corpora. The strategy we adopted in this purpose was to desynchronize our comparable corpora. Since we started from Wikipedia articles in two languages, we knew that interlinked articles would be highly likely to contain such parallel sentences: this is indeed a property that is often desired by past work on parallel sentence extraction. This is also how our previous shared task on detection of comparable texts has been setup (Sharoff et al., 2015): the gold standard was based on the iwiki links. In contrast to such work, we built pairs of monolingual corpora which never contained two interlinked Wikipedia articles. This was also in line with our desideratum not to include meta-information on the sentences, such as being found in two interlinked articles. The main drawback in doing so is that the most comparable pairs of documents for a given language pair are removed from the pairs of corpora we built: only one out of two interlinked pages can be kept in one of our corpora. This reduces the comparability of our datasets. However, the t"
L18-1605,N10-1063,0,0.0345766,"they display much more variety and are normally original texts rather than translations. They hold much promise therefore as a complement to parallel texts for machine translation and other applications. One way in which comparable corpora have been used to help machine translation is by spotting parallel sentences that occur naturally in these corpora, and using these sentence pairs to extend parallel corpora (Munteanu et al., 2004). This has motivated research into methods that aim to perform this task, such as (Utiyama and Isahara, 2003; Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). This task is usually called Parallel Sentence Extraction from Comparable Corpora. It is however difficult to compare earlier work and assess progress because of the absence of a shared dataset with gold standard annotations. Some past shared tasks addressed related objectives. Cross-language plagiarism detection in PAN (Potthast et al., 2012) aims to spot text that has been translated into a target language and reused in (inserted into) text in that target language. It is therefore quite close to our task. However, plagiarism detection can take advantage of differences in style between the o"
L18-1605,I05-3027,0,0.0204923,"of 0.84 on German-English (Azpeitia et al., 2017) (see Table 3). All four language pairs were addressed in 2018, with improved F-scores topping at 0.86 for German-English again. • Constraints on the range of sentence lengths. We report here how we included Chinese data in the present corpus. The Chinese writing system does not separate words with spaces6 . This raises issues for tokenization that have consequences on our dataset construction pipeline. Various methods have been proposed to tokenize Chinese, including Conditional Random Fields classifiers in the Stanford Chinese Word Segmenter (Tseng et al., 2005) and in the Chinese Mecab7 . Independently of these methods, several guidelines have been proposed for human annotation of Chinese tokens, including the Chinese Penn Treebank and the Peking University standard (Duan et al., 2003). This results in tokens with shorter or larger spans depending on the guideline, for instance 有线 (cable) 电视 (television) according to Peking University vs. 有线电视 (cable television) according to Chinese Penn Treebank. Chinese tokenizers display the same variety in their choices of token span length; some, such as Stanford or jieba,8 leave it to the user to choose which"
L18-1605,P03-1010,0,0.0953731,"r criteria such as domain, genre, time period. In contrast to parallel corpora, they display much more variety and are normally original texts rather than translations. They hold much promise therefore as a complement to parallel texts for machine translation and other applications. One way in which comparable corpora have been used to help machine translation is by spotting parallel sentences that occur naturally in these corpora, and using these sentence pairs to extend parallel corpora (Munteanu et al., 2004). This has motivated research into methods that aim to perform this task, such as (Utiyama and Isahara, 2003; Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). This task is usually called Parallel Sentence Extraction from Comparable Corpora. It is however difficult to compare earlier work and assess progress because of the absence of a shared dataset with gold standard annotations. Some past shared tasks addressed related objectives. Cross-language plagiarism detection in PAN (Potthast et al., 2012) aims to spot text that has been translated into a target language and reused in (inserted into) text in that target language. It is therefore quite close to our task. However, pla"
L18-1605,W17-2512,1,0.775299,"wever, on the one hand it addressed documents instead of sentences; and on the other hand, it included meta-information in the form of document URLs, a property that we want to avoid. This highlights the need for a publicly available dataset that would make it possible to compare methods that extract parallel sentences from comparable corpora. This paper describes the principles according to which we designed such a corpus, their implementation, the resulting corpus and a first use of that corpus in a shared task. This corpus was built in the context of the BUCC 2017 Shared Task described in (Zweigenbaum et al., 2017). The present paper provides more detail about our motivation and design criteria, about the rationale we followed to implement these design criteria, and about the processing of the Chinese part of the corpus. 2. A Dataset for Parallel Sentence Extraction from Comparable Corpora 2.1. Desiderata for a Dataset for the Task We aimed to build a bilingual corpus to measure progress on the identification of parallel sentences in monolingual corpora. This led us to the following desiderata and design choices. No metadata. We wish to focus on the cross-language comparison of sentence contents. Instea"
P06-2095,P98-2127,0,0.00486332,"languages. Unlike aligned parallel corpora, comparable corpora provide a model for each individual language, while dictionaries, which can serve as a bridge, are inadequate for the task in question, because the problem we want to address involves precisely translation equivalents that are not listed there. Therefore, a specific query needs first to be generalised in order to then retrieve a suitable candidate from a set of candidates. One way to generalise the query is by using similarity classes, i.e. groups of words with lexically similar behaviour. In his work on distributional similarity (Lin, 1998) designed a parser to identify grammatical relationships between words. However, broad-coverage parsers suitable for processing BNC-like corpora are not available for many languages. Another, resource-light approach treats the context as a bag of words (BoW) and detects the similarity of contexts on the basis of collocations in a window of a certain size, typically 3-4 words, e.g. (Rapp, 2004). Even if using a parser can increase precision in identification of contexts in the case of long-distance dependencies (e.g. to cook Alice a whole meal), we can find a reasonable set of relevant terms re"
P06-2095,J03-1002,0,0.00684622,"Missing"
P06-2095,C00-2090,0,0.0263067,"working on an option to identify semantic contexts by means of ‘semantic signatures’ obtained from a broad-coverage semantic parser, such as USAS (Rayson et al., 2004). The semantic tagset used by USAS is a languageindependent multi-tier structure with 21 major discourse fields, subdivided into 232 sub-categories (such as I1.1- = Money: lack; A5.1- = Evaluation: bad), which can be used to detect the semantic context. Identification of semantically similar situations can be also improved by the use of segment-matching algorithms as employed in Example-Based MT (EBMT) and translation memories (Planas and Furuse, 2000; Carl and Way, 2003). The proposed model looks similar to some implementations of statistical machine translation (SMT), which typically uses a parallel corpus for its translation model, and then finds the best possible recombination that fits into the target language model (Och and Ney, 2003). Just like an MT system, our tool can find translation equivalents for queries which are not explicitly coded as entries in system dictionaries. However, from the user perspective it resembles a dynamic dictionary or thesaurus: it translates difficult words and phrases, not entire sentences. The main th"
P06-2095,rapp-2004-freely,0,0.485094,"uitable candidate from a set of candidates. One way to generalise the query is by using similarity classes, i.e. groups of words with lexically similar behaviour. In his work on distributional similarity (Lin, 1998) designed a parser to identify grammatical relationships between words. However, broad-coverage parsers suitable for processing BNC-like corpora are not available for many languages. Another, resource-light approach treats the context as a bag of words (BoW) and detects the similarity of contexts on the basis of collocations in a window of a certain size, typically 3-4 words, e.g. (Rapp, 2004). Even if using a parser can increase precision in identification of contexts in the case of long-distance dependencies (e.g. to cook Alice a whole meal), we can find a reasonable set of relevant terms returned using the BoW approach, cf. the results of human evaluation for English and German by (Rapp, 2004). Finding translations in comparable corpora The proposed model finds potential translation equivalents in four steps, which include 1. expansion of words in the original expression using related words; 2. translation of the resultant set using existing bilingual dictionaries; 3. further ex"
P06-2095,P04-1079,1,0.82229,"terpretation of the results The results were surprising in so far as for the majority of problems translators preferred very different translation solutions and did not agree in their scores for the same solutions. For instance, concrete plan in Table 3 received the score 1 from translator t1 and 5 from t2. In general, the translators very often picked up on different opportunities presented by the suggestions from the lists, and most suggestions were equally legitimate ways of conveying the intended content, cf. the study of legitimate translation variation with respect to the BLEU score in (Babych and Hartley, 2004). In this respect it may be unfair to compute average scores for each potential solution, since for most interesting cases the scores do not fit into the normal distribution model. So averaging scores would mask the potential usability of really inventive solutions. In this case it is more reasonable to evaluate two sets of solutions – the one generated by ASSIST and the other found in dictionaries – but not each solution individually. In order to do that for each translation problem the best scores given by each translator in each of these two sets were selected. This way of generalising data"
P06-2095,C98-2122,0,\N,Missing
P07-1018,P98-1117,0,0.0155735,"tic field categories. Often a lexical item is mapped to multiple semantic categories, reflecting its potential multiple senses. In such cases, the tags are arranged by the order of likelihood of meanings, with the most prominent first. 3 Objective evaluation In the objective evaluation we tested the performance of our system on a selection of indirect translation problems, extracted from a parallel corpus consisting mostly of articles from English and Russian newspapers (118,497 words in the R-E direction, 589,055 words in the E-R direction). It has been aligned on the sentence level by JAPA (Langlais et al., 1998), and further on the word level by GIZA++ (Och and Ney, 2003). 3.1 Comparative performance The intuition behind the objective evaluation experiment is that the capacity of our tool to find indirect translation equivalents in comparable corpora can be compared with the results of automatic alignment of parallel texts used in translation models in SMT: one of the major advantages of the SMT paradigm is its ability to reuse indirect equivalents found in parallel corpora (equivalents that may never come up in hand-crafted dictionaries). Thus, automatically generated GIZA++ dictionaries with word a"
P07-1018,J03-1002,0,0.0109802,"ions are indirect in that they involve lexical shifts or POS transformations. Finding such translations is a hard task that can benefit from automated assistance. 'Mining' such indirect equivalents is difficult, precisely because of the structural mismatch, but also because of the paucity of suitable aligned corpora. The approach adopted here includes the use of comparable corpora in source and target languages, which are relatively easy to create. The challenge is to generate a list of usable solutions and to rank them such that the best are at the top. Thus the present system is unlike SMT (Och and Ney, 2003), where lexical selection is effected by a translation model based on aligned, parallel corpora, but the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terminology (Grefenstette, 2002), which allows a one-to-one correspondence irrespective of the context. Our system addresses difficulties in expressions in the general lexicon, whose translation is context-dependent. The structure of the paper is as follows. In Section 2 we present the method w"
P07-1018,2001.mtsummit-papers.68,0,0.017632,"ression плохо отремонтированные. It is also possible to translate it as unsatisfactory condition, bad state of repair, badly in need of repair, and so on. The objective evaluation shows that the system has been able to find the suggestion used by a particular translator for the problem studied. It does not tell us whether the system has found some other translations suitable for the context. Such legitimate translation variation implies that the performance of a system should be studied on the basis of multiple reference translations, though typically just two reference translations are used (Papineni, et al, 2001). This might be enough for the purposes of a fully automatic MT tool, but in the context of a translator's amanuensis which deals with expressions difficult for human translators, it is reasonable to work with a larger range of acceptable target expressions. With this in mind we evaluated the performance of the tool with a panel of 12 professional translators. Problematic expressions were highlighted and the translators were asked to find suitable suggestions produced by the tool for these expressions and rank their usability on a scale from 1 to 5 (not acceptable to fully idiomatic, so 1 mean"
P07-1018,P99-1067,0,0.112298,"able aligned corpora. The approach adopted here includes the use of comparable corpora in source and target languages, which are relatively easy to create. The challenge is to generate a list of usable solutions and to rank them such that the best are at the top. Thus the present system is unlike SMT (Och and Ney, 2003), where lexical selection is effected by a translation model based on aligned, parallel corpora, but the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terminology (Grefenstette, 2002), which allows a one-to-one correspondence irrespective of the context. Our system addresses difficulties in expressions in the general lexicon, whose translation is context-dependent. The structure of the paper is as follows. In Section 2 we present the method we use for mining translation equivalents. In Section 3 we present the results of an objective evaluation of the quality of suggestions produced by the system by comparing our output against a parallel corpus. Finally, in Section 4 we present a subjective evaluation focusing on the integrat"
P07-1018,rapp-2004-freely,0,0.174703,"general lexicon, which do not have established equivalents, but not yet for terminology. It relies on a high-quality bilingual dictionary (en-ru ~30k, ru-en ~50K words, combining ORD and the core part of Multitran) and large comparable corpora (~200M En, ~70M Ru) of news texts. For each of the SL query terms q the system generates its dictionary translation Tr(q) and its similarity class S(q) – a set of words with a similar distribution in a monolingual corpus. Similarity is measured as the cosine between collocation vectors, whose dimensionality is reduced by SVD using the implementation by Rapp (2004). The descriptor and each word in the similarity class are then translated into the TL using ORD or the Multitran dictionary, resulting in {Tr(q)∪ Tr(S(q))}. On the TL side we also generate similarity classes, 138 but only for dictionary translations of query terms Tr(q) (not for Tr(S(q)), which can make output too noisy). We refer to the resulting set of TL words as a translation class T. T = {Tr(q) ∪ Tr(S(q)) ∪ S(Tr(q))} Translation classes approximate lexical and structural transformations which can potentially be applied to each of the query terms. Automatically computed similarity classes"
P07-1018,P06-2095,1,0.875587,"transformation. The resulting translation may be the following: Children attend schools that are in poor repair and lacking basic essentials Thus our system supports translators in making decisions about indirect translation equivalents in a number of ways: it suggests possible structural and lexical transformations for contextual descriptors; it verifies which translation variants co-occur in the TL corpus; and it illustrates the use of the transformed TL lexical descriptors in actual contexts. 2.2 Generating translation equivalents We have generalised the method used in our previous study (Sharoff et al., 2006) for extracting equivalents for continuous multiword expressions (MWEs). Essentially, the method expands the search space for each word and its dictionary translations with entries from automatically computed thesauri, and then checks which combinations are possible in target corpora. These potential translation equivalents are then ranked by their similarity to the original query and presented to the user. The range of retrievable equivalents is now extended from a relatively limited range of two-word constructions which mirror POS categories in SL and TL to a much wider set of co-occurring l"
P07-1018,P02-1040,0,\N,Missing
P07-1018,C98-1113,0,\N,Missing
P10-1077,P97-1005,0,0.4678,"Missing"
P10-1077,O97-1002,0,0.154657,"Missing"
P10-1077,C94-2174,0,0.327327,"Missing"
P10-1077,sharoff-etal-2010-web,1,0.854367,"Missing"
P10-1077,P09-1076,0,0.197465,"Missing"
P10-1077,P94-1019,0,\N,Missing
P13-2047,J08-4004,0,0.0662236,"Missing"
P13-2047,P07-1018,1,0.897992,"Missing"
P13-2047,W11-2103,0,0.058082,"Missing"
P13-2047,W12-3102,0,0.0748328,"Missing"
P13-2047,2012.iwslt-evaluation.1,0,0.0626555,"Missing"
P13-2047,W02-1405,0,0.0637816,"Missing"
P13-2047,2001.mtsummit-papers.68,0,0.112338,"Missing"
P13-2047,W09-0441,0,0.0219405,"qual, the final ranking will preserve the initial order. Insertion sort of n sentences requires n − 1 comparisons in the best case of already sorted data and n(n−1) in 2 the worst case (reversely ordered data). Insertion sort with binary search requires ∼ n log n comparisons regardless of the initial order. For this study we ran exhaustive pairwise evaluation and used its results to simulate human-assisted sorting. In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al., 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2009), and GTM (Turian et al., 2003). We also wanted to estimate the correlations of these metrics with human judgements for the English→Russian pair on the corpus level and on the level of individual sentences. 4 Results We received results from five teams, two teams submitted two runs each, which totals seven participants’ runs (referred to as P1..P7 in the paper). The participants represent SMT, RBMT, and hybrid approaches. They included established groups from academia and industry, as well as new research teams. The evaluation runs also included the translations of the 947 test sentences produ"
P13-2047,2003.mtsummit-papers.51,0,0.0645424,"eserve the initial order. Insertion sort of n sentences requires n − 1 comparisons in the best case of already sorted data and n(n−1) in 2 the worst case (reversely ordered data). Insertion sort with binary search requires ∼ n log n comparisons regardless of the initial order. For this study we ran exhaustive pairwise evaluation and used its results to simulate human-assisted sorting. In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al., 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2009), and GTM (Turian et al., 2003). We also wanted to estimate the correlations of these metrics with human judgements for the English→Russian pair on the corpus level and on the level of individual sentences. 4 Results We received results from five teams, two teams submitted two runs each, which totals seven participants’ runs (referred to as P1..P7 in the paper). The participants represent SMT, RBMT, and hybrid approaches. They included established groups from academia and industry, as well as new research teams. The evaluation runs also included the translations of the 947 test sentences produced by four free online systems"
P13-2047,1994.amta-1.25,0,0.601581,"Missing"
P13-2047,P02-1040,0,\N,Missing
P13-2047,W05-0909,0,\N,Missing
P13-2047,C12-1014,0,\N,Missing
R19-1069,W15-2517,0,0.0675936,"Missing"
R19-1069,W06-2705,0,0.123593,"Missing"
R19-1069,W17-0237,0,0.0618958,"Missing"
rapp-etal-2012-identifying,C10-2070,0,\N,Missing
rapp-etal-2012-identifying,sharoff-etal-2008-designing,1,\N,Missing
rapp-etal-2012-identifying,W00-0901,0,\N,Missing
rapp-etal-2012-identifying,P99-1067,1,\N,Missing
rapp-etal-2012-identifying,J05-4003,0,\N,Missing
rapp-etal-2012-identifying,A00-1031,0,\N,Missing
rapp-etal-2012-identifying,P09-1017,0,\N,Missing
sharoff-2002-meaning,bateman-etal-2000-resources,1,\N,Missing
sharoff-2004-towards,rose-etal-2002-reuters,0,\N,Missing
sharoff-2006-uniform,sharoff-2004-towards,1,\N,Missing
sharoff-2006-uniform,baroni-bernardini-2004-bootcat,0,\N,Missing
sharoff-etal-2006-using,rapp-2004-freely,0,\N,Missing
sharoff-etal-2006-using,bennison-bowker-2000-designing,0,\N,Missing
sharoff-etal-2006-using,C00-2090,0,\N,Missing
sharoff-etal-2006-using,P02-1040,0,\N,Missing
sharoff-etal-2006-using,P04-1079,1,\N,Missing
sharoff-etal-2006-using,sharoff-2006-uniform,1,\N,Missing
sharoff-etal-2008-designing,J93-2004,0,\N,Missing
sharoff-etal-2008-designing,P98-1080,0,\N,Missing
sharoff-etal-2008-designing,C98-1077,0,\N,Missing
sharoff-etal-2008-designing,A00-1031,0,\N,Missing
sharoff-etal-2008-designing,gimenez-marquez-2004-svmtool,0,\N,Missing
sharoff-etal-2008-designing,feldman-etal-2006-cross,1,\N,Missing
sharoff-etal-2008-designing,erjavec-2004-multext,1,\N,Missing
sharoff-etal-2008-designing,W04-3229,1,\N,Missing
sharoff-etal-2010-web,C94-2174,0,\N,Missing
sharoff-etal-2010-web,J08-3001,0,\N,Missing
sharoff-etal-2010-web,J08-4004,0,\N,Missing
sharoff-etal-2010-web,P09-1076,0,\N,Missing
W04-0403,H92-1045,0,0.00744197,"ord expressions starting with a preposition in English have similar structure, but the difference with Russian is that there is no change in the structure of the prepositional group, unlike some English MWEs, e.g. in line, at large, which do not have a determiner. Thus, we cannot use the difference in the PP structure as an indicator of an MWE. The fact that MWEs are not fully compositional means that the meanings of their constituent words change resulting a specific idiomatic meaning of the whole contstruction. In this case we cannot accept the general assumption of one sense per discourse (Gale et al., 1992), because words such as line, large in English or kljuch in Russian can function in the same discourse in a totally different sense. However, the assumption of one sense per collocation can hold, because an MWE with a prepositional phrase typically has one and the same meaning: even though line, large or techenie are ambiguous, in line, at large and pod kljuch, v techenie have their specific meanings. 3 Methodology The study starts with the selection of the list of the most frequent prepositions to account for a large number of potential collocations. Information on the frequency of prepositio"
W04-0403,P98-1080,0,0.0238423,"Missing"
W04-0403,W03-1807,0,0.0118493,"h National Corpus (BNC) in its size and coverage. The goal of the study was to identify the list of statistically important MWEs in the corpus and to use them to reduce the ambiguity in corpus analysis. Existing research on the detection of MWEs can be positioned between two extremes: linguistic and statistical. The former approaches assume syntactic parsing of source texts (sometimes shallow, sometimes deep to identify the semantic roles of MWE components) and the ability to get information from a thesaurus. Detection results can be further improved by deep semantic analysis of source texts (Piao et al., 2003). When we apply such techniques to a Russian corpus of the size of the BNC, this means that we need accurate and robust parsing tools, which do not exist for Russian. Also, no electronic thesaurus, such as WordNet (Miller, 1990), is available for Russian. Purely statistical approaches treat multiword expressions as a bag of words and pay no attention to the possibility of variation in the inventory and order of MWE components. Given that the word order in Russian (and other Slavonic languages) is relatively free and a typical word (i.e. lemma) has many forms (typically from 9 for nouns to 50 f"
W04-0403,calzolari-etal-2002-towards,0,0.0362892,"collection and estimate the efficiency of the collected list for the resolution of morphosyntactic and semantic ambiguity in a corpus. 1 Introduction Computational research on multiword expressions (MWEs) has mostly addressed the topic for English (Sag et al., 2001). Some research has dealt with other languages, such as French (Michiels and Dufour, 1998) or Chinese (Zhang et al., 2000), but there has been no computationally tractable research on the topic for Russian. What is more, the study of MWEs in English has been mostly devoted to the description of nominal groups or light verbs, e.g. (Calzolari et al., 2002), (Sag et al., 2001), while constructions starting with a preposition, such as in line, at large, have not been the focus of attention. Even though the tradition of studying Russian idiomatic expressions resulted in many descriptions of Russian idioms and phraseological dictionaries, like (Dobrovol’skij, 2000) or (Fedorov, 1995), the studies and dictionaries often concentrate on non-decomposable colourful expressions of the ‘kick-the-bucket’ type, such as byt’ bez carja v golove (‘to have a screw loose’, lit. ‘to be without a tsar in one’s head’) and pay no attention to the very notion of thei"
W04-0403,W00-1219,0,0.0218308,"llect lists of such constructions in a corpus of 50 mln words using a simple mechanism that combines statistical methods with knowledge about the structure of Russian prepositional phrases. Then we analyse the results of this data collection and estimate the efficiency of the collected list for the resolution of morphosyntactic and semantic ambiguity in a corpus. 1 Introduction Computational research on multiword expressions (MWEs) has mostly addressed the topic for English (Sag et al., 2001). Some research has dealt with other languages, such as French (Michiels and Dufour, 1998) or Chinese (Zhang et al., 2000), but there has been no computationally tractable research on the topic for Russian. What is more, the study of MWEs in English has been mostly devoted to the description of nominal groups or light verbs, e.g. (Calzolari et al., 2002), (Sag et al., 2001), while constructions starting with a preposition, such as in line, at large, have not been the focus of attention. Even though the tradition of studying Russian idiomatic expressions resulted in many descriptions of Russian idioms and phraseological dictionaries, like (Dobrovol’skij, 2000) or (Fedorov, 1995), the studies and dictionaries often"
W04-0403,C98-1077,0,\N,Missing
W11-3603,P11-1061,0,0.0232462,"rt-of-speech tagger for Kannada exploiting the resources of Telugu. We also build large corpora and a morphological analyser (including lemmatisation) for Kannada. Our experiments reveal that a cross-language taggers are as efficient as mono-lingual taggers. We aim to extend our work to other Indian languages. Our tools are efficient and significantly faster than the existing monolingual tools. 1 2 Introduction Related Work There are several methods for building POS taggers for a target language using source language resources. Some researchers (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Das and Petrov, 2011) built POS taggers for a target language using parallel corpus. The source (cross) language is expected to have a POS tagger. First, the source language tools annotate the source side of the parallel corpora. Later these annotations are projected to the target language side using the alignments in the parallel corpora, creating virtual annotated corpora for the target language. A POS tagger for the target is then built from the virtual annotated corpora. Other methods which make use of parallel corpora are (Snyder et al., 2008; Naseem et al., 2009). These approaches are based on hierarchical B"
W11-3603,feldman-etal-2006-cross,0,0.0617409,"s and Rundell, 2008), building lexicons and morphological analysers is also possible to considerable extent. The other reason for the lack of POS taggers is partly due the lack of researchers working on a 11 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 11–19, Chiang Mai, Thailand, November 8-12, 2011. 2.2 sampling techniques. They aim to gain from information shared across languages. The main disadvantage of all such methods is that they rely on parallel corpora which itself is a costly resource for resource-poor languages. Hana et al. (2004) and Feldman et al. (2006) propose a method for developing a POS tagger for a target language using the resources of another typologically related language. Our method is motivated from them, but with the focus on resources available for Indian languages. 2.1 There exists literature on Kannada morphological analysers (Vikram and Urs, 2007; Antony et al., 2010; Shambhavi et al., 2011) and POS taggers (Antony and Soman, 2010) but none of them have any publicly downloadable resources. Murthy (2000) gives an overview of existing resources for Kannada and points out that most of these exist without public access. We are int"
W11-3603,W04-3229,0,0.760204,"in lexicography (Atkins and Rundell, 2008), building lexicons and morphological analysers is also possible to considerable extent. The other reason for the lack of POS taggers is partly due the lack of researchers working on a 11 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 11–19, Chiang Mai, Thailand, November 8-12, 2011. 2.2 sampling techniques. They aim to gain from information shared across languages. The main disadvantage of all such methods is that they rely on parallel corpora which itself is a costly resource for resource-poor languages. Hana et al. (2004) and Feldman et al. (2006) propose a method for developing a POS tagger for a target language using the resources of another typologically related language. Our method is motivated from them, but with the focus on resources available for Indian languages. 2.1 There exists literature on Kannada morphological analysers (Vikram and Urs, 2007; Antony et al., 2010; Shambhavi et al., 2011) and POS taggers (Antony and Soman, 2010) but none of them have any publicly downloadable resources. Murthy (2000) gives an overview of existing resources for Kannada and points out that most of these exist without"
W11-3603,J03-3001,0,0.0188693,"tion probabilities of Telugu by training TnT on the machine annotated corpora of Telugu. Since Telugu and Kannada are typologically related, we assume the transition probabilities of Kannada to be the same as of Telugu 3. Estimate the emission probabilities of Kannada from machine annotated Telugu corpus or machine annotate Kannada corpus 4. Use the probabilities from the step 2 and 3 to build a POS tagger for Kannada 4.1 Step1: Kannada and Telugu Corpus Creation Corpus collection once used to be long, slow and expensive. But with the advent of the Web and the success of Web-as-Corpus notion (Kilgarriff and Grefenstette, 2003), corpus collection can be highly automated, and thereby fast and inexpensive. We have used Corpus Factory method (Kilgarriff et al., 2010) to collect Kannada and Telugu corpora from the Web. The method is described in the following steps. Frequency List: Corpus Factory method requires a frequency list of the language of interest to start corpus collection. The frequency list of 3 Wikipedia Dumps: http://dumps.wikimedia. org 4 Bing: http://bing.com 14 which the ratio of non-frequent words to the highfrequent words is maintained. If a page doesn’t meet this criteria, we discard it. Near-Duplica"
W11-3603,kilgarriff-etal-2010-corpus,1,0.442272,"Missing"
W11-3603,A00-1031,0,0.047623,"d Karthik, 2007) was ranked best among all the existing taggers. Indian languages are morphologically rich with Dravidian languages posing extra challenge because of their agglutinative nature. Avinesh and Karthik (2007) noted that morphological information play an important role in Indian language POS tagging. Their CRF model is trained on all the important morphological features to predict the output tag for a word in a given context. The pipeline of (Avinesh and Karthik, 2007) can be described as below Hana et al. (2004) Hana et al. aim to develop a tagger for Russian from Czech using TnT (Brants, 2000), a second-order Markov model. Though the languages Czech and Russian are free-word order, they argue that TnT is as efficient as other models. TnT tagger is based on two probabilities - the transition and emission probabilities. The tag sequence of a given word sequence is selected by calculating argmax t1 ...tn "" n Y i=1 # P (ti |ti−1 , ti−2 )P (wi |ti ) Existing Tools for Kannada (1) where wi . . . wn is the word sequence and t1 . . . tn are their corresponding POS tags. Transition probabilities, P (ti |ti−1 , ti−2 ), describe the conditional probability of a tag given the tags of previous"
W11-3603,C08-1098,0,0.0117851,"agged corpus captures an approximation of the true transition probabilities in the manually annotated corpora. The tagged corpus is converted to the format in Figure 1 and then using TnT we estimate transition probabilities. 15 4.3.2 4.4 Source tags and target morphology We experimented with various TnT tagging models by selecting transition and emission probabilities from the Steps 2 and 3. Though one may question the performance of TnT for free-word order languages like Kannada, Hana et al. (2004) found that TnT models are as good as other models for free-word order languages. Additionally, Schmid and Laws (2008) observed that TnT models are also good at learning fined-grained transition probabilities. In our evaluation, we also found that our TnT models are competitive to the existing CRF model of (Avinesh and Karthik, 2007). Apart from building POS tagging models, we also learned the associations of each word with its lemma and suffix given a POS tag, from the machine annotated Kannada corpus. For example, Kannada word aramaneVgalYannu is associated with lemma aramaneV and suffix annu when occurred with the tag NN.n.n.pl..o and similarly word aramaneVgeV is associated with lemma aramaneV and suffix"
W11-3603,D08-1109,0,0.0144097,"Some researchers (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Das and Petrov, 2011) built POS taggers for a target language using parallel corpus. The source (cross) language is expected to have a POS tagger. First, the source language tools annotate the source side of the parallel corpora. Later these annotations are projected to the target language side using the alignments in the parallel corpora, creating virtual annotated corpora for the target language. A POS tagger for the target is then built from the virtual annotated corpora. Other methods which make use of parallel corpora are (Snyder et al., 2008; Naseem et al., 2009). These approaches are based on hierarchical Bayesian models and Markov Chain Monte Carlo Part-of-speech (POS) taggers are some of the basic tools for natural language processing in any language. For example, they are needed for terminology extraction using linguistic patterns or for selecting word lists in language teaching and lexicography. At the same time, many languages lack POS taggers. One reasons for this is the lack of other basic resources like corpora, lexicons or morphological analysers. With the advent of Web, collecting corpora is no longer a major problem ("
W11-3603,N01-1026,0,0.0125042,"build a cross-language part-of-speech tagger for Kannada exploiting the resources of Telugu. We also build large corpora and a morphological analyser (including lemmatisation) for Kannada. Our experiments reveal that a cross-language taggers are as efficient as mono-lingual taggers. We aim to extend our work to other Indian languages. Our tools are efficient and significantly faster than the existing monolingual tools. 1 2 Introduction Related Work There are several methods for building POS taggers for a target language using source language resources. Some researchers (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Das and Petrov, 2011) built POS taggers for a target language using parallel corpus. The source (cross) language is expected to have a POS tagger. First, the source language tools annotate the source side of the parallel corpora. Later these annotations are projected to the target language side using the alignments in the parallel corpora, creating virtual annotated corpora for the target language. A POS tagger for the target is then built from the virtual annotated corpora. Other methods which make use of parallel corpora are (Snyder et al., 2008; Naseem et al., 2009). These approaches are"
W11-3603,H01-1035,0,0.0315839,"paper, we show how to build a cross-language part-of-speech tagger for Kannada exploiting the resources of Telugu. We also build large corpora and a morphological analyser (including lemmatisation) for Kannada. Our experiments reveal that a cross-language taggers are as efficient as mono-lingual taggers. We aim to extend our work to other Indian languages. Our tools are efficient and significantly faster than the existing monolingual tools. 1 2 Introduction Related Work There are several methods for building POS taggers for a target language using source language resources. Some researchers (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Das and Petrov, 2011) built POS taggers for a target language using parallel corpus. The source (cross) language is expected to have a POS tagger. First, the source language tools annotate the source side of the parallel corpora. Later these annotations are projected to the target language side using the alignments in the parallel corpora, creating virtual annotated corpora for the target language. A POS tagger for the target is then built from the virtual annotated corpora. Other methods which make use of parallel corpora are (Snyder et al., 2008; Naseem et al., 200"
W12-0114,P07-1018,1,0.869022,"translation quality. (III) Extension to other languages: Structural similarity and translation by pivot languages is used to obtain extension to further languages: High-quality translation between closely related languages (e.g., Russian and Ukrainian or Portuguese and Spanish) can be achieved with relatively simple resources (using linguistic similarity, but also homomorphism assumptions with respect to parallel text, if available), while greater efforts are put into ensuring better-quality translation between more distant languages (e.g. German and Russian). According to our prior research (Babych et al., 2007b) the pipeline between languages of different similarity results in improved translation quality for a larger number of language pairs (e.g., MT from Portuguese or Ukrainian into German is easier if there are highquality analysis and transfer modules for Spanish and Russian into German (respectively). Of course, (III) draws heavily on the detailed analysis and MT systems that the industrial partner in HyghTra provides for a number of languages. In the following sections we give more details of the work currently done with regard to (I) and with regard to parts of (II): the creation of a new M"
W12-0114,2007.mtsummit-papers.5,1,0.957669,"Missing"
W12-0114,E06-1032,0,0.0769042,"Missing"
W12-0114,2001.mtsummit-papers.18,1,0.831384,"able and are unlikely to become available in the future. Also, SMT tends to disregard important classificatory knowledge (such as morphosyntactic, categorical and lexical class features), which can be provided and used relatively easily within non-statistical representations. On the other hand, advantages of RBMT are that its (grammar and lexical) rules and information are understandable by humans and can be exploited for a lot of applications outside of translation (dictionaries, text understanding, dialogue systems, etc.). The slot grammar approach used in Lingenio systems (cf. McCord 1989, Eberle 2001) is a prime example of such linguistically rich representations that can be used for a number of different applications. Fig.1 shows this by a visualization of (an excerpt of) the entry for the ambiguous German verb einstellen in the database that underlies (a) the Lingenio translation products, where it links up with corresponding set of the transfer rules, and (b) Lingenio’s dictionary product TranslateDict, which is primarily intended for human translators. 101 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 101–112, c Avign"
W12-0114,eberle-etal-2012-tool,1,0.868581,"Missing"
W12-0114,A94-1016,0,0.083521,"ation and statistical extension and training): (a) We start out with declarative analysis and generation components of the considered languages, and with basic bilingual dictionaries connecting to one another the entries of relatively small vocabularies comprising the most frequent words of each language in a given translation pair (cf. Fig 1 a). (b) Having completed this phase, we extend the dictionaries and train the analysis-, transfer- and generation-components of the rule-based core systems using monolingual and bilingual corpora. 1 A prominent early example is Frederking and colleagues (Frederking & Nirenburg, 1994). For an overview of hybrid MT till the late nineties see Streiter et al. (1999). More recent approaches include Groves & Way (2006a, 2006b). Commercial implementations include AppTek (http://www.apptek.com) and Language Weaver (http://www.languageweaver.com). An ongoing MT important project investigating hybrid methods is EuroMatrixPlus (http://www.euromatrixplus.net/) 102 (II) Error detection and improvement cycle: (a) We automatically discover the most frequent problematic grammatical constructions and multiword expressions for commercial RBMT and SMT systems using automatic construction-ba"
W12-0114,W97-0119,0,0.0461742,"Missing"
W12-0114,J93-1004,0,0.311866,"Missing"
W12-0114,2004.eamt-1.9,0,0.0664357,"Missing"
W12-0114,2006.eamt-1.15,0,0.0619246,"Missing"
W12-0114,habash-dorr-2002-handling,0,0.0417473,"Missing"
W12-0114,J06-4003,0,0.0132216,"Sentence Alignment; Melamed, 1999). For segmentation of text we use corresponding Lingenio-tools (unpublished).2 For word alignment Giza++ (Och & Ney, 2003) is the standard tool. Given a word alignment, the extraction of a (SMT) dictionary is relatively straightforward. With the exception of sentence segmentation, these algorithms are largely language independent and can be used for all of the languages that we consider. We did this for a number of language pairs on the basis of the 2 If these cannot be applied because of lack of information about a language, we intend to use the algorithm by Kiss & Strunk (2006). An open-source implementation of parts of the Kiss & Strunk algorithm is available from Patrick Tschorn at http://www.denkselbst.de/sentrick/index.html. 104 Europarl-texts considered (as stored in our database). In order to optimize the results we use the dictionaries of step 1 as set of cognates (cf. Simard at al 1992, Gough & Way 2004), as well as other words easily obtainable from the internet that can be used for this purpose (like company names and other named entities with cross-language identity and terminology translations). Using the morphology component of the new language and the"
W12-0114,2005.mtsummit-papers.11,0,0.0671921,"Missing"
W12-0114,J99-1003,0,0.113762,"Missing"
W12-0114,J05-4003,0,0.0183273,"quire parallel and comparable corpora As our parallel corpus, we use the Europarl. The size of the current version is up to 40 million words per language, and several of the languages we are currently considering are covered. Also, we make use of other parallel corpora such as the Canadian Hansards (Proceedings of the Canadian Parliament) for the English–French language pair. For non-EU Languages (mainly Russian), we intend to conduct a pilot study to establish the feasibility of retrieving parallel corpora from the web, a problem for which various approaches have been proposed (Resnik, 1999; Munteanu & Marcu, 2005; Wu & Fung, 2005). In addition to the parallel corpora, we will need large monolingual corpora in the future (at least 200 million words) for each of the six languages. Here, we intend to use newspaper corpora supplemented with text collections downloadable from the web. The corpora are stored in a database that allows for assigning analyses of different depth and nature to the sentences and for alignment between the sentences and their analyses. The architecture of this database and the corresponding analysis and evaluation frontend is described in (Eberle et al 2010, 2012). Section Results"
W12-0114,P02-1038,0,0.0217034,"Missing"
W12-0114,J03-1002,0,0.00439785,"enerationoriented representations from grammar models and statistical combinatorial properties of annotated features. Step 3: Generating dictionary extensions from parallel corpora Based on parallel corpora, dictionaries can be derived using established techniques of automatic sentence alignment and word alignment. For sentence alignment, the length-based Gale & Church aligner (1993) can be used, or – alternatively – Dan Melamed’s GSA-algorithm (Geometric Sentence Alignment; Melamed, 1999). For segmentation of text we use corresponding Lingenio-tools (unpublished).2 For word alignment Giza++ (Och & Ney, 2003) is the standard tool. Given a word alignment, the extraction of a (SMT) dictionary is relatively straightforward. With the exception of sentence segmentation, these algorithms are largely language independent and can be used for all of the languages that we consider. We did this for a number of language pairs on the basis of the 2 If these cannot be applied because of lack of information about a language, we intend to use the algorithm by Kiss & Strunk (2006). An open-source implementation of parts of the Kiss & Strunk algorithm is available from Patrick Tschorn at http://www.denkselbst.de/se"
W12-0114,P02-1040,0,0.0873178,"Missing"
W12-0114,P95-1050,1,0.218313,"Missing"
W12-0114,rapp-2004-freely,1,0.85454,"Missing"
W12-0114,P99-1068,0,0.0503705,"ps. Step 1: Acquire parallel and comparable corpora As our parallel corpus, we use the Europarl. The size of the current version is up to 40 million words per language, and several of the languages we are currently considering are covered. Also, we make use of other parallel corpora such as the Canadian Hansards (Proceedings of the Canadian Parliament) for the English–French language pair. For non-EU Languages (mainly Russian), we intend to conduct a pilot study to establish the feasibility of retrieving parallel corpora from the web, a problem for which various approaches have been proposed (Resnik, 1999; Munteanu & Marcu, 2005; Wu & Fung, 2005). In addition to the parallel corpora, we will need large monolingual corpora in the future (at least 200 million words) for each of the six languages. Here, we intend to use newspaper corpora supplemented with text collections downloadable from the web. The corpora are stored in a database that allows for assigning analyses of different depth and nature to the sentences and for alignment between the sentences and their analyses. The architecture of this database and the corresponding analysis and evaluation frontend is described in (Eberle et al 2010,"
W12-0114,C90-3044,0,0.0668576,"Missing"
W12-0114,P06-2095,1,0.899134,"f a manually compiled kernel does not show 105 an ambiguity problem of similar significance), and as experience shows that most low frequency words in a full-size lexicon tend to be unambiguous, the ambiguity problem is reduced further for the words investigated and extracted by this comparison method. Step 5: Expanding dictionaries comparable corpora (multiword units) using In order to account for technical terms, idioms, collocations, and typical short phrases, an important feature of an MT lexicon is a high coverage of multiword units. Very recent work conducted at the University of Leeds (Sharoff et al., 2006) shows that dictionary entries for such multiword units can be derived from comparable corpora if a dictionary of single words is available. It could even be shown that this methodology can be superior to deriving multiword-units from parallel corpora (Babych et al., 2007). This is a major breakthrough as comparable corpora are far easier to acquire than parallel corpora. It even opens up the possibility of building domainspecific dictionaries by using texts from different domains. The outline of the algorithm is as follows: • Extract collocations from a corpus of the source language (Smadja,"
W12-0114,sharoff-2006-uniform,1,0.886741,"in a machinetranslated corpus In a later work package of the project, we will run a large parallel corpus through available (competitive) MT engines, which will be enhanced by automatic dictionaries developed during the previous stages. On the source-language side of the corpus we will automatically generate lists of frequent multiword expressions (MWEs) and grammatical constructions using the methodology proposed in (Sharoff et al., 2006). For each of the identified MWEs and constructions we will generate a parallel concordance using open-source CSAR architecture developed by the Leeds team (Sharoff, 2006). The concordance will be generated by running queries to the sentencealigned parallel corpora and will return lists of corresponding sentences from gold-standard human translations and corresponding sentences generated by MT. Each of these concordances will be automatically evaluated using standard MT evaluation metrics, such as BLEU. Under these settings parallel concordances will be used as standard MT evaluation corpora in an automated MT evaluation scenario. Normally BLEU gives reliable results for MT corpora over 7000 words. However, in (Babych and Hartley, 2009; Babych and Hartley, 2008"
W12-0114,J93-1007,0,0.0374134,"., 2006) shows that dictionary entries for such multiword units can be derived from comparable corpora if a dictionary of single words is available. It could even be shown that this methodology can be superior to deriving multiword-units from parallel corpora (Babych et al., 2007). This is a major breakthrough as comparable corpora are far easier to acquire than parallel corpora. It even opens up the possibility of building domainspecific dictionaries by using texts from different domains. The outline of the algorithm is as follows: • Extract collocations from a corpus of the source language (Smadja, 1993) • To translate a collocation, look up all its words using any dictionary • Generate all possible permutations (sequences) of the word translations • Count the occurrence frequencies of these sequences in a corpus of the target language and test for significance • Consider the most significant sequence to be the translation of the source language collocation Of course, in later steps of the project, we will experiment on filtering these sequences by exploiting structural knowledge similarly to what was described in the two previous steps. This can be obtained on the basis of the declarative an"
W12-0114,2007.mtsummit-aptme.6,0,0.11913,"Missing"
W12-0114,I05-1023,0,\N,Missing
W12-0114,P99-1067,1,\N,Missing
W12-0114,W02-0902,0,\N,Missing
W12-0114,baroni-bernardini-2004-bootcat,0,\N,Missing
W14-1016,P07-1018,1,0.868464,"ectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words. There has been some previous work on identifying the translations of multiword units using comparable corpora, such as Robitaille et al. (2006), Babych et al. (2007), Daille & Morin (2012); Delpech et al. (2012). However, none of this work utilizes aligned comparable documents, and the underlying assumption is that the translation of a multiword unit can be determined by looking at its components individually, and by merging the results. In contrast, we try to explore whether the translation of a multiword unit can be determined solely by looking at its contextual behavior, i.e. whether it is possible to also apply the standard approach as successfully used for single words. The underlying fundamental question is whether the meaning of a multiword unit is"
W14-1016,C12-1110,0,0.045264,"Missing"
W14-1016,C12-1046,0,0.0216267,"sy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words. There has been some previous work on identifying the translations of multiword units using comparable corpora, such as Robitaille et al. (2006), Babych et al. (2007), Daille & Morin (2012); Delpech et al. (2012). However, none of this work utilizes aligned comparable documents, and the underlying assumption is that the translation of a multiword unit can be determined by looking at its components individually, and by merging the results. In contrast, we try to explore whether the translation of a multiword unit can be determined solely by looking at its contextual behavior, i.e. whether it is possible to also apply the standard approach as successfully used for single words. The underlying fundamental question is whether the meaning of a multiword unit is determined by Abstract Most previous attempts"
W14-1016,W95-0114,0,0.668044,"ith this method translation results for single words are rather good, the results for multiword units are considerably worse. This is an indication that the type of multiword expressions considered here is too infrequent to provide a sufficient amount of contextual information. Thus indirectly it is confirmed that it should make sense to look at the contextual behaviour of the components of a multiword expression individually, and to combine the results. 1 Introduction The task of identifying word translations from comparable text has received considerable attention. Some early papers include Fung (1995) and Rapp (1995). Fung (1995) utilized a context heterogeneity measure, thereby assuming that words with productive context in one language translate to words with productive context in another language, and words with rigid context translate into words with rigid context. In contrast, the underlying assumption in Rapp (1995) was that words which are translations of each other show similar co-occurrence patterns across languages. This assumption is effectively an extension of Harris' (1954) distributional hypotheses to the multilingual case. This work was further elaborated in some by now clas"
W14-1016,P98-1069,0,0.427217,"(1995) utilized a context heterogeneity measure, thereby assuming that words with productive context in one language translate to words with productive context in another language, and words with rigid context translate into words with rigid context. In contrast, the underlying assumption in Rapp (1995) was that words which are translations of each other show similar co-occurrence patterns across languages. This assumption is effectively an extension of Harris' (1954) distributional hypotheses to the multilingual case. This work was further elaborated in some by now classical papers, such as Fung & Yee (1998) 87 Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 87–95, c Gothenburg, Sweden, April 27, 2014. 2014 Association for Computational Linguistics the contextual behavior of the full unit, or by the contextual behavior of its components (or by a mix of both). But multiword expressions are of complex nature, as expressed e.g. by Moon (1998): ""there is no unified phenomenon to describe but rather a complex of features that interact in various, often untidy, ways and represent a broad continuum between non-compositional (or idiomatic) and compositional"
W14-1016,P08-1088,0,0.0860555,"matique Fondamentale F-13288 Marseille, France reinhardrapp@gmx.de Serge Sharoff University of Leeds Centre for Translation Studies Leeds, LS2 9JT, UK S.Sharoff@leeds.ac.uk and Rapp (1999). Based on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab & Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. H"
W14-1016,D09-1124,0,0.173008,"on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab & Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and wh"
W14-1016,P11-1133,0,0.0845959,"ard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab & Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words"
W14-1016,W00-0901,0,0.173899,"Missing"
W14-1016,E06-1029,0,0.290641,"rallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words. There has been some previous work on identifying the translations of multiword units using comparable corpora, such as Robitaille et al. (2006), Babych et al. (2007), Daille & Morin (2012); Delpech et al. (2012). However, none of this work utilizes aligned comparable documents, and the underlying assumption is that the translation of a multiword unit can be determined by looking at its components individually, and by merging the results. In contrast, we try to explore whether the translation of a multiword unit can be determined solely by looking at its contextual behavior, i.e. whether it is possible to also apply the standard approach as successfully used for single words. The underlying fundamental question is whether the meaning"
W14-1016,P95-1050,1,0.772987,"eille Université, Laboratoire d'Informatique Fondamentale F-13288 Marseille, France reinhardrapp@gmx.de Serge Sharoff University of Leeds Centre for Translation Studies Leeds, LS2 9JT, UK S.Sharoff@leeds.ac.uk and Rapp (1999). Based on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab & Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the appro"
W14-1016,W02-2026,0,0.0699667,".uk and Rapp (1999). Based on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab & Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to mu"
W14-1016,P99-1067,1,0.833329,"Missing"
W14-1016,rapp-etal-2012-identifying,1,0.695333,"a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab & Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words. There has been some p"
W14-1016,C98-1066,0,\N,Missing
W14-3706,asheghi-etal-2014-designing,1,0.912506,"Editorial Conversational Forum (forum) Biography (bio) Frequently Asked Questions (faq) Review Story Interview Number of web pages websites 304 264 299 244 292 231 332 330 310 280 242 201 266 184 185 288 264 299 215 209 142 116 127 69 106 190 140 179 24 154 # of pages from the same website max min med 9 1 1 9 23 15 8 12 11 11 15 8 15 38 11 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 7 1 Fleiss’s κ 0.858 0.713 0.953 0.812 0.830 0.871 0.971 0.801 0.877 0.951 0.905 0.915 0.880 0.953 0.905 Table 1: Statistics for each category illustrate source diversity and reliability of the corpus (Asheghi et al., 2014). To save space, in this paper we use the abbreviation of genre labels which are specified after the genre names. corpus: the original text and the main text corpora. First, we converted web pages to plain text by removing HTML markup using the KrdWrd tool (Steger and Stemle, 2009). This resulted in the original text corpus which contains individual web pages with all the textual elements present on them. Moreover, in order to investigate the influence of boilerplate parts (e.g. advertisements, headers, footers, template materials, navigation menus and lists of links) of the web pages on genre"
W14-3706,J08-1001,0,0.262085,"cal and structural features used in previous work, we reimplemented the following published approaches to AGI: function words (Argamon et al., 1998), part-of-speech n-grams (Santini, 2007), word unigrams (Freund et al., 2006) and character 4-grams binary representation (Sharoff et al., 2010). We also explored the discriminative power of other features such as readability features (Pitler and Nenkova, 2008), HTML tags 3 and named entity tags in genre classification (Table 3). This is the first time that some of these features such as average depth of syntax trees and entity coherence features (Barzilay and Lapata, 2008) are used for genre classification. To set a base-line, we used a list of genre names (e.g. news, editorial, interview, review) as features. We used two different feature representations: binary and normalized frequency. In the binary representation of a document, the value for each feature is either one or zero which represents the presence or the absence of each feature respectively. In the normalized fre3.3 Results and Discussion Table 4 shows the result of the different feature sets listed in the previous section on both the original text and the main text corpora. At first glance, we see"
W14-3706,D08-1020,0,0.0325347,"ti-class SVM implemented in Weka 2 with the default setting. All the experiments are carried out on both the original text and the main text corpora. 3.2 Features In order to compare the performance of different lexical and structural features used in previous work, we reimplemented the following published approaches to AGI: function words (Argamon et al., 1998), part-of-speech n-grams (Santini, 2007), word unigrams (Freund et al., 2006) and character 4-grams binary representation (Sharoff et al., 2010). We also explored the discriminative power of other features such as readability features (Pitler and Nenkova, 2008), HTML tags 3 and named entity tags in genre classification (Table 3). This is the first time that some of these features such as average depth of syntax trees and entity coherence features (Barzilay and Lapata, 2008) are used for genre classification. To set a base-line, we used a list of genre names (e.g. news, editorial, interview, review) as features. We used two different feature representations: binary and normalized frequency. In the binary representation of a document, the value for each feature is either one or zero which represents the presence or the absence of each feature respecti"
W14-3706,C94-2174,0,0.427039,"logs, search results could be more beneficial. With the aim of enhancing search engines, AGI has attracted a lot of attention (see Section 2). In this paper, we investigate two important open questions in AGI. The first question is: what set 2 Related Work There has been a considerable body of research in AGI. In previous studies on automatic genre classification of web pages, various types of features such as common words (Stamatatos et al., 2000), function words (Argamon et al., 1998), word unigrams (Freund et al., 2006), character n-grams (Kanaris and Stamatatos, 2007), part-ofspeech tags (Karlgren and Cutting, 1994) , partof-speech trigrams (Argamon et al., 1998; Santini, 2007), document statistics (e.g. average sentence length, average word length and type/token ratio) (Finn and Kushmerick, 2006; Kessler et 39 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39–47, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics and McDowell, 2012), random graph walk (Lin and Cohen, 2010) and weighted-vote relational neighbour algorithm (Macskassy and Provost, 2007). These classification algorithms which utilize hyper-link connections"
W14-3706,sharoff-etal-2010-web,1,0.685924,"ushin Rezapour Asheghi Katja Markert School of Modern School of Computing L3S Research Center Languages and Cultures University of Leeds Leibniz Universit¨at Hannover University of Leeds scs5nra@leeds.ac.uk and School of Computing s.sharoff@leeds.ac.uk University of Leeds markert@l3s.de Abstract of features produces the best result in genre classification on the web? The drawbacks of existing genre-annotated web corpora (low inter-coder agreement; false correlations between topic and genre classes) resulted in researchers’ doubt on the outcomes of classification models based on these corpora (Sharoff et al., 2010). Therefore, in order to answer this question, we perform genre classification with a wide range of features on a reliable and source diverse genre-annotated web corpus. The second question that we investigate in this paper is: could we exploit the graph structure of the web to increase genre classification accuracy? With the aim of learning from the neighbouring web pages, we investigated the performance of a semi-supervised graph-based model, which is a novel technique in genre classification. The remainder of this paper is structured as follows. After reviewing related work in Section 2, we"
W14-3706,P97-1005,0,0.395836,"Missing"
W14-3706,C00-2117,0,0.0501058,"application of AGI could be in Information Retrieval. If a user could use the search engine to retrieve web pages from a specific genre such as news articles, reviews or blogs, search results could be more beneficial. With the aim of enhancing search engines, AGI has attracted a lot of attention (see Section 2). In this paper, we investigate two important open questions in AGI. The first question is: what set 2 Related Work There has been a considerable body of research in AGI. In previous studies on automatic genre classification of web pages, various types of features such as common words (Stamatatos et al., 2000), function words (Argamon et al., 1998), word unigrams (Freund et al., 2006), character n-grams (Kanaris and Stamatatos, 2007), part-ofspeech tags (Karlgren and Cutting, 1994) , partof-speech trigrams (Argamon et al., 1998; Santini, 2007), document statistics (e.g. average sentence length, average word length and type/token ratio) (Finn and Kushmerick, 2006; Kessler et 39 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39–47, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics and McDowell, 2012), random graph"
W14-3706,P03-1054,0,0.00997698,"that feature which is normalized by the length of the document. For extracting lexical features, we tokenized each document using the Stanford tokenizer (included as part of the Stanford part of speech tagger (Toutanova et al., 2003)) and converted all the tokens to lower case. For extracting POS tags and named entity tags, we used the Stanford maximum entropy tagger 4 and the Stanford Named Entity Recognizer 5 respectively. For extracting some of the readability features such as average parse tree height and average number of noun and verb phrases per sentences, we used the Stanford Parser (Klein and Manning, 2003). However, web pages must be cleaned before they can be fed to a parser, because parsers cannot handle tables and list of links. Therefore, we only used the main text of each web page as an input to the parser. For web pages for which the justext tool produced empty files, we treated these features as missing values. Moreover, we used the Brown Coherence Toolkit 6 to construct the entity grid for each web page and computed the probability of each entity transition type. This tool needs the parsed version of the text as an input. Therefore, for web pages for which the justext tool produced empt"
W14-3706,N03-1033,0,0.00443519,"tp://code.google.com/p/justext/ Genre php com edu blog shop instruction recipe news editorial forum bio faq review story interview Number of web pages in corpora Original text Main text 304 221 264 190 299 191 244 242 292 221 231 229 332 243 330 320 310 307 280 251 242 242 201 160 266 262 184 184 185 183 quency representation of a document, the value for each feature is the frequency of that feature which is normalized by the length of the document. For extracting lexical features, we tokenized each document using the Stanford tokenizer (included as part of the Stanford part of speech tagger (Toutanova et al., 2003)) and converted all the tokens to lower case. For extracting POS tags and named entity tags, we used the Stanford maximum entropy tagger 4 and the Stanford Named Entity Recognizer 5 respectively. For extracting some of the readability features such as average parse tree height and average number of noun and verb phrases per sentences, we used the Stanford Parser (Klein and Manning, 2003). However, web pages must be cleaned before they can be fed to a parser, because parsers cannot handle tables and list of links. Therefore, we only used the main text of each web page as an input to the parser."
W14-3706,J11-2004,0,\N,Missing
W14-4811,baroni-bernardini-2004-bootcat,0,0.167707,"xtract more relevant terms for the needs of trainee interpreters. We collected and compared the annotation results from the students to select a single tool with comparatively better performance. We then invited the students to prepare for the other topic (SM) by using automatically-generated lists in the simultaneous interpreting preparation. 2.2 Corpus compilation There are two types of sources where comparable corpora are from: 1. Conference documents and relevant background documents provided by the conference organisers 2. Specialised corpora collected from the internet using WebBootCat (Baroni and Bernardini, 2004) Table 1 presents all the corpora we use in this study. FR0/SM0 has been created from a single relevant document, representing the speech that the trainee interpreters were asked to interpret from in this experiment. We also ran term extraction from this “corpus” since often a text of this length is the only source of information given to the interpreters in advance. We tried to balance the terminological difficulty for both languages, even if this was not always possible. After manual term selection, we found that FR0-Zh contains 147 terms per 591 seconds of delivery (15 terms per minute), FR"
W14-4811,baroni-etal-2008-cleaneval,1,0.777696,"ws, etc. FR2 (En & Zh) and SM2 (En & Zh) are corpora collected by Web crawling using Bootcat(Baroni and Bernardini, 2004). For instance, to produce FR2 we started with a set of ten relevant keywords in English and Chinese as shown in Table 2, then used BootCat to retrieve online resources and generate two corpora (FR2 En & Zh). All the keyword seeds are from the English speech-FR0 that the students were going to interpret from, and are therefore considered very relevant and important terms. The Chinese keywords are the translations of the English ones. Preprocessing included webpage cleaning (Baroni et al., 2008), as well as basic linguistic processing. Lemmatisation and tagging for English was done using TreeTagger (Schmid, 1994), while for Chinese we used “Segmenter”, an automatic tokenisation tool (Liang et al., 2010) followed by TreeTagger for POS tagging. Lemmatisation is needed because the keywords in a glossary are expected to be in their dictionary form. Lemmatisation also helps in reducing the nearly identical forms, e.g., sulphide deposit(s). However, lemmatisation also leads to imperfect terms, e.g., recognise type of marine resource, while the plurals and participles should be expected in"
W14-4811,E03-1009,0,0.0459923,"s based on lexical patterns defined in terms of Part-of-Speech (POS) tags with frequency comparison against a reference corpus using specificity index (Ahmad et al., 1994), which extracts both single (SWT) and multi-word terms (MWT) outputs their lemmas, part of speech, lexical pattern, term variants (if any), etc. The most important feature of the TTC TermSuite is the fact that term candidates can be output with their corresponding term variants. Syllabs Tools (Blancafort et al., 2013) is a knowledge-poor tool, which is based on unsupervised detection of POS tags, following the procedure of (Clark, 2003), and on the Conditional Random Field framework for term extraction (Lafferty et al., 2001). Teaboat (Sharoff, 2012) does term extraction by detecting noun phrases using simple POS patterns in IMS Corpus Workbench (Christ, 1994) and by applying log-likelihood statistics (Rayson and Garside, 2000) to rank terms by their relevance to the corpus in question against the Internet reference corpora for English and Chinese (Sharoff, 2006). 3 Term extraction evaluation Fantinuoli (2006) used five categories to find the level of specialisation and well-formedness of the automatically-generated candidat"
W14-4811,W00-0901,0,0.0993019,"rn, term variants (if any), etc. The most important feature of the TTC TermSuite is the fact that term candidates can be output with their corresponding term variants. Syllabs Tools (Blancafort et al., 2013) is a knowledge-poor tool, which is based on unsupervised detection of POS tags, following the procedure of (Clark, 2003), and on the Conditional Random Field framework for term extraction (Lafferty et al., 2001). Teaboat (Sharoff, 2012) does term extraction by detecting noun phrases using simple POS patterns in IMS Corpus Workbench (Christ, 1994) and by applying log-likelihood statistics (Rayson and Garside, 2000) to rank terms by their relevance to the corpus in question against the Internet reference corpora for English and Chinese (Sharoff, 2006). 3 Term extraction evaluation Fantinuoli (2006) used five categories to find the level of specialisation and well-formedness of the automatically-generated candidate termlist: 1. specialised terms that were manually extracted by the terminologist (and are contained in the reference term list); 88 FR-TTC EN ZH 0.541 0.500 FR-Teaboat EN ZH 0.166 0.435 FR-Syllabs EN ZH 0.181 0.662 SM-Syllabs EN ZH 0.117 0.221 Table 3: Krippendorff’s α for different term lists"
W14-4811,2003.tc-1.14,0,0.0159183,"mentioned the application of corpora as potential electronic tools for the interpreters. Fantinuoli (2006) and Gorjanc (2009) discussed the functions of specific online crawling tools and explored ways to extract specialised terminology from disposable web corpora for interpreters. Our work is most closely connected to Fantinuoli’s work on evaluation of termlists obtained from Webderived corpora. However, that study relied on a single method of corpus collection and term extraction, and did not include an investigation into integration of corpus research into practice of interpreter training. Rütten (2003) suggested a conceptual software model for interpreters’ terminology management, in which termlists are expected to be extracted (semi-)automatically and then to be revised by their users, the interpreters, who can concentrate on those terms which are relevant and important to remember. However the study neither tested the functions of the term extraction tools nor further discussed interpreters’ perception on the usefulness of the automatically lists in their preparation for interpreting tasks. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers"
W14-4811,2012.tc-1.14,1,0.769678,"rence corpus using specificity index (Ahmad et al., 1994), which extracts both single (SWT) and multi-word terms (MWT) outputs their lemmas, part of speech, lexical pattern, term variants (if any), etc. The most important feature of the TTC TermSuite is the fact that term candidates can be output with their corresponding term variants. Syllabs Tools (Blancafort et al., 2013) is a knowledge-poor tool, which is based on unsupervised detection of POS tags, following the procedure of (Clark, 2003), and on the Conditional Random Field framework for term extraction (Lafferty et al., 2001). Teaboat (Sharoff, 2012) does term extraction by detecting noun phrases using simple POS patterns in IMS Corpus Workbench (Christ, 1994) and by applying log-likelihood statistics (Rayson and Garside, 2000) to rank terms by their relevance to the corpus in question against the Internet reference corpora for English and Chinese (Sharoff, 2006). 3 Term extraction evaluation Fantinuoli (2006) used five categories to find the level of specialisation and well-formedness of the automatically-generated candidate termlist: 1. specialised terms that were manually extracted by the terminologist (and are contained in the referen"
W14-4912,2012.eamt-1.60,0,0.0149703,"of ambiguity in the annotation schema and/or guidelines. In Section 2 we briefly introduce the data to which we apply the methodology described in Section 3. In Section 4 we report results. In Section 5 we mention studies related to ours and in Section 6 we draw conclusions and identify steps for future work. 2 Data We tested our methodology on the SentiML corpus (Di Bari et al., 2013) for which the annotation guidelines, as well as the original and annotated texts, are publicly available 1 . The corpus consists of 307 English sentences (6987 tokens), taken from political speeches, TED talks (Cettolo et al., 2012), and news items from the MPQA opinion corpus (Wilson, 2008). The aim of its annotation is to encapsulate opinions in pairs, by marking the role that each word takes (modifier or target). For example, in “More of you have lost your homes and even more are watching your home values plummet” there would be two pairs: modifier “lost” and target “homes”, and modifier “values” and target “plummet”. Such two pairs are called appraisal groups. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence"
W14-4912,W11-1514,0,0.0134161,"ceeding words. • Dependency-based features, representing the reference to the word on which the current token depends in the dependency tree (head) along with its lemma, POS tag and relation type (see Figure 1) (Nivre, 2005). • Number of linked modifiers, representing the number of adjectives and adverbs linked to the current word in the dependency tree. • Role, representing the predicted role (modifier or target) of the current token in conveying sentiment. The predictions are computed using fixed syntactic rules. • Gazetteer-based sentiment. We used the NRC Word-Emotion Association Lexicon (Mohammad, 2011) to represent the a-priori sentiment of each word, i.e. regardless of its context. Once the features are ready, two or more feature partitions (called views in the co-training strategy) have to be defined in order to be as orthogonal as possible (Abney, 2007). We opted for a linguisticallygrounded dichotomy: lexical features (word features, role and gazetteer-based sentiment) versus syntactic features (contextual and dependency-based features, number of linked modifiers). The training and test sets are split accordingly. At this point, machine learning classifiers are chosen. These need to be"
W14-4912,W07-1515,0,0.0874921,"Missing"
W14-4912,D08-1027,0,0.152701,"Missing"
W14-4912,J08-4004,0,\N,Missing
W15-3410,W10-2608,0,0.0257328,"Missing"
W15-3410,N13-1056,0,0.0647627,"Missing"
W15-3410,W99-0626,0,0.0685833,"t there are more sources of comparable corpora in comparison to parallel ones, the lexicon obtained from them is likely to be richer and more variable. Detection of cognates is a well-known task, which has been explored for a range of languages using different methods. The two main approaches applied to detection of the cognates are the generative and discriminative paradigms. The first one is based on detection of the edit distance between potential candidate pairs. The distance can be a simple Levenshtein distance, or a distance measure with the scores learned from an existing parallel set (Tiedemann, 1999; Mann and Yarowsky, 2001). The discriminative paradigm uses standard approaches to machine learning, which are based on (1) extracting features, e.g., character n68 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 68–73, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics cludes the effects of having constraints for the cognates to be similar in their roots and in the endings, to occur in distributionally similar contexts and to have similar frequency. 2 set the weights α = 0.6 and β = 0.4 giving more importance to the roots. We se"
W15-3410,W14-3357,0,0.0385687,"Missing"
W15-3410,tiedemann-2012-parallel,0,0.0539768,"Missing"
W15-3410,N10-1103,0,0.0373281,"Missing"
W15-3410,P07-2045,0,0.00416913,"ith a small variance. The L and L-R lists show the opposite behaviour. Preliminary Results on Comparable Corpora After we extracted the n-best lists for the Romance family comparable corpora, we applied one of the ranking models on these lists and we manually evaluated over a sample of 50 words4 . We set n to 10 for the n-best lists. We use a frequency window of 200 for the n-best list search heuristic and the domain of the comparable corpora to Wiki-titles We add the produced Wikipedia n-best lists with the L metric into a SMT training dataset for the ptes pair. We use the Moses SMT toolkit (Koehn et al., 2007) to test the augmented datasets. We compare the augmented model with a baseline both trained by using the Zoo corpus of subtitles. We use a 1-best list consisting of 100K pairs. Te dataset used for pt-es baseline is: 80K training sentences, 1K sentences for tuning and 2K sen4 The sample consists of words with a frequency between 2K and 5. 71 Lang Pairs pt-es it-es fr-es List L acc@1 acc@10 20 60 16 53 10 48 List L-R acc@1 acc@10 22 59 18 45 12 51 List L-C acc@1 acc@10 32 70 44 66 29 59 Table 2: Accuracy at 1 and at 10 results of the ML model E over a sample of 50 words on Wikipedia dumps compa"
W15-3410,N03-2016,0,0.0532344,"ency lists. We order the target side list into bins of similar frequency and for the source side we filter words that appear only once. We use the window approach given that the frequency between the corpora under study can not be directly comparable. During testing we use a wide window of ±200 bins to minimise the loss of good candidate translations. The second search space constraint heuristic is the L-C metric. This metric only compares source words with the target words upto a given n prefix. For cs , ct in L-C , we use the first four characters to compare groups of words as suggested in (Kondrak et al., 2003). Methodology The methodology for producing the list of cognates is based on the following steps: 1) Produce several lists of cognates using a family of distance measures, discussed in Section 2.1 from comparable corpora, 2) Prune the candidate lists by ranking items, this is done using a Machine Learning (ML) algorithm trained over parallel corpora for detecting the outliers, discussed in Section 2.2; The initial frequency lists for alignment are based Wikipedia dumps for the following languages: Romance (French, Italian, Spanish, Portuguese) and Slavonic (Bulgarian, Russian, Ukrainian), wher"
W15-3410,N01-1020,0,0.363762,"sources of comparable corpora in comparison to parallel ones, the lexicon obtained from them is likely to be richer and more variable. Detection of cognates is a well-known task, which has been explored for a range of languages using different methods. The two main approaches applied to detection of the cognates are the generative and discriminative paradigms. The first one is based on detection of the edit distance between potential candidate pairs. The distance can be a simple Levenshtein distance, or a distance measure with the scores learned from an existing parallel set (Tiedemann, 1999; Mann and Yarowsky, 2001). The discriminative paradigm uses standard approaches to machine learning, which are based on (1) extracting features, e.g., character n68 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 68–73, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics cludes the effects of having constraints for the cognates to be similar in their roots and in the endings, to occur in distributionally similar contexts and to have similar frequency. 2 set the weights α = 0.6 and β = 0.4 giving more importance to the roots. We set a higher weight to roots"
W15-3410,J03-1002,0,\N,Missing
W15-3411,W15-3413,0,0.0497045,"ld possibly provide more interesting measures, but this would require a baseline system which works with all the languages in question. 3.1 4 Results Overall, we have received eleven runs: one entry for Chinese (Table 2), three entries for French (Table 2), and seven for German (Table 3). 4.1 Metrics Methods used The method used by the system CCNUNLP is described in (Li and Gaussier, 2013). In essence, it uses a bilingual dictionary for converting the word feature vectors between the languages and estimating their overlap. The other systems are discussed in details in the current proceedings (Morin et al., 2015; Zafarian et al., 2015). The LINA system (Morin et al., 2015) is based on matching hapax legomena, i.e., words occurring only once. In addition to using hapax legomena, the quality of linking in one language pair, e.g., French-English, is also assessed by using information available in pages in another language pair, e.g., GermanEnglish. The AUT system (Zafarian et al., 2015) uses the most complicated setup by combining several steps. First, documents in different languages are mapped into the same space using a For each source page there exists exactly one correct linked page in the gold sta"
W15-3411,W15-3412,0,0.0703684,"more interesting measures, but this would require a baseline system which works with all the languages in question. 3.1 4 Results Overall, we have received eleven runs: one entry for Chinese (Table 2), three entries for French (Table 2), and seven for German (Table 3). 4.1 Metrics Methods used The method used by the system CCNUNLP is described in (Li and Gaussier, 2013). In essence, it uses a bilingual dictionary for converting the word feature vectors between the languages and estimating their overlap. The other systems are discussed in details in the current proceedings (Morin et al., 2015; Zafarian et al., 2015). The LINA system (Morin et al., 2015) is based on matching hapax legomena, i.e., words occurring only once. In addition to using hapax legomena, the quality of linking in one language pair, e.g., French-English, is also assessed by using information available in pages in another language pair, e.g., GermanEnglish. The AUT system (Zafarian et al., 2015) uses the most complicated setup by combining several steps. First, documents in different languages are mapped into the same space using a For each source page there exists exactly one correct linked page in the gold standard. Systems were requ"
W15-5311,J93-2001,0,0.400784,"cify the difference between phrasal coordination (e.g., coordination of extended noun phrases) and independent clause coordination, using only POS tags and a small 3.3 MD Analysis for Russian Biber’s computational tools have been used to tag lexical, grammatical, and syntactical features and to count their frequencies in each analysed text. Using large-scale dictionaries and contextdependent disambiguation algorithms, the tagger marks word classes and syntactic information. The description of the early version of the tagger is presented in (Biber, 1988), computational methods are outlined in (Biber, 1993a; Biber et al., 2007). We have developed a program in Python, which uses a morphologically parsed corpus as an input.2 2 67 https://github.com/Askinkaty/MDRus_analyser Factors PA1 PA2 PA3 PA4 PA5 PA6 PA1 PA2 1.00 -0.01 -0.16 0.3 0.49 0.17 PA3 -0.01 1.00 -0.28 0.13 0.17 -0.03 PA4 -0.16 -0.28 1.00 -0.53 -0.43 -0.22 PA5 0.30 0.13 -0.53 1.00 0.46 0.48 PA6 0.49 0.17 -0.43 0.46 1.00 0.20 0.17 -0.03 -0.22 0.48 0.20 1.00 Table 3: Inter-factor correlation. PA4 PA1 PA2 PA3 PA5 PA6 Proportion Variance 0.08 0.09 0.06 0.06 0.05 0.04 Cumulative Variance 0.08 0.17 0.23 0.29 0.34 0.38 Proportion Explained 0."
W15-5311,C08-1098,0,\N,Missing
W15-5311,J00-4001,0,\N,Missing
W15-5710,W14-3338,0,0.0208042,"t al. (2013) develop the standard baseline framework for QE based on features that attempt to quantify the complexity of a segment to be translated. Other previous works extend the baseline framework by adding complex features between the source and machine translations. For example, syntax information of tree labels counts (Avramidis, 2014), information to quantify the acts of translation between any two datasets with respect to a reference in the same domain (Bicici and Way, 2014) and word alignment, word posterior probabilities and diversity scores features (Camargo de Souza et al., 2014). Beck et al. (2014) use multi-task learning techniques to improve QE by sharing information among different domains. However, the QE task is only applied to certain language pairs. On the other hand, de Souza et al. (2015) integrate QE into a CAT tool with online learning to constantly train the quality prediction model. This method can be used to extract QE training data or prediction models for several domains and languages. Transfer learning aims to transfer information learned in one or more source tasks (e.g. labelled dataset) and use it to improve learning in a related target task (e.g. unlabelled dataset)"
W15-5710,W14-3339,0,0.0141099,"hine translations. These sources of information are used as features to train a supervised ML algorithm to predict QE scores. Specia et al. (2013) develop the standard baseline framework for QE based on features that attempt to quantify the complexity of a segment to be translated. Other previous works extend the baseline framework by adding complex features between the source and machine translations. For example, syntax information of tree labels counts (Avramidis, 2014), information to quantify the acts of translation between any two datasets with respect to a reference in the same domain (Bicici and Way, 2014) and word alignment, word posterior probabilities and diversity scores features (Camargo de Souza et al., 2014). Beck et al. (2014) use multi-task learning techniques to improve QE by sharing information among different domains. However, the QE task is only applied to certain language pairs. On the other hand, de Souza et al. (2015) integrate QE into a CAT tool with online learning to constantly train the quality prediction model. This method can be used to extract QE training data or prediction models for several domains and languages. Transfer learning aims to transfer information learned in"
W15-5710,D08-1078,0,0.0238522,"e and target languages for evaluation. Turchi and Negri (2014) propose an automatic approach to produce training data for QE and tackle the problem of scarce training resources. The approach is based on features across the MT output, the post edited version and the human reference translation. The method produces a classifier for binary estimation by exploiting the characteristics of good translations and their relation with the post-editing process. The produced data is labelled with a binary quality score (i.e. god or bad translation) to overcome biases on the annotation. On the other hand, Birch et al. (2008) propose a large scale study on the performance of 110 European language pairs over Europarl. The study is based on the measuring the contribution of different features between language pairs that improve or are irrelevant to the performance of an MT system. The features consist of complexity indicators of morphology, language relatedness given word similarity, number of reordering between language pairs and number of reorderings over alignments. Overall, closely related languages showed the best potential for SMT. However, this study is mainly based on standard automatic evaluation metrics su"
W15-5710,W14-3302,0,0.0488685,"Missing"
W15-5710,W14-3340,0,0.0487689,"Missing"
W15-5710,P15-1022,0,0.0354128,"Missing"
W15-5710,P07-2045,0,0.0064734,"nd 3-857. Our objective is to score sentence-level QE for related languages for the en-target translation direction, where we vary the target language. The unlabelled data consist of subtitles from the Zoo corpus. Zoo is a proprietary corpus of subtitles produced by professional translators. We split the Zoo corpus into unlabelled training xu and testing for each one of the pairs: en-es, en-pt and en-it. We also test the pair en-de Europarl given that labelled data is available with 600 sentences for testing, as well as, a correspondent out-of-domain data with 297 sentences. We use the Moses (Koehn et al., 2007) toolkit with a phrase-based baseline to extract the QE features for the xl , xu , and testing. The Zoo dataset used for the SMT baseline is: 80K training sentences, 1K sentences for tuning and 2K sentences for testing. We use the Zoo test 2K sentences for testing our proposed methods. We use fast-align5 , KenLM6 with a 3-gram language model and Moses with the standard feature set. In addition, we run a small QE manual evaluation over a random sample of 100 sentences from the Zoo test dataset (original 2K sentences) for the pairs: en-es, en-pt and en-it. The annotation is performed by one prof"
W15-5710,P02-1040,0,0.108022,"es for Quality Estimation across related languages by using different transfer learning methods. The transfer learning methods are: Transductive SVM, Label Propagation and Self-taught Learning. We use transfer learning methods on the available labelled datasets, e.g. en-es, to produce a range of Quality Estimation models for Romance languages, while also adapting for subtitling as a new domain. The Self-taught Learning method shows the most promising results among the used techniques. 1 Introduction A common problem with automatic metrics for Machine Translation (MT) evaluation, such as BLEU (Papineni et al., 2002), is the need to have reference human translations. Also such metrics work best on a corpus of sentences, while they are not informative for evaluation of individual sentences (Specia et al., 2009). The aim of Quality Estimation (QE) is to predict a quality score for sentences output by MT without reference translations, for example, to judge whether they provide a suitable basis for PostEditing by the human translator or it is better to ask the human to translate this sentence from scratch. The QE task can be framed as a classification or a regression problem, where most of the methods for QE"
W15-5710,2009.eamt-1.5,0,0.0299364,"e transfer learning methods on the available labelled datasets, e.g. en-es, to produce a range of Quality Estimation models for Romance languages, while also adapting for subtitling as a new domain. The Self-taught Learning method shows the most promising results among the used techniques. 1 Introduction A common problem with automatic metrics for Machine Translation (MT) evaluation, such as BLEU (Papineni et al., 2002), is the need to have reference human translations. Also such metrics work best on a corpus of sentences, while they are not informative for evaluation of individual sentences (Specia et al., 2009). The aim of Quality Estimation (QE) is to predict a quality score for sentences output by MT without reference translations, for example, to judge whether they provide a suitable basis for PostEditing by the human translator or it is better to ask the human to translate this sentence from scratch. The QE task can be framed as a classification or a regression problem, where most of the methods for QE rely on supervised Machine Learning (ML) algorithms. The WMT evaluation campaigns (Bojar et al., 2014) goal is to create a framework to test the performance of participating systems for the QE tas"
W15-5710,P13-4014,0,0.0917561,"Missing"
W15-5710,turchi-negri-2014-automatic,0,0.0115666,"evaluation campaigns (Bojar et al., 2014) goal is to create a framework to test the performance of participating systems for the QE task. The WMT organizers provide the datasets for training and testing new proposed automatic QE approaches. However, the existing training data is only available for a limited number of languages. For example, in the WTM 2014 the available pairs were en-es and en-de (throughout the paper we will be using the two-letter ISO codes to indicate the languages). Most of the final MT users and projects need a wider variety of source and target languages for evaluation. Turchi and Negri (2014) propose an automatic approach to produce training data for QE and tackle the problem of scarce training resources. The approach is based on features across the MT output, the post edited version and the human reference translation. The method produces a classifier for binary estimation by exploiting the characteristics of good translations and their relation with the post-editing process. The produced data is labelled with a binary quality score (i.e. god or bad translation) to overcome biases on the annotation. On the other hand, Birch et al. (2008) propose a large scale study on the perform"
W15-5710,N15-1069,0,0.0288809,"and 2 from the training dataset. In order to tackle the unbalance labelled data, we use a simple heuristic of selecting the missing 3 class instances, where the Levenshtein distance between the available reference translations and MT outputs is over a certain threshold. The examples are tagged as 3 and added into the labelled training data. For the en-pt pair the number of artificial examples is 161 with a threshold of 0.5. The accuracy result for the validation is 0.56 and the test accuracy is 0.37. The validation score shows a marginal improvement, but the heuristic hurts the test accuracy. Yang and Eisenstein (2015) use features to characterise multi-domain shift by a binary vector of which instances share a given domain. In our case the instances can share information by computing similarity between the labelled and unlabelled datasets, as well as, the use of dimensionality reduction. Table 4 shows the accuracy results on en-de Europarl (1400 instances) as the labelled training and Table 3: Accuracy results 10-fold cross validation and test dataset en-pt for unlabelled data size variation. Unlabelled data 10-fold cross validation Test size (sentences) Training 500 0.52 0.39 1K 0.54 0.37 10K 0.54 0.48 20"
W16-2611,sharoff-etal-2010-web,1,0.718748,"Missing"
W17-1401,feldman-etal-2006-cross,0,0.108444,"Missing"
W17-1401,W11-3603,1,0.846409,"ch almost reach the accuracy of the original prediction model (Table 1). 3 Transfer of Lexica Linguistic models can be also transferred through re-using grammatical models trained in a donor language with substitution of the lexicons from a recipient language. For example, a POS tagger can use the transition probabilities from the donor, Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 1–2, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics while the lexical emission probabilities can come from the recipient (Feldman et al., 2006; Reddy and Sharoff, 2011). Similarly, a traditional MT engine for translation from Ukrainian into English and German can be surpassed by a crude MT pipeline consisting of a direct word-for-word transfer model from Ukrainian into Russian followed by a better resourced model translating from Russian into English and German (Babych et al., 2007). The reason for the success of the pipeline is that the OutOf-Vocabulary rate is reduced primarily because of the better coverage of the donor lexicon. Automatic induction of translation lexica between related languages is easier than in the more general case, since in addition t"
W17-1401,D15-1290,0,0.0147955,"word embeddings even in the absence of parallel corpora (Upadhyay et al., 2016). One of the problems in transferring the lexica concerns Multi-Word Expressions (MWEs), which tend to differ even for closely related languages. In particular, this concerns fixedform MWEs without a defined grammatical structure, such as by and large or of course in English. Such MWEs need to be detected individually in each language and linked to a grammatical model in a donor language via a distributional measure of their similarity to single-word expressions, e.g., generally or definitely in the examples above (Riedl and Biemann, 2015). In my talk I have also demonstrated an end-toend example for transferring feature spaces and lexicons by developing a Named Entity Recognition tagger, which starts with resources available for Slovene and transfers the features derived from a CRF model (Lafferty et al., 2001; Benikova et al., ) to other Slavic languages. References Bogdan Babych, Anthony Hartley, and Serge Sharoff. 2007. Translating from under-resourced languages: comparing direct transfer against pivot translation. In Proceedings of MT Summit XI, pages 412–418, Copenhagen. Darina Benikova, Seid Muhie Yimam, Prabhakaran Sant"
W17-1401,P16-1157,0,0.0229853,"d German (Babych et al., 2007). The reason for the success of the pipeline is that the OutOf-Vocabulary rate is reduced primarily because of the better coverage of the donor lexicon. Automatic induction of translation lexica between related languages is easier than in the more general case, since in addition to the similarity of the embedding vectors, they often have very similar forms. A reliable lexicon can be produced by combining detection of cognate forms via Levenshtein distance with assessment of semantic similarity via bilingual word embeddings even in the absence of parallel corpora (Upadhyay et al., 2016). One of the problems in transferring the lexica concerns Multi-Word Expressions (MWEs), which tend to differ even for closely related languages. In particular, this concerns fixedform MWEs without a defined grammatical structure, such as by and large or of course in English. Such MWEs need to be detected individually in each language and linked to a grammatical model in a donor language via a distributional measure of their similarity to single-word expressions, e.g., generally or definitely in the examples above (Riedl and Biemann, 2015). In my talk I have also demonstrated an end-toend exam"
W17-1401,L16-1262,0,\N,Missing
W17-2512,W17-2511,0,0.141723,"ts for a given language pair were generated with the same process and parameters, they received very similar numbers of parallel sentence pairs. This process was applied to five languages (Chinese (zh), English (en), French (fr), German (de), Russian (ru)) to produce four bilingual datasets, each split into sample, training, and test data. Table 1 shows the statistics of the resulting datasets. 3 4 Participants and systems About 17 teams downloaded datasets, among which four teams submitted runs: VIC (Spain) (Azpeitia et al., 2017), RALI (Canada) (Gr´egoire and Langlais, 2017), JUNLP (India) (Mahata et al., 2017), and LIMSI (France: ‘zNLP’) (Zhang and Zweigenbaum, 2017). Table 2 gives more detail about teams and runs. All systems had to include a way to cope with the bilingual dimension of the task. This was addressed with pre-existing dictionaries (LIMSI), machine translation systems (JUNLP, LIMSI), word alignments obtained from parallel corpora (VIC), or bilingual word embeddings trained from parallel corpora (RALI). Cross-language sentence similarity was then handled by Cosine similarity (JUNLP, LIMSI, RALI) or the Jaccard coefficient (VIC), possibly with weighting (a function of frequency: VIC; tf"
W17-2512,W09-3109,0,0.0709534,"Missing"
W17-2512,N04-1034,0,0.216543,"detection of comparable documents across languages. The Second BUCC Shared Task,1 presented here, addresses the detection of parallel sentences across languages in nonaligned, monolingual corpora. Let us recall the overall goals, design and principles of this task, which were introduced in (Zweigenbaum et al., 2016). A bottleneck in statistical machine translation is the scarceness of parallel resources for many language pairs and domains. Previous research has shown that this bottleneck can be reduced by utilizing parallel portions found within comparable corpora (Utiyama and Isahara, 2003; Munteanu et al., 2004; AbdulRauf and Schwenk, 2009). These are useful for many purposes, including automatic terminology extraction and the training of statistical MT systems. However, past work relied on metainformation, such as the publication date of news articles or inter-language links in Wikipedia documents, to help select promising sentence pairs before examining them more thoroughly. It is therefore difficult to separate the heuristic part of the methods that deals with this meta-information in clever ways from the cross-language part of the methods that deals with translation and comparability issues. We"
W17-2512,S16-1081,0,0.426211,"llel sentence spotting task. 1 Introduction Shared tasks and the associated datasets have proved their worth as a driving force in a number of subfields of Natural Language Processing. However, very few shared tasks were organized on the topic of comparable corpora. Therefore, we endeavored to design and organize shared tasks as companions of the BUCC workshop se1 https://comparable.limsi.fr/bucc2017/ bucc2017-task.html 60 Proceedings of the 10th Workshop on Building and Using Comparable Corpora, pages 60–67, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics (Agirre et al., 2016), and WMT’s bilingual document alignment (Buck and Koehn, 2016). The present paper reports the actual organization of the task as a companion to the BUCC 2017 workshop. We describe the final method we used to prepare bilingual corpora in four language pairs: Chinese-English, French-English, GermanEnglish, and Russian-English (Section 2), the evaluation method (Section 3), the participants’ systems (Section 4), the results they obtained (Section 5), and conclude (Section 6). 2 formed the actual insertion after all parallel sentence pairs were thus processed. Additionally, a different distributi"
W17-2512,W17-2508,0,0.183214,"nsertion were then performed on each split separately. Since the training and test sets for a given language pair were generated with the same process and parameters, they received very similar numbers of parallel sentence pairs. This process was applied to five languages (Chinese (zh), English (en), French (fr), German (de), Russian (ru)) to produce four bilingual datasets, each split into sample, training, and test data. Table 1 shows the statistics of the resulting datasets. 3 4 Participants and systems About 17 teams downloaded datasets, among which four teams submitted runs: VIC (Spain) (Azpeitia et al., 2017), RALI (Canada) (Gr´egoire and Langlais, 2017), JUNLP (India) (Mahata et al., 2017), and LIMSI (France: ‘zNLP’) (Zhang and Zweigenbaum, 2017). Table 2 gives more detail about teams and runs. All systems had to include a way to cope with the bilingual dimension of the task. This was addressed with pre-existing dictionaries (LIMSI), machine translation systems (JUNLP, LIMSI), word alignments obtained from parallel corpora (VIC), or bilingual word embeddings trained from parallel corpora (RALI). Cross-language sentence similarity was then handled by Cosine similarity (JUNLP, LIMSI, RALI) or the J"
W17-2512,W15-3411,1,0.785307,"Missing"
W17-2512,P16-1189,0,0.0982588,"Missing"
W17-2512,P03-1010,0,0.129546,"f et al., 2015) tackled the detection of comparable documents across languages. The Second BUCC Shared Task,1 presented here, addresses the detection of parallel sentences across languages in nonaligned, monolingual corpora. Let us recall the overall goals, design and principles of this task, which were introduced in (Zweigenbaum et al., 2016). A bottleneck in statistical machine translation is the scarceness of parallel resources for many language pairs and domains. Previous research has shown that this bottleneck can be reduced by utilizing parallel portions found within comparable corpora (Utiyama and Isahara, 2003; Munteanu et al., 2004; AbdulRauf and Schwenk, 2009). These are useful for many purposes, including automatic terminology extraction and the training of statistical MT systems. However, past work relied on metainformation, such as the publication date of news articles or inter-language links in Wikipedia documents, to help select promising sentence pairs before examining them more thoroughly. It is therefore difficult to separate the heuristic part of the methods that deals with this meta-information in clever ways from the cross-language part of the methods that deals with translation and co"
W17-2512,W17-2510,1,0.72964,"he same process and parameters, they received very similar numbers of parallel sentence pairs. This process was applied to five languages (Chinese (zh), English (en), French (fr), German (de), Russian (ru)) to produce four bilingual datasets, each split into sample, training, and test data. Table 1 shows the statistics of the resulting datasets. 3 4 Participants and systems About 17 teams downloaded datasets, among which four teams submitted runs: VIC (Spain) (Azpeitia et al., 2017), RALI (Canada) (Gr´egoire and Langlais, 2017), JUNLP (India) (Mahata et al., 2017), and LIMSI (France: ‘zNLP’) (Zhang and Zweigenbaum, 2017). Table 2 gives more detail about teams and runs. All systems had to include a way to cope with the bilingual dimension of the task. This was addressed with pre-existing dictionaries (LIMSI), machine translation systems (JUNLP, LIMSI), word alignments obtained from parallel corpora (VIC), or bilingual word embeddings trained from parallel corpora (RALI). Cross-language sentence similarity was then handled by Cosine similarity (JUNLP, LIMSI, RALI) or the Jaccard coefficient (VIC), possibly with weighting (a function of frequency: VIC; tf.idf: LIMSI) and with a trained classifier (RALI, LIMSI)."
W17-2512,W17-2509,0,\N,Missing
W17-2512,W16-2347,0,\N,Missing
Y10-1089,baroni-bernardini-2004-bootcat,0,0.0396684,"ompetence can be achieved only once an adequate discipline-specific vocabulary has been acquired. Thus, a prime application of such corpora for both language learning and translation is the extraction of terminology – single- and multiword terms – to create vocabulary lists of frequent words and collocations. Single-word terms are 773 774 Workshop on Advanced Corpus Solutions Figure 3: Comparison of frequencies for 情报 and 信息. detected by loglikelihood scores for their frequencies against a reference corpus, while for multiword terms we use an adaptation of the commonly-used Bootcat algorithm (Baroni and Bernardini, 2004). The corpora and derived word lists have provided a basis for teaching business Russian to British students. 4.4 Affix-based searches Initially the affix function was developed with learners of Russian in mind and was intended for verbal prefixes alone. However, it has a much broader application in teaching and research and is relevant to other languages, including English, German, Chinese and Japanese. The function now works also for suffix-based searches and is applicable to parts of speech other than verbs. Users may either enter a word and search for the prefixes or suffixes that are used"
Y10-1089,W08-0909,0,0.0139766,"entic texts of various genres and therefore no attention has gone into selecting texts of an “appropriate” level. Consequently, students at the beginner and (lower-)intermediate level have been highly restricted in what they can use corpora for. Prior research in grading texts by their difficulty relied on assessing it from the viewpoint of native speakers of English, normally in the context of US schools or the US army (DuBay, 2004). In recent years there have been attempts to address the needs of learners of English as a foreign language, e.g., (Kotani et al., 2008; Kilgarriff et al., 2008; Heilman et al., 2008), but some of these studies relied on the availability of syntactic parsers or WordNet, and none of them addressed the needs of learners of other languages. In prior research (Sharoff et al., 2008) we established parameters for assessing the difficulty of texts and individual sentences in several languages (English, Chinese, German and Russian) by comparing the parameters associated with texts judged to be more or less difficult by language tutors for these languages. These parameters were detected by using a Principal Component Analysis (PCA) transform of a large set of features, which were e"
Y10-1089,sharoff-2006-uniform,1,0.911297,"in detail how we have implemented the following functions: • • • • • searches using metadata and statistics for metadata; automatic genre identification; advanced definition of shallow patterns; operations with frequency lists, including affix-based searches; classification of concordance lines according to their level of difficulty and appropriateness for language learners. We have tested the tools with a range of corpora and languages, including representative webderived corpora for Arabic, Chinese, English, French, German, Greek, Italian, Japanese, Polish, Portuguese, Russian and Spanish (Sharoff, 2006a), as well more specific collections, such as newswire or business corpora. In this paper, we present examples from Chinese, Japanese and Russian. 2 Outline of the project IntelliText was conceived in order to allow humanities researchers, including those with little or no experience of working with electronic corpora, to make use of advanced methods of text collection and analysis. Rather than producing a new product we are developing our existing tools by enhancing the range of their functions and their usability. We are doing so by liaising with humanities researchers who represent several"
Y10-1089,sharoff-etal-2010-web,1,0.751656,"traditional corpora, such as the BNC, contain fairly extensive annotation of their texts according to domains, audience types and genres (Lee, 2001). This information is normally not available for corpora collected from the Web. Even in traditional corpora, important genre or register distinctions may not be made at all or may be made in incompatible ways, rendering it impossible to show, for example, the difference between expressions of requests or suggestions in English and Japanese (e.g., -ましょ、-ませんか) in a given register. We rely here on our current work on automatic genre classification (Sharoff et al., 2010), which can achieve reasonable accuracy provided that we have a manually annotated, topically diverse training sample of approximately 20-30 documents per genre or stylistic class on which to train the probabilistic classifier. The training documents within each genre need to cover a variety of topics. If this is not the case, even if a probabilistic classifier can achieve high accuracy in genre identification on the training set, it is more likely to be able discriminate between topics rather than genres in the rest of the corpus. This facility has been incorporated to display elaborate styli"
