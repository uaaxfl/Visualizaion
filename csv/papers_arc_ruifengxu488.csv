2021.semeval-1.63,{HITSZ}-{HLT} at {S}em{E}val-2021 Task 5: Ensemble Sequence Labeling and Span Boundary Detection for Toxic Span Detection,2021,-1,-1,8,0,1809,qinglin zhu,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"This paper presents the winning system that participated in SemEval-2021 Task 5: Toxic Spans Detection. This task aims to locate those spans that attribute to the text{'}s toxicity within a text, which is crucial for semi-automated moderation in online discussions. We formalize this task as the Sequence Labeling (SL) problem and the Span Boundary Detection (SBD) problem separately and employ three state-of-the-art models. Next, we integrate predictions of these models to produce a more credible and complement result. Our system achieves a char-level score of 70.83{\%}, ranking 1/91. In addition, we also explore the lexicon-based method, which is strongly interpretable and flexible in practice."
2021.findings-emnlp.70,Improving Empathetic Response Generation by Recognizing Emotion Cause in Conversations,2021,-1,-1,7,0,6568,jun gao,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Current approaches to empathetic response generation focus on learning a model to predict an emotion label and generate a response based on this label and have achieved promising results. However, the emotion cause, an essential factor for empathetic responding, is ignored. The emotion cause is a stimulus for human emotions. Recognizing the emotion cause is helpful to better understand human emotions so as to generate more empathetic responses. To this end, we propose a novel framework that improves empathetic response generation by recognizing emotion cause in conversations. Specifically, an emotion reasoner is designed to predict a context emotion label and a sequence of emotion cause-oriented labels, which indicate whether the word is related to the emotion cause. Then we devise both hard and soft gated attention mechanisms to incorporate the emotion cause into response generation. Experiments show that incorporating emotion cause information improves the performance of the model on both emotion recognition and response generation."
2021.findings-acl.220,{REAM}$\\sharp$: An Enhancement Approach to Reference-based Evaluation Metrics for Open-domain Dialog Generation,2021,-1,-1,3,0,6568,jun gao,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.19,Beta Distribution Guided Aspect-aware Graph for Aspect Category Sentiment Analysis with Affective Knowledge,2021,-1,-1,8,1,8669,bin liang,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we investigate the Aspect Category Sentiment Analysis (ACSA) task from a novel perspective by exploring a Beta Distribution guided aspect-aware graph construction based on external knowledge. That is, we are no longer entangled about how to laboriously search the sentiment clues for coarse-grained aspects from the context, but how to preferably find the words highly related to the aspects in the context and determine their importance based on the public knowledge base. In this way, the contextual sentiment clues can be explicitly tracked in ACSA for the aspects in the light of these aspect-related words. To be specific, we first regard each aspect as a pivot to derive aspect-aware words that are highly related to the aspect from external affective commonsense knowledge. Then, we employ Beta Distribution to educe the aspect-aware weight, which reflects the importance to the aspect, for each aspect-aware word. Afterward, the aspect-aware words are served as the substitutes of the coarse-grained aspect to construct graphs for leveraging the aspect-related contextual sentiment dependencies in ACSA. Experiments on 6 benchmark datasets show that our approach significantly outperforms the state-of-the-art baseline methods."
2021.emnlp-main.23,Progressive Self-Training with Discriminator for Aspect Term Extraction,2021,-1,-1,5,0,8679,qianlong wang,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Aspect term extraction aims to extract aspect terms from a review sentence that users have expressed opinions on. One of the remaining challenges for aspect term extraction resides in the lack of sufficient annotated data. While self-training is potentially an effective method to address this issue, the pseudo-labels it yields on unlabeled data could induce noise. In this paper, we use two means to alleviate the noise in the pseudo-labels. One is that inspired by the curriculum learning, we refine the conventional self-training to progressive self-training. Specifically, the base model infers pseudo-labels on a progressive subset at each iteration, where samples in the subset become harder and more numerous as the iteration proceeds. The other is that we use a discriminator to filter the noisy pseudo-labels. Experimental results on four SemEval datasets show that our model significantly outperforms the previous baselines and achieves state-of-the-art performance."
2021.emnlp-main.210,An Empirical Study on Multiple Information Sources for Zero-Shot Fine-Grained Entity Typing,2021,-1,-1,7,0,9077,yi chen,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Auxiliary information from multiple sources has been demonstrated to be effective in zero-shot fine-grained entity typing (ZFET). However, there lacks a comprehensive understanding about how to make better use of the existing information sources and how they affect the performance of ZFET. In this paper, we empirically study three kinds of auxiliary information: context consistency, type hierarchy and background knowledge (e.g., prototypes and descriptions) of types, and propose a multi-source fusion model (MSF) targeting these sources. The performance obtains up to 11.42{\%} and 22.84{\%} absolute gains over state-of-the-art baselines on BBN and Wiki respectively with regard to macro F1 scores. More importantly, we further discuss the characteristics, merits and demerits of each information source and provide an intuitive understanding of the complementarity among them."
2021.emnlp-main.319,Argument Pair Extraction with Mutual Guidance and Inter-sentence Relation Graph,2021,-1,-1,6,1,9361,jianzhu bao,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Argument pair extraction (APE) aims to extract interactive argument pairs from two passages of a discussion. Previous work studied this task in the context of peer review and rebuttal, and decomposed it into a sequence labeling task and a sentence relation classification task. However, despite the promising performance, such an approach obtains the argument pairs implicitly by the two decomposed tasks, lacking explicitly modeling of the argument-level interactions between argument pairs. In this paper, we tackle the APE task by a mutual guidance framework, which could utilize the information of an argument in one passage to guide the identification of arguments that can form pairs with it in another passage. In this manner, two passages can mutually guide each other in the process of APE. Furthermore, we propose an inter-sentence relation graph to effectively model the inter-relations between two sentences and thus facilitates the extraction of argument pairs. Our proposed method can better represent the holistic argument-level semantics and thus explicitly capture the complex correlations between argument pairs. Experimental results show that our approach significantly outperforms the current state-of-the-art model."
2021.acl-short.66,"Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking",2021,-1,-1,5,0,12559,binzong geng,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"This ability to learn consecutive tasks without forgetting how to perform previously trained problems is essential for developing an online dialogue system. This paper proposes an effective continual learning method for the task-oriented dialogue system with iterative network pruning, expanding, and masking (TPEM), which preserves performance on previously encountered tasks while accelerating learning progress on subsequent tasks. Specifically, TPEM (i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts network expanding to create free weights for new tasks, and (iii) introduces task-specific network masking to alleviate the negative impact of fixed weights of old tasks on new tasks. We conduct extensive experiments on seven different tasks from three benchmark datasets and show empirically that TPEM leads to significantly improved results over the strong competitors."
2021.acl-long.497,A Neural Transition-based Model for Argumentation Mining,2021,-1,-1,6,1,9361,jianzhu bao,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"The goal of argumentation mining is to automatically extract argumentation structures from argumentative texts. Most existing methods determine argumentative relations by exhaustively enumerating all possible pairs of argument components, which suffer from low efficiency and class imbalance. Moreover, due to the complex nature of argumentation, there is, so far, no universal method that can address both tree and non-tree structured argumentation. Towards these issues, we propose a neural transition-based model for argumentation mining, which incrementally builds an argumentation graph by generating a sequence of actions, avoiding inefficient enumeration operations. Furthermore, our model can handle both tree and non-tree structured argumentation without introducing any structural constraints. Experimental results show that our model achieves the best performance on two public datasets of different structures."
2020.lrec-1.619,The Design and Construction of a {C}hinese Sarcasm Dataset,2020,-1,-1,5,0,17892,xiaochang gong,Proceedings of the 12th Language Resources and Evaluation Conference,0,"As a typical multi-layered semi-conscious language phenomenon, sarcasm is widely existed in social media text for enhancing the emotion expression. Thus, the detection and processing of sarcasm is important to social media analysis.However, most existing sarcasm dataset are in English and there is still a lack of authoritative Chinese sarcasm dataset. In this paper, we presents the design and construction of a largest high-quality Chinese sarcasm dataset, which contains 2,486 manual annotated sarcastic texts and 89,296 non-sarcastic texts. Furthermore, a balanced dataset through elaborately sampling the same amount non-sarcastic texts for training sarcasm classifier. Using the dataset as the benchmark, some sarcasm classification methods are evaluated."
2020.lrec-1.620,Target-based Sentiment Annotation in {C}hinese Financial News,2020,-1,-1,7,0,17894,chaofa yuan,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper presents the design and construction of a large-scale target-based sentiment annotation corpus on Chinese financial news text. Different from the most existing paragraph/document-based annotation corpus, in this study, target-based fine-grained sentiment annotation is performed. The companies, brands and other financial entities are regarded as the targets. The clause reflecting the profitability, loss or other business status of financial entities is regarded as the sentiment expression for determining the polarity. Based on high quality annotation guideline and effective quality control strategy, a corpus with 8,314 target-level sentiment annotation is constructed on 6,336 paragraphs from Chinese financial news text. Based on this corpus, several state-of-the-art sentiment analysis models are evaluated."
2020.emnlp-main.242,{BERT}-{EMD}: Many-to-Many Layer Mapping for {BERT} Compression with Earth Mover{'}s Distance,2020,-1,-1,4,0,20274,jianquan li,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks. However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices. In this paper, we propose a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers. In this way, our model can learn from different teacher layers adaptively for different NLP tasks. In addition, we leverage Earth Mover{'}s Distance (EMD) to compute the minimum cumulative cost that must be paid to transform knowledge from teacher network to student network. EMD enables effective matching for the many-to-many layer mapping. Furthermore, we propose a cost attention mechanism to learn the layer weights used in EMD automatically, which is supposed to further improve the model{'}s performance and accelerate convergence time. Extensive experiments on GLUE benchmark demonstrate that our model achieves competitive performance compared to strong competitors in terms of both accuracy and model compression"
2020.emnlp-main.281,Amalgamating Knowledge from Two Teachers for Task-oriented Dialogue System with Adversarial Training,2020,-1,-1,6,0,11211,wanwei he,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention. In this paper, we propose a {``}Two-Teacher One-Student{''} learning framework (TTOS) for task-oriented dialogue, with the goal of retrieving accurate KB entities and generating human-like responses simultaneously. TTOS amalgamates knowledge from two teacher networks that together provide comprehensive guidance to build a high-quality task-oriented dialogue system (student network). Each teacher network is trained via reinforcement learning with a goal-specific reward, which can be viewed as an expert towards the goal and transfers the professional characteristic to the student network. Instead of adopting the classic student-teacher learning of forcing the output of a student network to exactly mimic the soft targets produced by the teacher networks, we introduce two discriminators as in generative adversarial network (GAN) to transfer knowledge from two teachers to the student. The usage of discriminators relaxes the rigid coupling between the student and teachers. Extensive experiments on two benchmark datasets (i.e., CamRest and In-Car Assistant) demonstrate that TTOS significantly outperforms baseline methods."
2020.emnlp-main.289,Emotion-Cause Pair Extraction as Sequence Labeling Based on A Novel Tagging Scheme,2020,-1,-1,4,0,17894,chaofa yuan,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"The task of emotion-cause pair extraction deals with finding all emotions and the corresponding causes in unannotated emotion texts. Most recent studies are based on the likelihood of Cartesian product among all clause candidates, resulting in a high computational cost. Targeting this issue, we regard the task as a sequence labeling problem and propose a novel tagging scheme with coding the distance between linked components into the tags, so that emotions and the corresponding causes can be extracted simultaneously. Accordingly, an end-to-end model is presented to process the input texts from left to right, always with linear time complexity, leading to a speed up. Experimental results show that our proposed model achieves the best performance, outperforming the state-of-the-art method by 2.26{\%} (p{\textless}0.001) in F1 measure."
2020.coling-main.13,Jointly Learning Aspect-Focused and Inter-Aspect Relations with Graph Convolutional Networks for Aspect Sentiment Analysis,2020,-1,-1,5,1,8669,bin liang,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this paper, we explore a novel solution of constructing a heterogeneous graph for each instance by leveraging aspect-focused and inter-aspect contextual dependencies for the specific aspect and propose an Interactive Graph Convolutional Networks (InterGCN) model for aspect sentiment analysis. Specifically, an ordinary dependency graph is first constructed for each sentence over the dependency tree. Then we refine the graph by considering the syntactical dependencies between contextual words and aspect-specific words to derive the aspect-focused graph. Subsequently, the aspect-focused graph and the corresponding embedding matrix are fed into the aspect-focused GCN to capture the key aspect and contextual words. Besides, to interactively extract the inter-aspect relations for the specific aspect, an inter-aspect GCN is adopted to model the representations learned by aspect-focused GCN based on the inter-aspect graph which is constructed by the relative dependencies between the aspect words and other aspects. Hence, the model can be aware of the significant contextual and aspect words when interactively learning the sentiment features for a specific aspect. Experimental results on four benchmark datasets illustrate that our proposed model outperforms state-of-the-art methods and substantially boosts the performance in comparison with BERT."
2020.coling-main.362,Dual Dynamic Memory Network for End-to-End Multi-turn Task-oriented Dialog Systems,2020,-1,-1,6,0,21458,jian wang,Proceedings of the 28th International Conference on Computational Linguistics,0,"Existing end-to-end task-oriented dialog systems struggle to dynamically model long dialog context for interactions and effectively incorporate knowledge base (KB) information into dialog generation. To conquer these limitations, we propose a Dual Dynamic Memory Network (DDMN) for multi-turn dialog generation, which maintains two core components: dialog memory manager and KB memory manager. The dialog memory manager dynamically expands the dialog memory turn by turn and keeps track of dialog history with an updating mechanism, which encourages the model to filter irrelevant dialog history and memorize important newly coming information. The KB memory manager shares the structural KB triples throughout the whole conversation, and dynamically extracts KB information with a memory pointer at each turn. Experimental results on three benchmark datasets demonstrate that DDMN significantly outperforms the strong baselines in terms of both automatic evaluation and human evaluation. Our code is available at https://github.com/siat-nlp/DDMN."
2020.ccl-1.63,"ç»åéèé¢åæ\
æè¯å\
¸åæ³¨æåæºå¶çç»ç²åº¦æ\
æåæ(Attention-based Recurrent Network Combined with Financial Lexicon for Aspect-level Sentiment Classification)",2020,-1,-1,5,0,1809,qinglin zhu,Proceedings of the 19th Chinese National Conference on Computational Linguistics,0,"éå¯¹å¨éèé¢åå®ä½çº§æ
æåæä»»å¡ä¸­,å¾å¾ç¼ºä¹è¶³å¤çæ æ³¨è¯­æ,ä»¥åéç¨çæ
æåææ¨¡åé¾ä»¥ææå¤çéèææ¬ç­é®é¢ãæ¬ææå»ºä¸ä¸ªç¾ä¸çº§å«çéèé¢åå®ä½æ
æåæè¯­æåº,å¹¶æ æ³¨äºåä½ä¸ªéèé¢åæ
æè¯ä½ä¸ºéèé¢åæ
æè¯å
¸ãåæ¶,åºäºè¯¥éèé¢åæ°æ®é,æåºä¸ç§ç»åéèé¢åæ
æè¯å
¸åæ³¨æåæºå¶çéèææ¬ç»ç²åº¦æ
æåææ¨¡åãè¯¥æ¨¡åä½¿ç¨ä¸¤ä¸ªLSTMç½ç»åå«æåè¯çº§å«çè¯­ä¹ä¿¡æ¯ååºäºæ
æè¯å
¸åç±»åçè¯ç±»çº§å«ä¿¡æ¯,è½ææè·åéèé¢åè¯è¯­çç¹å¾ä¿¡æ¯ãæ­¤å¤,ä¸ºäºè®©ææ¬ä¸­éèé¢åæ
æè¯è·å¾æ´å¤å
³æ³¨,æåºä¸ç§åºäºéèé¢åæ
æè¯å
¸çæ³¨æåæºå¶æ¥ä¸ºä¸åå®ä½è·åéè¦çæ
æä¿¡æ¯ãæç»å¨æå»ºçéèé¢åå®ä½çº§è¯­æåºä¸è¿è¡å®éª,åå¾äºæ¯å¯¹æ¯æ¨¡åæ´å¥½çææã"
2020.ccl-1.65,åºäºå¾ªç¯äº¤äºæ³¨æåç½ç»çé®ç­ç«åºåæ(A Recurrent Interactive Attention Network for Answer Stance Analysis),2020,-1,-1,4,0,22091,wangda luo,Proceedings of the 19th Chinese National Conference on Computational Linguistics,0,"éå¯¹é®ç­ç«åºä»»å¡ä¸­,ç°ææ¹æ³é¾ä»¥æåé®ç­ææ¬é´çä¾èµå
³ç³»é®é¢,æ¬ææåºä¸ç§åºäºå¾ªç¯äº¤äºæ³¨æå(Recurrent Interactive Attention, RIA)ç½ç»çé®ç­ç«åºåææ¹æ³ãè¯¥æ¹æ³éè¿æ¨¡ä»¿äººç±»é
è¯»çè§£æ¶çæç»´æ¹å¼,åºäºäº¤äºæ³¨æåæºå¶åå¾ªç¯è¿­ä»£æ¹æ³,ææå°ä»é®é¢åç­æ¡çç¸äºèç³»ä¸­ææé®ç­ææ¬çç«åºä¿¡æ¯ãæ­¤å¤,è¯¥æ¹æ³å°é®é¢è¿è¡éè¿°åè¡¨ç¤º,ææå°è§£å³çé®å¥è¡¨è¿°ä¸é®é¢ææ¬æ æ³æç¡®è¡¨è¾¾èªèº«ç«åºçé®é¢ãå®éªç»æè¡¨æ,æ¬ææ¹æ³åå¾äºæ¯ç°ææ¨¡åæ¹æ³æ´å¥½çææ,åæ¶è¯æè¯¥æ¹æ³è½æææåé®ç­ç«åºåæä»»å¡ä¸­çé®ç­å¯¹ä¾èµå
³ç³»ã"
2020.acl-main.342,Transition-based Directed Graph Construction for Emotion-Cause Pair Extraction,2020,-1,-1,6,1,9079,chuang fan,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding causes from unannotated emotion text. Most existing methods are pipelined framework, which identifies emotions and extracts causes separately, leading to a drawback of error propagation. Towards this issue, we propose a transition-based model to transform the task into a procedure of parsing-like directed graph construction. The proposed model incrementally generates the directed graph with labeled edges based on a sequence of actions, from which we can recognize emotions with the corresponding causes simultaneously, thereby optimizing separate subtasks jointly and maximizing mutual benefits of tasks interdependently. Experimental results show that our approach achieves the best performance, outperforming the state-of-the-art methods by 6.71{\%} (p{\textless}0.01) in F1 measure."
P19-1462,Context-aware Embedding for Targeted Aspect-based Sentiment Analysis,2019,0,3,3,1,8669,bin liang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Attention-based neural models were employed to detect the different aspects and sentiment polarities of the same target in targeted aspect-based sentiment analysis (TABSA). However, existing methods do not specifically pre-train reasonable embeddings for targets and aspects in TABSA. This may result in targets or aspects having the same vector representations in different contexts and losing the context-dependent information. To address this problem, we propose a novel method to refine the embeddings of targets and aspects. Such pivotal embedding refinement utilizes a sparse coefficient vector to adjust the embeddings of target and aspect from the context. Hence the embeddings of targets and aspects can be refined from the highly correlative words instead of using context-independent or randomly initialized vectors. Experiment results on two benchmark datasets show that our approach yields the state-of-the-art performance in TABSA task."
D19-1350,Neural Topic Model with Reinforcement Learning,2019,0,1,5,1,3936,lin gui,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"In recent years, advances in neural variational inference have achieved many successes in text processing. Examples include neural topic models which are typically built upon variational autoencoder (VAE) with an objective of minimising the error of reconstructing original documents based on the learned latent topic vectors. However, minimising reconstruction errors does not necessarily lead to high quality topics. In this paper, we borrow the idea of reinforcement learning and incorporate topic coherence measures as reward signals to guide the learning of a VAE-based topic model. Furthermore, our proposed model is able to automatically separating background words dynamically from topic words, thus eliminating the pre-processing step of filtering infrequent and/or top frequent words, typically required for learning traditional topic models. Experimental results on the 20 Newsgroups and the NIPS datasets show superior performance both on perplexity and topic coherence measure compared to state-of-the-art neural topic models."
D19-1563,A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis,2019,0,1,7,1,9079,chuang fan,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Emotion cause analysis, which aims to identify the reasons behind emotions, is a key topic in sentiment analysis. A variety of neural network models have been proposed recently, however, these previous models mostly focus on the learning architecture with local textual information, ignoring the discourse and prior knowledge, which play crucial roles in human text comprehension. In this paper, we propose a new method to extract emotion cause with a hierarchical neural model and knowledge-based regularizations, which aims to incorporate discourse context information and restrain the parameters by sentiment lexicon and common knowledge. The experimental results demonstrate that our proposed method achieves the state-of-the-art performance on two public datasets in different languages (Chinese and English), outperforming a number of competitive baselines by at least 2.08{\%} in F-measure."
D19-1654,A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis,2019,0,3,3,0,9815,qingnan jiang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Aspect-based sentiment analysis (ABSA) has attracted increasing attention recently due to its broad applications. In existing ABSA datasets, most sentences contain only one aspect or multiple aspects with the same sentiment polarity, which makes ABSA task degenerate to sentence-level sentiment analysis. In this paper, we present a new large-scale Multi-Aspect Multi-Sentiment (MAMS) dataset, in which each sentence contains at least two different aspects with different sentiment polarities. The release of this dataset would push forward the research in this field. In addition, we propose simple yet effective CapsNet and CapsNet-BERT models which combine the strengths of recent NLP advances. Experiments on our new dataset show that the proposed model significantly outperforms the state-of-the-art baseline methods"
L18-1078,The {UIR} Uncertainty Corpus for {C}hinese: Annotating {C}hinese Microblog Corpus for Uncertainty Identification from Social Media,2018,0,0,6,0.708678,21327,binyang li,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1069,Hybrid Neural Attention for Agreement/Disagreement Inference in Online Debates,2018,0,1,4,0,30457,di chen,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Inferring the agreement/disagreement relation in debates, especially in online debates, is one of the fundamental tasks in argumentation mining. The expressions of agreement/disagreement usually rely on argumentative expressions in text as well as interactions between participants in debates. Previous works usually lack the capability of jointly modeling these two factors. To alleviate this problem, this paper proposes a hybrid neural attention model which combines self and cross attention mechanism to locate salient part from textual context and interaction between users. Experimental results on three (dis)agreement inference datasets show that our model outperforms the state-of-the-art models."
D18-1354,Variational Autoregressive Decoder for Neural Response Generation,2018,0,16,4,1,6571,jiachen du,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Combining the virtues of probability graphic models and neural networks, Conditional Variational Auto-encoder (CVAE) has shown promising performance in applications such as response generation. However, existing CVAE-based models often generate responses from a single latent variable which may not be sufficient to model high variability in responses. To solve this problem, we propose a novel model that sequentially introduces a series of latent variables to condition the generation of each word in the response sequence. In addition, the approximate posteriors of these latent variables are augmented with a backward Recurrent Neural Network (RNN), which allows the latent variables to capture long-term dependencies of future tokens in generation. To facilitate training, we supplement our model with an auxiliary objective that predicts the subsequent bag of words. Empirical experiments conducted on Opensubtitle and Reddit datasets show that the proposed model leads to significant improvement on both relevance and diversity over state-of-the-art baselines."
D17-1167,A Question Answering Approach for Emotion Cause Extraction,2017,0,19,4,1,3936,lin gui,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01{\%} in F-measure."
D16-1170,Event-Driven Emotion Cause Extraction with Corpus Construction,2016,27,18,3,1,3936,lin gui,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
W15-3111,A Joint Model for {C}hinese Microblog Sentiment Analysis,2015,10,1,3,0,36820,yuhui cao,Proceedings of the Eighth {SIGHAN} Workshop on {C}hinese Language Processing,0,"Topic-based sentiment analysis for Chinese microblog aims to identify the user attitude on specified topics. In this paper, we propose a joint model by incorporating Support Vector Machines (SVM) and deep neural network to improve the performance of sentiment analysis. Firstly, a SVM Classifier is constructed using N-gram, NPOS and sentiment lexicons features. Meanwhile, a convolutional neural network is applied to learn paragraph representation features as the input of another SVM classifier. The classification results outputted by these two classifiers are merged as the final classification results. The evaluations on the SIGHAN-8 Topic-based Chinese microblog sentiment analysis task show that our proposed approach achieves the second rank on micro average F1 and the fourth rank on macro average F1 among a total of 13 submitted systems."
P15-2003,Improving Distributed Representation of Word Sense via {W}ord{N}et Gloss Composition and Context Clustering,2015,22,17,2,0,7684,tao chen,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"In recent years, there has been an increas-ing interest in learning a distributed rep-resentation of word sense. Traditional context clustering based models usually require careful tuning of model parame-ters, and typically perform worse on infre-quent word senses. This paper presents a novel approach which addresses these lim-itations by first initializing the word sense embeddings through learning sentence-level embeddings from WordNet glosses using a convolutional neural networks. The initialized word sense embeddings are used by a context clustering based model to generate the distributed representations of word senses. Our learned represen-tations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task."
W14-6817,Personal Attributes Extraction in {C}hinese Text Bakeoff in {CLP} 2014: Overview,2014,6,0,1,1,1816,ruifeng xu,Proceedings of The Third {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"This paper presents the overview of Personal Attributes Extraction in Chinese Text Bakeoff in CLP 2014. Personal attribute extraction plays an important role in information extraction, event tracking, entity disambiguation and other related research areas. This task is designed to evaluate the techniques for extracting person specific attributes from unstructured Chinese texts, which is similar to slot filling, but focuses on person attributes. This task brings some challenges issues because Chinese language contains some common words and lacks of capital clues as in English. The task organizer manually constructs the query names and corresponding documents. The value/presence of the texts corresponding 25 pre-defined attributes are annotated to construct the training and testing dataset. The bakeoff results achieved by the participators show the good progress in this field."
P14-5017,Web Information Mining and Decision Support Platform for the Modern Service Industry,2014,10,1,5,0.708678,21327,binyang li,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This demonstration presents an intelligent information platform MODEST. MODEST will provide enterprises with the services of retrieving news from websites, extracting commercial information, exploring customersxe2x80x99 opinions, and analyzing collaborative/competitive social networks. In this way, enterprises can improve the competitive abilities and facilitate potential collaboration activities. At the meanwhile, MODEST can also help governments to acquire information about one single company or the entire board timely, and make prompt strategies for better support. Currently, MODEST is applied to the pillar industries of Hong Kong, including innovative finance, modem logistics, information technology, etc."
P14-2101,Automatic Labelling of Topic Models Learned from {T}witter by Summarisation,2014,33,16,3,0,39156,amparo basave,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,Latent topics derived by topic models such as Latent Dirichlet Allocation (LDA) are the result of hidden thematic structures which provide further insights into the data. The automatic labelling of such topics derived from social media poses however new challenges since topics may characterise novel events happening in the real world. Existing automatic topic labelling approaches which depend on external knowledge sources become less applicable here since relevant articles/concepts of the extracted topics may not exist in external sources. In this paper we propose to address the problem of automatic labelling of latent topics learned from Twitter as a summarisation problem. We introduce a framework which apply summarisation algorithms to generate topic labels. These algorithms are independent of external sources and only rely on the identification of dominant terms in documents related to the latent topic. We compare the efficiency of existing state of the art summarisation algorithms. Our results suggest that summarisation algorithms generate better topic labels which capture event-related context compared to the top-n terms returned by LDA.
P14-2139,Cross-lingual Opinion Analysis via Negative Transfer Detection,2014,27,19,2,1,3936,lin gui,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Transfer learning has been used in opinion analysis to make use of available language resources for other resource scarce languages. However, the cumulative class noise in transfer learning adversely affects performance when more training data is used. In this paper, we propose a novel method in transductive transfer learning to identify noises through the detection of negative transfers. Evaluation on NLP&CC 2013 cross-lingual opinion analysis dataset shows that our approach outperforms the state-of-the-art systems. More significantly, our system shows a monotonic increase trend in performance improvement when more training data are used."
W12-6326,Explore {C}hinese Encyclopedic Knowledge to Disambiguate Person Names,2012,23,2,2,0,12452,jie liu,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"This paper presents the HITSZ-PolyU system in the CIPS-SIGHAN bakeoff 2012 Task 3, Chinese Personal Name Disambiguation. This system leveraged the Chinese encyclopedia Baidu Baike (Baike) as the external knowledge to disambiguate the person names. Three kinds of features are extracted from Baike. They are the entitiesxe2x80x99 texts in Baike, the entitiesxe2x80x99 work-of-art words and titles in the Baike. With these features, a Decision Tree (DT) based classifier is trained to link test names to nodes in the NameKB. Besides, the contextual information surrounding test names is used to verify whether test names are person name or not. Finally, a simple clustering approach is used to group NIL test names that have no links to the NameKB. Our proposed system attains 64.04% precision, 70.1% recall and 66.95% F-score."
W12-4512,Incorporating Rule-based and Statistic-based Techniques for Coreference Resolution,2012,14,1,1,1,1816,ruifeng xu,Joint Conference on {EMNLP} and {C}o{NLL} - Shared Task,0,"This paper describes a coreference resolution system for CONLL 2012 shared task developed by HLT_HITSZ group, which incorporates rule-based and statistic-based techniques. The system performs coreference resolution through the mention pair classification and linking. For each detected mention pairs in the text, a Decision Tree (DT) based binary classifier is applied to determine whether they form a coreference. This classifier incorporates 51 and 61 selected features for English and Chinese, respectively. Meanwhile, a rule-based classifier is applied to recognize some specific types of coreference, especially the ones with long distances. The outputs of these two classifiers are merged. Next, the recognized coreferences are linked to generate the final coreference chain. This system is evaluated on English and Chinese sides (Closed Track), respectively. It achieves 0.5861 and 0.6003 F1 score on the development data of English and Chinese, respectively. As for the test dataset, the achieved F1 scores are 0.5749 and 0.6508, respectively. This encouraging performance shows the effectiveness of our proposed coreference resolution system."
W11-1724,Instance Level Transfer Learning for Cross Lingual Opinion Analysis,2011,14,4,1,1,1816,ruifeng xu,Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis ({WASSA} 2.011),0,"This paper presents two instance-level transfer learning based algorithms for cross lingual opinion analysis by transferring useful translated opinion examples from other languages as the supplementary training data for improving the opinion classifier in target language. Starting from the union of small training data in target language and large translated examples in other languages, the Transfer AdaBoost algorithm is applied to iteratively reduce the influence of low quality translated examples. Alternatively, starting only from the training data in target language, the Transfer Self-training algorithm is designed to iteratively select high quality translated examples to enrich the training data set. These two algorithms are applied to sentence- and document-level cross lingual opinion analysis tasks, respectively. The evaluations show that these algorithms effectively improve the opinion analysis by exploiting small target language training data and large cross lingual training data."
I11-1168,Diversifying Information Needs in Results of Question Retrieval,2011,15,0,4,0,34339,yaoyun zhang,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Information need is an important factor in question retrieval. This paper proposes a method to diversify the results of question retrieval in term of types of information needs. CogQTaxo, a question hierarchy is leveraged to represent usersxe2x80x99 information needs cognitively from three linguistic levels. Based on a prediction model of question types, three factors, i.e., scores of IR model, question type similarity and question type novelty are linearly combined to re-rank the retrieved questions. Preliminary experimental results show that the proposed method enhances the question retrieval performance in information coverage and diversity."
W10-4154,Combine Person Name and Person Identity Recognition and Document Clustering for {C}hinese Person Name Disambiguation,2010,3,1,1,1,1816,ruifeng xu,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
S10-1100,"{HITSZ}{\\_}{CITYU}: Combine Collocation, Context Words and Neighboring Sentence Sentiment in Sentiment Adjectives Disambiguation",2010,7,2,1,1,1816,ruifeng xu,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"This paper presents the HIT_CITYU systems in Semeval-2 Task 18, namely, disambiguating sentiment ambiguous adjectives. The baseline system (HITSZ_CITYU_3) incorporates bi-gram and n-gram collocations of sentiment adjectives, and other context words as features in a one-class Support Vector Machine (SVM) classifier. To enhance the baseline system, collocation set expansion and characteristics learning based on word similarity and semisupervised learning are investigated, respectively. The final system (HITSZ_CITYU_1/2) combines collocations, context words and neighboring sentence sentiment in a two-class SVM classifier to determine the polarity of sentiment adjectives. The final systems achieved 0.957 and 0.953 (ranked 1st and 2nd) macro accuracy, and 0.936 and 0.933 (ranked 2nd and 3rd) micro accuracy, respectively."
xu-etal-2008-opinion,Opinion Annotation in On-line {C}hinese Product Reviews,2008,13,15,1,1,1816,ruifeng xu,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents the design and construction of a Chinese opinion corpus based on the online product reviews. Based on the observation on the characteristics of opinion expression in Chinese online product reviews, which is quite different from in the formal texts such as news, an annotation framework is proposed to guide the construction of the first Chinese opinion corpus based on online product reviews. The opinionated sentences are manually identified from the review text. Furthermore, for each comment in the opinionated sentence, its 13 describing elements are annotated including the expressions related to the interested product attributes and user opinions as well as the polarity and degree of the opinions. Currently, 12,724 comments are annotated in 10,935 sentences from review text. Through statistical analysis on the opinion corpus, some interesting characteristics of Chinese opinion expression are presented. This corpus is shown helpful to support systematic research on Chinese opinion analysis."
W07-1511,Annotating {C}hinese Collocations with Multi Information,2007,8,2,1,1,1816,ruifeng xu,Proceedings of the Linguistic Annotation Workshop,0,"This paper presents the design and construction of an annotated Chinese collocation bank as the resource to support systematic research on Chinese collocations. With the help of computational tools, the bi-gram and n-gram collocations corresponding to 3,643 head-words are manually identified. Furthermore, annotations for bi-gram collocations include dependency relation, chunking relation and classification of collocation types. Currently, the collocation bank annotated 23,581 bi-gram collocations and 2,752 n-gram collocations extracted from a 5-million-word corpus. Through statistical analysis on the collocation bank, some characteristics of Chinese bi-gram collocations are examined which is essential to collocation research, especially for Chinese."
li-etal-2006-interaction,Interaction between Lexical Base and Ontology with Formal Concept Analysis,2006,8,0,4,0,5814,sujian li,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"An ontology describes conceptual knowledge in a specific domain. A lexical base collects a repository of words and gives independent definition of concepts. In this paper, we propose to use FCA as a tool to help constructing an ontology through an existing lexical base. We mainly address two issues. The first issue is how to select attributes to visualize the relations between lexical terms. The second issue is how to revise lexical definitions through analysing the relations in the ontology. Thus the focus is on the effect of interaction between a lexical base and an ontology for the purpose of good ontology construction. Finally, experiments have been conducted to verify our ideas."
xu-etal-2006-design,The Design and Construction of A {C}hinese Collocation Bank,2006,0,3,1,1,1816,ruifeng xu,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper presents an annotated Chinese collocation bank developed at the Hong Kong Polytechnic University. The definition of collocation with good linguistic consistency and good computational operability is first discussed and the properties of collocations are then presented. Secondly, based on the combination of different properties, collocations are classified into four types. Thirdly, the annotation guideline is presented. Fourthly, the implementation issues for collocation bank construction are addressed including the annotation with categorization, dependency and contextual information. Currently, the collocation bank is completed for 3,643 headwords in a 5-million-word corpus."
O05-4006,The Design and Construction of the {P}oly{U} Shallow Treebank,2005,12,6,1,1,1816,ruifeng xu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 3, September 2005: Special Issue on Selected Papers from {ROCLING} {XVI}",0,"This paper presents the design and construction of the PolyU Treebank, a manually annotated Chinese shallow treebank. The PolyU Treebank is based on shallow annotation where only partial syntactical structures within sentences are annotated. Guided by the Phrase-Standard Grammar proposed by Peking University, the PolyU Treebank has been designed and constructed to provide a large amount of annotated data containing shallow syntactical information and limited semantic information for use in natural language processing (NLP) research. This paper describes the relevant design principles, annotation guidelines, and implementation issues, including the achievement of high quality annotation through the use of well-designed annotation workflow and effective post-annotation checking tools. Currently, the PolyU Treebank consists of a one-million-word annotated corpus and has been used in a number of NLP research projects with promising results."
O05-2006,Similarity Based {C}hinese Synonym Collocation Extraction,2005,12,10,3,1,43957,wanyin li,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 1, March 2005",0,"Collocation extraction systems based on pure statistical methods suffer from two major problems. The first problem is their relatively low precision and recall rates. The second problem is their difficulty in dealing with sparse collocations. In order to improve performance, both statistical and lexicographic approaches should be considered. This paper presents a new method to extract synonymous collocations using semantic information. The semantic information is obtained by calculating similarities from HowNet. We have successfully extracted synonymous collocations which normally cannot be extracted using lexical statistics. Our evaluation conducted on a 60MB tagged corpus shows that we can extract synonymous collocations that occur with very low frequency and that the improvement in the recall rate is close to 100%. In addition, compared with a collocation extraction system based on the Xtract system for English, our algorithm can improve the precision rate by about 44%."
W04-1113,Using Synonym Relations in {C}hinese Collocation Extraction,2004,9,0,3,0,43957,wanyin li,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,"A challenging task in Chinese collocation extraction is to improve both the precision and recall rate. Most lexical statistical methods including Xtract face the problem of unable to extract collocations with lower frequencies than a given threshold. This paper presents a method where HowNet is used to find synonyms using a similarity function. Based on such synonym information, we have successfully extracted synonymous collocations which normally cannot be extracted using the lexical statistical approach. We applied synonyms mapping to each headword to extract more synonymous word bi-grams. Our evaluation over 60MB tagged corpus shows that we can extract synonymous collocations that occur with very low frequency, sometimes even for collocations that occur only once in the training set. Comparing to a collocation extraction system based on Xtract, we have reached the precision rate of 43% on word bi-grams for a set of 9 headwords, almost 50% improvement from precision rate of 30% in the Xtract system. Furthermore, it improves the recall rate of word bi-gram collocation extraction by 30%."
W04-1114,The Construction of A {C}hinese Shallow Treebank,2004,7,6,1,1,1816,ruifeng xu,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper presents the construction of a manually annotated Chinese shallow Treebank, named PolyU Treebank. Different from traditional Chinese Treebank based on full parsing, the PolyU Treebank is based on shallow parsing in which only partial syntactical structures are annotated. This Treebank can be used to support shallow parser training, testing and other natural language applications. Phrase-based Grammar, proposed by Peking University, is used to guide the design and implementation of the PolyU Treebank. The design principles include good resource sharing, low structural complexity, sufficient syntactic information and large data scale. The design issues, including corpus material preparation, standard for word segmentation and POS tagging, and the guideline for phrase bracketing and annotation, are presented in this paper. Well-designed workflow and effective semiautomatic and automatic annotation checking are used to ensure annotation accuracy and consistency. Currently, the PolyU Treebank has completed the annotation of a 1-million-word corpus. The evaluation shows that the accuracy of annotation is higher than 98%."
