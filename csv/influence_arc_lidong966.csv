2020.aacl-main.2,D19-1572,0,0.0177822,"hods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-Tuning We use multilingual B"
2020.aacl-main.2,Q19-1038,0,0.0286828,"enchmarks show that our methods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-T"
2020.aacl-main.2,Q18-1039,0,0.0246351,"dge from monolingual pretrained models to multilingual ones. Experimental results on two cross-lingual classification benchmarks show that our methods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine"
2020.aacl-main.2,D19-1138,0,0.0253137,"nslation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-Tuning We use multilingual BERT (Devlin et al., 2019) for multilingual pretrained language models. The pretrained model uses the BERT-style Transformer (Vaswani et al., 2017) architecture, and follows the similar fine-tuning procedure as BERT for text classification, which is illustrated in Figure 1(a). To be specific, the first input token of the models is always a special classification token Contribution during internship at Microso"
2020.aacl-main.2,P19-1493,0,0.0185862,"fication models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-Tuning We use multilingual BERT (Devlin et al., 2019) for multilingual pretrained language models. The pretrained model uses the BERT-style Transformer (Vaswani et al., 2017) architecture, and follows the similar fine-tuning procedure as BERT for text classification, which is illustrated in Figure 1(a). To be specific, the first input token of the models is always a special classification token"
2020.aacl-main.2,2020.acl-main.747,0,0.067487,"Missing"
2020.aacl-main.2,P10-1114,0,0.0857345,"Missing"
2020.aacl-main.2,P16-1162,0,0.0418231,"Missing"
2020.aacl-main.2,D19-1077,0,0.0206782,"xploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-Tuning We use multilingual BERT (Devlin et al., 2019) for multilingual pretrained language models. The pretrained model uses the BERT-style Transformer (Vaswani et al., 2017) architecture, and follows the similar fine-tuning procedure as BERT for text classification, which is illustrated in Figure 1(a). To be specific, the first input token of the models is always a special classification token Contribution during"
2020.aacl-main.2,P17-1130,0,0.0144372,"sferring the knowledge from monolingual pretrained models to multilingual ones. Experimental results on two cross-lingual classification benchmarks show that our methods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these"
2020.aacl-main.2,W18-3023,0,0.0238199,"classification benchmarks show that our methods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Back"
2020.acl-main.600,P19-1620,0,0.0412653,"al., 2017), and QANet (Yu et al., 2018). Recently, unsupervised pre-training of language models such as BERT (Devlin et al., 2019), achieves significant improvement. However, these powerful models rely on the availability of human-labeled data. Large annotated corpora for a specific domain or language are limited and expensive to construct. Semi-Supervised QA Several semi-supervised approaches have been proposed to utilize unlabeled data. Neural question generation (QG) models are used to generate questions from unlabeled passages for training QA models (Yang et al., 2017; Zhu et al., 2019b; Alberti et al., 2019; Dong et al., 2019). However, the methods require labeled data to train the sequence-to-sequence QG model. Dhingra et al. (2018) propose to collect synthetic context-question-answer triples by generating cloze-style questions from the Wikipedia summary paragraphs in an unsupervised manner. Unsupervised QA Lewis et al. (2019) have explored the unsupervised method for QA. They create synthetic QA data in four steps. i) Sample paragraphs from the English Wikipedia. ii) Use NER or noun chunkers to extract answer candidates from the context. iii) Extract “fill-inthe-blank” cloze-style questions gi"
2020.acl-main.600,N19-1423,0,0.669722,"upervised approaches by a large margin and is competitive with early supervised models. We also show the effectiveness of our approach in the fewshot learning setting. 1 Introduction Extractive question answering aims to extract a span from the given document to answer the question. Rapid progress has been made because of the release of large-scale annotated datasets (Rajpurkar et al., 2016, 2018; Joshi et al., 2017), and well-designed neural models (Wang and Jiang, 2016; Seo et al., 2016; Yu et al., 2018). Recently, unsupervised pre-training of language models on large corpora, such as BERT (Devlin et al., 2019), has brought further performance gains. However, the above approaches heavily rely on the availability of large-scale datasets. The collection of high-quality training data is timeconsuming and requires significant resources, es∗ Contribution during internship at Microsoft Research. The code and data are available at https://github. com/Neutralzz/RefQA. 1 pecially for new domains or languages. In order to tackle the setting in which no training data available, Lewis et al. (2019) leverage unsupervised machine translation to generate synthetic contextquestion-answer triples. The paragraphs are"
2020.acl-main.600,N18-2092,0,0.43499,"19), achieves significant improvement. However, these powerful models rely on the availability of human-labeled data. Large annotated corpora for a specific domain or language are limited and expensive to construct. Semi-Supervised QA Several semi-supervised approaches have been proposed to utilize unlabeled data. Neural question generation (QG) models are used to generate questions from unlabeled passages for training QA models (Yang et al., 2017; Zhu et al., 2019b; Alberti et al., 2019; Dong et al., 2019). However, the methods require labeled data to train the sequence-to-sequence QG model. Dhingra et al. (2018) propose to collect synthetic context-question-answer triples by generating cloze-style questions from the Wikipedia summary paragraphs in an unsupervised manner. Unsupervised QA Lewis et al. (2019) have explored the unsupervised method for QA. They create synthetic QA data in four steps. i) Sample paragraphs from the English Wikipedia. ii) Use NER or noun chunkers to extract answer candidates from the context. iii) Extract “fill-inthe-blank” cloze-style questions given the candidate answer and context. iv) Translate cloze-style questions into natural questions by an unsupervised translator. C"
2020.acl-main.600,P17-1147,0,0.340613,"rs, which iteratively refines data over R E F QA. We conduct experiments1 on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data. Our approach outperforms previous unsupervised approaches by a large margin and is competitive with early supervised models. We also show the effectiveness of our approach in the fewshot learning setting. 1 Introduction Extractive question answering aims to extract a span from the given document to answer the question. Rapid progress has been made because of the release of large-scale annotated datasets (Rajpurkar et al., 2016, 2018; Joshi et al., 2017), and well-designed neural models (Wang and Jiang, 2016; Seo et al., 2016; Yu et al., 2018). Recently, unsupervised pre-training of language models on large corpora, such as BERT (Devlin et al., 2019), has brought further performance gains. However, the above approaches heavily rely on the availability of large-scale datasets. The collection of high-quality training data is timeconsuming and requires significant resources, es∗ Contribution during internship at Microsoft Research. The code and data are available at https://github. com/Neutralzz/RefQA. 1 pecially for new domains or languages. In"
2020.acl-main.600,P18-1249,0,0.0280697,"|ci , qi ) a0i ∈ZA + I(a0i (2) 6= ai )log P (a0i |ci , qi0 )], half of the statement tokens are not in the cited document. The article length is limited to 1,000 words for cited documents. Besides, we compute ROUGE-2 (Lin, 2004) as correlation scores between statements and context. We use the score’s median (0.2013) as a threshold, i.e., half of the data with lower scores are discarded. We obtain 303K remaining data to construct our R EF QA. We extract named entities as our answer candidates, using the NER toolkit of Spacy. We split the statements into sub-clauses with Berkeley Neural Parser (Kitaev and Klein, 2018). The questions are generated as in Section 3.2. We also discard sub-clauses that are less than 6 tokens, to prevent losing too much information of original sentences. Finally, we obtain 0.9M R EF QA examples. Algorithm 1: Iterative Data Refinement Input: synthetic context-question-answer triples S = {(ci , qi , ai )}N i=1 , a threshold τ and a decay factor γ. Sample a part of triples SI from S Update the model P parameters by maximizing SI log P (a|c, q) Split unseen triples into {SU1 , SU2 , ..., SUM } for k ← 1 to M do D←φ for (ci , qi , ai ) in SUk do ZA ← {a0i s.t. P (a0i |ci , qi ) ≥ τ }"
2020.acl-main.600,P19-1484,0,0.159185,"., 2016; Yu et al., 2018). Recently, unsupervised pre-training of language models on large corpora, such as BERT (Devlin et al., 2019), has brought further performance gains. However, the above approaches heavily rely on the availability of large-scale datasets. The collection of high-quality training data is timeconsuming and requires significant resources, es∗ Contribution during internship at Microsoft Research. The code and data are available at https://github. com/Neutralzz/RefQA. 1 pecially for new domains or languages. In order to tackle the setting in which no training data available, Lewis et al. (2019) leverage unsupervised machine translation to generate synthetic contextquestion-answer triples. The paragraphs are sampled from Wikipedia. NER and noun chunkers are employed to identify answer candidates. Cloze questions are first extracted from the sentences of the paragraph, and then translated into natural questions. However, there are a lot of lexical overlaps between the generated questions and the paragraph. Similar lexical and syntactic structures render the QA model tend to predict the answer just by word matching. Moreover, the answer category is limited to the named entity or noun p"
2020.acl-main.600,W04-1013,0,0.0162225,"predicted answers and their probabilities. Then we filter the predicted answers with Iterative QA Model Training After refining the dataset, we concatenate them with the filtered examples whose candidate answers agree with the predictions. The new training set is then used to continue to train the QA model. The training objective is defined as: 6722 max X [I(a0i = ai )log P (ai |ci , qi ) a0i ∈ZA + I(a0i (2) 6= ai )log P (a0i |ci , qi0 )], half of the statement tokens are not in the cited document. The article length is limited to 1,000 words for cited documents. Besides, we compute ROUGE-2 (Lin, 2004) as correlation scores between statements and context. We use the score’s median (0.2013) as a threshold, i.e., half of the data with lower scores are discarded. We obtain 303K remaining data to construct our R EF QA. We extract named entities as our answer candidates, using the NER toolkit of Spacy. We split the statements into sub-clauses with Berkeley Neural Parser (Kitaev and Klein, 2018). The questions are generated as in Section 3.2. We also discard sub-clauses that are less than 6 tokens, to prevent losing too much information of original sentences. Finally, we obtain 0.9M R EF QA examp"
2020.acl-main.600,P18-2124,0,0.0491469,"Missing"
2020.acl-main.600,D16-1264,0,0.770515,"extract more appropriate answers, which iteratively refines data over R E F QA. We conduct experiments1 on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data. Our approach outperforms previous unsupervised approaches by a large margin and is competitive with early supervised models. We also show the effectiveness of our approach in the fewshot learning setting. 1 Introduction Extractive question answering aims to extract a span from the given document to answer the question. Rapid progress has been made because of the release of large-scale annotated datasets (Rajpurkar et al., 2016, 2018; Joshi et al., 2017), and well-designed neural models (Wang and Jiang, 2016; Seo et al., 2016; Yu et al., 2018). Recently, unsupervised pre-training of language models on large corpora, such as BERT (Devlin et al., 2019), has brought further performance gains. However, the above approaches heavily rely on the availability of large-scale datasets. The collection of high-quality training data is timeconsuming and requires significant resources, es∗ Contribution during internship at Microsoft Research. The code and data are available at https://github. com/Neutralzz/RefQA. 1 pecially for n"
2020.acl-main.600,W17-2623,0,0.469066,"rs to continue the model training. Thanks to the pretrained linguistic knowledge in the BERTbased QA model, there are more appropriate and diverse answer candidates in the filtered predictions, some of which do not appear in the candidates extracted by NER tools. We also show 6719 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6719–6728 c July 5 - 10, 2020. 2020 Association for Computational Linguistics that iteratively refining the data further improves model performance. We conduct experiments on SQuAD 1.1 (Rajpurkar et al., 2016), and NewsQA (Trischler et al., 2017). Our method yields state-of-the-art results against strong baselines in the unsupervised setting. Specifically, the proposed model achieves 71.4 F1 on the SQuAD 1.1 test set and 45.1 F1 on the NewsQA test set without using annotated data. We also evaluate our method in a few-shot learning setting. Our approach achieves 79.4 F1 on the SQuAD 1.1 dev set with only 100 labeled examples, compared to 63.0 F1 using the method of Lewis et al. (2019). To summarize, the contributions of this paper include: i) R EF QA constructing in an unsupervised manner, which contains more informative context-questi"
2020.acl-main.600,P19-1415,1,0.910983,"16), R-NET (Wang et al., 2017), and QANet (Yu et al., 2018). Recently, unsupervised pre-training of language models such as BERT (Devlin et al., 2019), achieves significant improvement. However, these powerful models rely on the availability of human-labeled data. Large annotated corpora for a specific domain or language are limited and expensive to construct. Semi-Supervised QA Several semi-supervised approaches have been proposed to utilize unlabeled data. Neural question generation (QG) models are used to generate questions from unlabeled passages for training QA models (Yang et al., 2017; Zhu et al., 2019b; Alberti et al., 2019; Dong et al., 2019). However, the methods require labeled data to train the sequence-to-sequence QG model. Dhingra et al. (2018) propose to collect synthetic context-question-answer triples by generating cloze-style questions from the Wikipedia summary paragraphs in an unsupervised manner. Unsupervised QA Lewis et al. (2019) have explored the unsupervised method for QA. They create synthetic QA data in four steps. i) Sample paragraphs from the English Wikipedia. ii) Use NER or noun chunkers to extract answer candidates from the context. iii) Extract “fill-inthe-blank” c"
2020.acl-main.600,P17-1018,1,0.931513,"gment the question-answer pairs in R EF QA. 2 Related Work Extractive Question Answering Given a document and question, the task is to predict a continuous sub-span of the document to answer the question. Extractive question answering has garnered a lot of attention over the past few years. Benchmark datasets, such as SQuAD (Rajpurkar et al., 2016, 2018), NewsQA (Trischler et al., 2017) and TriviaQA (Joshi et al., 2017), play an important role in the progress. In order to improve the performance on these benchmarks, several models have been proposed, including BiDAF (Seo et al., 2016), R-NET (Wang et al., 2017), and QANet (Yu et al., 2018). Recently, unsupervised pre-training of language models such as BERT (Devlin et al., 2019), achieves significant improvement. However, these powerful models rely on the availability of human-labeled data. Large annotated corpora for a specific domain or language are limited and expensive to construct. Semi-Supervised QA Several semi-supervised approaches have been proposed to utilize unlabeled data. Neural question generation (QG) models are used to generate questions from unlabeled passages for training QA models (Yang et al., 2017; Zhu et al., 2019b; Alberti et"
2020.acl-main.600,K17-1028,0,0.0647072,"Missing"
2020.acl-main.600,P17-1096,0,0.0898366,"DAF (Seo et al., 2016), R-NET (Wang et al., 2017), and QANet (Yu et al., 2018). Recently, unsupervised pre-training of language models such as BERT (Devlin et al., 2019), achieves significant improvement. However, these powerful models rely on the availability of human-labeled data. Large annotated corpora for a specific domain or language are limited and expensive to construct. Semi-Supervised QA Several semi-supervised approaches have been proposed to utilize unlabeled data. Neural question generation (QG) models are used to generate questions from unlabeled passages for training QA models (Yang et al., 2017; Zhu et al., 2019b; Alberti et al., 2019; Dong et al., 2019). However, the methods require labeled data to train the sequence-to-sequence QG model. Dhingra et al. (2018) propose to collect synthetic context-question-answer triples by generating cloze-style questions from the Wikipedia summary paragraphs in an unsupervised manner. Unsupervised QA Lewis et al. (2019) have explored the unsupervised method for QA. They create synthetic QA data in four steps. i) Sample paragraphs from the English Wikipedia. ii) Use NER or noun chunkers to extract answer candidates from the context. iii) Extract “f"
2020.acl-main.600,P14-1090,0,0.105078,"Missing"
2021.acl-long.264,2020.acl-main.421,0,0.0139564,"ge of example consistency R1 . Although word alignment is a possible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, a"
2021.acl-long.264,2020.tacl-1.30,0,0.0152671,"ible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, and the models are fine-tuned on the concatenation of English t"
2021.acl-long.264,2020.acl-main.747,0,0.0682084,"Missing"
2021.acl-long.264,D18-1269,0,0.0258158,"do not change nword . Thus the three data augmentation strategies will not affect the usage of example consistency R1 . Although word alignment is a possible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on differen"
2021.acl-long.264,2020.acl-main.536,0,0.184978,"arization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation. In addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set. Experimental results on the XTREME benchmark show that our method1 significantly improves crosslingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling. 1 Introduction Pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020) have shown great transferability across languages. By fine-tuning on labeled data in a source language, the models can generalize to other target languages, even without any additional training. Such generalization ability reduces the required annotation efforts, which is prohibitively expensive for low-resource languages. Recent work has demonstrated that data augmentation is helpful for cross-lingual transfer, e.g., translating source language training data into target languages (Singh et al., 2019), and generating codeswitch data by randomly replacing input words in the"
2021.acl-long.264,N19-1423,0,0.032175,"Missing"
2021.acl-long.264,E14-1049,0,0.0225658,"ary to the first strand. We focus on the cross-lingual setting, where consistency regularization has not been fully explored. • We study four types of data augmentations that can be easily plugged into cross-lingual fine-tuning. • We give instructions on how to apply X T UNE to various downstream tasks, such as classification, span extraction, and sequence labeling. • We conduct extensive experiments to show that X T UNE consistently improves the performance of cross-lingual fine-tuning. 2 Related Work Cross-Lingual Transfer Besides learning crosslingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Guo et al., 2015; Xu et al., 2018; Wang et al., 2019), most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020). These models generate multilingual contextualized word representations for different languages with a shared encoder and show promising cross-lingual transferability. Cross-Lingual Data Augmentation Machine translation has been successfully applied to the cross-lingual scenario as data augmentation. A common way to use machine translation is to finetune models on both source"
2021.acl-long.264,2020.acl-main.627,0,0.19623,"s on seven datasets. Experimental results show that our method outperforms conventional fine-tuning with data augmentation. We also demonstrate that X T UNE is flexible to be plugged in various 3403 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3403–3417 August 1–6, 2021. ©2021 Association for Computational Linguistics tasks, such as classification, span extraction, and sequence labeling. We summarize our contributions as follows: the target language. Besides, Qin et al. (2020) finetuned models on multilingual code-switch data, which achieves considerable improvements. • We propose X T UNE, a cross-lingual finetuning method to better utilize data augmentations based on consistency regularization. Consistency Regularization One strand of work in consistency regularization focused on regularizing model predictions to be invariant to small perturbations on image data. The small perturbations can be random noise (Zheng et al., 2016), adversarial noise (Miyato et al., 2019; Carmon et al., 2019) and various data augmentation approaches (Hu et al., 2017; Ye et al., 2019; X"
2021.acl-long.264,P15-1119,1,0.817926,"We focus on the cross-lingual setting, where consistency regularization has not been fully explored. • We study four types of data augmentations that can be easily plugged into cross-lingual fine-tuning. • We give instructions on how to apply X T UNE to various downstream tasks, such as classification, span extraction, and sequence labeling. • We conduct extensive experiments to show that X T UNE consistently improves the performance of cross-lingual fine-tuning. 2 Related Work Cross-Lingual Transfer Besides learning crosslingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Guo et al., 2015; Xu et al., 2018; Wang et al., 2019), most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020). These models generate multilingual contextualized word representations for different languages with a shared encoder and show promising cross-lingual transferability. Cross-Lingual Data Augmentation Machine translation has been successfully applied to the cross-lingual scenario as data augmentation. A common way to use machine translation is to finetune models on both source language training"
2021.acl-long.264,2020.acl-main.197,0,0.0945311,"Missing"
2021.acl-long.264,P17-1178,0,0.028767,"translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, and the models are fine-tuned on the concatenation of English training data and its translated data on all target languages"
2021.acl-long.264,2020.acl-main.653,0,0.0247876,". Although word alignment is a possible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, and the models are fine-tune"
2021.acl-long.264,2020.acl-main.170,0,0.0340809,"(DA , θ, θ∗ ) where λ1 and λ2 are the corresponding weights of two regularization methods. Notice that the data augmentation strategies A, A0 , and A∗ can be either different or the same, which are tuned as hyper-parameters. 3.2 Gaussian Noise Data Augmentation We consider four types of data augmentation strategies in this work, which are shown in Figure 2. We aim to study the impact of different data augmentation strategies on cross-lingual transferability. 3.2.1 Subword Sampling Representing a sentence in different subword sequences can be viewed as a data augmentation strategy (Kudo, 2018; Provilkov et al., 2020). We utilize XLM-R (Conneau et al., 2020a) as our pre-trained Anchor points have been shown useful to improve cross-lingual transferability. Conneau et al. (2020b) analyzed the impact of anchor points in pre-training cross-lingual language models. Following Qin et al. (2020), we generate code-switch data in multiple languages as data augmentation. We randomly select words in the original text in the source language and replace them with target language words in the bilingual dictionaries to obtain code-switch data. Intuitively, this type of data augmentation explicitly helps pre-trained cross-"
2021.acl-long.264,D19-1575,1,0.837666,"ing, where consistency regularization has not been fully explored. • We study four types of data augmentations that can be easily plugged into cross-lingual fine-tuning. • We give instructions on how to apply X T UNE to various downstream tasks, such as classification, span extraction, and sequence labeling. • We conduct extensive experiments to show that X T UNE consistently improves the performance of cross-lingual fine-tuning. 2 Related Work Cross-Lingual Transfer Besides learning crosslingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Guo et al., 2015; Xu et al., 2018; Wang et al., 2019), most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020). These models generate multilingual contextualized word representations for different languages with a shared encoder and show promising cross-lingual transferability. Cross-Lingual Data Augmentation Machine translation has been successfully applied to the cross-lingual scenario as data augmentation. A common way to use machine translation is to finetune models on both source language training data and translated data in all targ"
2021.acl-long.265,2020.acl-main.421,0,0.382399,"Missing"
2021.acl-long.265,J93-2003,0,0.168381,"Missing"
2021.acl-long.265,2021.emnlp-main.125,1,0.795284,"Missing"
2021.acl-long.265,2021.naacl-main.280,1,0.918277,"most applications and resources are still English-centric, making non-English users hard to access. Therefore, it is essential to build cross-lingual transferable models that can learn from the training data in highresource languages and generalize on low-resource languages. Recently, pretrained cross-lingual language models have shown their effectiveness for cross-lingual transfer. By pre-training on monolingual text and parallel sentences, the models provide significant improvements on a wide range of crosslingual end tasks (Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Chi et al., 2021b). Cross-lingual language model pre-training is typically achieved by learning various pretext tasks on ∗ Contribution during internship at Microsoft Research. monolingual and parallel corpora. By simply learning masked language modeling (MLM; Devlin et al. 2019) on monolingual text of multiple languages, the models surprisingly achieve competitive results on cross-lingual tasks (Wu and Dredze, 2019; K et al., 2020). Besides, several pretext tasks are proposed to utilize parallel corpora to learn better sentence-level cross-lingual representations (Conneau and Lample, 2019; Chi et al., 2021b;"
2021.acl-long.265,2020.tacl-1.30,0,0.109618,"Missing"
2021.acl-long.265,2020.acl-main.747,0,0.181358,"Missing"
2021.acl-long.265,2020.acl-main.653,0,0.260231,"Missing"
2021.acl-long.265,P19-1124,0,0.0233816,"plicit alignment objective in large-scale pre-training can provide consistent improvements over baseline models. Word alignment The IBM models (Brown et al., 1993) are statistical models for modeling the translation process that can extract word alignments between sentence pairs. A large number of word alignment models are based on the IBM models (Och and Ney, 2003; Mermer and Sarac¸lar, ¨ 2011; Dyer et al., 2013; Ostling and Tiedemann, 2016). Recent studies have shown that word alignments can be extracted from neural machine translation models (Ghader and Monz, 2017; Koehn and Knowles, 2017; Li et al., 2019) or from pretrained cross-lingual LMs (Jalili Sabet et al., 2020; Nagata et al., 2020). 3 Method Figure 1 illustrates an overview of our method for pre-training our cross-lingual LM, which is called XLM-A LIGN. XLM-A LIGN is pretrained in an expectation-maximization manner with two alternating steps, which are word alignment selflabeling and denoising word alignment. We first formulate word alignment as an optimal transport problem, and self-label word alignments of the input translation pair on-the-fly. Then, we update the model parameters with the denoising word alignment task, where the mod"
2021.acl-long.265,P19-1015,0,0.133748,"Missing"
2021.ecnlp-1.7,N19-1423,0,0.657051,"the requests via phone, online chat, and email channels. Identifying appropriate requests and grouping them into categories is not a trivial task. While a deep discussion of the taxonomy building process is out of the scope of this manuscript, it is sufficient to know that we have a taxonomy system that is similar to those described in (Molino et al., 2018; Fotso et al., 2018), where customized reply templates are pre-compiled for each customer contact intent. This study elaborates our journey building machine learning models to classify the intents. 3 Methodology Since the inception of BERT (Devlin et al., 2019), an abundance of research in the area of NLP has demonstrated it to be an effective approach to transfer knowledge from pretrained language models to downstream tasks (Xia et al., 2020; Wang et al., 2018, 2019a; Rajpurkar et al., 2016; Lai et al., 2017). Following BERT’s architecture, there is a stream of research that achieve comparable or better performance, to name a few (Lan et al., 2019; Liu et al., 2019; Wang et al., 2019b; Clark et al., 2020; Yang et al., 2019; Sanh et al., 2019). Among these BERT variants, ALBERT aims to strike a balance between model performance and model size (Lan e"
2021.ecnlp-1.7,D19-1252,0,0.042652,"Missing"
2021.ecnlp-1.7,D15-1282,0,0.0218701,"Fotso et al., 2018; Gupta et al., 2010; Powell et al., 2020). Specifically, the CSRs are asked to confirm the intent predictions, a process we refer to as “curation” in this manuscript. The negative cases (N) identified by CSRs are indeed hard cases, since their prediction scores are above the preset confidence threshold yet they are mis-classified by the existing model. It is an active research area to create classifiers with only P and U (Elkan and Noto, 2008; Xu et al., 2017). Some research has explored models that also include N, but they have been only concerned with binary classifiers (Fei and Liu, 2015; Hsieh et al., 2019; Li et al., 2010). In this manuscript, we adopt the semi-supervised paradigm and the multi-task approach to deal with the U and the multiclass N, respectively. Moreover, in contrast to the above-mentioned works about intent classification for customer support, we use the ALBERT pretrained language model (Lan et al., 2019) plus domain- and task-adaptive pretraining (Ramponi and Plank, 2020; Gururangan et al., 2020) to process texts. In the following sections, we describe how these techniques improve the model performance. The paper outline is as follows. We start with Secti"
2021.ecnlp-1.7,D17-1082,0,0.0222555,"ent to know that we have a taxonomy system that is similar to those described in (Molino et al., 2018; Fotso et al., 2018), where customized reply templates are pre-compiled for each customer contact intent. This study elaborates our journey building machine learning models to classify the intents. 3 Methodology Since the inception of BERT (Devlin et al., 2019), an abundance of research in the area of NLP has demonstrated it to be an effective approach to transfer knowledge from pretrained language models to downstream tasks (Xia et al., 2020; Wang et al., 2018, 2019a; Rajpurkar et al., 2016; Lai et al., 2017). Following BERT’s architecture, there is a stream of research that achieve comparable or better performance, to name a few (Lan et al., 2019; Liu et al., 2019; Wang et al., 2019b; Clark et al., 2020; Yang et al., 2019; Sanh et al., 2019). Among these BERT variants, ALBERT aims to strike a balance between model performance and model size (Lan et al., 2019). Therefore, we use albert-basev2 as the backbone encoder and perform further pretraining and finetuning. The implementation is based on Transformers from Huggingface (Wolf et al., 2019). 3.1 3.1.1 Training Data Features The input to the mode"
2021.ecnlp-1.7,D10-1022,0,0.0474535,"; Powell et al., 2020). Specifically, the CSRs are asked to confirm the intent predictions, a process we refer to as “curation” in this manuscript. The negative cases (N) identified by CSRs are indeed hard cases, since their prediction scores are above the preset confidence threshold yet they are mis-classified by the existing model. It is an active research area to create classifiers with only P and U (Elkan and Noto, 2008; Xu et al., 2017). Some research has explored models that also include N, but they have been only concerned with binary classifiers (Fei and Liu, 2015; Hsieh et al., 2019; Li et al., 2010). In this manuscript, we adopt the semi-supervised paradigm and the multi-task approach to deal with the U and the multiclass N, respectively. Moreover, in contrast to the above-mentioned works about intent classification for customer support, we use the ALBERT pretrained language model (Lan et al., 2019) plus domain- and task-adaptive pretraining (Ramponi and Plank, 2020; Gururangan et al., 2020) to process texts. In the following sections, we describe how these techniques improve the model performance. The paper outline is as follows. We start with Section 2 by elaborating the business backg"
2021.ecnlp-1.7,W10-0202,0,0.0982269,"Missing"
2021.ecnlp-1.7,P19-1441,0,0.254298,"pre-compiled for each customer contact intent. This study elaborates our journey building machine learning models to classify the intents. 3 Methodology Since the inception of BERT (Devlin et al., 2019), an abundance of research in the area of NLP has demonstrated it to be an effective approach to transfer knowledge from pretrained language models to downstream tasks (Xia et al., 2020; Wang et al., 2018, 2019a; Rajpurkar et al., 2016; Lai et al., 2017). Following BERT’s architecture, there is a stream of research that achieve comparable or better performance, to name a few (Lan et al., 2019; Liu et al., 2019; Wang et al., 2019b; Clark et al., 2020; Yang et al., 2019; Sanh et al., 2019). Among these BERT variants, ALBERT aims to strike a balance between model performance and model size (Lan et al., 2019). Therefore, we use albert-basev2 as the backbone encoder and perform further pretraining and finetuning. The implementation is based on Transformers from Huggingface (Wolf et al., 2019). 3.1 3.1.1 Training Data Features The input to the model is a collection of emailed support requests in text format. The texts are minimally preprocessed, including removing invalid characters, lowercasing letters"
2021.ecnlp-1.7,W18-5446,0,0.074594,"Missing"
2021.ecnlp-1.7,D16-1264,0,0.0335167,"anuscript, it is sufficient to know that we have a taxonomy system that is similar to those described in (Molino et al., 2018; Fotso et al., 2018), where customized reply templates are pre-compiled for each customer contact intent. This study elaborates our journey building machine learning models to classify the intents. 3 Methodology Since the inception of BERT (Devlin et al., 2019), an abundance of research in the area of NLP has demonstrated it to be an effective approach to transfer knowledge from pretrained language models to downstream tasks (Xia et al., 2020; Wang et al., 2018, 2019a; Rajpurkar et al., 2016; Lai et al., 2017). Following BERT’s architecture, there is a stream of research that achieve comparable or better performance, to name a few (Lan et al., 2019; Liu et al., 2019; Wang et al., 2019b; Clark et al., 2020; Yang et al., 2019; Sanh et al., 2019). Among these BERT variants, ALBERT aims to strike a balance between model performance and model size (Lan et al., 2019). Therefore, we use albert-basev2 as the backbone encoder and perform further pretraining and finetuning. The implementation is based on Transformers from Huggingface (Wolf et al., 2019). 3.1 3.1.1 Training Data Features Th"
2021.ecnlp-1.7,2020.emnlp-main.608,0,0.0212521,"Missing"
2021.ecnlp-1.7,2020.coling-main.603,0,0.0385201,"o create classifiers with only P and U (Elkan and Noto, 2008; Xu et al., 2017). Some research has explored models that also include N, but they have been only concerned with binary classifiers (Fei and Liu, 2015; Hsieh et al., 2019; Li et al., 2010). In this manuscript, we adopt the semi-supervised paradigm and the multi-task approach to deal with the U and the multiclass N, respectively. Moreover, in contrast to the above-mentioned works about intent classification for customer support, we use the ALBERT pretrained language model (Lan et al., 2019) plus domain- and task-adaptive pretraining (Ramponi and Plank, 2020; Gururangan et al., 2020) to process texts. In the following sections, we describe how these techniques improve the model performance. The paper outline is as follows. We start with Section 2 by elaborating the business background and how we pose it as a machine learning problem. 2 Background The E-commerce website of interest receives many support requests from customers in each second. There is a team of CSRs to actively address the requests via phone, online chat, and email channels. Identifying appropriate requests and grouping them into categories is not a trivial task. While a deep disc"
2021.ecnlp-1.7,D19-1410,0,0.0425687,"Missing"
2021.emnlp-main.125,2020.acl-main.421,0,0.147399,"s on an Nvidia DGX-2 Station. Details of the pre-training hyperparameters are described in Appendix. 4.2 4.2.1 Results XTREME Cross-lingual Understanding To validate the performance of M T6, we evaluate the pretrained models on XTREME (Hu et al., 2020b), which is a widely used benchmark for cross-lingual understanding. Following M T5 (Xue et al., 2020), we consider six downstream tasks included by XTREME: the named entity recognition (NER) task on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset in 40 languages, the question answering (QA) task on MLQA (Lewis et al., 2020b), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020), the cross-lingual natural language inference task on XNLI (Conneau et al., 2018), and crosslingual paraphrase adversaries on PAWS-X (Yang et al., 2019). The models are evaluated under the cross-lingual transfer setting (Conneau et al., 2020; Hu et al., 2020b). Under this setting, the models should be fine-tuned only on English training data but evaluated on all target languages. Moreover, for each pretrained model, only one model is used for all languages rather than selecting fine-tuned models separately. Details of the fine-tuning hyperparameters are"
2021.emnlp-main.125,Q19-1038,0,0.0291276,". We believe the better-aligned representations potentially improve the cross-lingual transferability. Furthermore, the results also indicate that our pre-training objective is more effective for training the encoder than M T5. 4.6 Word Alignment In addition to cross-lingual sentence retrieval that We analyze the cross-lingual representations pro- evaluates sentence-level representations, we also duced by our M T6 model. Following Chi et al. explore whether the representations produced by (2021a), we evaluate the representations on the M T6 are better-aligned at token-level. Thus, we Tatoeba (Artetxe and Schwenk, 2019) cross-lingual compare our M T6 with M T5 on the word alignsentence retrieval task. The test sets consist of 14 ment task, where the goal is to find corresponding English-centric language pairs covered by the par- word pairs in a translation pair. We use the hidden allel data in our experiments. Figure 4 illustrates vectors from the last encoder layer, and apply the the average accuracy@1 scores of cross-lingual SimAlign (Jalili Sabet et al., 2020) tool to obtain sentence retrieval. The scores are averaged over the resulting word alignments. Table 5 shows the 14 language pairs and both the dir"
2021.emnlp-main.125,2021.naacl-main.280,1,0.930284,"asks and the 1 Introduction training objective. We present three cross-lingual Multilingual pretrained language models, such as tasks for text-to-text Transformer pre-training, i.e., mBERT (Devlin et al., 2019), have attracted in- machine translation, translation pair span corrupcreasing attention. They not only improve the tion, and translation span corruption. In the transperformance on downstream multilingual NLP lation span corruption task, the model is trained to tasks (Conneau and Lample, 2019; Conneau et al., predict the text spans based on the input translation 2020; Liu et al., 2020; Chi et al., 2021c), but pair. The cross-lingual tasks encourage the model also show an impressive cross-lingual transferabil- to align representations of different languages. ity (Wu and Dredze, 2019; K et al., 2020; Hu et al., We also propose a new objective for text-to-text 2020b; Chi et al., 2021a). pre-training, called partially non-autoregressive Multilingual pretrained models are typically (PNAT) decoding. The PNAT objective divides the trained on multilingual unlabeled text with unsu- target sequence into several groups, and constrains pervised language modeling tasks, e.g., masked that the predictions"
2021.emnlp-main.125,2021.acl-long.265,1,0.835814,"asks and the 1 Introduction training objective. We present three cross-lingual Multilingual pretrained language models, such as tasks for text-to-text Transformer pre-training, i.e., mBERT (Devlin et al., 2019), have attracted in- machine translation, translation pair span corrupcreasing attention. They not only improve the tion, and translation span corruption. In the transperformance on downstream multilingual NLP lation span corruption task, the model is trained to tasks (Conneau and Lample, 2019; Conneau et al., predict the text spans based on the input translation 2020; Liu et al., 2020; Chi et al., 2021c), but pair. The cross-lingual tasks encourage the model also show an impressive cross-lingual transferabil- to align representations of different languages. ity (Wu and Dredze, 2019; K et al., 2020; Hu et al., We also propose a new objective for text-to-text 2020b; Chi et al., 2021a). pre-training, called partially non-autoregressive Multilingual pretrained models are typically (PNAT) decoding. The PNAT objective divides the trained on multilingual unlabeled text with unsu- target sequence into several groups, and constrains pervised language modeling tasks, e.g., masked that the predictions"
2021.emnlp-main.125,2020.tacl-1.30,0,0.0611303,"the pre-training hyperparameters are described in Appendix. 4.2 4.2.1 Results XTREME Cross-lingual Understanding To validate the performance of M T6, we evaluate the pretrained models on XTREME (Hu et al., 2020b), which is a widely used benchmark for cross-lingual understanding. Following M T5 (Xue et al., 2020), we consider six downstream tasks included by XTREME: the named entity recognition (NER) task on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset in 40 languages, the question answering (QA) task on MLQA (Lewis et al., 2020b), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020), the cross-lingual natural language inference task on XNLI (Conneau et al., 2018), and crosslingual paraphrase adversaries on PAWS-X (Yang et al., 2019). The models are evaluated under the cross-lingual transfer setting (Conneau et al., 2020; Hu et al., 2020b). Under this setting, the models should be fine-tuned only on English training data but evaluated on all target languages. Moreover, for each pretrained model, only one model is used for all languages rather than selecting fine-tuned models separately. Details of the fine-tuning hyperparameters are described in Appendix. As shown in Tabl"
2021.emnlp-main.125,2020.acl-main.747,0,0.151214,"Missing"
2021.emnlp-main.125,2020.acl-main.703,0,0.382809,"put sentence or the hsepi token for the other situations. Classification The goal of the text classification task is to predict the label of a given text. Following T5 (Raffel et al., 2020), we directly use the label text as the output text sequence. We provide an example for the MNLI natural language infer- 4 Experiments ence task (Williams et al., 2018). Given an input sentence pair of “You have access to the facts .” 4.1 Setup and “The facts are accessible to you .”, the goal is Data Following previous work on cross-lingual to classify the input into the relationships of “en- pre-training (Conneau et al., 2020; Chi et al., tailment”, “contradiction”, or “neutral”. The input 2021a), we use the natural sentences from CCand target sequences are constructed as Net (Wenzek et al., 2019) in 94 languages Input: hbosi You have access to the facts. heosi for monolingual text-to-text tasks. For crossThe facts are accessible to you. heosi lingual text-to-text tasks, we use parallel corpora Output: hbosi entailment heosi of 14 English-centric language pairs, collected Since multi-task fine-tuning is not the focus of from MultiUN (Ziemski et al., 2016), IIT Bomthis work, we do not prepend a task prefix in the b"
2021.emnlp-main.125,D19-1252,0,0.0527437,"Missing"
2021.emnlp-main.125,2020.findings-emnlp.147,0,0.0582365,"Missing"
2021.emnlp-main.125,L18-1548,0,0.0349859,"i et al., tailment”, “contradiction”, or “neutral”. The input 2021a), we use the natural sentences from CCand target sequences are constructed as Net (Wenzek et al., 2019) in 94 languages Input: hbosi You have access to the facts. heosi for monolingual text-to-text tasks. For crossThe facts are accessible to you. heosi lingual text-to-text tasks, we use parallel corpora Output: hbosi entailment heosi of 14 English-centric language pairs, collected Since multi-task fine-tuning is not the focus of from MultiUN (Ziemski et al., 2016), IIT Bomthis work, we do not prepend a task prefix in the bay (Kunchukuttan et al., 2018), OPUS (Tiedeinput text. We also adopt a constrained decoding mann, 2012), and WikiMatrix (Schwenk et al., 1674 2019). Details of the pre-training data are described in Appendix. Training Details In the experiments, we consider the small-size Transformer model (Xue et al., 2020), with dmodel = 512, dff = 1, 024, 6 attention heads, and 8 layers for both the encoder and the decoder1 . We use the vocabulary provided by XLMR (Conneau et al., 2020), and extend it with 100 unique mask tokens for the span corruption tasks. We pretrain our M T6 for 0.5M steps with batches of 256 length-512 input seque"
2021.emnlp-main.125,2020.findings-emnlp.360,0,0.0333655,". Besides, under the setting with fewer training data, M T6 shows more improvements over M T5. Model es-en ru-en vi-en tr-en M T5 M T6 11.36 11.83 8.77 9.49 8.98 9.52 10.57 10.80 Table 3: ROUGE-2 scores on Wikilingua cross-lingual summarization. Results are averaged over three runs. Model XQuAD MLQA TyDiQA XNLI PAWS-X M T5 M T6 30.4 28.6 27.5 27.2 27.5 25.9 19.5 14.6 16.0 13.2 Table 4: The cross-lingual transfer gap scores on the XTREME tasks. A lower transfer gap score indicates better cross-lingual transferability. We use the EM scores to compute the gap scores for the QA tasks. Wikilingua (Ladhak et al., 2020) dataset containing passage-summary pairs in four language pairs. We fine-tune the models for 100K steps with a batch size of 32 and a learning rate of 0.0001. We use the greedy decoding for all evaluated models. The Cross-Lingual Summarization The cross- evaluation results are shown in Table 3, where lingual summarization task aims to generate M T6 outperforms M T5 on the test sets of four summaries in a different language. We use the language pairs. 1676 Averaged Accuracy 40 mT5 mT6 30 20 10 4 Layer 6 8 Figure 4: Evaluation results of different layers on Tatoeba cross-lingual sentence retrie"
2021.emnlp-main.125,2020.acl-main.653,0,0.538333,"procedure takes about 2.5 days on an Nvidia DGX-2 Station. Details of the pre-training hyperparameters are described in Appendix. 4.2 4.2.1 Results XTREME Cross-lingual Understanding To validate the performance of M T6, we evaluate the pretrained models on XTREME (Hu et al., 2020b), which is a widely used benchmark for cross-lingual understanding. Following M T5 (Xue et al., 2020), we consider six downstream tasks included by XTREME: the named entity recognition (NER) task on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset in 40 languages, the question answering (QA) task on MLQA (Lewis et al., 2020b), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020), the cross-lingual natural language inference task on XNLI (Conneau et al., 2018), and crosslingual paraphrase adversaries on PAWS-X (Yang et al., 2019). The models are evaluated under the cross-lingual transfer setting (Conneau et al., 2020; Hu et al., 2020b). Under this setting, the models should be fine-tuned only on English training data but evaluated on all target languages. Moreover, for each pretrained model, only one model is used for all languages rather than selecting fine-tuned models separately. Details of the"
2021.emnlp-main.125,W04-1013,0,0.0271001,"ion. RG is short for ROUGE. Results of XLM and XNLG are taken from (Chi et al., 2020). Results of M T5 and M T6 are averaged over three runs. lines as the input documents and summaries, respectively. The dataset consists of examples in the languages of English, French, and Chinese. For each language, it contains 500K, 5K, and 5K examples for the training, validation, and test, respectively. We fine-tune the models for 20 epochs with a batch size of 32 and a learning rate of 0.00001. During decoding, we use the greedy decoding for all evaluated models. As shown in Table 2, we report the ROUGE (Lin, 2004) scores of the models on Gigaword multilingual abstractive summarization. We observe that M T6 consistently outperforms M T5 on all the three target languages. Comparing with the XLM (Conneau and Lample, 2019) and XNLG (Chi et al., 2020) models with 800M parameters, our M T6 model achieves a similar performance with only 300M parameters. Besides, under the setting with fewer training data, M T6 shows more improvements over M T5. Model es-en ru-en vi-en tr-en M T5 M T6 11.36 11.83 8.77 9.49 8.98 9.52 10.57 10.80 Table 3: ROUGE-2 scores on Wikilingua cross-lingual summarization. Results are aver"
2021.emnlp-main.125,P19-1015,0,0.0907957,"optimizer (Kingma and Ba, 2015) with a linear learning rate scheduler. The pre-training procedure takes about 2.5 days on an Nvidia DGX-2 Station. Details of the pre-training hyperparameters are described in Appendix. 4.2 4.2.1 Results XTREME Cross-lingual Understanding To validate the performance of M T6, we evaluate the pretrained models on XTREME (Hu et al., 2020b), which is a widely used benchmark for cross-lingual understanding. Following M T5 (Xue et al., 2020), we consider six downstream tasks included by XTREME: the named entity recognition (NER) task on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset in 40 languages, the question answering (QA) task on MLQA (Lewis et al., 2020b), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020), the cross-lingual natural language inference task on XNLI (Conneau et al., 2018), and crosslingual paraphrase adversaries on PAWS-X (Yang et al., 2019). The models are evaluated under the cross-lingual transfer setting (Conneau et al., 2020; Hu et al., 2020b). Under this setting, the models should be fine-tuned only on English training data but evaluated on all target languages. Moreover, for each pretrained model, only one model is used"
2021.emnlp-main.125,tiedemann-2012-parallel,0,0.0711811,"Missing"
2021.emnlp-main.125,N18-1101,0,0.0159465,"e hsepi tag means the end of entity span. We use the following constrained decoding rules: (1) The model should decode entity tags or the end-of-sentence tag (heosi) after a hbosi token or a hsepi token; (2) Otherwise, the model should decode the tokens from the input sentence or the hsepi token for the other situations. Classification The goal of the text classification task is to predict the label of a given text. Following T5 (Raffel et al., 2020), we directly use the label text as the output text sequence. We provide an example for the MNLI natural language infer- 4 Experiments ence task (Williams et al., 2018). Given an input sentence pair of “You have access to the facts .” 4.1 Setup and “The facts are accessible to you .”, the goal is Data Following previous work on cross-lingual to classify the input into the relationships of “en- pre-training (Conneau et al., 2020; Chi et al., tailment”, “contradiction”, or “neutral”. The input 2021a), we use the natural sentences from CCand target sequences are constructed as Net (Wenzek et al., 2019) in 94 languages Input: hbosi You have access to the facts. heosi for monolingual text-to-text tasks. For crossThe facts are accessible to you. heosi lingual text"
2021.emnlp-main.2,N19-1388,0,0.0155195,"directly fine-tune all model parameters on WMT19 De-En training set. CRISS and m2m-100 are the state-of-the-art unsupervised and supervised multilingual NMT models, respectively. The CRISS model is initialized with the mBART model and iteratively finetuned on 1.8 billion sentences covering 90 language pairs. m2m-100 is trained with 7.5 billion parallel sentences across 2200 translation directions. The results of CRISS and m2m-100 are listed as reference, because CRISS and m2m-100 are manyto-many NMT models whose performance may degrade due to the competitions among different target languages (Aharoni et al., 2019; Zhang et al., 2020), while SixT is a many-to-one NMT model. The official m2m-100 model has three sizes: small (418M parameters), base (1.2B parameters) and large (12B parameters). The results of m2m-100 20 German De Nl Model # Sents mBART CRISS m2m-100 0.04B 1.8B 7.5B 27.4 28.8 28.0 43.3 47.0 48.5 24.7 32.2 30.0 SixT 0.04B 33.8 54.7 30.1 Es Romance Ro It Fi 28.2 35.4 34.1 29.8 48.9 50.0 18.8 23.9 24.9 33.9 43.0 26.3 Uralic Lv Indo-Aryan Ne Si East Asian Zh Ja Ko Gu Avg. Et Hi 14.2 18.6 19.9 15.7 23.5 25.8 12.3 23.1 21.9 9.6 14.7 3.7 7.2 14.4 10.6 10.3 19.0 0.4 8.3 13.4 19.5 6.0 7.9 11.5 21.1"
2021.emnlp-main.2,P17-1176,1,0.597432,"seen languages. SixT significantly outperforms mBART with an average improvement of 7.1 BLEU on zeroshot any-to-English translation across 14 source languages. Furthermore, with much less training computation cost and training data, the SixT model gets better performance on 15 any-to-English test sets than CRISS and m2m-100, two strong multilingual NMT baselines.1 2 NMT model is directly tested between the unseen language pairs lzi -to-lt in a zero-shot manner. Different from multilingual NMT (Johnson et al., 2017), unsupervised NMT (Lample et al., 2018) or zero-resource NMT through pivoting (Chen et al., 2017, 2018), neither the parallel nor monolingual data in the language lzi is directly accessible in the ZeXT task. The model has to rely on the offthe-shelf MPE to translate from language lzi . The challenge to this task is how to leverage an MPE for machine translation while preserving its crosslingual transferability. In this paper, we utilize XLM-R, which is jointly trained on 100 languages, as the off-the-shelf MPE. The ZeXT task calls for approaches to efficiently build a many-to-one NMT model that can translate from 100 languages supported by XLM-R with parallel dataset of only one language"
2021.emnlp-main.2,2020.acl-main.747,0,0.236641,"th the baselines. SixT gets 18.3 average BLEU and improves over the best baseline by 5.4 average BLEU, showing that SixT successfully learns to translate while preserving the cross-lingual transferability of XLM-R. For all language pairs, SixT obtains better transferring scores. In contrast, vanilla Transformer can hardly transfer and the other baselines do not well transfer to the distant languages. In addition to zero-shot performance, SixT also achieves the best result on De-En test set. Note that the best checkpoint is selected with zero-shot validation set for all methods. Previous work (Conneau et al., 2020; Hu et al., 2020) mainly uses XLM-R for cross-lingual transfer on NLU tasks. The experiments demonstrate that XLM-R can be also utilized for zero-shot neural machine translation if it is fine-tuned properly. We leave the exploration of cross-lingual transfer using XLM-R for other NLG tasks as the future work. Baselines We compare our model with vanilla Transformer and five conventional methods to apply pretrained Transformer encoder on NMT task. The pretrained encoders in these methods are replaced with XLM-R base for fair comparison. • Vanilla Transformer. The encoder is with the same size o"
2021.emnlp-main.2,2020.tacl-1.47,0,0.373676,")) while degrades with Resdrop (see results of (2)→(5) and (4)→(6)). This is expected since Resdrop helps to build a more language-agnostic encoder. Although Resdrop degrades supervised performance, it improves zero-shot translation. The zero-shot performance is related with both supervised performance and model transferability. By either enhancing the supervised performance (with TwoStage and BigDec) or the model transferability (with Resdrop), the overall performance of zero-shot translation can be improved. Analysis Comparison with multilingual NMT In this part, we compare SixT with mBART (Liu et al., 2020), CRISS (Tran et al., 2020) and m2m-100 (Fan et al., 2020) on any-to-English test sets. mBART is a strong pretrained multilingual encoderdecoder based Transformer explicitly designed for NMT. We follow their setting and directly fine-tune all model parameters on WMT19 De-En training set. CRISS and m2m-100 are the state-of-the-art unsupervised and supervised multilingual NMT models, respectively. The CRISS model is initialized with the mBART model and iteratively finetuned on 1.8 billion sentences covering 90 language pairs. m2m-100 is trained with 7.5 billion parallel sentences across 2200 tra"
2021.emnlp-main.2,D18-1398,1,0.602418,"is the layer normalization. Liu et al. (2021) aim at training a language-agnostic encoder for NMT using parallel corpus from scratch. Compared with them, our method shows that it’s possible to make a pretrained multilingual encoder more language-agnostic by relaxing the position constraint during fine-tuning. Capacity-enhanced decoder Some previous work (Zhu et al., 2020; Yang et al., 2020) incorporates BERT into NMT and configures the decoder size as Vaswani et al. (2017). For example, to train an NMT on Europarl De-En training dataset, the default decoder configuration is Transformer base (Gu et al., 2018; Currey et al., 2020). However, our model relies more on the decoder to learn from the labeled data, as the encoder is mainly responsible for cross-lingual transfer. This is also reflected in our training strategy: at the first stage only the decoder parameters are optimized, while at the second stage the encoder is only slightly fine-tuned to preserve its transferability. Therefore, the model capacity of SixT is smaller than vanilla Transformer with the same size. We propose to apply a capacityenhanced decoder that has larger dimension of feed forward network, more layers and more attention"
2021.emnlp-main.2,N19-4009,0,0.02752,"are optimized, while at the second stage the encoder is only slightly fine-tuned to preserve its transferability. Therefore, the model capacity of SixT is smaller than vanilla Transformer with the same size. We propose to apply a capacityenhanced decoder that has larger dimension of feed forward network, more layers and more attention heads at both the first and second training stages. The improvement brought by the big decoder is not simply because of more model parameters. More Model settings We use the XLM-R base model as the off-the-shelf MPE. The model is implemented on fairseq toolkit (Ott et al., 2019). We set Transformer encoder the same size as the XLMR base model. For the decoder, we use the same hyper-parameter setting as the encoder. We denote model with such configuration as SixT and use this configuration for our NMT models through the paper unless otherwise stated. The encoderdecoder attention modules are randomly initialized. We remove the residual connection at the 11-th (penultimate) encoder layer, which is selected on the validation dataset. For the empirical exploration in Table 1, we use two model configurations. For Strategy (1)–(7) where decoder layers are trained from scrat"
2021.emnlp-main.2,D19-5603,0,0.0395317,"rmer. The encoder is with the same size of XLM-R base, the decoder uses the size of BaseDec. All model parameters are randomly initialized. • +XLM-R fine-tune encoder (Conneau and Lample, 2019). The encoder is initialized with XLM-R. All parameters are trained. • +XLM-R fine-tune all (Conneau and Lample, 2019). All parameters except those of cross attention module are initialized with XLM-R and directly fine-tuned. • +XLM-R as encoder embedding (Zhu et al., 2020). The XLM-R output is leveraged as the encoder input of the NMT. The XLM-R model is fixed during training. • +Recycle XLM-R for NMT (Imamura and Sumita, 2019). The method initializes the encoder with XLM-R and only trains decoder at the first step. Then all are trained at the second step. • XLM-R fused model (Zhu et al., 2020). The XLM-R output is fused into encoder and decoder separately with attention mechanism. The encoder embedding is initialized from XLM-R to facilitate 4.3 Ablation Study We conduct an ablation study with the proposed SixT on the Europarl De-En training set, as shown 5 BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a +version.1.5.0 6 19 We use De-En validation dataset this time. Model De Nl Es It Ro Fi Lv Et Hi Ne Zh Avg. Vanilla"
2021.emnlp-main.2,P18-1007,0,0.0646372,"Missing"
2021.emnlp-main.2,D19-1077,0,0.0708843,"even do not have labeled data. Encoder Decoder Given that MPE has achieved great success in crossbody body lingual NLU tasks, a question worthy of research is how to perform zero-shot cross-lingual transfer Encoder Decoder in the NMT task by embed embed leveraging the MPE. Some work (Zhu et al., 2020; Yang et al., 2020; Weng (2) Training the second stageand Sumita, 2019) explores (1) Training in the first stage et al., in 2020; Imamura 1 Introduction approaches to improve NMT performance by inMultilingual pretrained encoders (MPE) such as corporating monolingual pretrained Transformer mBERT (Wu and Dredze, 2019), XLM (Con- encoder such as BERT (Devlin et al., 2019). Howneau and Lample, 2019), and XLM-R (Conneau ever, simply replacing the monolingual pretrained et al., 2020) have shown remarkably strong re- encoder in previous studies with MPE does not sults on zero-shot cross-lingual transfer mainly work well for cross-lingual transfer of NMT (see for natural language understanding (NLU) tasks, baselines in Table 2). Others propose to fine-tune including named entity recognition (NER), ques- the encoder-decoder-based multilingual pretrained tion answering (QA) and natural language infer- model for cr"
2021.emnlp-main.2,2020.emnlp-main.210,0,0.103365,"nneau ever, simply replacing the monolingual pretrained et al., 2020) have shown remarkably strong re- encoder in previous studies with MPE does not sults on zero-shot cross-lingual transfer mainly work well for cross-lingual transfer of NMT (see for natural language understanding (NLU) tasks, baselines in Table 2). Others propose to fine-tune including named entity recognition (NER), ques- the encoder-decoder-based multilingual pretrained tion answering (QA) and natural language infer- model for cross-lingual transfer of NMT (Liu et al., ence (NLI). These methods jointly train a Trans- 2020; Lin et al., 2020). It is still unclear how to former (Vaswani et al., 2017) encoder to perform conduct cross-lingual transfer for NMT model with ∗ existing multilingual pretrained encoders such as Contribution during internship at Microsoft Research. † Corresponding author. XLM-R. 15 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 15–26 c November 7–11, 2021. 2021 Association for Computational Linguistics In this paper, we focus on a Zero-shot crosslingual(X) NMT Transfer task (ZeXT, see Figure 1), which aims at translating multiple unseen languages by leveraging a"
2021.emnlp-main.2,2021.acl-long.101,0,0.37879,"her improve the translation performance by jointly fine-tuning all parameters except encoder embedding of the NMT.2 Since the decoder has been well adapted to the encoder at the first stage, we expect the model can be slightly fine-tuned to improve the translation capacity without losing the Position disentangled encoder The representations from XLM-R initialized encoder have a strong positional correspondence to the source sentence. The word order information inside is language-specific and may hinder the cross-lingual transfer from supervised source language to unseen languages. Inspired by Liu et al. (2021), we propose to relax this structural constraint and make the encoder outputs less position- and languagespecific. More specifically, at the second stage, we remove the residual connection after the selfattention sublayer in one of the encoder layers i 2 According to our preliminary experiment, the average BLEU is 0.2 lower when the encoder embedding is also learned at the second stage. Besides, freezing encoder embedding leads to higher computational efficiency. 17 Encoder body Decoder body Encoder body Decoder body Encoder embed Decoder embed Encoder embed Decoder embed (2) Training at the s"
2021.emnlp-main.2,2020.acl-main.148,0,0.0168613,"model parameters on WMT19 De-En training set. CRISS and m2m-100 are the state-of-the-art unsupervised and supervised multilingual NMT models, respectively. The CRISS model is initialized with the mBART model and iteratively finetuned on 1.8 billion sentences covering 90 language pairs. m2m-100 is trained with 7.5 billion parallel sentences across 2200 translation directions. The results of CRISS and m2m-100 are listed as reference, because CRISS and m2m-100 are manyto-many NMT models whose performance may degrade due to the competitions among different target languages (Aharoni et al., 2019; Zhang et al., 2020), while SixT is a many-to-one NMT model. The official m2m-100 model has three sizes: small (418M parameters), base (1.2B parameters) and large (12B parameters). The results of m2m-100 20 German De Nl Model # Sents mBART CRISS m2m-100 0.04B 1.8B 7.5B 27.4 28.8 28.0 43.3 47.0 48.5 24.7 32.2 30.0 SixT 0.04B 33.8 54.7 30.1 Es Romance Ro It Fi 28.2 35.4 34.1 29.8 48.9 50.0 18.8 23.9 24.9 33.9 43.0 26.3 Uralic Lv Indo-Aryan Ne Si East Asian Zh Ja Ko Gu Avg. Et Hi 14.2 18.6 19.9 15.7 23.5 25.8 12.3 23.1 21.9 9.6 14.7 3.7 7.2 14.4 10.6 10.3 19.0 0.4 8.3 13.4 19.5 6.0 7.9 11.5 21.1 24.8 32.7 18.4 25.0"
2021.emnlp-main.257,2020.emnlp-main.367,0,0.0571655,"Missing"
2021.emnlp-main.257,2020.tacl-1.30,0,0.0249007,"es on PAWS-X, POS and NER. three types of cross-lingual understanding tasks Then increasing the vocabulary from VO C AP250K from XTREME benchmark (Hu et al., 2020), into VO C AP500K mitigates the gap and bring imcluding two classification datasets: XNLI (Conprovements on six datasets except for PAWS-X, neau et al., 2018), PAWS-X (Yang et al., 2019), which only includes seven high-resource languages. three span extraction datasets: XQuAD (Artetxe However, increasing the size of vocabulary diet al., 2020), MLQA (Lewis et al., 2020), TyDiQArectly learned with Sentencepiece from J OINT250K GoldP (Clark et al., 2020), and two sequence labelto J OINT500K does not improve the performance ing datasets: NER (Pan et al., 2017), POS (Zeman as our VO C AP method does, showing the imporet al., 2019). The statistics of the datasets are shown tance of selecting language-specific subword units in the appendix. and leveraging how much vocabulary capacity each Implementation Details We adapt the Trans- language requires. former architecture from the base model setting Since increasing vocabulary size brings the isin Conneau et al. (2020), i.e., 12 layers and 768 sues of model size and pre-training speed, we study hidd"
2021.emnlp-main.257,2020.acl-main.747,0,0.24302,". In order to address the issues, we propose k-NN-based target sampling to accelerate the expensive softmax. Our experiments show that the multilingual vocabulary learned with VO C AP benefits cross-lingual language model pre-training. Moreover, k-NN-based target sampling mitigates the side-effects of increasing the vocabulary size while achieving comparable performance and faster pre-training speed. The code and the pretrained multilingual vocabularies are available at https://github. com/bozheng-hit/VoCapXLM. 1 Introduction Pretrained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2021b; Xue et al., 2020) have recently shown great success in improving cross-lingual transferability. These models encode texts from different languages into universal representations with a shared multilingual vocabulary and a shared Transformer encoder (Vaswani et al., 2017). By pretraining cross-lingual language models on the largescale multilingual corpus, the models achieve stateof-the-art performance on various downstream tasks, e.g., cross-lingual question answering and cross-lingual sentence classification. Although the Transformer architecture used in most pretrained mo"
2021.emnlp-main.257,P18-1007,0,0.374283,"020). Meanwhile, state-of-the-art pretrained cross-lingual language models use the shared multilingual vocabulary of 250K subword units to represent more than 100 languages (Conneau et al., 2020; Chi et al., 2021b; Xue et al., 2020). Although some subword units are shared across languages, no more than 2.5K language-specific subword units on average are allocated for each language, which is still relatively small. Besides, the multilingual vocabulary is trained on the combined multilingual corpus with subword segmentation algorithms like BPE (Sennrich et al., 2015) and unigram language model (Kudo, 2018). During vocabulary construction, these algorithms tend to select more subword units shared across languages with common scripts like Latin and Cyrillic (Chung et al., 2020b), but have a lower chance to select language-specific subword units. It is hard to determine how much vocabulary capacity a particular language requires and whether the shared multilingual vocabulary has allocated enough vocabulary capacity to represent the language. In this paper, we propose VO C AP, an algorithm to allocate large vocabulary for cross-lingual language model by separately evaluating the required vocabulary"
2021.emnlp-main.257,2020.acl-main.653,0,0.0420862,"uestion answering ness of our methods, we conduct experiments on datasets but degrades on PAWS-X, POS and NER. three types of cross-lingual understanding tasks Then increasing the vocabulary from VO C AP250K from XTREME benchmark (Hu et al., 2020), into VO C AP500K mitigates the gap and bring imcluding two classification datasets: XNLI (Conprovements on six datasets except for PAWS-X, neau et al., 2018), PAWS-X (Yang et al., 2019), which only includes seven high-resource languages. three span extraction datasets: XQuAD (Artetxe However, increasing the size of vocabulary diet al., 2020), MLQA (Lewis et al., 2020), TyDiQArectly learned with Sentencepiece from J OINT250K GoldP (Clark et al., 2020), and two sequence labelto J OINT500K does not improve the performance ing datasets: NER (Pan et al., 2017), POS (Zeman as our VO C AP method does, showing the imporet al., 2019). The statistics of the datasets are shown tance of selecting language-specific subword units in the appendix. and leveraging how much vocabulary capacity each Implementation Details We adapt the Trans- language requires. former architecture from the base model setting Since increasing vocabulary size brings the isin Conneau et al. (202"
2021.emnlp-main.257,2021.ccl-1.108,0,0.0879979,"Missing"
2021.emnlp-main.257,P17-1178,0,0.0880255,"us. 94 92 −280 −160 −280 −260 −240 −220 ALP −200 −180 −160 −150 Low res. Mid res. High res. Figure 3: F1 score on NER task with different vocabularies versus their ALP on the monolingual corpus. Figure 4: Comparison of vocabulary capacity of different-resourced languages. Shorter bars indicate larger vocabulary capacity. vocabularies for each language on the corresponding monolingual corpus, with vocabulary size ranging from 1K to 30K. Then we pretrain monolingual language models with the corresponding monolingual vocabularies. We evaluate these pretrained models on two downstream tasks: NER (Pan et al., 2017) and POS (Zeman et al., 2019) from the XTREME benchmark since there is annotated task data for a large number of languages. The vocabularies are learned on the reconstructed CommonCrawl corpus (Chi et al., 2021b; Conneau et al., 2020) using SentencePiece (Kudo and Richardson, 2018) with the unigram language model (Kudo, 2018). The unigram distributions are also counted on the CommonCrawl corpus. The Wikipedia corpus is used for all pre-training experiments in this paper since it is easier to run experiments due to its smaller size. More details about the pre-training data can be found in the a"
2021.emnlp-main.257,D19-1382,0,0.0210914,"ulary directly learned on multilingual corpus with 4.1 Setup SentencePiece, i.e., XLM-R250K and J OINT250K , Fine-Tuning Datasets To validate the effectiveour VO C AP250K improves on question answering ness of our methods, we conduct experiments on datasets but degrades on PAWS-X, POS and NER. three types of cross-lingual understanding tasks Then increasing the vocabulary from VO C AP250K from XTREME benchmark (Hu et al., 2020), into VO C AP500K mitigates the gap and bring imcluding two classification datasets: XNLI (Conprovements on six datasets except for PAWS-X, neau et al., 2018), PAWS-X (Yang et al., 2019), which only includes seven high-resource languages. three span extraction datasets: XQuAD (Artetxe However, increasing the size of vocabulary diet al., 2020), MLQA (Lewis et al., 2020), TyDiQArectly learned with Sentencepiece from J OINT250K GoldP (Clark et al., 2020), and two sequence labelto J OINT500K does not improve the performance ing datasets: NER (Pan et al., 2017), POS (Zeman as our VO C AP method does, showing the imporet al., 2019). The statistics of the datasets are shown tance of selecting language-specific subword units in the appendix. and leveraging how much vocabulary capacit"
2021.findings-acl.188,S17-2001,0,0.0155284,"l., 2019; Conneau et al., 2019) and task-agnostic distillation (Sun et al., 2019b; Jiao et al., 2019) work, we evaluate the English student models on GLUE benchmark and extractive question answering. The multilingual models are evaluated on cross-lingual natural language inference and cross-lingual question answering. GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classification tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) and QQP), and four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli 2144 Model Teacher #Param SQuAD2 MNLI QNLI QQP RTE SST MRPC CoLA STS Avg BERTBASE RoBERTaBASE - 109M 125M 76.8 83.7 84.5 87.6 91.7 92.8 91.3 91.9 68.6 78.7 93.2 94.8 87.3 90.2 58.9 63.6 89.5 91.2 82.4 86.1 12×768 Ours 12×768 Ours BERTLARGE RoBERTaLARGE 109M 125M 81.8 86.6 86.5 89.4 92.6 94.0 91.6 91.8 76.4 83.1 93.3 95.9 89.2 91.2 62.3 65.0 90.5 91.3 84.9 87.6 Table 3: Results of our 12×768 models on the dev sets"
2021.findings-acl.188,2021.naacl-main.280,1,0.810564,"Missing"
2021.findings-acl.188,P19-4007,0,0.249501,"en split them according to the desired number of relation heads. We choose to transfer Q-Q, K-K and V-V self-attention relations to achieve a balance between performance and training speed. For large-size teacher, we transfer the self-attention knowledge of an upper middle layer of the teacher. For base-size teacher, using the last layer achieves better performance. 2019a) employ a standard encoder-decoder structure and pretrain the decoder auto-regressively. Besides monolingual pretrained models, multilingual pretrained models (Devlin et al., 2018; Lample and Conneau, 2019; Chi et al., 2019; Conneau et al., 2019; Chi et al., 2020) also advance the state-of-theart on cross-lingual understanding and generation. 2.3 Knowledge Distillation Knowledge distillation has been proven to be a promising way to compress large models while maintaining accuracy. Knowledge of a single or an ensemble of large models is used to guide the training of small models. Hinton et al. (2015) propose to use soft target probabilities to train student models. More fine-grained knowledge such as hidden states (Romero et al., 2015) and attention distributions (Zagoruyko and Komodakis, 2017; Hu et al., 2018) are introduced to impro"
2021.findings-acl.188,D18-1269,0,0.0264397,"Avg M INI LM (Last Layer) + Upper Middle Layer 79.1 80.3 84.7 85.2 91.2 91.5 85.0 85.7 12×384 Ours 80.7 85.7 92.3 86.2 Table 4: Comparison of different methods using BERTLARGE-WWM as the teacher. We report dev results of 12×384 student model with 128 embedding size. et al., 2009) and WNLI (Levesque et al., 2012)). Extractive Question Answering The task aims to predict a continuous sub-span of the passage to answer the question. We evaluate on SQuAD 2.0 (Rajpurkar et al., 2018), which has been served as a major question answering benchmark. Cross-lingual Natural Language Inference (XNLI) XNLI (Conneau et al., 2018) is a cross-lingual classification benchmark. It aims to identity the semantic relationship between two sentences and provides instances in 15 languages. Cross-lingual Question Answering We use MLQA (Lewis et al., 2019b) to evaluate multilingual models. MLQA extends English SQuAD dataset (Rajpurkar et al., 2016) to seven languages. 4.3 Main Results Table 1 presents the dev results of 6×384 and 6×768 models distilled from BERTBASE , BERTLARGE and RoBERTaLARGE on GLUE and SQuAD 2.0. (1) Previous methods (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019a; Wang et al., 2020) usually distill"
2021.findings-acl.188,I05-5002,0,0.0319675,"ing (Devlin et al., 2018; Liu et al., 2019; Conneau et al., 2019) and task-agnostic distillation (Sun et al., 2019b; Jiao et al., 2019) work, we evaluate the English student models on GLUE benchmark and extractive question answering. The multilingual models are evaluated on cross-lingual natural language inference and cross-lingual question answering. GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classification tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) and QQP), and four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli 2144 Model Teacher #Param SQuAD2 MNLI QNLI QQP RTE SST MRPC CoLA STS Avg BERTBASE RoBERTaBASE - 109M 125M 76.8 83.7 84.5 87.6 91.7 92.8 91.3 91.9 68.6 78.7 93.2 94.8 87.3 90.2 58.9 63.6 89.5 91.2 82.4 86.1 12×768 Ours 12×768 Ours BERTLARGE RoBERTaLARGE 109M 125M 81.8 86.6 86.5 89.4 92.6 94.0 91.6 91.8 76.4 83.1 93.3 95.9 89.2 91.2 62.3 65.0 90.5 91.3 84.9 87.6 Table 3: Results of our 12×7"
2021.findings-acl.188,W07-1401,0,0.0271052,"k and extractive question answering. The multilingual models are evaluated on cross-lingual natural language inference and cross-lingual question answering. GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classification tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) and QQP), and four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli 2144 Model Teacher #Param SQuAD2 MNLI QNLI QQP RTE SST MRPC CoLA STS Avg BERTBASE RoBERTaBASE - 109M 125M 76.8 83.7 84.5 87.6 91.7 92.8 91.3 91.9 68.6 78.7 93.2 94.8 87.3 90.2 58.9 63.6 89.5 91.2 82.4 86.1 12×768 Ours 12×768 Ours BERTLARGE RoBERTaLARGE 109M 125M 81.8 86.6 86.5 89.4 92.6 94.0 91.6 91.8 76.4 83.1 93.3 95.9 89.2 91.2 62.3 65.0 90.5 91.3 84.9 87.6 Table 3: Results of our 12×768 models on the dev sets of GLUE benchmark and SQuAD 2.0. The fine-tuning results are an average of 4 runs for each task. We report F1 for SQuAD 2.0, Pearson correlation for STS-B, Matthews corre"
2021.findings-acl.188,D18-1232,1,0.896184,"Missing"
2021.findings-acl.188,2020.emnlp-main.242,0,0.0131517,"t probabilities to train student models. More fine-grained knowledge such as hidden states (Romero et al., 2015) and attention distributions (Zagoruyko and Komodakis, 2017; Hu et al., 2018) are introduced to improve the student model. In this work, we focus on task-agnostic knowledge distillation of pretrained Transformers. The distilled task-agnostic model can be fine-tuned to adapt to downstream tasks. It can also be utilized to initialize task-specific distillation (Sun et al., 2019a; Turc et al., 2019; Aguilar et al., 2019; Mukherjee and Awadallah, 2020; Xu et al., 2020; Hou et al., 2020; Li et al., 2020), which uses a finetuned teacher model to guide the training of the student on specific tasks. Knowledge used for distillation and layer mapping function are two key points for task-agnostic distillation of pretrained Transformers. Most previous work uses soft target probabilities, hidden states, self-attention distributions and value-relation to train the student model. For the layer mapping function, TinyBERT (Jiao et al., 2019) uses a uniform strategy to map teacher and student layers. MobileBERT (Sun et al., 2019b) assumes the student has the same number of layers as its teacher to perform"
2021.findings-acl.188,2021.ccl-1.108,0,0.0476769,"Missing"
2021.findings-acl.188,2020.acl-main.202,0,0.139384,"ng of small models. Hinton et al. (2015) propose to use soft target probabilities to train student models. More fine-grained knowledge such as hidden states (Romero et al., 2015) and attention distributions (Zagoruyko and Komodakis, 2017; Hu et al., 2018) are introduced to improve the student model. In this work, we focus on task-agnostic knowledge distillation of pretrained Transformers. The distilled task-agnostic model can be fine-tuned to adapt to downstream tasks. It can also be utilized to initialize task-specific distillation (Sun et al., 2019a; Turc et al., 2019; Aguilar et al., 2019; Mukherjee and Awadallah, 2020; Xu et al., 2020; Hou et al., 2020; Li et al., 2020), which uses a finetuned teacher model to guide the training of the student on specific tasks. Knowledge used for distillation and layer mapping function are two key points for task-agnostic distillation of pretrained Transformers. Most previous work uses soft target probabilities, hidden states, self-attention distributions and value-relation to train the student model. For the layer mapping function, TinyBERT (Jiao et al., 2019) uses a uniform strategy to map teacher and student layers. MobileBERT (Sun et al., 2019b) assumes the student ha"
2021.findings-acl.188,P18-2124,0,0.0738541,"Missing"
2021.findings-acl.188,D16-1264,0,0.303169,"al., 2019) work, we evaluate the English student models on GLUE benchmark and extractive question answering. The multilingual models are evaluated on cross-lingual natural language inference and cross-lingual question answering. GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classification tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) and QQP), and four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli 2144 Model Teacher #Param SQuAD2 MNLI QNLI QQP RTE SST MRPC CoLA STS Avg BERTBASE RoBERTaBASE - 109M 125M 76.8 83.7 84.5 87.6 91.7 92.8 91.3 91.9 68.6 78.7 93.2 94.8 87.3 90.2 58.9 63.6 89.5 91.2 82.4 86.1 12×768 Ours 12×768 Ours BERTLARGE RoBERTaLARGE 109M 125M 81.8 86.6 86.5 89.4 92.6 94.0 91.6 91.8 76.4 83.1 93.3 95.9 89.2 91.2 62.3 65.0 90.5 91.3 84.9 87.6 Table 3: Results of our 12×768 models on the dev sets of GLUE benchmark and SQuAD 2.0. The fine-tuning results are an average of 4 runs for each task."
2021.findings-acl.188,D13-1170,0,0.00971792,"ts using 8 V100 GPUs with mixed precision training. 4.2 Downstream Tasks Following previous pre-training (Devlin et al., 2018; Liu et al., 2019; Conneau et al., 2019) and task-agnostic distillation (Sun et al., 2019b; Jiao et al., 2019) work, we evaluate the English student models on GLUE benchmark and extractive question answering. The multilingual models are evaluated on cross-lingual natural language inference and cross-lingual question answering. GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classification tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) and QQP), and four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli 2144 Model Teacher #Param SQuAD2 MNLI QNLI QQP RTE SST MRPC CoLA STS Avg BERTBASE RoBERTaBASE - 109M 125M 76.8 83.7 84.5 87.6 91.7 92.8 91.3 91.9 68.6 78.7 93.2 94.8 87.3 90.2 58.9 63.6 89.5 91.2 82.4 86.1 12×768 Ours 12×768 Ours BERTLARGE RoBERTaLARGE 109M 125M 81.8 86.6 86.5 89.4 92"
2021.findings-acl.188,D19-1441,0,0.363617,"rs 6×768 Ours BERTBASE BERTLARGE RoBERTaLARGE 66M 66M 81M ×2.0 ×2.0 ×2.0 76.3 77.7 81.6 84.2 85.0 87.0 90.8 91.4 92.7 91.1 91.1 91.4 72.1 73.0 78.7 92.4 92.5 94.5 88.9 88.9 90.4 52.5 53.9 54.0 81.0 81.7 83.8 BERTBASE RoBERTaBASE BERTSMALL Truncated BERTBASE Truncated RoBERTaBASE DistilBERT TinyBERT M INI LM Table 1: Results of our students distilled from base-size and large-size teachers on the development sets of GLUE and SQuAD 2.0. We report F1 for SQuAD 2.0, Matthews correlation coefficient for CoLA, and accuracy for other datasets. The GLUE results of DistilBERT are taken from Sanh et al. (2019). The rest results of DistilBERT, TinyBERT2 , BERTSMALL , Truncated BERTBASE and M INI LM are taken from Wang et al. (2020). BERTSMALL (Turc et al., 2019) is trained using the MLM objective, without using KD. We also report the results of truncated BERTBASE and truncated RoBERTaBASE , which drops the top 6 layers of the base model. Top-layer dropping has been proven to be a strong baseline (Sajjad et al., 2020). The fine-tuning results are an average of 4 runs. the restriction on the number of attention heads of students, which is required to be the same as its teacher. To introduce more fine-"
2021.findings-acl.188,N18-1101,0,0.0215865,"on (Sun et al., 2019b; Jiao et al., 2019) work, we evaluate the English student models on GLUE benchmark and extractive question answering. The multilingual models are evaluated on cross-lingual natural language inference and cross-lingual question answering. GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classification tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) and QQP), and four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli 2144 Model Teacher #Param SQuAD2 MNLI QNLI QQP RTE SST MRPC CoLA STS Avg BERTBASE RoBERTaBASE - 109M 125M 76.8 83.7 84.5 87.6 91.7 92.8 91.3 91.9 68.6 78.7 93.2 94.8 87.3 90.2 58.9 63.6 89.5 91.2 82.4 86.1 12×768 Ours 12×768 Ours BERTLARGE RoBERTaLARGE 109M 125M 81.8 86.6 86.5 89.4 92.6 94.0 91.6 91.8 76.4 83.1 93.3 95.9 89.2 91.2 62.3 65.0 90.5 91.3 84.9 87.6 Table 3: Results of our 12×768 models on the dev sets of GLUE benchmark and SQuAD 2.0. The fine-tuning results are an a"
2021.findings-acl.188,2020.emnlp-main.633,1,0.858797,"l. (2015) propose to use soft target probabilities to train student models. More fine-grained knowledge such as hidden states (Romero et al., 2015) and attention distributions (Zagoruyko and Komodakis, 2017; Hu et al., 2018) are introduced to improve the student model. In this work, we focus on task-agnostic knowledge distillation of pretrained Transformers. The distilled task-agnostic model can be fine-tuned to adapt to downstream tasks. It can also be utilized to initialize task-specific distillation (Sun et al., 2019a; Turc et al., 2019; Aguilar et al., 2019; Mukherjee and Awadallah, 2020; Xu et al., 2020; Hou et al., 2020; Li et al., 2020), which uses a finetuned teacher model to guide the training of the student on specific tasks. Knowledge used for distillation and layer mapping function are two key points for task-agnostic distillation of pretrained Transformers. Most previous work uses soft target probabilities, hidden states, self-attention distributions and value-relation to train the student model. For the layer mapping function, TinyBERT (Jiao et al., 2019) uses a uniform strategy to map teacher and student layers. MobileBERT (Sun et al., 2019b) assumes the student has the same number"
2021.findings-acl.188,D19-1374,0,0.0164205,"inilm. 1 ing in real-life applications due to the restrictions of computation resources and latency. Knowledge distillation (KD; Hinton et al. 2015, Romero et al. 2015) has been widely employed to compress pretrained Transformers, which transfers knowledge of the large model (teacher) to the small model (student) by minimizing the differences between teacher and student features. Soft target probabilities (soft labels) and intermediate representations are usually utilized to perform KD training. In this work, we focus on task-agnostic compression of pretrained Transformers (Sanh et al., 2019; Tsai et al., 2019; Jiao et al., 2019; Sun et al., 2019b; Wang et al., 2020). The student models are distilled from large pretrained Transformers using large-scale text corpora. The distilled task-agnostic model can be directly finetuned on downstream tasks, and can be utilized to initialize task-specific distillation. DistilBERT (Sanh et al., 2019) uses soft target probabilities for masked language modeling predictions and embedding outputs to train the student. The student model is initialized from the teacher by taking one layer out of two. TinyBERT (Jiao et al., 2019) utilizes hidden states and self-attenti"
2021.findings-acl.372,D18-1045,0,0.017158,"bserved in So 4260 BLEU with different model size WMT'14 En-De BLEU 30 28 26 24 22 0 25 50 75 100 125 Model size (million) Transformer DARTSformer 150 175 200 Figure 5: BLEU comparison between DARTSformer and standard Transformers with different model sizes. et al. (2019). Based on this observation, DARTSformer is more pronounced for environments with resource limitations, such as mobile phones. A possible reason for the decreased performance gap at larger model sizes is that the effect of overfitting becomes more important. We expect that some data augmentation skills (Sennrich et al., 2015; Edunov et al., 2018; Qu et al., 2020) might be of help. 4.4 e d BLEU Tiny Small Medium DARTSformer 128 128 256 256 120 240 480 960 24.2 26.3 27.5 28.4 DARTS + Transformer Transformer 320 - 320 - 27.7 27.7 Search Settings The Impact of Search Hidden Size The main motivation for our presented method is that we want to search in a large hidden size to reduce the performance gap between searching and re-training. However, whether this gap exists needs rigorous validation. Otherwise, it would suffice to instead use a small hidden size d in architecture search, and then increase d after search for training the actual"
2021.findings-acl.372,D19-1367,0,0.0244248,"0 GPU. Limited by GPU memory, DARTS in Transformers has to search in small sizes while evaluating in large sizes, which will cause performance gaps (Chen et al., 2019). Introduction Current neural architecture search (NAS) studies have produced models that surpass the performance of those designed by humans (Real et al., 2019; Lu et al., 2020). For sequence tasks, efforts are made in reinforcement learning-based (Pham et al., 2018) and evolution-based (So et al., 2019; Wang et al., 2020) methods, which suffer from the huge computational cost. Instead, gradient-based methods (Liu et al., 2018; Jiang et al., 2019; Yang et al., 2020) are less demanding in computing resources and easy to implement, attracting many attentions recently. The idea of gradient-based NAS is to train a super network covering all candidate operations. Different sub-graphs of the super network form the search space. To find a well-performing subgraph, Liu et al. (2018) (DARTS) introduced search parameters jointly optimized with the network weights. Operations corresponding to the largest search parameters are kept for each intermediate node after searching. A limitation of DARTS is its memory inefficiency because it needs to sto"
2021.findings-acl.372,D19-1437,0,0.0271931,"l. (2019) invented different reversible architectures based on the ResNet (He et al., 2016). MacKay et al. (2018) extended RevNets to the recurrent network, which is particularly memory-efficient. Bai et al. (2019, 2020) conducted experiments with reversible Transformers by fixed point iteration. Kitaev et al. (2020) combined local sensitive hashing attention with reversible transformers to save memory in training with long sequences. An important application of reversible networks is the flow-based models (Kingma and Dhariwal, 2018; Huang et al., 2018; Tran et al., 2019). For sequence tasks, Ma et al. (2019) achieved success in non-autoregressive machine translation. 6 Conclusion We have proposed a memory-efficient differentiable architecture search (DARTS) method on sequence-to-sequence tasks. In particular, we have first devised a multi-split reversible network whose intermediate layer outputs can be reconstructed from top to bottom by the last layer’s output. We have then combined this reversible network with DARTS and developed a backpropagation-withreconstruction algorithm to significantly relieve the memory burden during the gradient-based architecture search process. We have validated the"
2021.findings-acl.372,C18-1250,0,0.0273577,"the network weights simultaneously. After searching, one can have a ready-to-run network. We use a simple one-stage NAS algorithm (Guo et al., 2020) as a baseline in Section 4.1. (Z190001), National Key Research and Development Project of China (No. 2018AAA0101004), and Beijing Academy of Artificial Intelligence (BAAI). References Reversible networks The idea of reversible networks is first introduced by RevNets (Gomez et al., 2017). Later on, Jacobsen et al. (2018); Chang et al. (2018); Behrmann et al. (2019) invented different reversible architectures based on the ResNet (He et al., 2016). MacKay et al. (2018) extended RevNets to the recurrent network, which is particularly memory-efficient. Bai et al. (2019, 2020) conducted experiments with reversible Transformers by fixed point iteration. Kitaev et al. (2020) combined local sensitive hashing attention with reversible transformers to save memory in training with long sequences. An important application of reversible networks is the flow-based models (Kingma and Dhariwal, 2018; Huang et al., 2018; Tran et al., 2019). For sequence tasks, Ma et al. (2019) achieved success in non-autoregressive machine translation. 6 Conclusion We have proposed a memo"
2021.findings-acl.372,P02-1040,0,0.114487,"sists of 36 million training sentence pairs. (3) WMT’18 English-Czech (En-Cs), again without ParaCrawl, which consists of 15.8 million training sentence pairs. Tokenization is done by Moses2 . We employ BPE (Sennrich et al., 2016) to generate a shared vocabulary for each language pair. The BPE merge operation numbers are 32K (WMT’18 En-De), 40K (WMT’14 En-Fr), 32K (WMT’18 En-Cs). We discard sentences longer than 250 tokens. For the retraining validation set, we randomly choose 3300 2 https://github.com/moses-smt/mosesdecoder sentence pairs from the training set. The evaluation metric is BLEU (Papineni et al., 2002). We use beam search for test sets with a beam size of 5, and we tune the length penalty parameter from 0.5 to 1.0. Suppose the input length is m, and the maximum output length is 1.2m + 10. 3.2 Search Configuration The architecture searches are all run on WMT’14 En-De. DARTS is a bilevel optimization process, which updates network weights θ on one dataset and search parameters α on another dataset. We split the 4.5 million sentence pairs into 2.5/2.0 million for θ and α. Both Ltrain and Lval are cross entropy loss with a label smoothing factor of 0.1. The split number n is 2 for the encoder a"
2021.findings-acl.372,P16-1009,0,0.0703353,"Missing"
2021.findings-acl.372,P16-1162,0,0.0342308,"he search process is the most memory intensive part, such that we use BP-with-reconstruction as shown in Line 2-5 of Algorithm 2. 3 Experiment Setup 3.1 Datasets We use three standard datasets to perform our experiments as So et al. (2019): (1) WMT’18 EnglishGerman (En-De) without ParaCrawl, which consists of 4.5 million training sentence pairs. (2) WMT’14 French-English (En-Fr), which consists of 36 million training sentence pairs. (3) WMT’18 English-Czech (En-Cs), again without ParaCrawl, which consists of 15.8 million training sentence pairs. Tokenization is done by Moses2 . We employ BPE (Sennrich et al., 2016) to generate a shared vocabulary for each language pair. The BPE merge operation numbers are 32K (WMT’18 En-De), 40K (WMT’14 En-Fr), 32K (WMT’18 En-Cs). We discard sentences longer than 250 tokens. For the retraining validation set, we randomly choose 3300 2 https://github.com/moses-smt/mosesdecoder sentence pairs from the training set. The evaluation metric is BLEU (Papineni et al., 2002). We use beam search for test sets with a beam size of 5, and we tune the length penalty parameter from 0.5 to 1.0. Suppose the input length is m, and the maximum output length is 1.2m + 10. 3.2 Search Config"
2021.findings-acl.372,P19-1355,0,0.0673408,"Missing"
2021.findings-acl.372,2020.acl-main.686,0,0.17934,"bone model of DARTS. Experiments are run on a single step of forward-backward pass on a batch of 3584 tokens with a NVIDIA P100 GPU. Limited by GPU memory, DARTS in Transformers has to search in small sizes while evaluating in large sizes, which will cause performance gaps (Chen et al., 2019). Introduction Current neural architecture search (NAS) studies have produced models that surpass the performance of those designed by humans (Real et al., 2019; Lu et al., 2020). For sequence tasks, efforts are made in reinforcement learning-based (Pham et al., 2018) and evolution-based (So et al., 2019; Wang et al., 2020) methods, which suffer from the huge computational cost. Instead, gradient-based methods (Liu et al., 2018; Jiang et al., 2019; Yang et al., 2020) are less demanding in computing resources and easy to implement, attracting many attentions recently. The idea of gradient-based NAS is to train a super network covering all candidate operations. Different sub-graphs of the super network form the search space. To find a well-performing subgraph, Liu et al. (2018) (DARTS) introduced search parameters jointly optimized with the network weights. Operations corresponding to the largest search parameters"
2021.findings-acl.394,W07-1401,0,0.155731,"8 66.7 67.3 91.0 90.9 Model Table 1: Comparisons between our models and previous pretrained models on GLUE dev set. Reported results are medians over five random seeds. on both the base-size and small-size model, the best configuration is γ = 1. The detailed pre-training configurations are provided in the supplemental materials. 5.2 Results on GLUE Benchmark The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) is a collection of diverse natural language understanding (NLU) tasks, including inference tasks (MNLI, QNLI, RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018; Rajpurkar et al. 2016b), similarity and paraphrase tasks (MRPC, QQP, STSB; Dolan and Brockett 2005; Cer et al. 2017), and single-sentence tasks (CoLA, SST-2; Warstadt et al. 2018; Socher et al. 2013). The detailed descriptions of GLUE datasets are provided in the supplementary materials. The evaluation metrics are Spearman correlation for STS-B, Matthews correlation for CoLA, and accuracy for the other GLUE tasks. For small-size settings, we use the hyperparameter configuration as suggested in (Clark et al., 2020a). For base-size settings, we con"
2021.findings-acl.394,S17-2001,0,0.0337325,"both the base-size and small-size model, the best configuration is γ = 1. The detailed pre-training configurations are provided in the supplemental materials. 5.2 Results on GLUE Benchmark The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) is a collection of diverse natural language understanding (NLU) tasks, including inference tasks (MNLI, QNLI, RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018; Rajpurkar et al. 2016b), similarity and paraphrase tasks (MRPC, QQP, STSB; Dolan and Brockett 2005; Cer et al. 2017), and single-sentence tasks (CoLA, SST-2; Warstadt et al. 2018; Socher et al. 2013). The detailed descriptions of GLUE datasets are provided in the supplementary materials. The evaluation metrics are Spearman correlation for STS-B, Matthews correlation for CoLA, and accuracy for the other GLUE tasks. For small-size settings, we use the hyperparameter configuration as suggested in (Clark et al., 2020a). For base-size settings, we consider a limited hyperparameter searching for each task, with learning rates ∈ {5e-5, 1e-4, 1.5e-4} and training epochs ∈ {3, 4, 5}. The remaining hyperparameters ar"
2021.findings-acl.394,2020.emnlp-main.20,0,0.211754,"t al., 2020; Raffel et al., 2020). The most representative pretext task is masked language modeling (MLM), which is introduced to pretrain a bidirectional BERT (Devlin et al., 2019) encoder. RoBERTa (Liu et al., 2019) apply several strategies to enhance the BERT performance, including training with more data and dynamic masking. UniLM (Dong et al., 2019; Bao et al., 2020) extend the mask prediction to generation tasks by adding the auto-regressive objectives. XLNet (Yang et al., 2019) propose the permuted language modeling to learn the dependencies among the masked tokens. Besides, E LEC TRA (Clark et al., 2020a) propose a novel training objective called replaced token detection which is defined over all input tokens. Moreover, E LEC TRIC (Clark et al., 2020b) extends the idea of E LEC TRA by energy-based cloze models. Some prior efforts demonstrate that sampling more hard examples is conducive to more effective training. Lin et al. (2017) propose the focal loss in order to focus on more hard examples. Generative adversarial networks (Goodfellow et al., 2014) is trained to maximize the probability of the discriminator making a mistake, which is closely related to E LECTRA’s training framework. In th"
2021.findings-acl.394,2021.ccl-1.108,0,0.0456304,"Missing"
2021.findings-acl.394,N18-1202,0,0.0406331,"to E LECTRA for training from scratch. Experimental results on various tasks show that our methods outperform E LECTRA despite the simplicity. Specifically, under the small-size setting, our model performance is 0.9 higher than E LECTRA on MNLI (Williams et al., 2018) and 4.2 higher on SQuAD 2.0 (Rajpurkar et al., 2016a), respectively. Under the base-size setting, our model performance is 0.26 higher than E LECTRA on MNLI and 0.52 higher on SQuAD 2.0, respectively. 2 Related Work State-of-the-art NLP models are mostly pretrained on a large unlabeled corpus with the self-supervised objectives (Peters et al., 2018; Lan et al., 2020; Raffel et al., 2020). The most representative pretext task is masked language modeling (MLM), which is introduced to pretrain a bidirectional BERT (Devlin et al., 2019) encoder. RoBERTa (Liu et al., 2019) apply several strategies to enhance the BERT performance, including training with more data and dynamic masking. UniLM (Dong et al., 2019; Bao et al., 2020) extend the mask prediction to generation tasks by adding the auto-regressive objectives. XLNet (Yang et al., 2019) propose the permuted language modeling to learn the dependencies among the masked tokens. Besides, E LE"
2021.findings-acl.394,D16-1264,0,0.269278,"ing cross-entropy loss. The method adaptively downweights the well-predicted replacements for MLM, which avoids sampling too many correct tokens as replacements. We conduct pre-training experiments on the WikiBooks corpus for both small-size and base-size models. The proposed techniques are plugged into E LECTRA for training from scratch. Experimental results on various tasks show that our methods outperform E LECTRA despite the simplicity. Specifically, under the small-size setting, our model performance is 0.9 higher than E LECTRA on MNLI (Williams et al., 2018) and 4.2 higher on SQuAD 2.0 (Rajpurkar et al., 2016a), respectively. Under the base-size setting, our model performance is 0.26 higher than E LECTRA on MNLI and 0.52 higher on SQuAD 2.0, respectively. 2 Related Work State-of-the-art NLP models are mostly pretrained on a large unlabeled corpus with the self-supervised objectives (Peters et al., 2018; Lan et al., 2020; Raffel et al., 2020). The most representative pretext task is masked language modeling (MLM), which is introduced to pretrain a bidirectional BERT (Devlin et al., 2019) encoder. RoBERTa (Liu et al., 2019) apply several strategies to enhance the BERT performance, including training"
2021.findings-acl.394,D13-1170,0,0.00344594,"etailed pre-training configurations are provided in the supplemental materials. 5.2 Results on GLUE Benchmark The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) is a collection of diverse natural language understanding (NLU) tasks, including inference tasks (MNLI, QNLI, RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018; Rajpurkar et al. 2016b), similarity and paraphrase tasks (MRPC, QQP, STSB; Dolan and Brockett 2005; Cer et al. 2017), and single-sentence tasks (CoLA, SST-2; Warstadt et al. 2018; Socher et al. 2013). The detailed descriptions of GLUE datasets are provided in the supplementary materials. The evaluation metrics are Spearman correlation for STS-B, Matthews correlation for CoLA, and accuracy for the other GLUE tasks. For small-size settings, we use the hyperparameter configuration as suggested in (Clark et al., 2020a). For base-size settings, we consider a limited hyperparameter searching for each task, with learning rates ∈ {5e-5, 1e-4, 1.5e-4} and training epochs ∈ {3, 4, 5}. The remaining hyperparameters are the same as E LECTRA. We report the median performance on the dev set over five d"
2021.findings-acl.394,N18-1101,0,0.240925,", 2017) for the generator’s MLM task, rather than using cross-entropy loss. The method adaptively downweights the well-predicted replacements for MLM, which avoids sampling too many correct tokens as replacements. We conduct pre-training experiments on the WikiBooks corpus for both small-size and base-size models. The proposed techniques are plugged into E LECTRA for training from scratch. Experimental results on various tasks show that our methods outperform E LECTRA despite the simplicity. Specifically, under the small-size setting, our model performance is 0.9 higher than E LECTRA on MNLI (Williams et al., 2018) and 4.2 higher on SQuAD 2.0 (Rajpurkar et al., 2016a), respectively. Under the base-size setting, our model performance is 0.26 higher than E LECTRA on MNLI and 0.52 higher on SQuAD 2.0, respectively. 2 Related Work State-of-the-art NLP models are mostly pretrained on a large unlabeled corpus with the self-supervised objectives (Peters et al., 2018; Lan et al., 2020; Raffel et al., 2020). The most representative pretext task is masked language modeling (MLM), which is introduced to pretrain a bidirectional BERT (Devlin et al., 2019) encoder. RoBERTa (Liu et al., 2019) apply several strategies"
2021.findings-acl.40,2020.findings-emnlp.414,0,0.0284982,"-words embedding to initialize. As shown in Figure 3, the word ‘lymphoma’ is not included in BERT vocabulary. We tokenize it into three subwords (lym, ##pho, ##ma). The embedding 3.3 Vocabulary Expansion Vocabulary expansion is the core module of AdaLM. It augments domain-specific terms or subword units to leverage domain knowledge. The size of the incremental vocabulary is a vital parameter for vocabulary expansion. Considering that unigram language modeling (Kudo, 2018) aligns more closely with morphology and avoids problems stemming from BPE’s greedy construction procedure, as proposed in (Bostrom and Durrett, 2020), we followed Kudo (2018) and introduced a corpus occurrence probability as a metric to optimize the size of incremental vocabulary automati462 -200 Initial from Original BERT -210 Transformer Encoder -220 -230 Expanded Embedding [29709 × 768] Original Embedding [30522 × 768] -240 -250 recently 3728 lymphoma 30735 entity 9178 developed 2764 quickly 2856 -260 30 recently lymphoma entity developed quickly Figure 3: Concatenate original embedding with expanded embedding. cally. We assume that each subword occurs independently and we assign to each subword in the corpus a probability equal to its"
2021.findings-acl.40,W04-1213,0,0.190167,"/arxiv 4 https://microsoft.github.io/BLURB/ 3 Test 8,662 16,364 15,745 Train 1,688 3,219 Task 114 455 Test 139 974 Classes 6 7 Table 2: Computer science dataset used in our experiment. We use the same train, development, and test splits as Gururangan et al. (2020) Fine-tuning tasks: For the biomedical domain, we choose three tasks: named entity recognition (NER), evidence-based medical information extraction (PICO), and relation extraction (RE). We perform entity-level F1 in NER task and wordlevel macro-F1 in the PICO task. The RE task uses the micro-F1 of positive classes evaluation. JNLPBA (Collier and Kim, 2004) NER dataset contains 6,892 disease mentions, which are mapped to 790 unique disease concepts with BIO tagging (Ramshaw and Marcus, 1995). EBM PICO (Nye et al., 2018) datasets annotates text spans with four tags: Participants, Intervention, Comparator and Outcome. ChemProt (Krallinger et al., 2017) dataset consists of five interactions between chemical and protein entities. We list the statistics of those tasks in Table 1. We fine-tune two downstream tasks in the computer science domain. They are both classification tasks. The ACL-ARC (Jurgens et al., 2018) dataset mainly focuses on analyzing"
2021.findings-acl.40,N19-1423,0,0.0259702,"probability to choose the size of incremental vocabulary automatically. Then we systematically explore different strategies to compress the large pretrained models for specific domains. We conduct our experiments in the biomedical and computer science domain. The experimental results demonstrate that our approach achieves better performance over the BERTBASE model in domain-specific tasks while 3.3× smaller and 5.1× faster than BERTBASE . The code and pretrained models are available at https://aka.ms/adalm. 1 Domain Corpus Pre-trained language models, such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and UniLM (Dong et al., 2019) have achieved impressive success in many natural language processing tasks. These models usually have hundreds of millions of parameters. They are pre-trained on a large corpus of general domain and fine-tuned on target domain tasks. However, it is not optimal to deploy these models directly to edge devices in specific domains. First, heavy model size and high latency makes it difficult Contribution during internship at Microsoft Research. (b) Distill-then-Adapt (c) Adapt-then-Distill (d) Adapt-and-Distill Figure 1: The four alternativ"
2021.findings-acl.40,2020.acl-main.740,0,0.0362375,"Missing"
2021.findings-acl.40,2020.findings-emnlp.372,0,0.0428914,"in and fine-tune the domainspecific small models on different downstream tasks. Experiments demonstrate that Adapt-andDistill achieves state-of-the-art results for domainspecific tasks. Specifically, the 6-layer model of 384 hidden dimensions outperforms the BERTBASE model while 3.3× smaller and 5.1× faster than BERTBASE . 2 unseen words. Task-agnostic knowledge distillation In recent years, tremendous progress has been made in model compression (Cheng et al., 2017). Knowledge distillation has proven to be a promising way to compress large models while maintaining accuracy (Sanh et al., 2019; Jiao et al., 2020; Sun et al., 2020; Wang et al., 2020). In this paper, we focus on task-agnostic knowledge distillation approaches, where a distilled small pre-trained model can be directly fine-tuned on downstream tasks. DistilBERT (Sanh et al., 2019) employs the soft label and embedding outputs to supervise the student. TinyBERT (Jiao et al., 2020) and MobileBERT (Sun et al., 2020) introduce self-attention distributions and hidden states to train the student model. MiniLM (Wang et al., 2020) avoids restrictions on the number of student layers and employs the self-attention distributions and value relation o"
2021.findings-acl.40,Q18-1028,0,0.0138259,"ositive classes evaluation. JNLPBA (Collier and Kim, 2004) NER dataset contains 6,892 disease mentions, which are mapped to 790 unique disease concepts with BIO tagging (Ramshaw and Marcus, 1995). EBM PICO (Nye et al., 2018) datasets annotates text spans with four tags: Participants, Intervention, Comparator and Outcome. ChemProt (Krallinger et al., 2017) dataset consists of five interactions between chemical and protein entities. We list the statistics of those tasks in Table 1. We fine-tune two downstream tasks in the computer science domain. They are both classification tasks. The ACL-ARC (Jurgens et al., 2018) dataset mainly focuses on analyzing how scientific works frame their contributions through different types of citations. SCIERC (Luan et al., 2018) dataset includes annotations for scientific entities, their relations, and coreference clusters. The statistics are available in Table 2. 2 Dev 4551 85321 11268 Table 1: Biomedical dataset used in our experiment. All selected from BLURB4 Domain corpus: For the biomedical domain, we collect a 16GB corpus from PubMed2 abstracts to adapt our model. We use the latest collection and pre-process the corpora with the same process as PubMedBERT (we omit a"
2021.findings-acl.40,P18-1007,0,0.148257,"is tokenized into [l, ##ym, ##ph, ##oma]). Gu et al.(2020) mentions that domainspecific vocabularies play a vital role in domain adaptation of pre-trained models. Specifically, we propose a domain-specific vocabulary expansion in the adaptation stage, which augments in-domain terms or subword units automatically given indomain text. Also, it is critical to decide the size of incremental vocabulary. Motivated by subword reg460 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 460–470 August 1–6, 2021. ©2021 Association for Computational Linguistics ularization (Kudo, 2018), AdaLM introduces a corpus occurrence probability as a metric to optimize the size of incremental vocabulary automatically. We systematically explore different strategies to compress general BERT models to specific domains (Figure 1): (a) From scratch: pre-training domain-specific small model from scratch with domain corpus; (b) Distill-then-Adapt: first distilling large model into small model, then adapting it into a specific domain; (c) Adapt-then-Distill: first adapting BERT into a specific domain, then distilling model into small size; (d) Adapt-and-Distill: adapting both the large and sm"
2021.findings-acl.40,2021.ccl-1.108,0,0.0636689,"Missing"
2021.findings-acl.40,D18-1360,0,0.0192212,"with BIO tagging (Ramshaw and Marcus, 1995). EBM PICO (Nye et al., 2018) datasets annotates text spans with four tags: Participants, Intervention, Comparator and Outcome. ChemProt (Krallinger et al., 2017) dataset consists of five interactions between chemical and protein entities. We list the statistics of those tasks in Table 1. We fine-tune two downstream tasks in the computer science domain. They are both classification tasks. The ACL-ARC (Jurgens et al., 2018) dataset mainly focuses on analyzing how scientific works frame their contributions through different types of citations. SCIERC (Luan et al., 2018) dataset includes annotations for scientific entities, their relations, and coreference clusters. The statistics are available in Table 2. 2 Dev 4551 85321 11268 Table 1: Biomedical dataset used in our experiment. All selected from BLURB4 Domain corpus: For the biomedical domain, we collect a 16GB corpus from PubMed2 abstracts to adapt our model. We use the latest collection and pre-process the corpora with the same process as PubMedBERT (we omit any abstracts with less than 128 words to reduce noise.). For the computer science domain, we use the abstracts text from the arXiv3 Dataset. We sele"
2021.findings-acl.40,P18-1019,0,0.0185183,"r experiment. We use the same train, development, and test splits as Gururangan et al. (2020) Fine-tuning tasks: For the biomedical domain, we choose three tasks: named entity recognition (NER), evidence-based medical information extraction (PICO), and relation extraction (RE). We perform entity-level F1 in NER task and wordlevel macro-F1 in the PICO task. The RE task uses the micro-F1 of positive classes evaluation. JNLPBA (Collier and Kim, 2004) NER dataset contains 6,892 disease mentions, which are mapped to 790 unique disease concepts with BIO tagging (Ramshaw and Marcus, 1995). EBM PICO (Nye et al., 2018) datasets annotates text spans with four tags: Participants, Intervention, Comparator and Outcome. ChemProt (Krallinger et al., 2017) dataset consists of five interactions between chemical and protein entities. We list the statistics of those tasks in Table 1. We fine-tune two downstream tasks in the computer science domain. They are both classification tasks. The ACL-ARC (Jurgens et al., 2018) dataset mainly focuses on analyzing how scientific works frame their contributions through different types of citations. SCIERC (Luan et al., 2018) dataset includes annotations for scientific entities,"
2021.findings-acl.40,W95-0107,0,0.0313798,": Computer science dataset used in our experiment. We use the same train, development, and test splits as Gururangan et al. (2020) Fine-tuning tasks: For the biomedical domain, we choose three tasks: named entity recognition (NER), evidence-based medical information extraction (PICO), and relation extraction (RE). We perform entity-level F1 in NER task and wordlevel macro-F1 in the PICO task. The RE task uses the micro-F1 of positive classes evaluation. JNLPBA (Collier and Kim, 2004) NER dataset contains 6,892 disease mentions, which are mapped to 790 unique disease concepts with BIO tagging (Ramshaw and Marcus, 1995). EBM PICO (Nye et al., 2018) datasets annotates text spans with four tags: Participants, Intervention, Comparator and Outcome. ChemProt (Krallinger et al., 2017) dataset consists of five interactions between chemical and protein entities. We list the statistics of those tasks in Table 1. We fine-tune two downstream tasks in the computer science domain. They are both classification tasks. The ACL-ARC (Jurgens et al., 2018) dataset mainly focuses on analyzing how scientific works frame their contributions through different types of citations. SCIERC (Luan et al., 2018) dataset includes annotati"
2021.findings-acl.40,2020.acl-main.195,0,0.0235285,"e domainspecific small models on different downstream tasks. Experiments demonstrate that Adapt-andDistill achieves state-of-the-art results for domainspecific tasks. Specifically, the 6-layer model of 384 hidden dimensions outperforms the BERTBASE model while 3.3× smaller and 5.1× faster than BERTBASE . 2 unseen words. Task-agnostic knowledge distillation In recent years, tremendous progress has been made in model compression (Cheng et al., 2017). Knowledge distillation has proven to be a promising way to compress large models while maintaining accuracy (Sanh et al., 2019; Jiao et al., 2020; Sun et al., 2020; Wang et al., 2020). In this paper, we focus on task-agnostic knowledge distillation approaches, where a distilled small pre-trained model can be directly fine-tuned on downstream tasks. DistilBERT (Sanh et al., 2019) employs the soft label and embedding outputs to supervise the student. TinyBERT (Jiao et al., 2020) and MobileBERT (Sun et al., 2020) introduce self-attention distributions and hidden states to train the student model. MiniLM (Wang et al., 2020) avoids restrictions on the number of student layers and employs the self-attention distributions and value relation of the teacher’s la"
2021.findings-acl.40,2020.findings-emnlp.129,0,0.0347896,"text. Gururangan et al. (2020) also employ continual pre-training to adapt pre-trained models into different domains including biomedical, computer science and news. However, many specialized domains contain their own specific words that are not included in pre-trained language model vocabulary. Gu et al.(2020) propose a biomedical pre-trained model PubMedBERT, where the vocabulary was created from scratch and the model is pre-trained from scratch. Furthermore, in many specialized domains, large enough corpora may not be available to support pre-training from scratch. Zhang et al. (2020) and Tai et al. (2020) extend the open-domain vocabulary with top frequent in-domain words to resolve this out-of-vocabulary issue. This approach ignores domain-specific sub-word units (e.g., blasto-, germin- in biomedical domain). These subword units help generalize domain knowledge and avoid 3 3.1 Methods Overview We systematically explore different strategies to achieve an effective and efficient small model in specific domains. We summarize them into four strategies: from scratch, distill-then-adapt, adaptthen-distill and adapt-and-distill. 461 Pretrain-from-scratch Domain-specific pretraining from scratch empl"
2021.naacl-main.280,D19-1252,1,0.753364,"Missing"
2021.naacl-main.280,L18-1548,0,0.0642253,"oss-lingual understanding tasks. We also conduct ablation studies to understand the major components of I NFOXLM. 4.1 Setup Corpus We use the same pre-training corpora as previous models (Conneau et al., 2020a; Conneau and Lample, 2019). Specifically, we reconstruct CC-100 (Conneau et al., 2020a) for MMLM, which remains 94 languages by filtering the language code larger than 0.1GB. Following (Conneau and Lample, 2019), for the TLM and X L C O tasks, we employ 14 language pairs of parallel data that involves English. We collect translation pairs from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019). The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix. Model Size We follow the model configurations of XLM-R (Conneau et al., 2020a). For the Transformer (Vaswani et al., 2017) architecture, we use 12 layers and 768 hidden states for I NFOXLM (i.e., base size), and 24 layers and 1,024 hidden states for I NFOXLM LARGE (i.e., large size). 4.2 Evaluation Cross-Lingual Natural Language Inference The Cross-Lingual Natural Language Inference corpus (XNLI; Conneau et al. 2018"
2021.naacl-main.280,2020.acl-main.653,0,0.715277,"h monolingual and parallel corpora. Contribution during internship at Microsoft Research. Contact person: Li Dong and Furu Wei. We jointly train I NFOXLM with MMLM, TLM 3576 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3576–3588 June 6–11, 2021. ©2021 Association for Computational Linguistics and X L C O. We conduct extensive experiments on several cross-lingual understanding tasks, including cross-lingual natural language inference (Conneau et al., 2018), cross-lingual question answering (Lewis et al., 2020), and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019). Experimental results show that I NFOXLM outperforms strong baselines on all the benchmarks. Moreover, the analysis indicates that I NFOXLM achieves better cross-lingual transferability. 2 2.1 Related Work between the sampled positive and negative pairs. In addition to the estimators, various view pairs are employed in these methods. The view pair can be the local and global features of an image (Hjelm et al., 2019; Bachman et al., 2019), the random data augmentations of the same image (Tian et al., 2019; He et al., 2020; Chen"
2021.naacl-main.280,L16-1561,0,0.0179259,"OXLM with previous work on three cross-lingual understanding tasks. We also conduct ablation studies to understand the major components of I NFOXLM. 4.1 Setup Corpus We use the same pre-training corpora as previous models (Conneau et al., 2020a; Conneau and Lample, 2019). Specifically, we reconstruct CC-100 (Conneau et al., 2020a) for MMLM, which remains 94 languages by filtering the language code larger than 0.1GB. Following (Conneau and Lample, 2019), for the TLM and X L C O tasks, we employ 14 language pairs of parallel data that involves English. We collect translation pairs from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019). The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix. Model Size We follow the model configurations of XLM-R (Conneau et al., 2020a). For the Transformer (Vaswani et al., 2017) architecture, we use 12 layers and 768 hidden states for I NFOXLM (i.e., base size), and 24 layers and 1,024 hidden states for I NFOXLM LARGE (i.e., large size). 4.2 Evaluation Cross-Lingual Natural Language Inference The Cross-Lingual Natural Language Inf"
2021.naacl-main.280,2020.findings-emnlp.147,0,0.110105,"Missing"
2021.naacl-main.280,tiedemann-2012-parallel,0,0.0390072,"e also conduct ablation studies to understand the major components of I NFOXLM. 4.1 Setup Corpus We use the same pre-training corpora as previous models (Conneau et al., 2020a; Conneau and Lample, 2019). Specifically, we reconstruct CC-100 (Conneau et al., 2020a) for MMLM, which remains 94 languages by filtering the language code larger than 0.1GB. Following (Conneau and Lample, 2019), for the TLM and X L C O tasks, we employ 14 language pairs of parallel data that involves English. We collect translation pairs from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019). The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix. Model Size We follow the model configurations of XLM-R (Conneau et al., 2020a). For the Transformer (Vaswani et al., 2017) architecture, we use 12 layers and 768 hidden states for I NFOXLM (i.e., base size), and 24 layers and 1,024 hidden states for I NFOXLM LARGE (i.e., large size). 4.2 Evaluation Cross-Lingual Natural Language Inference The Cross-Lingual Natural Language Inference corpus (XNLI; Conneau et al. 2018) is a widely used cross"
D14-1054,D08-1083,0,0.0209071,"Sentiment Analysis Duyu Tang∗, Furu Wei‡ , Bing Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contai"
D14-1054,P14-2009,1,0.766966,"hou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a separate unit, which losses the word order and does"
D14-1054,P13-2087,0,0.0212728,"using iterated matrix multiplication. Socher et al. propose Recursive Autoencoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is"
D14-1054,P11-2008,0,0.131527,"Missing"
D14-1054,P11-1015,0,0.3622,"cial Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic ch"
D14-1054,D13-1171,0,0.046832,"Missing"
D14-1054,J11-2001,0,0.0176143,"notated only with sentiment polarity, without any segmentation annotations. We evaluate the effectiveness of our joint model on a benchmark Twitter sentiment classification dataset in SemEval 2013. Results show that the joint model performs comparably with stateof-the-art methods, and consistently outperforms pipeline methods in various experiment settings. The main contributions of the work presented in this paper are as follows. Related Work Existing approaches for sentiment classification are dominated by two mainstream directions. Lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) typically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corpora collected by sentiment signals like e"
D14-1054,S13-2053,0,0.183839,"such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a separate unit, which losses the word order and does not capture the phrasal information. The segmentations based on syntactic chunkers typically aim to identify noun groups, verb groups or named entities from a sentence. However, many sentiment indicators are phrases constituted of adjectives, negations, adverbs or idioms (Liu, 2012; Mohammad et al., 2013a), which are splitted by syntactic chunkers. Besides, a better approach would be to utilize the sentiment information to improve the segmentor. Accordingly, the sentiment-specific segmentor will enhance the performance of sentiment classification in turn. In this paper, we propose a joint segmentation and classification framework (JSC) for sentiment analysis, which simultaneous conducts sentence segmentation and sentence-level sentiment classification. The framework is illustrated in FigIn this paper, we propose a joint segmentation and classification framework for sentiment analysis. Existin"
D14-1054,C14-1018,1,0.854768,"encoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is to identify the sentiment polarity of a sentence (or document) as"
D14-1054,N10-1120,0,0.161612,"Tang∗, Furu Wei‡ , Bing Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, b"
D14-1054,P14-1146,1,0.670131,"encoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is to identify the sentiment polarity of a sentence (or document) as"
D14-1054,pak-paroubek-2010-twitter,0,0.0427636,"pically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corpora collected by sentiment signals like emoticons (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Zhao et al., 2012). Majority of existing approaches follow Pang et al. (2002) and employ corpus-based method for sentiment classification. Pang et al. (2002) pioneer to treat the sentiment classification of reviews as a special case of text categorization problem and first investigate machine learning methods. They employ Naive Bayes, Maximum Entropy and Support Vector Machines (SVM) with a diverse set of features. In their experiments, the best performance is achieved by SVM with bagof-words feature. Under this perspective, many studies focus on designing or learnin"
D14-1054,P10-1141,0,0.148379,"Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-w"
D14-1054,P02-1053,0,0.0058165,"the joint model from sentences annotated only with sentiment polarity, without any segmentation annotations. We evaluate the effectiveness of our joint model on a benchmark Twitter sentiment classification dataset in SemEval 2013. Results show that the joint model performs comparably with stateof-the-art methods, and consistently outperforms pipeline methods in various experiment settings. The main contributions of the work presented in this paper are as follows. Related Work Existing approaches for sentiment classification are dominated by two mainstream directions. Lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) typically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corp"
D14-1054,W02-1011,0,0.0553792,"fication performance. The joint model is trained only based on the annotated sentiment polarity of sentences, without any segmentation annotations. Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that, our joint model performs comparably with the state-of-the-art methods. 1 Introduction Sentiment classification, which classifies the sentiment polarity of a sentence (or document) as positive or negative, is a major research direction in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Majority of existing approaches follow Pang et al. (2002) and treat sen∗ This work was partly done when the first and fourth authors were visiting Microsoft Research. 477 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477–487, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics CG SC SEG SEG SC that is not bad -1 <+1,-1> NO 0.6 0.6 that is not bad that is not bad -1 <+1,-1> NO 0.4 0.4 Polarity: +1 that is not bad +1 <+1,+1> YES 2.3 2.3 that is not bad +1 <+1,+1> YES 1.6 1.6 Segmentations Polarity Update Rank Update Input Top K Figure 1: The joint segmentation and c"
D14-1054,P12-2018,0,0.141338,"l. (2002) and employ corpus-based method for sentiment classification. Pang et al. (2002) pioneer to treat the sentiment classification of reviews as a special case of text categorization problem and first investigate machine learning methods. They employ Naive Bayes, Maximum Entropy and Support Vector Machines (SVM) with a diverse set of features. In their experiments, the best performance is achieved by SVM with bagof-words feature. Under this perspective, many studies focus on designing or learning effective features to obtain better classification performance. On movie or product reviews, Wang and Manning (2012) present NBSVM, which trades-off • To our knowledge, this is the first work that automatically produces sentence segmentation for sentiment classification within a joint framework. • We show that the joint model yields comparable performance with the state-of-the-art methods on the benchmark Twitter sentiment classification datasets in SemEval 2013. 478 overview of the proposed joint segmentation and classification model (JSC) for sentiment analysis. The segmentation candidate generation model and the segmentation ranking model are described in Section 4. The details of the sentiment classific"
D14-1054,H05-1044,0,0.0828802,"Missing"
D14-1054,D11-1014,0,0.256554,"ytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a sepa"
D14-1054,D13-1016,0,0.0546336,"Missing"
D14-1054,D12-1110,0,0.0455462,"Missing"
D14-1054,D11-1016,0,0.0130106,"the document feature. On Twitter, Mohammad et al. (2013b) develop a state-of-the-art Twitter sentiment classifier in SemEval 2013, using a variety of sentiment lexicons and hand-crafted features. With the revival of deep learning (representation learning (Hinton and Salakhutdinov, 2006; Bengio et al., 2013; Jones, 2014)), more recent studies focus on learning the low-dimensional, dense and real-valued vector as text features for sentiment classification. Glorot et al. (2011) investigate Stacked Denoising Autoencoders to learn document vector for domain adaptation in sentiment classification. Yessenalina and Cardie (2011) represent each word as a matrix and compose words using iterated matrix multiplication. Socher et al. propose Recursive Autoencoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabili"
D14-1054,P14-1011,1,0.830375,"ords or phrases of variable length. Under this scenario, phrase embedding is highly suitable as it is capable to represent phrases with different length into a consistent distributed vector space (Mikolov et al., 2013). For each phrase, phrase embedding is a dense, real-valued and continuous vector. After the phrase embedding is trained, the nearest neighbors in the embedding space are favored to have similar grammatical usages and semantic meanings. The effectiveness of phrase embedding has been verified for building large-scale sentiment lexicon (Tang et al., 2014a) and machine translation (Zhang et al., 2014). We learn phrase embedding with Skip-Gram model (Mikolov et al., 2013), which is the state-of(2) k where φij is the segmentation score of Ωij ; sf eijk is the k-th segmentation feature of Ωij ; w and b are the parameters of the segmentation ranking model. During training, given a sentence si and its gold sentiment polarity polig , the optimization objec2 j∈Hi Segmentation-Specific Feature We empirically design four segmentation-specific features to reflect the information of each segmentation, as listed in Table 3. The objective of the segmentation ranking model is to assign a scalar to each"
D14-1054,D13-1170,0,0.203308,"ng Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal"
D14-1054,J13-3004,0,\N,Missing
D14-1054,P11-1016,1,\N,Missing
D16-1053,N16-1181,0,0.0137262,"Missing"
D16-1053,P14-1062,0,0.20293,"Missing"
D16-1053,D15-1075,0,0.119869,"est system T-CNN (Lei et al., 2015). Figure 5 shows examples of intra-attention for sentiment words. Interestingly, the network learns to associate sentiment important words such as though and fantastic or not and good. 5.3 Natural Language Inference The ability to reason about the semantic relationship between two sentences is an integral part of text understanding. We therefore evaluate our model on recognizing textual entailment, i.e., whether two premise-hypothesis pairs are entailing, contradictory, or neutral. For this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premisehypothesis pairs and target labels indicating their relation. After removing sentences with unknown labels, we end up with 549,367 pairs for training, 9,842 for development and 9,824 for testing. The vocabulary size is 36,809 and the average sentence length is 22. We performed lower-casing and tokenization for the entire dataset. Recent approaches use two sequential LSTMs to encode the premise and the hypothesis respectively, and apply neural attention to reason about their logical relationship (Rockt¨aschel et al., 2016; Wang and Jiang, 2016). Furthermore, Rockt¨aschel"
D16-1053,P16-1139,0,0.0886747,"block component which interacts with its hidden state. Kumar et al. (2016) employ a structured neural network with episodic memory modules for natural language and also visual question answering (Xiong et al., 2016). Similar to the above work, we leverage memory and attention in a recurrent neural network for inducing relations between tokens as a module in a larger network responsible for representation learning. As a property of soft attention, all intermediate relations we aim to capture are soft and differentiable. This is in contrast to shift-reduce type neural models (Dyer et al., 2015; Bowman et al., 2016) where the intermediate decisions are hard and induction is more difficult. Finally, note that our model captures undirected lexical relations and is thus distinct from work on dependency grammar induction (Klein and Manning, 2004) where the learned head-modifier relations are directed. 3 The Machine Reader In this section we present our machine reader which is designed to process structured input while retaining the incrementality of a recurrent neural network. The core of our model is a Long Short-Term Mem553 ory (LSTM) unit with an extended memory tape that explicitly simulates the human me"
D16-1053,P15-1033,0,0.0663298,"an external memory block component which interacts with its hidden state. Kumar et al. (2016) employ a structured neural network with episodic memory modules for natural language and also visual question answering (Xiong et al., 2016). Similar to the above work, we leverage memory and attention in a recurrent neural network for inducing relations between tokens as a module in a larger network responsible for representation learning. As a property of soft attention, all intermediate relations we aim to capture are soft and differentiable. This is in contrast to shift-reduce type neural models (Dyer et al., 2015; Bowman et al., 2016) where the intermediate decisions are hard and induction is more difficult. Finally, note that our model captures undirected lexical relations and is thus distinct from work on dependency grammar induction (Klein and Manning, 2004) where the learned head-modifier relations are directed. 3 The Machine Reader In this section we present our machine reader which is designed to process structured input while retaining the incrementality of a recurrent neural network. The core of our model is a Long Short-Term Mem553 ory (LSTM) unit with an extended memory tape that explicitly"
D16-1053,D11-1142,0,0.0100303,"ling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art. 1 Introduction How can a sequence-level network induce relations which are presumed latent during text processing? How can a recurrent network attentively memorize longer sequences in a way that humans do? In this paper we design a machine reader that automatically learns to understand text. The term machine reading is related to a wide range of tasks from answering reading comprehension questions (Clark et al., 2013), to fact and relation extraction (Etzioni et al., 2011; Fader et al., 2011), ontology learning (Poon and Domingos, 2010), and textual entailment (Dagan et al., 2005). Rather than focusing on a specific task, we develop a general-purpose reading simulator, drawing inspiration from human language processing and the fact language comprehension is incremental with readers continuously extracting the meaning of utterances on a word-by-word basis. In order to understand texts, our machine reader should provide facilities for extracting and representing meaning from natural language text, storing meanings internally, and working with stored meanings to derive further conseq"
D16-1053,D14-1181,0,0.0160223,"Missing"
D16-1053,P04-1061,0,0.0132699,"the above work, we leverage memory and attention in a recurrent neural network for inducing relations between tokens as a module in a larger network responsible for representation learning. As a property of soft attention, all intermediate relations we aim to capture are soft and differentiable. This is in contrast to shift-reduce type neural models (Dyer et al., 2015; Bowman et al., 2016) where the intermediate decisions are hard and induction is more difficult. Finally, note that our model captures undirected lexical relations and is thus distinct from work on dependency grammar induction (Klein and Manning, 2004) where the learned head-modifier relations are directed. 3 The Machine Reader In this section we present our machine reader which is designed to process structured input while retaining the incrementality of a recurrent neural network. The core of our model is a Long Short-Term Mem553 ory (LSTM) unit with an extended memory tape that explicitly simulates the human memory span. The model performs implicit relation analysis between tokens with an attention-based memory addressing mechanism at every time step. In the following, we first review the standard Long Short-Term Memory and then describe"
D16-1053,D15-1180,0,0.0137192,"Missing"
D16-1053,D14-1162,0,0.124032,"Missing"
D16-1053,P10-1031,0,0.00955993,"nguage inference show that our model matches or outperforms the state of the art. 1 Introduction How can a sequence-level network induce relations which are presumed latent during text processing? How can a recurrent network attentively memorize longer sequences in a way that humans do? In this paper we design a machine reader that automatically learns to understand text. The term machine reading is related to a wide range of tasks from answering reading comprehension questions (Clark et al., 2013), to fact and relation extraction (Etzioni et al., 2011; Fader et al., 2011), ontology learning (Poon and Domingos, 2010), and textual entailment (Dagan et al., 2005). Rather than focusing on a specific task, we develop a general-purpose reading simulator, drawing inspiration from human language processing and the fact language comprehension is incremental with readers continuously extracting the meaning of utterances on a word-by-word basis. In order to understand texts, our machine reader should provide facilities for extracting and representing meaning from natural language text, storing meanings internally, and working with stored meanings to derive further consequences. Ideally, such a system should be robu"
D16-1053,D15-1044,0,0.0329954,"supervision signals, achieving performance comparable or better to state-of-the-art models and superior to vanilla LSTMs. 2 Related Work Our machine reader is a recurrent neural network exhibiting two important properties: it is incremental, simulating human behavior, and performs shallow structure reasoning over input streams. Recurrent neural network (RNNs) have been successfully applied to various sequence modeling and sequence-to-sequence transduction tasks. The latter have assumed several guises in the literature such as machine translation (Bahdanau et al., 2014), sentence compression (Rush et al., 2015), and reading comprehension (Hermann et al., 2015). A key contributing factor to their success has been the ability to handle well-known problems with exploding or vanishing gradients (Bengio et al., 1994), leading to models with gated activation functions (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), and more advanced architectures that enhance the information flow within the network (Koutn´ık et al., 2014; Chung et al., 2015; Yao et al., 2015). A remaining practical bottleneck for RNNs is memory compression (Bahdanau et al., 2014): since the inputs are recursively combined into a sin"
D16-1053,D13-1170,0,0.185016,"ich is typically too small in terms of parameters, it becomes difficult to accurately memorize sequences (Zaremba and Sutskever, 2014). In the encoder-decoder architecture, this problem can be sidestepped with an attention mechanism which learns soft alignments between the decoding states and the encoded memories (Bahdanau et al., 2014). In our model, memory and attention are added within a sequence encoder allowing the network to uncover lexical relations between tokens. The idea of introducing a structural bias to neural models is by no means new. For example, it is reflected in the work of Socher et al. (2013a) who apply recursive neural networks for learning natural language representations. In the context of recurrent neural networks, efforts to build modular, structured neural models date back to Das et al. (1992) who connect a recurrent neural network with an external memory stack for learning context free grammars. Recently, Weston et al. (2015) propose Memory Networks to explicitly segregate memory storage from the computation of neural networks in general. Their model is trained end-to-end with a memory addressing mechanism closely related to soft attention (Sukhbaatar et al., 2015) and has"
D16-1053,P15-1150,0,0.0628032,"regularization constant was 1E-4 and the mini-batch size was 5. A dropout rate of 0.5 was applied to the neural network classifier. We compared our model with a wide range of topperforming systems. Most of these models (including ours) are LSTM variants (third block in Table 2), recursive neural networks (first block), or convolutional neural networks (CNNs; second block). Recursive models assume the input sentences are represented as parse trees and can take advantage of annotations at the phrase level. LSTM-type models and CNNs are trained on sequential input, with the exception of CT-LSTM (Tai et al., 2015) which operates over tree-structured network topologies such as constituent trees. For comparison, we also report the performance of the paragraph vector model (PV; Le and Mikolov (2014); see Table 2, second block) which neither operates on trees nor sequences but learns distributed document representations parameterized directly. The results in Table 2 show that both 1- and 2-layer LSTMNs outperform the LSTM baselines while achieving numbers comparable to state of the art. The number of layers for our models was set to be comparable to previously published results. On the fine-grained and bin"
D16-1053,N16-1170,0,0.45912,"e Inference (SNLI) dataset (Bowman et al., 2015), which contains premisehypothesis pairs and target labels indicating their relation. After removing sentences with unknown labels, we end up with 549,367 pairs for training, 9,842 for development and 9,824 for testing. The vocabulary size is 36,809 and the average sentence length is 22. We performed lower-casing and tokenization for the entire dataset. Recent approaches use two sequential LSTMs to encode the premise and the hypothesis respectively, and apply neural attention to reason about their logical relationship (Rockt¨aschel et al., 2016; Wang and Jiang, 2016). Furthermore, Rockt¨aschel et al. (2016) show that a non-standard encoder-decoder architecture which processes the hypothesis conditioned on 558 it ’s although tough i did to n’t watch hate this but one it , ’s it ’s a fantastic not very good movie either Figure 5: Examples of intra-attention (sentiment analysis). Bold lines (red) indicate attention between sentiment important words. the premise results significantly boosts performance. We use a similar approach to tackle this task with LSTMNs. Specifically, we use two LSTMNs to read the premise and hypothesis, and then match them by comparin"
D16-1081,W14-1605,0,0.0311354,"lack ones are associated with clues. Compared with our riddle task, the clues in the CPs are derived from each question where the radicals in solution are derived from the metaphors in the riddles. Proverb (Littman et al., 2002) is the first system for the automatic resolution of CPs. Ernandes et al. (2005) utilize a web-search module to find sensible candidates to questions expressed in natural language and get the final answer by ranking the candidates. And the rule-based module and the dictionary module are mentioned in his work. The tree kernel is used to rerank the candidates proposed by Barlacchi et al. (2014) for automatic resolution of crossword puzzles. From another perspective, there are a few projects Offline Learning Riddle/Solution Pairs Phrase-Radical Alignment and Rule Learning Alignment Table Rule Table Riddle Solving Solution Solution Ranking Solution Candidate Generation Riddle Description Riddle Ranking Riddle Candidate Generation Solution (Chinese Character) Riddle Generation Riddle Description Figure 3: The pipeline of offline learning, riddle solving and riddle generation on Chinese language cultures, such as the couplet generation and the poem generation. A statistical machine tran"
D16-1081,C92-4176,0,0.132808,"Missing"
D16-1081,C08-1048,1,0.756656,"on Pairs Phrase-Radical Alignment and Rule Learning Alignment Table Rule Table Riddle Solving Solution Solution Ranking Solution Candidate Generation Riddle Description Riddle Ranking Riddle Candidate Generation Solution (Chinese Character) Riddle Generation Riddle Description Figure 3: The pipeline of offline learning, riddle solving and riddle generation on Chinese language cultures, such as the couplet generation and the poem generation. A statistical machine translation (SMT) framework is proposed to generate Chinese couplets and classic Chinese poetry (He et al., 2012; Zhou et al., 2009; Jiang and Zhou, 2008). Jiang and Zhou (2008) use a phrasebased SMT model with linguistic filters to generate Chinese couplets satisfied couplet constraints, using both human judgments and BLEU scores as the evaluation. Zhou et al. (2009) use the SMT model to generate quatrain with a human evaluation. He et al. (2012) generate Chinese poems with the given topic words by combining a statistical machine translation model with an ancient poetic phrase taxonomy. Following the approaches in SMT framework, it is valid to regard the metaphors with its radicals as the alignments. There are several works using neural networ"
D16-1081,J03-1002,0,0.00555444,"d to extract the metaphors based on the phrase-radical alignments and rules. We exploit the phrase-radical alignments as to de848 scribe the simple metaphors, e.g. “千 里” aligns “马”, which aligns the phrase and the radical by the meaning. We employ a statistical framework with a word alignment algorithm to automatically mine phrase-radical metaphors from riddle dataset. Considering the alignment is often represented as the matching between successive words in the riddle and a radical in the solution, we propose two methods specifically to extract alignments. The first method in according with (Och and Ney, 2003) is described as follows. With a riddle description q and corresponding solution s, we tokenize the input riddle q to character as (w1 , w2 , . . . , wn ) and decompose the solution s into radicals as (r1 , r2 , . . . , rm ). We count all ([wi , wj ], rk )(i, j ∈ [1, n], k ∈ [1, m]) as alignments. The second method takes into account more structural information of characters. Let (w1 , w2 ) denote two successive characters in the riddle q. If w1 is a radical of w2 and the rest parts of w2 as r appear in the solution q, we strongly support that ((w1 , w2 ), r) is a alignment. It is identical if"
D16-1081,D14-1074,0,0.0239932,"a phrasebased SMT model with linguistic filters to generate Chinese couplets satisfied couplet constraints, using both human judgments and BLEU scores as the evaluation. Zhou et al. (2009) use the SMT model to generate quatrain with a human evaluation. He et al. (2012) generate Chinese poems with the given topic words by combining a statistical machine translation model with an ancient poetic phrase taxonomy. Following the approaches in SMT framework, it is valid to regard the metaphors with its radicals as the alignments. There are several works using neural network to generate Chinese poems(Zhang and Lapata, 2014; Yi et al., 2016). Due to the limited data and strict rules, it is hard to transfer to the riddle generation. 3 Phrase-Radical Alignments and Rules The metaphor is one of the key components in both solving and generation. On the one hand we need to identify these metaphors since each of them aligns a radical in the final solution. On the other hand, we need to integrate these metaphors into the riddle descriptions to generate riddles. Thus, how to extract the metaphors of riddles becomes a big challenge in our task. Below we introduce our method to extract the metaphors based on the phrase-ra"
D16-1081,Y09-1006,1,0.90926,"rning Riddle/Solution Pairs Phrase-Radical Alignment and Rule Learning Alignment Table Rule Table Riddle Solving Solution Solution Ranking Solution Candidate Generation Riddle Description Riddle Ranking Riddle Candidate Generation Solution (Chinese Character) Riddle Generation Riddle Description Figure 3: The pipeline of offline learning, riddle solving and riddle generation on Chinese language cultures, such as the couplet generation and the poem generation. A statistical machine translation (SMT) framework is proposed to generate Chinese couplets and classic Chinese poetry (He et al., 2012; Zhou et al., 2009; Jiang and Zhou, 2008). Jiang and Zhou (2008) use a phrasebased SMT model with linguistic filters to generate Chinese couplets satisfied couplet constraints, using both human judgments and BLEU scores as the evaluation. Zhou et al. (2009) use the SMT model to generate quatrain with a human evaluation. He et al. (2012) generate Chinese poems with the given topic words by combining a statistical machine translation model with an ancient poetic phrase taxonomy. Following the approaches in SMT framework, it is valid to regard the metaphors with its radicals as the alignments. There are several wo"
D16-1081,J92-1011,0,\N,Missing
D17-1091,D13-1160,0,0.104457,"ialized from a uniform distribution U (−0.08, 0.08). The learning rate and decay rate of RMSProp were 0.01 and 0.95, respectively. The batch size was set to 150. To alleviate the exploding gradient problem (Pascanu et al., 2013), the gradient norm was clipped to 5. Early stopping was used to determine the number of epochs. 3.2 3.3 Our model was trained on three datasets, representative of different types of QA tasks. The first two datasets focus on question answering over a structured knowledge base, whereas the third one is specific to answer sentence selection. W EB Q UESTIONS This dataset (Berant et al., 2013) contains 3, 778 training instances and 2, 032 test instances. Questions were collected by querying the Google Suggest API. A breadth-first search beginning with wh- was conducted and the answers were crowd-sourced using Freebase as the backend knowledge base. G RAPH Q UESTIONS The dataset (Su et al., 2016) contains 5, 166 question-answer pairs (evenly split into a training and a test set). It was created by asking crowd workers to paraphrase 500 Freebase graph queries in natural language. Implementation Details Table 3 presents descriptive statistics on the paraphrases generated by the variou"
D17-1091,P14-1133,0,0.107546,"Missing"
D17-1091,D14-1067,0,0.0459875,".edu, mlap@inf.ed.ac.uk Abstract the use of paraphrases in relation to question answering. There have been three main strands of research. The first one applies paraphrasing to match natural language and logical forms in the context of semantic parsing. Berant and Liang (2014) use a template-based method to heuristically generate canonical text descriptions for candidate logical forms, and then compute paraphrase scores between the generated texts and input questions in order to rank the logical forms. Another strand of work uses paraphrases in the context of neural question answering models (Bordes et al., 2014a,b; Dong et al., 2015). These models are typically trained on question-answer pairs, and employ question paraphrases in a multi-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases. The third strand of research uses paraphrases more directly. The idea is to paraphrase the question and then submit the rewritten version to a QA module. Various resources have been used to produce question paraphrases, such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Da"
D17-1091,D15-1181,0,0.0144941,"tead, we translate from multiple pivot sentences (Mallinson et al., 2016). As shown in Figure 2, question q is translated to K-best German pivots, Gq = {g1 , . . . , gK }. The probability of generating paraphrase q 0 = y1 . . . y|q0 |is decomposed as: 0 0  p q |Gq = |q | Y p (yt |y<t , Gq ) t=1 (2) 0 = |q |K Y X 2.2 p (gk |q) p (yt |y<t , gk ) Recall from Equation (1) that pθ (q 0 |q) scores the generated paraphrases q 0 ∈ Hq ∪ {q}. We estimate pθ (q 0 |q) using neural networks given their successful application to paraphrase identification tasks (Socher et al., 2011; Yin and Sch¨utze, 2015; He et al., 2015). Specifically, the input question and its paraphrases are encoded as vectors. Then, we employ a neural network to obtain the score s (q 0 |q) which after normalization becomes the probability pθ (q 0 |q). t=1 k=1 where y<t = y1 , . . . , yt−1 , and |q 0 |is the length of q 0 . Probabilities p (gk |q) and p (yt |y<t , gk ) are computed by the E N -D E and D E -E N models, respectively. We use beam search to decode tokens by conditioning on multiple pivoting sentences. The results with the best decoding scores are considered candidate paraphrases. Examples of NMT paraphrases are shown in Table"
D17-1091,D08-1021,0,0.0997306,"Missing"
D17-1091,P82-1020,0,0.849456,"Missing"
D17-1091,P16-1073,0,0.0916487,"Missing"
D17-1091,N03-1017,0,0.0842091,"ch as the ability to learn continuous representations and to consider wider context while paraphrasing. In our work, we select German as our pivot following Mallinson et al. (2016) who show that it outperforms other languages in a wide range of paraphrasing experiments, and pretrain two NMT systems, English-to-German (E N -D E) and PPDB-based Generation Bilingual pivoting (Bannard and Callison-Burch, 2005) is one of the most well-known approaches to paraphrasing; it uses bilingual parallel corpora to learn paraphrases based on techniques from phrase-based statistical machine translation (SMT, Koehn et al. 2003). The intuition is that two English strings that translate to the same foreign string can be assumed to have the same meaning. The method first extracts a bilingual phrase table and then obtains English paraphrases by pivoting through foreign language phrases. Drawing inspiration from syntax-based SMT, Callison-Burch (2008) and Ganitkevitch et al. (2011) extended this idea to syntactic paraphrases, 877 Source the average size of be locate on which continent language speak in what be the money in WikiAnswers1 users. This corpus is a large resource (the average cluster size is 25), but is relati"
D17-1091,P15-1026,1,0.292885,"Abstract the use of paraphrases in relation to question answering. There have been three main strands of research. The first one applies paraphrasing to match natural language and logical forms in the context of semantic parsing. Berant and Liang (2014) use a template-based method to heuristically generate canonical text descriptions for candidate logical forms, and then compute paraphrase scores between the generated texts and input questions in order to rank the logical forms. Another strand of work uses paraphrases in the context of neural question answering models (Bordes et al., 2014a,b; Dong et al., 2015). These models are typically trained on question-answer pairs, and employ question paraphrases in a multi-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases. The third strand of research uses paraphrases more directly. The idea is to paraphrase the question and then submit the rewritten version to a QA module. Various resources have been used to produce question paraphrases, such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Database (Narayan et al.,"
D17-1091,N06-2009,0,0.0954207,"such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Database (Narayan et al., 2016), as well as rules mined from Wiktionary (Chen et al., 2016) and large-scale paraphrase corpora (Fader et al., 2013). A common problem with the generated paraphrases is that they often contain inappropriate candidates. Hence, treating all paraphrases as equally felicitous and using them to answer the question could degrade performance. To remedy this, a scoring model is often employed, however independently of the QA system used to find the answer (Duboue and Chu-Carroll, 2006; Narayan et al., 2016). Problematically, the separate paraphrase models used in previous work do not fully utilize the supervision signal from the training data, and as such cannot be properly tuned Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need. In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our method is trained end-toend using question-answer pairs as a supervision signal. A question and its par"
D17-1091,P13-1158,0,0.123542,"ti-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases. The third strand of research uses paraphrases more directly. The idea is to paraphrase the question and then submit the rewritten version to a QA module. Various resources have been used to produce question paraphrases, such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Database (Narayan et al., 2016), as well as rules mined from Wiktionary (Chen et al., 2016) and large-scale paraphrase corpora (Fader et al., 2013). A common problem with the generated paraphrases is that they often contain inappropriate candidates. Hence, treating all paraphrases as equally felicitous and using them to answer the question could degrade performance. To remedy this, a scoring model is often employed, however independently of the QA system used to find the answer (Duboue and Chu-Carroll, 2006; Narayan et al., 2016). Problematically, the separate paraphrase models used in previous work do not fully utilize the supervision signal from the training data, and as such cannot be properly tuned Question answering (QA) systems are"
D17-1091,D16-1147,0,0.00856936,"Missing"
D17-1091,D11-1108,0,0.0149075,"Missing"
D17-1091,W16-6625,1,0.865193,"g et al., 2015). These models are typically trained on question-answer pairs, and employ question paraphrases in a multi-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases. The third strand of research uses paraphrases more directly. The idea is to paraphrase the question and then submit the rewritten version to a QA module. Various resources have been used to produce question paraphrases, such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Database (Narayan et al., 2016), as well as rules mined from Wiktionary (Chen et al., 2016) and large-scale paraphrase corpora (Fader et al., 2013). A common problem with the generated paraphrases is that they often contain inappropriate candidates. Hence, treating all paraphrases as equally felicitous and using them to answer the question could degrade performance. To remedy this, a scoring model is often employed, however independently of the QA system used to find the answer (Duboue and Chu-Carroll, 2006; Narayan et al., 2016). Problematically, the separate paraphrase models used in previous work do not fully utilize the"
D17-1091,P02-1040,0,0.106423,"a test set). It was created by asking crowd workers to paraphrase 500 Freebase graph queries in natural language. Implementation Details Table 3 presents descriptive statistics on the paraphrases generated by the various systems across datasets (training set). As can be seen, the average paraphrase length is similar to the average length of the original questions. The NMT method generates more paraphrases and has wider coverage, while the average number and coverage of the other two methods varies per dataset. As a way of quantifying the extent to which rewriting takes place, we report BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores between the original questions and their paraphrases. The NMT Paraphrase Generation Candidate paraphrases were stemmed (Minnen et al., 2001) and lowercased. We discarded duplicate or trivial paraphrases which only rewrite stop words or punctuation. For the NMT model, we followed the implementation2 and settings described in Mallinson et al. (2016), and used English↔German as the language pair. The system was trained on data released as part of the WMT15 shared translation task (4.2 million sentence pairs). We also had access to back-translated monolingual"
D17-1091,2006.amta-papers.25,0,0.0466375,"asking crowd workers to paraphrase 500 Freebase graph queries in natural language. Implementation Details Table 3 presents descriptive statistics on the paraphrases generated by the various systems across datasets (training set). As can be seen, the average paraphrase length is similar to the average length of the original questions. The NMT method generates more paraphrases and has wider coverage, while the average number and coverage of the other two methods varies per dataset. As a way of quantifying the extent to which rewriting takes place, we report BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores between the original questions and their paraphrases. The NMT Paraphrase Generation Candidate paraphrases were stemmed (Minnen et al., 2001) and lowercased. We discarded duplicate or trivial paraphrases which only rewrite stop words or punctuation. For the NMT model, we followed the implementation2 and settings described in Mallinson et al. (2016), and used English↔German as the language pair. The system was trained on data released as part of the WMT15 shared translation task (4.2 million sentence pairs). We also had access to back-translated monolingual training data (Sennrich et al."
D17-1091,D16-1244,0,0.0259126,"Missing"
D17-1091,D16-1054,0,0.0434992,"Missing"
D17-1091,P15-2070,0,0.0669942,"Missing"
D17-1091,D14-1162,0,0.0941715,"del which we call PARA 4QA (as shorthand for learning to paraphrase for question answering) against multiple previous systems on three datasets. In the following we introduce these datasets, provide implementation details for our model, describe the systems used for comparison, and present our results. 3.1 Datasets W IKI QA This dataset (Yang et al., 2015) has 3, 047 questions sampled from Bing query logs. The questions are associated with 29, 258 candidate answer sentences, 1, 473 of which contain the correct answers to the questions. Training For the paraphrase scoring model, we used GloVe (Pennington et al., 2014) vectors3 pretrained on Wikipedia 2014 and Gigaword 5 to initialize the word embedding matrix. We kept this matrix fixed across datasets. Out-of-vocabulary words were replaced with a special unknown symbol. We also augmented questions with start-ofand end-of-sequence symbols. Word vectors for these special symbols were updated during training. Model hyperparameters were validated on the development set. The dimensions of hidden vectors and word embeddings were selected from {50, 100, 200} and {100, 200}, respectively. The dropout rate was selected from {0.2, 0.3, 0.4}. The B I LSTM for the ans"
D17-1091,N16-1170,0,0.0417696,"Missing"
D17-1091,Q16-1010,1,0.849541,"Missing"
D17-1091,P16-1220,1,0.501367,"Missing"
D17-1091,D17-1009,1,0.05169,"Missing"
D17-1091,D15-1237,0,0.0652633,"Missing"
D17-1091,P16-1009,0,0.0474696,"Query graphs for the questions typically contain more than one predicate. For example, to answer the question “who is the ceo of microsoft in 2008”, we need to use one relation to query “ceo of microsoft” and another relation for the constraint “in 2008”. For this task, we employ the S IMPLE G RAPH model described in Reddy et al. (2016, 2017), and follow their training protocol and feature design. In brief, their method uses rules to a ˆ = arg max p a0 |q a0 ∈Cq 879  (8) where Cq is the set of candidate answers, and p (a0 |q) is computed as shown in Equation (1). 3 split into subword units (Sennrich et al., 2016b) to handle out-of-vocabulary words in questions. We used the top 15 decoding results as candidate paraphrases. We used the S size package of PPDB 2.0 (Pavlick et al., 2015) for high precision. At most 10 candidate paraphrases were considered. We mined paraphrase rules from WikiAnswers (Fader et al., 2014) as described in Section 2.1.3. The extracted rules were ranked using the pointwise mutual information between template pairs in the WikiAnswers corpus. The top 10 candidate paraphrases were used. Experiments We compared our model which we call PARA 4QA (as shorthand for learning to paraphra"
D17-1091,N15-3014,0,0.0292453,"Missing"
D17-1091,P16-1162,0,0.0159355,"Query graphs for the questions typically contain more than one predicate. For example, to answer the question “who is the ceo of microsoft in 2008”, we need to use one relation to query “ceo of microsoft” and another relation for the constraint “in 2008”. For this task, we employ the S IMPLE G RAPH model described in Reddy et al. (2016, 2017), and follow their training protocol and feature design. In brief, their method uses rules to a ˆ = arg max p a0 |q a0 ∈Cq 879  (8) where Cq is the set of candidate answers, and p (a0 |q) is computed as shown in Equation (1). 3 split into subword units (Sennrich et al., 2016b) to handle out-of-vocabulary words in questions. We used the top 15 decoding results as candidate paraphrases. We used the S size package of PPDB 2.0 (Pavlick et al., 2015) for high precision. At most 10 candidate paraphrases were considered. We mined paraphrase rules from WikiAnswers (Fader et al., 2014) as described in Section 2.1.3. The extracted rules were ranked using the pointwise mutual information between template pairs in the WikiAnswers corpus. The top 10 candidate paraphrases were used. Experiments We compared our model which we call PARA 4QA (as shorthand for learning to paraphra"
D17-1091,D16-1015,0,0.0326675,"Missing"
D17-1091,P15-1128,0,0.0349307,"Missing"
D17-1091,N15-1091,0,0.0236111,"Missing"
D17-1091,P14-1090,0,\N,Missing
D17-1091,Q15-1039,0,\N,Missing
D17-1091,E17-1083,1,\N,Missing
D17-1091,J13-3001,0,\N,Missing
D19-1424,D19-1539,0,0.0219444,"Missing"
D19-1424,I05-5002,0,0.0145753,"tes the angle between the current optimization direction and the final optimization direction. Then we get the projection values dαi and dβi by computing the deviation degrees between the optimization direction δ i and the axes. 4 Experimental Setup We conduct experiments on four datasets: Multi-genre Natural Language Inference Corpus (MNLI; Williams et al. 2018), Recognizing Textual Entailment (RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009), Stanford Sentiment Treebank (SST-2; Socher et al. 2013), and Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett 2005). We use the same data split as in (Wang et al., 2019). The accuracy metric is used for evaluation. We employ the pre-trained BERT-large model in our experiments. The cased version of tokenizer is used. We follow the settings and the hyperparameters suggested in (Devlin et al., 2018). The Adam (Kingma and Ba, 2015) optimizer is used for fine-tuning. The number of fine-tuning epochs is selected from {3, 4, 5}. For RTE and MRPC, we set the batch size to 32, and the learning rate to Pre-training Gets a Good Initial Point Across Downstream Tasks Fine-tuning BERT on the usually performs significant"
D19-1424,W07-1401,0,0.0278316,"the cross product of two vectors, and k·k denotes the Euclidean norm. To be specific, we first compute cosine similarity between δ i and δ1 , which indicates the angle between the current optimization direction and the final optimization direction. Then we get the projection values dαi and dβi by computing the deviation degrees between the optimization direction δ i and the axes. 4 Experimental Setup We conduct experiments on four datasets: Multi-genre Natural Language Inference Corpus (MNLI; Williams et al. 2018), Recognizing Textual Entailment (RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009), Stanford Sentiment Treebank (SST-2; Socher et al. 2013), and Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett 2005). We use the same data split as in (Wang et al., 2019). The accuracy metric is used for evaluation. We employ the pre-trained BERT-large model in our experiments. The cased version of tokenizer is used. We follow the settings and the hyperparameters suggested in (Devlin et al., 2018). The Adam (Kingma and Ba, 2015) optimizer is used for fine-tuning. The number of fine-tuning epochs is selected from {3, 4, 5}. For RTE and MRPC, we set the ba"
D19-1424,P18-1031,0,0.357239,"to overfitting, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language. 1 Introduction Language model pre-training has achieved strong performance in many NLP tasks (Peters et al., 2018; Howard and Ruder, 2018a; Radford et al., 2018; Devlin et al., 2018; Baevski et al., 2019; Dong et al., 2019). A neural encoder is trained on a large text corpus by using language modeling objectives. Then the pre-trained model either is used to extract vector representations for input, or is fine-tuned on the specific datasets. Recent work (Tenney et al., 2019b; Liu et al., 2019a; Goldberg, 2019; Tenney et al., 2019a) has shown that the pre-trained models can encode syntactic and semantic information of language. However, it is unclear why pre-training ∗ Contribution during internship at Microsoft Research. is effe"
D19-1424,P18-1132,0,0.0733257,"Missing"
D19-1424,Q16-1037,0,0.0469084,"ons of language, which makes them more invariant across tasks. Moreover, high layers seem to play a more important role in learning task-specific information during fine-tuning. 8 Related Work Pre-trained contextualized word representations learned from language modeling objectives, such as CoVe (McCann et al., 2017), ELMo (Peters et al., 2018), ULMFit (Howard and Ruder, 2018b), GPT (Radford et al., 2018, 2019), and BERT (Devlin et al., 2018), have shown strong performance on a variety of natural language processing tasks. Recent work of inspecting the effectiveness of the pre-trained models (Linzen et al., 2016; Kuncoro et al., 2018; Tenney et al., 2019b; Liu et al., 2019a) focuses on analyzing the syntactic and semantic properties. Tenney et al. (2019b) and Liu et al. (2019a) suggest that pre-training helps the models to encode much syntactic information and many transferable features through evaluating models on several probing tasks. Goldberg (2019) assesses the syntactic abilities of BERT and draws the similar conclusions. Our work explores the effectiveness of pre-training from another angle. We propose to visualize the loss landscapes and optimization trajectories of the BERT fine-tuning proce"
D19-1424,N19-1112,0,0.333702,"ine-tuning, which suggests that the layers that are close to input learn more transferable representations of language. 1 Introduction Language model pre-training has achieved strong performance in many NLP tasks (Peters et al., 2018; Howard and Ruder, 2018a; Radford et al., 2018; Devlin et al., 2018; Baevski et al., 2019; Dong et al., 2019). A neural encoder is trained on a large text corpus by using language modeling objectives. Then the pre-trained model either is used to extract vector representations for input, or is fine-tuned on the specific datasets. Recent work (Tenney et al., 2019b; Liu et al., 2019a; Goldberg, 2019; Tenney et al., 2019a) has shown that the pre-trained models can encode syntactic and semantic information of language. However, it is unclear why pre-training ∗ Contribution during internship at Microsoft Research. is effective on downstream tasks in terms of both trainability and generalization capability. In this work, we take BERT (Devlin et al., 2018) as an example to understand the effectiveness of pretraining. We visualize the loss landscapes and the optimization procedure of fine-tuning on specific datasets in three ways. First, we compute the one-dimensional (1D) los"
D19-1424,P19-1441,0,0.309652,"ine-tuning, which suggests that the layers that are close to input learn more transferable representations of language. 1 Introduction Language model pre-training has achieved strong performance in many NLP tasks (Peters et al., 2018; Howard and Ruder, 2018a; Radford et al., 2018; Devlin et al., 2018; Baevski et al., 2019; Dong et al., 2019). A neural encoder is trained on a large text corpus by using language modeling objectives. Then the pre-trained model either is used to extract vector representations for input, or is fine-tuned on the specific datasets. Recent work (Tenney et al., 2019b; Liu et al., 2019a; Goldberg, 2019; Tenney et al., 2019a) has shown that the pre-trained models can encode syntactic and semantic information of language. However, it is unclear why pre-training ∗ Contribution during internship at Microsoft Research. is effective on downstream tasks in terms of both trainability and generalization capability. In this work, we take BERT (Devlin et al., 2018) as an example to understand the effectiveness of pretraining. We visualize the loss landscapes and the optimization procedure of fine-tuning on specific datasets in three ways. First, we compute the one-dimensional (1D) los"
D19-1424,N18-1202,0,0.248143,"g procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language. 1 Introduction Language model pre-training has achieved strong performance in many NLP tasks (Peters et al., 2018; Howard and Ruder, 2018a; Radford et al., 2018; Devlin et al., 2018; Baevski et al., 2019; Dong et al., 2019). A neural encoder is trained on a large text corpus by using language modeling objectives. Then the pre-trained model either is used to extract vector representations for input, or is fine-tuned on the specific datasets. Recent work (Tenney et al., 2019b; Liu et al., 2019a; Goldberg, 2019; Tenney et al., 2019a) has shown that the pre-trained models can encode syntactic and semantic information of language. However, it is unclear why pre-training ∗ Contribution during internship at Mic"
D19-1424,N18-1101,0,0.0313096,"= vcos = kδ1 k kδ1 k2 s kδ i k 2 dβi = ( ) − (dαi )2 kδ1 k vcos = kδ i k (3) (4) (5) where × denotes the cross product of two vectors, and k·k denotes the Euclidean norm. To be specific, we first compute cosine similarity between δ i and δ1 , which indicates the angle between the current optimization direction and the final optimization direction. Then we get the projection values dαi and dβi by computing the deviation degrees between the optimization direction δ i and the axes. 4 Experimental Setup We conduct experiments on four datasets: Multi-genre Natural Language Inference Corpus (MNLI; Williams et al. 2018), Recognizing Textual Entailment (RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009), Stanford Sentiment Treebank (SST-2; Socher et al. 2013), and Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett 2005). We use the same data split as in (Wang et al., 2019). The accuracy metric is used for evaluation. We employ the pre-trained BERT-large model in our experiments. The cased version of tokenizer is used. We follow the settings and the hyperparameters suggested in (Devlin et al., 2018). The Adam (Kingma and Ba, 2015) optimizer is used for fi"
D19-1424,W19-4302,0,0.0223545,"1 2 3 4 2.0 1.5 1.0 0.5 0.0 1.0 0.5 0.0 Figure 7: Layer-wise training loss surfaces on the MNLI dataset (top) and the MRPC dataset (bottom). The dotshaped point represents the fine-tuned model. The star-shaped point represents the model with rollbacking different layer groups. Low layers correspond to the 0th-7th layers of BERT, middle layers correspond to the 8th-15th layers, and high layers correspond to the 16th-23rd layers. BERT can achieve better generalization capability than training from scratch. Liu et al. (2019a) find that different layers of BERT exhibit different transferability. Peters et al. (2019) show that the classification tasks build up information mainly in the intermediate and last layers of BERT. Tenney et al. (2019a) observe that low layers of BERT encode more local syntax, while high layers capture more complex semantics. Zhang et al. (2019) also show that not all layers of a deep neural model have equal contributions to model performance. We draw the similar conclusion by visualizing layer-wise loss surface of BERT on downstream tasks. Besides, we find that low layers of BERT are more invariant and transferable across datasets. In the computer vision community, many efforts h"
D19-1424,D13-1170,0,0.042939,"we first compute cosine similarity between δ i and δ1 , which indicates the angle between the current optimization direction and the final optimization direction. Then we get the projection values dαi and dβi by computing the deviation degrees between the optimization direction δ i and the axes. 4 Experimental Setup We conduct experiments on four datasets: Multi-genre Natural Language Inference Corpus (MNLI; Williams et al. 2018), Recognizing Textual Entailment (RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009), Stanford Sentiment Treebank (SST-2; Socher et al. 2013), and Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett 2005). We use the same data split as in (Wang et al., 2019). The accuracy metric is used for evaluation. We employ the pre-trained BERT-large model in our experiments. The cased version of tokenizer is used. We follow the settings and the hyperparameters suggested in (Devlin et al., 2018). The Adam (Kingma and Ba, 2015) optimizer is used for fine-tuning. The number of fine-tuning epochs is selected from {3, 4, 5}. For RTE and MRPC, we set the batch size to 32, and the learning rate to Pre-training Gets a Good Initial Point Ac"
D19-1424,P19-1452,0,0.436118,"ore invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language. 1 Introduction Language model pre-training has achieved strong performance in many NLP tasks (Peters et al., 2018; Howard and Ruder, 2018a; Radford et al., 2018; Devlin et al., 2018; Baevski et al., 2019; Dong et al., 2019). A neural encoder is trained on a large text corpus by using language modeling objectives. Then the pre-trained model either is used to extract vector representations for input, or is fine-tuned on the specific datasets. Recent work (Tenney et al., 2019b; Liu et al., 2019a; Goldberg, 2019; Tenney et al., 2019a) has shown that the pre-trained models can encode syntactic and semantic information of language. However, it is unclear why pre-training ∗ Contribution during internship at Microsoft Research. is effective on downstream tasks in terms of both trainability and generalization capability. In this work, we take BERT (Devlin et al., 2018) as an example to understand the effectiveness of pretraining. We visualize the loss landscapes and the optimization procedure of fine-tuning on specific datasets in three ways. First, we compute the one-d"
D19-5802,D14-1179,0,0.0154774,"Missing"
D19-5802,N18-2074,0,0.0275226,"toolkit to preprocess data. We use 300-dimensional GloVe embeddings (Pennington et al., 2014) to initialize word vectors of both questions and passages, and keep them fixed during training. A special trainable token <UNK> is used to represent out-of-vocabulary words. We randomly mask some words in the passage to <UNK> with 0.2 probability while training. The dimension of character embedding and segment embedding is 64 and 128, respectively. The number of Transformer layers used in our model is 12. For each Transformer layer, we set the hidden size to 128, and use relative position embedding (Shaw et al., 2018) whose clipping distance is 16. The number of the attention heads is 8. During training, the batch size is 32 and the number of the max training epochs is 80. We use 3.2 Results Exact match (EM) and F1 scores are two evaluation metrics for SQuAD. EM measures the percentage of the prediction that matches the groundtruth answer exactly, while F1 measures the overlap between the predicted answer answer and the ground-truth answer. The scores on the development set are evaluated by the official script. As shown in Table 1, the unified model outperforms previous state-of-the-art models and the base"
D19-5802,P82-1020,0,0.673204,"Missing"
D19-5802,P17-1147,0,0.024587,"er for the MRC task. Experimental results on SQuAD show that the unified model outperforms previous networks that separately treat encoding and matching. We also introduce a metric to inspect whether a Transformer layer tends to perform encoding or matching. The analysis results show that the unified model learns different modeling strategies compared with previous manually-designed models. 1 Introduction In spite of different neural network structures, encoding and matching components are two basic building blocks for many NLP tasks like machine reading comprehension (Rajpurkar et al., 2016; Joshi et al., 2017). A widely-used paradigm is that the input texts are encoded into vectors, and then these vectors are aggregated to model interactions between them by matching layers. Figure 1(a) shows a typical machine reading comprehension model, encoding components separately encode question and passage to vector representations. Then, we obtain context-sensitive representations for input words by considering the interactions between question and passage. Finally, an output layer is used to predict the prob∗ Contribution during internship at Microsoft Research 14 Proceedings of the Second Workshop on Machi"
D19-5802,P17-1018,1,0.922662,"soft Research hangbobao@gmail.com,piaosh@hit.edu.cn lidong1,fuwei,wenwan,nanya,lecu,mingzhou@microsoft.com Abstract ability of each token being the start or end position of the answer span. The encoding layers are usually built upon recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), and self-attention networks (Yu et al., 2018). For the matching component, various model components have been developed to fuse question and passage vector representations, such as match-LSTM (Wang and Jiang, 2016), coattention (Seo et al., 2016; Xiong et al., 2016), and self-matching (Wang et al., 2017). Recently, Devlin et al. (2018) employ Transformer networks to pretrain a bidirectional language model (called BERT), and then fine-tune the layers on specific tasks, which obtains state-of-the-art results on MRC. A research question is: apart from the benefits of pretraining, how many performance gain comes from the unified network architecture. In this paper, we evaluate and analyze unifying encoding and matching components with Transformer layers (Vaswani et al., 2017), using MRC as a case study. As shown in Figure 1(b), compared with previous specially-designed MRC networks, we do not exp"
D19-5802,D14-1162,0,0.0867023,"re Transformer layers used to compute the question-sensitive passage representations. To make a fair comparison, we only compare with the models that do not rely on pre-trained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). Experiments Experimental Setup Dataset Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is composed of over 100,000 instances created by crowdworkers. Every answer is constrained to be a continuous sub-span of the passage. Settings We employ the spaCy toolkit to preprocess data. We use 300-dimensional GloVe embeddings (Pennington et al., 2014) to initialize word vectors of both questions and passages, and keep them fixed during training. A special trainable token <UNK> is used to represent out-of-vocabulary words. We randomly mask some words in the passage to <UNK> with 0.2 probability while training. The dimension of character embedding and segment embedding is 64 and 128, respectively. The number of Transformer layers used in our model is 12. For each Transformer layer, we set the hidden size to 128, and use relative position embedding (Shaw et al., 2018) whose clipping distance is 16. The number of the attention heads is 8. Duri"
D19-5802,N18-1202,0,0.0240129,"Apart from comparing with previous state-of-the-art models (Seo et al., 2016; Wang et al., 2017; Yu et al., 2018), we implement a baseline model that separately perform encoding and matching. The same settings as above are used. The first three Transformer layers are utilized to encode passage and question separately. Then we add a passage-question matching layer following Yu et al. (2018), with nine more Transformer layers used to compute the question-sensitive passage representations. To make a fair comparison, we only compare with the models that do not rely on pre-trained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). Experiments Experimental Setup Dataset Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is composed of over 100,000 instances created by crowdworkers. Every answer is constrained to be a continuous sub-span of the passage. Settings We employ the spaCy toolkit to preprocess data. We use 300-dimensional GloVe embeddings (Pennington et al., 2014) to initialize word vectors of both questions and passages, and keep them fixed during training. A special trainable token <UNK> is used to represent out-of-vocabulary words. We randomly ma"
E17-1059,P12-1039,1,0.752484,"our encoderdecoder model, which has been proved very helpful in various tasks (Bahdanau et al., 2015; Xu et al., 2015), to generate user- and product-specific reviews. Maqsud (2015) compare latent Dirichlet allocation, Markov chains, and hidden Markov models for text generation on review data. However, we focus on generating product reviews conditioned on input attributes. Park et al. (2015) propose to retrieve relevant opinion sentences using product specifications as queries, while we work on generation instead of retrieval. Our task definition is also related to concept-totext generation (Konstas and Lapata, 2012; Konstas and Lapata, 2013), such as generating weather forecast or sportscasting from database records. A typical system contains three main stages: content planning, sentence planning, and surface realization. Mei et al. (2016) treat database records and output texts as sequences, and use recurrent neural networks to encode and decode them. In contrast, our input is a set of discrete attributes instead of database records or sequences. In addition, the contents of database records are strong constraints on results in concept-to-text generation. However, in our setting, user and product infor"
E17-1059,P16-1004,1,0.841718,"model in order to generate reviews conditioned on input attributes. • We create a dataset based on Amazon book 624 predict results by conditioning outputs on the encoding vectors. This general framework is flexible because different neural networks can be used for encoders and decoders depending on the nature of inputs and outputs, which has been used to address various tasks. For example, recurrent neural networks are used to model sequences, such as machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), syntactic parsing (Vinyals et al., 2015b), and semantic parsing (Dong and Lapata, 2016). Additionally, convolutional neural networks are employed for image data, such as image caption generation (Vinyals et al., 2015a), and video description generation (Donahue et al., 2015; Venugopalan et al., 2015). Our model employs multilayer perceptron to encode attribute information, and uses recurrent neural networks to decode product reviews. In order to better handle alignments between inputs and outputs, the attention mechanism is introduced for the encoder-decoder model. The attention model boosts performance for various tasks (Bahdanau et al., 2015; Luong et al., 2015; Xu et al., 201"
E17-1059,D15-1166,0,0.0304329,"Missing"
E17-1059,W15-2922,0,0.026596,"ries. Recently, deep learning has achieved promising results on sentiment analysis (Socher et al., 2011; Dong et al., 2014; Kim, 2014). Lipton et al. (2015) use character-level concatenated input recurrent neural networks as a generative model to predict rating and category for reviews. In contrast, our model is mainly evaluated on the review generation task rather than classification. Moreover, we use an attention mechanism in our encoderdecoder model, which has been proved very helpful in various tasks (Bahdanau et al., 2015; Xu et al., 2015), to generate user- and product-specific reviews. Maqsud (2015) compare latent Dirichlet allocation, Markov chains, and hidden Markov models for text generation on review data. However, we focus on generating product reviews conditioned on input attributes. Park et al. (2015) propose to retrieve relevant opinion sentences using product specifications as queries, while we work on generation instead of retrieval. Our task definition is also related to concept-totext generation (Konstas and Lapata, 2012; Konstas and Lapata, 2013), such as generating weather forecast or sportscasting from database records. A typical system contains three main stages: content"
E17-1059,P82-1020,0,0.85057,"Missing"
E17-1059,D13-1176,0,0.0263763,"valuable for sentiment analysis, but not well studied previously. • We propose an attention-enhanced attributeto-sequence model in order to generate reviews conditioned on input attributes. • We create a dataset based on Amazon book 624 predict results by conditioning outputs on the encoding vectors. This general framework is flexible because different neural networks can be used for encoders and decoders depending on the nature of inputs and outputs, which has been used to address various tasks. For example, recurrent neural networks are used to model sequences, such as machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), syntactic parsing (Vinyals et al., 2015b), and semantic parsing (Dong and Lapata, 2016). Additionally, convolutional neural networks are employed for image data, such as image caption generation (Vinyals et al., 2015a), and video description generation (Donahue et al., 2015; Venugopalan et al., 2015). Our model employs multilayer perceptron to encode attribute information, and uses recurrent neural networks to decode product reviews. In order to better handle alignments between inputs and outputs, the attention mechanism is introduced for the encoder-decoder model. T"
E17-1059,N16-1086,0,0.0159003,"nd hidden Markov models for text generation on review data. However, we focus on generating product reviews conditioned on input attributes. Park et al. (2015) propose to retrieve relevant opinion sentences using product specifications as queries, while we work on generation instead of retrieval. Our task definition is also related to concept-totext generation (Konstas and Lapata, 2012; Konstas and Lapata, 2013), such as generating weather forecast or sportscasting from database records. A typical system contains three main stages: content planning, sentence planning, and surface realization. Mei et al. (2016) treat database records and output texts as sequences, and use recurrent neural networks to encode and decode them. In contrast, our input is a set of discrete attributes instead of database records or sequences. In addition, the contents of database records are strong constraints on results in concept-to-text generation. However, in our setting, user and product information implicitly indicates the style of generated reviews, which makes the results extremely diverse. Another line of related work is the encoderdecoder model with neural networks. Specifically, an encoder is employed to encode"
E17-1059,P02-1040,0,0.117645,"employs distributed representations to avoid using sparse indicator features. Then, we evaluate the NN methods that use different attributes to retrieve reviews, which is a strong baseline for the generation task. The results show that our method outperforms the baseline methods. Moreover, the improvements of the attention mechanism are significant with p < 0.05 according to the bootstrap resampling test (Koehn, 2004). We further show some examples to analyze the attention model in Section 4.4. we use the greedy search algorithm to generate reviews. 4.3 MELM 59.00 Evaluation Results The BLEU (Papineni et al., 2002) score is used for automatic evaluation, which has been shown to correlate well with human judgment on many generation tasks. The BLEU score measures the precision of n-gram matching by comparing the generated results with references, and penalizes length using a brevity penalty term. We compute BLEU-1 (unigram) and BLEU-4 (up to 4 grams) in experiments. 4.3.1 Comparison with Baseline Methods We describe the comparison methods as follows: Rand. The predicted results are randomly sampled from all the reviews in the T RAIN set. This baseline method suggests the expected lower bound for this task"
E17-1059,D14-1181,0,0.00361927,"nce against baseline methods. Moreover, we demonstrate that the attention mechanism significantly improves the performance of our model. The contributions of this work are three-fold: 2 Related Work Sentiment analysis and opinion mining aim to identify and extract subjective content in text (Liu, 2015). Most previous work focuses on using rulebased methods or machine learning techniques for sentiment classification, which classifies reviews into different sentiment categories. Recently, deep learning has achieved promising results on sentiment analysis (Socher et al., 2011; Dong et al., 2014; Kim, 2014). Lipton et al. (2015) use character-level concatenated input recurrent neural networks as a generative model to predict rating and category for reviews. In contrast, our model is mainly evaluated on the review generation task rather than classification. Moreover, we use an attention mechanism in our encoderdecoder model, which has been proved very helpful in various tasks (Bahdanau et al., 2015; Xu et al., 2015), to generate user- and product-specific reviews. Maqsud (2015) compare latent Dirichlet allocation, Markov chains, and hidden Markov models for text generation on review data. However"
J15-2004,W11-0705,0,0.0607269,"structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina, Yue, and Cardie (2010) used sentence-level latent variables to improve document-level ¨ and McDonald (2011a) presented a latent variable model for prediction. T¨ackstrom only using document-level annotations to learn sentence-level sentiment labels, and ¨ and McDonald (2011b) improved it by using a semi-supervised latent variT¨ackstrom able model to utilize manually crafted sentence labels. Agarwal et al. (2011) and Tu et al. (2012) explored part-of-speech tag features and tree-kernel. Wang and Manning (2012) used SVM built over Na¨ıve Bayes log-count ratios as feature values to classify polarity. They showed that SVM was better at full-length reviews, and Multinomial Na¨ıve Bayes was better at short-length reviews. Liu, Agam, and Grossman (2012) proposed a set of heuristic rules based on dependency structure to detect negations and sentimentbearing expressions. Most of these methods are built on bag-of-words features, and sentiment compositions are handled by manually crafted rules. In contrast to t"
J15-2004,Q13-1005,0,0.00541006,"al of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a natural language sentence to its semantic representation without annotated logical form. In this work, we build a sentiment parser. Specifically, we use a modified version of the"
J15-2004,P14-1091,1,0.77232,"h iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a natural language sentence to its semantic representation without annotated logical form. In this work, we build a sentiment parser. Specifically, we use a modified version of the CYK algorithm that parses sentences in a bottom–up fashion. We use the log-linear model to score candidates generated by beam search. Instead of using question-answ"
J15-2004,P05-1022,0,0.023269,"ssification compositions by defining sentiment grammar and borrowing some techniques in the parsing research field. Moreover, our method uses symbolic representations instead of vector spaces. 2.2 Syntactic Parsing and Semantic Parsing The work presented in this article is close to traditional statistical parsing, as we borrow some algorithms to build the sentiment parser. Syntactic parsers are learned from the Treebank corpora, and find the most likely parse tree with the largest probability. In this article, we borrow some well-known techniques from syntactic parsing methods (Charniak 1997; Charniak and Johnson 2005; McDonald, Crammer, and Pereira 2005; ¨ Kubler, McDonald, and Nivre 2009), such as the CYK algorithm and Context-Free Grammar. These techniques are used to build the sentiment grammar and parsing model. They provide a natural way of defining the structure of sentiment trees and parse sentences to trees. The key difference lies in that our task is to calculate the polarity label of a sentence, instead of obtaining the parse tree. We only have sentencepolarity pairs as our training instances instead of annotated tree structures. Moreover, in the decoding process, our goal is to compute correct"
J15-2004,J07-2003,0,0.0181793,"represent the form of an inference rule as: (r) H1 . . . [i, X, j] HK (8) where, if all the terms r and Hk are true, then we can infer [i, X, j] as true. Here, r denotes a sentiment rule, and Hk denotes an item. When we refer to both rules and items, we employ the word terms. Theoretically, we can convert the sentiment rules to CNF versions, and then use the CYK algorithm to conduct parsing. Because the maximum number of non-terminal symbols in a rule is already restricted to two, we formulate the statistical sentiment parsing based on a customized CYK algorithm that is similar to the work of Chiang (2007). Let X, X1 , X2 represent the non-terminals N or P; the inference rules for the statistical sentiment parsing are summarized in Figure 3. 3.3 Ranking Model The parsing model generates many candidate parse trees T(s) for a sentence s. The goal of the ranking model is to score and rank these parse trees. The sentiment tree with the highest score is treated as the best representation for sentence s. We extract a feature vector φ(s, t) ∈ Rd for the specific sentence-tree pair (s, t), where t ∈ T(s) is the parse tree. Let ψ ∈ Rd be the parameter vector for the features. We use the log-linear model"
J15-2004,D08-1083,0,0.669511,"contrast] The negation expressions, intensification modifiers, and the contrastive conjunction can change the polarity (Examples (1), (3), (4), (5)), strength (Examples (2), (3), (5)), or both (Examples (3), (5)) of the sentiment of the sentences. We do not need any detailed explanations here as they can be commonly found and easily understood in people’s 1 http://twitter.com. 2 http://www.imdb.com. 266 Dong et al. A Statistical Parsing Framework for Sentiment Classification daily lives. Existing works to address these issues usually rely on syntactic parsing results either used as features (Choi and Cardie 2008; Moilanen, Pulman, and Zhang 2010) in learning-based methods or hand-crafted rules (Moilanen and Pulman 2007; Jia, Yu, and Meng 2009; Klenner, Petrakis, and Fahrni 2009; Liu and Seneff 2009) in lexiconbased methods. However, even with the difficulty and feasibility of deriving the sentiment structure from syntactic parsing results put aside, it is an even more challenging task to generate stable and reliable parsing results for text that is ungrammatical in nature and has a high ratio of out-of-vocabulary words. The accuracy of the linguistic parsers trained on standard data sets (e.g., the P"
J15-2004,D09-1062,0,0.0171966,"Missing"
J15-2004,P10-2050,0,0.0145972,"tect negations and sentimentbearing expressions. Most of these methods are built on bag-of-words features, and sentiment compositions are handled by manually crafted rules. In contrast to these models, we derive polarity labels from tree structures parsed by the sentiment grammar. There have been several attempts to assume that the problem of sentiment analysis is compositional. Sentiment classification can be solved by deriving the sentiment of a complex constituent (sentence) from the sentiment of small units (words and phrases) (Moilanen and Pulman 2007; Klenner, Petrakis, and Fahrni 2009; Choi and Cardie 2010; Nakagawa, Inui, and Kurohashi 2010). Moilanen and Pulman (2007) proposed using delicate written linguistic patterns as heuristic decision rules when computing the sentiment from individual words to phrases and finally to the sentence. The manually compiled rules were powerful enough to discriminate between the different sentiments in effective remedies (positive) / effective torture (negative), and in too colorful (negative) and too sad (negative). Nakagawa, Inui, and Kurohashi (2010) leveraged a conditional random field model to calculate the sentiment of all the parsed elements in the depe"
J15-2004,W10-2903,0,0.0205801,"parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without a"
J15-2004,W10-3110,0,0.0535975,"Missing"
J15-2004,C10-2028,0,0.0707075,"Missing"
J15-2004,P10-1018,0,0.0204095,"Missing"
J15-2004,J99-4004,0,0.0826633,"In order to tackle the OOV problem, we treat a text span that consists of OOV words as empty text span, and derive them to E. The OOV text spans are combined with other text spans without considering their sentiment information. Finally, each sentence is derived to the symbol S using the start rules that are the beginnings of derivations. We can use the sentiment grammar to compactly describe the derivation process of a sentence. 3.2 Parsing Model We present the formal description of the statistical sentiment parsing model following deductive proof systems (Shieber, Schabes, and Pereira 1995; Goodman 1999) as used in traditional syntactic parsing. For a concrete example, (A → BC) 274 [i, B, k] [i, A, j] [k, C, j] (6) Dong et al. A Statistical Parsing Framework for Sentiment Classification ∗ ∗ j ∗ which represents if we have the rule A → BC and B ⇒ wki and C ⇒ wk (⇒ is used to represent the reflexive and transitive closure of immediate derivation), then we can ∗ j obtain A ⇒ wi . By adding a unary rule j (A → w i ) [i, A, j] (7) with the binary rule in Equation (6), we can express the standard CYK algorithm for CFG in Chomsky Normal Form (CNF). And the goal is [0, S, n], in which S is the start"
J15-2004,P14-1022,0,0.0365465,"Missing"
J15-2004,P97-1023,0,0.045005,"rage and domain adaption problems. Moreover, lexicons are often built and used without considering the context (Wilson, Wiebe, and Hoffmann 2009). Also, hand-crafted rules are often matched heuristically. The sentiment dictionaries used for lexicon-based sentiment analysis can be created manually, or automatically using seed words to expand the list of words. Kamps et al. (2004) and Williams and Anand (2009) used various lexical relations (such as synonym and antonym relations) in WordNet to expend a set of seed words. Some other methods learn lexicons from data directly. Hatzivassiloglou and McKeown (1997) used a log-linear regression model with conjunction constraints to predict whether conjoined adjectives have similar or different polarities. Combining conjunction constraints across many adjectives, a clustering algorithm separated the adjectives into groups of different polarity. Finally, adjectives were labeled as positive or negative. Velikovich et al. (2010) constructed a term similarity graph using the cosine similarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polar"
J15-2004,D07-1115,0,0.0357653,"akamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2 -based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas, Zhang, and Levene (2012) used word count as a feature template and trained a classifier using Support Vector Machines with linear kernel. They then regarded the weights as polarity strengths. Krestel and Siersdorfer (2013) generated topicdependent lexicons from review articles by incorporating topic and rating probabilities and defined the polarity stren"
J15-2004,kamps-etal-2004-using,0,0.0896409,"Missing"
J15-2004,W06-1642,0,0.0313257,"adjectives, a clustering algorithm separated the adjectives into groups of different polarity. Finally, adjectives were labeled as positive or negative. Velikovich et al. (2010) constructed a term similarity graph using the cosine similarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2 -based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas"
J15-2004,P06-1115,0,0.0399416,"latent sentiment trees. Recently, Hall, Durrett, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) pr"
J15-2004,P03-1054,0,0.0210265,"ey regard the composition operator as a matrix, and use matrix-vector multiplication to obtain the transformed vector representation. Socher et al. (2012) proposed a recursive neural network model that learned compositional vector representations for phrases and sentences. Their model assigned a vector and a matrix to every node in a parse tree. The vector captured the inherent meaning of the constituent, and the matrix captured how it changes the meaning of neighboring words or phrases. Socher et al. (2013) recently introduced a sentiment treebank based on the results of the Stanford parser (Klein and Manning 2003). The sentiment treebank included polarity labels of phrases that are annotated using Amazon Mechanical Turk. The authors trained recursive neural tensor networks on the sentiment treebank. For a new sentence, the model predicted polarity labels based on the syntactic parse tree, and used tensors to handle compositionality in the vector space. Dong et al. (2014) proposed utilizing multiple composition functions in recursive neural models and learning to select them adaptively. Most previous methods are either rigid in terms of handcrafted rules, or sensitive to the performance of existing synt"
J15-2004,R09-1034,0,0.0605772,"Missing"
J15-2004,D12-1069,0,0.0126019,"ount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a n"
J15-2004,J13-2005,0,0.0371933,"Missing"
J15-2004,D09-1017,0,0.0285956,"Missing"
J15-2004,C12-2072,0,0.0397133,"Missing"
J15-2004,P11-1015,0,0.111172,"en widely recognized that sentiment expressions are colloquial and evolve over time very frequently. Taking tweets from Twitter1 and movie reviews on IMDb2 as examples, people use very casual language as well as informal and new vocabulary to comment on general topics and movies. In practice, it is not feasible to create and maintain sentiment lexicons to capture sentiment expressions with high coverage. On the other hand, the learning-based approach relies on large annotated samples to overcome the vocabulary coverage and deals with variations of words in sentences. Human ratings in reviews (Maas et al. 2011) and emoticons in tweets (Davidov, Tsur, and Rappoport 2010; Zhao et al. 2012) are extensively used to collect a large number of training corpora to train the sentiment classifier. However, it is usually not easy to design effective features to build the classifier. Among others, unigrams have been reported as the most effective features (Pang, Lee, and Vaithyanathan 2002) in sentiment classification. Handling complicated expressions delivering people’s opinions is one of the most challenging problems in sentiment analysis. Compositionalities such as negation, intensification, contrast, and th"
J15-2004,J93-2004,0,0.0523085,"Missing"
J15-2004,P05-1012,0,0.100479,"Missing"
J15-2004,P07-1055,0,0.014092,"ated the contribution of different features including unigrams, bigrams, adjectives, and part-of-speech tags. Their experimental results suggested that a SVM classifier with unigram presence features outperforms other competitors. Pang and Lee (2004) separated subjective portions from the objective by finding minimum cuts in graphs to achieve better sentiment classification performance. Matsumoto, Takamura, and Okumura (2005) used text mining techniques to 269 Computational Linguistics Volume 41, Number 2 extract frequent subsequences and dependency subtrees, and used them as features of SVM. McDonald et al. (2007) investigated a global structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina, Yue, and Cardie (2010) used sentence-level latent variables to improve document-level ¨ and McDonald (2011a) presented a latent variable model for prediction. T¨ackstrom only using document-level annotations to learn sentence-level sentiment labels, and ¨ and McDonald (2011b) improved it by using a semi-supervised latent variT¨ackstrom able model to utilize manually cra"
J15-2004,N10-1120,0,0.417582,"Missing"
J15-2004,P04-1035,0,0.1445,"r of classes in sentiment classification is smaller than that in topic-based text classification (Pang and Lee 2008). Pang, Lee, and Vaithyanathan (2002) investigated three machine learning methods to produce automated classifiers to generate class labels for movie reviews. They tested them on Na¨ıve Bayes, Maximum Entropy, and Support Vector Machine (SVM), and evaluated the contribution of different features including unigrams, bigrams, adjectives, and part-of-speech tags. Their experimental results suggested that a SVM classifier with unigram presence features outperforms other competitors. Pang and Lee (2004) separated subjective portions from the objective by finding minimum cuts in graphs to achieve better sentiment classification performance. Matsumoto, Takamura, and Okumura (2005) used text mining techniques to 269 Computational Linguistics Volume 41, Number 2 extract frequent subsequences and dependency subtrees, and used them as features of SVM. McDonald et al. (2007) investigated a global structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina,"
J15-2004,P05-1015,0,0.976359,"over, the Stanford Sentiment Treebank5 contains polarity labels of all syntactically plausible phrases. In addition, we use the MPQA6 data set for the phrase-level task. We describe these data sets as follows. RT-C: 436,000 critic reviews from Rotten Tomatoes. It consists of 218,000 negative and 218,000 positive critic reviews. The average review length is 23.2 words. Critic reviews from Rotten Tomatoes contain a label (Rotten: Negative, Fresh: Positive) to indicate the polarity, which we use directly as the polarity label of corresponding reviews. PL05-C: The sentence polarity data set v1.0 (Pang and Lee 2005) contains 5,331 positive and 5,331 negative snippets written by critics from Rotten Tomatoes. This data set is widely used as the benchmark data set in the sentence-level polarity classification task. The data source is the same as RT-C. SST: The Stanford Sentiment Treebank (Socher et al. 2013) is built upon PL05-C. The sentences are parsed to parse trees. Then, 215,154 syntactically plausible phrases are extracted and annotated by workers from Amazon Mechanical Turk. The experimental settings of positive/negative classification for sentences are the same as in Socher et al. (2013). RT-U: 737,"
J15-2004,W02-1011,0,0.0223195,"Missing"
J15-2004,P06-2034,0,0.0277218,"s. Recently, Hall, Durrett, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration,"
J15-2004,D12-1110,0,0.0833857,"Missing"
J15-2004,D11-1014,0,0.2693,"Missing"
J15-2004,D13-1170,0,0.308991,"egation for sentiment analysis. Some other methods model sentiment compositionality in the vector space. They regard the composition operator as a matrix, and use matrix-vector multiplication to obtain the transformed vector representation. Socher et al. (2012) proposed a recursive neural network model that learned compositional vector representations for phrases and sentences. Their model assigned a vector and a matrix to every node in a parse tree. The vector captured the inherent meaning of the constituent, and the matrix captured how it changes the meaning of neighboring words or phrases. Socher et al. (2013) recently introduced a sentiment treebank based on the results of the Stanford parser (Klein and Manning 2003). The sentiment treebank included polarity labels of phrases that are annotated using Amazon Mechanical Turk. The authors trained recursive neural tensor networks on the sentiment treebank. For a new sentence, the model predicted polarity labels based on the syntactic parse tree, and used tensors to handle compositionality in the vector space. Dong et al. (2014) proposed utilizing multiple composition functions in recursive neural models and learning to select them adaptively. Most pre"
J15-2004,J11-2001,0,0.865765,"28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 1. Introduction Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from both research and industry communities in recent years. Sentiment classification, which identifies sentiment polarity (positive or negative) from text (sentence or document), has been the most extensively studied task in sentiment analysis. Until now, there have been two mainstream approaches for sentiment classification. The lexicon-based approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity of a sentence from the polarity of words or phrases found in the sentence, and the learning-based approach (Pang, Lee, and Vaithyanathan 2002) treats sentiment polarity identification as a special text classification task and focuses on building classifiers from a set of sentences (or documents) annotated with their corresponding sentiment polarity. The lexicon-based sentiment classification approach is simple and interpretable, but suffers from scalability and is inevitably limited by sentiment lexicons that are commonly created manually by experts. I"
J15-2004,P11-2100,0,0.0312767,"Missing"
J15-2004,P05-1017,0,0.0398978,"Missing"
J15-2004,P12-2066,0,0.204703,"Missing"
J15-2004,P02-1053,0,0.0173161,"publication: 28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 1. Introduction Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from both research and industry communities in recent years. Sentiment classification, which identifies sentiment polarity (positive or negative) from text (sentence or document), has been the most extensively studied task in sentiment analysis. Until now, there have been two mainstream approaches for sentiment classification. The lexicon-based approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity of a sentence from the polarity of words or phrases found in the sentence, and the learning-based approach (Pang, Lee, and Vaithyanathan 2002) treats sentiment polarity identification as a special text classification task and focuses on building classifiers from a set of sentences (or documents) annotated with their corresponding sentiment polarity. The lexicon-based sentiment classification approach is simple and interpretable, but suffers from scalability and is inevitably limited by sentiment lexicons that are commonly created"
J15-2004,J09-3003,0,0.0514213,"Missing"
J15-2004,D10-1102,0,0.041237,"Missing"
J15-2004,W03-1017,0,0.152178,"n this article. Regardless of what granularity the task is performed on, existing approaches deriving sentiment polarity from text fall into two major categories, namely, lexicon-based and learning-based approaches. The lexicon-based sentiment analysis uses dictionary matching on a predefined sentiment lexicon to derive sentiment polarity. These methods often use a set of manually defined rules to deal with the negation of polarity. Turney (2002) proposed using the average sentiment orientation of phrases, which contains adjectives or adverbs, in a review to predict its sentiment orientation. Yu and Hatzivassiloglou (2003) calculated a modified log-likelihood ratio for every word by the co-occurrences with positive and negative seed words. To determine the polarity of a sentence, they compare the average log-likelihood value with threshold. Taboada et al. (2011) presented a lexicon-based approach for extracting sentiment from text. They used dictionaries of words with annotated sentiment orientation (polarity and strength) while incorporating intensification and negation. The lexicon-based methods often achieve high precisions and do not need 268 Dong et al. A Statistical Parsing Framework for Sentiment Classif"
J15-2004,D07-1071,0,0.0408997,"t, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the"
J15-2004,P09-1110,0,0.026688,"Missing"
J15-2004,N10-1119,0,\N,Missing
J15-2004,P12-2018,0,\N,Missing
J15-2004,P11-1060,0,\N,Missing
K19-1048,D15-1064,1,0.887973,"e hidden vector of the space after a word from the forward LSTM and the hidden vector of the space before a word from the backward LSTM to form a character-level repre− → ← − sentation of the word: hci = [hci ; hci ]. The wordlevel BiLSTM then takes the concatenation of hci and the word embedding as input xi = [ei ; hci ] to learn contextualized representations. Neural-CRFs. Conditional Random Fields (CRFs) (Lafferty et al., 2001) are sequence tagging models that capture the inter-dependencies between the output tags; they have been widely used for NER (McCallum and Li, 2003; Lu et al., 2015; Peng and Dredze, 2015, 2016, 2017). Given a set of training data {xi , yi }N , a CRF minimizes negative log-likelihood: min − X Θ log P (yi |xi ; Θ), (1) i P (yi |xi ; Θ) = Gold Energy St(yi ) =P 0 P artition y 0 St(y ) (2) where y 0 is any possible tag sequence with the same length as yi , St(y 0 ) is the potential of the tag sequence y 0 , and St(yi ) is the potential of the gold tag sequence. The numerator St(yi ) is called P the gold energy function, and the denominator y0 St(y 0 ) is the partition function. The likelihood function using globally annotated data is illustrated in Figure 2a. The potential of a t"
K19-1048,D14-1010,0,0.0317585,"ons (noisy labels), where some occurrences of entities are neglected in the annotation process and falsely labeled as non-entities (negative). A related problem is learning from unlabeled data with distant supervision. A major challenge of all these settings, including ours, is that a positive instance might be labeled as negative. A well-explored solution to this problem is proposed by Tsuboi et al. (2008), which instead of maximizing the likelihood of the gold tag sequence, we maximize the total likelihood for all possible tag sequences consistent with the gold labels. Tsuboi et al. (2008); Yang and Vozila (2014) applied this idea to the incomplete annotation setting; Shang et al. (2018); Liu et al. (2014) applied it to the unlabeled data with distant supervision setting; and Greenberg et al. (2018) applied it to the partial annotation setting. While this is a general solution, its primary drawback is that it assumes a uniform prior on all labels consistent with the gold labels. This may have the result of overly encouraging the prediction of entities, resulting in low precision. To tackle the problem of incomplete annotations, Carlson et al. (2009); Yang et al. (2018) explored bootstrap-based semi-su"
K19-1048,W17-2612,1,0.824835,"Missing"
K19-1048,C18-1183,0,0.0114825,"abels. Tsuboi et al. (2008); Yang and Vozila (2014) applied this idea to the incomplete annotation setting; Shang et al. (2018); Liu et al. (2014) applied it to the unlabeled data with distant supervision setting; and Greenberg et al. (2018) applied it to the partial annotation setting. While this is a general solution, its primary drawback is that it assumes a uniform prior on all labels consistent with the gold labels. This may have the result of overly encouraging the prediction of entities, resulting in low precision. To tackle the problem of incomplete annotations, Carlson et al. (2009); Yang et al. (2018) explored bootstrap-based semi-supervised learning on unlabeled data, iteratively identifying new entities with the taggers and then re-training the taggers. Bellare and McCallum (2007); Li and Liu (2005); Fernandes and Brefeld (2011) explored an EM algorithm with semi-supervision. For the partial annotation problem, most previous work has focused on building individual taggers for each dataset and using single-task learning (Liu et al., 2018) or multi-task learning (Crichton et al., 2017; Wang et al., 2018). In singletask learning, each model is trained separately on each dataset Ci , and mak"
K19-1048,D11-1141,0,0.0188705,"es Institute, University of Southern California 2 Department of Computer Science, University of Southern California 3 Outreach, Inc. {huan183, lidong}@usc.edu, {boschee, npeng}@isi.edu Abstract One problem in NER is the diversity of entity types, which vary in scope for different domains and downstream tasks. Traditional NER for the news domain focuses on three coarsegrained entity types: person, location, and organization (Tjong Kim Sang and De Meulder, 2003). However, as NLP technologies have been applied to a broader set of domains, many other entity types have been targeted. For instance, Ritter et al. (2011) add seven new entity types (e.g., product, tv-show) on top of the previous three when annotating tweets. Other efforts also define different but partially overlapping sets of entity types (Walker et al., 2006; Ji et al., 2010; Consortium, 2013; Aguilar et al., 2014). These non-unified annotation schemas result in partially annotated datasets: each dataset is only annotated with a subset of possible entity types. One approach to this problem is to train individual NE taggers for each partially annotated dataset and combine their results using some heuristics. Figure 1 shows an example that dem"
K19-1048,W03-0419,0,0.573762,"Missing"
K19-1048,D18-1230,0,0.0418022,"Missing"
K19-1048,J01-4004,0,0.457443,"t test time. Experiments show that the proposed model significantly outperforms strong multi-task learning baselines when training on multiple, partially annotated datasets and testing on datasets that contain tags from more than one of the training corpora.1 1 Introduction Named Entity Recognition (NER), which identifies the boundaries and types of entity mentions from raw text, is a fundamental problem in natural language processing (NLP). It is a basic component for many downstream tasks, such as relation extraction (Hasegawa et al., 2004; Mooney and Bunescu, 2005), coreference resolution (Soon et al., 2001), and knowledge base construction (Craven et al., 1998; Craven and Kumlien, 1999). ∗ 2 Work done while the author was at USC ISI. The code and the datasets will be made available at https://github.com/xhuang28/NewBioNer https://corposaurus.github.io/ corpora/ summarizes dozens of partially annotated biomedical datasets. 1 515 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 515–527 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics Figure 1: An example sentence from the CellFinder corpus (Neves et al., 2012) showing the ch"
K19-1048,C08-1113,0,0.333001,"ould be more generally thought of as learning from imperfect annotations. In that broad sense, there are several notable areas of prior work. One of the most prominent concerns learning from incomplete annotations (noisy labels), where some occurrences of entities are neglected in the annotation process and falsely labeled as non-entities (negative). A related problem is learning from unlabeled data with distant supervision. A major challenge of all these settings, including ours, is that a positive instance might be labeled as negative. A well-explored solution to this problem is proposed by Tsuboi et al. (2008), which instead of maximizing the likelihood of the gold tag sequence, we maximize the total likelihood for all possible tag sequences consistent with the gold labels. Tsuboi et al. (2008); Yang and Vozila (2014) applied this idea to the incomplete annotation setting; Shang et al. (2018); Liu et al. (2014) applied it to the unlabeled data with distant supervision setting; and Greenberg et al. (2018) applied it to the partial annotation setting. While this is a general solution, its primary drawback is that it assumes a uniform prior on all labels consistent with the gold labels. This may have"
K19-1048,W04-1213,0,\N,Missing
K19-1048,P04-1053,0,\N,Missing
K19-1048,D14-1093,0,\N,Missing
K19-1048,W03-0430,0,\N,Missing
K19-1048,W13-2002,0,\N,Missing
K19-1048,N16-1030,0,\N,Missing
K19-1048,D18-1306,0,\N,Missing
K19-1048,P16-1101,0,\N,Missing
K19-1048,W14-2907,0,\N,Missing
P14-2009,W02-1011,0,0.0546421,"Missing"
P14-2009,D12-1110,0,0.0247666,"Missing"
P14-2009,P11-2008,0,0.0271014,"Missing"
P14-2009,D13-1170,0,0.118265,"Missing"
P14-2009,J11-2001,0,0.042909,"respectively. If target information is ignored, it is difficult to obtain the correct sentiment for a specified target. For target-dependent sentiment classification, the manual evaluation of Jiang et al. (2011) ∗ Contribution during internship at Microsoft Research. 49 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49–54, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 3 determines how to propagate the sentiments towards the target and handles the negation or intensification phenomena (Taboada et al., 2011) in sentiment analysis. In addition, we introduce a manually annotated dataset, and conduct extensive experiments on it. The experimental results suggest that our approach yields better performances than the baseline methods. 2 We use the dependency parsing results to find the words syntactically connected with the interested target. Adaptive Recursive Neural Network is proposed to propagate the sentiments of words to the target node. We model the adaptive sentiment propagations as semantic compositions. The computation process is conducted in a bottom-up manner, and the vector representations"
P14-2009,P11-1016,1,0.747726,"is to classify their sentiments for a given target as positive, negative, and neutral. People may mention several entities (or targets) in one tweet, which affects the availabilities for most of existing methods. For example, the tweet “@ballmer: windows phone is better than ios!” has three targets (@ballmer, windows phone, and ios). The user expresses neutral, positive, and negative sentiments for them, respectively. If target information is ignored, it is difficult to obtain the correct sentiment for a specified target. For target-dependent sentiment classification, the manual evaluation of Jiang et al. (2011) ∗ Contribution during internship at Microsoft Research. 49 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49–54, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 3 determines how to propagate the sentiments towards the target and handles the negation or intensification phenomena (Taboada et al., 2011) in sentiment analysis. In addition, we introduce a manually annotated dataset, and conduct extensive experiments on it. The experimental results suggest that our approach yields better perfo"
P14-2009,P03-1054,0,0.0549664,"Missing"
P14-2009,N10-1120,0,0.0373418,"Missing"
P15-1026,P13-1042,0,0.20894,"itive interpretations for MCCNNs by developing a method to detect salient question words in the different column networks. 2 Related Work The state-of-the-art methods for question answering over a knowledge base can be classified into two classes, i.e., semantic parsing based and information retrieval based. Semantic parsing based approaches aim at learning semantic parsers which parse natural language questions into logical forms and then query knowledge base to lookup answers. The most important step is mapping questions into predefined logical forms, such as combinatory categorial grammar (Cai and Yates, 2013) and dependencybased compositional semantics (Liang et al., Another line of related work is applying deep learning techniques for the question answering task. Grefenstette et al. (2014) proposed a deep architecture to learn a semantic parser from annotated logic forms of questions. Iyyer et al. (2014) introduced dependency-tree recursive neural networks for the quiz bowl game which asked players to answer an entity for a given paragraph. Yu et 261 al. (2014) proposed a bigram model based on convolutional neural networks to select answer sentences from text data. The model learned a similarity"
P15-1026,D11-1142,0,0.0529251,"task. Grefenstette et al. (2014) proposed a deep architecture to learn a semantic parser from annotated logic forms of questions. Iyyer et al. (2014) introduced dependency-tree recursive neural networks for the quiz bowl game which asked players to answer an entity for a given paragraph. Yu et 261 al. (2014) proposed a bigram model based on convolutional neural networks to select answer sentences from text data. The model learned a similarity function between questions and answer sentences. Yih et al. (2014) used convolutional neural networks to answer single-relation questions on R E V ERB (Fader et al., 2011). However, the system worked on relation-entity triples instead of more structured knowledge bases. For instance, the question shown in Figure 1 is answered by using several triples in F REEBASE. Also, we can utilize richer information (such as entity types) in structured knowledge bases. 3 cess method does not ease the task because W E B Q UESTIONS only contains about 2k entities. WikiAnswers Fader et al. (2013) extracted the similar questions on W IKI A NSWERS and used them as question paraphrases. There are 350,000 paraphrase clusters which contain about two million questions. They are used"
P15-1026,P14-1091,1,0.92368,"crosoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dimensional vectors. Then, a score layer is employed to rank candi"
P15-1026,P13-1158,0,0.250666,"The model learned a similarity function between questions and answer sentences. Yih et al. (2014) used convolutional neural networks to answer single-relation questions on R E V ERB (Fader et al., 2011). However, the system worked on relation-entity triples instead of more structured knowledge bases. For instance, the question shown in Figure 1 is answered by using several triples in F REEBASE. Also, we can utilize richer information (such as entity types) in structured knowledge bases. 3 cess method does not ease the task because W E B Q UESTIONS only contains about 2k entities. WikiAnswers Fader et al. (2013) extracted the similar questions on W IKI A NSWERS and used them as question paraphrases. There are 350,000 paraphrase clusters which contain about two million questions. They are used to generalize for unseen words and question patterns. 4 Methods The overview of our framework is shown in Figure 1. For instance, for the question when did Avatar release in UK, the related nodes of the entity Avatar are queried from F REEBASE. These related nodes are regarded as candidate answers (Cq ). Then, for every candidate answer a, the model predicts a score S (q, a) to determine whether it is a correct"
P15-1026,P14-1133,0,0.653202,"during internship at Microsoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dimensional vectors. Then, a score layer is empl"
P15-1026,D13-1160,0,0.755738,"r, how ∗ Contribution during internship at Microsoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dimensional vectors. The"
P15-1026,D14-1070,0,0.0457346,"Missing"
P15-1026,D12-1069,0,0.0673897,"Missing"
P15-1026,D14-1067,0,0.859341,"Dong†∗ Furu Wei‡ Ming Zhou‡ Ke Xu† † SKLSDE Lab, Beihang University, Beijing, China ‡ Microsoft Research, Beijing, China donglixp@gmail.com {fuwei,mingzhou}@microsoft.com kexu@nlsde.buaa.edu.cn Abstract to understand questions and bridge the gap between natural languages and structured semantics of knowledge bases is still very challenging. Up to now, there are two mainstream methods for this task. The first one is based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014) and the other relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b). The semantic parsers learn to understand natural language questions by converting them into logical forms. Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals (Liang et al., 2011; Krishnamurthy and Mitchell, 2012) to reduce annotation costs. However, some of them still assume a fixed and pre-defined set of lexical triggers which limit their domains and scalability capability. I"
P15-1026,D10-1119,0,0.0063562,"rgescale knowledge bases, such as F REEBASE (Bollacker et al., 2008), provides a rich resource to answer open-domain questions. However, how ∗ Contribution during internship at Microsoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities"
P15-1026,D13-1161,0,0.0182072,"n-domain questions. However, how ∗ Contribution during internship at Microsoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dim"
P15-1026,C02-1150,0,0.255307,"2, we compute salience scores for several questions, and normalize them by the max values in different columns. We clearly see that these words play different roles in a question. The overall conclusion is that the wh- words (such as what, who and where) tend to be important for question understanding. Moreover, nouns dependent of the wh- words and verbs are important clues to obtain question representations. For instance, the figure demonstrates that the nouns type/country/leader and the verbs speak/located are salient in the columns of networks. These observations agree with previous works (Li and Roth, 2002). Some manually defined rules (Yao and Van Durme, 2014) used in the question answering task are also based on them. Salient Words Detection In order to analyze the model, we detect salient words in questions. The salience score of a question word depends on how much the word affects the computation of question representation. In other words, if a word plays more important role in the model, its salience score should be larger. We compute several salience scores for a same word to illustrate its importance in different columns of networks. For the i-th column, the salience score of word wj in t"
P15-1026,P11-1060,0,0.168473,"sk. The first one is based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014) and the other relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b). The semantic parsers learn to understand natural language questions by converting them into logical forms. Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals (Liang et al., 2011; Krishnamurthy and Mitchell, 2012) to reduce annotation costs. However, some of them still assume a fixed and pre-defined set of lexical triggers which limit their domains and scalability capability. In addition, they need to manually design features for semantic parsers. The second approach uses information extraction techniques for open question answering. These methods retrieve a set of candidate answers from the knowledge base, and the extract features for the question and these candidates to rank them. However, the method proposed by Yao and Van Durme (2014) relies on rules and dependenc"
P15-1026,W12-3016,0,0.00897462,"ll the answers can be found in F REEBASE. Freebase It is a large-scale knowledge base that consists of general facts (Bollacker et al., 2008). These facts are organized as subject-propertyobject triples. For example, the fact Avatar is directed by James Cameron is represented by (/m/0bth54, film.film.directed by, /m/03 gd) in RDF format. The preprocess method presented in (Bordes et al., 2014a) was used to make F REE BASE fit in memory. Specifically, we kept the triples where one of the entities appeared in the training/development set of W EB Q UESTIONS or C LUE W EB extractions provided in (Lin et al., 2012), and removed the entities appearing less than five times. Then, we obtained 18M triples that contained 2.9M entities and 7k relation types. As described in (Bordes et al., 2014a), this preproS (q, a) = f1 (q)T g1 (a) + f2 (q)T g2 (a) + f3 (q)T g3 (a) | {z } | {z } | {z } answer path answer context answer type (1) where fi (q) and gi (a) have the same dimension. As shown in Figure 1, the score layer computes scores and adds them together. 4.1 Candidate Generation The first step is to retrieve candidate answers from F REEBASE for a question. Questions should contain an identified entity that ca"
P15-1026,Q14-1030,0,0.218962,"260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dimensional vectors. Then, a score layer is employed to rank candidate answers accordin"
P15-1026,P10-1040,0,0.0222071,"q2 , q3 ) (8) P q1 ,q2 ∈P q3 ∈RP where RP contains kp questions which are randomly sampled from other clusters. The same optimization algorithm described in Section 4.4 is used to update parameters. 5 F1 31.4 39.9 37.5 33.0 39.2 29.7 40.8 Experiments In order to evaluate the model, we use the dataset W EB Q UESTIONS (Section 3) to conduct experiments. Settings The development set is used to select hyper-parameters in the experiments. The nonlinearity function f = tanh is employed. The dimension of word vectors is set to 25. They are initialized by the pre-trained word embeddings provided in (Turian et al., 2010). The window size of MCCNNs is 5. The dimension of the pooling layers and the dimension of answer embeddings are set to 64. The parameters are initialized by the techniques described in (Bengio, 2012). The max value used for max-norm regularization is 3. The initial learning rate used in AdaGrad is set to 0.01. A mini-batch consists of 10 question-answer pairs, and every question-answer pair has k negative samples that are randomly sampled from its candidate set. The margin values in Equation (4) and Equation (7) is set to m = 0.5 and mp = 0.1. 5.2 Model Analysis We also conduct ablation exper"
P15-1026,P14-1090,0,0.797611,"Missing"
P15-1026,P14-2105,0,0.262001,"ng et al., Another line of related work is applying deep learning techniques for the question answering task. Grefenstette et al. (2014) proposed a deep architecture to learn a semantic parser from annotated logic forms of questions. Iyyer et al. (2014) introduced dependency-tree recursive neural networks for the quiz bowl game which asked players to answer an entity for a given paragraph. Yu et 261 al. (2014) proposed a bigram model based on convolutional neural networks to select answer sentences from text data. The model learned a similarity function between questions and answer sentences. Yih et al. (2014) used convolutional neural networks to answer single-relation questions on R E V ERB (Fader et al., 2011). However, the system worked on relation-entity triples instead of more structured knowledge bases. For instance, the question shown in Figure 1 is answered by using several triples in F REEBASE. Also, we can utilize richer information (such as entity types) in structured knowledge bases. 3 cess method does not ease the task because W E B Q UESTIONS only contains about 2k entities. WikiAnswers Fader et al. (2013) extracted the similar questions on W IKI A NSWERS and used them as question pa"
P16-1004,S13-1045,0,0.0221039,"Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems rely on high-quality lexicons, manually-built templates, and features which are either domain- or representationspecific. We instead present a general method that can be easily adapted to different domains and meaning representations. We adopt the general encoder-decoder framework based on neural networks which has been recently repurposed for various NLP tasks such as syntactic parsing (Vinyals et al., 2015a), machine translation (Kalchbrenner and Blunsom, 2013"
P16-1004,W10-2903,0,0.087492,"based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems rely on high-quality lexicons, manually-built templates, and features which are either domain- or representationspecific. We instead present a general method that can be easily adapted to different domains and meaning representations. We adopt the general encoder-decoder framework based on neural networks which has been recently repurposed for various NLP tasks s"
P16-1004,D12-1069,0,0.167243,"Missing"
P16-1004,D10-1119,0,0.0853908,"s include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems rely on high-quality lexicon"
P16-1004,W05-0602,0,0.0378387,"their translations into knowledge base queries, whereas the second model generates the queries conditioned on the learned representations. However, they do not report empirical evaluation results. Our work synthesizes two strands of research, namely semantic parsing and the encoder-decoder architecture with neural networks. The problem of learning semantic parsers has received significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semant"
P16-1004,D11-1140,0,0.756642,"ng models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems rely on high-quality lexicons, manually-built templates"
P16-1004,D13-1161,0,0.383307,"Missing"
P16-1004,W14-2405,0,0.0217722,"Missing"
P16-1004,J13-2005,0,0.831853,"ls (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems rely on high-quality lexicons, manually-built templates, and features which are either domain- or representationspecific. We instead present a general method that can be easily adapted to different domains and meaning representations. We adopt the general encoder-decoder framework based on neural networks which has been recently repurposed for various NLP tasks such as syntactic pars"
P16-1004,D08-1082,0,0.847713,"nto knowledge base queries, whereas the second model generates the queries conditioned on the learned representations. However, they do not report empirical evaluation results. Our work synthesizes two strands of research, namely semantic parsing and the encoder-decoder architecture with neural networks. The problem of learning semantic parsers has received significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers withou"
P16-1004,P15-1002,0,0.49244,"adapt the general encoder-decoder paradigm to the semantic parsing task. Our model learns from natural language descriptions paired with meaning representations; it encodes sentences and decodes logical forms using recurrent neural networks with long short-term memory (LSTM) units. We present two model variants, the first one treats semantic parsing as a vanilla sequence transduction task, whereas our second model is equipped with a hierarchical tree decoder which explicitly captures the compositional structure of logical forms. We also introduce an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015b) allowing the model to learn soft alignments between natural language and logical forms and present an argument identification step to handle rare mentions of entities and numbers. Introduction Semantic parsing is the task of translating text to a formal meaning representation such as logical forms or structured queries. There has recently been a surge of interest in developing machine learning methods for semantic parsing (see the references in Section 2), due in part to the existence of corpora containing utterances annotated with formal meaning representations. Figure 1 shows an example o"
P16-1004,P15-1001,0,0.0241346,"eveloped with question-answering in mind. In the typical application setting, natural language questions are mapped into logical forms and executed on a knowledge base to obtain an answer. Due to the nature of the question-answering task, many natural language utterances contain entities or numbers that are often parsed as arguments in the logical form. Some of them are unavoidably rare or do not appear in the training set at all (this is especially true for small-scale datasets). Conventional sequence encoders simply replace rare words with a special unknown word symbol (Luong et al., 2015a; Jean et al., 2015), which would be detrimental for semantic parsing. We have developed a simple procedure for argument identification. Specifically, we identify entities and numbers in input questions and replace them with their type names and unique IDs. For instance, we pre-process the training example “jobs with a salary of 40000” and its logical form “job(ANS), salary greater than(ANS, 40000, year)” as “jobs with a salary of num0 ” G EO This is a standard semantic parsing benchmark which contains 880 queries to a database of U.S. geography. G EO has 880 instances split into a training set of 680 training ex"
P16-1004,D15-1166,0,0.278376,"adapt the general encoder-decoder paradigm to the semantic parsing task. Our model learns from natural language descriptions paired with meaning representations; it encodes sentences and decodes logical forms using recurrent neural networks with long short-term memory (LSTM) units. We present two model variants, the first one treats semantic parsing as a vanilla sequence transduction task, whereas our second model is equipped with a hierarchical tree decoder which explicitly captures the compositional structure of logical forms. We also introduce an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015b) allowing the model to learn soft alignments between natural language and logical forms and present an argument identification step to handle rare mentions of entities and numbers. Introduction Semantic parsing is the task of translating text to a formal meaning representation such as logical forms or structured queries. There has recently been a surge of interest in developing machine learning methods for semantic parsing (see the references in Section 2), due in part to the existence of corpora containing utterances annotated with formal meaning representations. Figure 1 shows an example o"
P16-1004,D13-1176,0,0.0782393,"red features and is easy to adapt across domains and meaning representations. LSTM what microsoft jobs do not require a bscs? Sequence Sequence/Tree Encoder Decoder answer(J,(compa ny(J,&apos;microsoft&apos;),j ob(J),not((req_de g(J,&apos;bscs&apos;))))) Logical Form Figure 1: Input utterances and their logical forms are encoded and decoded with neural networks. An attention layer is used to learn soft alignments. Encoder-decoder architectures based on recurrent neural networks have been successfully applied to a variety of NLP tasks ranging from syntactic parsing (Vinyals et al., 2015a), to machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), and image description generation (Karpathy and FeiFei, 2015; Vinyals et al., 2015b). As shown in Figure 1, we adapt the general encoder-decoder paradigm to the semantic parsing task. Our model learns from natural language descriptions paired with meaning representations; it encodes sentences and decodes logical forms using recurrent neural networks with long short-term memory (LSTM) units. We present two model variants, the first one treats semantic parsing as a vanilla sequence transduction task, whereas our second model is equipped with a hierarch"
P16-1004,P96-1008,0,0.418122,"airs of questions and their translations into knowledge base queries, whereas the second model generates the queries conditioned on the learned representations. However, they do not report empirical evaluation results. Our work synthesizes two strands of research, namely semantic parsing and the encoder-decoder architecture with neural networks. The problem of learning semantic parsers has received significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Oth"
P16-1004,P06-1115,0,0.805764,"neural networks. The problem of learning semantic parsers has received significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and d"
P16-1004,P13-1092,0,0.0469775,"Missing"
P16-1004,P07-1121,0,0.439228,"ved significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates"
P16-1004,P15-1085,0,0.623324,"training set of 680 training examples and 200 test examples (Zettlemoyer and Collins, 2005). We used the same meaning representation based on lambda-calculus as Kwiatkowski et al. (2011). Values for the variables city, state, country, river, and number are identified. ATIS This dataset has 5, 410 queries to a flight booking system. The standard split has 4, 480 training instances, 480 development instances, and 450 test instances. Sentences are paired with lambda-calculus expressions. Values for the variables date, time, city, aircraft code, airport, airline, and number are identified. I FTTT Quirk et al. (2015) created this dataset by extracting a large number of if-this-then-that 37 Dataset J OBS G EO ATIS I FTTT Length 9.80 22.90 7.60 19.10 11.10 28.10 6.95 21.80 Example what microsoft jobs do not require a bscs? answer(company(J,’microsoft’),job(J),not((req deg(J,’bscs’)))) what is the population of the state with the largest area? (population:i (argmax $0 (state:t $0) (area:i $0))) dallas to san francisco leaving after 4 in the afternoon please (lambda $0 e (and (>(departure time $0) 1600:ti) (from $0 dallas:ci) (to $0 san francisco:ci))) Turn on heater when temperature drops below 58 degree TRI"
P16-1004,Q14-1030,1,0.504553,"s et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems rely on high-quality lexicons, manually-built templates, and features which are either domain- or representationspecific. We instead present a general method that can be easily adapted to different domains and meaning representations. We adopt the general encoder-decoder framework based on neural networks which has been recently repurposed for various NLP tasks such as syntactic parsing (Vinyals et al., 2015a), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; S"
P16-1004,D15-1044,0,0.0421264,", and features which are either domain- or representationspecific. We instead present a general method that can be easily adapted to different domains and meaning representations. We adopt the general encoder-decoder framework based on neural networks which has been recently repurposed for various NLP tasks such as syntactic parsing (Vinyals et al., 2015a), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), image description generation (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015b), question answering (Hermann et al., 2015), and summarization (Rush et al., 2015). Mei et al. (2016) use a sequence-to-sequence model to map navigational instructions to actions. 3 Problem Formulation Our aim is to learn a model which maps natural language input q = x1 · · · x|q |to a logical form representation of its meaning a = y1 · · · y|a |. The conditional probability p (a|q) is decomposed as: p (a|q) = |a| Y p (yt |y<t , q) (1) t=1 where y<t = y1 · · · yt−1 . Our method consists of an encoder which encodes natural language input q into a vector representation and a decoder which learns to generate y1 , · · · , y|a |conditioned on the encoding vector. In the followin"
P16-1004,W00-1317,0,0.0596882,"learned representations. However, they do not report empirical evaluation results. Our work synthesizes two strands of research, namely semantic parsing and the encoder-decoder architecture with neural networks. The problem of learning semantic parsers has received significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi a"
P16-1004,D07-1071,0,0.935906,"us modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems re"
P16-1004,N15-1162,0,0.465285,"e queries, whereas the second model generates the queries conditioned on the learned representations. However, they do not report empirical evaluation results. Our work synthesizes two strands of research, namely semantic parsing and the encoder-decoder architecture with neural networks. The problem of learning semantic parsers has received significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfro"
P16-1004,P16-1008,0,0.025337,"curated descriptions are often of low quality, and thus align very loosely to their corresponding ASTs. 4.4 Error Analysis Finally, we inspected the output of our model in order to identify the most common causes of errors which we summarize below. Under-Mapping The attention model used in our experiments does not take the alignment history into consideration. So, some question words, expecially in longer questions, may be ignored in the decoding process. This is a common problem for encoder-decoder models and can be addressed by explicitly modelling the decoding coverage of the source words (Tu et al., 2016; Cohn et al., 2016). Keeping track of the attention history would help adjust future attention and guide the decoder towards untranslated source words. Acknowledgments We would like to thank Luke Zettlemoyer and Tom Kwiatkowski for sharing the ATIS dataset. The support of the European Research Council under award number 681760 “Translating Multiple Modalities into Text” is gratefully acknowledged. References Jacob Andreas, Andreas Vlachos, and Stephen Clark. 2013. Semantic parsing as machine translation. In Proceedings of the 51st ACL, pages 47–52, Sofia, Bulgaria. Argument Identification Som"
P16-1004,N06-1056,0,0.694417,"Missing"
P16-1004,D11-1039,0,\N,Missing
P16-1004,P13-2009,0,\N,Missing
P16-1004,P11-1060,0,\N,Missing
P16-1004,Q13-1005,0,\N,Missing
P18-1068,P13-2009,0,0.0588517,"ed in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for mul"
P18-1068,Q13-1005,0,0.123826,"syntax trees (Aho et al., 2007) in depth-ﬁrst, left-to-right order. Rabinovich et al. (2017) propose a modular decoder whose submodels are dynamically composed according to the generated tree structure. Our own work also aims to model the structure of meaning representations more faithfully. The ﬂexibility of our approach enables us to easily apply sketches to different types of meaning representations, e.g., trees or other structured objects. Coarse-to-ﬁne methods have been popular in the NLP literature, and are perhaps best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Artzi and Zettlemoyer (2013) and Zhang et al. (2017) use coarse lexical entries or macro grammars to reduce the search space of semantic parsers. Compared with coarse-to-ﬁne inference for lexical induction, sketches in our case are abstractions of the ﬁnal meaning representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively rep"
P18-1068,D13-1160,0,0.176179,"LP literature, and are perhaps best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Artzi and Zettlemoyer (2013) and Zhang et al. (2017) use coarse lexical entries or macro grammars to reduce the search space of semantic parsers. Compared with coarse-to-ﬁne inference for lexical induction, sketches in our case are abstractions of the ﬁnal meaning representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation"
P18-1068,P17-1089,0,0.212727,"entation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for multiple languages or meaning representations (Susanto and Lu, 2017; Herzig and Berant, 2017), and utilizing user feedback signals (Iyer et al., 2017). There are also efforts to develop structured decoders that make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) develop models which generate tree structures in a topdown fashion. Xiao et al. (2016) and Krishnamurthy et al. (2017) employ the grammar to constrain the decoding process. Cheng et al. (2017) 3 Problem Formulation Our goal is to learn semantic parsers from instances of natural language expressions paired with their structured meaning representations. 732 (lambda $0 e (and $0 e (flight $0 ) (< $0 ) Sketch-Guided Output Deco"
P18-1068,N06-1022,0,0.0590303,"r model for the generation of abstract syntax trees (Aho et al., 2007) in depth-ﬁrst, left-to-right order. Rabinovich et al. (2017) propose a modular decoder whose submodels are dynamically composed according to the generated tree structure. Our own work also aims to model the structure of meaning representations more faithfully. The ﬂexibility of our approach enables us to easily apply sketches to different types of meaning representations, e.g., trees or other structured objects. Coarse-to-ﬁne methods have been popular in the NLP literature, and are perhaps best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Artzi and Zettlemoyer (2013) and Zhang et al. (2017) use coarse lexical entries or macro grammars to reduce the search space of semantic parsers. Compared with coarse-to-ﬁne inference for lexical induction, sketches in our case are abstractions of the ﬁnal meaning representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using pro"
P18-1068,P16-1002,0,0.786326,"sults on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders. 1 Introduction Semantic parsing maps natural language utterances onto machine interpretable meaning representations (e.g., executable queries or logical forms). The successful application of recurrent neural networks to a variety of NLP tasks (Bahdanau et al., 2015; Vinyals et al., 2015) has provided strong impetus to treat semantic parsing as a sequence-to-sequence problem (Jia and Liang, 2016; Dong and Lapata, 2016; Ling et al., 2016). The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure. Examples include tree decoders (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017), decoders constrained by a grammar model (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017), or modular 731 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 731–742 c Melbourne, Australia, July 15 - 20, 2018. 2018 Ass"
P18-1068,P17-1005,1,0.870424,"cisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for multiple languages or meaning representations (Susanto and Lu, 2017; Herzig and Berant, 2017), and utilizing user feedback signals (Iyer et al., 2017). There are also efforts to develop structured decoders that make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) develop models which generate tree structures in a topdown fashion. Xiao et al. (2016) and Krishnamurthy et al. (2017) employ the grammar to constrain the decoding process. Cheng et al. (2017) 3 Problem Formulation Our goal is to learn semantic parsers from instances of natural language expressions paired with their structured meaning representations. 732 (lambda $0 e (and $0 e (flight $0 ) (< $0 ) Sketch-Guided Output Decoding <s> (departure _time $0 ) ti0 ) $0 ) ti0 ) </s> ) ( |, ) Sketch Encoding (lambda#2 (and flight@1 (< departure _time@1 ? ) ) ) </s> ? ) ) ) Sketch Decoding <s> (lambda#2 (and flight@1 (< departure _time@1 ( |) Input Encoding Encoder units Decoder units all flights before ti0 Figure 1: We ﬁrst generate the meaning sketch a for natural language input x. Then, a"
P18-1068,D16-1116,0,0.129875,"Missing"
P18-1068,P16-1004,1,0.90972,"s characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders. 1 Introduction Semantic parsing maps natural language utterances onto machine interpretable meaning representations (e.g., executable queries or logical forms). The successful application of recurrent neural networks to a variety of NLP tasks (Bahdanau et al., 2015; Vinyals et al., 2015) has provided strong impetus to treat semantic parsing as a sequence-to-sequence problem (Jia and Liang, 2016; Dong and Lapata, 2016; Ling et al., 2016). The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure. Examples include tree decoders (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017), decoders constrained by a grammar model (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017), or modular 731 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 731–742 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computatio"
P18-1068,D17-1160,0,0.500369,"ion of recurrent neural networks to a variety of NLP tasks (Bahdanau et al., 2015; Vinyals et al., 2015) has provided strong impetus to treat semantic parsing as a sequence-to-sequence problem (Jia and Liang, 2016; Dong and Lapata, 2016; Ling et al., 2016). The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure. Examples include tree decoders (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017), decoders constrained by a grammar model (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017), or modular 731 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 731–742 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Dataset Length Example G EO 7.6 13.7 6.9 x : which state has the most rivers running through it? y : (argmax $0 (state:t $0) (count $1 (and (river:t $1) (loc:t $1 $0)))) a : (argmax#1 state:t@1 (count#1 (and river:t@1 loc:t@2 ) ) ) ATIS 11.1 21.1 9.2 x : all ﬂights from dallas before 10am y : (lambda $0 e (and (ﬂight $0) (from $0 dallas:ci) (< (departure time $0) 1000:ti))"
P18-1068,W17-2607,0,0.112404,"wiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for multiple languages or meaning representations (Susanto and Lu, 2017; Herzig and Berant, 2017), and utilizing user feedback signals (Iyer et al., 2017). There are also efforts to develop structured decoders that make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) develop models which generate tree structures in a topdown fashion. Xiao et al. (2016) and Krishnamurthy et al. (2017) employ the grammar to constrain the decoding process. Cheng et al. (2017) 3 Problem Formulation Our goal is to learn semantic parsers"
P18-1068,D10-1119,0,0.209378,"Missing"
P18-1068,W05-0602,0,0.0862539,"ns of the ﬁnal meaning representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augme"
P18-1068,D13-1161,0,0.089859,"Missing"
P18-1068,P16-1154,0,0.0584332,"he original tokens with their token types, except delimiters (e.g., “[”, and “:”), operators (e.g., “+”, and “*”), and built-in keywords (e.g., “True”, and “while”). For instance, the expression “if s[:4].lower() == ’http’:” becomes “if NAME [ : NUMBER ] . NAME ( ) == STRING :”, with details about names, values, and strings being omitted. D JANGO is a diverse dataset, spanning various real-world use cases and as a result models are often faced with out-of-vocabulary (OOV) tokens (e.g., variable names, and numbers) that are unseen during training. We handle OOV tokens with a copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; Jia and Liang, 2016), which allows the ﬁne meaning decoder (Section 3.2) to directly copy tokens from the natural language input. The ﬁrst element between a pair of brackets is an operator or predicate name, and any remaining elements are its arguments. Algorithm 1 shows the pseudocode used to extract sketches from λ-calculus-based meaning representations. We strip off arguments and variable names in logical forms, while keeping predicates, operators, and composition information. We use the symbol “@” to denote the number of missing arguments in a predicate. For exampl"
P18-1068,D11-1140,0,0.832537,"tions has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), shar"
P18-1068,P16-1014,0,0.0317806,"s with their token types, except delimiters (e.g., “[”, and “:”), operators (e.g., “+”, and “*”), and built-in keywords (e.g., “True”, and “while”). For instance, the expression “if s[:4].lower() == ’http’:” becomes “if NAME [ : NUMBER ] . NAME ( ) == STRING :”, with details about names, values, and strings being omitted. D JANGO is a diverse dataset, spanning various real-world use cases and as a result models are often faced with out-of-vocabulary (OOV) tokens (e.g., variable names, and numbers) that are unseen during training. We handle OOV tokens with a copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; Jia and Liang, 2016), which allows the ﬁne meaning decoder (Section 3.2) to directly copy tokens from the natural language input. The ﬁrst element between a pair of brackets is an operator or predicate name, and any remaining elements are its arguments. Algorithm 1 shows the pseudocode used to extract sketches from λ-calculus-based meaning representations. We strip off arguments and variable names in logical forms, while keeping predicates, operators, and composition information. We use the symbol “@” to denote the number of missing arguments in a predicate. For example, we extract “from@2”"
P18-1068,J13-2005,0,0.161781,"Missing"
P18-1068,P16-1057,0,0.379224,"Missing"
P18-1068,P17-2098,0,0.14554,"mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for multiple languages or meaning representations (Susanto and Lu, 2017; Herzig and Berant, 2017), and utilizing user feedback signals (Iyer et al., 2017). There are also efforts to develop structured decoders that make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) develop models which generate tree structures in a topdown fashion. Xiao et al. (2016) and Krishnamurthy et al. (2017) employ the grammar to constrain the decoding process. Cheng et al. (2017) 3 Problem Formulation Our goal is to learn semantic parsers from instances of natural language expressions paired with their structured meaning representations. 732 (lambda $0 e"
P18-1068,D08-1082,0,0.105245,"ediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learnin"
P18-1068,W00-1317,0,0.552254,"our case are abstractions of the ﬁnal meaning representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these mode"
P18-1068,D15-1166,0,0.0942872,"ion probabilities are factorized as: p (a|x) = |a|  p (at |a<t , x) (2) p (yt |y<t , x, a) (3) t p (y|x, a) = LSTM − − et = [→ e t, ← e t] t+1 t (6) where [·, ·] denotes vector concatenation, et ∈ Rn , and fLSTM is the LSTM function. Coarse Meaning Decoder The decoder’s hidden vector at the t-th time step is computed by dt = fLSTM (dt−1 , at−1 ), where at−1 ∈ Rn is the embedding of the previously predicted token. The hidden states of the ﬁrst time step in the decoder are initialized by the concatenated encoding − − e |x |, ← e 1 ]. Additionally, we use an vectors d0 = [→ attention mechanism (Luong et al., 2015) to learn soft alignments. We compute the attention score for the current time step t of the decoder, with the k-th hidden state in the encoder as: t=1 |y|  Sketch Generation t=1 where a<t = a1 · · · at−1 , and y<t = y1 · · · yt−1 . In the following, we will explain how p (a|x) and p (y|x, a) are estimated. st,k = exp{dt · ek }/Zt 733 (7) |x| where Zt = j=1 exp{dt · ej } is a normalization term. Then we compute p (at |a<t , x) via: edt = |x|  st,k ek output will indicate whether missing details have been generated (e.g., if the decoder emits a closing quote token for “STRING”). Moreover, ty"
P18-1068,P14-5010,0,0.00422056,"16), where natural language expressions are lowercased and stemmed with NLTK (Bird et al., 2009), and entity mentions are replaced by numbered markers. We combined predicates and left brackets that indicate hierarchical structures to make meaning representations compact. We employed the preprocessed D JANGO data provided by Yin and Neubig (2017), where input expressions are tokenized by NLTK, and quoted strings in the input are replaced with place holders. W IK I SQL was preprocessed by the script provided by Zhong et al. (2017), where inputs were lowercased and tokenized by Stanford CoreNLP (Manning et al., 2014). Conﬁguration Model hyperparameters were cross-validated on the training set for G EO, and were validated on the development split for the other datasets. Dimensions of hidden vectors and word embeddings were selected from {250, 300} and {150, 200, 250, 300}, respectively. The dropout rate was selected from {0.3, 0.5}. Label smoothing (Szegedy et al., 2016) was employed for G EO and ATIS. The smoothing parameter was set to 0.1. For W IKI SQL, the hidden size of σ(·) 737 Method G EO ATIS Method Accuracy ZC07 (Zettlemoyer and Collins, 2007) UBL (Kwiatkowksi et al., 2010) FUBL (Kwiatkowski et al"
P18-1068,P07-1121,0,0.347102,"sing sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016),"
P18-1068,D14-1162,0,0.0850496,"Missing"
P18-1068,P16-1127,0,0.4597,"r logical forms). The successful application of recurrent neural networks to a variety of NLP tasks (Bahdanau et al., 2015; Vinyals et al., 2015) has provided strong impetus to treat semantic parsing as a sequence-to-sequence problem (Jia and Liang, 2016; Dong and Lapata, 2016; Ling et al., 2016). The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure. Examples include tree decoders (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017), decoders constrained by a grammar model (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017), or modular 731 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 731–742 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Dataset Length Example G EO 7.6 13.7 6.9 x : which state has the most rivers running through it? y : (argmax $0 (state:t $0) (count $1 (and (river:t $1) (loc:t $1 $0)))) a : (argmax#1 state:t@1 (count#1 (and river:t@1 loc:t@2 ) ) ) ATIS 11.1 21.1 9.2 x : all ﬂights from dallas before 10am y : (lambda $0 e (and (ﬂight $0) ("
P18-1068,P13-1092,0,0.0762511,"Missing"
P18-1068,P17-1105,0,0.671258,"ikhail Snitko record for after 1996? y : SELECT Record Company WHERE (Year of Recording > 1996) AND (Conductor = Mikhail Snitko) a : WHERE > AND = Table 1: Examples of natural language expressions x, their meaning representations y, and meaning sketches a. The average number of tokens is shown in the second column. with previous systems, despite employing relatively simple sequence decoders. 2 use a transition system to generate variable-free queries. Yin and Neubig (2017) design a grammar model for the generation of abstract syntax trees (Aho et al., 2007) in depth-ﬁrst, left-to-right order. Rabinovich et al. (2017) propose a modular decoder whose submodels are dynamically composed according to the generated tree structure. Our own work also aims to model the structure of meaning representations more faithfully. The ﬂexibility of our approach enables us to easily apply sketches to different types of meaning representations, e.g., trees or other structured objects. Coarse-to-ﬁne methods have been popular in the NLP literature, and are perhaps best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Artzi and Zettlemoyer (2013) and Zhang et al. (2017) use coarse lexical entries or macro gram"
P18-1068,P17-1041,0,0.367847,"he successful application of recurrent neural networks to a variety of NLP tasks (Bahdanau et al., 2015; Vinyals et al., 2015) has provided strong impetus to treat semantic parsing as a sequence-to-sequence problem (Jia and Liang, 2016; Dong and Lapata, 2016; Ling et al., 2016). The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure. Examples include tree decoders (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017), decoders constrained by a grammar model (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017), or modular 731 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 731–742 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Dataset Length Example G EO 7.6 13.7 6.9 x : which state has the most rivers running through it? y : (argmax $0 (state:t $0) (count $1 (and (river:t $1) (loc:t $1 $0)))) a : (argmax#1 state:t@1 (count#1 (and river:t@1 loc:t@2 ) ) ) ATIS 11.1 21.1 9.2 x : all ﬂights from dallas before 10am y : (lambda $0 e (and (ﬂight $0) (from $0 dallas:ci) (<"
P18-1068,P17-2007,0,0.0503599,"ally learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for multiple languages or meaning representations (Susanto and Lu, 2017; Herzig and Berant, 2017), and utilizing user feedback signals (Iyer et al., 2017). There are also efforts to develop structured decoders that make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) develop models which generate tree structures in a topdown fashion. Xiao et al. (2016) and Krishnamurthy et al. (2017) employ the grammar to constrain the decoding process. Cheng et al. (2017) 3 Problem Formulation Our goal is to learn semantic parsers from instances of natural language expressions paired with their structured meaning represe"
P18-1068,D07-1071,0,0.804567,"g representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016"
P18-1068,D17-1125,0,0.0300138,"in depth-ﬁrst, left-to-right order. Rabinovich et al. (2017) propose a modular decoder whose submodels are dynamically composed according to the generated tree structure. Our own work also aims to model the structure of meaning representations more faithfully. The ﬂexibility of our approach enables us to easily apply sketches to different types of meaning representations, e.g., trees or other structured objects. Coarse-to-ﬁne methods have been popular in the NLP literature, and are perhaps best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Artzi and Zettlemoyer (2013) and Zhang et al. (2017) use coarse lexical entries or macro grammars to reduce the search space of semantic parsers. Compared with coarse-to-ﬁne inference for lexical induction, sketches in our case are abstractions of the ﬁnal meaning representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty"
P18-1068,N15-1162,0,0.512991,"ram synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for multiple languages or mean"
P18-1068,P11-1060,0,\N,Missing
P18-1069,C04-1046,0,0.0758888,"santo and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To the best of our knowledge, conﬁdence modeling for semantic parsing remains largely unexplored. A common scheme for modeling uncertainty in neural networks is to place distributions over the network’s weights (Denker and Lecun, 1991; MacKay, 1992; Neal, 1996; Blundell et al., 2015; Gan et al., 2017). But the resulting models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difﬁcult to work with. Gal and Ghahramani (2016) develop"
P18-1069,P17-2098,0,0.026462,"have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; So"
P18-1069,P17-1089,0,0.0291545,"(Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and ques"
P18-1069,P16-1002,0,0.32272,"dence model signiﬁcantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores. 1 Introduction Semantic parsing aims to map natural language text to a formal meaning representation (e.g., logical forms or SQL queries). The neural sequenceto-sequence architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has been widely adopted in a variety of natural language processing tasks, and semantic parsing is no exception. However, despite achieving promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), neural semantic parsers remain difﬁcult to interpret, acting in most cases as a black box, not providing any information about what made them arrive at a particular decision. In this work, we explore ways to estimate and interpret the ∗ Work carried out during an internship at Microsoft Research. 743 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 743–753 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics We compute these conﬁdence metrics for a given prediction and use th"
P18-1069,P16-1004,1,0.939878,"ults show that our conﬁdence model signiﬁcantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores. 1 Introduction Semantic parsing aims to map natural language text to a formal meaning representation (e.g., logical forms or SQL queries). The neural sequenceto-sequence architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has been widely adopted in a variety of natural language processing tasks, and semantic parsing is no exception. However, despite achieving promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), neural semantic parsers remain difﬁcult to interpret, acting in most cases as a black box, not providing any information about what made them arrive at a particular decision. In this work, we explore ways to estimate and interpret the ∗ Work carried out during an internship at Microsoft Research. 743 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 743–753 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics We compute these conﬁdence metrics for a given"
P18-1069,W17-2631,0,0.0143956,"do indeed produce uncertain outputs, which we would like our framework to identify. A widely-used conﬁdence scoring method is based on posterior probabilities p (y|x) where x is the input and y the model’s prediction. For a linear model, this method makes sense: as more positive evidence is gathered, the score becomes larger. Neural models, in contrast, learn a complicated function that often overﬁts the training data. Posterior probability is effective when making decisions about model output, but is no longer a good indicator of conﬁdence due in part to the nonlinearity of neural networks (Johansen and Socher, 2017). This observation motivates us to develop a conﬁdence modeling framework for sequenceto-sequence models. We categorize the causes of uncertainty into three types, namely model uncertainty, data uncertainty, and input uncertainty and design different metrics to characterize them. In this work we focus on conﬁdence modeling for neural semantic parsers which are built upon sequence-to-sequence models. We outline three major causes of uncertainty, and design various metrics to quantify these factors. These metrics are then used to estimate conﬁdence scores that indicate whether model predictions"
P18-1069,P18-1068,1,0.721265,"Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical"
P18-1069,D16-1116,0,0.0361875,"Missing"
P18-1069,W17-2607,0,0.0279905,"; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To t"
P18-1069,D17-1160,0,0.0356097,"ores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the c"
P18-1069,D11-1140,0,0.172747,"Missing"
P18-1069,P17-1030,0,0.0311205,"an estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To the best of our knowledge, conﬁdence modeling for semantic parsing remains largely unexplored. A common scheme for modeling uncertainty in neural networks is to place distributions over the network’s weights (Denker and Lecun, 1991; MacKay, 1992; Neal, 1996; Blundell et al., 2015; Gan et al., 2017). But the resulting models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difﬁcult to work with. Gal and Ghahramani (2016) develop a theoretical framework which shows that the use of dropout in neural networks can be interpreted as a Bayesian approximation of Gaussian Process. We adapt their framework so as to represent uncertainty in the encoder-decoder architectures, and extend it by adding Gaussian noise to weights. 3 Neural Semantic Parsing Model In the following section we describe the neural semantic parsing model (Dong"
P18-1069,P16-1057,0,0.0299597,"Missing"
P18-1069,P13-2121,0,0.0350755,"Missing"
P18-1069,D08-1082,0,0.0305941,"Hochreiter and Schmidhuber 1997) which process tokens sequentially. The probability of generating the whole sequence p (a|q) is factorized as: p (a|q) = |a|  p (at |a&lt;t , q) (1) t=1 where a&lt;t = a1 · · · at−1 . Let et ∈ Rn denote the hidden vector of the encoder at time step t. It is computed via et = fLSTM (et−1 , qt ), where fLSTM refers to the LSTM unit, and qt ∈ Rn is the word embedding Semantic Parsing Various methods have been developed to learn a semantic parser from natural language descriptions paired with meaning representations (Tang and Mooney, 2000; Zettlemoyer and Collins, 2007; Lu et al., 2008; Kwiatkowski 744 ܽଵ … … ܽȁȁ Algorithm 1 Dropout Perturbation iv) ଵ i) Input: q, a: Input and its prediction M: Model parameters 1: for i ← 1, · · · , F do ˆ i ← Apply dropout layers to M  Figure 1 2: M ˆ i) 3: Run forward pass and compute pˆ(a|q; M  ii) ݍଵ ଶ … ȁȁ ݍଶ … ݍȁȁ iii) i) ଵ ଶ … &lt;s&gt; ܽଵ … ȁȁ ܽ  ିଵ ˆ i )}F 4: Compute variance of {ˆ p(a|q; M i=1  Equation (6) Figure 1: We use dropout as approximate Bayesian inference to obtain model uncertainty. The dropout layers are applied to i) token vectors; ii) the encoder’s output vectors; iii) bridge vectors; and iv) de"
P18-1069,D15-1166,0,0.0607755,"o estimate “what we do not know”. To this end, we identify three causes of uncertainty, and design various metrics characterizing each one of them. We then feed these metrics into a regression model in order to predict s (q, a). of qt . Once the tokens of the input sequence are encoded into vectors, e|q |is used to initialize the hidden states of the ﬁrst time step in the decoder. Similarly, the hidden vector of the decoder at time step t is computed by dt = fLSTM (dt−1 , at−1 ), where at−1 ∈ Rn is the word vector of the previously predicted token. Additionally, we use an attention mechanism (Luong et al., 2015a) to utilize relevant encoder-side context. For the current time step t of the decoder, we compute its attention score with the k-th hidden state in the encoder as: rt,k ∝ exp{dt · ek } 4.1 The model’s parameters or structures contain uncertainty, which makes the model less conﬁdent about the values of p (a|q). For example, noise in the training data and the stochastic learning algorithm itself can result in model uncertainty. We describe metrics for capturing uncertainty below: Dropout Perturbation Our ﬁrst metric uses dropout (Srivastava et al., 2014) as approximate Bayesian inference to es"
P18-1069,P17-2007,0,0.0191705,"nce-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004"
P18-1069,P15-1002,0,0.0402218,"o estimate “what we do not know”. To this end, we identify three causes of uncertainty, and design various metrics characterizing each one of them. We then feed these metrics into a regression model in order to predict s (q, a). of qt . Once the tokens of the input sequence are encoded into vectors, e|q |is used to initialize the hidden states of the ﬁrst time step in the decoder. Similarly, the hidden vector of the decoder at time step t is computed by dt = fLSTM (dt−1 , at−1 ), where at−1 ∈ Rn is the word vector of the previously predicted token. Additionally, we use an attention mechanism (Luong et al., 2015a) to utilize relevant encoder-side context. For the current time step t of the decoder, we compute its attention score with the k-th hidden state in the encoder as: rt,k ∝ exp{dt · ek } 4.1 The model’s parameters or structures contain uncertainty, which makes the model less conﬁdent about the values of p (a|q). For example, noise in the training data and the stochastic learning algorithm itself can result in model uncertainty. We describe metrics for capturing uncertainty below: Dropout Perturbation Our ﬁrst metric uses dropout (Srivastava et al., 2014) as approximate Bayesian inference to es"
P18-1069,W00-1317,0,0.0545413,"al networks with long short-term memory units (LSTMs; Hochreiter and Schmidhuber 1997) which process tokens sequentially. The probability of generating the whole sequence p (a|q) is factorized as: p (a|q) = |a|  p (at |a&lt;t , q) (1) t=1 where a&lt;t = a1 · · · at−1 . Let et ∈ Rn denote the hidden vector of the encoder at time step t. It is computed via et = fLSTM (et−1 , qt ), where fLSTM refers to the LSTM unit, and qt ∈ Rn is the word embedding Semantic Parsing Various methods have been developed to learn a semantic parser from natural language descriptions paired with meaning representations (Tang and Mooney, 2000; Zettlemoyer and Collins, 2007; Lu et al., 2008; Kwiatkowski 744 ܽଵ … … ܽȁȁ Algorithm 1 Dropout Perturbation iv) ଵ i) Input: q, a: Input and its prediction M: Model parameters 1: for i ← 1, · · · , F do ˆ i ← Apply dropout layers to M  Figure 1 2: M ˆ i) 3: Run forward pass and compute pˆ(a|q; M  ii) ݍଵ ଶ … ȁȁ ݍଶ … ݍȁȁ iii) i) ଵ ଶ … &lt;s&gt; ܽଵ … ȁȁ ܽ  ିଵ ˆ i )}F 4: Compute variance of {ˆ p(a|q; M i=1  Equation (6) Figure 1: We use dropout as approximate Bayesian inference to obtain model uncertainty. The dropout layers are applied to i) token vectors; ii) the encoder’s"
P18-1069,H05-1096,0,0.10971,"Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To the best of our knowledge, conﬁdence modeling for semantic parsing remains largely unexplored. A common scheme for modeling uncertainty in neural networks is to place distributions over the network’s weights (Denker and Lecun, 1991; MacKay, 1992; Neal, 1996; Blundell et al., 2015; Gan et al., 2017). But the resulting models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difﬁcult to work with. Gal and Ghahramani (2016) develop a theoretical framewo"
P18-1069,P16-1127,0,0.0949186,"atively more interpretable compared to those based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Rela"
P18-1069,P15-1085,1,0.857422,"trics for a given prediction and use them as features in a regression model which is trained on held-out data to ﬁt prediction F1 scores. At test time, the regression model’s outputs are used as conﬁdence scores. Our approach does not interfere with the training of the model, and can be thus applied to various architectures, without sacriﬁcing test accuracy. Furthermore, we propose a method based on backpropagation which allows to interpret model behavior by identifying which parts of the input contribute to uncertain predictions. Experimental results on two semantic parsing datasets (I FTTT, Quirk et al. 2015; and D JANGO, Oda et al. 2015) show that our model is superior to a method based on posterior probability. We also demonstrate that thresholding conﬁdence scores achieves a good trade-off between coverage and accuracy. Moreover, the proposed uncertainty backpropagation method yields results which are qualitatively more interpretable compared to those based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and s"
P18-1069,P17-1041,0,0.135118,"based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimat"
P18-1069,P17-1105,0,0.0122733,"retable compared to those based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estima"
P18-1069,D07-1071,0,0.115715,"hort-term memory units (LSTMs; Hochreiter and Schmidhuber 1997) which process tokens sequentially. The probability of generating the whole sequence p (a|q) is factorized as: p (a|q) = |a|  p (at |a&lt;t , q) (1) t=1 where a&lt;t = a1 · · · at−1 . Let et ∈ Rn denote the hidden vector of the encoder at time step t. It is computed via et = fLSTM (et−1 , qt ), where fLSTM refers to the LSTM unit, and qt ∈ Rn is the word embedding Semantic Parsing Various methods have been developed to learn a semantic parser from natural language descriptions paired with meaning representations (Tang and Mooney, 2000; Zettlemoyer and Collins, 2007; Lu et al., 2008; Kwiatkowski 744 ܽଵ … … ܽȁȁ Algorithm 1 Dropout Perturbation iv) ଵ i) Input: q, a: Input and its prediction M: Model parameters 1: for i ← 1, · · · , F do ˆ i ← Apply dropout layers to M  Figure 1 2: M ˆ i) 3: Run forward pass and compute pˆ(a|q; M  ii) ݍଵ ଶ … ȁȁ ݍଶ … ݍȁȁ iii) i) ଵ ଶ … &lt;s&gt; ܽଵ … ȁȁ ܽ  ିଵ ˆ i )}F 4: Compute variance of {ˆ p(a|q; M i=1  Equation (6) Figure 1: We use dropout as approximate Bayesian inference to obtain model uncertainty. The dropout layers are applied to i) token vectors; ii) the encoder’s output vectors; iii) bridge ve"
P18-1069,D17-1298,0,0.0270318,"ground truth; we compute the overlap between tokens identiﬁed as contributing to uncertainty by our method and those found in the gold standard. Overlap is shown for top 2 and 4 tokens. Best results are in bold. 7 Conclusions In this paper we presented a conﬁdence estimation model and an uncertainty interpretation method for neural semantic parsing. Experimental results show that our method achieves better performance than competitive baselines on two datasets. Directions for future work are many and varied. The proposed framework could be applied to a variety of tasks (Bahdanau et al., 2015; Schmaltz et al., 2017) employing sequence-to-sequence architectures. We could also utilize the conﬁdence estimation model within an active learning framework for neural semantic parsing. google calendar−any event starts THEN facebook −create a status message−(status message ({description})) ATT post calendar event to facebook BP post calendar event to facebook feed−new feed item−(feed url( url sports.espn.go.com)) THEN ... ATT espn mlb headline to readability BP espn mlb headline to readability weather−tomorrow’s low drops below−(( temperature(0)) (degrees in(c))) THEN ... ATT warn me when it’s going to be freezing"
P18-1069,P10-1063,0,0.0574285,"7), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To the best of our knowledge, conﬁdence modeling for semantic parsing remains largely unexplored. A common scheme for modeling uncertainty in neural networks is to place distributions over the network’s weights (Denker and Lecun, 1991; MacKay, 1992; Neal, 1996; Blundell et al., 2015; Gan et al., 2017). But the resulting models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difﬁcult to work with. Gal and Ghahramani (2016) develop a theoretical framework which shows that the use o"
P18-1069,N15-1162,0,0.0310473,"nterpret model behavior by identifying which parts of the input contribute to uncertain predictions. Experimental results on two semantic parsing datasets (I FTTT, Quirk et al. 2015; and D JANGO, Oda et al. 2015) show that our model is superior to a method based on posterior probability. We also demonstrate that thresholding conﬁdence scores achieves a good trade-off between coverage and accuracy. Moreover, the proposed uncertainty backpropagation method yields results which are qualitatively more interpretable compared to those based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapa"
P18-1069,P13-2009,0,\N,Missing
P19-1195,J04-4001,0,0.049962,"urage further work in this area; a comprehensive evaluation and comparison study which highlights the merits and shortcomings of various recently proposed datato-text generation models on two datasets. 2 Related Work The sports domain has attracted considerable attention since the early days of generation systems (Robin, 1994; Tanaka-Ishii et al., 1998). Likewise, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). Modeling entities and their communicative actions has also been shown to improve system output in interactive storytelling 2024 (Cavazza et al., 2002; Cavazza and Charles, 2005) and dialogue generation (Walker et al., 2011). More recently, the benefits of modeling entities explicitly have been demonstrated in various tasks and neural network models. Ji et al. (2017) make use of dynamic entity representations for language modeling. And Clark et al. (2018) extend this work by adding entity context as input to the decoder. Both approaches condition on a single entity at a time, while we dynamic"
P19-1195,D16-1032,0,0.0307624,"en shown to improve system output in interactive storytelling 2024 (Cavazza et al., 2002; Cavazza and Charles, 2005) and dialogue generation (Walker et al., 2011). More recently, the benefits of modeling entities explicitly have been demonstrated in various tasks and neural network models. Ji et al. (2017) make use of dynamic entity representations for language modeling. And Clark et al. (2018) extend this work by adding entity context as input to the decoder. Both approaches condition on a single entity at a time, while we dynamically represent and condition on multiple entities in parallel. Kiddon et al. (2016) make use of fixed entity representations to improve the coverage and coherence of the output for recipe generation. Bosselut et al. (2018) model actions and their effects on entities for the same task. However, in contrast to our work, they keep entity representations fixed during generation. Henaff et al. (2017) make use of dynamic entity representations in machine reading. Entity representations are scored against a query vector to directly predict an output class or combined as a weighted sum followed by softmax over the vocabulary. We make use of a similar entity representation model, ext"
P19-1195,P17-4012,0,0.0845709,"Missing"
P19-1195,D16-1128,0,0.22009,"Missing"
P19-1195,J95-2003,0,0.824578,"aramanis et al., 2004). Without knowing anything about baseball or how game summaries are typically written, a glance at the text in Figure 1 reveals that it is about a few entities, namely players who had an important part in the game (e.g., Brad Keller, Hunter Dozier) and their respective teams (e.g., Orioles, Royals). The prominent role of entities in achieving discourse coherence has been long recognized within the linguistic and cognitive science literature (Kuno, 1972; Chafe, 1976; Halliday and Hasan, 1976; Karttunen, 1976; Clark and Haviland, 1977; Prince, 1981), with Centering Theory (Grosz et al., 1995) being most prominent at formalizing how entities are linguistically realized and distributed in texts. In this work we propose an entity-centric neural architecture for data-to-text generation. Instead of treating entities as ordinary tokens, we create entity-specific representations (i.e., for players and teams) which are dynamically updated as text is 2023 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2023–2035 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics TEAM Inn1 Inn2 Inn3 Inn4 . . . R H E . ."
P19-1195,P16-1014,0,0.0364786,"(blocks B and C). Module fθ represents update equations (6)–(8) where θ is the set of trainable parameters. The gate represents the entity memory update (Equation (9)). Block B covers Equations (10) and (11), and block C Equations (12) and (13). where Wy ∈ R|Vy |×n , by ∈ R|Vy |are parameters and |Vy |is the output vocabulary size. We further augment the decoder with a copy mechanism i.e., the ability to copy values from the input; copy implies yt = rj,1 for some t and j (e.g., Royals, Orioles, 9, 2 in the summary in Figure 1 are copied from r). We use the conditional copy method proposed in Gulcehre et al. (2016) where a binary variable is introduced as a switch gate to indicate whether yt is copied or not. 4 To capture the fact that discourse in descriptive texts may shift from one entity to the next, e.g., some entities may be salient in the beginning of the game summary (see Brad Kelly in the text in Figure 1), others only towards the end (see Dozier in Figure 1), and a few throughout (e.g., references to teams), we update entity representations at each time step during decoding. We use gate γ t to indicate whether there should be an update in the entity representation: Entity Memory and Hierarchic"
P19-1195,D15-1166,0,0.189521,"Missing"
P19-1195,D17-1195,0,0.0425517,"of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). Modeling entities and their communicative actions has also been shown to improve system output in interactive storytelling 2024 (Cavazza et al., 2002; Cavazza and Charles, 2005) and dialogue generation (Walker et al., 2011). More recently, the benefits of modeling entities explicitly have been demonstrated in various tasks and neural network models. Ji et al. (2017) make use of dynamic entity representations for language modeling. And Clark et al. (2018) extend this work by adding entity context as input to the decoder. Both approaches condition on a single entity at a time, while we dynamically represent and condition on multiple entities in parallel. Kiddon et al. (2016) make use of fixed entity representations to improve the coverage and coherence of the output for recipe generation. Bosselut et al. (2018) model actions and their effects on entities for the same task. However, in contrast to our work, they keep entity representations fixed during gene"
P19-1195,N16-1086,0,0.408435,"tion is the task of generating textual output from non-linguistic input (Reiter and Dale, 1997; Gatt and Krahmer, 2018). The input may take on several guises including tables of records, simulations of physical systems, spreadsheets, and so on. As an example, Figure 1 shows (in a table format) the scoring summary of a major league baseball (MLB) game, a play-by-play summary with details of the most important events in the game recorded chronologically (i.e., in which play), and a human-written summary. Modern approaches to data-to-text generation have shown great promise (Lebret et al., 2016; Mei et al., 2016; Perez-Beltrachini and Lapata, 2018; Puduppully et al., 2019; Wiseman et al., 1 Our code and dataset can be found at https:// github.com/ratishsp/data2text-entity-py. 2017) thanks to the use of large-scale datasets and neural network models which are trained end-toend based on the very successful encoder-decoder architecture (Bahdanau et al., 2015). In contrast to traditional methods which typically implement pipeline-style architectures (Reiter and Dale, 2000) with modules devoted to individual generation components (e.g., content selection or lexical choice), neural models have no special-p"
P19-1195,P04-1050,0,0.031904,"(e.g., content selection or lexical choice), neural models have no special-purpose mechanisms for ensuring how to best generate a text. They simply rely on representation learning to select content appropriately, structure it coherently, and verbalize it grammatically. In this paper we are interested in the generation of descriptive texts such as the game summary shown in Figure 1. Descriptive texts are often characterized as “entity coherent” which means that their coherence is based on the way entities (also known as domain objects or concepts) are introduced and discussed in the discourse (Karamanis et al., 2004). Without knowing anything about baseball or how game summaries are typically written, a glance at the text in Figure 1 reveals that it is about a few entities, namely players who had an important part in the game (e.g., Brad Keller, Hunter Dozier) and their respective teams (e.g., Orioles, Royals). The prominent role of entities in achieving discourse coherence has been long recognized within the linguistic and cognitive science literature (Kuno, 1972; Chafe, 1976; Halliday and Hasan, 1976; Karttunen, 1976; Clark and Haviland, 1977; Prince, 1981), with Centering Theory (Grosz et al., 1995) be"
P19-1195,P02-1040,0,0.108954,"+Gate 31.84 91.97 RW Results Automatic Evaluation We first discuss the results of automatic evaluation using the metrics defined in Wiseman et al. (2017). Let yˆ be the gold output and y the model output. Relation Generation measures how factual y is compared to input r. Specifically, it measures the precision and number of relations extracted from y which are also found in r. Content Selection measures the precision and recall of relations between yˆ and y. Content Ordering measures the DamerauLevenshtein distance between relations in y and relations in yˆ. In addition, we also report BLEU (Papineni et al., 2002) with the gold summaries as reference. Table 2 (top) summarizes our results on the RO TOW IRE test set (results on the development set are available in the Appendix). We report results for our dynamic entity memory model (ENT), the best system of Wiseman et al. (2017) (WS2017) which is an encoder-decoder model with conditional copy, and NCP+CC (Puduppully et al., 2019). We see that ENT achieves scores comparable to NCP+CC, but performs better on the metrics of RG precision, CS precision, and CO. ENT achieves substantially higher scores in CS precision compared to WS-2017 and NCP+CC, without an"
P19-1195,N18-1137,1,0.909429,"Missing"
P19-1195,P98-2209,0,0.0811669,"ons in this work are three-fold: a novel entity-aware model for data-to-text generation which is linguistically motivated, yet resource lean (no preprocessing is required, e.g., to extract document plans); a new dataset for data-to-text generation which we hope will encourage further work in this area; a comprehensive evaluation and comparison study which highlights the merits and shortcomings of various recently proposed datato-text generation models on two datasets. 2 Related Work The sports domain has attracted considerable attention since the early days of generation systems (Robin, 1994; Tanaka-Ishii et al., 1998). Likewise, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). Modeling entities and their communicative actions has also been shown to improve system output in interactive storytelling 2024 (Cavazza et al., 2002; Cavazza and Charles, 2005) and dialogue generation (Walker et al., 2011). More recently, the benefits of modeling entities explicitly have been demonstrated in various tasks and ne"
P19-1195,N16-1174,0,0.0567619,"8) model actions and their effects on entities for the same task. However, in contrast to our work, they keep entity representations fixed during generation. Henaff et al. (2017) make use of dynamic entity representations in machine reading. Entity representations are scored against a query vector to directly predict an output class or combined as a weighted sum followed by softmax over the vocabulary. We make use of a similar entity representation model, extend it with hierarchical attention and apply it to data-to text generation. The hierarchical attention mechanism was first introduced in Yang et al. (2016) as a way of learning document-level representations. We apply attention over records and subsequently over entity memories. Several models have been proposed in the last few years for data-to-text generation (Mei et al. 2016; Lebret et al. 2016; Wiseman et al. 2017, inter alia) based on the very successful encoderdecoder architecture (Bahdanau et al., 2015). Various attempts have also been made to improve these models, e.g., by adding content selection (PerezBeltrachini and Lapata, 2018) and content planning (Puduppully et al., 2019) mechanisms. However, we are not aware of any prior work in"
P19-1195,D17-1239,0,\N,Missing
P19-1195,N18-1204,0,\N,Missing
P19-1195,C69-7001,0,\N,Missing
P19-1195,C69-6902,0,\N,Missing
P19-1195,C98-2204,0,\N,Missing
P19-1415,D14-1179,0,0.0545077,"Missing"
P19-1415,P18-1078,0,0.0736195,"eparate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do not contradict question answering for humans to the paragraph. Clark and Gardner (2018) and Tan et al. (2018) use questions to retrieve paragraphs that do not contain the answer as adversarial examples. Rajpurkar et al. (2018) create unanswerable questions through rigid rules, which swap entities, numbers and antonyms of answerable questions. It has been shown that adversarial examples generated by rule-based systems are much easier to detect than ones in the SQuAD 2.0 dataset. Data Augmentation for MRC Several attempts have been made to augment training data for machine reading comprehension. We catego4239 SEQ2SEQ tween inputs. The decoder of two models generates unanswerable q"
P19-1415,N19-1423,0,0.183228,") = Interac+on |˜ q| Y P (˜ qt |˜ q&lt;t , q, p, a) (1) t=1 Encoder W W W C … C C P A P W Encoder W W W W C … C C … C C C … C C C Q P A P Q Q Q Paragraph + Ques+on W Encoder P Paragraph W P W W W … where q˜&lt;t = q˜1 . . . q˜t−1 . W C C Q Q 3.1 Ques+on Figure 2: Diagram of the proposed pair-to-sequence model and sequence-to-sequence model. The input embeddings is the sum of the word embeddings, the character embeddings and the token type embeddings. The input questions are all answerable. rize these work according to the type of the augmentation data: external data source, paragraphs or questions. Devlin et al. (2019) fine-tune BERT on the SQuAD dataset jointly with another dataset TriviaQA (Joshi et al., 2017). Yu et al. (2018) paraphrase paragraphs with backtranslation. Another line of work adheres to generate answerable questions. Yang et al. (2017) propose to generate questions based on the unlabeled text for semisupervised question answering. Sun et al. (2019) propose a rule-based system to generate multiplechoice questions with candidate options upon the paragraphs. We aim at generating unanswerable questions as a means of data augmentation. 3 Problem Formulation Given an answerable question q and it"
P19-1415,P18-1177,0,0.049005,"Missing"
P19-1415,P17-1123,0,0.0430977,"l., 2018; Yu et al., 2018; Wang et al., 2018) have outperformed humans on this task in terms of automatic metrics. The SQuAD 2.0 dataset (Rajpurkar et al., 2018) extends SQuAD with more than 50, 000 crowdsourced unanswerable questions. So far, neural reading comprehension models still fall behind humans on SQuAD 2.0. Abstaining from answering when no answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. I"
P19-1415,P16-1154,0,0.0669924,"Missing"
P19-1415,N10-1086,0,0.0764351,"t al., 2018) extends SQuAD with more than 50, 000 crowdsourced unanswerable questions. So far, neural reading comprehension models still fall behind humans on SQuAD 2.0. Abstaining from answering when no answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable quest"
P19-1415,P82-1020,0,0.820425,"Missing"
P19-1415,D17-1215,0,0.184369,"xtra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do not contradict question answering for humans to the paragraph. Clark and Gardner (2018) and Tan et al. (2018) use questions to retrieve paragraphs that do not contain the answer as adversarial examples. Rajpurkar et al. (2018) create unanswerable questions through rigid rules, which swap entities, numbers and antonyms of answerable questions. It has been shown that adversarial examples generated by rule-based systems are much easier to detect than ones in the SQuAD 2.0 dataset. Data Augmentation for MRC Several attempts hav"
P19-1415,P17-1147,0,0.0420112,"W W W C … C C … C C C … C C C Q P A P Q Q Q Paragraph + Ques+on W Encoder P Paragraph W P W W W … where q˜&lt;t = q˜1 . . . q˜t−1 . W C C Q Q 3.1 Ques+on Figure 2: Diagram of the proposed pair-to-sequence model and sequence-to-sequence model. The input embeddings is the sum of the word embeddings, the character embeddings and the token type embeddings. The input questions are all answerable. rize these work according to the type of the augmentation data: external data source, paragraphs or questions. Devlin et al. (2019) fine-tune BERT on the SQuAD dataset jointly with another dataset TriviaQA (Joshi et al., 2017). Yu et al. (2018) paraphrase paragraphs with backtranslation. Another line of work adheres to generate answerable questions. Yang et al. (2017) propose to generate questions based on the unlabeled text for semisupervised question answering. Sun et al. (2019) propose a rule-based system to generate multiplechoice questions with candidate options upon the paragraphs. We aim at generating unanswerable questions as a means of data augmentation. 3 Problem Formulation Given an answerable question q and its corresponding paragraph p that contains the answer a, we aim to generate unanswerable questio"
P19-1415,P17-4012,0,0.0800098,"Missing"
P19-1415,Q18-1023,0,0.0474809,"Missing"
P19-1415,K17-1034,0,0.0257042,"ast example shows that inserting negation words in different positions (“n’t public” versus “not in victoria”) can express different meanings. Such cases are critical for generated questions’ answerability, which is hard to handle in a rule-based system. 4.2 4.2.1 Data Augmentation for Machine Reading Comprehension Question Answering Models We apply our automatically generated unanswerable questions as augmentation data to the following reading comprehension models: BiDAF-No-Answer (BNA) BiDAF (Seo et al., 2017) is a benchmark model on extractive machine reading comprehension. Based on BiDAF, Levy et al. (2017) propose the BiDAF-No-Answer model to predict the distribution of answer candidates and the probability of a question being unanswerable at the same time. DocQA Clark and Gardner (2018) propose the DocQA model to address document-level reading comprehension. The no-answer probability is also predicted jointly. BERT Fine-Tuning It is the state-of-the-art model on unanswerable machine reading comprehension. We adopt the uncased version of BERT (Devlin et al., 2019) for fine-tuning. The batch sizes of BERT-base and BERT-large are set to 12 and 24 respectively. The rest hyperparameters are kept un"
P19-1415,W04-1013,0,0.0112197,"d embeddings. The hidden state size of LSTM network is 150. Dropout probability is set to 0.2. The data are shuffled and split into mini-batches of size 32 for training. The model is optimized with Adagrad (Duchi et al., 2011) with an initial learning rate of 0.15. During inference, the beam size is 5. We prohibit producing unknown words by setting the score of &lt;unk&gt; token to -inf. We filter the beam outputs that make no differences to the input question. 4.1.3 Evaluation Metrics The generation quality is evaluated using three automatic evaluation metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and GLEU (Napoles et al., 2015). BLEU1 is a commonly used metric in machine translation that computes ngram precisions over references. Recall-oriented ROUGE2 metric is widely adopted in summarization, and ROUGE-L measures longest common subsequence between system outputs and references. GLEU3 is a variant of BLEU with the modification that penalizes system output n-grams that present in input but absent from the reference. This makes GLEU a preferable metric for tasks with subtle but critical differences in a monolingual setting as in our unanswerable question generation task. We also conduc"
P19-1415,P18-1157,0,0.0277041,"Missing"
P19-1415,D15-1166,0,0.0405388,"rent neural networks with long short-term memory units (LSTM; Hochreiter and Schmidhuber, 1997) to produce encoder hidden states hi = fBiLSTM (hi−1 , ei ). On each decoding step t, the hidden states of decoder (a single-layer unidirectional LSTM network) are computed by st = fLSTM (st−1 , [yt−1 ; ct−1 ]), where yt−1 is the word embedding of previously predicted token and ct−1 is the encoder context vector of previous step. Besides, we use an attention mechanism to summarize the encoder-side information into ct for current step. The attention distribution γt over source words is computed as in Luong et al. (2015): score(hi , st ) = hT i Wγ s t γi,t = exp(score(hi , st ))/Zt ct = |x| X γi,t hi (2) (3) (4) i P|x| where Zt = k exp(score(hk , st )), Wγ in score function is a learnable parameter. Next, st is concatenated with ct to produce the vocabulary distribution Pv : 4240 Pv = softmax(Wv [st ; ct ] + bv ) (5) where Wv and bv are learnable parameters. Copy mechanism (See et al., 2017) is incorporated to directly copy words from inputs, because words in paragraphs or source questions are of great value for unanswerable question generation. Specifically, we use st and ct to produce a gating probability g"
P19-1415,P15-2097,0,0.0231868,"den state size of LSTM network is 150. Dropout probability is set to 0.2. The data are shuffled and split into mini-batches of size 32 for training. The model is optimized with Adagrad (Duchi et al., 2011) with an initial learning rate of 0.15. During inference, the beam size is 5. We prohibit producing unknown words by setting the score of &lt;unk&gt; token to -inf. We filter the beam outputs that make no differences to the input question. 4.1.3 Evaluation Metrics The generation quality is evaluated using three automatic evaluation metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and GLEU (Napoles et al., 2015). BLEU1 is a commonly used metric in machine translation that computes ngram precisions over references. Recall-oriented ROUGE2 metric is widely adopted in summarization, and ROUGE-L measures longest common subsequence between system outputs and references. GLEU3 is a variant of BLEU with the modification that penalizes system output n-grams that present in input but absent from the reference. This makes GLEU a preferable metric for tasks with subtle but critical differences in a monolingual setting as in our unanswerable question generation task. We also conduct human evaluation on 100 sample"
P19-1415,P02-1040,0,0.104033,"are the same vocabulary and word embeddings. The hidden state size of LSTM network is 150. Dropout probability is set to 0.2. The data are shuffled and split into mini-batches of size 32 for training. The model is optimized with Adagrad (Duchi et al., 2011) with an initial learning rate of 0.15. During inference, the beam size is 5. We prohibit producing unknown words by setting the score of &lt;unk&gt; token to -inf. We filter the beam outputs that make no differences to the input question. 4.1.3 Evaluation Metrics The generation quality is evaluated using three automatic evaluation metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and GLEU (Napoles et al., 2015). BLEU1 is a commonly used metric in machine translation that computes ngram precisions over references. Recall-oriented ROUGE2 metric is widely adopted in summarization, and ROUGE-L measures longest common subsequence between system outputs and references. GLEU3 is a variant of BLEU with the modification that penalizes system output n-grams that present in input but absent from the reference. This makes GLEU a preferable metric for tasks with subtle but critical differences in a monolingual setting as in our unanswerable question generation t"
P19-1415,D14-1162,0,0.0826805,"Missing"
P19-1415,P18-2124,0,0.555636,"aset. The annotated (plausible) answer span in the paragraph is used as a pivot to align the pair of answerable and unanswerable questions. Introduction Extractive reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016) obtains great attentions from both research and industry in recent years. End-to-end neural models (Seo et al., 2017; Wang et al., 2017; Yu et al., 2018) have achieved remarkable performance on the task if answers are assumed to be in the given paragraph. Nonetheless, the current systems are still not good at deciding whether no answer is presented in the context (Rajpurkar et al., 2018). For unanswerable questions, the systems are supposed to abstain from answering rather than making unreliable guesses, which is an embodiment of language understanding ability. ∗ Asia. Contribution during internship at Microsoft Research We attack the problem by automatically generating unanswerable questions for data augmentation to improve question answering models. The generated unanswerable questions should not be too easy for the question answering model so that data augmentation can better help the model. For example, a simple baseline method is randomly choosing a question asked for an"
P19-1415,D16-1264,0,0.0603667,"ion on the SQuAD 2.0 dataset, yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model. 1 Ans. Question: What organization runs the public schools in Victoria? UnAns. Question: What organization runs the waste management in Victoria? (Plausible) Answer: Victoria Department of Education Figure 1: An example taken from the SQuAD 2.0 dataset. The annotated (plausible) answer span in the paragraph is used as a pivot to align the pair of answerable and unanswerable questions. Introduction Extractive reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016) obtains great attentions from both research and industry in recent years. End-to-end neural models (Seo et al., 2017; Wang et al., 2017; Yu et al., 2018) have achieved remarkable performance on the task if answers are assumed to be in the given paragraph. Nonetheless, the current systems are still not good at deciding whether no answer is presented in the context (Rajpurkar et al., 2018). For unanswerable questions, the systems are supposed to abstain from answering rather than making unreliable guesses, which is an embodiment of language understanding ability. ∗ Asia. Contribution during int"
P19-1415,P17-1099,0,0.0281374,"s the encoder context vector of previous step. Besides, we use an attention mechanism to summarize the encoder-side information into ct for current step. The attention distribution γt over source words is computed as in Luong et al. (2015): score(hi , st ) = hT i Wγ s t γi,t = exp(score(hi , st ))/Zt ct = |x| X γi,t hi (2) (3) (4) i P|x| where Zt = k exp(score(hk , st )), Wγ in score function is a learnable parameter. Next, st is concatenated with ct to produce the vocabulary distribution Pv : 4240 Pv = softmax(Wv [st ; ct ] + bv ) (5) where Wv and bv are learnable parameters. Copy mechanism (See et al., 2017) is incorporated to directly copy words from inputs, because words in paragraphs or source questions are of great value for unanswerable question generation. Specifically, we use st and ct to produce a gating probability gt : gt = sigmoid(Wg [st ; ct ] + bg ) (6) where Wg and bg are learnable parameters. The gate gt determines whether generating a word from the vocabulary or copying a word from inputs. Finally, we obtain the probability of generating q˜t by: X P (˜ qt |˜ q&lt;t , q, p, a) = gt Pv (˜ qt ) + (1 − gt ) γˆi,t i∈ζq˜t where ζq˜t denotes all the occurrence of q˜t in inputs, and the copy"
P19-1415,N18-2090,0,0.0319965,"document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do not contradict question answering for h"
P19-1415,N19-1270,0,0.0353075,"Missing"
P19-1415,D18-1427,0,0.0242532,"answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do"
P19-1415,P18-1158,0,0.0133866,"ith BERT-large model. 2 Related Work Machine Reading Comprehension (MRC) Various large-scale datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2018; Kocisky et al., 2018) have spurred rapid progress on machine reading comprehension in recent years. SQuAD (Rajpurkar et al., 2016) is an extractive benchmark whose questions and answers spans are annotated by humans. Neural reading comprehension systems (Wang and Jiang, 2017; Seo et al., 2017; Wang et al., 2017; Hu et al., 2018; Huang et al., 2018; Liu et al., 2018; Yu et al., 2018; Wang et al., 2018) have outperformed humans on this task in terms of automatic metrics. The SQuAD 2.0 dataset (Rajpurkar et al., 2018) extends SQuAD with more than 50, 000 crowdsourced unanswerable questions. So far, neural reading comprehension models still fall behind humans on SQuAD 2.0. Abstaining from answering when no answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models base"
P19-1415,P17-1018,1,0.86445,"1 Ans. Question: What organization runs the public schools in Victoria? UnAns. Question: What organization runs the waste management in Victoria? (Plausible) Answer: Victoria Department of Education Figure 1: An example taken from the SQuAD 2.0 dataset. The annotated (plausible) answer span in the paragraph is used as a pivot to align the pair of answerable and unanswerable questions. Introduction Extractive reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016) obtains great attentions from both research and industry in recent years. End-to-end neural models (Seo et al., 2017; Wang et al., 2017; Yu et al., 2018) have achieved remarkable performance on the task if answers are assumed to be in the given paragraph. Nonetheless, the current systems are still not good at deciding whether no answer is presented in the context (Rajpurkar et al., 2018). For unanswerable questions, the systems are supposed to abstain from answering rather than making unreliable guesses, which is an embodiment of language understanding ability. ∗ Asia. Contribution during internship at Microsoft Research We attack the problem by automatically generating unanswerable questions for data augmentation to improve"
P19-1415,P17-1096,0,0.0389431,"ues+on Figure 2: Diagram of the proposed pair-to-sequence model and sequence-to-sequence model. The input embeddings is the sum of the word embeddings, the character embeddings and the token type embeddings. The input questions are all answerable. rize these work according to the type of the augmentation data: external data source, paragraphs or questions. Devlin et al. (2019) fine-tune BERT on the SQuAD dataset jointly with another dataset TriviaQA (Joshi et al., 2017). Yu et al. (2018) paraphrase paragraphs with backtranslation. Another line of work adheres to generate answerable questions. Yang et al. (2017) propose to generate questions based on the unlabeled text for semisupervised question answering. Sun et al. (2019) propose a rule-based system to generate multiplechoice questions with candidate options upon the paragraphs. We aim at generating unanswerable questions as a means of data augmentation. 3 Problem Formulation Given an answerable question q and its corresponding paragraph p that contains the answer a, we aim to generate unanswerable questions q˜ that fulfills certain requirements. First, it cannot be answered by paragraph p. Second, it must be relevant to both answerable question q"
P19-1415,P13-1171,0,0.0355318,"Research We attack the problem by automatically generating unanswerable questions for data augmentation to improve question answering models. The generated unanswerable questions should not be too easy for the question answering model so that data augmentation can better help the model. For example, a simple baseline method is randomly choosing a question asked for another paragraph, and using it as an unanswerable question. However, it would be trivial to determine whether the retrieved question is answerable by using wordoverlap heuristics, because the question is irrelevant to the context (Yih et al., 2013). In this work, we propose to generate unanswerable questions by editing an answerable question and conditioning on the corresponding paragraph that contains the answer. So the generated unanswerable questions are more lexically similar and relevant to the context. Moreover, by using the answerable question as a prototype and its answer span as a plausible answer, the generated examples can provide more discriminative training signal to the question answering model. 4238 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4238–4248 c Florence, Italy,"
S15-2086,S14-2115,0,0.0131346,"l (Mikolov et al., 2013) to learn 40-dimensional word embeddings from a twitter dataset. Then, we employ K-means algorithm and L2 distance of word vectors to cluster the 255, 657 words to 4960 classes. The clusters are used to represent words. We extract unigrams and bigrams as features, and use them in sub/obj classifier. The word2vec clustering results are publicly available3 for research purposes. As shown in Table 1, similar words are clustered into the same clusters. This feature template is used in sub/obj classification. CNN predicted distribution The convolutional neural networks (dos Santos, 2014) are used to predict the probabilities of three sentiment classes, and the predicted distribution is used as a threedimension feature template. As illustrated in Figure 2, we use the network architecture proposed by Collobert et al. (2011). The dimension of word vectors is 50, and the window size is 5. Then the concatenated word vectors are fed into a convolutional layer. The vector representation of a sentence is obtained by a max pooling layer, and is used to predict the probabilities of three classes by the softmax layer. We employ stochastic gradient descent to minimize the cross-entropy l"
S15-2086,D14-1181,0,0.0112281,"Missing"
S15-2086,S13-2052,0,0.0610536,"performance of our system. 1 Introduction In the task 10 of SemEval-2015, submitted systems are required to categorize tweets to positive, negative, and neutral classes (Rosenthal et al., 2015). There are six testing sets in SemEval2015. Four of them are tweets: Twitter13, Twitter14, Twitter14Sarcasm, and Twitter15. The TwitterSarcasm14 consists of the tweets which express sarcasm. In order to evaluate the performance on out-of-domain data, the other two datasets are LiveJournal14 and SMS13 that are from web blogs and SMS messages respectively. The details of these datasets are described in (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). ∗ Contribution during internship at Microsoft Research. 2 2.1 System Description Overview As shown in Figure 1, our sentiment analysis system is a two-stage sentiment classifier which consists of a subjective/objective (sub/obj) classifier and a positive/negative (pos/neg) classifier. By using this artitacture, we can design different feature sets for the two classification steps. Notably, the predicted values of pos/neg classifier is employed to help classify tweets to sub/obj classes. We employ the LIBLINEAR (Fan et al., 2008) with option “-"
S15-2086,N13-1039,0,0.0449785,"ize. Word ngrams We use unigrams and bigrams for words. Character ngrams For each word, character ngrams are extracted. We use four-grams and fivegrams in our system. Word skip-grams For all the trigrams and fourgrams, one of the words is replaced by ∗ to indicate the presence of non-contiguous words. This feature template is used in sub/obj classification. Brown cluster ngrams We use Brown clusters1 to represent words, and extract unigrams and bigrams as features. POS The presence or absence of part-of-speech tags are used as binary features. We use the CMU ARK Twitter Part-of-Speech Tagger (Owoputi et al., 2013) in our implementation. Lexicons The NRC Hashtag Sentiment Lexicon 1 http://www.ark.cs.cmu.edu/TweetNLP/clusters/50mpaths2 516 2.3 Deep Learning Features In order to automatically extract features, we explore using some deep learning techniques in our system. These features and the basic features described in Section 2.2 are used together to learn classifiers. Word2vec cluster ngrams We use the word2vec tool (Mikolov et al., 2013) to learn 40-dimensional word embeddings from a twitter dataset. Then, we employ K-means algorithm and L2 distance of word vectors to cluster the 255, 657 words to 49"
S15-2086,S14-2009,0,0.0279426,"system. 1 Introduction In the task 10 of SemEval-2015, submitted systems are required to categorize tweets to positive, negative, and neutral classes (Rosenthal et al., 2015). There are six testing sets in SemEval2015. Four of them are tweets: Twitter13, Twitter14, Twitter14Sarcasm, and Twitter15. The TwitterSarcasm14 consists of the tweets which express sarcasm. In order to evaluate the performance on out-of-domain data, the other two datasets are LiveJournal14 and SMS13 that are from web blogs and SMS messages respectively. The details of these datasets are described in (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). ∗ Contribution during internship at Microsoft Research. 2 2.1 System Description Overview As shown in Figure 1, our sentiment analysis system is a two-stage sentiment classifier which consists of a subjective/objective (sub/obj) classifier and a positive/negative (pos/neg) classifier. By using this artitacture, we can design different feature sets for the two classification steps. Notably, the predicted values of pos/neg classifier is employed to help classify tweets to sub/obj classes. We employ the LIBLINEAR (Fan et al., 2008) with option “-s 1” as our classifier."
S15-2086,S15-2078,0,0.0805037,"he sentiment labels for tweets, which enables us to design different features for subjective/objective classification and positive/negative classification. In addition to n-grams, lexicons, word clusters, and twitter-specific features, we develop several deep learning methods to automatically extract features for the message-level sentiment classification task. Moreover, we propose a polarity boosting trick which improves the performance of our system. 1 Introduction In the task 10 of SemEval-2015, submitted systems are required to categorize tweets to positive, negative, and neutral classes (Rosenthal et al., 2015). There are six testing sets in SemEval2015. Four of them are tweets: Twitter13, Twitter14, Twitter14Sarcasm, and Twitter15. The TwitterSarcasm14 consists of the tweets which express sarcasm. In order to evaluate the performance on out-of-domain data, the other two datasets are LiveJournal14 and SMS13 that are from web blogs and SMS messages respectively. The details of these datasets are described in (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). ∗ Contribution during internship at Microsoft Research. 2 2.1 System Description Overview As shown in Figure 1, our sentiment"
S15-2086,D13-1170,0,0.0263646,"Missing"
S15-2086,P14-1146,1,0.837196,"lovve, love/miss, luuuvvv, lubb, lurve Table 1: Examples of word2vec clusters. Similar words are clustered to the same cluster. 3 300 300 300 Softmax layer Hidden layer ... ... Max pooling layer Convolutional layer 5x50 Figure 2: Architecture of convolutional neural network used in our system. The lines represent vectors, and the numbers indicate the vector dimensions. vent overfitting, a L2-norm constraint for the column vectors of weight matrices is used. The backpropagation algorithm (Rumelhart et al., 1986) is employed to compute the gradients for parameters. The word vectors provided by Tang et al. (2014) are used for initialization. Sentiment-specific embedding Tang et al. (2014) improve the word2vec model to learn sentimentspecific word embeddings from tweets annotated by emoticons. We use element-wise max, min, and avg operations for the word vectors to extract features. 2.4 Polarity Boosting Trick Predicted scores indicate the confidence of classifier. If the pos/neg classifier has a high confidence to classify a tweet to positive or negative, it is less likely that this tweet is objective. Consequently, the absolute value of output of pos/neg classifier is used as a feature in sub/obj cla"
S15-2086,H05-1044,0,0.0384794,"ributes to the performances. It provides more explicit sentiment information than word2vec vectors. As shown in Table 2, the polarity boosting trick also contributes to the performance of our system on all the six datasets. Table 2: We compare the macro-averaged F1-scores of our system (Spp) with the best results of other teams in SemEval-2015. Our system achieves the highest F1scores on three out of six datasets. LvJn14 and Sarc14 become better. Moreover, the automatically learned lexicons play a positive role in our system. We also try some manually annotated lexicons (such as MPQA Lexicon (Wilson et al., 2005), and Bing Liu Lexicon (Hu and Liu, 2004)), but the performance drops on the dev data. It illustrates the coverage of lexicons is important for the informal text data. The cluster features are also useful in this task, because the clusters reduce the feature sparsity and have the ability to deal with out-ofvocabulary words. The deep learning significantly improves test results on all the datasets except on the sarcastic tweets. Using the clustering results of word2vec performs better and more stable than directly using the vectors as features. This feature template contributes more than other"
