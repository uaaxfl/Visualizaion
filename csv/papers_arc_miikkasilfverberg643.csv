2021.sigmorphon-1.8,Findings of the {SIGMORPHON} 2021 Shared Task on Unsupervised Morphological Paradigm Clustering,2021,-1,-1,6,1,1304,adam wiemerslage,"Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"We describe the second SIGMORPHON shared task on unsupervised morphology: the goal of the SIGMORPHON 2021 Shared Task on Unsupervised Morphological Paradigm Clustering is to cluster word types from a raw text corpus into paradigms. To this end, we release corpora for 5 development and 9 test languages, as well as gold partial paradigms for evaluation. We receive 14 submissions from 4 teams that follow different strategies, and the best performing system is based on adaptor grammars. Results vary significantly across languages. However, all systems are outperformed by a supervised lemmatizer, implying that there is still room for improvement."
2021.sigmorphon-1.11,Unsupervised Paradigm Clustering Using Transformation Rules,2021,-1,-1,3,0,1317,changbing yang,"Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"This paper describes the submission of the CU-UBC team for the SIGMORPHON 2021 Shared Task 2: Unsupervised morphological paradigm clustering. Our system generates paradigms using morphological transformation rules which are discovered from raw data. We experiment with two methods for discovering rules. Our first approach generates prefix and suffix transformations between similar strings. Secondly, we experiment with more general rules which can apply transformations inside the input strings in addition to prefix and suffix transformations. We find that the best overall performance is delivered by prefix and suffix rules but more general transformation rules perform better for languages with templatic morphology and very high morpheme-to-word ratios."
2021.sigmorphon-1.21,An {FST} morphological analyzer for the Gitksan language,2021,-1,-1,3,0,1348,clarissa forbes,"Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"This paper presents a finite-state morphological analyzer for the Gitksan language. The analyzer draws from a 1250-token Eastern dialect wordlist. It is based on finite-state technology and additionally includes two extensions which can provide analyses for out-of-vocabulary words: rules for generating predictable dialect variants, and a neural guesser component. The pre-neural analyzer, tested against interlinear-annotated texts from multiple dialects, achieves coverage of (75-81{\%}), and maintains high precision (95-100{\%}). The neural extension improves coverage at the cost of lowered precision."
2021.naacl-main.435,Do {RNN} States Encode Abstract Phonological Alternations?,2021,-1,-1,1,1,1308,miikka silfverberg,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Sequence-to-sequence models have delivered impressive results in word formation tasks such as morphological inflection, often learning to model subtle morphophonological details with limited training data. Despite the performance, the opacity of neural models makes it difficult to determine whether complex generalizations are learned, or whether a kind of separate rote memorization of each morphophonological process takes place. To investigate whether complex alternations are simply memorized or whether there is some level of generalization across related sound changes in a sequence-to-sequence model, we perform several experiments on Finnish consonant gradation{---}a complex set of sound changes triggered in some words by certain suffixes. We find that our models often{---}though not always{---}encode 17 different consonant gradation processes in a handful of dimensions in the RNN. We also show that by scaling the activations in these dimensions we can control whether consonant gradation occurs and the direction of the gradation."
2021.computel-1.1,Expanding the {JHU} {B}ible Corpus for Machine Translation of the Indigenous Languages of {N}orth {A}merica,2021,-1,-1,4,0.287001,1307,garrett nicolai,Proceedings of the 4th Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume 1 (Papers),0,None
2020.sigmorphon-1.1,{SIGMORPHON} 2020 Shared Task 0: Typologically Diverse Morphological Inflection,2020,-1,-1,27,0.483871,1282,ekaterina vylomova,"Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"A broad goal in natural language processing (NLP) is to develop a system that has the capacity to process any natural language. Most systems, however, are developed using data from just one language such as English. The SIGMORPHON 2020 shared task on morphological reinflection aims to investigate systems{'} ability to generalize across typologically distinct languages, many of which are low resource. Systems were developed using data from 45 languages and just 5 language families, fine-tuned with data from an additional 45 languages and 10 language families (13 in total), and evaluated on all 90 languages. A total of 22 systems (19 neural) from 10 teams were submitted to the task. All four winning systems were neural (two monolingual transformers and two massively multilingual RNN-based models with gated attention). Most teams demonstrate utility of data hallucination and augmentation, ensembles, and multilingual training for low-resource languages. Non-neural learners and manually designed grammars showed competitive and even superior performance on some languages (such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited data. Some language families (Afro-Asiatic, Niger-Congo, Turkic) were relatively easy for most systems and achieved over 90{\%} mean accuracy while others were more challenging."
2020.sigmorphon-1.16,One Model to Pronounce Them All: Multilingual Grapheme-to-Phoneme Conversion With a Transformer Ensemble,2020,-1,-1,3,0,14892,kaili vesik,"Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"The task of grapheme-to-phoneme (G2P) conversion is important for both speech recognition and synthesis. Similar to other speech and language processing tasks, in a scenario where only small-sized training data are available, learning G2P models is challenging. We describe a simple approach of exploiting model ensembles, based on multilingual Transformers and self-training, to develop a highly effective G2P solution for 15 languages. Our models are developed as part of our participation in the SIGMORPHON 2020 Shared Task 1 focused at G2P. Our best models achieve 14.99 word error rate (WER) and 3.30 phoneme error rate (PER), a sizeable improvement over the shared task competitive baselines."
2020.lrec-1.433,Automated Phonological Transcription of {A}kkadian Cuneiform Text,2020,-1,-1,2,0,14358,aleksi sahala,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Akkadian was an East-Semitic language spoken in ancient Mesopotamia. The language is attested on hundreds of thousands of cuneiform clay tablets. Several Akkadian text corpora contain only the transliterated text. In this paper, we investigate automated phonological transcription of the transliterated corpora. The phonological transcription provides a linguistically appealing form to represent Akkadian, because the transcription is normalized according to the grammatical description of a given dialect and explicitly shows the Akkadian renderings for Sumerian logograms. Because cuneiform text does not mark the inflection for logograms, the inflected form needs to be inferred from the sentence context. To the best of our knowledge, this is the first documented attempt to automatically transcribe Akkadian. Using a context-aware neural network model, we are able to automatically transcribe syllabic tokens at near human performance with 96{\%} recall @ 3, while the logogram transcription remains more challenging at 82{\%} recall @ 3."
2020.lrec-1.479,{B}aby{FST} - Towards a Finite-State Based Computational Model of Ancient Babylonian,2020,-1,-1,2,0,14358,aleksi sahala,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Akkadian is a fairly well resourced extinct language that does not yet have a comprehensive morphological analyzer available. In this paper we describe a general finite-state based morphological model for Babylonian, a southern dialect of the Akkadian language, that can achieve a coverage up to 97.3{\%} and recall up to 93.7{\%} on lemmatization and POS-tagging task on token level from a transcribed input. Since Akkadian word forms exhibit a high degree of morphological ambiguity, in that only 20.1{\%} of running word tokens receive a single unambiguous analysis, we attempt a first pass at weighting our finite-state transducer, using existing extensive Akkadian corpora which have been partially validated for their lemmas and parts-of-speech but not the entire morphological analyses. The resultant weighted finite-state transducer yields a moderate improvement so that for 57.4{\%} of the word tokens the highest ranked analysis is the correct one. We conclude with a short discussion on how morphological ambiguity in the analysis of Akkadian could be further reduced with improvements in the training data used in weighting the finite-state transducer as well as through other, context-based techniques."
2020.lrec-1.483,{U}ni{M}orph 3.0: {U}niversal {M}orphology,2020,-1,-1,10,0.564516,1305,arya mccarthy,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological paradigms for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation and a type-level resource of annotated data in diverse languages realizing that schema. We have implemented several improvements to the extraction pipeline which creates most of our data, so that it is both more complete and more correct. We have added 66 new languages, as well as new parts of speech for 12 languages. We have also amended the schema in several ways. Finally, we present three new community tools: two to validate data for resource creators, and one to make morphological data available from the command line. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the schema, tooling, and dissemination of project resources since the UniMorph 2.0 release described at LREC 2018."
2020.coling-main.255,Noise Isn{'}t Always Negative: Countering Exposure Bias in Sequence-to-Sequence Inflection Models,2020,-1,-1,2,0.287001,1307,garrett nicolai,Proceedings of the 28th International Conference on Computational Linguistics,0,"Morphological inflection, like many sequence-to-sequence tasks, sees great performance from recurrent neural architectures when data is plentiful, but performance falls off sharply in lower-data settings. We investigate one aspect of neural seq2seq models that we hypothesize contributes to overfitting - teacher forcing. By creating different training and test conditions, exposure bias increases the likelihood that a system too closely models its training data. Experiments show that teacher-forced models struggle to recover when they enter unknown territory. However, a simple modification to the training algorithm to more closely mimic test conditions creates models that are better able to generalize to unseen environments."
W19-6132,Ensembles of Neural Morphological Inflection Models,2019,-1,-1,2,0,23710,ilmari kylliainen,Proceedings of the 22nd Nordic Conference on Computational Linguistics,0,"We investigate different ensemble learning techniques for neural morphological inflection using bidirectional LSTM encoder-decoder models with attention. We experiment with weighted and unweighted majority voting and bagging. We find that all investigated ensemble methods lead to improved accuracy over a baseline of a single model. However, contrary to expectation based on earlier work by Najafi et al. (2018) and Silfverberg et al. (2017), weighting does not deliver clear benefits. Bagging was found to underperform plain voting ensembles in general."
W19-4226,The {SIGMORPHON} 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection,2019,23,2,8,1,1305,arya mccarthy,"Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual analysis in morphology examined transfer learning of inflection between 100 language pairs, as well as contextual lemmatization and morphosyntactic description in 66 languages. The first task evolves past years{'} inflection tasks by examining transfer of morphological inflection knowledge from a high-resource language to a low-resource language. This year also presents a new second challenge on lemmatization and morphological feature analysis in context. All submissions featured a neural component and built on either this year{'}s strong baselines or highly ranked systems from previous years{'} shared tasks. Every participating team improved in accuracy over the baselines for the inflection task (though not Levenshtein distance), and every team in the contextual analysis task improved on both state-of-the-art neural and non-neural baselines."
W19-1401,A Report on the Third {V}ar{D}ial Evaluation Campaign,2019,-1,-1,6,0,622,marcos zampieri,"Proceedings of the Sixth Workshop on {NLP} for Similar Languages, Varieties and Dialects",0,"In this paper, we present the findings of the Third VarDial Evaluation Campaign organized as part of the sixth edition of the workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with NAACL 2019. This year, the campaign included five shared tasks, including one task re-run {--} German Dialect Identification (GDI) {--} and four new tasks {--} Cross-lingual Morphological Analysis (CMA), Discriminating between Mainland and Taiwan variation of Mandarin Chinese (DMT), Moldavian vs. Romanian Cross-dialect Topic identification (MRC), and Cuneiform Language Identification (CLI). A total of 22 teams submitted runs across the five shared tasks. After the end of the competition, we received 14 system description papers, which are published in the VarDial workshop proceedings and referred to in this report."
W19-0301,Data-Driven Morphological Analysis for Uralic Languages,2019,0,1,1,1,1308,miikka silfverberg,Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages,0,None
K19-1014,Weird Inflects but {OK}: Making Sense of Morphological Generation Errors,2019,0,1,5,0,1324,kyle gorman,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"We conduct a manual error analysis of the CoNLL-SIGMORPHON Shared Task on Morphological Reinflection. This task involves natural language generation: systems are given a word in citation form (e.g., hug) and asked to produce the corresponding inflected form (e.g., the simple past hugged). We propose an error taxonomy and use it to annotate errors made by the top two systems across twelve languages. Many of the observed errors are related to inflectional patterns sensitive to inherent linguistic properties such as animacy or affect; many others are failures to predict truly unpredictable inflectional behaviors. We also find nearly one quarter of the residual {``}errors{''} reflect errors in the gold data."
W18-6011,Marrying {U}niversal {D}ependencies and {U}niversal {M}orphology,2018,0,9,2,1,1305,arya mccarthy,Proceedings of the Second Workshop on Universal Dependencies ({UDW} 2018),0,"The Universal Dependencies (UD) and Universal Morphology (UniMorph) projects each present schemata for annotating the morphosyntactic details of language. Each project also provides corpora of annotated text in many languages{---}UD at the token level and UniMorph at the type level. As each corpus is built by different annotators, language-specific decisions hinder the goal of universal schemata. With compatibility of tags, each project{'}s annotations could be used to validate the other{'}s. Additionally, the availability of both type- and token-level resources would be a boon to tasks such as parsing and homograph disambiguation. To ease this interoperability, we present a deterministic mapping from Universal Dependencies v2 features into the UniMorph schema. We validate our approach by lookup in the UniMorph corpora and find a macro-average of 64.13{\%} recall. We also note incompatibilities due to paucity of data on either side. Finally, we present a critical evaluation of the foundations, strengths, and weaknesses of the two annotation projects."
W18-5818,Phonological Features for Morphological Inflection,2018,-1,-1,2,1,1304,adam wiemerslage,"Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"Modeling morphological inflection is an important task in Natural Language Processing. In contrast to earlier work that has largely used orthographic representations, we experiment with this task in a phonetic character space, representing inputs as either IPA segments or bundles of phonological distinctive features. We show that both of these inputs, somewhat counterintuitively, achieve similar accuracies on morphological inflection, slightly lower than orthographic models. We conclude that providing detailed phonological representations is largely redundant when compared to IPA segments, and that articulatory distinctions relevant for word inflection are already latently present in the distributional properties of many graphemic writing systems."
W18-3904,Sub-label dependencies for Neural Morphological Tagging {--} The Joint Submission of {U}niversity of {C}olorado and {U}niversity of {H}elsinki for {V}ar{D}ial 2018,2018,0,0,1,1,1308,miikka silfverberg,"Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)",0,"This paper presents the submission of the UH{\&}CU team (Joint University of Colorado and University of Helsinki team) for the VarDial 2018 shared task on morphosyntactic tagging of Croatian, Slovenian and Serbian tweets. Our system is a bidirectional LSTM tagger which emits tags as character sequences using an LSTM generator in order to be able to handle unknown tags and combinations of several tags for one token which occur in the shared task data sets. To the best of our knowledge, using an LSTM generator is a novel approach. The system delivers sizable improvements of more than 6{\%}-points over a baseline trigram tagger. Overall, the performance of our system is quite even for all three languages."
W18-0314,Sound Analogies with Phoneme Embeddings,2018,18,2,1,1,1308,miikka silfverberg,Proceedings of the Society for Computation in Linguistics ({SC}i{L}) 2018,0,None
W18-0209,Initial Experiments in Data-Driven Morphological Analysis for {F}innish,2018,-1,-1,1,1,1308,miikka silfverberg,Proceedings of the Fourth International Workshop on Computational Linguistics of Uralic Languages,0,None
L18-1294,A Computational Architecture for the Morphology of {U}pper {T}anana,2018,0,0,3,0,29834,olga lovick,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
K18-3001,The {C}o{NLL}{--}{SIGMORPHON} 2018 Shared Task: Universal Morphological Reinflection,2018,33,1,10,0.0232215,1281,ryan cotterell,Proceedings of the {C}o{NLL}{--}{SIGMORPHON} 2018 Shared Task: Universal Morphological Reinflection,0,"The CoNLL--SIGMORPHON 2018 shared task on supervised learning of morphological generation featured data sets from 103 typologically diverse languages. Apart from extending the number of languages involved in earlier supervised tasks of generating inflected forms, this year the shared task also featured a new second task which asked participants to inflect words in sentential context, similar to a cloze task. This second task featured seven languages. Task 1 received 27 submissions and task 2 received 6 submissions. Both tasks featured a low, medium, and high data condition. Nearly all submissions featured a neural component and built on highly-ranked systems from the earlier 2017 shared task. In the inflection task (task 1), 41 of the 52 languages present in last year's inflection task showed improvement by the best systems in the low-resource setting. The cloze task (task 2) proved to be difficult, and few submissions managed to consistently improve upon both a simple neural baseline system and a lemma-repeating baseline."
D18-1315,An Encoder-Decoder Approach to the Paradigm Cell Filling Problem,2018,0,0,1,1,1308,miikka silfverberg,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"The Paradigm Cell Filling Problem in morphology asks to complete word inflection tables from partial ones. We implement novel neural models for this task, evaluating them on 18 data sets in 8 languages, showing performance that is comparable with previous work with far less training data. We also publish a new dataset for this task and code implementing the system described in this paper."
C18-1137,A Computational Model for the Linguistic Notion of Morphological Paradigm,2018,0,1,1,1,1308,miikka silfverberg,Proceedings of the 27th International Conference on Computational Linguistics,0,"In supervised learning of morphological patterns, the strategy of generalizing inflectional tables into more abstract paradigms through alignment of the longest common subsequence found in an inflection table has been proposed as an efficient method to deduce the inflectional behavior of unseen word forms. In this paper, we extend this notion of morphological {`}paradigm{'} from earlier work and provide a formalization that more accurately matches linguist intuitions about what an inflectional paradigm is. Additionally, we propose and evaluate a mechanism for learning full human-readable paradigm specifications from incomplete data{---}a scenario when we only have access to a few inflected forms for each lexeme, and want to reconstruct the missing inflections as well as generalize and group the witnessed patterns into a model of more abstract paradigmatic behavior of lexemes."
W17-4107,Weakly supervised learning of allomorphy,2017,0,0,1,1,1308,miikka silfverberg,Proceedings of the First Workshop on Subword and Character Level Models in {NLP},0,"Most NLP resources that offer annotations at the word segment level provide morphological annotation that includes features indicating tense, aspect, modality, gender, case, and other inflectional information. Such information is rarely aligned to the relevant parts of the words{---}i.e. the allomorphs, as such annotation would be very costly. These unaligned weak labelings are commonly provided by annotated NLP corpora such as treebanks in various languages. Although they lack alignment information, the presence/absence of labels at the word level is also consistent with the amount of supervision assumed to be provided to L1 and L2 learners. In this paper, we explore several methods to learn this latent alignment between parts of word forms and the grammatical information provided. All the methods under investigation favor hypotheses regarding allomorphs of morphemes that re-use a small inventory, i.e. implicitly minimize the number of allomorphs that a morpheme can be realized as. We show that the provided information offers a significant advantage for both word segmentation and the learning of allomorphy."
W17-0418,Automatic Morpheme Segmentation and Labeling in {U}niversal {D}ependencies Resources,2017,8,1,1,1,1308,miikka silfverberg,Proceedings of the {N}o{D}a{L}i{D}a 2017 Workshop on Universal Dependencies ({UDW} 2017),0,None
K17-2010,Data Augmentation for Morphological Reinflection,2017,4,10,1,1,1308,miikka silfverberg,Proceedings of the {C}o{NLL} {SIGMORPHON} 2017 Shared Task: Universal Morphological Reinflection,0,None
W16-2406,Data-Driven Spelling Correction using Weighted Finite-State Methods,2016,21,3,1,1,1308,miikka silfverberg,Proceedings of the {SIGFSM} Workshop on Statistical {NLP} and Weighted Automata,0,"This paper presents two systems for spelling correction formulated as a sequence labeling task. One of the systems is an unstructured classifier and the other one is structured. Both systems are implemented using weighted finite-state methods. The structured system delivers stateof-the-art results on the task of tweet normalization when compared with the recent AliSeTra system introduced by Eger et al. (2016) even though the system presented in the paper is simpler than AliSeTra because it does not include a model for input segmentation. In addition to experiments on tweet normalization, we present experiments on OCR post-processing using an Early Modern Finnish corpus of OCR processed newspaper text."
W15-4806,Automated Lossless Hyper-Minimization for Morphological Analyzers,2015,9,0,2,1,28221,senka drobac,"Proceedings of the 12th International Conference on Finite-State Methods and Natural Language Processing 2015 ({FSMNLP} 2015 D{\\\u}sseldorf)""",0,"This paper presents a fully automated lossless hyper-minimization method for finitestate morphological analyzers in Xerox lexc formalism. The method utilizes flag diacritics to preserve the structure of the original lexc description in the finite-state analyzer, which results in reduced size of the analyzer. We compare our method against an earlier solution by Drobac et al. (2014) which requires manual selection of flag diacritics and results in slow lookup. We show that our method gives similar size reductions while maintaining fast lookup without requiring any manual input."
W15-1842,Extracting Semantic Frames using hfst-pmatch,2015,3,2,2,0,14359,sam hardwick,Proceedings of the 20th Nordic Conference of Computational Linguistics ({NODALIDA} 2015),0,"We use hfst-pmatch (Linden et al., 2013), a pattern-matching tool mimicking and extending Xerox fst (Karttunen, 2011), for demonstrating how to develop a semantic frame extractor. We select a FrameNet (Baker et al., 1998) frame and write shallowly syntactic pattern-matching rules based on part-of-speech information and morphology from either a morphological automaton or tagged text."
P14-2043,Part-of-Speech Tagging using Conditional Random Fields: Exploiting Sub-Label Dependencies for Improved Accuracy,2014,13,9,1,1,1308,miikka silfverberg,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We discuss part-of-speech (POS) tagging in presence of large, fine-grained label sets using conditional random fields (CRFs). We propose improving tagging accuracy by utilizing dependencies within sub-components of the fine-grained labels. These sub-label dependencies are incorporated into the CRF model via a (relatively) straightforward feature extraction scheme. Experiments on five languages show that the approach can yield significant improvement in tagging accuracy in case the labels have sufficiently rich inner structure."
drobac-etal-2014-heuristic,Heuristic Hyper-minimization of Finite State Lexicons,2014,4,5,4,1,28221,senka drobac,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Flag diacritics, which are special multi-character symbols executed at runtime, enable optimising finite-state networks by combining identical sub-graphs of its transition graph. Traditionally, the feature has required linguists to devise the optimisations to the graph by hand alongside the morphological description. In this paper, we present a novel method for discovering flag positions in morphological lexicons automatically, based on the morpheme structure implicit in the language description. With this approach, we have gained significant decrease in the size of finite-state networks while maintaining reasonable application speed. The algorithm can be applied to any language description, where the biggest achievements are expected in large and complex morphologies. The most noticeable reduction in size we got with a morphological transducer for Greenlandic, whose original size is on average about 15 times larger than other morphologies. With the presented hyper-minimization method, the transducer is reduced to 10,1{\%} of the original size, with lookup speed decreased only by 9,5{\%}."
E14-4015,Accelerated Estimation of Conditional Random Fields using a Pseudo-Likelihood-inspired Perceptron Variant,2014,22,0,2,0.689655,35500,teemu ruokolainen,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"We discuss a simple estimation approach for conditional random fields (CRFs). The approach is derived heuristically by defining a variant of the classic perceptron algorithm in spirit of pseudo-likelihood for maximum likelihood estimation. The resulting approximative algorithm has a linear time complexity in the size of the label set and contains a minimal amount of tunable hyper-parameters. Consequently, the algorithm is suitable for learning CRFbased part-of-speech (POS) taggers in presence of large POS label sets. We present experiments on five languages. Despite its heuristic nature, the algorithm provides surprisingly competetive accuracies and running times against reference methods."
W13-5618,Modeling {OOV} Words With Letter N-Grams in Statistical Taggers: Preliminary Work in Biomedical Entity Recognition,2013,17,0,2,0.689655,35500,teemu ruokolainen,Proceedings of the 19th Nordic Conference of Computational Linguistics ({NODALIDA} 2013),0,",We discuss sequential tagging problems in natural language processing using statistical methodology. We propose an automatic and domain-independent approach to modeling out-ofvocabulary (OOV) words, that is words that do not occur in training data. Our method is based on using probabilistic letter n-gram models to model orthography of different tags. We show how to combine the approach with two widely used statistical models Hidden Markov Models and Conditional Random Fields. Instead of taking the common approach of directly using sub-strings as features resulting in an explosion in the number of model parameters, we compress orthographic information into a small number of parameters. Experiments in biomedical entity recognition on the Genia corpus show that the approach can alleviate the OOV problem resulting in improvement in overall model performance."
W13-5641,Finite State Applications with Javascript,2013,12,2,2,0,1309,mans hulden,Proceedings of the 19th Nordic Conference of Computational Linguistics ({NODALIDA} 2013),0,"In this paper we present a simple and useful Javascript application programming interface for performing basic online operations with weighted and unweighted finite-state machines, such as word lookup, transductions, and least-cost-path finding. The library, jsfst, provides access to frequently used online functionality in finite-state machine-based language technology. The library is technology-agnostic in that it uses a neutral representation of finite-state machines into which most formats can be converted. We demonstrate the usefulness of the library through addressing a task that is useful in web and mobile environmentsxe2x80x94a multilingual spell checker application that also detects real-word errors."
W12-6210,Implementation of Replace Rules Using Preference Operator,2012,14,1,2,1,28221,senka drobac,Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing,0,"We explain the implementation of replace rules with the .r-glc. operator and preference relations. Our modular approach combines various preference constraints to form different replace rules. In addition to describing the method, we present illustrative examples."
W11-4625,Combining Statistical Models for {POS} Tagging using Finite-State Calculus,2011,15,8,1,1,1308,miikka silfverberg,Proceedings of the 18th Nordic Conference of Computational Linguistics ({NODALIDA} 2011),0,We introduce a framework for POS tagging which can incorporate a variety of different information sources such as statistical models and hand-written rules. The information sources are compiled into a set of weighted finite-state transducers and tagging is accomplished using weighted finite-state algorithms. Our aim is to develop a fast and flexible way for trying out different tagger designs and combining them into hybrid systems. We test the applicability of the framework by constructing HMM taggers with augmented lexical models for English and Finnish. We compare our taggers with two existing statistical taggers TnT and Hunpos and find that we achieve superior accuracy.
W10-2205,A Method for Compiling Two-Level Rules with Multiple Contexts,2010,17,3,2,0,28691,kimmo koskenniemi,Proceedings of the 11th Meeting of the {ACL} Special Interest Group on Computational Morphology and Phonology,0,"A novel method is presented for compiling two-level rules which have multiple context parts. The same method can also be applied to the resolution of so-called right-arrow rule conflicts. The method makes use of the fact that one can efficiently compose sets of two-level rules with a lexicon transducer. By introducing variant characters and using simple pre-processing of multi-context rules, all rules can be reduced into single-context rules. After the modified rules have been combined with the lexicon transducer, the variant characters may be reverted back to the original surface characters. The proposed method appears to be efficient but only partial evidence is presented yet."
W09-4625,Conflict Resolution Using Weighted Rules in {HFST}-{TWOLC},2009,5,7,1,1,1308,miikka silfverberg,Proceedings of the 17th Nordic Conference of Computational Linguistics ({NODALIDA} 2009),0,"In this article we demonstrate a novel way to resolve conicts in two-level grammars by weighting the rules. The rules are transformed into probabilistic constraints, which are allowed to compete with each other. We demonstrate a method to automatically assign weights to the rules. It acts in a similar way as traditional conict resolution, except that traditionally unresolvable left-arrow rule conicts do not cause lexical forms to be ltered out. The two-level lexicon and probabilistic twolevel grammar are combined using the new transducer operation weighted intersecting composition. The result is a weighted lexical transducer. To the best of our knowledge, this is the rst time probabilistic rules have been used to solve two-level rule conicts. The possible applications of probabilistic lexical transducers range from debugging a wed two-level grammars to computer-assisted language learning. We test our method using a twolevel lexicon and grammar compiled with the open source tools HFST-LEXC and HFST-TWOLC."
