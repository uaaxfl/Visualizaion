2005.jeptalnrecital-long.30,brewster-etal-2004-data,0,0.0842459,"Missing"
2005.jeptalnrecital-long.30,C96-1016,0,0.044871,"Missing"
2007.jeptalnrecital-poster.10,J95-4004,0,0.0481669,"Missing"
2007.jeptalnrecital-poster.10,cunningham-etal-2000-software,0,0.0423403,"Missing"
2007.jeptalnrecital-poster.10,nazarenko-etal-2006-alvis,0,0.0496818,"Missing"
2007.jeptalnrecital-poster.10,1993.iwpt-1.22,0,0.0776124,"Missing"
2007.jeptalnrecital-poster.10,2005.jeptalnrecital-court.23,0,0.0618415,"Missing"
2009.jeptalnrecital-court.31,P98-1082,1,0.804799,"Missing"
2009.jeptalnrecital-court.31,W08-0507,0,0.0226735,"Missing"
2009.jeptalnrecital-court.31,C04-1054,0,0.07182,"Missing"
2015.jeptalnrecital-long.16,W03-1802,0,0.0927467,"Missing"
2015.jeptalnrecital-long.16,W07-1007,0,0.374198,"Missing"
2015.jeptalnrecital-long.16,W14-4812,1,0.265192,"Missing"
2015.jeptalnrecital-long.16,S12-1066,0,0.0470717,"Missing"
2015.jeptalnrecital-long.16,S12-1054,0,0.0232111,"Missing"
2015.jeptalnrecital-long.16,2009.jeptalnrecital-court.7,0,0.121054,"Missing"
2015.jeptalnrecital-long.16,S12-1068,0,0.0283365,"Missing"
2015.jeptalnrecital-long.16,D12-1066,0,0.0563253,"Missing"
2015.jeptalnrecital-long.16,S12-1069,0,0.0492536,"Missing"
2016.jeptalnrecital-poster.20,boulaknadel-etal-2008-multi,0,0.045874,"Missing"
2016.jeptalnrecital-poster.20,E93-1011,0,0.603963,"Missing"
2016.jeptalnrecital-poster.20,W03-1802,0,0.0639815,"Missing"
2016.jeptalnrecital-poster.20,C00-1077,0,0.201914,"Missing"
2016.jeptalnrecital-poster.20,W14-4807,0,0.0541764,"Missing"
2018.jeptalnrecital-court.27,P02-1051,0,0.139816,"Missing"
2018.jeptalnrecital-court.27,D11-1006,0,0.03472,"ogies établies en anglais ou en français, alors considérées comme des terminologies de référence (Lelubre, 2008). Ce constat n’est toutefois pas valable pour tous les domaines. Ainsi, certains termes médicaux anglais sont issus de l’arabe (Wulff, 2004). Partant de ce constat mais aussi d’observations réalisées en corpus, nous avons choisi d’exploiter la présence de termes anglais dans les terminologies et les textes de spécialité rédigés en arabe. La méthode que nous proposons vise à exploiter la translittération des termes anglais en caractères arabes et le principe du transfert translingue (McDonald et al., 2011). À notre connaissance, il n’existe pas de systèmes de translittération de l’anglais vers l’arabe. L’application de la méthode sur un corpus parallèle anglais-arabe aligné au niveau des mots permet d’extraire des termes arabes après reconnaissance ou extraction des termes anglais. Afin d’évaluer l’apport de la translittération de termes anglais en caractères arabes sur la construction d’une terminologie bilingue à partir d’un corpus parallèle anglais-arabe de textes médicaux, nous présentons dans cet article un système de translittération qui permet d’extraire des couples de termes médicaux tr"
2018.jeptalnrecital-court.27,L16-1054,0,0.0227641,"Missing"
2018.jeptalnrecital-court.27,J03-1002,0,0.0113783,"aitements automatiques, pose des nombreux problèmes (Habash, 2010) que nous avons tenté de résoudre mais qui rendent la tâche de nettoyage très coûteuse en temps : erreur de forme des caractères, utilisation d’un caractère persan ressemblant graphiquement à un caractère arabe, etc. L’ensemble des documents du corpus est ensuite segmenté par MADAMIRA (Pasha et al., 2014) et aligné au niveau des paragraphes et au niveau des phrases. La qualité de l’alignement a été vérifiée manuellement. 3.2 Alignement au niveau des mots Un alignement au niveau des mots a été obtenu en utilisant l’outil GIZA++ (Och & Ney, 2003). Nous avons défini trois alignements entre le corpus anglais et le corpus arabe qui prennent en compte diffé1 http://www.nlm.nih.gov/medlineplus/languages/all_healthtopics.html rentes informations morphologiques fournies par MADAMIRA sur le corpus arabe : (Alignement1) l’alignement est réalisé sans traitement morphologique particulier ; il s’agit de notre base de comparaison ; (Alignement2) avant l’alignement, les enclitiques et proclitiques sont désagglutinés du mot arabe auquel ils se rapportent ; (Alignement3) avant alignement, les enclitiques, les proclitiques et les articles sont désaggl"
2018.jeptalnrecital-court.27,pasha-etal-2014-madamira,0,0.0171699,"on, nous avons ajouté 62 documents, soit 25 379 mots pour le corpus anglais et 21 950 mots pour le corpus arabe. Contrairement aux documents en anglais et en français, la conversion au format texte des documents en arabe, en vue de réaliser des traitements automatiques, pose des nombreux problèmes (Habash, 2010) que nous avons tenté de résoudre mais qui rendent la tâche de nettoyage très coûteuse en temps : erreur de forme des caractères, utilisation d’un caractère persan ressemblant graphiquement à un caractère arabe, etc. L’ensemble des documents du corpus est ensuite segmenté par MADAMIRA (Pasha et al., 2014) et aligné au niveau des paragraphes et au niveau des phrases. La qualité de l’alignement a été vérifiée manuellement. 3.2 Alignement au niveau des mots Un alignement au niveau des mots a été obtenu en utilisant l’outil GIZA++ (Och & Ney, 2003). Nous avons défini trois alignements entre le corpus anglais et le corpus arabe qui prennent en compte diffé1 http://www.nlm.nih.gov/medlineplus/languages/all_healthtopics.html rentes informations morphologiques fournies par MADAMIRA sur le corpus arabe : (Alignement1) l’alignement est réalisé sans traitement morphologique particulier ; il s’agit de not"
2018.jeptalnrecital-court.27,samy-etal-2012-medical,0,0.0236681,"Missing"
2018.jeptalnrecital-court.27,P07-1109,0,0.0453716,"Missing"
2019.jeptalnrecital-court.8,W10-2608,0,0.0755228,"Missing"
2019.jeptalnrecital-court.8,hamon-etal-2017-pomelo,1,0.849767,"Missing"
2019.jeptalnrecital-court.8,P07-1034,0,0.220338,"Missing"
2019.jeptalnrecital-court.8,S13-2056,0,0.0133475,"roche basée sur la représentation des relations à partir des instances est efficace pour identifier les correspondances des types et évaluer la cohérence entre les 2 domaines. Les nouvelles étiquettes obtenues peuvent servir d’appui pour préciser les interactions non spécifiées et pré-annoter de nouvelles données. Pour la suite du travail, nous envisageons d’utiliser d’autre corpus et méthode DDI et confronter les résultats pour une meilleure annotation les FDIs. Remerciements Ce travail est financé par l’ANR dans le cadre du projet MIAM (ANR-16-CE23-0012). Références A AGAARD L. & H ANSEN E. (2013). Adverse drug reactions reported by consumers for nervous system medications in europe 2007 to 2011. BMC Pharmacology & Toxicology, 14, 30. A RONSON J. & F ERNER R. (2005). Clarification of terminology in drug safety. Drug Safety, 28(10), 851–70. B EN A BACHA A., C HOWDHURY M. F. M., K ARANASIOU A., M RABET Y., L AVELLI A. & Z WEIGENBAUM P. (2015). Text mining for pharmacovigilance : Using machine learning for drug name recognition and drug-drug interaction extraction and classification. Journal of Biomedical Informatics, 58, 122–132. B LITZER J., M C D ONALD R. & P EREIRA F. (2006). Domain a"
2019.jeptalnrecital-court.8,E12-2021,0,0.118095,"Missing"
2019.jeptalnrecital-long.5,W18-5614,1,0.847588,"Missing"
2019.jeptalnrecital-long.5,W15-2604,1,0.853082,"Missing"
2019.jeptalnrecital-long.5,X96-1019,0,0.802383,"Missing"
2019.jeptalnrecital-long.5,P10-1052,0,0.116292,"Missing"
2019.jeptalnrecital-long.5,W08-0606,0,0.0536337,"Missing"
2019.jeptalnrecital-tia.2,W11-0216,0,0.0400453,"Missing"
2019.jeptalnrecital-tia.2,hamon-etal-2017-pomelo,1,0.883224,"Missing"
2020.jeptalnrecital-deft.1,2020.lrec-1.851,1,0.759904,"Missing"
2020.jeptalnrecital-deft.1,2020.jeptalnrecital-deft.4,0,0.0657627,"Missing"
2020.jeptalnrecital-deft.1,W18-7002,1,0.875702,"Missing"
2020.jeptalnrecital-deft.1,W18-5614,1,0.866685,"Missing"
2020.jeptalnrecital-deft.1,W19-5029,1,0.892526,"Missing"
2020.jeptalnrecital-deft.1,E12-2021,0,0.0586855,"Missing"
2020.jeptalnrecital-taln.21,jiang-etal-2014-native,0,0.0579351,"Missing"
2020.jeptalnrecital-taln.21,N15-1160,0,0.0184865,"Missing"
2020.jeptalnrecital-taln.21,C16-1198,0,0.0668921,"Missing"
2020.jeptalnrecital-taln.21,P16-1176,0,0.0460771,"Missing"
2020.jeptalnrecital-taln.32,R19-1088,0,0.0293476,"Missing"
2020.jeptalnrecital-taln.32,rebholz-schuhmann-etal-2010-calbc,0,0.0873844,"Missing"
2020.jeptalnrecital-taln.32,P17-1107,0,0.064072,"Missing"
2020.jeptalnrecital-taln.32,S13-2056,0,0.127424,"plication de plusieurs types de relation dans une instance, privilégier une relation par rapport à l’autre ou annoter les types en même temps (multi-étiquette) ou scinder l’instance en plusieurs instances de relation différentes. Dans la suite du travail, nous proposons d’appliquer ces solutions dans une phase de correction d’annotation en utilisant le Silver Standard comme pré-annotation et entraîner des modèles d’extraction automatique sur l’annotation obtenue. Remerciements Ce travail est financé par l’ANR dans le cadre du projet MIAM (ANR-16-CE23-0012). Références A AGAARD L. & H ANSEN E. (2013). Adverse drug reactions reported by consumers for nervous system medications in europe 2007 to 2011. BMC Pharmacology & Toxicology, 14, 30. A RONSON J. & F ERNER R. (2005). Clarification of terminology in drug safety. Drug Safety, 28(10), 851–70. B EN A BACHA A., C HOWDHURY M. F. M., K ARANASIOU A., M RABET Y., L AVELLI A. & Z WEIGENBAUM P. (2015). Text mining for pharmacovigilance : Using machine learning for drug name recognition and drug-drug interaction extraction and classification. Journal of Biomedical Informatics, 58, 122–132. B OJANOWSKI P., G RAVE E., J OULIN A. & M IKOLOV T. (2017)"
2020.jeptalnrecital-taln.32,E12-2021,0,0.125297,"Missing"
2020.jeptalnrecital-taln.33,W18-7002,0,0.0527161,"Missing"
2020.jeptalnrecital-taln.33,P17-4012,0,0.0298304,"Missing"
2020.jeptalnrecital-taln.33,W11-2132,0,0.085714,"Missing"
2020.jeptalnrecital-taln.33,moore-2002-fast,0,0.241101,"Missing"
2020.jeptalnrecital-taln.33,P14-1041,0,0.0591115,"Missing"
2020.jeptalnrecital-taln.33,P17-2014,0,0.0355399,"Missing"
2020.jeptalnrecital-taln.33,P02-1040,0,0.112683,"Missing"
2020.jeptalnrecital-taln.33,L18-1553,0,0.0326444,"Missing"
2020.jeptalnrecital-taln.33,P18-2113,0,0.0332244,"Missing"
2020.jeptalnrecital-taln.33,W16-2323,0,0.0779722,"Missing"
2020.jeptalnrecital-taln.33,P19-1198,0,0.0545531,"Missing"
2020.jeptalnrecital-taln.33,P12-1107,0,0.0564794,"Missing"
2020.jeptalnrecital-taln.33,Q15-1021,0,0.0343622,"Missing"
2020.jeptalnrecital-taln.33,D17-1062,0,0.0371706,"Missing"
2020.jeptalnrecital-taln.33,D18-1355,0,0.0274487,"Missing"
2020.jeptalnrecital-taln.33,C10-1152,0,0.104219,"Missing"
C98-1079,C96-1005,0,0.0153362,"-of-speech modules, the system are extended to the verbal phrases (tree cutting, tree have been cut down) (Klavans et al., 1997). Dealing with syntactic paraphrase in the general language, (Dras, 1997) propose a similar representation by using the STAG formalism to detect syntactic related sentences. Because we deal with the semantic level, our work is complementary of those. Semantic variation is rarely studied in specialized domains. Works on word similarity and word sense disambiguation are generally based on statistical methods designed for large or even very large corpora (Hindle, 1.990; Agirre and Rigau, 1996). Therefore, they cannot be applied for technical documents which usually are medium size corpora. However, dealing with already linguistic filtered data, (Assadi, 1.997) aims at statistically build rough clusters supposing that similar candidate terms have similar exp~msions. Then he relies on human expertise tbr the semantic interpretation. It differs from our work which tries to automatically explicit the semantic relations. In order to disambiguate noun objects in a short text (30 000 words), (Li et al., 1995) design heuristic rules using semantic similarity information in WordNet and verb"
C98-1079,P97-1066,0,0.0303785,"Missing"
C98-1079,C92-3150,0,0.0764949,"reover, it appeared that numerous errors are due to few mis-interpreted links: they could be eliminated by few exception rules. 1 Introduction 1.1 S t r u c t u r i n g a t e r m i n o l o g y The work presented here is a part of an industrial project of Technical Document Consultation System (Gros et al., 1996) at the French electricity company EDF. The goal is to develop tools to help a terminologist in the construction of a structured terminology (cf. figure 1) providing : • t e r m s of a domain, i.e. simple or complex lexical units pointing out accurate concepts in a technical document, (Bourigault, 1992); • semantic links such as the see-also relation. This can be viewed as a two-step process. The candidate t e r m s (i.e. lexical units which can 498 be terms if a domain expert validates them) are first automatically extracted from the technical document with a Terminology Extraction Software (LEXTER) (Bourigault, 1992). The list of candidate terms is then structured into a semantic network. We focus on the latter point by detecting semantic variants, especially synonyms. ligne afirienne (overhead line) See_also : D@art a~rien (overhead outlet) Synonym : Liaison 51ectrique afirienne (overhead"
C98-1079,C94-2119,0,0.0144369,"differ: disambiguation vs. semantic relation identification. 5 Conclusion and future works The use of a synonym dictionary and the rules of synonymous candidate terms detection we have designed allow to extract an encouraging number of links in a very technical corpus. An expert validated these links. More than one third of the detected links are synonymy relations. Beside synonymy, our method detects various kinds of semantic variants. Wrong links due to the polysemy can be easily eliminated with exception rules by comparing selectional patterns and generalized contexts (Basili et al., 1993; Grishman and Sterling, 1994). Our work shows that general semantic data are useful for the terminology structuration and the synonym detection in a corpus of specialized language. The results show that semantic variants can be automatically detected. Of course, the number of acquired links is relatively low but our method is not to be used in isolation. Acknowledgment This work is the result of a collaboration with the Direction des Etudes et Recherche (DER) d'Electricit6 de France (EDF). We thank MarieLuce Picard from EDF and Beno[t Habert from ENS Fontenay-St Cloud for their help, Didier Bourigault and Jean-Yves Hamon"
C98-1079,C96-1083,1,0.888894,"Missing"
C98-1079,P90-1034,0,0.0439407,"Missing"
F13-1005,W98-0709,0,0.031896,"Missing"
F13-1005,W02-1106,0,0.0965263,"Missing"
F13-1005,W07-0908,0,0.0600387,"Missing"
F14-1021,S13-2050,0,0.0629944,"Missing"
F14-1021,W02-1403,0,0.0380536,"Missing"
F14-1021,W14-1503,0,0.028304,"Missing"
F14-1021,J07-2002,0,0.111134,"Missing"
F14-1021,E14-1025,0,0.0614909,"Missing"
F14-1021,2003.mtsummit-papers.42,0,0.0379597,"Missing"
F14-1021,E09-3009,0,0.0747946,"Missing"
F14-1021,J05-4002,0,0.0710342,"Missing"
F14-1021,J09-3004,0,0.0239863,"Missing"
hamon-hu-2002-evaluate,C96-1083,0,\N,Missing
hamon-hu-2002-evaluate,C92-2082,0,\N,Missing
hamon-hu-2002-evaluate,A94-1006,0,\N,Missing
hamon-hu-2002-evaluate,C92-3150,0,\N,Missing
hamon-hu-2002-evaluate,P98-1082,1,\N,Missing
hamon-hu-2002-evaluate,C98-1079,1,\N,Missing
L16-1420,W14-1202,1,\N,Missing
P98-1082,C96-1005,0,0.0137346,"t-of-speech modules, the system are extended to the verbal phrases (tree cutting, tree have been cut down) (Klavans et al., 1997). Dealing with syntactic paraphrase in the general language, (Dras, 1997) propose a similar representation by using the STAG formalism to detect syntactic related sentences. Because we deal with the semantic level, our work is complementary of those. Semantic variation is rarely studied in specialized domains. Works on word similarity and word sense disambiguation are generally based on statistical methods designed for large or even very large corpora (Hindle, 1990; Agirre and Rigau, 1996). Therefore, they cannot be applied for technical documents which usually are medium size corpora. However, dealing with already linguistic filtered data, (Assadi, 1997) aims at statistically build rough clusters supposing that similar candidate terms have similar expansions. Then he relies on human expertise for the semantic interpretation. It differs from our work which tries to automatically explicit the semantic relations. In order to disambiguate noun objects in a short text (30 000 words), (Li et al., 1995) design heuristic rules using semantic similarity information in WordNet and verbs"
P98-1082,P97-1066,0,0.0652869,"t matter, giving the overall relevant relations in the document. One may think that the comparison with links manually detected by an expert is the best evaluation, but such manual detection is subjective. Regarding the validation by several experts, it is well-known that such validation would give different results depending on the background of each expert (Szpakowicz et al., 1996). So, we are reduced to compare our results with those obtained by different methods even though they are not perfect either. We are planning to compare the clusters found by our method with the clustering one of (Assadi, 1997) to study how the results overlap and are complementary. 4 Related works The variant detection in specialized corpora must be taken into account for information retrieval. This complex operation involves the semantic as well as the morphological and syntactic level. (Jacquemin, 1996) design a unification-based partial parser FASTER which analyses raw technical text while meta-rules detect morpho-syntactic variants of controlled terms (blood cell, blood mononuclear cell). By 503 using morphological and part-of-speech modules, the system are extended to the verbal phrases (tree cutting, tree hav"
P98-1082,C92-3150,0,0.0595469,"inks as synonymy. Moreover, it appeared that numerous errors are due to few mis-interpreted links: they could be eliminated by few exception rules. 1 Introduction 1.1 Structuring a terminology The work presented here is a part of an industrial project of Technical Document Consultation System (Gros et al., 1996) at the French electricity company EDF. The goal is to develop tools to help a terminologist in the construction of a structured terminology (cf. figure 1) providing : • t e r m s of a domain, i.e. simple or complex lexical units pointing out accurate concepts in a technical document, (Bourigault, 1992); • semantic links such as the see-also relation. This can be viewed as a two-step process. The c a n d i d a t e t e r m s (i.e. lexical units which can 498 be terms if a domain expert validates them) are first automatically extracted from the technical document with a Terminology Extraction Software (LEXTER) (Bourigault, 1992). The list of candidate terms is then structured into a semantic network. We focus on the latter point by detecting semantic variants, especially synonyms. ligne a~rienne (overhead line) See_also : D~part a~rien (overhead outlet) Synonym : Liaison ~lectrique a~rienne (o"
P98-1082,C94-2119,0,0.0158252,"differ: disambiguation vs. semantic relation identification. 5 Conclusion and future works The use of a synonym dictionary and the rules of synonymous candidate terms detection we have designed allow to extract an encouraging number of links in a very technical corpus. An expert validated these links. More than one third of the detected links are synonymy relations. Beside synonymy, our method detects various kinds of semantic variants. Wrong links due to the polysemy can be easily eliminated with exception rules by comparing selectional patterns and generalized contexts (Basili et al., 1993; Grishman and Sterling, 1994). Our work shows that general semantic data are useful for the terminology structuration and the synonym detection in a corpus of specialized language. The results show that semantic variants can be automatically detected. Of course, the number of acquired links is relatively low but our method is not to be used in isolation. Acknowledgment This work is the result of a collaboration with the Direction des Etudes et Recherche (DER) d'Electricit~ de France (EDF). We thank MarieLuce Picard from EDF and Benoit Habert from ENS Fontenay-St Cloud for their help, Didier Bourigault and Jean-Yves Hamon"
P98-1082,C96-1083,1,0.894909,"Missing"
P98-1082,P90-1034,0,0.0173072,"ogical and part-of-speech modules, the system are extended to the verbal phrases (tree cutting, tree have been cut down) (Klavans et al., 1997). Dealing with syntactic paraphrase in the general language, (Dras, 1997) propose a similar representation by using the STAG formalism to detect syntactic related sentences. Because we deal with the semantic level, our work is complementary of those. Semantic variation is rarely studied in specialized domains. Works on word similarity and word sense disambiguation are generally based on statistical methods designed for large or even very large corpora (Hindle, 1990; Agirre and Rigau, 1996). Therefore, they cannot be applied for technical documents which usually are medium size corpora. However, dealing with already linguistic filtered data, (Assadi, 1997) aims at statistically build rough clusters supposing that similar candidate terms have similar expansions. Then he relies on human expertise for the semantic interpretation. It differs from our work which tries to automatically explicit the semantic relations. In order to disambiguate noun objects in a short text (30 000 words), (Li et al., 1995) design heuristic rules using semantic similarity informa"
W04-1207,E99-1043,0,\N,Missing
W09-1311,W08-0507,0,0.0145304,"thesis, replication of mitochondrial DNA and mtDNA replication) convey the same or different meaning. This is particularly important for deciphering and computing semantic similarity between words and terms. 89 In our previous work, we proposed to use the existing biomedical terminologies (i.e., Gene Ontology (Gene Ontology Consortium, 2001), Snomed (Cˆot´e et al., 1997), UMLS (NLM, 2007)), wich provide complex terms, and to acquire from them lexical resources of synonyms. Indeed, the use of complex biomedical terms seems to be less suitable and generalizable as compared to lexical resources (Poprat et al., 2008). Within the biological area, we proposed to exploit the Gene Ontology (GO), and more specifically to exploit compositional structure of its terms (Hamon and Grabar, 2008). However, with the acquisition of synonymy we faced two problems: (1) contextual character of these relations (Cruse, 1986), i.e., two terms or words are considered as synonyms if they can occur within the Proceedings of the Workshop on BioNLP, pages 89–96, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics same context, which makes this relation more or less broad depending on the usage; (2) abil"
W09-1311,C04-1054,0,0.0744704,"depending on languages and domains, such resources are not equally well described. Morphological description is the most complete for both general (Burnage, 1990; Hathout et al., 2001) and biomedical (NLM, 2007; Schulz et al., 1999; Zweigenbaum et al., 2003) languages. But the situation is not as successful at the semantic level: little synonym resources can be found. If WordNet (Fellbaum, 1998) proposes general language synonym relations for English, the corresponding resources for other languages are not freely available. Moreover, the initiative for fitting WordNet to the biomedical area (Smith and Fellbaum, 2004) seems to have been abandoned, although there is a huge need for this kind of resources. Computing the semantic similarity between terms relies on existence and usage of semantic resources. However, these resources, often composed of equivalent units, or synonyms, must be first analyzed and weighted in order to define within them the reliability zones where the semantic cohesiveness is stronger. We propose an original method for acquisition of elementary synonyms based on exploitation of structured terminologies, analysis of syntactic structure of complex (multi-unit) terms and their compositi"
W09-1311,W08-0500,0,\N,Missing
W12-1103,C00-1077,0,0.11434,"Missing"
W12-2413,E09-1005,0,0.024365,"e pagerank-derived algorithm (Brin and Page, 1998). When 109 Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 109–117, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics expansion component lipid head component storage expansion component head component retention (of) lipids Figure 1: Parsing tree of the terms lipid storage and retention of lipids processing textual data, this algorithm has been previously applied in different contexts such as semantic disambiguation (Mihalcea et al., 2004; Sinha and Mihalcea, 2007; Agirre and Soroa, 2009), summarization (Fernandez et al., 2009) and, more recently, for the identification of synonyms (Sinha and Mihalcea, 2011). This last work takes into account the usage of a given word in corpora and its known synonyms from lexical resources. Other related works propose also the exploitation of the random walk algorithm for the detection of semantic relatedness of words (Gaume, 2006; Hughes and Ramage, 2007) and of documents (Hassan et al., 2007). Our work is different from the previous work in several ways: (1) the acquisition of synonymy is done on resources provided by a specialized domain;"
W12-2413,P98-1082,1,0.639561,"answers of the system will be more or less exhaustive. Several solutions may be exploited when deciphering the synonymy relations: 1. Exploitation of the existing resources in which the synonyms are already encoded. However, in the biomedical domain, such resources are not well described. If the morphological description is the most complete (NLM, 2007; Schulz et al., 1999; Zweigenbaum et al., 2003), little or no freely available synonym resources can be found, while the existing terminologies often lack the synonyms. 2. Exploitation and adaptation of the existing methods (Grefenstette, 1994; Hamon et al., 1998; Jacquemin et al., 1997; Shimizu et al., 2008; Wang and Hirst, 2011). 3. Proposition of new methods specifically adapted to the processed data. Due to the lack of resources, we propose to exploit the solutions 2 and 3. In either of these situations, the question arises about the robustness and the validity of the acquired relations. For instance, (Hamon and Grabar, 2008) face two problems: (1) contextual character of synonymy relations (Cruse, 1986), i.e., two words are considered as synonyms if they can occur within the same context, which makes this relation more or less broad depending on"
W12-2413,D07-1061,0,0.0349937,"etention of lipids processing textual data, this algorithm has been previously applied in different contexts such as semantic disambiguation (Mihalcea et al., 2004; Sinha and Mihalcea, 2007; Agirre and Soroa, 2009), summarization (Fernandez et al., 2009) and, more recently, for the identification of synonyms (Sinha and Mihalcea, 2011). This last work takes into account the usage of a given word in corpora and its known synonyms from lexical resources. Other related works propose also the exploitation of the random walk algorithm for the detection of semantic relatedness of words (Gaume, 2006; Hughes and Ramage, 2007) and of documents (Hassan et al., 2007). Our work is different from the previous work in several ways: (1) the acquisition of synonymy is done on resources provided by a specialized domain; (2) the pagerank algorithm is exploited for the filtering of semantic relations generated with linguistically-based approaches; (3) the pagerank algorithm is adapted to the small size of the processed data. In the following of this paper, we present first the material (section 2), then the method we propose (section 3). We then describe the experiments performed and the results (section 4), as well as their"
W12-2413,P97-1004,0,0.103427,"m will be more or less exhaustive. Several solutions may be exploited when deciphering the synonymy relations: 1. Exploitation of the existing resources in which the synonyms are already encoded. However, in the biomedical domain, such resources are not well described. If the morphological description is the most complete (NLM, 2007; Schulz et al., 1999; Zweigenbaum et al., 2003), little or no freely available synonym resources can be found, while the existing terminologies often lack the synonyms. 2. Exploitation and adaptation of the existing methods (Grefenstette, 1994; Hamon et al., 1998; Jacquemin et al., 1997; Shimizu et al., 2008; Wang and Hirst, 2011). 3. Proposition of new methods specifically adapted to the processed data. Due to the lack of resources, we propose to exploit the solutions 2 and 3. In either of these situations, the question arises about the robustness and the validity of the acquired relations. For instance, (Hamon and Grabar, 2008) face two problems: (1) contextual character of synonymy relations (Cruse, 1986), i.e., two words are considered as synonyms if they can occur within the same context, which makes this relation more or less broad depending on the usage; (2) ability o"
W12-2413,C04-1162,0,0.0249817,"eight and to filter the synonym relations with the pagerank-derived algorithm (Brin and Page, 1998). When 109 Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 109–117, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics expansion component lipid head component storage expansion component head component retention (of) lipids Figure 1: Parsing tree of the terms lipid storage and retention of lipids processing textual data, this algorithm has been previously applied in different contexts such as semantic disambiguation (Mihalcea et al., 2004; Sinha and Mihalcea, 2007; Agirre and Soroa, 2009), summarization (Fernandez et al., 2009) and, more recently, for the identification of synonyms (Sinha and Mihalcea, 2011). This last work takes into account the usage of a given word in corpora and its known synonyms from lexical resources. Other related works propose also the exploitation of the random walk algorithm for the detection of semantic relatedness of words (Gaume, 2006; Hughes and Ramage, 2007) and of documents (Hassan et al., 2007). Our work is different from the previous work in several ways: (1) the acquisition of synonymy is d"
W12-2413,C08-1100,0,0.0219499,"xhaustive. Several solutions may be exploited when deciphering the synonymy relations: 1. Exploitation of the existing resources in which the synonyms are already encoded. However, in the biomedical domain, such resources are not well described. If the morphological description is the most complete (NLM, 2007; Schulz et al., 1999; Zweigenbaum et al., 2003), little or no freely available synonym resources can be found, while the existing terminologies often lack the synonyms. 2. Exploitation and adaptation of the existing methods (Grefenstette, 1994; Hamon et al., 1998; Jacquemin et al., 1997; Shimizu et al., 2008; Wang and Hirst, 2011). 3. Proposition of new methods specifically adapted to the processed data. Due to the lack of resources, we propose to exploit the solutions 2 and 3. In either of these situations, the question arises about the robustness and the validity of the acquired relations. For instance, (Hamon and Grabar, 2008) face two problems: (1) contextual character of synonymy relations (Cruse, 1986), i.e., two words are considered as synonyms if they can occur within the same context, which makes this relation more or less broad depending on the usage; (2) ability of automatic tools to d"
W12-2413,C98-1079,1,\N,Missing
W12-2413,W06-3809,0,\N,Missing
W13-2013,D10-1096,0,0.0159182,"This pre-processing was identical to that applied in the BioNLP 2011 Shared Task, and included sentence splitting of the annotated texts using the Genia Sentence Splitter,4 the application of a set of postprocessing heuristics to correct frequently occurring sentence splitting errors, and Genia Treebanklike tokenisation (Tateisi et al., 2004) using a tokenisation script created by the shared task organisers. 5 Since several studies have indicated that representations of syntax and aspects of syntactic dependency formalism differ in their applicability to support information extraction tasks (Buyko and Hahn, 2010; Miwa et al., 2010; Quirk et al., 2011), we further converted the output of each of the parsers from the PTB representation into three other representations: CoNNL-X, Stanford Dependencies and Stanford Collapsed Dependencies. For the CoNLL-X format we employed the conversion tool of Johansson and Nugues (2007), and for the two Stanford Dependency variants we used the converter provided with the Stanford CoreNLP tools (de Marneffe et al., 2006). These analyses were provided to participants in the output formats created by the respective tools, i.e. the TABseparated column-oriented format CoNLL"
W13-2013,P05-1022,0,0.00694797,"deep parser based on the Head-Driven Phrase Structure Grammar (HPSG) formalism. Enju analyses its input in terms of phrase structure trees with predicate-argument structure links, represented in a specialised XML-format. To make the analyses of the parser more accessible to participants, we converted its output into the Penn Treebank (PTB) format using tools included with the parser. The use of the PTB format also allow for its output to be exchanged freely for that of the other two syntactic parsers and facilitates further conversions into dependency representations. McCCJ The BLLIP Parser (Charniak and Johnson, 2005), also variously known as the Charniak parser, the Charniak-Johnson parser, or the Brown reranking parser, has been applied in numerous biomedical domain NLP efforts, frequently using the self-trained biomedical model of McClosky (2010) (i.e. the McClosky-Charniak-Johnson or McCCJ parser). The BLLIP Parser is a constituency (phrase structure) parser and the applied model produces PTB analyses as its native output. These analyses were made available to participants without modification. 5 Pre-processing and Conversions Results and Discussion Just like in previous years the supporting resources"
W13-2013,W07-2416,0,0.0161951,"like tokenisation (Tateisi et al., 2004) using a tokenisation script created by the shared task organisers. 5 Since several studies have indicated that representations of syntax and aspects of syntactic dependency formalism differ in their applicability to support information extraction tasks (Buyko and Hahn, 2010; Miwa et al., 2010; Quirk et al., 2011), we further converted the output of each of the parsers from the PTB representation into three other representations: CoNNL-X, Stanford Dependencies and Stanford Collapsed Dependencies. For the CoNLL-X format we employed the conversion tool of Johansson and Nugues (2007), and for the two Stanford Dependency variants we used the converter provided with the Stanford CoreNLP tools (de Marneffe et al., 2006). These analyses were provided to participants in the output formats created by the respective tools, i.e. the TABseparated column-oriented format CoNLL and the custom text-based format of the Stanford Dependencies. Table 2: Parsers used for the syntactic analyses. and format conversions applied to their output. The applied parsers are listed in Table 2. 4.1 Syntactic Parsers Enju Enju (Miyao and Tsujii, 2008) is a deep parser based on the Head-Driven Phrase S"
W13-2013,W09-1401,0,0.12787,"valuation can improve understanding of the applicability and benefits of specific tools and representations. The supporting resources described in this paper will continue to be publicly available from the shared task homepage http://2013.bionlp-st.org/ 1 Introduction The BioNLP Shared Task (ST), first organised in 2009, is an ongoing series of events focusing on novel challenges in biomedical domain information extraction. In the first BioNLP ST, the organisers provided the participants with automatically generated syntactic analyses from a variety of Natural Language Processing (NLP) tools (Kim et al., 2009) and similar syntactic analyses have since then been a key component of the best performing systems participating in the shared tasks. This initial work was followed up by a similar effort in the second event in the series (Kim et al., 2011), extended by the inclusion of software tools and contributions from the broader BioNLP com2 Organisation Following the practice established in the BioNLP ST 2011, the organisers issued an open call for supporting resources, welcoming contributions relevant to the task from all authors of NLP tools. In the call it was mentioned that points such as availabil"
W13-2013,W11-1802,0,0.0137379,"onlp-st.org/ 1 Introduction The BioNLP Shared Task (ST), first organised in 2009, is an ongoing series of events focusing on novel challenges in biomedical domain information extraction. In the first BioNLP ST, the organisers provided the participants with automatically generated syntactic analyses from a variety of Natural Language Processing (NLP) tools (Kim et al., 2009) and similar syntactic analyses have since then been a key component of the best performing systems participating in the shared tasks. This initial work was followed up by a similar effort in the second event in the series (Kim et al., 2011), extended by the inclusion of software tools and contributions from the broader BioNLP com2 Organisation Following the practice established in the BioNLP ST 2011, the organisers issued an open call for supporting resources, welcoming contributions relevant to the task from all authors of NLP tools. In the call it was mentioned that points such as availability for research purposes, support for well-established formats and access 99 Proceedings of the BioNLP Shared Task 2013 Workshop, pages 99–103, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics Name Annotation"
W13-2013,W12-3806,1,0.841552,"their methods (Emadzadeh et al., 2011; McClosky et al., 2011; McGrath et al., 2011; Nguyen and Tsuruoka, 2011; Bj¨orne et al., 2012; Vlachos and Craven, 2012). These resources have been available also after the original tasks, and several subsequent studies have also built on the resources. Van Landeghem et al. (2012) applied a visualisation tool that was made available as a part of the supporting resources, Vlachos (2012) employed the syntactic parses in a follow-up study on event extraction, Van Landeghem et al. (2013) used the parsing pipeline created to produce the syntactic analyses, and Stenetorp et al. (2012) presented a study of the compatibility of two different representations for negation and speculation annotation included in the data. These research contributions and the overall positive reception of the supporting resources prompted us to continue to provide supporting resources for the BioNLP Shared Task 2013. This paper presents the details of this technical contribution. This paper describes the technical contribution of the supporting resources provided for the BioNLP Shared Task 2013. Following the tradition of the previous two BioNLP Shared Task events, the task organisers and several"
W13-2013,W11-1806,0,0.0125311,"or Biotechnology Information, National Library of Medicine, National Institutes of Health, Bethesda, MD, USA pontus@nii.ac.jp wiktoria.golik@jouy.inra.fr thierry.hamon@univ-paris13.fr {comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov Abstract munity in addition to task organisers (Stenetorp et al., 2011). Although no formal study was carried out to estimate the extent to which the participants utilised the supporting resources in these previous events, we note that six participating groups mention using the supporting resources in published descriptions of their methods (Emadzadeh et al., 2011; McClosky et al., 2011; McGrath et al., 2011; Nguyen and Tsuruoka, 2011; Bj¨orne et al., 2012; Vlachos and Craven, 2012). These resources have been available also after the original tasks, and several subsequent studies have also built on the resources. Van Landeghem et al. (2012) applied a visualisation tool that was made available as a part of the supporting resources, Vlachos (2012) employed the syntactic parses in a follow-up study on event extraction, Van Landeghem et al. (2013) used the parsing pipeline created to produce the syntactic analyses, and Stenetorp et al. (2012) presented a study of the compatibili"
W13-2013,N10-1004,0,0.0373258,"e parser more accessible to participants, we converted its output into the Penn Treebank (PTB) format using tools included with the parser. The use of the PTB format also allow for its output to be exchanged freely for that of the other two syntactic parsers and facilitates further conversions into dependency representations. McCCJ The BLLIP Parser (Charniak and Johnson, 2005), also variously known as the Charniak parser, the Charniak-Johnson parser, or the Brown reranking parser, has been applied in numerous biomedical domain NLP efforts, frequently using the self-trained biomedical model of McClosky (2010) (i.e. the McClosky-Charniak-Johnson or McCCJ parser). The BLLIP Parser is a constituency (phrase structure) parser and the applied model produces PTB analyses as its native output. These analyses were made available to participants without modification. 5 Pre-processing and Conversions Results and Discussion Just like in previous years the supporting resources were well-received by the shared task participants and as many as five participating teams mentioned utilising the supporting resources in their initial submissions (at the time of writing, the cameraready versions were not yet availabl"
W13-2013,W11-1818,0,0.0123952,"ation, National Library of Medicine, National Institutes of Health, Bethesda, MD, USA pontus@nii.ac.jp wiktoria.golik@jouy.inra.fr thierry.hamon@univ-paris13.fr {comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov Abstract munity in addition to task organisers (Stenetorp et al., 2011). Although no formal study was carried out to estimate the extent to which the participants utilised the supporting resources in these previous events, we note that six participating groups mention using the supporting resources in published descriptions of their methods (Emadzadeh et al., 2011; McClosky et al., 2011; McGrath et al., 2011; Nguyen and Tsuruoka, 2011; Bj¨orne et al., 2012; Vlachos and Craven, 2012). These resources have been available also after the original tasks, and several subsequent studies have also built on the resources. Van Landeghem et al. (2012) applied a visualisation tool that was made available as a part of the supporting resources, Vlachos (2012) employed the syntactic parses in a follow-up study on event extraction, Van Landeghem et al. (2013) used the parsing pipeline created to produce the syntactic analyses, and Stenetorp et al. (2012) presented a study of the compatibility of two different re"
W13-2013,C10-1088,0,0.0168273,"as identical to that applied in the BioNLP 2011 Shared Task, and included sentence splitting of the annotated texts using the Genia Sentence Splitter,4 the application of a set of postprocessing heuristics to correct frequently occurring sentence splitting errors, and Genia Treebanklike tokenisation (Tateisi et al., 2004) using a tokenisation script created by the shared task organisers. 5 Since several studies have indicated that representations of syntax and aspects of syntactic dependency formalism differ in their applicability to support information extraction tasks (Buyko and Hahn, 2010; Miwa et al., 2010; Quirk et al., 2011), we further converted the output of each of the parsers from the PTB representation into three other representations: CoNNL-X, Stanford Dependencies and Stanford Collapsed Dependencies. For the CoNLL-X format we employed the conversion tool of Johansson and Nugues (2007), and for the two Stanford Dependency variants we used the converter provided with the Stanford CoreNLP tools (de Marneffe et al., 2006). These analyses were provided to participants in the output formats created by the respective tools, i.e. the TABseparated column-oriented format CoNLL and the custom tex"
W13-2013,J08-1002,0,0.0281123,"LL-X format we employed the conversion tool of Johansson and Nugues (2007), and for the two Stanford Dependency variants we used the converter provided with the Stanford CoreNLP tools (de Marneffe et al., 2006). These analyses were provided to participants in the output formats created by the respective tools, i.e. the TABseparated column-oriented format CoNLL and the custom text-based format of the Stanford Dependencies. Table 2: Parsers used for the syntactic analyses. and format conversions applied to their output. The applied parsers are listed in Table 2. 4.1 Syntactic Parsers Enju Enju (Miyao and Tsujii, 2008) is a deep parser based on the Head-Driven Phrase Structure Grammar (HPSG) formalism. Enju analyses its input in terms of phrase structure trees with predicate-argument structure links, represented in a specialised XML-format. To make the analyses of the parser more accessible to participants, we converted its output into the Penn Treebank (PTB) format using tools included with the parser. The use of the PTB format also allow for its output to be exchanged freely for that of the other two syntactic parsers and facilitates further conversions into dependency representations. McCCJ The BLLIP Par"
W13-2013,W11-1814,0,0.0254302,"y of Medicine, National Institutes of Health, Bethesda, MD, USA pontus@nii.ac.jp wiktoria.golik@jouy.inra.fr thierry.hamon@univ-paris13.fr {comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov Abstract munity in addition to task organisers (Stenetorp et al., 2011). Although no formal study was carried out to estimate the extent to which the participants utilised the supporting resources in these previous events, we note that six participating groups mention using the supporting resources in published descriptions of their methods (Emadzadeh et al., 2011; McClosky et al., 2011; McGrath et al., 2011; Nguyen and Tsuruoka, 2011; Bj¨orne et al., 2012; Vlachos and Craven, 2012). These resources have been available also after the original tasks, and several subsequent studies have also built on the resources. Van Landeghem et al. (2012) applied a visualisation tool that was made available as a part of the supporting resources, Vlachos (2012) employed the syntactic parses in a follow-up study on event extraction, Van Landeghem et al. (2013) used the parsing pipeline created to produce the syntactic analyses, and Stenetorp et al. (2012) presented a study of the compatibility of two different representations for negation"
W13-2013,W11-1825,0,0.0141209,"t applied in the BioNLP 2011 Shared Task, and included sentence splitting of the annotated texts using the Genia Sentence Splitter,4 the application of a set of postprocessing heuristics to correct frequently occurring sentence splitting errors, and Genia Treebanklike tokenisation (Tateisi et al., 2004) using a tokenisation script created by the shared task organisers. 5 Since several studies have indicated that representations of syntax and aspects of syntactic dependency formalism differ in their applicability to support information extraction tasks (Buyko and Hahn, 2010; Miwa et al., 2010; Quirk et al., 2011), we further converted the output of each of the parsers from the PTB representation into three other representations: CoNNL-X, Stanford Dependencies and Stanford Collapsed Dependencies. For the CoNLL-X format we employed the conversion tool of Johansson and Nugues (2007), and for the two Stanford Dependency variants we used the converter provided with the Stanford CoreNLP tools (de Marneffe et al., 2006). These analyses were provided to participants in the output formats created by the respective tools, i.e. the TABseparated column-oriented format CoNLL and the custom text-based format of the"
W13-2013,W11-1816,1,0.851328,"Missing"
W13-2013,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2013,W11-1824,0,\N,Missing
W13-2013,W11-1805,0,\N,Missing
W14-1114,J07-2002,0,0.0355019,"Missing"
W14-1114,S13-2050,0,0.0302055,"ss is a major issue and methods need to be adapted. Semantic grouping of contexts should decrease their diversity, and thus increase the frequency of 2 Related work Our approach relates with works that influence distributional contexts to improve the performance of VSMs. Some of them intend to change the way to consider contexts; Broda et al. (2009) do not use the raw context frequency in DA, but they first rank contexts according to their frequency, and take the rank into account. Other models use statistical language models to determine the most likely substitutes to represent the contexts (Baskaya et al., 2013). They assign probabilities to arbitrary sequences of words that are then used to create word pairs to feed a co-occurrence model, before performing a clustered algorithm (Yuret, 2012). The limit of such methods is that their performance is proportional to vocabulary size and requires the availability of training data. Influence on contexts may also be performed by embedding additional semantic information. The semantic relations may be issued from an existing resource or automatically computed. With a method based on bootstrapping, ZhitomirskyGeffet and Dagan (2009) modify the weights of 90 P"
W14-1114,2003.mtsummit-papers.42,0,0.0232926,"the resource by the total semantic neighbors acquired by our method. We consider three sets of neighbors: precision after examining 1 (P@1), 5 (P@5) and 10 (P@10) neighbors. Distributional parameters We consider two window sizes: a large window of 21 words (± 10 words, centered on the target, henceforth W21) and a narrow one of 5 words (± 2 words, centered on the target, W5). The window size influences on the type, the volume and the quality of the acquired relations. Generally, the smaller windows allow to acquire more relevant contexts for a target, but increase the data sparseness problem (Rapp, 2003). They give better results for classical types of relations (eg. synonymy), whereas larger windows are more appropriate for domain relations (eg. collocations)(Sahlgren, 2006; Peirsman et al., 2008). 4.3 Evaluation 5 Results and discussion Best results are obtained with a large window of 21 words, with a precision P@1 of 0.243 against 0.032 for a 5 word window, both for VSMonly, with a threshold of 0.001. Thus, a high threshold on the similarity score is not always relevant. We observe on this corpus that the generalization with the several linguistic approaches does not improve the results. F"
W14-1114,F13-1004,0,0.0368088,"ns and uses the syntactic analysis of the terms. Based on the hypothesis that if a term is lexically included in another, generally there is a hypernymy relation between the two terms (kidney transplant - cadaveric kidney transplant) (Bodenreider et al., 2001). Terminological Variation (TV) acquires both hypernyms and synonyms. TV uses rules that define a morpho-syntactic transformation, mainly the insertion (blood transfusion blood cell transfusion (Jacquemin, 1996). the elements in contexts relying on the semantic neighbors found with a distributional similarity measure. Based on this work, Ferret (2013) uses a set of examples selected from an original distributional thesaurus to train a supervised classifier. This classifier is then applied for reranking the neighbors of the thesaurus selection. Within Vector Space Model, Tsatsaronis and Panagiotopoulou (2009) use a word thesaurus to interpret the orthogonality of terms and measure semantic relatedness. With the same purpose of solving the problem of data sparseness, other methods are based on dimensionality reduction, such as Latent Semantic Analysis (LSA) in (Pad´o and Lapata, 2007) or Non-negative Matrix Factorization (NMF) (Zheng et al.,"
W14-1114,J98-1004,0,0.536592,"Missing"
W14-1114,E09-3009,0,0.057319,"Missing"
W14-1114,C92-2082,0,0.481801,"Missing"
W14-1114,J05-4002,0,0.0138812,"eir distance in the vector space, or the cosine of the angle between them (Mitchell and Lapata, 2010). On the other hand, a major inconvenience is data sparseness within the matrix that represents the vector space (Turney and Pantel, 2010). The data sparseness problem is the consequence of the word distribution in a corpus (Baroni et al., 2009): in any corpus, most of the words have a very low frequency and appear only a few times. Thus, those words have a limited set of contexts and similarity is difficult to catch. Thus, methods based on DA perform better when more information is available (Weeds and Weir, 2005; van der Plas, 2008) and are efficient with large corpora of general language. But with specialized texts, as EHR texts that are usually of smaller size, reducing data sparseness is a major issue and methods need to be adapted. Semantic grouping of contexts should decrease their diversity, and thus increase the frequency of 2 Related work Our approach relates with works that influence distributional contexts to improve the performance of VSMs. Some of them intend to change the way to consider contexts; Broda et al. (2009) do not use the raw context frequency in DA, but they first rank context"
W14-1114,J09-3004,0,0.0266013,"Missing"
W14-1116,pustejovsky-etal-2010-iso,0,0.209033,"lenge, 2004), SemEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), I2B2 2012 (Sun et al., 2013). We propose to continue working on the extraction of temporal information related to medical events. This kind of study relies on several important tasks when processing the narrative documents : identification and normalization of linguistic expressions that are indicative of the temporality (Verhagen et al., 2007; Chang and Manning, 2012; Str¨otgen and Gertz, 2012; Kessler et al., 2012), and their modelization and chaining (Batal et al., 2009; Moskovitch and Shahar, 2009; Pustejovsky et al., 2010; Sun et al., 2013; Grouin et al., 2013). The identification of temporal expressions provides basic knowledge for other tasks processing the temporality information. The existing available automatic systems such as HeidelTime (Str¨otgen and Gertz, 2012) or SUTIME (Chang and Manning, 2012) exploit rule-based approaches, which makes them adaptable to new data and areas. During a preliminary study, we tested several such systems for identification of temporal relations and found that HeidelTime has the best combination of performance and adaptability. We propose to exploit this automatic systems,"
W14-1116,chang-manning-2012-sutime,0,0.0989398,"the reference data for evaluation. Temporality has become an important research field in the NLP topics and several challenges addressed this taks: ACE (ACE challenge, 2004), SemEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), I2B2 2012 (Sun et al., 2013). We propose to continue working on the extraction of temporal information related to medical events. This kind of study relies on several important tasks when processing the narrative documents : identification and normalization of linguistic expressions that are indicative of the temporality (Verhagen et al., 2007; Chang and Manning, 2012; Str¨otgen and Gertz, 2012; Kessler et al., 2012), and their modelization and chaining (Batal et al., 2009; Moskovitch and Shahar, 2009; Pustejovsky et al., 2010; Sun et al., 2013; Grouin et al., 2013). The identification of temporal expressions provides basic knowledge for other tasks processing the temporality information. The existing available automatic systems such as HeidelTime (Str¨otgen and Gertz, 2012) or SUTIME (Chang and Manning, 2012) exploit rule-based approaches, which makes them adaptable to new data and areas. During a preliminary study, we tested several such systems for iden"
W14-1116,strotgen-gertz-2012-temporal,0,0.392422,"Missing"
W14-1116,S13-2001,0,0.0470794,"xtraction tasks related to different kinds of contextual information. 1 https://www.i2b2.org/NLP 101 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 101–105, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics documents from the test set are annotated to provide the reference data for evaluation. Temporality has become an important research field in the NLP topics and several challenges addressed this taks: ACE (ACE challenge, 2004), SemEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), I2B2 2012 (Sun et al., 2013). We propose to continue working on the extraction of temporal information related to medical events. This kind of study relies on several important tasks when processing the narrative documents : identification and normalization of linguistic expressions that are indicative of the temporality (Verhagen et al., 2007; Chang and Manning, 2012; Str¨otgen and Gertz, 2012; Kessler et al., 2012), and their modelization and chaining (Batal et al., 2009; Moskovitch and Shahar, 2009; Pustejovsky et al., 2010; Sun et al., 2013; Grouin et al., 2013). The identification of te"
W14-1116,S07-1014,0,0.0144859,"ous I2B2 contests1 addressed the information extraction tasks related to different kinds of contextual information. 1 https://www.i2b2.org/NLP 101 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 101–105, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics documents from the test set are annotated to provide the reference data for evaluation. Temporality has become an important research field in the NLP topics and several challenges addressed this taks: ACE (ACE challenge, 2004), SemEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), I2B2 2012 (Sun et al., 2013). We propose to continue working on the extraction of temporal information related to medical events. This kind of study relies on several important tasks when processing the narrative documents : identification and normalization of linguistic expressions that are indicative of the temporality (Verhagen et al., 2007; Chang and Manning, 2012; Str¨otgen and Gertz, 2012; Kessler et al., 2012), and their modelization and chaining (Batal et al., 2009; Moskovitch and Shahar, 2009; Pustejovsky et al., 2010; Sun et al., 2013;"
W14-1116,P12-1077,0,0.0147441,"ecome an important research field in the NLP topics and several challenges addressed this taks: ACE (ACE challenge, 2004), SemEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), I2B2 2012 (Sun et al., 2013). We propose to continue working on the extraction of temporal information related to medical events. This kind of study relies on several important tasks when processing the narrative documents : identification and normalization of linguistic expressions that are indicative of the temporality (Verhagen et al., 2007; Chang and Manning, 2012; Str¨otgen and Gertz, 2012; Kessler et al., 2012), and their modelization and chaining (Batal et al., 2009; Moskovitch and Shahar, 2009; Pustejovsky et al., 2010; Sun et al., 2013; Grouin et al., 2013). The identification of temporal expressions provides basic knowledge for other tasks processing the temporality information. The existing available automatic systems such as HeidelTime (Str¨otgen and Gertz, 2012) or SUTIME (Chang and Manning, 2012) exploit rule-based approaches, which makes them adaptable to new data and areas. During a preliminary study, we tested several such systems for identification of temporal relations and found that He"
W14-1116,W04-3113,0,0.0286453,"Missing"
W14-1116,moriceau-tannier-2014-french,0,0.0127311,"2 3 Method HeidelTime is a cross-domain temporal tagger that extracts temporal expressions from documents and normalizes them according to the Timex3 annotation standard, which is part of the markup language TimeML (Pustejovsky et al., 2010). This is a rule-based system. Because the source code and the resources (patterns, normalization information, and rules) are strictly separated, it is possible to develop and implement resources for additional languages and areas using HeidelTime’s rule syntax. HeidelTime is provided with modules for processing documents in several languages, e.g. French (Moriceau and Tannier, 2014). In English, several versions of the system exist, such as general-language English and scientific English. HeidelTime uses different normalization strategies depending on the domain of the documents that are to be processed: news, narratives (e.g. Wikipedia articles), colloquial (e.g. SMS, tweets), and scientific (e.g. biomedical studies). The news strategy allows to fix the document creation date. This date is important for computing and normalizing the relative dates, such as two weeks ago or 5 days later, for which the reference point in time is necessary: if the document creation date is"
W14-1116,S10-1010,0,\N,Missing
W14-1202,S12-1067,0,0.0138193,"pecia et al., 2012). The participants applied rule-based and/or machine learning systems. Combinations of various features have been used: lexicon from spoken corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, latent semantic analysis, mutual information and word frequency (Jauhar and Specia, 2012); Wikipedia frequency, word length, n-grams of characters and of words, random indexing and syntactic complexity of documents (Johannsen et al., 2012); n-grams and frequency from Wikipedia, Google n-grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). 3 orders and abnormalities, procedures, chemical products, living organisms, anatomy, social status, etc. We keep here five axes related to the main medical notions (disorders, abnormalities, procedures, functions, anatomy). The objective is not to consider axes such as chemical products (trisulfure d’hydrog`ene (hydrogen sulfide)) and living organisms (Sapromyces, Acholeplasma laidlawii) that group very specific terms hardly known by laymen. The 104,649 selected terms are tokenized and segmented into words (or tokens) to obtain 29,641 unique words: trisulfure d’hydrog`ene gives three words"
W14-1202,S12-1066,0,0.0126059,"to the lexical simplification within the SemEval challenge in 20121 . Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how ”simple” they are (Specia et al., 2012). The participants applied rule-based and/or machine learning systems. Combinations of various features have been used: lexicon from spoken corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, latent semantic analysis, mutual information and word frequency (Jauhar and Specia, 2012); Wikipedia frequency, word length, n-grams of characters and of words, random indexing and syntactic complexity of documents (Johannsen et al., 2012); n-grams and frequency from Wikipedia, Google n-grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). 3 orders and abnormalities, procedures, chemical products, living organisms, anatomy, social status, etc. We keep here five axes related to the main medical notions (disorders, abnormalities, procedures, functions, anatomy). The objective is not to consider axes such as chemical products (trisulfure d’hydrog`ene ("
W14-1202,S12-1054,0,0.0251193,"Missing"
W14-1202,W07-1007,0,0.0353449,"t. Such studies can be used for filtering the terms extracted from specialized corpora (Korkontzelos et al., 2008). The features exploited include for instance the presence and the specificity of pivot words (Drouin and Langlais, 2006), the neighborhood of the term in corpus or the diversity of its components computed with statistical measures such as C-Value or PageRank (Daille, 1995; Frantzi et al., 1997; Maynard and Ananiadou, 2000). Another possibility is to check whether lexical units occur within reference terminologies and, if they do, they are considered to convey specialized meaning (Elhadad and Sutaria, 2007). 2.3 NLP studies The application of the readability measures is another way to evaluate the complexity of words and terms. Among these measures, it is possible to distinguish classical readability measures and computational readability measures (Franc¸ois, 2011). Classical measures usually rely on number of letters and/or of syllables a word contains and on linear regression models (Flesch, 1948; Gunning, 1973), while computational readability measures may involve vector models and a great variability of features, among which the following have been used to process the biomedical documents an"
W14-1202,S12-1069,0,0.0133027,"different features (Wang, 2006; ZengTreiler et al., 2007; Leroy et al., 2008). Specific task has been dedicated to the lexical simplification within the SemEval challenge in 20121 . Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how ”simple” they are (Specia et al., 2012). The participants applied rule-based and/or machine learning systems. Combinations of various features have been used: lexicon from spoken corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, latent semantic analysis, mutual information and word frequency (Jauhar and Specia, 2012); Wikipedia frequency, word length, n-grams of characters and of words, random indexing and syntactic complexity of documents (Johannsen et al., 2012); n-grams and frequency from Wikipedia, Google n-grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). 3 orders and abnormalities, procedures, chemical products, living organisms, anatomy, social status, etc. We keep here five axes related to the main medical notions (disorders, abnormalities"
W14-1202,S12-1068,0,0.0133474,"e substitutes according to how ”simple” they are (Specia et al., 2012). The participants applied rule-based and/or machine learning systems. Combinations of various features have been used: lexicon from spoken corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, latent semantic analysis, mutual information and word frequency (Jauhar and Specia, 2012); Wikipedia frequency, word length, n-grams of characters and of words, random indexing and syntactic complexity of documents (Johannsen et al., 2012); n-grams and frequency from Wikipedia, Google n-grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). 3 orders and abnormalities, procedures, chemical products, living organisms, anatomy, social status, etc. We keep here five axes related to the main medical notions (disorders, abnormalities, procedures, functions, anatomy). The objective is not to consider axes such as chemical products (trisulfure d’hydrog`ene (hydrogen sulfide)) and living organisms (Sapromyces, Acholeplasma laidlawii) that group very specific terms hardly known by laymen. The 104,649 selected terms are tokenized and segmented into words (or tokens) to obtain 29,641"
W14-1202,C00-1077,0,0.0836542,"l in terminology (W¨uster, 1981; Cabr´e and Estop`a, 2002; Cabr´e, 2000). The specificity of terms to a given field is usually studied. The notion of understandability can be derived from it. Such studies can be used for filtering the terms extracted from specialized corpora (Korkontzelos et al., 2008). The features exploited include for instance the presence and the specificity of pivot words (Drouin and Langlais, 2006), the neighborhood of the term in corpus or the diversity of its components computed with statistical measures such as C-Value or PageRank (Daille, 1995; Frantzi et al., 1997; Maynard and Ananiadou, 2000). Another possibility is to check whether lexical units occur within reference terminologies and, if they do, they are considered to convey specialized meaning (Elhadad and Sutaria, 2007). 2.3 NLP studies The application of the readability measures is another way to evaluate the complexity of words and terms. Among these measures, it is possible to distinguish classical readability measures and computational readability measures (Franc¸ois, 2011). Classical measures usually rely on number of letters and/or of syllables a word contains and on linear regression models (Flesch, 1948; Gunning, 197"
W14-1202,S12-1046,0,\N,Missing
W14-4801,S13-2050,0,0.0171702,"e evaluated and analysed with precision, R-precision and MAP. 2 Related work Reducing data sparsity is a main issue in distributional analysis. The existing methods aim at influencing the selection of useful contexts or at integrating semantic information to modify context distribution. Thus, contrary to the common usage, Broda et al. (2009) propose to weight contexts by first ranking contexts according to their frequency, and then take the rank into account to weight contexts. Other approaches rely on statistical language models to determine the most likely substitutes to represent contexts (Baskaya et al., 2013). These models assign probabilities to arbitrary sequences of words based on their co-occurrence frequencies in a training corpora (Yuret, 2012). These substitutes and their probabilities are then used to create word pairs to feed a co-occurrence model and to cluster the word list. The limit of such methods is their performance which depends on vocabulary size and requires an increasing amount of training data. Influence on contexts may also be done by incorporating additional semantic information: it has been shown that such information used to modify the standard distributional method can im"
W14-4801,W02-0908,0,0.0573006,"esent the number of relations acquired (Acq. Rel), the number of relations found in the UMLS resource (Rel. UMLS), and the results in terms of MAP, R-precision and precision to 1 (P@1). Before discussing our results, we briefly present some results of similar work. Results in existing similar work In order to understand better our results, we first provide some results obtained in similar work. Keep in mind that these results are not obtained with the same corpus. Indeed, a major problem is that the comparisons with reference resources are often given for large copora and very frequent words (Curran and Moens, 2002), or for different tasks than our task. An effective comparison 1 http://www.nlm.nih.gov/research/umls/ 6 is then difficult. We first present results obtained on a similar task, but with general copora, and then results obtained on similar documents (i.e. specialised medical texts) but in a different tasks. Despite this limit, we can still quote for comparison the values obtained by Ferret (2011) for the evaluation of semantic neighbours extraction on English general copora (of 380 million words). The parameters of his VSM are a small sliding window of 3 words (± 1 word centered on the target)"
W14-4801,F13-1004,0,0.0231379,"amount of training data. Influence on contexts may also be done by incorporating additional semantic information: it has been shown that such information used to modify the standard distributional method can improve its performance (Tsatsaronis and Panagiotopoulou, 2009). This semantic information, in particular semantic relations, may be automatically computed or issued from an existing resource. Thus, with a bootstrap method, Zhitomirsky-Geffet and Dagan (2009) modify the context weights with the semantic neighbours proposed by a distributional similarity measure. Based on this latter work, Ferret (2013) addresses the problem of low frequency words. To better consider this information, a set of positive and negative examples are selected with an unsupervised classifier. A supervised classifier is then applied for re-ranking the semantic neighbours. The method allows to improve the quality of the similarity relation between nouns with low or mid frequency. The sparseness problem may also be tackled from the algorithmic point of view by limiting the dimensions of the context matrix, especially by smoothing it in order to reduce the number of vector compounds (Turney and Pantel, 2010). Thus, Lat"
W14-4801,P98-1082,1,0.535062,"l variation (TV) Terminological variant acquisition method proposed by (Jacquemin, 2001) exploits morphosyntactic transformation rules, as the insertion or the permutation, (chirurgie coronarienne (coronary surgery) / chirurgie de revascularisation coronarienne (Coronary revascularisation surgery)) to identify semantic relations between terms. The terminological variation rules, essentially the insertion on our French corpus, allow to acquire 171 hypernymy relations. • Semantic compositionality (Syn) For context normalisation, we use 168 synonymy relations acquired with the method defined in (Hamon et al., 1998). Based on the semantic compositionality principle, a synonymy relation is inferred between complex terms, if at least one of their component are synonyms (infection de blessure (wound infection) and septicit de blessure (wound sepsis)). 4 Distributional context generalisation and normalisation A solution to the problem of data sparsity on specialised corpora or smaller size corpora consists in increasing the density of the context matrix by disregarding superficial variations of contexts that are not strongly statistically significant or that result from the noise of the distributional method"
W14-4801,W14-1118,0,0.0529631,"Missing"
W14-4801,J07-2002,0,0.0803431,"Missing"
W14-4801,E14-1025,0,0.0295644,"does not solve the initial issue of building a huge co-occurence matrix. Random Indexing (RI) (Kanerva et al., 2000) may be considered as a solution to this problem, as it incrementally builds the context matrix according to an index vector of the target word randomly generated, as well as reducing the matrix dimension. RI and LSA have similar performance when identifying synonyms in a similar way than the TOEFL test (Karlgren and Sahlgren, 2001). Recently, the selection of the best contexts combined with a normalisation of their weights allows to improve the quality of a SVD reduced matrix (Polajnar and Clark, 2014). In the context of definition retrieval and phrase similarity computation, their impact depends on the compositional semantics operators used. As above work, we aim at incorporating semantic information within distributional contexts, but by reducing the number of contexts and increasing their frequency. Contrary to SVD based methods that limit the contexts by removing information, here we both generalise and normalise contexts through the integration of additional semantic knowledge computed from our corpora. 2 3 Material In this section, we present the corpus we use. We also describe the ap"
W14-4801,2003.mtsummit-papers.42,0,0.00972885,"txti (w)) – VSM/ALL3. With context normalisation, we consider only one set of experiment, with normalisation of contexts (VSM/Syn). All the experiments have been performed on both window sizes: 5 words (± 2 words, centered on the target) and 21 words (± 10 words, centered on the target). Indeed the window size influences the number and quality, but also the type of the relations acquired with distributional analysis. In general, a small window size (5 words) allows to have a highest number of relevant contexts for a given target word, but leads to more data sparsity than with a larger window (Rapp, 2003). Furthermore, the results obtained with small size windows are of greatest quality, especially for classical relations (synonymy, antonymy, hypernymy, meronymy, etc.), whereas larger windows are more adapted to the identification of domain specific relations (Sahlgren, 2006; Peirsman et al., 2008). 5.2 Evaluation As usual to evaluate distributional methods, the obtained relations are considered as semantic neighbour sets associated to target words, and the quality of the neighbour sets is measured by comparing them to semantic relations issued from existing resources (Curran, 2004; Ferret, 20"
W14-4801,E09-3009,0,0.0262639,"robabilities to arbitrary sequences of words based on their co-occurrence frequencies in a training corpora (Yuret, 2012). These substitutes and their probabilities are then used to create word pairs to feed a co-occurrence model and to cluster the word list. The limit of such methods is their performance which depends on vocabulary size and requires an increasing amount of training data. Influence on contexts may also be done by incorporating additional semantic information: it has been shown that such information used to modify the standard distributional method can improve its performance (Tsatsaronis and Panagiotopoulou, 2009). This semantic information, in particular semantic relations, may be automatically computed or issued from an existing resource. Thus, with a bootstrap method, Zhitomirsky-Geffet and Dagan (2009) modify the context weights with the semantic neighbours proposed by a distributional similarity measure. Based on this latter work, Ferret (2013) addresses the problem of low frequency words. To better consider this information, a set of positive and negative examples are selected with an unsupervised classifier. A supervised classifier is then applied for re-ranking the semantic neighbours. The meth"
W14-4801,J05-4002,0,0.011744,"senting the vector space (Chatterjee and Mohan, 2008): many elements are equal to zero because only few contexts are associated to a target word. This disadvantage is partly due to word distribution in corpora: whatever the corpus size, most words have low frequencies and a very limited set of contexts compared to the number of words in the corpora. These last two elements make the similarity between two words hard to compute. Hence, methods based on the distributional hypothesis show better results when much information is available and especially with general corpora, usually of great size (Weeds and Weir, 2005; van der Plas, 2008). But the reduction of data sparseness is still an important aspect with general corpora. It is as well a major issue when working with specialised corpora. Indeed, these corpora are characterised by smaller sizes, and with frequencies and a number of different contexts especially lower. We focus here on this last point. We propose a rule-based method that aims at reducing context diversity by generalising contexts. The frequency of the obtained distributional contexts is then increased and, consequently, data sparseness and the dimensions of the vector space model are red"
W14-4801,J09-3004,0,0.0121682,"to feed a co-occurrence model and to cluster the word list. The limit of such methods is their performance which depends on vocabulary size and requires an increasing amount of training data. Influence on contexts may also be done by incorporating additional semantic information: it has been shown that such information used to modify the standard distributional method can improve its performance (Tsatsaronis and Panagiotopoulou, 2009). This semantic information, in particular semantic relations, may be automatically computed or issued from an existing resource. Thus, with a bootstrap method, Zhitomirsky-Geffet and Dagan (2009) modify the context weights with the semantic neighbours proposed by a distributional similarity measure. Based on this latter work, Ferret (2013) addresses the problem of low frequency words. To better consider this information, a set of positive and negative examples are selected with an unsupervised classifier. A supervised classifier is then applied for re-ranking the semantic neighbours. The method allows to improve the quality of the similarity relation between nouns with low or mid frequency. The sparseness problem may also be tackled from the algorithmic point of view by limiting the d"
W14-4812,W07-1007,0,0.121199,"ocabularies are aligned (in examples (4) to (6), the technical terms are followed by their non-technical equivalents). The first initiative of the kind appeared with the collaborative effort Consumer Health Vocabulary (Zeng and Tse, 2006) (examples in (4)). One of the methods was applied to the most frequently occurring medical queries aligned to the UMLS (Unified Medical Language System) concepts (Lindberg et al., 1993). Another work exploited a small corpus and several statistical association measures for building aligned lexicon with technical terms from the UMLS and their lay equivalents (Elhadad and Sutaria, 2007). Similar work in other languages followed. In French, researchers proposed methods for the acquisition of syntactic variation (Del´eger and Zweigenbaum, 2008; Cartoni and Del´eger, 2011) from comparable specialized and non-specialized corpora, that led to the detection of verb/noun variations (examples in (5)) and a larger set of syntactic variations (examples in (6)). Besides, research on the acquisition of terminological variation (Hahn et al., 2001), synonymy (Fern´andez-Silva et al., 2011) and paraphrasing (Max et al., 2012) is also relevant to outline the topics. (4) {myocardial infarcti"
W14-4812,S12-1066,0,0.0433777,"on. The lexical simplification helps to make text easier to understand. Lexical simplification of texts in English has been addressed during the SemEval 2012 challengea . Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual information and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length, n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012); n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). • Dedicated resources. The building of resources suitable for performing the simplification is another related research topics. Such resources are mainly two-fold lexica in which specialized and non-specialized vocabularies are aligned (in examples (4) to (6), the technical terms are followed by their non-technical equivalents). The fi"
W14-4812,S12-1054,0,0.174723,"Missing"
W14-4812,S12-1068,0,0.0563235,"word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual information and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length, n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012); n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). • Dedicated resources. The building of resources suitable for performing the simplification is another related research topics. Such resources are mainly two-fold lexica in which specialized and non-specialized vocabularies are aligned (in examples (4) to (6), the technical terms are followed by their non-technical equivalents). The first initiative of the kind appeared with the collaborative effort Consumer Health Vocabulary (Zeng and Tse, 2006) (examples in (4)). One of the methods was applied to the most frequently occurring medical"
W14-4812,D12-1066,0,0.137719,"echnical terms from the UMLS and their lay equivalents (Elhadad and Sutaria, 2007). Similar work in other languages followed. In French, researchers proposed methods for the acquisition of syntactic variation (Del´eger and Zweigenbaum, 2008; Cartoni and Del´eger, 2011) from comparable specialized and non-specialized corpora, that led to the detection of verb/noun variations (examples in (5)) and a larger set of syntactic variations (examples in (6)). Besides, research on the acquisition of terminological variation (Hahn et al., 2001), synonymy (Fern´andez-Silva et al., 2011) and paraphrasing (Max et al., 2012) is also relevant to outline the topics. (4) {myocardial infarction, heart attack}, {abortion, termination of pregnancy}, {acrodynia, pink disease} (5) {consommation r´eguli`ere, consommer de fac¸on r´eguli`ere} (regular use), {gˆene a` la lecture, empˆeche de lire} (reading difficulty), {´evolution de l’affection, la maladie e´ volue} (evolution of the condition) (6) {retard de cicatrisation, retarder la cicatrisation} (delay the healing), {apports caloriques, apport en calories} (calorie supply), {calculer les doses, doses sont calcul´ees} (calculate the dose), {efficacit´e est renforc´ee, r"
W14-4812,S12-1069,0,0.091097,"eiler et al., 2007; Leroy et al., 2008; Franc¸ois and Fairon, 2013). • Lexical simplification. The lexical simplification helps to make text easier to understand. Lexical simplification of texts in English has been addressed during the SemEval 2012 challengea . Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual information and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length, n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012); n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). • Dedicated resources. The building of resources suitable for performing the simplification is another related research topics. Such resources are mainly two-fold lexica in which specialized and non-specialized vocabularies are aligne"
W14-6301,W14-6306,0,0.0610287,"Missing"
W14-6301,W14-6304,0,0.0536407,"Missing"
W14-6301,W14-6308,0,0.035181,"Missing"
W19-5011,S16-1160,0,0.0139093,"g which are important factors in readability, such as cohesion, syntactic ambiguity, rhetorical organization, and propositional density (Collins-Thompson, 2014). Moreover, traditional readability measures were demonstrated to be unreliable for non-traditional documents (Si and P. Callan, 2001). As a result of such limitations and due to the recent growth of computational and data resources, the focus of NLP researchers moved to computational readability measurements, which rely on the use of machine learning algorithms on richer linguistic features (Malmasi et al., 2016; Ronzano et al., 2016; Bingel et al., 2016). 3 Materials For the experiments, we used the publicly available set of words with annotations1 . The process of words collection and annotation is briefly described below. 3.1 Linguistic data description The set of required biomedical terms was obtained from the French part of Snomed International2 (Cˆot´e et al., 1993). Snomed Int contains 151,104 medical terms organized into eleven semantic axes such as disorders and abnormalities, procedures, chemical products, living organisms, anatomy, social status, etc. For the word understandability study, five axes related to the main medical notion"
W19-5011,S16-1157,0,0.0706934,"Missing"
W19-5011,goeuriot-etal-2008-characterization,1,0.489302,"s (acn´e (acne), fragment (fragment)). Not so much effort has been devoted to the exploitation of NLP potential in the measurement of readability of medical texts. In the biomedical domain, as well as in general language, the readability assessment is currently approached as a classification task. The difference is that in the former a much smaller variety of features has been tested: a combination of classical readability formulas with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters (Poprat et al., 2006); stylistic (Grabar et al., 2007) or discursive (Goeuriot et al., 2008) features; morphological features (Chmielik and Grabar, 2011); combinations of different features from those listed above (Zeng-Treiler et al., 2007). Among the recent experiments dedicated to readability study in the medical domain are, for example, manual rating of medical words (Zheng et al., 2002), automatic rating of medical words on the basis of their presence in different vocabularies (Borst et al., 2008), exploitation of machine learning approach with various features (Grabar et al., 2014). The last experiment achieved up to 85.0 F-score on individual annotations. 1 http://natalia.grab"
W19-5011,W14-1202,1,0.896377,"Missing"
W19-5011,S16-1154,0,0.0256188,"xt, ignoring deeper levels of text processing which are important factors in readability, such as cohesion, syntactic ambiguity, rhetorical organization, and propositional density (Collins-Thompson, 2014). Moreover, traditional readability measures were demonstrated to be unreliable for non-traditional documents (Si and P. Callan, 2001). As a result of such limitations and due to the recent growth of computational and data resources, the focus of NLP researchers moved to computational readability measurements, which rely on the use of machine learning algorithms on richer linguistic features (Malmasi et al., 2016; Ronzano et al., 2016; Bingel et al., 2016). 3 Materials For the experiments, we used the publicly available set of words with annotations1 . The process of words collection and annotation is briefly described below. 3.1 Linguistic data description The set of required biomedical terms was obtained from the French part of Snomed International2 (Cˆot´e et al., 1993). Snomed Int contains 151,104 medical terms organized into eleven semantic axes such as disorders and abnormalities, procedures, chemical products, living organisms, anatomy, social status, etc. For the word understandability study,"
W19-5013,S13-2056,0,0.0882014,"Missing"
W19-5013,hamon-etal-2017-pomelo,1,0.891174,"Missing"
W19-5029,J92-4003,0,0.506649,"Missing"
W19-5029,P10-1052,0,0.120129,"Missing"
W19-5029,L18-1201,0,0.0614524,"Missing"
W19-5029,W18-5614,1,0.510736,"ized domains (e.g., clinical notes or justice decisions) are not easily accessible unless authorization (Chapman et al., 2011). 273 Proceedings of the BioNLP 2019 workshop, pages 273–282 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 2.1 Corpus and annotation guidelines work, we only focus on the clinical case description. This set has been manually annotated with general and fine-grained information, which is described in the two following sections. This corpus is part of a larger and yet growing corpus, which currently contains over 4,100 clinical cases (Grabar et al., 2018). Corpus In the clinical domain, in order to overcome the privacy and ethical issues when working on electronic health records, one solution consists in using clinical case reports. Indeed, it is quite common to find freely available publications from scientific journals which report in clinical cases of real de-identified or fake patients. Such clinical cases are usually published and discussed to improve medical knowledge (Atkinson, 1992) of colleagues and medical students. One may find scientific journals specifically dedicated to case reports, such as the Journal of Medical Case Reports la"
W19-5029,W11-0411,1,0.539433,"Recall and Fmeasure values (Sebastiani, 2002). General information We computed interannotator agreement scores on the normalized values for general information: Age, Gender and Outcome, and on the annotated text spans for Origin. We achieved excellent agreements for Age and Gender (κ=0.939), differences being due to omissions; poor agreement for Outcome (κ=0.369) due to differences of interpretation between close values (e.g., recovery vs. improvement for long-term diseases); and very low agreement for Origin (κ=-0.762) since spans of text were often distinct between annotators. As stated by Grouin et al. (2011), the κ metric is not well suited for annotations of text since it relies on a random baseline for which the number of units that may be annotated is hard to define. As a consequence, the classical F-measure is often used as an approximation of inter-annotator agreement. In the following experiments, we present the inter-annotator agreements through Precision, Recall, and F-measure. P 0.5660 0.5714 0.7042 0.3151 0.3744 0.7500 0.4260 0.5135 0.5597 0.4328 0.5563 0.2596 0.5567 0.3077 0.5950 0.5378 0.4426 R 0.8511 0.2857 0.2747 0.8519 0.8913 0.5816 0.8267 0.2879 0.8824 0.8056 0.8778 0.6116 0.6888"
