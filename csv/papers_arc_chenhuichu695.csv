2021.wat-1.1,Overview of the 8th Workshop on {A}sian Translation,2021,-1,-1,12,0,283,toshiaki nakazawa,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"This paper presents the results of the shared tasks from the 8th workshop on Asian translation (WAT2021). For the WAT2021, 28 teams participated in the shared tasks and 24 teams submitted their translation results for the human evaluation. We also accepted 5 research papers. About 2,100 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated."
2021.wat-1.20,{TMEKU} System for the {WAT}2021 Multimodal Translation Task,2021,-1,-1,4,1,366,yuting zhao,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"We introduce our TMEKU system submitted to the English-Japanese Multimodal Translation Task for WAT 2021. We participated in the Flickr30kEnt-JP task and Ambiguous MSCOCO Multimodal task under the constrained condition using only the officially provided datasets. Our proposed system employs soft alignment of word-region for multimodal neural machine translation (MNMT). The experimental results evaluated on the BLEU metric provided by the WAT 2021 evaluation site show that the TMEKU system has achieved the best performance among all the participated systems. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use."
2021.naacl-main.169,{WRIME}: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations,2021,-1,-1,2,0.311035,367,tomoyuki kajiwara,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We annotate 17,000 SNS posts with both the writer{'}s subjective emotional intensity and the reader{'}s objective one to construct a Japanese emotion analysis dataset. In this study, we explore the difference between the emotional intensity of the writer and that of the readers with this dataset. We found that the reader cannot fully detect the emotions of the writer, especially anger and trust. In addition, experimental results in estimating the emotional intensity show that it is more difficult to estimate the writer{'}s subjective labels than the readers{'}. The large gap between the subjective and objective emotions imply the complexity of the mapping from a post to the subjective emotion intensities, which also leads to a lower performance with machine learning models."
2021.acl-srw.8,Attending Self-Attention: A Case Study of Visually Grounded Supervision in Vision-and-Language Transformers,2021,-1,-1,4,0,12436,jules samaran,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"The impressive performances of pre-trained visually grounded language models have motivated a growing body of research investigating what has been learned during the pre-training. As a lot of these models are based on Transformers, several studies on the attention mechanisms used by the models to learn to associate phrases with their visual grounding in the image have been conducted. In this work, we investigate how supervising attention directly to learn visual grounding can affect the behavior of such models. We compare three different methods on attention supervision and their impact on the performances of a state-of-the-art visually grounded language model on two popular vision-and-language tasks."
2021.acl-srw.9,Video-guided Machine Translation with Spatial Hierarchical Attention Network,2021,-1,-1,3,0,12439,weiqi gu,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"Video-guided machine translation, as one type of multimodal machine translations, aims to engage video contents as auxiliary information to address the word sense ambiguity problem in machine translation. Previous studies only use features from pretrained action detection models as motion representations of the video to solve the verb sense ambiguity, leaving the noun sense ambiguity a problem. To address this problem, we propose a video-guided machine translation system by using both spatial and motion representations in videos. For spatial features, we propose a hierarchical attention network to model the spatial information from object-level to video-level. Experiments on the VATEX dataset show that our system achieves 35.86 BLEU-4 score, which is 0.51 score higher than the single model of the SOTA method."
2021.acl-long.226,Lightweight Cross-Lingual Sentence Representation Learning,2021,-1,-1,3,1,13030,zhuoyuan mao,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Large-scale models for learning fixed-dimensional cross-lingual sentence representations like LASER (Artetxe and Schwenk, 2019b) lead to significant improvement in performance on downstream tasks. However, further increases and modifications based on such large-scale models are usually impractical due to memory limitations. In this work, we introduce a lightweight dual-transformer architecture with just 2 layers for generating memory-efficient cross-lingual sentence representations. We explore different training tasks and observe that current cross-lingual training tasks leave a lot to be desired for this shallow architecture. To ameliorate this, we propose a novel cross-lingual language model, which combines the existing single-word masked language model with the newly proposed cross-lingual token-level reconstruction task. We further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model."
2020.wnut-1.62,{IDSOU} at {WNUT}-2020 Task 2: Identification of Informative {COVID}-19 {E}nglish Tweets,2020,-1,-1,3,0,12618,sora ohashi,Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020),0,We introduce the IDSOU submission for the WNUT-2020 task 2: identification of informative COVID-19 English Tweets. Our system is an ensemble of pre-trained language models such as BERT. We ranked 16th in the F1 score.
2020.wat-1.5,Meta Ensemble for {J}apanese-{C}hinese Neural Machine Translation: {K}yoto-{U}+{ECNU} Participation to {WAT} 2020,2020,-1,-1,3,1,13030,zhuoyuan mao,Proceedings of the 7th Workshop on Asian Translation,0,"This paper describes the Japanese-Chinese Neural Machine Translation (NMT) system submitted by the joint team of Kyoto University and East China Normal University (Kyoto-U+ECNU) to WAT 2020 (Nakazawa et al.,2020). We participate in APSEC Japanese-Chinese translation task. We revisit several techniques for NMT including various architectures, different data selection and augmentation methods, denoising pre-training, and also some specific tricks for Japanese-Chinese translation. We eventually perform a meta ensemble to combine all of the models into a single model. BLEU results of this meta ensembled model rank the first both on 2 directions of ASPEC Japanese-Chinese translation."
2020.lrec-1.238,Constructing a Public Meeting Corpus,2020,-1,-1,2,0,17088,koji tanaka,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we propose a full pipeline of analysis of a large corpus about a century of public meeting in historical Australian news papers, from construction to visual exploration. The corpus construction method is based on image processing and OCR. We digitize and transcribe texts of the specific topic of public meeting. Experiments show that our proposed method achieves a F-score of 87.8{\%} for corpus construction. As a result, we built a content search tool for temporal and semantic content analysis."
2020.lrec-1.836,Annotation of Adverse Drug Reactions in Patients{'} Weblogs,2020,-1,-1,3,0,6514,yuki arase,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Adverse drug reactions are a severe problem that significantly degrade quality of life, or even threaten the life of patients. Patient-generated texts available on the web have been gaining attention as a promising source of information in this regard. While previous studies annotated such patient-generated content, they only reported on limited information, such as whether a text described an adverse drug reaction or not. Further, they only annotated short texts of a few sentences crawled from online forums and social networking services. The dataset we present in this paper is unique for the richness of annotated information, including detailed descriptions of drug reactions with full context. We crawled patient{'}s weblog articles shared on an online patient-networking platform and annotated the effects of drugs therein reported. We identified spans describing drug reactions and assigned labels for related drug names, standard codes for the symptoms of the reactions, and types of effects. As a first dataset, we annotated 677 drug reactions with these detailed labels based on 169 weblog articles by Japanese lung cancer patients. Our annotation dataset is made publicly available at our web site (https://yukiar.github.io/adr-jp/) for further research on the detection of adverse drug reactions and more broadly, on patient-generated text processing."
2020.eamt-1.12,Double Attention-based Multimodal Neural Machine Translation with Semantic Image Regions,2020,-1,-1,4,1,366,yuting zhao,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"Existing studies on multimodal neural machine translation (MNMT) have mainly focused on the effect of combining visual and textual modalities to improve translations. However, it has been suggested that the visual modality is only marginally beneficial. Conventional visual attention mechanisms have been used to select the visual features from equally-sized grids generated by convolutional neural networks (CNNs), and may have had modest effects on aligning the visual concepts associated with textual objects, because the grid visual features do not capture semantic information. In contrast, we propose the application of semantic image regions for MNMT by integrating visual and textual features using two individual attention mechanisms (double attention). We conducted experiments on the Multi30k dataset and achieved an improvement of 0.5 and 0.9 BLEU points for English-German and English-French translation tasks, compared with the MNMT with grid visual features. We also demonstrated concrete improvements on translation performance benefited from semantic image regions."
2020.coling-tutorials.3,Multilingual Neural Machine Translation,2020,0,28,2,0.229286,286,raj dabre,Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts,0,"The advent of neural machine translation (NMT) has opened up exciting research in building multilingual translation systems i.e. translation models that can handle more than one language pair. Many advances have been made which have enabled (1) improving translation for low-resource languages via transfer learning from high resource languages; and (2) building compact translation models spanning multiple languages. In this tutorial, we will cover the latest advances in NMT approaches that leverage multilingualism, especially to enhance low-resource translation. In particular, we will focus on the following topics: modeling parameter sharing for multi-way models, massively multilingual models, training protocols, language divergence, transfer learning, zero-shot/zero-resource learning, pivoting, multilingual pre-training and multi-source translation."
2020.acl-main.33,Text Classification with Negative Supervision,2020,-1,-1,4,0,12618,sora ohashi,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Advanced pre-trained models for text representation have achieved state-of-the-art performance on various text classification tasks. However, the discrepancy between the semantic similarity of texts and labelling standards affects classifiers, i.e. leading to lower performance in cases where classifiers should assign different labels to semantically similar texts. To address this problem, we propose a simple multitask learning model that uses negative supervision. Specifically, our model encourages texts with different labels to have distinct representations. Comprehensive experiments show that our model outperforms the state-of-the-art pre-trained model on both single- and multi-label classifications, sentence and document classifications, and classifications in three different languages."
D19-1146,Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource Neural Machine Translation,2019,0,5,3,0.293228,286,raj dabre,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"This paper highlights the impressive utility of multi-parallel corpora for transfer learning in a one-to-many low-resource neural machine translation (NMT) setting. We report on a systematic comparison of multistage fine-tuning configurations, consisting of (1) pre-training on an external large (209k{--}440k) parallel corpus for English and a helping target language, (2) mixed pre-training or fine-tuning on a mixture of the external and low-resource (18k) target parallel corpora, and (3) pure fine-tuning on the target parallel corpora. Our experiments confirm that multi-parallel corpora are extremely useful despite their scarcity and content-wise redundancy thus exhibiting the true power of multilingualism. Even when the helping target language is not one of the target languages of our concern, our multistage fine-tuning can give 3{--}9 BLEU score gains over a simple one-to-one model."
Y18-3015,"{O}saka {U}niversity {MT} Systems for {WAT} 2018: Rewarding, Preordering, and Domain Adaptation",2018,0,0,3,0,27453,yuki kawara,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
P18-3004,Recursive Neural Network Based Preordering for {E}nglish-to-{J}apanese Machine Translation,2018,18,1,2,0,27453,yuki kawara,"Proceedings of {ACL} 2018, Student Research Workshop",0,"The word order between source and target languages significantly influences the translation quality. Preordering can effectively address this problem. Previous preordering methods require a manual feature design, making language dependent design difficult. In this paper, we propose a preordering method with recursive neural networks that learn features from raw inputs. Experiments show the proposed method is comparable to the state-of-the-art method but without a manual feature design."
C18-1111,A Survey of Domain Adaptation for Neural Machine Translation,2018,76,21,1,1,293,chenhui chu,Proceedings of the 27th International Conference on Computational Linguistics,0,"Neural machine translation (NMT) is a deep learning based approach for machine translation, which yields the state-of-the-art translation performance in scenarios where large-scale parallel corpora are available. Although the high-quality and domain-specific translation is crucial in the real world, domain-specific corpora are usually scarce or nonexistent, and thus vanilla NMT performs poorly in such scenarios. Domain adaptation that leverages both out-of-domain parallel corpora as well as monolingual corpora for in-domain translation, is very important for domain-specific translation. In this paper, we give a comprehensive survey of the state-of-the-art domain adaptation techniques for NMT."
C18-1295,i{P}araphrasing: Extracting Visually Grounded Paraphrases via an Image,2018,57,1,1,1,293,chenhui chu,Proceedings of the 27th International Conference on Computational Linguistics,0,"A paraphrase is a restatement of the meaning of a text in other words. Paraphrases have been studied to enhance the performance of many natural language processing tasks. In this paper, we propose a novel task iParaphrasing to extract visually grounded paraphrases (VGPs), which are different phrasal expressions describing the same visual concept in an image. These extracted VGPs have the potential to improve language and image multimodal tasks such as visual question answering and image captioning. How to model the similarity between VGPs is the key of iParaphrasing. We apply various existing methods as well as propose a novel neural network-based method with image attention, and report the results of the first attempt toward iParaphrasing."
P17-2061,An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation,2017,16,47,1,1,293,chenhui chu,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we propose a novel domain adaptation method named {``}mixed fine tuning{''} for neural machine translation (NMT). We combine two existing approaches namely fine tuning and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-of-domain corpora. All corpora are augmented with artificial tags to indicate specific domains. We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings."
W16-5407,{SCTB}: A {C}hinese Treebank in Scientific Domain,2016,0,0,1,1,293,chenhui chu,Proceedings of the 12th Workshop on {A}sian Language Resources ({ALR}12),0,"Treebanks are curial for natural language processing (NLP). In this paper, we present our work for annotating a Chinese treebank in scientific domain (SCTB), to address the problem of the lack of Chinese treebanks in this domain. Chinese analysis and machine translation experiments conducted using this treebank indicate that the annotated treebank can significantly improve the performance on both tasks. This treebank is released to promote Chinese NLP research in scientific domain."
W16-4616,{K}yoto {U}niversity Participation to {WAT} 2016,2016,0,11,2,0,17599,fabien cromieres,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"We describe here our approaches and results on the WAT 2016 shared translation tasks. We tried to use both an example-based machine translation (MT) system and a neural MT system. We report very good translation results, especially when using neural MT for Chinese-to-Japanese translation."
W16-2201,Cross-language Projection of Dependency Trees with Constrained Partial Parsing for Tree-to-Tree Machine Translation,2016,23,1,2,0,33916,yu shen,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,None
P16-3002,Dependency Forest based Word Alignment,2016,20,0,2,0,34384,hitoshi otsuki,Proceedings of the {ACL} 2016 Student Research Workshop,0,"A hierarchical word alignment model that searches for k-best partial alignments on target constituent 1-best parse trees has been shown to outperform previous models. However, relying solely on 1-best parses trees might hinder the search for good alignments because 1-best trees are not necessarily the best for word alignment tasks in practice. This paper introduces a dependency forest based word alignment model, which utilizes target dependency forests in an attempt to minimize the impact on limitations attributable to 1-best parse trees. We present how k-best alignments are constructed over target-side dependency forests. Alignment experiments on the Japanese-English language pair show a relative error reduction of 4% of the alignment score compared to a model with 1-best parse trees."
L16-1101,Paraphrasing Out-of-Vocabulary Words with Word Embeddings and Semantic Lexicons for Low Resource Statistical Machine Translation,2016,15,3,1,1,293,chenhui chu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Out-of-vocabulary (OOV) word is a crucial problem in statistical machine translation (SMT) with low resources. OOV paraphrasing that augments the translation model for the OOV words by using the translation knowledge of their paraphrases has been proposed to address the OOV problem. In this paper, we propose using word embeddings and semantic lexicons for OOV paraphrasing. Experiments conducted on a low resource setting of the OLYMPICS task of IWSLT 2012 verify the effectiveness of our proposed method."
L16-1348,Simultaneous Sentence Boundary Detection and Alignment with Pivot-based Machine Translation Generated Lexicons,2016,10,1,2,0,35088,antoine bourlon,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Sentence alignment is a task that consists in aligning the parallel sentences in a translated article pair. This paper describes a method to perform sentence boundary detection and alignment simultaneously, which significantly improves the alignment accuracy on languages like Chinese with uncertain sentence boundaries. It relies on the definition of hard (certain) and soft (uncertain) punctuation delimiters, the latter being possibly ignored to optimize the alignment result. The alignment method is used in combination with lexicons automatically generated from the input article pairs using pivot-based MT, achieving better coverage of the input words with fewer entries than pre-existing dictionaries. Pivot-based MT makes it possible to build dictionaries for language pairs that have scarce parallel data. The alignment method is implemented in a tool that will be freely available in the near future."
L16-1468,Parallel Sentence Extraction from Comparable Corpora with Neural Network Features,2016,7,4,1,1,293,chenhui chu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Parallel corpora are crucial for machine translation (MT), however they are quite scarce for most language pairs and domains. As comparable corpora are far more available, many studies have been conducted to extract parallel sentences from them for MT. In this paper, we exploit the neural network features acquired from neural MT for parallel sentence extraction. We observe significant improvements for both accuracy in sentence extraction and MT performance."
C16-1029,"Consistent Word Segmentation, Part-of-Speech Tagging and Dependency Labelling Annotation for {C}hinese Language",2016,10,0,4,0,35693,mo shen,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we propose a new annotation approach to Chinese word segmentation, part-of-speech (POS) tagging and dependency labelling that aims to overcome the two major issues in traditional morphology-based annotation: Inconsistency and data sparsity. We re-annotate the Penn Chinese Treebank 5.0 (CTB5) and demonstrate the advantages of this approach compared to the original CTB5 annotation through word segmentation, POS tagging and machine translation experiments."
Y15-2010,Cross-language Projection of Dependency Trees for Tree-to-tree Machine Translation,2015,14,2,2,0,33916,yu shen,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation: Posters",0,"Syntax-based machine translation (MT) is an attractive approach for introducing additional linguistic knowledge in corpus-based MT. Previous studies have shown that treeto-string and string-to-tree translation models perform better than tree-to-tree translation models since tree-to-tree models require two high quality parsers on the source as well as the target language side. In practice, high quality parsers for both languages are difficult to obtain and thus limit the translation quality. In this paper, we explore a method to transfer parse trees from the language side which has a high quality parser to the side which has a low quality parser to obtain transferred parse trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately."
Y15-1033,Large-scale Dictionary Construction via Pivot-based Statistical Machine Translation with Significance Pruning and Neural Network Features,2015,15,0,2,0.293228,286,raj dabre,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"We present our ongoing work on large-scale Japanese-Chinese bilingual dictionary construction via pivot-based statistical machine translation. We utilize statistical significance pruning to control noisy translation pairs that are induced by pivoting. We construct a large dictionary which we manually verify to be of a high quality. We then use this dictionary and a parallel corpus to learn bilingual neural network language models to obtain features for reranking the n-best list, which leads to an absolute improvement of 5% in accuracy when compared to a setting that does not use significance pruning and reranking."
W15-5006,{K}yoto{EBMT} System Description for the 2nd Workshop on {A}sian Translation,2015,-1,-1,3,0,30411,john richardson,Proceedings of the 2nd Workshop on {A}sian Translation ({WAT}2015),0,None
Y14-1032,Improving Statistical Machine Translation Accuracy Using Bilingual Lexicon Extractionwith Paraphrases,2014,30,2,1,1,293,chenhui chu,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"Statistical machine translation (SMT) suffers from theaccuracy problemthat the translation pairs and their feature scores in the transla- tion model can be inaccurate. Theaccuracy problemis caused by the quality of the unsu- pervised methods used for translation model learning. Previous studies propose estimating comparable features for the translation pairs in the translation model from comparable cor- pora, to improve the accuracy of the transla- tion model. Comparable feature estimation is based on bilingual lexicon extraction (BLE) technology. However, BLE suffers from the data sparseness problem, which makes the comparable features inaccurate. In this paper, we propose using paraphrases to address this problem. Paraphrases are used to smooth the vectors used in comparable feature estimation with BLE. In this way, we improve the qual- ity of comparable features, which can improve the accuracy of the translation model thus im- prove SMT performance. Experiments con- ducted on Chinese-English phrase-based SMT (PBSMT) verify the effectiveness of our pro- posed method."
chu-etal-2014-constructing,Constructing a {C}hinese{---}{J}apanese Parallel Corpus from {W}ikipedia,2014,11,9,1,1,293,chenhui chu,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Parallel corpora are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as ChineseâJapanese. As comparable corpora are far more available, many studies have been conducted to automatically construct parallel corpora from comparable corpora. This paper presents a robust parallel sentence extraction system for constructing a ChineseâJapanese parallel corpus from Wikipedia. The system is inspired by previous studies that mainly consist of a parallel sentence candidate filter and a binary classifier for parallel sentence identification. We improve the system by using the common Chinese characters for filtering and two novel feature sets for classification. Experiments show that our system performs significantly better than the previous studies for both accuracy in parallel sentence extraction and SMT performance. Using the system, we construct a ChineseâJapanese parallel corpus with more than 126k highly accurate parallel sentences from Wikipedia. The constructed parallel corpus is freely available at http://orchid.kuee.kyoto-u.ac.jp/{\textasciitilde}chu/resource/wiki{\_}zh{\_}ja.tgz."
W13-2505,{C}hinese{--}{J}apanese Parallel Sentence Extraction from Quasi{--}Comparable Corpora,2013,18,8,1,1,293,chenhui chu,Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,0,"Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinesexe2x80x90Japanese. Many studies have been conducted on extracting parallel sentences from noisy parallel or comparable corpora. We extract Chinesexe2x80x90Japanese parallel sentences from quasixe2x80x90comparable corpora, which are available in far larger quantities. The task is significantly more difficult than the extraction from noisy parallel or comparable corpora. We extend a previous study that treats parallel sentence identification as a binary classification problem. Previous method of classifier training by the Cartesian product is not practical, because it differs from the real process of parallel sentence extraction. We propose a novel classifier training method that simulates the real sentence extraction process. Furthermore, we use linguistic knowledge of Chinese character features. Experimental results on quasixe2x80x90 comparable corpora indicate that our proposed approach performs significantly better than the previous study."
I13-1163,Accurate Parallel Fragment Extraction from Quasi{--}Comparable Corpora using Alignment Model and Translation Lexicon,2013,15,10,1,1,293,chenhui chu,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Although parallel sentences rarely exist in quasixe2x80x90comparable corpora, there could be parallel fragments that are also helpful for statistical machine translation (SMT). Previous studies cannot accurately extract parallel fragments from quasixe2x80x90comparable corpora. To solve this problem, we propose an accurate parallel fragment extraction system that uses an alignment model to locate the parallel fragment candidates, and uses an accurate lexicon filter to identify the truly parallel ones. Experimental results indicate that our system can accurately extract parallel fragments, and our proposed method significantly outperforms a statexe2x80x90ofxe2x80x90thexe2x80x90art approach. Furthermore, we investigate the factors that may affect the performance of our system in detail."
chu-etal-2012-chinese,"{C}hinese Characters Mapping Table of {J}apanese, Traditional {C}hinese and Simplified {C}hinese",2012,6,7,1,1,293,chenhui chu,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Chinese characters are used both in Japanese and Chinese, which are called Kanji and Hanzi respectively. Chinese characters contain significant semantic information, a mapping table between Kanji and Hanzi can be very useful for many Japanese-Chinese bilingual applications, such as machine translation and cross-lingual information retrieval. Because Kanji characters are originated from ancient China, most Kanji have corresponding Chinese characters in Hanzi. However, the relation between Kanji and Hanzi is quite complicated. In this paper, we propose a method of making a Chinese characters mapping table of Japanese, Traditional Chinese and Simplified Chinese automatically by means of freely available resources. We define seven categories for Kanji based on the relation between Kanji and Hanzi, and classify mappings of Chinese characters into these categories. We use a resource from Wiktionary to show the completeness of the mapping table we made. Statistical comparison shows that our proposed method makes a more complete mapping table than the current version of Wiktionary."
2012.iwslt-evaluation.12,{EBMT} system of {K}yoto {U}niversity in {OLYMPICS} task at {IWSLT} 2012,2012,15,1,1,1,293,chenhui chu,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the EBMT system of Kyoto University that participated in the OLYMPICS task at IWSLT 2012. When translating very different language pairs such as Chinese-English, it is very important to handle sentences in tree structures to overcome the difference. Many recent studies incorporate tree structures in some parts of translation process, but not all the way from model training (alignment) to decoding. Our system is a fully tree-based translation system where we use the Bayesian phrase alignment model on dependency trees and example-based translation. To improve the translation quality, we conduct some special processing for the IWSLT 2012 OLYMPICS task, including sub-sentence splitting, non-parallel sentence filtering, adoption of an optimized Chinese segmenter and rule-based decoding constraints."
2012.eamt-1.7,Exploiting Shared {C}hinese Characters in {C}hinese Word Segmentation Optimization for {C}hinese-{J}apanese Machine Translation,2012,14,15,1,1,293,chenhui chu,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"Unknown words and word segmentation granularity are two main problems in Chinese word segmentation for ChineseJapanese Machine Translation (MT). In this paper, we propose an approach of exploiting common Chinese characters shared between Chinese and Japanese in Chinese word segmentation optimization for MT aiming to solve these problems. We augment the system dictionary of a Chinese segmenter by extracting Chinese lexicons from a parallel training corpus. In addition, we adjust the granularity of the training data for the Chinese segmenter to that of Japanese. Experimental results of Chinese-Japanese MT on a phrase-based SMT system show that our approach improves MT performance significantly."
2011.mtsummit-papers.53,{J}apanese-{C}hinese Phrase Alignment Using Common {C}hinese Characters Information,2011,-1,-1,1,1,293,chenhui chu,Proceedings of Machine Translation Summit XIII: Papers,0,None
